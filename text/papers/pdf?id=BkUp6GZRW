Under review as a conference paper at ICLR 2018
BOOSTING THE ACTOR WITH DUAL CRITIC
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes a new actor-crtitic-style algorithm called Dual Actor-Critic or Dual-AC . It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic. Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.
1 INTRODUCTION
Reinforcement learning (RL) algorithms aim to learn a policy that maximizes the long-term return by sequentially interacting with an unknown environment. Value-function-based algorithms first approximate the optimal value function, which can then be used to derive a good policy. These methods (Sutton, 1988; Watkins, 1989) often take advantage of the Bellman equation and use bootstrapping to make learning more sample efficient than Monte Carlo estimation (Sutton & Barto, 1998). However, the relation between the quality of the learned value function and the quality of the derived policy is fairly weak (Bertsekas & Tsitsiklis, 1996). Policy-search-based algorithms such as REINFORCE (Williams, 1992) and others (Kakade, 2002; Schulman et al., 2015a), on the other hand, assume a fixed space of parameterized policies and search for the optimal policy parameter based on unbiased Monte Carlo estimates. The parameters are often updated incrementally along stochastic directions that on average are guaranteed to increase the policy quality. Unfortunately, they often have a greater variance that results in a higher sample complexity.
Actor-critic methods combine the benefits of these two classes, and have proved successful in a number of challenging problems such as robotics (Deisenroth et al., 2013), meta-learning (Bello et al., 2016), and games (Mnih et al., 2016). An actor-critic algorithm has two components: the actor (policy) and the critic (value function). As in policy-search methods, actor is updated towards the direction of policy improvement. However, the update directions are computed with the help of the critic, which can be more efficiently learned as in value-function-based methods (Sutton et al., 2000; Konda & Tsitsiklis, 2003; Peters et al., 2005; Bhatnagar et al., 2009; Schulman et al., 2015b). Although the use of a critic may introduce bias in learning the actor, its reduces variance and thus the sample complexity as well, compared to pure policy-search algorithms.
While the use of a critic is important for the efficiency of actor-critic algorithms, it is not entirely clear how the critic should be optimized to facilitate improvement of the actor. For some parametric family of policies, it is known that a certain compatibility condition ensures the actor parameter update is an unbiased estimate of the true policy gradient (Sutton et al., 2000). In practice, temporaldifference methods are perhaps the most popular choice to learn the critic, especially when nonlinear function approximation is used (e.g., Schulman et al. (2015b)).
In this paper, we propose a new actor-critic-style algorithm where the actor and the critic-like function, which we named as dual critic, are trained cooperatively to optimize the same objective function. The algorithm, called Dual Actor-Critic , is derived in a principled way by solving a dual form of the Bellman equation (Bertsekas & Tsitsiklis, 1996). The algorithm can be viewed as a
1

Under review as a conference paper at ICLR 2018

two-player game between the actor and the dual critic, and in principle can be solved by standard optimization algorithms like stochastic gradient descent (Section 2). We emphasize the dual critic is not fitting the value function for current policy, but that of the optimal policy. We then show that, when function approximation is used, direct application of standard optimization techniques can result in instability in training, because of the lack of convex-concavity in the objective function (Section 3). Inspired by the augmented Lagrangian method (Luenberger & Ye, 2015; Boyd et al., 2010), we propose path regularization for enhanced numerical stability. We also generalize the two-player game formulation to the multi-step case to yield a better bias/variance tradeoff. The full algorithm is derived and described in Section 4, and is compared to existing algorithms in Section 5. Finally, our algorithm is evaluated on several locomotion tasks in the MuJoCo benchmark (Todorov et al., 2012), and compares favorably to state-of-the-art algorithms across the board.

Notation. We denote a discounted MDP by M = (S, A, P, R, ), where S is the state space, A the action space, P (·|s, a) the transition probability kernel defining the distribution over next-state upon taking action a in state x, R(s, a) the corresponding immediate rewards, and   (0, 1) the
discount factor. If there is no ambiguity, we will use a f (a) and f (a)da interchangeably.

2 DUALITY OF BELLMAN OPTIMALITY EQUATION

In this section, we first describe the linear programming formula of the Bellman optimality equation (Bertsekas et al., 1995; Puterman, 2014), paving the path for a duality view of reinforcement learning via Lagrangian duality. In the main text, we focus on MDPs with finite state and action spaces for simplicity of exposition. We extend the duality view to continuous state and action spaces in Appendix A.2.

Given an initial state distribution µ(s), the reinforcement learning problem aims to find a policy

(·|s) : S  P(A) that maximizes the total expected discounted reward with P(A) denoting all

the probability measures over A, i.e.,

Es0µ(s)E

 i=0



iR(si,

ai)

,

(1)

where si+1  P (·|si, ai), ai  (·|si).

Define V (s) := maxP(A) E

 i=0

iR(si,

ai)|s0

=

s

, the Bellman optimality equation states

that:

V (s) = (T V )(s) := max
aA

R(s, a) + Es |s,a [V (s )]

,

(2)

which can be formulated as a linear program (Puterman, 2014; Bertsekas et al., 1995):

P := min
V

(1 - )Esµ(s) [V (s)]

(3)

s.t. V (s) R(s, a) + Es |s,a [V (s )] , (s, a)  S × A.

For completeness, we provide the derivation of the above equivalence in Appendix A. Without loss

of generality, we assume there exists an optimal policy for the given MDP, namely, the linear pro-

gramming is solvable. The optimal policy can be obtained from the solution to the linear program (3)

via

(s) = argmax R(s, a) + Es |s,a [V (s )] .
aA

(4)

The dual form of the LP below is often easier to solve and yield more direct relations to the optimal

policy.

D := max 0

R(s, a)(s, a)

(5)

(s,a)S ×A

s.t. aA (s , a) = (1 - )µ(s ) +  s,aS×A (s, a)P (s |s, a)ds, s  S. Since the primal LP is solvable, the dual LP is also solvable, and P - D = 0. The optimal dual variables (s, a) and optimal policy (a|s) are closely related in the following manner:

Theorem 1 (Policy from dual variables)

s,aS×A (s, a) = 1, and (a|s) =

. (s,a)
aA (s,a)

2

Under review as a conference paper at ICLR 2018

Since the goal of reinforcement learning task is to learn an optimal policy, it is appealing to deal with the Lagrangian dual which optimizes the policy directly, or its equivalent saddle point problem that jointly learns the optimal policy and value function.

Theorem 2 (Competition in one-step setting) The optimal policy , actor, and its corresponding value function V , dual critic, is the solution to the following saddle-point problem

max min L(V, , ) := (1 - ) Esµ(s) [V (s)] +

(s) (a|s) [V ](s, a), (6)

P(S),P(A) V

(s,a)S ×A

where [V ](s, a) := R(s, a) + Es |s,a[V (s )] - V (s).

The saddle point optimization (6) provides a game perspective in understanding the reinforcement learning problem (Goodfellow et al., 2014). The learning procedure can be thought as a game between the dual critic, i.e., value function for optimal policy, and the weighted actor, i.e., (s)(a|s): the dual critic V seeks the value function to satisfy the Bellman equation, while the actor  tries to generate state-action pairs that break the satisfaction. Such a competition introduces new roles for the actor and the dual critic, and more importantly, bypasses the unnecessary separation of policy evaluation and policy improvement procedures needed in a traditional actor-critic framework.

3 SOURCES OF INSTABILITY
To solve the dual problem in (6), a straightforward idea is to apply stochastic mirror prox (Nemirovski et al., 2009) or stochastic primal-dual algorithm (Chen et al., 2014) to address the saddle point problem in (6). Unfortunately, such algorithms have limited use beyond special cases. For example, for an MDP with finite state and action spaces, the one-step saddle-point problem (6) with tabular parametrization is convex-concave, and finite-sample convergence rates can be established; see e.g., Chen & Wang (2016) and Wang (2017). However, when the state/action spaces are large or continuous so that function approximation must be used, such convergence guarantees no longer hold due to lack of convex-concavity. Consequently, directly solving (6) can suffer from severe bias and numerical issues, resulting in poor performance in practice (see, e.g., Figure 1):
1. Large bias in one-step Bellman operator: It is well-known that one-step bootstrapping in temporal difference algorithms has lower variance than Monte Carlo methods and often require much fewer samples to learn. But it produces biased estimates, especially when function approximation is used. Such a bias is especially troublesome in our case as it introduces substantial noise in the gradients to update the policy parameters.
2. Absence of local convexity and duality: Using nonlinear parametrization will easily break the local convexity and duality between the original LP and the saddle point problem, which are known as the necessary conditions for the success of applying primal-dual algorithm to constrained problems (Luenberger & Ye, 2015). Thus none of the existing primal-dual type algorithms will remain stable and convergent when directly optimizing the saddle point problem without local convexity.
3. Biased stochastic gradient estimator with under-fitted value function: In the absence of local convexity, the stochastic gradient w.r.t. the policy  constructed from under-fitted value function will presumably be biased and futile to provide any meaningful improvement of the policy. Hence, naively extending the stochastic primal-dual algorithms in Chen & Wang (2016); Wang (2017) for the parametrized Lagrangian dual, will also lead to biased estimators and sample inefficiency.

4 DUAL ACTOR-CRITIC
In this section, we will introduce several techniques to bypass the three instability issues in the previous section: (1) generalization of the minimax game to the multi-step case to achieve a better bias-variance tradeoff; (2) use of path regularization in the objective function to promote local convexity and duality; and (3) use of stochastic dual ascent to ensure unbiased gradient estimates.

3

Under review as a conference paper at ICLR 2018

4.1 COMPETITION IN MULTI-STEP SETTING

In this subsection, we will extend the minimax game between the actor and critic to the multi-step setting, which has been widely utilized in temporal-difference algorithms for better bias/variance tradeoffs (Sutton & Barto, 1998; Kearns & Singh, 2000). By the definition of the optimal value function, it is easy to derive the k-step Bellman optimality equation as

V (s) = T kV  (s) := maxP E

k i=0

iR(si,

ai)

+ k+1E [V (sk+1)]

.

(7)

Similar to the one-step case, we can reformulate the multi-step Bellman optimality equation into a form similar to the LP formulation, and then apply Lagrangian duality to obtain the corresponding dual problem. This leads to the following mimimax problem:

Theorem 3 (Competition in multi-step setting) The optimal policy  and its corresponding value function V  is the solution to the following saddle point problem

max
P (S ),P (A)

min
V

Lk (V ,

,

)

=

(1

-

k+1)Eµ

[V

(s)]

+

E



{si, ai}ik=0 , sk+1

,

(8)

where  {si, ai}ik=0 , sk+1 =

k i=0



iR(si,

ai)

+



k+1

V

(sk+1)

-

V

(s)

and

E  {si, ai}ik=0 , sk+1

k

= (s0) (ai|si)p(si+1|si, ai)

{si ,ai }ki=0 ,sk+1

i=0

{si, ai}ki=0 , sk+1

.

Although the saddle-point problem (8) is similar to the one-step Lagrangian (6), it should be emphasized that due to the existence of max-operator over the space of distributions P(A), rather than A, in the multi-step Bellman optimality equation (7), the establishment of the competition in multi-step setting in Theorem 3 is not straightforward: i), its corresponding optimization is no longer a linear programming; ii), the strong duality in (8) is not obvious because of the lack of the convex-concave structure. Due to space limit, detailed analyses for generalizing the competition to multi-step setting are provided in Appendix A.

4.2 PATH REGULARIZATION

When function approximation is used, the one-step or multi-step saddle point problems (8) will no longer be convex in the primal parameter space. This could lead to severe instability and even divergence when solved by brute-force stochastic primal-dual algorithms. One then desires to partially convexify the objectives without affecting the optimal solutions. The augmented Lagrangian method (Boyd et al., 2010; Luenberger & Ye, 2015), also known as method of multipliers, is designed and widely used for such purposes. However, directly applying this method would require introducing penalty functions of the multi-step Bellman operator, which renders extra complexity and challenges in optimization. Interested readers are referred to Appendix B.2 for details.

Instead, we propose to use path regularization, as a stepping stone for promoting local convex-

ity and computation efficiency. The regularization term is motivated by the fact that the op-

timal value function satisfies the constraint V (s) = E

 i=0



iR(si,

ai)|s

.

In the same

spirit as augmented Lagrangian, we will introduce to the objective the simple penalty function

Esµ(s) Eb

 i=0

iR(si,

ai)

- V (s)

2

, resulting in

Lr(V, , ) := (1 - k+1)Eµ [V (s)] + E  {si, ai}ki=0 , sk+1

+ V Esµ(s) Eb

 i=0



iR(si,

ai

)

- V (s)

2

.

(9)

Note that in the penalty function we use some behavior policy b instead of the optimal policy, since the latter is unavailable. Adding such a regularization enables local duality in the primal parameter space. Indeed, this can be easily verified by showing the positive definite of the Hessian at a local solution.
Intuitively, one can also see that the regularization indeed provides guidance and preference to search for the solution path. We name the regularization as path regularization, since it exploits the rewards

4

Under review as a conference paper at ICLR 2018

in the sample path to regularize the solution path of value function V in the optimization procedure. As a by-product, the regularization also provides the mechanism to utilize off-policy samples from behavior policy b. Moreover, with appropriate V , the optimal solution (V , , ) is not affected. The main results of this subsection are summarized by the following theorem.
Theorem 4 (Property of path regularization) The local duality holds for Lr(V, , ). Denote (V , , ) as the solution to Bellman optimality equation, with some appropriate V , (V , , ) = argmaxP(S),P(A) argminV Lr(V, , ).
The proof of the theorem is given in Appendix B.3.
4.3 STOCHASTIC DUAL ASCENT UPDATE The remaining technical challenge is to optimize maxP(S),P(A) minV Lr(V, , ), which is the focus of this subsection. First define the regularized dual function r(, ) := minV Lr(V, , ). We first show the unbiased gradient estimator of r w.r.t.  = (, ), which are parameters associated with  and . Then, we incorporate the stochastic update rule to dual ascent algorithm (Boyd et al., 2010), resulting in the dual actor-critic (Dual-AC ) algorithm.
The gradient estimators of the dual functions can be derived using chain rule and are provided below.

Theorem 5 The regularized dual function r(, ) has gradients estimators

 r (, ) = E  {si, ai}ik=0 , sk+1  log (s) ,

 r (, ) = E  {si, ai}ik=0 , sk+1

k i=0



log

(a|s)

.

(10) (11)

Therefore, we can apply stochastic mirror descent algorithm with the gradient estimator given in

Theorem 5 to the regularized dual function r(, ). Since the dual variables are probabilistic distributions, it is natural to use KL-divergence as the prox-mapping to characterize the geometry

in the family of parameters (Amari & Nagaoka, 1993; Nemirovski et al., 2009). Specifically, in the

t-th iteration,

t = argmin -

g^t-1

+

1 t

K L(

(s,

a) ||t-1 (s,

a)),

(12)

where g^t-1 =  r t-1, t-1 denotes the stochastic gradients estimated through (10) and (11)

via given samples and KL(q(s, a)||p(s, a)) =

q(s, a) log

q(s,a) p(s,a)

dsda.

Rather

than

just

update

V

once via the stochastic gradient of V Lr(V, , ) in each iteration for solving saddle-point prob-

lem (Nemirovski et al., 2009), which is only valid in convex-concave setting, Dual-AC requires

V t = argminV Lr(V, t-1, t-1) in t-th iteration for estimating  r (, ). As we discussed,

such operation will keep the gradient estimator of dual variables unbiased.

In Algorithm 1, we update V t by solving optimization minV Lr(V, t-1, t-1). In fact, the V function in the path-regularized Lagrangian Lr(V, , ) plays two roles: i), inherited from the original Lagrangian, the first two terms in regularized Lagrangian (9) push the V towards the value function
of the optimal policy with on-policy samples; ii), on the other hand, the path regularization enforces V to be close to the value function of behavior policy b with off-policy samples. Therefore, the V function in the DAC algorithm can be understood as an interpolation between these two value
functions learned from both on and off policy samples.

4.4 PRACTICAL IMPLEMENTATION

In above, we have introduced path regularization for recovering local duality property of the parametrized multi-step Lagrangian dual form and tailored stochastic mirror descent algorithm for optimizing the regularized dual function. Here, we present several strategies for practical computation considerations.

Update depends

rule of V t. In on b and V ,

each iteration, for estimating

we the

need to solve V t gradient for dual

= argminV variables. In

Lr(V, t-1, t-1), which fact, the closer b to  is,

the smaller Esµ(s) Eb

 i=0

iR(si,

ai)

- V (s) 2

will be. Therefore, we can set V to be

large for better local convexity and faster convergence. Intuitively, the t-1 is approaching to  as

5

Under review as a conference paper at ICLR 2018

Algorithm 1 Dual Actor-Critic (Dual-AC )

1:

Initialize V0 , 0 and 0

randomly, set 



[

1 2

,

1].

2: for episode t = 1, . . . , T do

3: Start from s  t-1(s), collect samples {l}ml=1 follows behavior policy t-1.

4: Update Vt = argminV L^r(V, t-1, t-1) by SGD based on {l}ml=1.

5: Update ~t(s) according to closed-form (14).

6: Decay the stepsize t in rate O 1/t .

7: Compute the stochastic gradients for  following (11). 8: Update t according to the exact prox-mapping (16) or the approximate closed-form (17). 9: end for

the algorithm iterates. Therefore, we can exploit the policy obtained in previous iteration, i.e., t-1, as the behavior policy. The experience replay can also be used.

Furthermore, notice the L(V, t-1, t-1) is a expectation of functions of V , we will use stochastic

gradient descent algorithm for the subproblem. Other efficient optimization algorithms can be used too. Specifically, the unbiased gradient estimator for V L(V, t-1, t-1) is

V Lr(V, t-1, t-1) = (1 - k+1)Eµ [V V (s)] + E V  {si, ai}ik=0 , sk+1 (13)

-2V Eµb

 i=0

i

R(si

,

ai

)

-

V

(s)

V V (s)

.

We can use k-step Monte Carlo approximation for Eµb

 i=0

iR(si

,

ai)

in the gradient estimator.

As k is large enough, the truncate error is negligible (Sutton & Barto, 1998). We will iterate via

Vt,i = Vt,i-1 + iVt,i-1 Lr(V, t-1, t-1) until the algorithm converges.

Update rule of t. In practice, we may face with the situation that the initial sampling distribution is fixed, e.g., in MuJoCo tasks. Therefore, we cannot obtain samples from t(s) at each iteration. We assume that µ  (0, 1], such that (s) = (1 - µ)(s) + µµ(s) with (s)  P(S). Hence, we have
E  {si, ai}ki=0 , sk+1 = Eµ (~(s) + µ)  {si, ai}ik=0 , sk+1

where ~(s)

=

(1

-

µ

)

(s) µ(s)

.

Note that such an assumption is much weaker comparing with the

requirement for popular policy gradient algorithms (e.g., Sutton et al. (1999); Silver et al. (2014))

that assumes µ(s) to be a stationary distribution. In fact, we can obtain a closed-form update for ~

if a square-norm regularization term is introduced into the dual function. Specifically,

Theorem 6 In t-th iteration, given V t and t-1,

argmax Eµ(s)t-1(s) (~(s) + µ)  {si, ai}ik=0 , sk+1
0

=

1 max


0, Et-1



{si, ai}ki=0 , sk+1

.

- 

~

2 µ

(14) (15)

Then, we can update ~t through (14) with Monte Carlo approximation of Et-1  {si, ai}ik=0 , sk+1 , avoiding the parametrization of ~. As we can see, the ~t(s)
reweighs the samples based on the temporal differences and this offers a principled justification for the heuristic prioritized reweighting trick used in (Schaul et al., 2015).

Update rule of t . The parameters for dual function, , are updated by the prox-mapping operator (12) following the stochastic mirror descent algorithm for the regularized dual function. Specifically, in t-th iteration, given V t and t, for , the prox-mapping (12) reduces to

t = argmin -

g^t

+

1 t

KL

 (a|s)||t-1 (a|s)

,

(16)

where g^t =  r (t , t ). Then, the update rule will become exactly the natural policy gradient (Kakade, 2002) with a principled way to compute the "policy gradient" g^t . This can be understood as the penalty version of the trust region policy optimization (Schulman et al., 2015a).

Exactly solving the prox-mapping for  requires another optimization, which may be expensive. To further accelerate the prox-mapping, we approximate the KL-divergence with the second-order

6

Under review as a conference paper at ICLR 2018

Taylor expansion, and obtain an approximate closed-form update given by

t



argmin


-

g^t

+

1 2

 - t-1

2 Ft

= t-1 + tFt-1g^t

(17)

where Ft := Ett-1 2 log t-1 denotes the Fisher information matrix. Empirically, we may

normalize the gradient by its norm gt Ft-1gt (Rajeswaran et al., 2017) for better performances.

Combining these practical tricks to the stochastic mirror descent update eventually gives rise to the dual actor-critic algorithm outlined in Algorithm 1.

5 RELATED WORK
The dual actor-critic algorithm includes both the learning of optimal value function and optimal policy in a unified framework based on the duality of the linear programming (LP) representation of Bellman optimality equation. The linear programming representation of Bellman optimality equation and its duality have been utilized for (approximate) planning problem (de Farias & Roy, 2004; Wang et al., 2008; Pazis & Parr, 2011; O'Donoghue et al., 2011; Malek et al., 2014; Cogill, 2015), in which the transition probability of the MDP is known and the value function or policy are in tabular form. Chen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization and are not applicable to MDPs with large or continuous state/action spaces.
The duality view has also been exploited in Neu et al. (2017). Their algorithm is based on the duality of entropy-regularized Bellman equation (Todorov, 2007; Rubin et al., 2012; Fox et al., 2015; Haarnoja et al., 2017; Nachum et al., 2017), rather than the exact Bellman optimality equation used in our work. Meanwhile, their algorithm is only derived and tested in tabular form.
Our dual actor-critic algorithm can be understood as a nontrivial extension of the (approximate) dual gradient method (Bertsekas, 1999, Chapter 6.3) using stochastic gradient and Bregman divergence, which essentially parallels the view of (approximate) stochastic mirror descent algorithm (Nemirovski et al., 2009) in the primal space. As a result, the algorithm converges with diminishing stepsizes and decaying errors from solving subproblems.
Particularly, the update rules of  and  in the dual actor-critic are related to several existing algorithms. As we see in the update of , the algorithm reweighs the samples which are not fitted well. This is related to the heuristic prioritized experience replay (Schaul et al., 2015). For the update in , the proposed algorithm bears some similarities with trust region poicy gradient (TRPO) (Schulman et al., 2015a) and natural policy gradient (NPR) (Kakade, 2002; Rajeswaran et al., 2017). Indeed, TRPO and NPR solve the same prox-mapping but are derived from different perspectives. We emphasize that although the updating rules share some resemblance to several reinforcement learning algorithms in the literature, they are purely originated from a stochastic dual ascent algorithm for solving the two-play game derived from Bellman optimality equation.

6 EXPERIMENTS
We evaluated the dual actor-critic (Dual-AC ) algorithm on several continuous control environments from the OpenAI Gym (Brockman et al., 2016) with MuJoCo physics simulator (Todorov et al., 2012). We compared Dual-AC with several representative actor-critic algorithms, including trust region policy optimization (TRPO) (Schulman et al., 2015a) and proximal policy optimization (PPO) (Schulman et al., 2017)1. We ran the algorithms with 5 random seeds and reported the average rewards with 50% confidence interval. Details of the tasks and setups of these experiments including the policy/value function architectures and the hyperparameters values, are provided in Appendix C.
1As discussed in Henderson et al. (2017), different implementations of TRPO can provide different performances. For a fair comparison, we use the one from https://github.com/joschu/modular rl reported to have achieved the best scores in Henderson et al. (2017).

7

Under review as a conference paper at ICLR 2018

6.1 ABLATION STUDY

To justify our analysis in identifying the sources of instability in directly optimizing the parametrized one-step Lagrangian duality and the effect of the corresponding components in the dual actor-critic algorithm, we perform comprehensive Ablation study in InvertedDoublePendulum-v1 environment.

We conducted comparison between the Dual-AC and its variants, including Dual-AC w/o

multi-step, Dual-AC w/o path-regularization, Dual-AC w/o unbiased V , and the naive

Dual-AC , for demonstrating the three instability sources in Section 3, respectively.

Specifically, Dual-AC w/o path-regularization re-

moves the path-regularization components; Dual-

AC w/o multi-step removes the multi-step exten-

sion and the path-regularization; Dual-AC w/o un-

biased V calculates the stochastic gradient with-

out achieving the convergence of inner optimiza-

tion on V ; and the naive Dual-AC is the one with-

out all components. The empirical performances on

InvertedDoublePendulum-v1 tasks are shown in Fig-

ure 1. The results are consistent with the analysis.

The naive Dual-AC performs the worst. The per-

formances of the Dual-AC found the optimal policy

which solves the problem much faster than the alternative variants. The Dual-AC w/o unbiased V converges slower, showing its sample inefficiency caused by the bias in gradient calculation. The Dual-AC w/o

Figure 1: Comparison between the DualAC and its variants for justifying the analysis of the source of instability.

multi-step and Dual-AC w/o path-regularization cannot converge to the optimal policy, indicating

the importance of the path-regularization in recovering the local duality. Meanwhile, the perfor-

mance of Dual-AC w/o multi-step is worse than Dual-AC w/o path-regularization, showing the bias

in one-step can be alleviated via multi-step trajectories.

6.2 COMPARISON IN CONTINUOUS CONTROL TASKS
In this section, we evaluated the Dual-AC against TRPO and PPO across multiple tasks, including the InvertedDoublePendulum-v1, Hopper-v1, HalfCheetah-v1, and Swimmer-v1. These tasks have different dynamic properties, ranging from unstable to stable, Therefore, they provide sufficient benchmarks for testing the algorithms. In Figure 2, we reported the average rewards across 5 runs of each algorithm with 50% confidence interval during the training stage. We also reported the average final rewards in Table 1.
The proposed Dual-AC achieves the best performance in almost all environments, including Pendulum, InvertedDoublePendulum, Hopper, and HalfCheetah. These results demonstrate that Dual-AC is a viable and competitive RL algorithm for a wide spectrum of RL tasks with different dynamic properties.

Table 1: The average final performances of the policies learned from Dual-AC and the competitors.

Environment Pendulum InvertedDoublePendulum Swimmer
Hopper HalfCheetah

Dual-AC -155.45 8599.47
234.56 2983.79 3041.47

PPO -266.98 1776.26 223.13 2376.15 2249.10

TRPO -245.11 3070.96 232.89 2483.57 2347.19

A notable case is the InvertedDoublePendulum, where Dual-AC substantially outperforms TRPO and PPO in terms of the learning speed and sample efficiency, implying that Dual-AC is preferable to unstable dynamics. We conjecture this advantage might come from the different meaning of V in our algorithm. For unstable system, the failure will happen frequently, resulting the collected data are far away from the optimal trajectories. Therefore, the policy improvement through the value
8

Under review as a conference paper at ICLR 2018

(a) Pendulum

(b) InvertedDoublePendulum-v1

(a) Swimmer-v1

(b) Hopper-v1

(c) HalfCheetah-v1

Figure 2: The results of Dual-AC against TRPO and PPO baselines. Each plot shows average reward during training across 5 random seeded runs, with 50% confidence interval. The x-axis is the number of training iterations. The Dual-AC achieves comparable performances to he best algorithm in TRPO and PPO in some tasks, but outperforms on more challenging tasks.

function corresponding to current policy is slower, while our algorithm learns the optimal value function and enhances the sample efficiency.

7 CONCLUSION
In this paper, we revisited the linear program formulation of the Bellman optimality equation, whose Lagrangian dual form yields a game-theoretic view for the roles of the actor and the dual critic. Although such a framework for actor and dual critic allows them to be optimized for the same objective function, parametering the actor and dual critic unfortunately induces instablity in optimization. We analyze the sources of instability, which is corroborated by numerical experiments. We then propose Dual Actor-Critic , which exploits stochastic dual ascent algorithm for the path regularized, multi-step bootstrapping two-player game, to bypass these issues. The algorithm achieves the stateof-the-art performances on several MuJoco benchmarks.

REFERENCES
Shun-ichi Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press, 1993.
Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, second edition, 1999.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, September 1996. ISBN 1-886529-10-8.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena Scientific Belmont, MA, 1995.
Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actorcritic algorithms. Automatica, 45(11):2471­2482, 2009.

9

Under review as a conference paper at ICLR 2018
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1­123, 2010.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Martin Burger. Infinite-dimensional optimization and optimal design. 2003.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Optimal primal-dual methods for a class of saddle point problems. SIAM Journal on Optimization, 24(4):1779­1814, 2014.
Randy Cogill. Primal-dual algorithms for discounted markov decision processes. In Control Conference (ECC), 2015 European, pp. 260­265. IEEE, 2015.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems, pp. 3041­3049, 2014.
D. Pucci de Farias and B. Van Roy. On constraint sampling in the linear programming approach to approximate dynamic programming. Mathematics of Operations Research, 29(3):462­478, 2004.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends R in Robotics, 2(1­2):1­142, 2013.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672­2680, 2014.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
S. Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems 14, pp. 1531­1538. MIT Press, 2002.
M. Kearns and S. Singh. Bias-variance error bounds for temporal difference updates. In Proc. 13th Annu. Conference on Comput. Learning Theory, pp. 142­147. Morgan Kaufmann, San Francisco, 2000.
Vijay R. Konda and John N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143­1166, 2003.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer Publishing Company, Incorporated, 2015. ISBN 3319188410, 9783319188416.
Alan Malek, Yasin Abbasi-Yadkori, and Peter Bartlett. Linear programming for large-scale markov decision problems. In International Conference on Machine Learning, pp. 496­504, 2014.
Volodymyr Mnih, Adria` Puigdome`nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1928­ 1937, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. arXiv preprint arXiv:1702.08892, 2017.
10

Under review as a conference paper at ICLR 2018
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19(4):1574­1609, January 2009. ISSN 1052-6234.
Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127­152, 2005.
Gergely Neu, Anders Jonsson, and Vicenc¸ Go´mez. A unified view of entropy-regularized markov decision processes. arXiv preprint arXiv:1705.07798, 2017.
Brendan O'Donoghue, Yang Wang, and Stephen Boyd. Min-max approximate dynamic programming. In Computer-Aided Control System Design (CACSD), 2011 IEEE International Symposium on, pp. 424­431. IEEE, 2011.
Jason Pazis and Ronald Parr. Non-parametric approximate linear programming for mdps. In AAAI, 2011.
Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In Machine Learning: ECML 2005, 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceedings, pp. 280­291. Springer, 2005.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization and simplicity in continuous control. arXiv preprint arXiv:1703.02660, 2017.
Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in mdps. Decision Making with Imperfect Decision Makers, pp. 57­74, 2012.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pp. 1889­1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1): 9­44, 1988.
R. S. Sutton, David McAllester, S. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and K.-R. Mu¨ller (eds.), Advances in Neural Information Processing Systems 12, pp. 1057­1063, Cambridge, MA, 2000. MIT Press.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pp. 1057­ 1063, 1999.
R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing systems, pp. 1369­1376, 2007.
11

Under review as a conference paper at ICLR 2018 Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. Mengdi Wang. Randomized Linear Programming Solves the Discounted Markov Decision Problem In Nearly-Linear Running Time. ArXiv e-prints, 2017. Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Dual representations for dynamic programming. 2008. C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, Oxford, May 1989. (To be reprinted by MIT Press.). Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229­256, 1992.
12

Appendix

A DETAILS OF THE PROOFS FOR SECTION 2

A.1 DUALITY OF BELLMAN OPTIMALITY EQUATION

Puterman (2014); Bertsekas et al. (1995) provide details in deriving the linear programming form of the Bellman optimality equation. We provide a briefly proof here.

Proof We rewrite the linear programming 3 as

V  = argmin Eµ [V (s)] .
V TV

(18)

Recall the T is monotonic, i.e., if V T V  T V T 2V and V  = T V for arbitrary V , we have for V feasible, V T V T 2V . . . T V = V .

Theorem 1 (Optimal policy from occupancy) . (s,a)
aA (s,a)

s,aS×A (s, a) = 1, and (a|s) =

Proof For the optimal occupancy measure, it must satisfy

(s , a) = 

(s, a)p(s |s, a) + (1 - )µ(s ), s  S

aA

s,aS ×A

 (1 - )µ +

(P - I)(s, a) = 0,

s,aS ×A

where P denotes the transition distribution and I denotes a |S| × |SA| matrix where Iij = 1 if and only if j  [(i - 1) |A| + 1, . . . , i |A|]. Multiply both sides with 1, due to µ and P are probabilities, we have 1,  = 1.

Without loss of generality, we assume there is only one best action in each state. Therefore, by the KKT complementary conditions of (3), i.e.,
(s, a) R(s, a) + Es |s,a [V (s )] - V (s) = 0, which implies (s, a) = 0 if and only if a = a, therefore, the  by normalization.

Theorem 2 The optimal policy  and its corresponding value function V  is the solution to the following saddle problem

max min L(V, , ) := (1 - ) Esµ(s) [V (s)] +

(s) (a|s) [V ](s, a)

P(S),P(A) V

(s,a)S ×A

where [V ](s, a) = R(s, a) + Es |s,a[V (s )] - V (s).

Proof Due to the strong duality of the optimization (3), we have

min max (1 - ) Esµ(s) [V (s)] +

(s, a)[V ](s, a)

V (s,a) 0

(s,a)S ×A

=

max min
(s,a) 0 V

(1 - ) Esµ(s) [V (s)] +

(s, a)[V ](s, a).

(s,a)S ×A

Then, plugging the property of the optimum in Theorem 1, we achieve the final optimization (6).

A.2 CONTINUOUS STATE AND ACTION MDP EXTENSION
In this section, we extend the linear programming and its duality to continuous state and action MDP. In general, the only weak duality holds for infinite constraints, i.e., P D. With a mild assumption, we will recover the strong duality for continuous state and action MDP, and most of the conclusions in discrete state and action MDP still holds.
13

Under review as a conference paper at ICLR 2018

Specifically, without loss of generality, we consider the solvable MDP, i.e., the optimal policy,

(a|s), exists. If R(s, a) 

CR, V  

CR 1-

.

Moreover,

V

2 2,µ

=

(V (s))2 µ(s)ds = R(s, a) + Es |s,a [V (s )] 2 (a|s)µ(s)d(s, a)

2 (R(s, a))2 (a|s)µ(s)ds + 22 Es |s,a [V (s )] 2 (a|s)µ(s)ds

2 max
aA

R(s, a)

2 µ

+

22

P (s |s)µ(s)ds (V (s ))2 ds

2 max
aA

R(s, a)

2 µ

+

22

V (s )

2 

2 max
aA

R(s, a)

2 µ

+

22

V (s )

2 

,

P (s |s)µ(s)dsds

V  - Es |s,a [V (s )]

2 µb

2

V

2 µ

+

22

Es |s,a [V (s )]

2 µb

2

V

2 µ

+

22

V (s )

2 

,

for some b  P that b(a|s) > 0 for  (s, a)  S × A. Therefore, with the assumption that

R(s, a)

2 µ

CRµ , a  A, we have R(s, a)  Lµ2b (S × A) and V (s )  L2µ(S). The constraints

in the primal form of linear programming can be written as

(I - P) V - R L2µb 0,

where I - P : Lµ2 (S)  Lµ2b (S × A) without any effect on the optimality. For simplicity, we
denote as L2µb and f, g = f (s, a)g(s, a)µ(s)b(a|s)dsda. Apply the Lagrangian multiplier for constraints in ordered Banach space in Burger (2003), we have

P = min max (1 - )Eµ [V (s)] - , (I - P) V - R .
V L 0

(19)

The solution (V , ) also satisfies the KKT conditions,

(1 - )1 - (I - P)  = 0,

 0,

(I - P) V  - R

0,

, (I - P) V  - R = 0.

(20) (21) (22) (23)

where denotes the conjugate operation. By the KKT condition, we have

1, (1 - )1 - (I - P)  = 0  1, = 1.

(24)

The strongly duality also holds, i.e., P = D := max
0
s.t.

R(s, a), (s, a) (1 - )1 - (I - P)

=0

(25) (26)

Proof We compute the duality gap
= =

(1 - ) 1, V  - R,  , (I - P) V  - R,  , (I - P) V  - R = 0,

which shows the strongly duality holds.

B DETAILS OF THE PROOFS FOR SECTION 4

B.1 COMPETITION IN MULTI-STEP SETTING

Once we establish the k-step Bellman optimality equation (7), it is easy to derive the -Bellman optimality equation, i.e.,

k

V (s) = max (1 - ) kE
P

iR(si, ai) + k+1V (sk+1) := (TV )(s).

k=0

i=0

(27)

14

Under review as a conference paper at ICLR 2018

Proof Denote the optimal policy as (a|s), we have

V

(s)

=

E
{st }i=0 |s

k
iR(si, ai)
i=0

+



k+1

E
sk+1

|s

[V

(sk+1)]

,

holds for arbitrary k  N. Then, we conduct k  Geo() and take expectation over the countable

infinite many equation, resulting

k

V (s) = (1 - ) kE

iR(si, ai) + k+1V (sk+1)

k=0

i=0

k

= max (1 - ) kE
P

iR(si, ai) + k+1V (sk+1)

k=0

i=0

Next, we investigate the equivalent optimization form of the k-step and -Bellman optimality equation, which requires the following monotonic property of Tk and T.

Lemma 7 Both Tk and T are monotonic.

Proof Assume U and V are the value functions corresponding to 1 and 2, and U U (s) V (s), s  S, apply the operator Tk on U and V , we have

k

(TkU ) (s)

=

max
P


E{si }ki=1 |s

iR(si, ai)

+



k+1


Esk+1

|s

[U

(sk+1)]

,

i=0

k

(TkV ) (s)

=

max
P

E
{si }ki=1 |s

iR(si, ai)

+



k+1

E
sk+1

|s

[V

(sk+1)]

.

i=0

V , i.e.,

Due to U

V

,

we

have

E
sk+1 |s

[U (sk+1)]

E
sk+1 |s

[V

(sk+1)],





P,

which

leads

to

the

first

conclusion, TkU TkV .

Since T = (1 - )

 k=1

Tk

=

EkGeo()

[Tk ],

therefore,

T

is

also

monotonic.

With the monotonicity of Tk and T, we can rewrite the V  as the solution to an optimization,

Theorem 8 The optimal value function V  is the solution to the optimization
V  = argmin 1 - k+1 Esµ(s) [V (s)] ,
V TkV
where µ(s) is an arbitrary distribution over S.

(28)

Proof Recall the Tk is monotonic, i.e., V V , we have for V , V TkV Tk2V

Tk V ...

 TkV TkV

= TVk2V,

and V where

 = TkV for arbitrary the last equality comes

from the Banach fixed point theorem (Puterman, 2014). Similarly, we can also show that V ,

V TV = V . By combining these two inequalities, we achieve the optimization.

We rewrite the optimization as

min
V

(1 - k+1)Esµ(s) [V (s)]

(29)

k

s.t.

V (s)

R(s,

a)

+

max
P

E
{si }ki=+11 |s

iR(si, ai) + k+1V (sk+1) ,

i=1

(s, a)  S × A,

We emphasize that this optimization is no longer linear programming since the existence of maxoperator over distribution space in the constraints. However, Theorem 1 still holds for the dual variables in (32).

15

Under review as a conference paper at ICLR 2018

Proof

Denote the optimal policy as ~V

= argmaxP

E
{si }ik=+11 |s

the KKT condition of the optimization (29) can be written as

k i=1

i

R(si

,

ai

)

+

k+1V

(sk+1

)

,

k-1

k

1 - k+1 µ(s ) + k+1

p(s |sk, ak) p(si+1|si, ai) ~V (ai|si)(s0, a0)

{si ,ai }ik=0

i=0

i=1

kk

= p(si+1|si, ai)(s , a) ~V (ai|si).

a0,{si,ai}ki=1 i=0

i=1

Denote Pk(sk+1|s, a) = plify the condition, i.e.,

{si,ai}ik=1 p(sk+1|sk, ak)

k-1 i=0

p(si+1|si

,

ai)

k i=1

(ai|si),

we

sim-

1 - k+1 µ(s ) + k+1 Pk~V (s |s, a)(s, a) = (s , a).

s,a a

Due to the we have

sP,akV ((ss|,sa,)a=) is1.a

conditional

probability

for

V

,

with

similar

argument

in

Theorem

1,

By the KKT complementary condition, the primal and dual solutions, i.e., V  and , satisfy

(s, a)

k

R(s,

a)

+

E~V 
{si }ik=+11 |s

iR(si, ai) + k+1V (sk+1)
i=1

- V (s)

= 0.

(30)

Recall which

V  denotes the value function of denotes the optimal policy. Then,

the the

optimal policy, then, based condition (30) implies (s,

on the a) = 0

definition, if and only

~V  if a

= =

 a,

therefore, we can decompose (s, a) = (s)(a|s).

The corresponding Lagrangian of optimization (29) is

min max Lk(V, ) = (1 - k+1)Eµ [V (s)] +
V (s,a) 0

(s, a)

max
P

k

[V

](s,

a)

,

(s,a)S ×A

where

k [V

](s,

a)

=

R(s,

a)

+

E
{st }ki=+11 |s

k i=1

iR(si,

ai)

+

k+1V

(sk+1)

- V (s).

We further simplify the optimization. Since the dual variables are positive, we have

min max Lk(V, ) = (1 - k+1)Eµ [V (s)] +
V (s,a) 0,P

(s, a) (k [V ](s, a)) .

(s,a)S ×A

(31) (32)

After clarifying these properties of the optimization corresponding to the multi-step Bellman optimality equation, we are ready to prove the Theorem 3.

Theorem 3 The optimal policy  and its corresponding value function V  is the solution to the following saddle point problem

max min Lk(V, , ) := (1 - k+1)Eµ [V (s)]
P(S),P(A) V

(8)

where [V ]

{si, ai}ki=0 , sk+1

k

+ (s0) (ai|si)p(si+1|si, ai)[V ]

{si,ai}ki=0,sk+1 i=0

=

k i=0



iR(si,

ai)

+



k+1

V

(sk+1)

-

V

(s).

{si, ai}ik=0 , sk+1

Proof By Theorem 1 in multi-step setting, we can decompose (s, a) = (s)(a|s) without any loss. Plugging such decomposition into the Lagrangian 32 and realizing the equivalence among the optimal policies, we arrive the optimization as minV maxP(S),P(A) Lk(V, , ). Then, because of the strong duality as we proved in Lemma 9, we can switch min and max operators in optimization 8 without any loss.

16

Under review as a conference paper at ICLR 2018

Lemma 9 The strong duality holds in optimization (8).

Proof Specifically, for every   P(S),   P(A),

(, ) = min Lk(V, , )
V

min
V

Lk(V, , ); [V ]

{si, ai}ik=0 , sk+1

0

min
V

(1 - k+1)Esµ(s) [V (s)] , s.t. [V ] {si, ai}ki=0 , sk+1 0

= (1 - k+1)Esµ(s) [V (s)] .

On the other hand, since Lk(V, , ) is convex w.r.t. V , we have V   argminV Lk(V, , ), by checking the first-order optimality. Therefore, we have

max (, ) =

max Lk(V, , )

P (S ),P (A)

P(S),P(A),V argminV Lk(V,,)

L(V , , ) = (1 - k+1)Esµ(s) [V (s)] .

Combine these two conditions, we achieve the strong duality even without convex-concave property

(1 - k+1)Esµ(s) [V (s)]

max

(, ) (1 - k+1)Esµ(s) [V (s)] .

P (S ),P (A)

B.2 THE COMPOSITION IN APPLYING AUGMENTED LAGRANGIAN METHOD

We consider the one-step Lagrangian duality first. Following the vanilla augmented Lagrangian method, one can achieve the dual function as

(, ) = min (1 - ) Esµ(s) [V (s)] +

Pc ([V ](s, a), (s)(a|s)) ,

V

(s,a)S ×A

where

Pc

([V

](s, a), (s)(a|s))

=

1 2c

[max (0, (s)(a|s) + c[V ](s, a))]2 - 2(s)2(a|s)

.

The computation of Pc is in general intractable due to the composition of max and the condition ex-

pectation in [V ](s, a), which makes the optimization for augmented Lagrangian method difficult.

For the multi-step Lagrangian duality, the objective will become even more difficult due to constraints are on distribution family P(S) and P(A), rather than S × A.

B.3 PATH REGULARIZATION

Theorem 4 The local duality holds for Lr(V, , ). Denote (V , , ) as the solution to Bellman optimality equation, with some appropriate V , (V , , ) = argmaxP(S),P(A) argminV Lr(V, , ).

Proof The local duality can be verified by checking the Hessian of Lr(V  ). We apply the local duality theorem (Luenberger & Ye, 2015)[Chapter 14]. Suppose (V~ , ~, ~) is a local solution
to minV maxP(S),P(A) Lr(V, , ), then, maxP(S),P(A) minV Lr(V, , ) has a local solution V~  with corresponding ~, ~.

Next, we show that with some appropriate V , the path regularization does not change the optimum.

Let U (s) = E

 i=0

i

R(si

,

ai

)|s

,

and

thus,

U 

=

V .

We

first

show

that

for

b



P (A),

17

Under review as a conference paper at ICLR 2018

we have E Eb

 i=0

i

R(si

,

ai

)

- V (s) 2

=E

Ub(s) - U  (s) + U  (s) - V (s) 2

= E U b (s) - U  (s) 2

 



E

b(ai|si) - (ai|si)

p(si+1|si, ai)

iR(si, ai) d {si, ai}i=0

i=0 i=0 i=0

i=1

 i=1



iR(si,

ai)



(

 i=0

b(ai|si)

-

 i=0



(ai

|si))

 i=0

p(si+1|si,

ai)

1

2

 i=1



iR(si,

ai)



2 1-

R(s, a) 

where the last inequality comes from the fact that b(ai|si)p(si+1|si, ai) is distribution.

We then rewrite the optimization minV maxP(S),P(A) Lr(V, , ) as

min max
V P(S),P(A)
s.t.

Lk(V, , ) V   ,b := V : Esµ(s) Eb

 i=0



iR(si,

ai)

- V (s) 2

,

due to the well-known one-to-one correspondence between regularization V and Nesterov (2005).

If we set V with appropriate value so that its corresponding (V )

2 1-

R(s, a) , we will have

V    (V ), which means adding such constraint, or equivalently, adding the path regularization,

does not affect the optimality. Combine with the local duality, we achieve the conclusion.

In fact, based on the proof, the closer b to  is, the smaller

Esµ(s) Eb

 i=0

iR(si,

ai)

- V (s)

2

will be.

Therefore, we can set V bigger for

better local convexity, which resulting faster convergence.

B.4 STOCHASTIC DUAL ASCENT UPDATE

Corollary 5 The regularized dual function r(, ) has gradients estimators

 r (, ) = E  {si, ai}ik=0 , sk+1  log (s) ,

 r (, ) = E  {si, ai}ki=0 , sk+1

k i=0



log

(a|s)

.

Proof We mainly focus on deriving  r (, ). The derivation of  r (, ) is similar.

By chain rule, we have  r (, ) = V Lk(V (, ), , ) - 2V Eb

 i=0

i

R(si

,

ai)

- V (s)

 V (, )

0

k

+E  {si, ai}ik=0 , sk+1

 log (a|s)

i=0

k

= E  {si, ai}ik=0 , sk+1

 log (a|s) .

i=0

The first term in RHS equals to zero due to the first-order optimality condition for

V (, ) = argminV Lr(V, , ).

18

Under review as a conference paper at ICLR 2018

B.5 PRACTICAL ALGORITHM

Theorem 6 In t-th iteration, given V t and t-1,

argmax Eµ(s)t-1(s) (~(s) + µ)  {si, ai}ik=0 , sk+1
0

=

1 max


0, Et-1



{si, ai}ki=0 , sk+1

.

- 

~

2 µ

Proof Recall the optimization w.r.t. ~ is max~ 0 Eµ ~(s)E  {si, ai}ki=0 , sk+1 - ~2(s) , denote  (s) as the dual variables of the optimization, we have the KKT condition as

 ~    (s)~(s)
~   

=  + E  {si, ai}ik=0 , sk+1 = 0,
0, 0,

,



~





   (s)

 (s) + E



{si, ai}ki=0 , sk+1

~  

=



+E

[({si

,ai 

}ik=0

,sk+1

)]

,

= 0,

0, 0,

 -E   (s) =



{si, ai}ik=0 , sk+1

0

E  {si, ai}ik=0 , sk+1 E  {si, ai}ik=0 , sk+1

<0 .
0

Therefore,

in

t-th

iteration,

~t(s)

=

1 

max

0, E



{si, ai}ik=0 , sk+1

.

C EXPERIMENT DETAILS

Policy and value function parametrization. For fairness, we use the same parametrization across
all the algorithms. The parametrization of policy and value functions are largely based on the recent
paper by Rajeswaran et al. (2017), which shows the natural policy gradient with the RBF neural
network achieves the state-of-the-art performances of TRPO on MuJoCo. For the policy distribution, we parametrize it as  (a|s) = N (µ (s),  ), where µ (s) is a two-layer neural nets with the random features of RBF kernel as the hidden layer and the  is a diagonal matrix. The RBF kernel bandwidth is chosen via median trick (Dai et al., 2014; Rajeswaran et al., 2017). The same as Rajeswaran et al. (2017), we use 100 hidden nodes in Pendulum, InvertedDoublePendulum, Swimmer, Hopper, and use 500 hidden nodes in HalfCheetah. Since the TRPO and PPO uses GAE (Schulman et al., 2015b) with linear baseline as V , we also use the parametrization for V
in our algorithm. However, the Dual-AC can adopt arbitrary function approximator without any
change.

Training details. We report the hyperparameters for each algorithms here. We use the  = 0.995

for all the algorithms. We keep constant stepsize and tuned for TRPO, PPO and Dual-AC in

{0.001, 0.01, 0.1}. The batchsize are set to be 52 trajectories for comparison to the competitors

in Section 6.2. For the Ablation study, we set batchsize to be 24 trajectories for accelerating. The

CG damping parameter for TRPO is set to be 10-4. We iterate 20 steps for the Fisher information

matrix

computation.

For

the

V , µ,

1 

in

Dual-AC

from

{0.001, 0.01, 0.1, 1}.

19

