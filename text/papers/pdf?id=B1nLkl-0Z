Under review as a conference paper at ICLR 2018
LEARNING GAUSSIAN POLICIES FROM SMOOTHED ACTION VALUE FUNCTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. Our approach is amenable to proximal optimization techniques by augmenting the objective with a penalty on KLdivergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve much better results on standard continuous control benchmarks.
1 INTRODUCTION
Model-free reinforcement learning algorithms often alternate between two concurrent but interacting processes: (1) policy evaluation, where an action value function (i.e., a Q-value) is updated to obtain a better estimate of the return associated with taking a specific action, and (2) policy improvement, where the policy is updated aiming to maximize the current value function. In the past, different notions of Q-value have led to distinct but important families of RL methods. For example, SARSA (Rummery & Niranjan, 1994; Sutton & Barto, 1998; Van Seijen et al., 2009) uses the expected Q-value, defined as the expected return of following the current policy. Q-learning (Watkins, 1989) exploits a hard-max notion of Q-value, defined as the expected return of following an optimal policy. Soft Q-learning (Haarnoja et al., 2017) and PCL (Nachum et al., 2017a) both use a soft-max form of Q-value, defined as the future return of following an optimal entropy regularized policy. Clearly, the choice of Q-value function has a considerable effect on the resulting algorithm; for example, restricting the types of policies that can be expressed, and determining the type of exploration that can be naturally applied.
In this work we introduce a new notion of action value: the smoothed action value function Q~. Unlike previous notions, which associate a value with a specific action at each state, the smoothed Qvalue associates a value with a specific distribution over actions. In particular, the smoothed Q-value of a state-action pair (s, a) is defined as the expected return of first taking an action sampled from a normal distribution N (a, (s)), centered at a, then following actions sampled from the current policy thereafter. In this way, the smoothed Q-value can also be interpreted as a Gaussian-smoothed or noisy version of the expected Q-value.
We show that smoothed Q-values possess a number of interesting properties that make them attractive for use in RL algorithms. For one, the smoothed Q-values satisfy a single-step Bellman consistency, which allows bootstrapping to be used to train a function approximator. Secondly, for Gaussian policies, the standard optimization objective (expected return) can be expressed in terms of smoothed Q-values. Moreover, the gradient of this objective with respect to the mean and covariance of the Gaussian policy is equivalent to the gradient and the Hessian of the smoothed Q-value function, which allows one to derive updates to the policy parameters by having access to the derivatives of a sufficiently accurate smoothed Q-value function.
1

Under review as a conference paper at ICLR 2018

This observation leads us to propose an algorithm called Smoothie, which in the spirit of (Deep) Deterministic Policy Gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016), trains a policy using the derivatives of a trained (smoothed) Q-value function, thus avoiding the high-variance of stochastic updates used in standard policy gradient algorithms (Williams & Peng, 1991; Konda & Tsitsiklis, 2000). Unlike DDPG, which is well-known to have poor exploratory behavior (Haarnoja et al., 2017), the approach we develop is able to utilize a non-deterministic Gaussian policy parameterized by both a mean and a covariance, thus allowing the policy to be exploratory by default and alleviating the need for excessive hyperparameter tuning.
Furthermore, we show that Smoothie can be easily adapted to incorporate proximal policy optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous version of the policy. The inclusion of a KL-penalty is not feasible in the standard DDPG algorithm, but we show that it is possible with our formulation, and it significantly improves stability and overall performance. On standard continuous control benchmarks, our results are competitive with or exceed state-of-the-art, especially for more difficult tasks in the low-data regime.

2 NOTATION & BACKGROUND

We consider the standard model-free RL framework, where an agent interacts with a stochastic
black-box environment by sequentially observing the state of the environment, emitting an action,
and receiving a reward feedback; the goal is to find an agent that achieves maximal cumulative
discounted reward. This problem can be expressed in terms of a Markov decision process (MDP) that consists of a state space S and an action space A, where at iteration t the agent encounters a state st  S and emits an action at  A, after which the environment returns a scalar reward rt  R(st, at) and places the agent in a new state st+1  P (st, at).

We model the behavior of the agent using a stochastic policy  that produces a distribution over feasible actions at each state s as (a | s). The optimization objective (expected discounted return),
as a function of the policy, can then be expressed in terms of the expected action value function Q(s, a) by,

OER() =

(a | s)Q(s, a) da d(s) ,

(1)

SA

where (s) is the stationary distribution of the states under , and Q(s, a) is recursively defined

using the Bellman equation,

Q(s, a) = Er,s r +  Q(s , a )(a | s ) da ,
A

(2)

where   [0, 1] is the discount factor. For brevity, we will often suppress explicit denotation of the

sampling distribution R over immediate rewards and the distribution P over state transitions.

The policy gradient theorem (Sutton et al., 2000) expresses the gradient of OER() w.r.t. , the tunable parameters of a policy , as,

OER() =

(a | s)Q(s, a) da d(s)

SA

(3)

= Ea(a|s) [ log (a | s)Q(s, a)] d(s) .
S

(4)

Many reinforcement learning algorithms, including policy gradient and actor-critic variants, trade off

variance and bias when estimating the random variable inside the expectation in (4); for example, by

attempting to estimate Q(s, a) accurately using function approximation. In the simplest scenario,

an unbiased estimate of Q(s, a) is formed by accumulating discounted rewards from each state

forward using a single Monte Carlo sample.

In this paper, we focus on multivariate Gaussian policies over continuous action spaces, A  Rda . We represent the observed state of the MDP as a ds-dimensional feature vector (s)  Rds , and parametrize the Gaussian policy by a mean and covariance function, respectively µ(s) : Rds  Rda and (s) : Rds  Rda × Rda . These map the observed state of the environment to a Gaussian
distribution,

(a | s) = N (a | µ(s), (s)) = |2(s)|-1/2 exp

-1 2

a - µ(s)

2 (s)-1

,

(5)

2

Under review as a conference paper at ICLR 2018

where

v

2 A

=

vTAv.

Below we develop new RL training methods for this family of parametric

policies, but some of the ideas presented may generalize to other families of policies as well. We

begin the formulation by reviewing some prior work on learning Gaussian policies.

2.1 DETERMINISTIC POLICY GRADIENT

Silver et al. (2014) present a new formulation of the policy gradient, called the deterministic policy gradient, for the family of Gaussian policies in the limit where the policy covariance approaches zero. In such a scenario, the policy becomes deterministic because sampling from the policy always returns the Gaussian mean. The key observation of (Silver et al., 2014) is that under a deterministic policy   (µ,   0), one can estimate the expected future return from a state s as,

lim (a | s)Q(s, a) da = Q(s, µ(s)) .
0 A

(6)

Then, one can express the gradient of the optimization objective (expected discounted return) for a parameterized   µ as,

OER() =

Q(s, µ(s))d(s) =
S

S

Q(s, a) a

a=µ(s)µ(s)d(s) .

(7)

This can be thought of as a characterization of the policy gradient theorem for deterministic policies.

In the limit of   0, one can also re-express the Bellman equation (2) as,

Q(s, a) = Er,s [r + Q(s , µ(s ))] .

(8)

Therefore, a value function approximator Qw can be optimized by minimizing the Bellman error,

E(w) =

(Qw (s, a) - r - Qw(s , µ(s ))2 ,

(s,a,r,s )D

(9)

for transitions (s, a, r, s ) sampled from a dataset D of interactions of the agent with the environment. Algorithms like DDPG (Lillicrap et al., 2016) alternate between improving the value function by gradient descent on (9), and improving the policy based on (7).

In practice, to gain better sample efficiency, Degris et al. (2012) and Silver et al. (2014) replace the on-policy state distribution (s) in (7) with an off-policy distribution (s) based on a replay
buffer. After this substitution, the policy gradient identity in (7) does not hold exactly, however,
prior work finds that this works well in practice and improves sample efficiency. We also adopt a
similar approximation in our method to make use of off-policy data.

3 SMOOTHED ACTION VALUE FUNCTIONS

In this paper, we introduce smoothed action value functions, the gradients of which provide an
effective signal for optimizing the parameters of a Gaussian policy. Our notion of smoothed Qvalues, denoted Q~(s, a), differs from ordinary Q-values Q(s, a) in that smoothed Q-values do
not assume the first action of the agent is fully specified, but rather they assume that only the mean of the distribution of the first action is known. Hence, to compute Q~(s, a), one has to perform an expectation of Q(s, a~) for actions a~ drawn in the vicinity of a. More formally, smoothed action
values are defined as,

Q~(s, a) = N (a~ | a, (s)) Q(s, a~)da~ .

(10)

A

With this definition of Q~, one can re-express the expected reward objective for a Gaussian policy

  (µ, ) as,

OER() = Q~(s, µ(s))d(s).
S

(11)

The insight that differentiates this approach from prior work including Heess et al. (2015); Ciosek

& Whiteson (2017) is that instead of learning a function approximator for Q(s, a) and then draw-

ing samples to approximate the expectation in (10) and its derivative, we directly learn a function

approximator for Q~(s, a).

3

Under review as a conference paper at ICLR 2018

The key observation that enables direct bootstrapping of smoothed Q-values, Q~(s, a), is that their

form allows a notion of Bellman consistency. First, note that for Gaussian policies   (µ, ) we

have

Q(s, a) = Er,s [r + Q~(s , µ(s ))] .

(12)

Then, combining (10) and (12), one can derive the following one-step Bellman equation for smoothed Q-values,

Q~(s, a) = N (a~ | a, (s)) Er~,s~ r~ + Q~(s~ , µ(s~ )) da~ ,
A

(13)

where r~ and s~ are sampled from R(s, a~) and P (s, a~). Below, we elaborate on how one can make use of the derivatives of Q~ to learn µ and , and how the Bellman equation in (13) enables direct optimization of Q~.

3.1 POLICY IMPROVEMENT - OPTIMIZING (µ, ) GIVEN Q~

We parameterize a Gaussian policy ,  (µ, ) in terms of two sets of parameters  and  for the mean and the covariance. The gradient of the objective w.r.t. mean parameters follows from the
policy gradient theorem and is almost identical to (7),

OER(,) =

S

Q~(s, a) a

a=µ (s)  µ (s)d (s).

(14)

Estimating the derivative of the objective w.r.t. covariance parameters is not as straightforward, since Q~ is not a direct function of . However, a key observation of this work is that the second derivative of Q~ w.r.t. actions is sufficient to exactly compute the derivative of Q~ w.r.t. ,

Q~(s, a) 1 2Q~(s, a)

=· (s) 2

a2

.

(15)

A proof of this identity is provided in the Appendix. Then, the full derivative w.r.t.  takes the form,

OER(,)

=

1 2

S

2Q~(s, a) a2

a=µ (s)   (s)d (s).

(16)

3.2 POLICY EVALUATION - OPTIMIZING Q~w GIVEN (µ, )

We can think of two ways to optimize Q~w . The first approach leverages (10) to update Q~ based on expected Q-value function Q. In such an approach, one trains a parameterized Qw to approximate the standard expected Q-value function Q using standard methods (see e.g., Rummery & Niranjan
(1994); Sutton & Barto (1998); Van Seijen et al. (2009)). Then, one fits Q~w based on Qw . In particular, given transitions (s, a, r, s ) sampled from interactions with the environment, one can train Qw to minimize the Bellman error (Qw (s, a)-r -Qw(s , a ))2 where a  N (µ(s ), (s )). Then, Q~w can be optimized to minimize the squared error (Q~w(s, a) - Ea~Qw (s, a~))2 where a~  N (a, (s)), using several samples. When the target values in these residuals are treated as fixed
(i.e., using a target network), such a training procedure will achieve a fixed point when Q~w(s, a) satisfies the recursion in the Bellman equation (10).

The second approach requires a single function approximator for Q~w(s, a), resulting in a simpler implementation, and thus we use this approach in our experimental evaluation. Suppose one has ac-

cess to a tuple (s, a~, r~, s~ ) sampled from a replay buffer with knowledge of the sampling probability

q(a~ | s) (possibly unnormalized). Then assuming that this sampling distribution has a full support,

we draw a phantom action a  N (a~, (s)) and optimize Q~w (s, a) by minimizing a weighted Bell-

man

error

1 q(a~|s)

(Q~ w

(s,

a)

-

r~ -

Q~w(s~ , µ(s~ ))2.

For

a

specific

pair

of

state

and

action

(s, a)

the

expected value of the objective is,

E(w | (s, a)) = Eq(a~ | s),r~,s~

N

(a | a~, q(a~ |

(s)) s)

(Q~ w

(s,

a)

-

r~

-



Q~ w

(s~

,

µ(s~

)))2

.

(17)

4

Under review as a conference paper at ICLR 2018

Note that N (a|a~, (s)) = N (a~|a, (s)). Therefore, when the target value r~ + Q~w (s~ , µ(s~ )) is treated as fixed (e.g., when using target networks) this training procedure reaches an optimum when Q~w(s, a) satisfies the recursion in the Bellman equation (13).
In practice, we find that it is unnecessary to keep track of the probabilities q(a~ | s), and assume the replay buffer provides a near-uniform distribution of actions conditioned on states. Other recent work has also benefited from ignoring or heavily damping importance weights (Munos et al., 2016; Wang et al., 2017; Schulman et al., 2017). However, it is possible when interacting with the environment to save the probability of sampled actions along with their transitions, and thus have access to q(a~ | s)  N (a~ | µold(s), old(s)).

3.3 COMPATIBLE FUNCTION APPROXIMATION

A function approximator Q~w of Q~ should be sufficiently accurate so that updates for µ,  are

not

affected

by

substituting

Q~w (s,a) a

and

2Q~w (s,a) a2

for

Q~ (s,a) a

and



2

Q~ (s,a) a2

,

respectively.

We claim that a Q~w is compatible with respect to µ if

1. aQ~w(s, a) a=µ(s) = µ(s)T w,
2
2. w S aQ~w(s, a) a=µ(s) - aQ~(s, a) a=µ(s) d(s) = 0 (i.e., w minimizes the expected squared error of the gradients).

Additionally, Q~w is compatible with respect to  if

1. a2Q~w(s, a) a=µ(s) = (s)T w,

2. w

S

a2 Q~w (s, a) a=µ(s) - a2 Q~(s, a) a=µ(s)

2
d(s) = 0 (i.e., w minimizes the

expected squared error of the Hessians).

We provide a proof of these claims in the Appendix. One possible parameterization of Q~w may be achieved by taking w = [w0, w1, w2] and parameterizing
Q~w(s, a) = Vw0 (s) + (a - µ(s))T µ(s)T w1 + (a - µ(s))T (s)T w2(a - µ(s)). (18)

While it is reassuring to know that there exists a class of function approximators which are compat-
ible, this fact is largely ignored in practice. Not only is the class of compatible functions heavily restricted in terms of expressiveness, due to the first set of conditions for µ, , it is also impossible to satisfy the second set of conditions without access to derivative and Hessian information of the true Q~. This problem is also present in DDPG, and we feel this issue merits additional investigation
in future work.

3.4 PROXIMAL POLICY OPTIMIZATION

Policy gradient algorithms are notoriously unstable, particularly in continuous control problems. Such instability has motivated the development of trust region methods that attempt to mitigate the issue by constraining each gradient step to lie within a trust region (Schulman et al., 2015), or augmenting the expected reward objective with a penalty on KL-divergence from a previous policy (Nachum et al., 2017b; Schulman et al., 2017). These stabilizing techniques have thus far not been applicable to algorithms like DDPG, since the policy is deterministic. The formulation we propose in this paper, however, is easily amenable to trust region optimization. Specifically, we may augment the objective (11) with a penalty

OTR() = OER() -  KL ( old) d(s),
S

(19)

where old  (µold, old) is a previous parameterization of the policy. The optimization is straightforward, since the KL-divergence of two Gaussians can be expressed analytically.

5

Under review as a conference paper at ICLR 2018
4 RELATED WORK
This paper follows a long line of work that uses Q-value functions to stably learn a policy, which in the past has been used to either approximate expected (Rummery & Niranjan, 1994; Van Seijen et al., 2009; Gu et al., 2017) or optimal (Watkins, 1989; Silver et al., 2014; Nachum et al., 2017a; Haarnoja et al., 2017; Metz et al., 2017) future value.
Work that is most similar to what we present are methods that exploit gradient information from the Q-value function to train a policy. Deterministic policy gradient (Silver et al., 2014) is perhaps the best known of these. The method we propose can be interpreted as a generalization of the deterministic policy gradient. Indeed, if one takes the limit of the policy covariance (s) as it goes to 0, the proposed Q-value function becomes the deterministic value function of DDPG, and the updates for training the Q-value approximator and the policy mean are identical.
Stochastic Value Gradient (SVG) (Heess et al., 2015) also trains stochastic policies using an update that is similar to DDPG (i.e., SVG(0) with replay). The key differences with our approach are that SVG does not provide an update for the covariance, and the mean update in SVG estimates the gradient with a noisy Monte Carlo sample, which we avoid by estimating the smoothed Q-value function. Although a covariance update could be derived using the same reparameterization trick as in the mean update, that would also require a noisy Monte Carlo estimate. Methods for updating the covariance along the gradient of expected reward are essential for applying the subsequent trust region and proximal policy techniques.
More recently, Ciosek & Whiteson (2017) introduced expected policy gradients (EPG), a generalization of DDPG that provides updates for the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. In that work, the expected Q-value used in standard policy gradient algorithms such as SARSA (Sutton & Barto, 1998; Rummery & Niranjan, 1994; Van Seijen et al., 2009) is estimated. The updates in EPG therefore require approximating an integral of the expected Q-value function. Our analogous process directly estimates an integral (via the smoothed Q-value function) and avoids approximate integrals, thereby making the updates simpler. Moreover, while Ciosek & Whiteson (2017) rely on a quadratic Taylor expansion of the estimated Q-value function, we instead rely on the strength of neural network function approximators to directly estimate the smoothed Q-value function.
The novel training scheme we propose for learning the covariance of a Gaussian policy relies on properties of Gaussian integrals (Bonnet, 1964; Price, 1958). Similar identities have been used in the past to derive updates for variational auto-encoders (Kingma & Welling, 2014) and Gaussian back-propagation (Rezende et al., 2014).
Finally, the perspective presented in this paper, where Q-values represent the averaged return of a distribution of actions rather than a single action, is distinct from recent advances in distributional RL (Bellemare et al., 2017). Those approaches focus on the distribution of returns of a single action, whereas we consider the single average return of a distribution of actions. Although we restrict our attention in this paper to Gaussian policies, an interesting topic for further investigation is to study the applicability of this new perspective to a wider class of policy distributions.
5 EXPERIMENTS
We utilize the insights from Section 3 to introduce a new RL algorithm, Smoothie. Smoothie maintains a parameterized Q~w trained via the procedure described in Section 3.2. It then uses the gradient and Hessian of this approximation to train a Gaussian policy µ,  using the updates stated in (14) and (16). We perform a number of evaluations of Smoothie compared to DDPG. We choose DDPG as a baseline because it (1) utilizes gradient information of a Q-value approximator, much like our algorithm; and (2) is a standard algorithm well-known to have achieve good, sample-efficient performance on continuous control benchmarks.
5.1 A SYNTHETIC TASK
To evaluate Smoothie we begin with a simple synthetic task which allows us to study its behavior in a restricted setting. We devised a simple single-action one-shot environment in which the reward
6

Under review as a conference paper at ICLR 2018

function is a mixture of two Gaussians, one better than the other (see Figure 1 (Right)). We initialize the policy mean to be centered on the worse of the two Gaussians. We plot the learnable policy mean and standard deviation during training for Smoothie and DDPG in Figure 1 (Left). Smoothie learns both the mean and variance, while DDPG learns only the mean and the variance plotted is the exploratory noise, whose scale is kept fixed during training.
As expected we observe that DDPG cannot escape the local optimum. At the beginning of training it exhibits some movement away from the local optimum (likely due to the initial noisy approximation given by Qw), it is unable to progress very far from the initial mean. Note that this is not an issue of exploration. The exploration scale is high enough that Qw is aware of the better Gaussian. The issue is in the update for µ, which is only with regard to the derivative of Qw at the current mean.
On the other hand, we find Smoothie is successfully able to solve the task. This is because the smoothed reward function approximated by Q~w has a derivative which clearly points µ towards the better Gaussian. We also observe that Smoothie is able to suitably adjust the covariance  during training. Initially,  decreases due to the concavity of the smoothed reward function. As a region of convexity is entered, it begins to increase, before again decreasing to near-zero as µ approaches the global optimum.

Action Reward

1.0 0.5 0.0 -0.5 -1.0
0

5000

10000

Training iteration

15000

0.8 0.7 0.6 0.5 0.4 0.3 0.2
-1.0

Smoothie

DDPG

Reward

-0.5 0.0 Action

0.5

Smoothed Reward

1.0

Figure 1: Left: The learnable policy mean and standard deviation during training for Smoothie and DDPG on a simple one-shot synthetic task. The standard deviation for DDPG is the exploratory noise kept constant during training. Right: The reward function for the synthetic task along with its Gaussian-smoothed version. We find that Smoothie can successfully escape the lower-reward local optimum. We also notice Smoothie increases and decreases its policy variance as the convexity/concavity of the smoothed reward function changes.

5.2 CONTINUOUS CONTROL
We now turn our attention to standard continuous control benchmarks available on OpenAI Gym (Brockman et al., 2016) utilizing the MuJoCo environment (Todorov et al., 2012).
Our implementations utilize feed forward neural networks for policy and Q-values. We parameterize the covariance  as a diagonal given by e. The exploration for DDPG is determined by an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930; Lillicrap et al., 2016). Additional implementation details are provided in the Appendix.
We compare the results of Smoothie and DDPG in Figure 2. For each task we performed a hyperparameter search over actor learning rate, critic learning rate and reward scale, and plot the average of six runs for the best hyperparameters. For DDPG we extended the hyperparameter search to also consider the scale and damping of exploratory noise provided by the Ornstein-Uhlenbeck process.
Despite DDPG having the advantage of its exploration decided by a hyperparameter search while Smoothie must learn its exploration without supervision, we find that Smoothie performs competitively across all tasks, exhibiting a slight advantage in Swimmer, Ant, and Humanoid.
These results make it clear that on more difficult continuous control tasks (Hopper, Walker2d, Ant, Humanoid), both DDPG and Smoothie suffer from instability, showing high variance across runs and

7

Under review as a conference paper at ICLR 2018

HalfCheetah

Swimmer

8000 6000

350 300 250

4000

200 150

2000

100

50

00

0.00 0.25 0.50 0.75 1.00

0.0 0.5 1.0

Walker2d

Ant

3500 3000 2500 2000 1500 1000

4000 3500 3000 2500 2000 1500

500 1000

500

0.0 0.5 1.0 1.5 2.0

0.0 0.5 1.0

Smoothie

2000 1500 1000
500

1.5 2.0

0.0

4000 3500 3000 2500 2000 1500 1000
500

1.5 2.0 DDPG

0.0

Hopper

0.5 1.0 Humanoid

1.5

0.5 1.0 1.5

2.0 2.0

Figure 2: Results of Smoothie and DDPG on continuous control benchmarks with x-axis in millions of environment steps. Each plot shows the average reward and standard deviation clipped at the min and max of six randomly seeded runs after choosing best hyperparameters. We see that Smoothie is competitive with DDPG even when DDPG uses a hyperparameter-tuned noise scale and Smoothie learns the optimal noise scale (the covariance) during training. We observe some advantages in Ant and Humanoid. Additionally, Swimmer exhibits less variance near the end of training.

a performance that is far behind trust region and proximal policy methods (Schulman et al., 2015; Nachum et al., 2017b; Schulman et al., 2017). Thus, we evaluated Smoothie with a KL-penalty and present these results in Figure 3.
We observe that Smoothie augmented with a KL-penalty consistently improves performance. The improvement is especially dramatic for Hopper, where the average reward is doubled. We also highlight the results for Humanoid, which also exhibits a significant improvement. As far as we know, these Humanoid results are the best published results for a method that only trains on the order of millions of environment steps. In contrast, TRPO, which to the best of our knowledge is the only other algorithm which can achieve better performance, requires on the order of tens of millions of environment steps. This gives added evidence to the benefits of using a learnable covariance and not restricting a policy to be deterministic.

3500 3000 2500 2000 1500 1000
500
0.0

Hopper
0.5 1.0 1.5

4000 3000 2000 1000
2.0 0.0

Walker2d
0.5 1.0 1.5 Smoothie

Ant
4000 3000 2000 1000 2.0 0.0 0.5 1.0 1.5 Smoothie with KL-penalty

4000 3000 2000 1000
2.0 0.0

Humanoid
0.5 1.0 1.5

2.0

Figure 3: Results of Smoothie and Smoothie augmented with a KL-penalty with x-axis in millions of environment steps. We observe benefits of using a proximal policy optimization method, especially in Hopper and Humanoid, where the performance improvement is significant without sacrificing sample efficiency.

8

Under review as a conference paper at ICLR 2018
6 CONCLUSION
We have presented a new Q-value function, Q~, that is a Gaussian-smoothed version of the standard expected Q-value, Q. The advantage of using Q~ over Q is that its gradient and Hessian possess an intimate relationship with the gradient of expected reward with respect to mean and covariance of a Gaussian policy. The resulting algorithm, Smoothie, is able to successfully learn both mean and covariance during training, leading to performance that can match or surpass that of DDPG, especially when incorporating a penalty on divergence from a previous policy. The success of Q~ is encouraging. Intuitively it may be argued that learning Q~ is more sensible than learning Q. The smoothed Q-values by definition make the true reward surface smoother, thus possibly easier to learn; moreover the smoothed Q-values have a more direct relationship with the expected discounted return objective. We encourage future work to further investigate these claims as well as techniques to apply the underlying motivations for Q~ to other types of policies.
REFERENCES
Marc G Bellemare, Will Dabney, and Re´mi Munos. A distributional perspective on reinforcement learning. In ICML, pp. 449­458, 2017.
Georges Bonnet. Transformations des signaux ale´atoires a travers les systemes non line´aires sans me´moire. Annals of Telecommunications, 19(9):203­220, 1964.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.
Kamil Ciosek and Shimon Whiteson. Expected policy gradients. arXiv preprint arXiv:1706.05374, 2017.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. ICML, 2012.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Scho¨lkopf, and Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. NIPS, 2017.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. ICML, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In NIPS, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms, 2000.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR, 2016.
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of continuous actions for deep RL. CoRR, abs/1705.05035, 2017. URL http://arxiv.org/ abs/1705.05035.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In NIPS, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. NIPS, 2017a.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust region method for continuous control. arXiv preprint arXiv:1707.01891, 2017b.
9

Under review as a conference paper at ICLR 2018
Robert Price. A useful theorem for nonlinear devices having gaussian inputs. IRE Transactions on Information Theory, 4(2):69­72, 1958.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278­1286, 2014.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. 1994.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. NIPS, 2000. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical review, 36(5):823, 1930. Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and empirical analysis of expected sarsa. In Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on, pp. 177­184. IEEE, 2009. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. ICLR, 2017. Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 1991.
10

Under review as a conference paper at ICLR 2018

A PROOF OF EQUATION (15)

We note that similar identities for Gaussian integrals exist in the literature (Price, 1958; Rezende et al., 2014) and point the reader to these works for further information.

The specific identity we state may be derived using standard matrix calculus. We make use of the

fact that

 |A|-1/2 = - 1 |A|-3/2  |A| = - 1 |A|-1/2A-1,

A

2 A

2

(20)

and for symmetric A,

 A

||v||2A-1

=

-A-1vvT A-1.

(21)

We omit s from (s) in the following equations for succinctness. The LHS of (15) is

Q(s, a~)  N (a~|a, )da~ A 

=

Q(s, a~) exp
A

-

1 2

||a~

-

a||2-1

 |2|-1/2 

-

1 |2|-1/2 2

 ||a~ 

-

a||2 -1

da~

1 =

Q(s, a~)N (a~|a, ) --1 + -1(a~ - a)(a~ - a)T -1 da~. (22)

2A

Meanwhile, towards tackling the RHS of (15) we note that

Q~(s, a) =

Q(s, a~)N (a~|a, )-1(a~ - a)da~.

a A

(23)

Thus we have

2Q~(s, a) a2 =

Q(s, a~)
A

-1(a~ - a)  N (a~|a, ) + N (a~|a, )  -1(a~ - a) a a

da~

(24)

= Q(s, a~)N (a~|a, )(-1(a~ - a)(a~ - a)T -1 - -1)da~.
A

(25)

B PROOF OF THE CONDITIONS FOR COMPATIBILITY

We shall show how the conditions stated for compatibility with respect to  are sufficient. The reasoning for µ follows via a similar argument. We also refer the reader to Silver et al. (2014) which includes a similar procedure for showing compatibility.
From the second condition for compatibility with respect to  we have

S a2 Q~w (s, a) a=µ(s) - a2 Q~(s, a) a=µ(s) w 2aQ~w(s, a) a=µ(s) d(s) = 0. (26) We may combine this with the first condition to find

S 2aQ~w (s, a) a=µ(s)(s)d(s) = S 2aQ~(s, a) a=µ(s)(s)d(s), which is the desired property for compatibility.

(27)

C IMPLEMENTATION DETAILS
We utilize feed forward networks for both policy and Q-value approximator. For µ(s) we use two hidden layers of dimensions (400, 300) and relu activation functions. For Q~w(s, a) and Qw (s, a) we first embed the state into a 400 dimensional vector using a fully-connected layer and tanh nonlinearity. We then concatenate the embedded state with a and pass the result through a 1-hidden

11

Under review as a conference paper at ICLR 2018

Hyperparameter
actor learning rate critic learning rate
reward scale OU damping OU stddev
 discount factor target network lag
batch size clipping on gradients of Q num gradient updates per observation
Huber loss clipping

Range
[1e-6,1e-3] [1e-6,1e-3] [0.01,0.3] [1e-4,1e-3] [1e-3,1.0] [1e-6, 4e-2]
0.995 0.01 128 4.0
1 1.0

Sampling
log log log log log log fixed fixed fixed fixed fixed fixed

Table 1: Random hyperparameter search procedure. We also include the hyperparameters which we kept fixed.

layer neural network of dimension 300 with tanh activations. We use a diagonal (s) = e for Smoothie, with  initialized to -1.
To find optimal hyperparameters we perform a 100-trial random search over the hyperparameters specified in Table 1. The OU exploration parameters only apply to DDPG. The  coefficient on KL-penalty only applies to Smoothie with a KL-penalty.

12

