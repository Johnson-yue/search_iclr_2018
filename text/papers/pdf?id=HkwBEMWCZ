Under review as a conference paper at ICLR 2018
SKIP CONNECTIONS ELIMINATE SINGULARITIES
Anonymous authors Paper under double-blind review
ABSTRACT
Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the "ghosts" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.
1 INTRODUCTION
Skip connections are extra connections between nodes in different layers of a neural network that skip one or more layers of nonlinear processing. The introduction of skip (or residual) connections has substantially improved the training of very deep neural networks (He et al., 2015; 2016; Huang et al., 2016; Srivastava et al., 2015). Despite informal intuitions put forward to motivate skip connections, a clear understanding of how these connections improve training has been lacking. Such understanding is invaluable both in its own right and for the possibilities it might offer for further improvements in training very deep neural networks. In this paper, we attempt to shed light on this question. We argue that skip connections improve the training of deep networks partly by eliminating the singularities inherent in the loss landscapes of deep networks. These singularities are caused by the non-identifiability of subsets of parameters when nodes in the network either get eliminated or collapse into each other, called elimination and overlap singularities, respectively (Wei et al., 2008). Previous work has identified these singularities and has shown that they significantly slow down learning in shallow neural networks (Saad & Solla, 1995; Amari et al., 2006; Wei et al., 2008). We show that skip connections eliminate these singularities and provide evidence suggesting that they improve training partly by ameliorating the learning slow-down caused by the singularities.
2 RESULTS
2.1 SINGULARITIES IN FULLY-CONNECTED LAYERS AND HOW SKIP CONNECTIONS BREAK
THEM
In fully-connected layers, two types of singularity have been identified in previous work (Amari et al., 2006; Wei et al., 2008): elimination and overlap singularities, both related to the nonidentifiability of the model. The Hessian of the loss function becomes singular at both types of singularity (Supplementary Note 1). These are sometimes called degenerate or higher-order saddles (Anandkumar & Ge, 2016). Elimination singularities arise when a hidden unit is effectively killed, e.g. when its incoming (or outgoing) weights become zero (Figure 1a). This makes the outgoing (or incoming) connections of the unit non-identifiable. Overlap singularities are caused by the
1

Under review as a conference paper at ICLR 2018

a w ...

w =0
...

J=0 J

Elimination singularities

w
...
J =0

w =0
...
J

identifiable? yneos skip

b wa ... wb wa ... wb

Ja =Jb

Ja =Jb

Overlap singularity

Figure 1: Singularities in a fully connected layer and how skip connections break them. (a) In elimination singularities, zero incoming weights, J = 0, eliminate units, and thereby make outgoing weights, w, non-identifiable (red). Skip connections (blue) ensure units are active at least sometimes, so the outgoing weights are identifiable (green). The reverse holds for zero outgoing weights, w = 0: skip connections recover identifiability for J. (b) In overlap singularities, overlapping incoming weights, Ja = Jb, make outgoing weights non-identifiable; skip connections again break the degeneracy.

permutation symmetry of the hidden units at a given layer and they arise when two units become identical, e.g. when their incoming weights become identical (Figure 1b). In this case, the outgoing connections of the units are no longer identifiable individually (only their sum is identifiable).
How do skip connections eliminate these singularities? Skip connections between adjacent layers break the elimination singularities by ensuring that the units are active at least for some inputs, even when their adjustable incoming or outgoing connections become zero (Figure 1a; right). They also eliminate the overlap singularities by breaking the permutation symmetry of the hidden units at a given layer (Figure 1b; right). Thus, even when the adjustable incoming weights of two units become identical, the units do not collapse into each other, since their distinct skip connections still disambiguate them.
2.2 WHY ARE SINGULARITIES HARMFUL FOR LEARNING?
The effect of singularities on gradient-based learning has been analyzed previously for shallow networks (Amari et al., 2006; Wei et al., 2008). Figure 2a shows the simplified two hidden unit model analyzed in Wei et al. (2008) and its reduction to a two-dimensional system in terms of the variables h and z. Both types of singularity cause degenerate manifolds in the loss landscape, represented by the lines h = 0 and z = ±1 in Figure 2b, corresponding to the overlap and elimination singularities respectively. The elimination manifolds divide the overlap manifolds into stable and unstable segments. According to the analysis presented in Wei et al. (2008), these manifolds give rise to two types of plateaus in the learning dynamics: on-singularity plateaus which are caused by the random walk behavior of stochastic gradient descent (SGD) along a stable segment of the overlap manifolds (thick segment on the h = 0 line in Figure 2b) until it escapes the stable segment (Figure 2d), and near-singularity plateaus which cause a general slowing of the dynamics near the overlap manifolds, even when the initial location is not within the basin of attraction of the stable segment (Figure 2c).
Although this analysis only holds for two hidden units, for higher dimensional cases, it suggests that overlaps between hidden units significantly slow down learning along the overlap directions. These overlap directions become more numerous as the number of hidden units increases, thereby reducing the effective dimensionality of the model. We provide empirical evidence for this claim below.
2.3 PLAIN NETWORKS ARE MORE DEGENERATE THAN NETWORKS WITH SKIP CONNECTIONS
To investigate the relationship between degeneracy, training difficulty and skip connections in deep networks, we conducted several experiments with deep fully-connected networks. We compared three different architectures: (i) the plain architecture is a fully-connected feedforward network with no skip connections, described by the equation:
xl+1 = f (Wlxl + bl+1) l = 0, . . . , L - 1
where f is the ReLU nonlinearity and x0 denotes the input layer. (ii) The residual architecture introduces identity skip connections between adjacent layers (note that we do not allow skip connections

2

Under review as a conference paper at ICLR 2018

a b c dwa

wb

epliamrainmaettioern

wa­ wb wa+wb

=z

Gradient flow
2
1
z0

3 0

Near-singularity plateau
4
z z,h h

On-singularity plateau
1
z,h 0
­1

log speed

J J ­1

a

b

overlap

1 2

||Ja­

Jb||2

=

h

­2 0

h

1 ­3

0 0 50 100
Time

­2

0

2500

5000

Time

Figure 2: Why singularities are harmful for gradient-based learning. (a) Diagram of the analyzed network and parameter reduction performed in the analysis in Wei et al. (2008). h = 0 corresponds to the overlap singularity and z = ±1 correspond to the elimination singularities. (b) The gradient flow field for the two-dimensional reduced system. The gradient norm is indicated by color. The segment marked by the thick solid line is stable in this example; its basin of attraction is shaded in gray. Dynamics of the reduced parameters starting from the black dot in (b) illustrates the nearsingularity plateau (c). Starting from the blue dot in (b) illustrates the on-singularity plateau (d). Analysis and plots adapted from Wei et al. (2008).

from the input layer):
x1 = f (W0x0 + b1), xl+1 = f (Wlxl + bl+1) + xl l = 1, . . . , L - 1
(iii) The hyper-residual architecture adds skip connections between each layer and all layers above it:
l-1
x1 = f (W0x0+b1), x2 = f (W1x1+b2)+x1, xl+1 = f (Wlxl+bl+1)+xl+ Qkxk l = 2, . . . , L-1
k=1
The skip connectivity from the immediately preceding layer is always the identity matrix, whereas the remaining skip connections Qk are fixed, but allowed to be different from the identity (see Supplementary Note 2 for further details). This architecture is inspired by the DenseNet architecture (Huang et al., 2016). In both architectures, each layer projects skip connections to layers above it. However, in the DenseNet architecture, the skip connectivity matrices are learned, whereas in the hyper-residual architecture considered here, they are fixed.
In the experiments of this subsection, the networks all had L = 20 hidden layers (followed by a softmax layer at the top) and n = 128 hidden units in each hidden layer. Hence, the networks had the same total number of parameters. The biases were initialized to 0, the weights were initialized with the Glorot normal initialization scheme (Glorot & Bengio, 2010). The networks were trained on the CIFAR-100 dataset (with coarse labels) using the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.0005 and a batch size of 500. Because we are mainly interested in understanding how singularities, and their removal, change the shape of the loss landscape and consequently affect the optimization difficulty, we primarily monitor the training accuracy rather than test accuracy in the results reported below.
To measure degeneracy, we estimated the eigenvalue density of the Hessian during training for the three different network architectures. The probability of small eigenvalues in the eigenvalue density reflects the dimensionality of the degenerate parameter space. To estimate this eigenvalue density in our  1M-dimensional parameter spaces, we first estimated the first four moments of the spectral density using the method of Skilling (Skilling, 1989) and fit the estimated moments with a flexible mixture density model (see Supplementary Note 3 for details) consisting of a narrow Gaussian component to capture the bulk of the spectral density, and a skew Gaussian density to capture the tails (see Figure 3d for example fits). From the fitted mixture density, we estimated the fraction of degenerate eigenvalues and the fraction of negative eigenvalues during training.
We validated our main results with smaller networks with  5K parameters where we could calculate all eigenvalues of the Hessian numerically (Supplementary Note 4). Tests with these numerically tractable networks indicated that although there is a significant positive correlation between the actual and estimated fractions of degenerate and negative eigenvalues, the absolute values of the actual
3

Under review as a conference paper at ICLR 2018

Training acc. (%) Eigval. deg. index Eigval. neg. index
p( )

a bCIFAR100 (20 layers) 100 max

50 Plain
Residual Hyperres.
0 0 50 100
Epoch

min 0

c max

50 100
Epoch

min 0

50 100
Epoch

d Ex. spectra after ep. 3 1e2 1e4 1e6 1e850 0 50

Figure 3: Model degeneracy increases training difficulty. (a) Training accuracy of different architectures. (b) Eigenvalue degeneracy index of the models during training. Error bars are standard errors over 50 independent runs of the simulations. (c) Eigenvalue negativity index of the models during training. (d) Example fitted spectra after 3 training epochs.

fractions could not be estimated reliably from our mixture model, at least in these smaller networks (Supplementary Note 4). Therefore, we report the estimated fractions of degenerate and negative eigenvalues as eigenvalue degeneracy and negativity indices below, which should be thought of as positively correlated with, but not identical to, the actual fractions of degenerate and negative eigenvalues, respectively.
Figure 3b shows the evolution of the eigenvalue degeneracy index during training. A large degeneracy index at a particular point during optimization indicates a more degenerate model. By this measure, the hyper-residual architecture is the least degenerate and the plain architecture is the most degenerate one. We observe the opposite pattern for the eigenvalue negativity index (Figure 3c). The differences between the architectures are more prominent early on in the training and there is an indication of a crossover later during training, with less degenerate models early on becoming slightly more degenerate later on as the training performance starts to saturate (Figure 3b). Importantly, the hyper-residual architecture has the highest training speed and the plain architecture has the lowest training speed (Figure 3a), consistent with our hypothesis that the degeneracy of a model increases the training difficulty and skip connections reduce the degeneracy.
2.4 TRAINING ACCURACY IS RELATED TO DISTANCE FROM DEGENERATE MANIFOLDS
To establish a more direct relationship between the elimination and overlap singularities discussed earlier on the one hand, and model degeneracy and training difficulty on the other, we exploited the natural variability in training the same model caused by the stochasticity of SGD and random initialization. Specifically, we trained 80 plain networks (30 hidden layers, 128 neurons per layer) on CIFAR-100 using different random initializations and random mini-batch selection. Training performance varied widely across runs. We compared the best 10 and the worst 10 runs (measured by mean accuracy over 100 training epochs, Figure 4a). The worst networks were more degenerate (Figure 4b); they were significantly closer to elimination singularities, as measured by the average l2-norm of the incoming weights of their hidden units (Figure 4c), and significantly closer to overlap singularities (Figure 4d), as measured by the mean correlation between the incoming weights of their hidden units. Interestingly, the overlaps of the weight vectors for the best vs. the worst networks start to diverge from each other earlier than the weight vector norms. This seems to be consistent with the analysis from Wei et al. (2008) above suggesting that overlap manifolds have a stronger influence on learning dynamics than elimination manifolds (Figure 2).
2.5 BENEFITS OF SKIP CONNECTIONS ARE NOT EXPLAINED BY GOOD INITIALIZATION
ALONE
To investigate if the benefits of skip connections can be explained in terms of favorable initialization of the parameters, we introduced a malicious initialization scheme for the residual network by subtracting the identity matrix from the initial weight matrices, Wl. If the benefits of skip connections can be explained primarily by favorable initialization, this malicious initialization would be expected to cancel the effects of skip connections at initialization and hence significantly deteriorate the performance. However, the malicious initialization only had a small adverse effect on the performance of the residual network (Figure 5; ResMalInit), suggesting that the benefits of skip connections cannot be explained by favorable initialization alone. This result reveals a fundamental weakness in previous explanations of the benefits of skip connections based purely on linear models (Hardt &
4

Under review as a conference paper at ICLR 2018

Tr. accuracy (%) Eigval. deg. index
Mean norm Mean overlap

a 100 50

30 layers
Best 10 Worst 10

b max

0 0 50 100
Epoch

min 0

c 1.4

50 100
Epoch

10

d 0.02

50 100
Epoch

0 0

50 100
Epoch

Figure 4: Training accuracy is correlated with distance from degenerate manifolds. (a) Training accuracies of the best 10 and the worst 10 plain networks trained on CIFAR-100. (b) Eigenvalue degeneracy index of the networks throughout training. Error bars are standard errors over 10 networks. (c) Mean norm of the incoming weight vectors of hidden units. (d) Mean overlap of the weight vectors of hidden units.

Ma, 2016; Li et al., 2016). In Supplementary Note 5 we show that skip connections do not eliminate the singularities in deep linear networks, but only shift the landscape so that typical initializations are farther from the singularities. Thus, in linear networks, any benefits of skip connections are due entirely to better initialization. In contrast, skip connections genuinely eliminate the singularities in nonlinear networks (Supplementary Note 1). The fact that malicious initialization of the residual network does reduce its performance suggests that "ghosts" of these singularities still exist in the loss landscape of nonlinear networks, but the performance reduction is only slight, suggesting that skip connections alter the landscape around these ghosts to alleviate the learning slow-down that would otherwise take place near them.
2.6 ALTERNATIVE WAYS OF ELIMINATING THE SINGULARITIES
If the success of skip connections can be attributed, at least partly, to eliminating singularities, then alternative ways of eliminating them should also improve training. We tested this hypothesis by introducing a particularly simple way of eliminating singularities: for each layer we drew random target biases from a Gaussian distribution, N (µ, ), and put an l2-norm penalty on learned biases deviating from those targets. This breaks the permutation symmetry between units and eliminates the overlap singularities. In addition, positive µ values decrease the average threshold of the units and make the elimination of units less likely (but not impossible), hence reducing the elimination singularities. Note that setting µ = 0 and  = 0 corresponds to the standard l2-norm regularization of the biases, which does not eliminate any of the overlap or elimination singularities. Hence, we expect the performance to be worse in this case than in cases with properly eliminated singularities. On the other hand, although in general, larger values of µ and  correspond to greater elimination of singularities, the network also has to perform well in the classification task and very large µ,  values might be inconsistent with the latter requirement. Therefore, we expect the performance to be optimal for intermediate values of µ and . In the experiments reported below, we optimized the hyperparameters µ, , and , i.e. the mean and the standard deviation of the target bias distribution and the strength of the bias regularization term, through random search (Bergstra & Bengio, 2012).
We trained 30-layer fully-connected feedforward networks on CIFAR-10 and CIFAR-100 datasets. Figure 5a-b shows the training accuracy of different models on the two datasets. For both datasets, among the models shown in Figure 5, the residual network performs the best and the plain network the worst. Our simple singularity elimination through bias regularization scheme (BiasReg, cyan) significantly improves performance over the plain network. Importantly, the standard l2-norm regularization on the biases (BiasL2Reg (µ = 0,  = 0), magenta) does not improve performance over the plain network. These results are consistent with the singularity elimination hypothesis.
There is still a significant performance gap between our BiasReg network and the residual network despite the fact that both break degeneracies. This can be partly attributed to the fact that the residual network breaks the degeneracies more effectively than the BiasReg network (Figure 5c). Secondly, even in models that completely eliminate the singularities, the learning speed would still depend on the behavior of the gradient norms, and the residual network fares better than the BiasReg network in this respect as well. At the beginning of training, the gradient norms with respect to the layer activities do not diminish in earlier layers of the residual network (Figure 6a, Epoch 0), demon-
5

Under review as a conference paper at ICLR 2018

a 100

CIFAR10 (30 layers)

b CIFAR100 (30 layers) 100

c max

Eigval. deg. index

Training accuracy (%)

Training accuracy (%)

50 Residual

50

ResMalInit

BiasReg

BiasL2Reg ( = 0, = 0)

Plain
0 0 50

100 0 0

50 100 min 0

50 100

Epoch

Epoch

Epoch

Figure 5: Singularity elimination through bias regularization improves training. (a-b) Training accuracy of 30-layer networks on the CIFAR-10 and CIFAR-100 benchmarks. Error bars represent ±1 SEM over 50 independent runs. (c) Eigenvalue degeneracy indices of the plain, residual and BiasReg networks.

Mean grad. norm

a 0.01 01

Epoch 0
Residual ResMalInit BiasReg+BN BiasReg Plain
15 30
Layer

Epoch 1
1 15 30

Epoch 2

b 100

Tr. accuracy (%)

50

1 15 30

00

50 100
Epoch

Figure 6: Skip connections effectively deal with the vanishing gradients problem. (a) Mean gradient norms with respect to layer activities at the beginning of the first three epochs. Note that the mean gradient norms of Plain and BiasReg networks are almost identical initially. (b) Training accuracy of the networks on the CIFAR-100 dataset. The BiasReg network with a single batch normalization layer inserted at layer 15 (BiasReg+BN) is shown in yellow. Its performance approaches the performance of the residual network. The results shown are averages over 50 independent runs. Standard errors are small, hence are not shown for clarity.

strating that it effectively solves the vanishing gradients problem (Hochreiter, 1991; Bengio et al., 1994). On the other hand, both in the plain network and in the BiasReg network, the gradient norms decay quickly as one descends from the top of the network. Moreover, as training progresses (Figure 6a, Epochs 1 and 2), the gradient norms are larger for the residual network than for the plain or the BiasReg network. Even for the maliciously initialized residual network, gradients do not decay quickly at the beginning of training and the gradient norms behave similarly to those of the residual network during training (Figure 6a; ResMalInit), suggesting that skip connections boost the gradient norms near the ghosts of singularities and reduce the learning slow-down that would otherwise take place near them. Adding a single batch normalization layer (Ioffe & Szegedy, 2015) in the middle of the BiasReg network alleviates the vanishing gradients problem for this network and brings its performance closer to that of the residual network (Figure 6a-b; BiasReg+BN).
2.7 NON-IDENTITY SKIP CONNECTIONS
If the singularity elimination hypothesis is correct, there should be nothing special about identity skip connections. Skip connections other than identity should lead to training improvements if they eliminate singularities. For the permutation symmetry breaking of the hidden units, ideally the skip connection vector for each unit should disambiguate that unit maximally from all other units in that layer. This is because as shown by the analysis in Wei et al. (2008) (Figure 2), even partial overlaps between hidden units significantly slow down learning (near-singularity plateaus). Mathematically, the maximal disambiguation requirement corresponds to an orthogonality condition on the skip connectivity matrix (any full-rank matrix breaks the permutation symmetry, but only orthogonal matrices maximally disambiguate the units). We therefore tested random dense orthogonal matrices as skip connectivity matrices. Random dense orthogonal matrices performed slightly bet-
6

Under review as a conference paper at ICLR 2018

Training accuracy (%) Training accuracy (%)
Prob. of zero resp.

a CIFAR10 (30 layers) 100 1 2 4 50 8 16 32 64 128 identity 0 0 50 100 Epoch

bCIFAR100 (30 layers)

CIFAR100 (30 layers)

100 0.07

50

identity

128 (dense ortho.)

0 1 50 100 0.03 1 50 100

Epoch

Epoch

Figure 7: Random dense orthogonal skip connectivity matrices work slightly better than identity skip connections. (a) Increasing the non-orthogonality of the skip connectivity matrix reduces the performance (represented by lighter shades of gray). The results shown are averages over 10 independent runs of the simulations. (b) Probability of zero responses for residual networks with identity skip connections (blue) and dense random orthogonal skip connections (black), averaged over all hidden units and all training examples.

ter than identity skip connections in both CIFAR-10 and CIFAR-100 datasets (Figure 7a, black vs. blue). This is because, even with skip connections, units can be deactivated for some inputs because of the ReLU nonlinearity (recall that we do not allow skip connections from the input layer). When this happens to a single unit at layer l, that unit is effectively eliminated for that subset of inputs, hence eliminating the skip connection to the corresponding unit at layer l+1, if the skip connectivity is the identity. This causes a potential elimination singularity for that particular unit. With dense skip connections, however, this possibility is reduced, since all units in the previous layer are used. Moreover, when two distinct units at layer l are deactivated together, the identity skips cannot disambiguate the corresponding units at the next layer, causing a potential overlap singularity. On the other hand, with dense orthogonal skips, because all units at layer l are used, even if some of them are deactivated, the units at layer l + 1 can still be disambiguated with the remaining active units. Figure 7b confirms for the CIFAR-100 dataset that throughout most of the training, the hidden units of the network with dense orthogonal skip connections have a lower probability of zero responses than those of the network with identity skip connections.
Next, we gradually decreased the degree of "orthogonality" of the skip connectivity matrix to see how the orthogonality of the matrix affects performance. Starting from a random dense orthogonal matrix, we first divided the matrix into two halves and copied the first half to the second half. Starting from n orthonormal vectors, this reduces the number of orthonormal vectors to n/2. We continued on like this until the columns of the matrix were repeats of a single unit vector. We predict that as the number of orthonormal vectors in the skip connectivity matrix is decreased, the performance should deteriorate, because the permutation symmetry-breaking capacity of the skip connectivity matrix is reduced. Figure 7 shows the results for n = 128 hidden units. Darker colors correspond to "more orthogonal" matrices (e.g. "128" means all 128 skip vectors are orthonormal to each other, "1" means all 128 vectors are identical). The blue line is the identity skip connectivity. More orthogonal skip connectivity matrices yield better performance, consistent with our hypothesis.
The less orthogonal skip matrices also suffer from the vanishing gradients problem. So, their failure could be partly attributed to the vanishing gradients problem. To control for this effect, we also designed skip connectivity matrices with eigenvalues on the unit circle (hence with eigenvalue spectra equivalent to an orthogonal matrix), but with varying degrees of orthogonality (see Supplementary Note 6 for details). More specifically, the columns (or rows) of an orthogonal matrix are orthonormal to each other, hence the covariance matrix of these vectors is the identity matrix. We designed matrices where this covariance matrix was allowed to have non-zero off-diagonal values, reflecting the fact that the vectors are not orthogonal any more. By controlling the magnitude of the correlations between the vectors, we manipulated the degree of orthogonality of the vectors. We achieved this by setting the eigenvalue spectrum of the covariance matrix to be given by i = exp(- (i - 1)) where i denotes the i-th eigenvalue of the covariance matrix and  is the parameter that controls the degree of orthogonality:  = 0 corresponds to the identity covariance matrix, hence to an orthonor-
7

Under review as a conference paper at ICLR 2018

Eigvals of the cov. matrix Mean grad. norm
Training accuracy (%) Training accuracy (%)

a 1 0.5

b cCIFAR100 (epoch 0) 0.05 = 0.025 = 0.02 = 0.015 = 0.01 = 0.005 =0

dCIFAR100 (30 layers)

CIFAR10 (30 layers)

100 100

50 50

0 1 64 128 0 1 15 30

0 0 50 100 0 0 50 100

Eigenvalue index

Layer

Epoch

Epoch

Figure 8: Success of orthogonal skip connections cannot be explained by their ability to deal with vanishing gradients only. (a) Eigenvalues of the covariance matrices with different  values:  = 0 corresponds to orthogonal skip connectivity matrices, larger  values correspond to less orthogonal matrices. Note that these eigenvalues are the eigenvalues of the covariance matrix of the skip connectivity vectors. The eigenvalue spectra of the skip connectivity matrices are always fixed to be on the unit circle, hence equivalent to that of an orthogonal matrix. (b) Mean gradient norms with respect to layer activations at the beginning of training. Gradients do not vanish (or explode) in less orthogonal skip connectivity matrices. (c-d) Training accuracies on CIFAR-100 and CIFAR-10.

mal set of vectors, whereas larger values of  correspond to gradually more correlated vectors. This orthogonality manipulation was done while fixing the eigenvalue spectrum of the skip connectivity matrix to be on the unit circle. Hence, the effects of this manipulation cannot be attributed to any change in the eigenvalue spectrum, but only to the degree of orthogonality of the skip vectors. The results of this experiment are shown in Figure 8. More orthogonal skip connectivity matrices still perform better than less orthogonal ones (Figure 8c-d), even when their eigenvalue spectrum is fixed and the vanishing gradients problem does not arise (Figure 8b), suggesting that the results of the earlier experiment (Figure 7) cannot be explained solely by the vanishing gradients problem.
3 DISCUSSION
In this paper, we proposed a novel explanation for the benefits of skip connections in terms of the elimination of singularities. Our results suggest that elimination of singularities contributes at least partly to the success of skip connections. However, we emphasize that singularity elimination is not the only factor explaining the benefits of skip connections. Even in completely non-degenerate models, other independent factors such as the behavior of gradient norms would affect training performance. Indeed, we presented evidence suggesting that skip connections are also quite effective at dealing with the problem of vanishing gradients and not every form of singularity elimination can be expected to be equally good at dealing with such additional problems that beset the training of deep networks.
Alternative explanations: Several of our experiments rule out vanishing gradients as the sole explanation for training difficulties in deep networks and strongly suggest an independent role for the singularities arising from the non-identifiability of the model. (i) In Figure 4, all nets have the exact same plain architecture and similarly vanishing gradients at the beginning of training, yet they have diverging performances correlated with measures of distance from singular manifolds. (ii) Vanishing gradients cannot explain the difference between identity skips and dense orthogonal skips in Figure 7, because both eliminate vanishing gradients, yet dense orthogonal skips perform better. (iii) In Figure 8, spectrum-equalized non-orthogonal skips often have larger gradient norms, yet worse performance than orthogonal skips. (iv) Vanishing gradients cannot even explain the BiasReg results in Figure 5. The BiasReg and the plain net have almost identical (and vanishing) gradients early on in training (Figure 6a), yet the former has better performance as predicted by the symmetry-breaking hypothesis. (v) Similar results hold for two-layer shallow networks where the problem of vanishing gradients does not arise (Supplementary Note 7). In particular, shallow residual nets are less degenerate and have better accuracy than shallow plain nets; moreover, gradient norms and accuracy are strongly correlated with distance from the overlap manifolds in these shallow nets.
Our malicious initialization experiment with residual nets (Figure 5) suggests that the benefits of skip connections cannot be explained solely in terms of well-conditioning or improved initialization either. This result reveals a fundamental weakness in purely linear explanations of the benefits of
8

Under review as a conference paper at ICLR 2018
skip connections (Hardt & Ma, 2016; Li et al., 2016). Unlike in nonlinear nets, improved initialization entirely explains the benefits of skip connections in linear nets (Supplementary Note 5).
A recent paper (Balduzzi et al., 2017) suggested that the loss of spatial structure in the covariance of the gradients, a phenomenon called "shattered gradients", could be partly responsible for training difficulties in deep networks. They argued that skip connections alleviate this problem by essentially making the model "more linear". The precise relationship between this phenomenon and other phenomena that cause training difficulties in deep networks, including the one proposed in this paper, remains to be determined.
Symmetry-breaking in other architectures: We only reported results from experiments with fullyconnected networks, but we note that limited receptive field sizes and weight sharing between units in a single feature channel in convolutional neural networks also reduce the permutation symmetry in a given layer. The symmetry is not entirely eliminated since although individual units do not have permutation symmetry in this case, feature channels do, but they are far fewer in number than the number of individual units. Similarly, a recent extension of the residual architecture called ResNeXt (Xie et al., 2016) uses parallel, segregated processing streams inside the "bottleneck" blocks, which can again be seen as a way of reducing the permutation symmetry inside the block.
Our method of singularity reduction through bias regularization (BiasReg; Figure 5) can be thought of as indirectly putting a prior over the unit activities. More complicated joint priors over hidden unit responses that favor decorrelated (Cogswell et al., 2015) or clustered (Liao et al., 2016) responses have been proposed before. Although the primary motivation for these regularization schemes was to improve the generalizability or interpretability of the learned representations, they can potentially be understood from a singularity elimination perspective as well. For example, a prior that favors decorrelated responses can facilitate the breaking of permutation symmetries between hidden units.
Our results lead to an apparent paradox: over-parametrization and redundancy in large neural network models have been argued to make optimization easier. Yet, our results seem to suggest the opposite. We think that there is an important distinction to be made between data-independent and data-dependent degeneracies. The degeneracies we have dealt with in this paper are dataindependent in the sense that they arise from intrinsic properties of the model architecture itself (e.g. permutation symmetry of the nodes in a given layer) and hold for any data. Data-dependent degeneracies, on the other hand, arise from simply having more degrees of freedom than the number of data points. An example would be fitting two data points with a quadratic model (three parameters). Although this problem is degenerate, the particular degeneracy that arises depends on the data points. We conjecture that these two types of degeneracies have different consequences both for the model's expressive capacity and for optimization under SGD, with data-independent degeneracies reducing the model's effective expressive capacity and being more harmful for optimization with SGD. The results reported here suggest that it could be useful for neural network researchers to pay closer attention to data-independent degeneracies inherent in their models. As a general design principle, we recommend reducing such degeneracies in a model as much as possible.
REFERENCES
S. Amari, H. Park, and T. Ozeki. Singularities affect dynamics of learning in neuromanifolds. Neural Comput., 18:1007­65, 2006.
A. Anandkumar and R. Ge. Efficient approaches for escaping higher order saddle points in nonconvex optimization. 2016. URL https://arxiv.org/abs/1602.05908.
D. Balduzzi, M. Frean, L. Leary, J.P. Lewis, K.W.-D. Ma, and B. McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? ICML, 70:342­350, 2017.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Trans. Neural. Netw., 5:157­66, 1994.
J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 13:281­305, 2012.
9

Under review as a conference paper at ICLR 2018
M. Cogswell, F. Ahmed, R. Girshick, L. Zitnick, and D. Batra. Reducing overfitting in deep networks by decorrelating representations. 2015. URL https://arxiv.org/abs/1511. 06068.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. AISTAS, 9:249­256, 2010.
M. Hardt and T. Ma. Identity matters in deep learning. 2016. URL https://arxiv.org/abs/ 1611.04231.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2015. URL https://arxiv.org/abs/1512.03385.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. 2016. URL https://arxiv.org/abs/1603.05027.
S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, Institut f. Informatik, Technische Univ. Munich, 1991.
G. Huang, Z. Liu, K.Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. 2016. URL https://arxiv.org/abs/1608.06993.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015. URL https://arxiv.org/abs/1502.03167.
D.P. Kingma and J.L. Ba. Adam: a method for stochastic optimization. 2014. URL https: //arxiv.org/abs/1412.6980.
S. Li, J. Jiao, Y. Han, and T. Weissman. Demystifying resnet. 2016. URL https://arxiv. org/abs/1611.01186.
R. Liao, A.G. Schwing, R.S. Zemel, and R. Urtasun. Learning deep parsimonious representations. NIPS, pp. 5076­5084, 2016.
B.A. Pearlmutter. Fast exact multiplication by the hessian. Neural Comput., 6:147­60, 1994. D. Saad and S.A. Solla. On-line learning in soft committee machines. Phys. Rev. E, 52:4225, 1995. A.M. Saxe, J.M. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks. 2013. URL https://arxiv.org/abs/1312.6120. J. Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian
Methods, pp. 455­466. Kluwer Academic Publishers, 1989. R.K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. NIPS, pp. 2377­2385,
2015. H. Wei, J. Zhang, F. Cousseau, T. Ozeki, and S. Amari. Dynamics of learning near singularities in
layered networks. Neural Comput., 20:813­43, 2008. S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated residual transformations for deep neural
networks. 2016. URL https://arxiv.org/abs/1611.05431.
10

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY MATERIALS

SUPPLEMENTARY NOTE 1: SINGULARITY OF THE HESSIAN IN NON-LINEAR MULTILAYER
NETWORKS

Because the cost function can be expressed as a sum over training examples, it is enough to consider

the cost for a single example:

E

=

1 2

||y

-

xL||2



1 2

e

e, where xl are defined recursively as

xl = f (Wl-1xl-1) for l = 1, . . . , L. We denote the inputs to units at layer l by the vector hl:

hl = Wl-1xl-1. We ignore the biases for simplicity. The derivative of the cost function with

respect to a single weight Wl,ij between layers l and l + 1 is given by:

0

E Wl,ij

  = - f  


...

 

(hl+1,i)xl,j

 

...

 

0

Wl+1diag(fl+2)Wl+2diag(fl+3) · · · WL-1diag(fL)e

(1)

Now, consider a different connection between the same output unit i at layer l + 1 and a different input unit j at layer l. The crucial thing to note is that if the units j and j have the same set of incoming weights, then the derivative of the cost function with respect to Wl,ij becomes identical to its derivative with respect to Wl,ij : E/Wl,ij = E/Wl,ij . This is because in this condition xl,j = xl,j for all possible inputs and all the remaining terms in Equation 1 are independent of the input index j. Thus, the columns (or rows) corresponding to the connections Wl,ij and Wl,ij in the Hessian become identical, making the Hessian degenerate. This is a re-statement of the simple observation that when the units j and j have the same set of incoming weights, the parameters Wl,ij and Wl,ij become non-identifiable (only their sum is identifiable). Thus, this corresponds to an overlap singularity.
Moreover, it is easy to see from Equation 1 that, when the presynaptic unit xl,j is always zero, i.e. when that unit is effectively killed, the column (or row) of the Hessian corresponding to the parameter Wl,ij becomes the zero vector for any i, and thus the Hessian becomes singular. This is a re-statement of the simple observation that when the unit xl,j is always zero, its outgoing connections, Wl,ij, are no longer identifiable. This corresponds to an elimination singularity.
In the residual case, the only thing that changes in Equation 1 is that the factors Wk diag(fk+1) on the right-hand side become Wk diag(fk+1) + I where I is an identity matrix of the appropriate size. The overlap singularities are eliminated, because xl,j and xl,j cannot be the same for all possible inputs in the residual case (even when the adjustable incoming weights of these units are identical). Similarly, elimination singularities are also eliminated, because xl,j cannot be identically zero for all possible inputs (even when the adjustable incoming weights of this unit are all zero), assuming that the corresponding unit at the previous layer xl-1,j is not always zero, which, in turn, is guaranteed with an identity skip connection if xl-2,j is not always zero etc., all the way down to the first hidden layer.

SUPPLEMENTARY NOTE 2: SIMULATION DETAILS
In Figure 3, for the skip connections between non-adjacent layers in the hyper-residual networks, i.e. Qk, we used matrices of the type labeled "32" in Figure 7, i.e. matrices consisting of four copies of a set of 32 orthonormal vectors. We found that these matrices performed slightly better than orthogonal matrices.
We augmented the training data in both CIFAR-10 and CIFAR-100 by adding reflected versions of each training image, i.e. their mirror images. This yields a total of 100000 training images for both datasets. The test data were not augmented, consisting of 10000 images in both cases. We used the standard splits of the data into training and test sets.
For the BiasReg network of Figures 5-6, random hyperparameter search returned the following values for the target bias distributions: µ = 0.51,  = 0.96 for CIFAR-10 and µ = 0.91,  = 0.03 for CIFAR-100.

11

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY NOTE 3: ESTIMATING THE EIGENVALUE SPECTRAL DENSITY OF THE HESSIAN IN DEEP NETWORKS

We use Skilling's moment matching method (Skilling, 1989) to estimate the eigenvalue spectra

of the Hessian. We first estimate the first few non-central moments of the density by computing

mk

=

1 N

r

Hkr where r is a random vector drawn from the standard multivariate Gaussian with

zero mean and identity covariance, H is the Hessian and N is the dimensionality of the parameter

space. Because the standard multivariate Gaussian is rotationally symmetric and the Hessian is a

symmetric matrix, it is easy to show that mk gives an unbiased estimate of the k-th moment of the

spectral density:

1

mk

=

r N

Hkr = 1 N

N

~r2i ki 

i=1

p()kd as N  

(2)

where i are the eigenvalues of the Hessian, and p() is the spectral density of the Hessian as N  . In Equation 2, we make use of the fact that ~r2i are random variables with expected value 1.

Despite appearances, the products in mk do not require the computation of the Hessian explicitly and can instead be computed efficiently as follows:

v0 = r, vk = Hvk-1 k = 1, . . . , K

(3)

where the Hessian times vector computation can be performed without computing the Hessian explicitly through Pearlmutter's R-operator (Pearlmutter, 1994). In terms of the vectors vk, the estimates of the moments are given by the following:

1 m2k = N vk vk,

1 m2k+1 = N vk vk+1

(4)

For the results shown in Figure 3, we use 20-layer fully-connected feedforward networks and the number of parameters is N = 709652. For the remaining simulations, we use 30-layer fullyconnected networks and the number of parameters is N = 874772.

We estimate the first four moments of the Hessian and fit the estimated moments with a parametric density model. The parametric density model we use is a mixture of a narrow Gaussian distribution (to capture the bulk of the density) and a skew-normal distribution (to capture the tails):

q() = wSN (; , , ) + (1 - w)N (; 0,  = 0.001)

(5)

with 4 parameters in total: the mixture weight w, and the location , scale  and shape  parameters of the skew-normal distribution. We fix the parameters of the Gaussian component to µ = 0 and  = 0.001. Since the densities are heavy-tailed, the moments are dominated by the tail behavior of the model, hence the fits are not very sensitive to the precise choice of the parameters of the Gaussian component. The moments of our model can be computed in closed-form. We had difficulty fitting the parameters of the model with gradient-based methods, hence we used a simple grid search method instead. The ranges searched over for each parameter was as follows. w: logarithmically spaced between 10-9 and 10-3; : linearly spaced between -50 and 50; : linearly spaced between -10 and 10; : logarithmically spaced between 10-1 and 103. 57 parameters were evaluated along each parameter dimension for a total of 574  10M parameter configurations evaluated.

The estimated moments ranged over several orders of magnitude. To make sure that the optimization gave roughly equal weight to fitting each moment, we minimized a normalized objective function:

L(w, , , ) = 4 |m^ k(w, , , ) - mk| k=1 |mk|
where m^ k(w, , , ) is the model-derived estimate of the k-th moment.

(6)

SUPPLEMENTARY NOTE 4: VALIDATION OF THE RESULTS WITH SMALLER NETWORKS
Here, we validate our main results for smaller, numerically tractable networks with  5K parameters. The networks in this section are 10-layer fully-connected feedforward networks with 16 units

12

Under review as a conference paper at ICLR 2018

a 40

Plain Residual

b 40

Test accuracy (%)

Training accuracy (%)

00
c6

150
Epoch

300 0 0
d 100

150
Epoch

300

Degenerate eigs. (%)

Negative eigs. (%)

0 0 150 300 75 0 150 300

Epoch

Epoch

Figure S1: Validation of the results with 10-layer plain and residual networks trained on CIFAR-100. (a-b) Training and test accuracy. (c-d) Fraction of negative and degenerate eigenvalues throughout training. The results are averages over 4 independent runs ±1 standard errors.

in each hidden layer. The networks are trained on CIFAR-100. The input dimensionality is reduced from 3072 to 128 through PCA. This results in a total of 4852 parameters. This is small enough that we can calculate all eigenvalues of the Hessian numerically. In what follows, we calculate the fraction of degenerate eigenvalues by counting the number of eigenvalues inside a small window around 0, and the fraction of negative eigenvalues by the number of eigenvalues to the left of this window.
We first compare residual networks with plain networks (Figure S1). Residual networks have better training and test performance (Figure S1a-b); they are less degenerate (Figure S1d) and have more negative eigenvalues than plain networks (Figure S1c). These results are consistent with the results reported in Figure 3 for deeper and larger networks.
Next, we validate the results reported in Figure 4 by running 400 independent plain networks and comparing the best-performing 40 with the worst-performing 40 among them (Figure S2). We observe that the best networks are less degenerate (Figure S2d) and have more negative eigenvalues than the worst networks (Figure S2c). Moreover, the hidden units of the best networks have less overlap (Figure S2f), and, at least toward the end of the training period shown here, have slightly larger weight norms than the worst-performing networks (Figure S2e). The difference in Figure S2e is not large, because training was stopped at epoch 30 for this experiment. Again, these results (including the smaller difference between the weight norms than between the overlaps) are all consistent with those reported in Figure 4 for deeper and larger networks.
Finally, using the same numerically tractable plain networks described above, we tested whether we could reliably estimate the fractions of degenerate and negative eigenvalues with our mixture model. Just as we do for the larger networks, we first fit the mixture model to the first four moments of the spectral density estimated with the method of Skilling (1989). We then estimate the fraction of degenerate and negative eigenvalues from the fitted mixture model and compare these estimates with
13

Under review as a conference paper at ICLR 2018

a 35

Training accuracy (%)
Best 40 Worst 40

b 35

00
c5

15
Negative eigs. (%)

30 0 0
d100

Test accuracy (%)
15
Degenerate eigs. (%)

30

00
e 1.2
10

15
Mean norm
15
Epoch

30 75 0
f
0.0025 0
0.0025 0.005 0.0075 30 0

15
Mean overlap
15
Epoch

30 30

Figure S2: Validation of the results with 400 10-layer plain networks trained on CIFAR-100. We compare the best 40 networks with the worst 40 networks, as in Figure 4. (a-b) Training and test accuracy. (c-d) Fraction of negative and degenerate eigenvalues throughout training. Better performing networks are less degenerate and have more negative eigenvalues. (e) Mean norms of the incoming weight vectors of the hidden units. (f) Mean overlaps of the hidden units as measured by the mean correlation between their incoming weight vectos. The results are averages over 40 best or worst runs ±1 standard errors.

14

Under review as a conference paper at ICLR 2018

a 100

Fraction of degenerate eigs. (%)

b2

Fraction of negative eigs. (%)

Estimated Estimated

9585 100 0 0

4

Actual

Actual

Figure S3: Estimates obtained from the mixture model overestimate the fraction of degenerate eigenvalues, and underestimate the fraction of negative eigenvalues; however, there is a highly significant linear relationship between the actual values and the estimates. (a) Actual vs. estimated fraction of degenerate eigenvalues for the 10-layer plain networks shown in the previous figure. (b) Actual vs. estimated fraction of negative eigenvalues for the same networks. Dashed line shows the identity line. The red line is the linear regression fit.

those obtained from the numerically calculated actual eigenvalues. We observe that, at least for these small networks, the mixture model usually overestimates the fraction of degenerate eigenvalues and underestimates the fraction of negative eigenvalues. However, there is a significant positive correlation between the actual and estimated fractions (Figure S3).

SUPPLEMENTARY NOTE 5: DYNAMICS OF LEARNING IN LINEAR NETWORKS WITH SKIP
CONNECTIONS

To get a better analytic understanding of the effects of skip connections on the learning dynamics, we turn to linear networks. In an L-layer linear plain network, the input-output mapping is given by (again ignoring the biases for simplicity):

xL = WL-1WL-2 . . . W1x1

(7)

where x1 and xL are the input and output vectors, respectively. In linear residual networks with identity skip connections between adjacent layers, the input-output mapping becomes:

xL = (WL-1 + I)(WL-2 + I) . . . (W1 + I)x1

(8)

Finally, in hyper-residual linear networks where all skip connection matrices are assumed to be the identity, the input-output mapping is given by:

xL = WL-1 + (L - 1)I WL-2 + (L - 2)I . . . W1 + I x1

(9)

In the derivations to follow, we do not have to assume that the connectivity matrices are square matrices. If they are rectangular matrices, the identity matrix I should be interpreted as a rectangular identity matrix of the appropriate size. This corresponds to zero-padding the layers when they are not the same size, as is usually done in practice.

Three-layer networks: Dynamics of learning in plain linear networks with no skip connections was analyzed in Saxe et al. (2013). For a three-layer network (L = 3), the learning dynamics can be expressed by the following differential equations (Saxe et al., 2013):

 d a dt

=

(s - a · b)b -

(a · b )b

=

 d b dt

=

(s - a · b)a -

(a · b)a

=

(10) (11)

15

Under review as a conference paper at ICLR 2018

Here a and b are n-dimensional column vectors (where n is the number of hidden units) connecting the hidden layer to the -th input and output modes, respectively, of the input-output correlation matrix and s is the corresponding singular value (see Saxe et al. (2013) for further details). The first term on the right-hand side of Equations 10-11 facilitates cooperation between a and b corresponding to the same input-output mode , while the second term encourages competition between vectors corresponding to different modes.

In the simplest scenario where there are only two input and output modes, the learning dynamics of Equations 10, 11 reduces to:

d a1 dt

=

(s1 - a1 · b1)b1 - (a1 · b2)b2

d a2 dt

=

(s2 - a2 · b2)b2 - (a2 · b1)b1

d b1 dt

=

(s1 - a1 · b1)a1 - (a1 · b2)a2

d b2 dt

=

(s2 - a2 · b2)a2 - (a2 · b1)a1

(12) (13) (14) (15)

How does adding skip connections between adjacent layers change the learning dynamics? Considering again a three-layer network (L = 3) with only two input and output modes, a straightforward extension of Equations 12-15 shows that the learning dynamics changes as follows:

d a1 = dt

s1 - (a1 + v1) · (b1 + u1) (b1 + u1) - (a1 + v1) · (b2 + u2) (b2 + u2) (16)

d a2 = dt

s2 - (a2 + v2) · (b2 + u2) (b2 + u2) - (a2 + v2) · (b1 + u1) (b1 + u1) (17)

d b1 = dt

s1 - (a1 + v1) · (b1 + u1) (a1 + v1) - (a1 + v1) · (b2 + u2) (a2 + v2) (18)

d b2 = dt

s2 - (a2 + v2) · (b2 + u2) (a2 + v2) - (a2 + v2) · (b1 + u1) (a1 + v1) (19)

where u1 and u2 are orthonormal vectors (similarly for v1 and v2). The derivation proceeds es-

sentially identically to the corresponding derivation for plain networks in Saxe et al. (2013). The

only differences are: (i) we substitute the plain weight matrices Wl with their residual counterparts Wl + I and (ii) when changing the basis from the canonical basis for the weight matrices W1, W2 to the input and output modes of the input-output correlation matrix, U and V, we note that:

W2 + I = UW2 + UU = U(W2 + U )

(20)

W1 + I = W1V + VV = (W1 + V)V

(21)

where U and V are orthogonal matrices and the vectors a, b, u and v in Equations 16-19

correspond to the -th columns of the matrices W1, W2 , U and V, respectively.

Figure S4 shows, for two different initializations, the evolution of the variables a1 and a2 in plain
and residual networks with two input-output modes and two hidden units. When the variables are
initialized to small random values, the dynamics in the plain network initially evolves slowly (Fig-
ure S4a, blue); whereas it is much faster in the residual network (Figure S4a, red). This effect is attributable to two factors. First, the added orthonormal vectors u and v increase the initial ve-
locity of the variables in the residual network. Second, even when we equalize the initial norms of the vectors, a and a + v (and those of the vectors b and b + u) in the plain and the residual
networks, respectively, we still observe an advantage for the residual network (Figure S4b), because
the cooperative and competitive terms are orthogonal to each other in the residual network (or close to orthogonal, depending on the initialization of a and b; see right-hand side of Equations 16-19),
whereas in the plain network they are not necessarily orthogonal and hence can cancel each other
(Equations 12-15), thus slowing down convergence.

Singularity of the Hessian in linear three-layer networks: The dynamics in Equations 10, 11 can be interpreted as gradient descent on the following energy function:

1 E=
2

(s

-

a

·

b)2

+

1 2

(a · b)2

 =

(22)

16

Under review as a conference paper at ICLR 2018

a1 a1 a2 a2

a 1.5

1.5

0 plain 0 residual
-1.5 0 200 400 -1.5 0 200 400
Time Time

b2

1

1 0

0 0 200 400 -0.5 0 200 400
Time Time

Figure S4: Evolution of a1 and a2 in linear plain and residual networks (evolution of b1 and b2
proceeds similarly). The weights converge faster in residual networks. Simulation details are as
follows: the number of hidden units is 2 (the two solid lines for each color represent the weights associated with the two hidden nodes, e.g. a11 and a21 on the left), the singular values are s1 = 3.0, s2 = 1.5. For the residual network, u1 = v1 = [1/ 2, 1/ 2] and u2 = v2 = [1/ 2, -1/ 2] . In (a), the weights of both plain and residual networks are initialized to random values drawn from
a Gaussian with zero mean and standard deviation of 0.0001. The learning rate was set to 0.1. In (b), the weights of the plain network are initialized as follows: the vectorsa1 and a2 are initialized to [1/ 2, 1/ 2] and the vectors b1 and b2 are initialized to [1/ 2, -1/ 2] ; the weights of the residual network are all initialized to zero, thus equalizing the initial norms of the vectors a and a + v (and those of the vectors b and b + u) between the plain and residual networks. The
residual network still converges faster than the plain network. In (b), the learning rate was set to
0.01 to make the different convergence rates of the two networks more visible.

17

Under review as a conference paper at ICLR 2018

This energy function is invariant to a (simultaneous) permutation of the elements of the vectors a and b for all . This causes degenerate manifolds in the landscape. Specifically, for the permutation symmetry of hidden units, these manifolds are the hyperplanes ai = aj , for each pair of hidden units i, j (similarly, the hyperplanes bi = bj ) that make the model non-identifiable. Formally, these correspond to the singularities of the Hessian or the Fisher information matrix. Indeed, we shall quickly check below that when ai = aj  for any pair of hidden units i, j, the Hessian becomes singular (overlap singularities). The Hessian also has additional singularities at the hyperplanes ai = 0  for any i and at bi = 0  for any i (elimination singularities).

Starting from the energy function in Equation 22 and taking the derivative with respect to a single input-to-hidden layer weight, ai:

E ai

=

-(s - a

· b)bi

+ (a · b)bi
=

(23)

and the second derivatives are as follows:

2E (ai )2
2E aiaj

= (bi )2 + (bi )2 = (bi)2

=



= bjbi +

bj bi =

bi bj

=



(24) (25)

Note that the second derivatives are independent of mode index , reflecting the fact that the energy function is invariant to a permutation of the mode indices. Furthermore, when bi = bj for all , the columns in the Hessian corresponding to ai and aj become identical, causing an additional degeneracy reflecting the non-identifiability of ai and aj. A similar derivation establishes that ai = aj for all  also leads to a degeneracy in the Hessian, this time reflecting the non-identifiability of bi and bj . These correspond to the overlap singularities.
In addition, it is easy to see from Equations 24, 25 that when bi = 0 , the right-hand sides of both equations become identically zero, reflecting the non-identifiability of ai for all . A similar derivation shows that when ai = 0 , the columns of the Hessian corresponding to bi become identically zero for all , this time reflecting the non-identifiability of bi for all . These correspond to the elimination singularities.

When we add skip connections between adjacent layers, i.e. in the residual architecture, the energy function changes as follows:

1 E=
2

(s

-

(a

+

v)

·

(b

+

u))2

+

1 2

((a + v) · (b + u))2

 =

(26)

and straightforward algebra yields the following second derivatives:

2E (ai)2
2E aiaj

= =

(bi + ui )2

(bi + ui)(bj + uj )


(27) (28)

Unlike in the plain network, setting bi = bj for all , or setting bi = 0 , does not lead to a degeneracy here, thanks to the orthogonal skip vectors u. However, this just shifts the locations of
the singularities. In particular, the residual network suffers from the same overlap and elimination singularities as the plain network when we make the following change of variables: b  b - u and a  a - v.

Networks with more than three-layers: As shown in Saxe et al. (2013), in linear networks with
more than a single hidden layer, assuming that there are orthogonal matrices Rl and Rl+1 for each layer l that diagonalize the initial weight matrix of the corresponding layer (i.e. Rl+1Wl(0)Rl = Dl is a diagonal matrix), dynamics of different singular modes decouple from each other and each

18

Under review as a conference paper at ICLR 2018

b u

a 2 Plain
0 -2-2 0 2
a

2 Residual
0 -2-2 0 2

b2Hyper-residual

2

01

-2-2 0 2

00

hyper-residual residual plain
200 400
Time

Figure S5: (a) Phase portraits for three-layer plain, residual and hyper-residual linear networks. (b)

Evolution of u =

Nl -1 l=1

al

for

10-layer

plain,

residual

and

hyper-residual

linear

networks.

In

the

plain network, u did not converge to its asymptotic value s within the simulated time window.

mode  evolves according to gradient descent dynamics in an energy landscape described by (Saxe

et al., 2013):

1 Eplain = 2

Nl -1

2

s -

al

l=1

(29)

where al can be interpreted as the strength of mode  at layer l and Nl is the total number of layers.
In residual networks, assuming further that the orthogonal matrices Rl satisfy Rl+1Rl = I, the energy function changes to:

1 Eres = 2

Nl -1

2

s -

(al + 1)

l=1

(30)

and in hyper-residual networks, it is:

1 Ehyperres = 2

Nl -1

2

s -

(al + l)

l=1

(31)

Figure S5a illustrates the effect of skip connections on the phase portrait of a three layer network.
The two axes, a and b, represent the mode strength variables for l = 1 and l = 2, respectively: i.e. a  a1 and b  a2. The plain network has a saddle point at (0, 0) (Figure S5a; left). The dynamics around this point is slow, hence starting from small random values causes initially very
slow learning. The network funnels the dynamics through the unstable manifold a = b to the stable
hyperbolic solution corresponding to ab = s. Identity skip connections between adjacent layers in the residual architecture move the saddle point to (-1, -1) (Figure S5a; middle). This speeds up the
dynamics around the origin, but not as much as in the hyper-residual architecture where the saddle point is moved further away from the origin and the main diagonal to (-1, -2) (Figure S5a; right).
We found these effects to be more pronounced in deeper networks. Figure S5b shows the dynamics
of learning in 10-layer linear networks, demonstrating a clear advantage for the residual architecture
over the plain architecture and for the hyper-residual architecture over the residual architecture.

Singularity of the Hessian in reduced linear multilayer networks with skip connections: The

derivative of the cost function of a linear multilayer residual network (Equation 30) with respect

to the mode strength variable at layer i, ai, is given by (suppressing the mode index  and taking

 = 1):

E ai

=

-(s - u) (al
l=i

+ 1)

(32)

and the second derivatives are:

2E ai2 =

2
(al + 1)
l=i

(33)

2E =
aiak

2 (al + 1) - s

(al + 1)

l l=i,k

(34)

19

Under review as a conference paper at ICLR 2018
It is easy to check that the columns (or rows) corresponding to ai and aj in the Hessian become identical when ai = aj, making the Hessian degenerate. The hyper-residual architecture does not eliminate these degeneracies but shifts them to different locations in the parameter space by adding distinct constants to ai and aj (and to all other variables). SUPPLEMENTARY NOTE 6: DESIGNING SKIP CONNECTIVITY MATRICES WITH VARYING
DEGREES OF ORTHOGONALITY AND WITH EIGENVALUES ON THE UNIT CIRCLE
We generated the covariance matrix of the eigenvectors by S = QQ , where Q is a random orthogonal matrix and  is the diagonal matrix of eigenvalues, ii = exp(- (i - 1)), as explained in the main text. We find the correlation matrix through R = D-1/2SD-1/2 where D is the diagonal matrix of the variances: i.e. Dii = Sii. We take the Cholesky decomposition of the correlation matrix, R = TT . Then the designed skip connectivity matrix is given by  = TULU-1T-1, where L and U are the matrices of eigenvalues and eigenvectors of another randomly generated orthogonal matrix, O: i.e. O = ULU . With this construction,  has the same eigenvalue spectrum as O, however the eigenvectors of  are linear combinations of the eigenvectors of O such that their correlation matrix is given by R. Thus, the eigenvectors of  are not orthogonal to each other unless  = 0. Larger values of  yield more correlated, hence less orthogonal, eigenvectors. SUPPLEMENTARY NOTE 7: VALIDATION OF THE RESULTS WITH SHALLOW NETWORKS To further demonstrate the generality of our results and the independence of the problem of singularities from the vanishing gradients problem in optimization, we performed an experiment with shallow plain and residual networks with only two hidden layers and 16 units in each hidden layer. Because we do not allow skip connections from the input layer, a network with two hidden layers is the shallowest network we can use to compare the plain and residual architectures. Figure S6 shows the results of this experiment. The residual network performs slightly better both on the training and test data (Figure S6a-b); it is less degenerate (Figure S6d) and has more negative eigenvalues (Figure S6c); it has larger gradients (Figure S6e) ­note that the gradients in the plain network do not vanish even at the beginning of training­, and its hidden units has less overlap than the plain network (Figure S6f). Moreover, the gradient norms closely track the mean overlap between the hidden units and the degeneracy of the network (Figure S6d-f) throughout training. These results suggest that the degeneracies caused by the overlaps of hidden units slow down learning, consistent with our symmetry-breaking hypothesis and with the results from larger networks.
20

Under review as a conference paper at ICLR 2018

a 40

Plain Residual

Training acc. (%)

c0 5

Test acc. (%)

b 40 d0
100

Degenerate eigs. (%)

Negative eigs. (%)

e0 0.0015

f 75 0.02

Mean overlap

Mean grad. norm

0

Layer 1

Layer 2

0 0 5 10 15 20 25 0.02 0 5 10 15 20 25

Epoch

Epoch

Figure S6: Main results hold for two-layer shallow nets trained on CIFAR-100. (a-b) Training and test accuracies. Residual nets perform slightly better. (c-d) Fraction of negative and degenerate eigenvalues. Residual nets are less degenerate. (e) Mean gradient norms with respect to the two layer activations throughout training. (f) Mean overlap for the second hidden layer units, measured as the mean correlation between the incoming weights of the hidden units. Results in (a-e) are averages over 16 independent runs; error bars are small, hence not shown for clarity. In (f), error bars represent standard errors.

21

