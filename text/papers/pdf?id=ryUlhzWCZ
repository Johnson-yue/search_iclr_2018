Under review as a conference paper at ICLR 2018
TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION
Anonymous authors Paper under double-blind review
ABSTRACT
Combination of Reinforcement Learning and Imitation Learning brings the best from both: we can quickly learn by imitating near-optimal oracles that achieve good performance on the task, while also explore and exploit to improve what we have learned. In this paper, we propose a novel way to combine imitation and reinforcement via the idea of reward shaping using such an oracle. We theoretically study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner's planning horizon as function of its accuracy. When the oracle is sub-optimal, to ensure to find a policy that can outperform the oracle, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximizing the total reshaped reward over a finite planning horizon. We experimentally demonstrate a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.
1 INTRODUCTION
Reinforcement Learning (RL), equipped with modern deep learning techniques, has dramatically advanced the state-of-the-art in challenging sequential decision problems including high-dimensional robotics control tasks as well as video and board games (Mnih et al., 2015; Silver et al., 2016). However, these approaches typically require a large amount of training data and computational resources to succeed. In response to these challenges, researchers have explored strategies for making RL more efficient by leveraging additional information to guide the learning process. Imitation learning (IL) is one such approach. In IL, the learner can reference expert demonstrations (Abbeel & Ng, 2004) or can access a cost-to-go oracle (Ross & Bagnell, 2014) that provides additional information about the long-term effects of learner decisions. Through these strategies, imitation learning lowers sample complexity by reducing random global exploration. For example, Sun et al. (2017) shows that, with access to an optimal expert, imitation learning can exponentially lower sample complexity compared to pure RL approaches. Experimentally, researchers also have demonstrated sample efficiency by properly leveraging experts' demonstrations, such as by adding demonstrations into a replay buffer (Vecer´ik et al., 2017; Nair et al., 2017).
Although imitating experts can speed up the learning process in RL tasks, the performance of the learned policies are generally limited to the performance of the expert, which is often sub-optimal in practice. Previous imitation learning approaches with strong theoretical guarantees such as Data Aggregation (DAgger) (Ross et al., 2011) and Aggregation with Values (AggreVaTe) (Ross & Bagnell, 2014) can only guarantee a policy which performs as well as the expert policy. Unfortunately, this implies that a sub-optimal expert will result in a sub-optimal learned policy. Ideally, we want the best of both IL and RL: we want to use the expert to quickly learn reasonable policy by imitation, while also exploring how to improve upon the expert with RL. This would allow the learner to overcome the sample inefficiencies inherent in a pure RL strategy while also allowing the learner to eventually surpass a potentially sub-optimal expert. Combining RL and IL is, in fact, not new. Chang et al. (2015) attempted to combine IL and RL by stochastically interleaving incremental RL and IL updates. By doing so, the learned policy will either perform as well as the expert policy­the property of IL (Ross & Bagnell, 2014), or eventually reach a local optimal policy­the property of policy iteration-based RL approaches. Although when the expert policy is sub-optimal the learned
1

Under review as a conference paper at ICLR 2018

locally optimal policy could potentially perform better than the expert policy, it is still difficult to precisely quantify how much the learner can improve over the expert.
In this work, we propose a novel way of combining IL and RL through the idea of Reward Shaping (Ng et al., 1999). Throughout our paper we use cost instead of reward, and we refer reward shaping as cost shaping. We assume access to a cost-to-go oracle that provides an estimate of expert costto-go during training. The key idea is that the cost-to-go oracle can serve as a potential function for cost shaping. For example, consider a task modeled by a Markov Decision Process (MDP). Cost shaping with the cost-to-go oracle produces a new MDP with an optimal policy that is equivalent to the optimal policy of the original MDP (Ng et al., 1999). The idea of cost shaping naturally suggests a strategy for IL: pick a favourite RL algorithm and run it on the new MDP reshaped using expert's cost-to-go oracle. In fact, Ng et al. (1999) demonstrated that running SARSA (Sutton & Barto, 1998) on an MDP reshaped with a potential function that approximates the optimal policy's value-to-go, is an effective strategy.
We take this idea one step further and study the effectiveness of the cost shaping with the expert's cost-to-go oracle, with a focus on the setting where we only have an imperfect cost-to-go oracle V e, i.e., V e = V , where V  is the optimal policy's cost-to-go in the original MDP. We show that cost shaping with the cost-to-go oracle shortens the learner's planning horizon as a function of the accuracy of the oracle V e. Consider two extremes. On the one hand, when we reshape the cost of the original MDP with V  (i.e., expert is optimal), the reshaped MDP has an effective planning horizon of one: a policy that minimizes the one-step cost of the reshaped MDP is in fact the optimal policy (hence the optimal policy of the original MDP). On the other hand, when the cost-to-go oracle provides no information regarding Q, we have no choice but simply optimize the reshaped MDP (or just the original MDP) using RL.
With the above insight, we propose the high-level strategy for combining IL and RL, which we name Truncated HORizon Policy Search with cost-to-go oracle (THOR). The idea is to first shape the cost using the expert's cost-to-go oracle, and then truncate the planning horizon of the new MDP and search for a policy that optimizes over the truncated planning horizon. For MDPs, we mathematically formulate this strategy and guarantee that we will find a policy that performs better than the expert with a gap that can be exactly quantified (which is missing in the previous work of Chang et al. (2015)). In practice, we propose a gradient-based algorithm that is motivated from this insight. The practical algorithm allows us to leverage modern neural networks to represent policies and can be applied to continuous state and action spaces. We verify our approach on several MDPs with continuous state and action spaces and show that THOR can be much more sample efficient than strong RL baselines (we compared to Trust Region Policy Optimization with Generalized Advantage Estimation (TRPO-GAE) (Schulman et al., 2016)), and can learn a significantly better policy than AggreVaTe (we compared to the policy gradient version of AggreVaTe from (Sun et al., 2017)) with access only to an imperfect cost-to-go oracle.

2 PRELIMINARIES

We consider the problem of optimizing Markov Decision Process defined as M0 = (S, A, P, C, ). Here, S is a set of S states and A is a set of A actions; P is the transition dynamics at such that for any s  S, s  S, a  A, P (s |s, a) is the probability of transitioning to state s from state s by
taking action a; For notation simplicity, in the rest of the paper, we will use short notation Psa to represent the distribution P (·|s, a). The cost for a given pair of s and a is c(s, a), which is sampled from the cost distribution C(s, a) with mean value c¯(s, a). A stationary stochastic policy (a|s)
computes the probability of generating action a given state s.

The value function VM 0 and the state action cost-to-go QM0,h(s, a) of  on M0 are defined as:



VM 0 (s) = E

tc(st, at)|s0 = s, a   , QM 0 (s, a) = E c(s, a) + Es Psa [VM 0 (s )] ,

t=0

where the expectation is taken with respect to the randomness of M0 and the stochastic policy . The objective is to search for the optimal policy  such that  = arg min V (s), s  S.

NThortoeutghhaot uwt ethdisownoortkr,ewqueiaressVume(esa)ctcoesbsetoeqaunaol rtaoclVeMV 0e.(sF) o:rSexamRpl(eo, rVQeeh(s(s),

a) : S could

× be

A  R) obtained

2

Under review as a conference paper at ICLR 2018

by learning from trajectories demonstrated by the expert e (e.g., Temporal Difference Learning (Sutton & Barto, 1998)), or V e could be computed by near-optimal search algorithms via access
to ground truth information (Daume´ III et al., 2009) or via access to a simulator (Choudhury et al.,
2017; Pan et al., 2017).

2.1 COST SHAPING

Given the original MDP M0 and any potential functions  : S  R, we can reshape the cost c(s, a) sampled from C(s, a) to be:

c (s, a) = c(s, a) + (s ) - (s), s  Psa.

(1)

Denote the new MDP M as the MDP obtained by replacing c by c in M0: M = (S, A, P, c , ). oNorngigMeinta,altlhM.e(nD19wP9ea9ra)elsstohhoefiwnsaedmdetMh: a0t,Mtthh(ees)oopp=ttiimmMaall 0pp(oosll)iic,cyyso.nMItnheootonhreiMrgiwnoaalrndMds,DthifPewMoeptc0iamtnhaasltupwcoceleisucsylftuimllMyat0efilnyodnwtaMhnet to optimize.

3 EFFECTIVENESS OF COST-TO-GO ORACLE ON PLANNING HORIZON

In this section we study the dependency of effective planning horizon on the cost-to-go oracle. We focus on the setting where we have access to an oracle V e(s) of which is the cost-to-go of some expert policy e. The oracle is imperfect: |V e - VM 0 | = for some  R+. We first show that with such an imperfect oracle, previous IL algorithms AggreVaTe and AggreVaTeD (Ross &

Bagnell, 2014; Sun et al., 2017) are only guaranteed to learn a policy that is away from the optimal.

Theorem 3.1. , such that

There exists an MDP the performance of

and the

an imperfect oracle V e(s) with |V e(s) induced policy from the cost-to-go

- VM 0 oracle

,h(s)| ^

= =

arg mina [c(s, a) + Es Psa [V e(s )]] is at least ( ) away from the optimiality:

J(^) - J()  ( ).

(2)

As AggreVaTe essentially aims to approximately computing ^, the induced policy from the cost-to-
go oracle, it eventually has to suffer from the above lower bound. This gap in fact is not surprising
as AggreVaTe is a one-step greedy algorithm in a sense that it is only optimizing the one-step cost function c from the reshaped MDP M. To see this, note that the cost of the reshaped MDP M is E[c (s, a)] = [c(s, a) + Es Psa V e(s ) - V e(s)], and we have ^(s) = arg mina E[c (s, a)]. Hence AggreVaTe can be regarded as a special algorithm that aims to optimizing the one-step cost of MDP M that is reshaped from the original MDP M0 using the cost-to-go oracle.

Though when the cost-to-go oracle is imperfect, AggreVaTe will suffer from the above lower bound due to being greedy, when the cost-to-go oracle is perfect, i.e., V e = V , being greedy on one-step

cost makes perfect sense. To see this, use the property of the cost shaping (Ng et al., 1999), we can verify that when V e = V :

VM (s) = 0,

M (s)

=

arg

min
a

E[c

(s,

a)],

s  S.

(3)

Namely the optimal policy on the reshaped MDP M only optimizes the one-step cost, which indi-

cates that the optimal cost-to-go oracle shortens the planning horizon to one: finding the optimal

policy on M0 becomes equivalent to optimizing the immediate cost function on M at every state s.

When the cost-to-go oracle is away from the optimality, we lose the one-step greedy property
shown in Eq. 3. In the next section, we show that how we can break the lower bound ( ) only with access to an imperfect cost-to-go oracle V e, by being less greedy and looking head for more than
one-step.

3.1 OUTPERFORMING THE EXPERT

Given the reshaped MDP M, for any state, let us focus on minimizing the total cost of a policy  over a finite k  1 steps at any state s  S:

k
E i-1c (si, ai)|s1 = s; a   .

(4)

i=1

3

Under review as a conference paper at ICLR 2018

Using the definition of cost shaping and telescoping sum trick, we can re-write Eq. 4 in the following format, which we define as k-step disadvantage with respect to the cost-to-go oracle:

k
E i-1c(si, ai) + kV e(sk+1) - V e(s0)|s0 = s; a   , s  S.
i=1

(5)

We assume that our policy class  is rich enough that there always exists a policy ^   that
can simultaneously minimizes the k-step disadvantage at every state. Note that when k = 1, Eq. 5 becomes the problem of finding a policy that minimizes the disadvantage AMe 0 (s, a) (classic definition in RL) with respect to the expert.

To outperform expert, we need to optimize Eq. 5 with k > 1. Again let us denote the policy that minimizes Eq. 5 in every state as ^, and the value function of ^ as V ^ . As ^ is the policy that minimize the k-step disadvantage with k > 1 with respect to V e at any state s, we show that the policy improvement theory still holds for ^:
Lemma 3.2. For ^ that minimizes Eq. 5 at every state s:

V ^ (s)  V e(s), s  S

(6)

Proof. For any state s0  S, we have:

V ^ (s0) - V e(s0)

kk
= E[ ic(si, ai) + k+1V ^ (sk+1)|a  ^] - E[ ic(si, ai) + k+1V e(sk+1)|a  e]

i=0 i=0

kk
 E[ ic(si, ai) + k+1V ^ (sk+1)|a  ^] - E[ ic(si, ai) + k+1V e(sk+1)|a  ^]

i=0
= E[k+1(V ^ (sk+1) - V e(sk+1))].

i=0

(7)

If we keep recursively expanding V  (sk+1) - V e(sk+1), we will see that V ^ (s0) - V e(s0)  0, s0  S.

Lemma 3.2 only tells us that ^ performs at least as well as the expert e, which is no different from what we have shown in the above section. Now we compare ^ to .
Theorem 3.3. Assume ^ minimizes Eq. 5 for every state s  S with k > 1 and |V^ e(s) - V (s)| = ( ), s. We have that for any state s  S:

V ^ (s) - V (s)  O(k ).

(8)

Proof. Using the definition of value function V , for any state s1  S we have:

V ^ (s1) - V (s1)

kk

=E

i-1c(si, ai) + kV ^ (sk+1)|^ - E

i-1c(si, ai) + kV (sk+1)|

i=1 i=1

k k-1
 E i-1c(si, ai) + kV e(sk+1)|^ - E ic(si, ai) + kV (sk+1)|

i=1 i=1

kk
 E i-1c(si, ai) + kV e(sk+1)| - E i-1c(si, ai) + kV (sk+1)|
i=1 i=1

= E k[V e(sk+1) - V (sk+1))]  O(k ),

(9)

where the first inequality comes from the fact that V ^ (s)  V e(s) (Lemma 3.2), the second inequality comes from the fact that ^ is the minimizer from Eq. 5.

4

Under review as a conference paper at ICLR 2018

Algorithm 1 Truncated Horizon Policy Search (THOR)
1: Input: The original MDP M0. Truncation Step k. Oracle V e. 2: Initialize policy 0 with parameter 0 and truncated advantage estimator A^M0,k. 3: for n = 0, ... do 4: Reset system. 5: Execute n to generate a set of trajectories {i}iN=1. 6: Reshape cost c (st, at) = c(st, at) + Vte+1(st+1) - Vte(st), for every t  [1, |i|] in every
trajectory i, i  [N ]. 7: Compute gradient:

|=n =

(ln (at|st))|=n A^Mn,k(st, at)

i t

(10)

8: Update disadvantage estimator to A^Mn,k using {i}i with reshaped cost c . 9: Update policy parameter to n+1. 10: end for

Connecting back to the lower bound shown in Eq. 3.1, we see that ^ from optimizing Eq. 5 outperforms the sub-optimal expert e by at least (1 - k) , which is not achievable by previous IL algorithms. Theorem 3.3 and Theorem 3.1 together summarize that when the expert is imperfect, simply computing a policy that minimizes the one-step disadvantage (i.e., (k = 1)) is not sufficient to guarantee near-optimal performance; however, optimizing a k-step disadvantage with k > 1 leads to a policy that outperforms the expert.
As we already showed, if we set k = 1, then optimizing Eq. 5 becomes optimizing the disadvantage over the expert AeM0 , which is exactly what AggreVaTe aims for. When we set k = , optimizing Eq. 5 or Eq. 4 just becomes optimizing the total cost of the original MDP. Optimizing over a shorter finite horizon is much easier than optimizing over the entire infinite long horizon due to advantages such as smaller variance of the objective function, less temporal correlations between states and costs along a shorter trajectory.

4 PRACTICAL ALGORITHM

Given the original MDP M0 and the cost-to-go oracle V e, the reshaped MDP's cost function c is obtained from Eq. 1 using the cost-to-go oracle as a potential function. Instead of directly applying RL algorithms on M0, we use the fact that the cost-to-go oracle shortens the effective planning horizon of M, and propose THOR: Truncated HORizon Policy Search summarized in Alg. 1. The general idea of THOR is that instead of searching for policies that optimize the total cost over the entire infinitely long horizon, we focus on searching for polices that minimizes the total cost over a truncated horizon, i.e., a k-step time window. Below we first show how we derive THOR from the insight we obtained in Sec. 3.
Let us define a k-step truncated value function VM,k and similar state action value function QM,k on the reshaped MDP M as:

k

VM,k(s) = E

t-1c (st, at)|s1 = s, a   ,

t=1

k-1
QM,k(s, a) = E c (s, a) + ic (si, ai)|si  Psa, ai  (si) ,

i=1

(11)

At any time state s, VM,k only considers (reshaped) cost signals c from a k-step time window.
We are interested in searching for a policy that can optimizes the total cost over a finite k-step horizon as shown in Eq. 4. For MDPs with large or continuous state spaces, we cannot afford to enumerate all states s  S to find a policy that minimizes the k-step disadvantage function as in Eq. 4. Instead one can leverage the approximate policy iteration idea and minimize the weighted

5

Under review as a conference paper at ICLR 2018

cost over state space using a state distribution  (Kakade & Langford, 2002; Bagnell et al., 2004):

k

min


Es0



E

ic (si, ai)|a  

i=1

.

(12)

For parameterized policy  (e.g., neural network policies), we can implement the minimization in
Eq. 12 using gradient-based update procedures (e.g., Stochastic Gradient Descent, Natural Gradient
(Kakade, 2002; Bagnell & Schneider, 2003)) in the policy's parameter space. In the setting where
the system cannot be reset to any state, a typical choice of exploration policy is the currently learned
policy (possibly mixed with a random process (Lillicrap et al., 2015) to futher encourage exploration). Denote n as the currently learned policy after iteration n and P rn (·) as the average state distribution induced by executing n (parameterized by n) on the MDP. Replacing the exploration distribution by P rn (·) in Eq. 12, and taking the derivative with respect to the policy parameter , the policy gradient is:


k


k+i

EsP rn Ekn [  ln (ai|si; )( j-ic (sj , aj ))]

i=1 j=i

k
 EsP rn Ekn [  ln (ai|si; )QM,k(si, ai)]
i=1

where  k  n denotes a partial k-step trajectory  k = {s1, a1, ..., sk, ak|s1 = s} sampled
from executing n on the MDP from state s. Replacing the expectation by empirical samples from n, replacing QM,k by a critic approximated by Generalized disadvantage Estimator (GAE) A^M,k (Schulman et al., 2016), we get back to the gradient used in Alg. 1:

EsP rn
k


k
Ekn [  ln (ai|si; )QM,k(si, ai)]
i=1
| |
 ln((at|st; ))A^M,k(st, at) /H,
t=1

where | | denotes the length of the trajectory  .

(13)

4.1 INTERPRETATION USING TRUNCATED BACK-PROPAGATION THROUGH TIME

If using the classic policy gradient formulation on the reshaped MDP M we should have the following expression, which is just a re-formulation of the classic policy gradient (Williams, 1992):

| | t-1

 = E

ct ( ln (at-i|st-i; )) ,

t=1 i=0

(14)

which is true since the cost ci (we denote ci(s, a) as ci for notation simplicity) at time step i is correlated with the actions at time step t = i all the way back to the beginning t = 1. In other
words, in the policy gradient format, the effectiveness of the cost ct is back-propagated through time all the way back the first step. Our proposed gradient formulation in Eq. 1 shares a similar spirit
of Truncated Back-Propagation Through Time (Zipser, 1990), and can be regarded as a truncated
version of the classic policy gradient formulation: at any time step t, the cost c is back-propagated
through time at most k-steps:

| | k-1

 = E

ct ( ln(at-i|st-i)) ,

t=1 i=0

(15)

In Eq. 15, for any time step t, we ignore the correlation between ct and the actions that are executed k-step before t, hence elimiates long temporal correlations between costs and old actions. In fact,
AggreVaTeD (Sun et al., 2017), a policy gradient version of AggreVaTe, sets k = 1 and can be
regarded as No Back-Propagation through time.

6

Under review as a conference paper at ICLR 2018

4.2 CONNECTION TO IL AND RL

The above gradient formulation provides a natural half-way point between IL and RL. When k = 1 and V e = VM 0 (the optimal value function in the original MDP M0):

| | | |
E (ln (at|st))AMe,1(st, at) = E (ln (at|st))QMe,1(st, at)
t=1 t=1

| | | |

= E

(ln (at|st))E[c (st, at)] = E

(ln (at|st))AM0 (st, at) ,

t=1 t=1

(16)

where, for notation simplicity, we here use E to represent the expectation over trajectories sampled from executing policy , and AM0 is the advantage function on the original MDP M0. The fourth expression in the above equation is exactly the gradient proposed by AggreVaTeD (Sun et al., 2017). AggreVaTeD performs gradient descent with gradient in the format of the fourth expression in Eq. 16 to discourage the log-likelihood of an action at that has low advantage over  at a given state st.

On the other hand, when we set k = , i.e., no truncation on horizon, then we return back to the classic policy gradient on the MDP M obtained from cost shaping with V e. As optimizing M is
the same as optimizing the original MDP M0 (Ng et al., 1999), our formulation is equivalent to a pure RL approach on M0.In the extreme case when the oracle V^ e has nothing to do with the true optimal oracle V , as there is no useful information we can distill from the oracle and RL becomes
the only approach to solve M0.

5 EXPERIMENTS
We evaluated THOR on robotics simulators from OpenAI Gym (Brockman et al., 2016). Throughout this section, we report reward instead of cost, since OpenAI Gym by default uses reward. The baseline we compare against is TRPO-GAE (Schulman et al., 2016) and AggreVaTeD (Sun et al., 2017).
To simulate oracles, we first train TRPO-GAE until convergence to obtain a policy as an expert e. We then collected a batch of trajectories by executing e. Finally, we use TD learning Sutton (1988) to train a value function V^ e that approximates V e. In all our experiments, we ignored e and only used the pre-trained V^ e for reward shaping. Hence our experimental setting simulates the situation where we only have a batch of expert demonstrations available, and not the experts themselves. This is a much harder setting than the interactive setting considered in previous work (Ross et al., 2011; Sun et al., 2017; Chang et al., 2015). Note that e is not guaranteed to be an optimal policy, and V^ e is only trained on the demonstrations from e, therefore the oracle V^ e is just a coarse estimator of VM 0 . Our goal is to show that, compared to AggreVaTeD, THOR with k > 1 results in significantly better performance; compared to TRPO-GAE, THOR with some k << H converges faster and is more sample efficient. For fair comparison to RL approaches, we do not pre-train policy or critic A^ using demonstration data, though initialization using demonstration data is suggested in theory and has been used in practice to boost the performance (Ross et al., 2011; Bahdanau et al., 2016).
For all methods we report statistics (mean and standard deviation) from 25 seeds that are i.i.d generated. For trust region optimization on the actor  and GAE on the critic, we simply use the recommended parameters in the code-base from TRPO-GAE (Schulman et al., 2016). We did not tune any parameters except the truncation length k.

5.1 DISCRETE ACTION CONTROL
We consider two discrete action control tasks with sparse rewards: Mountain-Car, Acrobot and a modified sparse reward version of CartPole. All simulations have sparse reward in the sense that no reward signal is given until the policy succeeds (e.g., Acrobot swings up). In these settings, pure RL approaches that rely on random exploration strategies, suffer from the reward sparsity. On the other hand, THOR can leverage oracle information for more efficient exploration. Results are shown in Fig. 1.

7

Under review as a conference paper at ICLR 2018

Return

TRPO-GAE

100 K=1

K=2

120

K=3 K=10

MountainCar-v0

140

160

180

200

220 0 25 50 75 100 125 150 175 200 Batch Iteration

(a) Mountain Car with H = 200

Return

80 TRPO-GAE

70

K=1 K=3

K=5

60 K=10

K=20

50 K=25

CartPole-v0

40

30

20

10

0

0 25 50 75 100 125 150 175 200 Batch Iteration

(b) SR-CartPole with H = 200

Acrobot-v1 0
100
200

60

TRPO-GAE K=1

80

K=3 K=5

K=10

100 K=50

K=100

120

Acrobot-v1

Return

300 140

TRPO-GAE

160

400

K=1

K=3 K=5

180

500

K=10 K=50

200

K=100

220

0 25 50 75 100 125 150 175 200 Batch Iteration

0 25 50 75 100 125 150 175 200 Batch Iteration

(c) Acrobot with H = 500

(d) Acrobot with H = 200

Return

Figure 1: Reward versus batch iterations of THOR with different k and TRPO-GAE (blue) for Mountain car, Sparse Reward (SR) CartPole, and Acrobot with different horizon. Average rewards across 25 runs are shown in solid lines and averages + std are shown in dotted lines.

Note that in our setting where V^ e is imperfect, THOR with k > 1 works much better than AggreVaTeD (THOR with k = 1) in Acrobot. In Mountain Car, we observe that AggreVaTeD achieves good performance in terms of the mean, but THOR with k > 1 (especially k = 10) results in much higher mean+std, which means that once THOR receives the reward signal, it can leverage this signal to perform better than the oracles.
We also show that THOR with k > 1 (but much smaller than H) can perform better than TRPOGAE. In general, as k increases, we get better performance. We make the acrobot setting even harder by setting H = 200 to even reduce the chance of a random policy to receive reward signals. Compare Fig. 1 (c) to Fig. 1 (b), we can see that THOR with different settings of k always learns faster than TRPO-GAE, and THOR with k = 50 and k = 100 significantly outperform TRPO-GAE in both mean and mean+std. This indicates that THOR can leverage both reward signals (to perform better than AggreVaTeD) and the oracles (to learn faster or even outperform TRPO).
5.2 CONTINUOUS ACTION CONTROL
We tested our approach on simulators with continuous state and actions from MuJoCo simulators: a modified sparse reward InvertedPendulum, Hopper and Swimmer. Note that, compared to the sparse reward setting considered in the previous section, Hopper and Swimmer do not have reward sparsity and policy gradient methods have shown great results (Schulman et al., 2015; 2016). Also, due to the much larger and more complex state space and control space compared to the simulations we consider in the previous section, the value function estimator V^ e is much less accurate in terms of estimating VM 0 since the trajectories demonstrated from experts may only cover a very small part of the state and control space. Fig. 2 shows the results of our approach. For all simulations, we require k to be around 20%  30% of the original planning horizon H to achieve good performance.

8

Under review as a conference paper at ICLR 2018

Return Return

1000 800 600 400 200
0 0

InvertedPendulum-v1

140

120

100

80

TRPO-GAE K=1 K=50 K=100 K=200

60 40 20 0

20 40 60 80 100 20 0 Batch Iteration

(a) SR-InvertedPendulum (H=1000)

Swimmer-v1
TRPO-GAE K=1 K=50 K=200 K=300 K=1000 25 50 75 100 125 150 175 200 Batch Iteration
(b) Swimmer (H=1000)

Return

3500 3000 2500 2000 1500 1000 500
00

Hopper-v1
TRPO-GAE K=1 K=50 K=100 K=150 K=200 K=1000 25 50 75 100 125 150 175 200 Batch Iteration
(c) Hopper (H=1000)

Figure 2: Reward versus batch iterations of THOR with different k and TRPO-GAE (blue) for Sparse Reward (SR) InvertedPendulum, Swimmer and Hopper. Average rewards across 25 runs are shown in solid lines and averages + std are shown in dotted lines.

AggreVaTeD (k = 1) learned very little due to the imperfect value function estimator V^ e. We also tested k = H, where we observe that reward shaping with V^ e gives better performance than TRPOGAE. This empirical observation is consistent with the observation from (Ng et al., 1999) ( Ng et al. (1999) used SARSA (Sutton, 1988), not policy gradient based methods). This indicates that even when V^ e is not close to V , policy gradient methods can still employ the oracle V^ e just via reward shaping.
Finally, we also observed that our approach significantly reduces the variance of the performance of the learned polices (e.g., Swimmer in Fig. 2(a)) in all experiments, including the sparse reward setting. This is because truncation can significantly reduce the variance from the policy gradient estimation when k is small compared to H.

6 CONCLUSION
We propose a novel way of combining IL and RL through the idea of cost shaping with a cost-togo oracle. Our theory indicates that cost shaping with the cost-to-go oracle shortens the learner's planning horizon as a function of the accuracy of the oracle compared to the optimal policy's value function. With this insight, we propose THOR (Truncated HORizon policy search), a gradient based policy search algorithm that explicitly focusing on minimizing the total cost over a finite planning horizon. Our formulation provides a natural half-way point between IL and RL, and experimentally we demonstrate that our approach can outperform RL and IL baselines.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, pp. 1. ACM, 2004.
J Andrew Bagnell and Jeff Schneider. Covariant policy search. IJCAI, 2003.
J Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic programming. In Advances in neural information processing systems, pp. 831­838, 2004.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Kai-wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume, and John Langford. Learning to search better than your teacher. In ICML, 2015.
Sanjiban Choudhury, Ashish Kapoor, Gireeja Ranade, and Debadeepta Dey. Learning to gather information via imitation. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 908­915. IEEE, 2017.
Hal Daume´ III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine learning, 2009.
Sham Kakade. A natural policy gradient. NIPS, 2002.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, 2002.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Volodymyr Mnih et al. Human-level control through deep reinforcement learning. Nature, 2015.
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089, 2017.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999.
Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile off-road autonomous driving using end-to-end deep imitation learning. arXiv preprint arXiv:1709.07174, 2017.
Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.
Ste´phane Ross, Geoffrey J Gordon, and J.Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pp. 1889­1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. ICLR, 2016.
David Silver et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. 2017.
10

Under review as a conference paper at ICLR 2018 Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT
Press Cambridge, 1998. RichardS. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
3:9­44, 1988. Matej Vecer´ik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nico-
las Heess, Thomas Rotho¨rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. David Zipser. Subgrouping reduces complexity and speeds up learning in recurrent networks. In Advances in neural information processing systems, pp. 638­641, 1990.
11

Under review as a conference paper at ICLR 2018
A PROOF OF THEOREM 3.1

Figure 3: The special MDP we constructed for theorem 3.1

To prove theorem 3.1, we construct a special MDP as shown in Fig. 3. The MDP has deterministic

transition, 2H + 2 states, and each state has two actions a1 and a2 as shown in Fig. 3. Every episode

starts at state s0. For state si (states on the top line), we have c(si) = 0 and for state si (states at

the bottom line) we have merged to the final state.

c(si) = 1. At state sH It is clear that for any

and sH , state si,

no we

matter what have Q(si,

actions we take we all get a1) = 0, Q(si, a2) = 1,

Q(si, a1) = 1 and Q(si, a2) = 2, for 1  i  H - 1. Let us assume that we have an oracle Q^e

such that Q^e(si, a1) = 0.5+, Q^e(si, a2) = 0.5-, Q^e(si, a1) = 1.5+ and Q^e(si, a2) = 1.5-,

where  > 0. Hence we can see that Q^e - Q   0.5 + . Namely in our constructed example

we have = 0.5 + .

It is clear that the optimal policy  has cost J() = 0. Now let us compute the cost of the induced
policy from oracle Q^e: ^(s) = arg mina Q^e(s, a). As we can see ^ makes a mistake at every state as arg mina Q^e(s, a) = arg mina Q(s, a). Hence we have J(^) = H  2H , as   0+ (by   0+ we mean  approaches to zero from the right side).

12

