Under review as a conference paper at ICLR 2018
FAST AND ACCURATE TEXT CLASSIFICATION: SKIMMING, REREADING AND EARLY STOPPING
Anonymous authors Paper under double-blind review
ABSTRACT
Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches.
1 INTRODUCTION
Recurrent neural nets (RNNs), including GRU nets (Chung et al., 2014) and LSTM nets (Hochreiter & Schmidhuber, 1997), have been increasingly applied to many problems in natural language processing. Most of the problems can be divided into two categories: sequence to sequence (seq2seq) tasks (Sutskever et al., 2014) (e.g., language modeling (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner & Blunsom, 2013), conversational/dialogue modeling (Serban et al., 2016), question answering (Hermann et al., 2015; Weston et al., 2015; Lee et al., 2016), and document summarization (Rush et al., 2015; Nallapati et al., 2016)); and the sequence to scalar (seq2scalar) tasks (e.g., part-of-speech tagging (Santos & Zadrozny, 2014), chunking, named entity recognition (Collobert et al., 2011), sentimental analysis (Socher et al., 2011), and document classification (Kim, 2014; Sebastiani, 2002)). To solve these problems, our models often need to read every token or word of the text from beginning to the end, which is necessary for most seq2seq problems. However, for seq2scalar problems, we do not have to treat each individual word equally, since certain words or chunks are more relevant to the classification task at hand. For instance, for sentiment analysis it is sufficient to read the first half of a review like "this movie is amazing" or "it is the best I have ever seen," to provide an answer even without reading the rest of the review. In other cases, we may want to skip or skim some text without carefully checking it. For example, sentences such as "it's worth to try" are usually more important than irrelevant text such as "we got here while it's still raining outside" or "I visited on Saturday." On the other hand, sometimes, we want to re-read some sentences to figure out the actual hidden message of the text. All of these techniques enable us to achieve fast and accurate reading. Similarly, we expect RNN models to intelligently determine the importance or the relevance of the current sentence in order to decide whether to make a prediction, whether to skip some texts, or whether to re-read the current sentence.
In this paper, we aim to augment existing RNN models by introducing efficient partial reading for classification, while maintaining a higher or comparable accuracy compared to reading the full text. To do so, we introduce a recurrent agent which uses an RNN module to encode information from the past and the current tokens, and applies a policy module to decide what token to read next
1

Under review as a conference paper at ICLR 2018

LSTM

ht Yes s
Stop?
c
Classifier

No
n
action

Policy Module

label probability

01

2 ...K

Reread

Read Sequentially

Skip 1 Token

Skip K-1 Tokens
...

Token 1 Encoder
LSTM

Output Label

Next Move

Token 4

Skip 2 Tokens

Encoder
LSTM

Reread

Figure 1: Outline of the proposed model.

Token 5
Encoder Read Sequentially
LSTM
Stop
Output Label

(e.g., rereading the current token, reading the next one, or skipping the next few tokens) or whether the model should stop reading and form a decision. To encourage fast and accurate reading, we incorporate both classification accuracy and the computational cost as a reward function to score classification or other actions made by the agent during reading. We expect that our agent will be able to achieve fast reading for classification with both high computational efficiency and good classification performance. To train this model, we develop an end-to-end approach based on the policy gradient method which backpropagates the reward signal into both the policy module (also including the classification policy) and the recurrent encoder.
We evaluate our approach on four different sentiment analysis and document topic classification datasets. By comparing to the standard RNN models and a recent LSTM-skip model which implements a skip action (Yu et al., 2017), we find that our approach achieves both higher efficiency and better accuracy.
2 METHODS
2.1 MODEL OVERVIEW
Given an input sequence x1:T with length T , our model aims to predict a single label y for the entire sequence, such as the topic or the sentiment of a document. We develop a technique for skimming, re-reading, early stopping and prediction, with the goal of (i) skipping irrelevant information and reinforcing the important parts, and (ii) to enable fast and accurate text classification. Specifically, the model will read the current token/chunk xit at time step t, encode the data xit and previous information ht-1 into a feature ht, and then decide the next token to read by skimming/skipping or to stop to form a final prediction (see Figure 1). Such a model can be fully defined on top of a RNN structure and trained in an end-to-end fashion via back-propagation of a well defined reward signal. Both skimming and re-reading actions can be defined similarly by first choosing a step size k  {0, 1, 2, · · · , K} and then setting it+1 = it + k. When k = 0, the model rereads the current token; when k = 1, the model moves to the next token sequentially; when k > 1, the model skips the next k - 1 tokens. If the current action is to stop or the next token to read is after the last token of the input sequence text, the model will stop and output a label. All of these actions are defined by a policy module  which takes the recurrent feature ht as input and outputs a stop signal and a label or generates a step size k and moves to the next token xit+1=it+k.
2

Under review as a conference paper at ICLR 2018

2.2 MODEL SPECIFICATION AND TRAINING

The design of the policy module  plays an critical role in our framework. It should (i) read as much
significant text as possible to ensure a confident classification output and (ii) be computationally
efficient, e.g., avoiding reading to the end of the text if sufficient information is already obtained
and skipping irrelevant or unimportant part of the text. More specifically, for each step, the policy
module  should decide whether the information collected is convincing enough to stop reading and
make a prediction. Otherwise it will need to evaluate the importance of the current semantic unit
or token just read to decide which token to be read in the next step. By formulating this process
as a sequential decision process, at each time step t, the policy module takes the output ht of an encoder, which summarizes the text read before and the current token xit , and outputs a probability distribution t defined over actions. At each time step t, a sequence of actions are generated by first sampling a stopping decision in the form of a binary variable s from a Bernoulli distribution S(·|ht). If s = 1, the model stops and draws a label y^ from a conditional multinomial distribution specified by a classifier C(·|ht, s = 1); otherwise, the model draws a step size k  {0, . . . , K} from another conditional multinomial distribution N (·|ht, s = 0) to jump to the token xit+1=it+k. Thus the probability of a sequence of actions that reads text Xi1:it = {xi1 , xi2 , ..., xit }, stops at time t, and outputs a label y^ can be written as the joint distribution

t-1
(Xi1:it , y^) = S(s = 1|ht)C (y^|ht, s = 1) S(s = 0|hj)N (kj = ij+1 - ij|hj, s = 0),
j=1

or simply as

t-1
(Xi1:it , y^) = S(1|ht)C (y^|ht, 1) S(0|hj)N (kj|hj, 0).
j=1

(1)

Hereby, kj = ij+1 - ij is the step size sampled at time j which ensures the model moves from token xij to xij+1 , and hj = RN N (xij , hj-1) is computed by the RNN module.

To encourage fast and accurate text reading, we want to minimize the difference between true label
and predicted while ensuring a low computational cost, which is measured by the length of the assessed text. Hence, as the reward for the last output action, we use -L (y^, y), where L is a loss function that measures the accuracy between predicted label y^ and true label y. For other actions we use a negative computational cost -F. Hereby, F is the normalized FLOP count used at each time step which is approximately constant. Note that the FLOP count for the last step, Ft, differs, since it also includes the cost of the classification. Overall, the reward signal is defined as:

rj =

-L (y^, y) - Ft -F

if j = t is the final time step , otherwise

where  is a trade-off parameter between accuracy and efficiency.

Assume that the entire policy  is parameterized by  = {S , C , N , RNN }, where RNN subsumes the parameters for the encoder. Our final goal is to find the optimal  which maximize the
expected return defined by:


t

J () = E(x,y)D  E(Xi1:it ,y^)

j-1rj  ,

t j=1

(2)

where the first summation is used for integrating all possible sequences with different lengths to ensure the normalization of the distribution , and   (0, 1) is a discount factor. It is not hard to see that J is infeasible to compute by enumerating all possibilities in the summation and expectation. Fortunately, we can apply the policy gradient algorithm (Williams, 1992) to optimize this objective by estimating the gradient using Monte Carlo rollout samples, without doing expensive integration or enumeration. The REINFORCE policy gradient of the objective on data (x, y) can be derived as follows:

t-1 t
J = [log S(1|ht) + log C (y^|ht, 1) + (log S(0|hj) + log N (kj|hj, 0))] j-1rj.

j=1

j=1

3

Under review as a conference paper at ICLR 2018
Considering that the length of the rollout sequence can differ significantly, the space for policy exploration is very large, thus making the variance of the gradient estimation very high. To remedy this, we also implement the advantage actor-critic algorithm (Konda & Tsitsiklis, 2000), which couples partial future return with each action and estimates a value function as the baseline for variance reduction. We find this procedure to provide better performance than the vanilla REINFORCE algorithm.
It is worth noting that this policy gradient method eventually is able to backpropagate both classification accuracy and computational cost signals to every module in our model, including the stopping/skipping distributions, the label distribution and even the recurrent encoder, thus providing an end-to-end solution to text classification problems.
Overall, our model aims to accelerate text classification while still achieving a high accuracy. The hyperparameter  is used to control the trade-off between accuracy and time cost. If we set  to be a relatively large value, our model will be more boldly to skip tokens, stop reading and output a label. If  is small, our model would like to (re)read more tokens. Actually, the reward for penalizing the computational cost can be seen as a Lagrangian multiplier which is used to constrain the average cost of the computation. Therefore, there is a mapping between  and the amortized computational budget allocated for each sample. Given a budget, we can tune  to provide a model with best classification accuracy with the amortized cost within the budget. This is desirable for many cost-sensitive applications, such as those on mobile devices.
3 EXPERIMENTS
In this section, we illustrate our approach using two representative seq2scalar tasks: sentiment analysis and topic classification. To perform a solid demonstration on re-reading and skimming, we conduct experiments on three different syntactic levels. We will first introduce the results on the word level before discussing character and sentence level performance.
General Experimental Settings: In our experiments, we use the IMDB and Yelp dataset for sentiment analysis, and the AG news and DBpedia for topic classification. To evaluate each classifier, we use accuracy as the performance metric and average per-data floating point operations (FLOPs) as the computational cost metric. It is worthy to note that the policy networks have very low FLOPs compared with the classifier.
Hyper-parameters: We use the Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.001 in all experiments. For the recurrent network structure, we use a convolution layer with 128 kernels of size 5 and stack it as input to an LSTM with a hidden size of 128. For S and N policy network, we use three-hidden-layer MLPs with 128 hidden units per layer. For C and value network, we use single layer MLPs with 128 hidden units. K is set to 3.
3.1 RESULTS
We first evaluate our method on the IMDB movie dataset (Maas et al., 2011). We randomly split it into 20,000 training, 5,000 validation and 25,000 test samples. The average length in the dataset is 240 words. We use a chunk-size of 20 words, i.e., at each step, the classifier reads 20 words. When the action is rereading or skipping, it rereads or skips a chunk of 20 words.
To demonstrate the effectiveness of re-reading and skimming, we design three baseline models: (1) the early stopping model, which has only a stopping module to decide when to terminate reading the paragraph; (2) the partial reading model, which is a same classifier training on the truncated sentences decided by the stopping module. Thus, the partial reading model has the same computational budget as the early stopping model; (3) the whole reading model, which tries to use the whole corpus as training data.
Figure 2 shows our comparison on the IMDB dataset, where the blue line indicates our proposed model while green and red one denote early-stopping and partial reading, respectively. The x-axis denotes the FLOP count (in millions) and the y-axis indicates the accuracy. From this plot, we observe that both blue line and green line outperform the red line significantly. Furthermore, rereading
4

Under review as a conference paper at ICLR 2018

90

85

Accuracy

80

75

70 0

25 50

75 100
FLOPs(M)

Partial Reading Early Stopping Ours
125 150 175

Figure 2: Comparison of partial reading, early stopping and our model on the IMDB dataset. Curves are obtained by changing the computational budget for each method. For Ours and Early Stopping, we adjust the parameter .

Accuracy Accuracy

90
88
86
84
82
80
78
Partial Reading 76 Ours
10 20 30 40 50
FLOPs(M)
(a) AG news comparison curve

97
96
95
94
93
Partial Reading Ours
92 20 25 30 35 40 45 50
FLOPs(M)
(b) DBpedia comparison curve

Figure 3: Comparisons on character-level topic classification on two datasets: The x-axis and y-axis are representing FLOPs and accuracy, respectively. The red line denotes the partial reading baseline. The blue line indicates our proposed model.

and skipping further improve the performance of the model with only the early stopping mechanism. This observation implies that training the classifier jointly with the policy model improves both computational efficiency and accuracy.
Besides the word-level evaluation, we also conduct experiments on a smaller scale syntactic unit: character-level. In detail, we perform topic classification on two large-scale text datasets: the AG news and the DBpedia dataset (Zhang et al., 2015) contains four topics, 120,000 training news, 10,000 validation news, 7,600 testing news. The DBpedia dataset contains 14 topics, 560,000 training entities, 10,000 validation entities and 70,000 testing entities. The results are summarized in Figure 3. We observe that our proposed model outperforms the partial reading baseline by a significant margin.
5

Under review as a conference paper at ICLR 2018

Accuracy

96
94
92
90
88
Partial Reading Ours
86 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5
FLOPs(M)
Figure 4: Comparison of the partial reading model and our model on the binary Yelp challenge dataset.

Syntactic level Word
Character
Sentence

Dataset IMDB AG news DBpedia Yelp

Speedup 4.11x 1.85x 2.42x 1.58x

Accuracy 88.32% 88.50% 95.99% 94.95%

Relative PR Accuracy -7.19% -4.42% -1.94% -3.38%

Table 1: Summary of our results: We compare our model to a complete reading baseline and a partial reading (PR) baseline on four datasets and three different syntactic levels. Here we report the speedups of our model compared to whole-reading baseline at the same accuracy level. Also we report the relative performance of the partial reading baseline with the same computational cost as our model.

Furthermore, we evaluate our proposed model on a larger syntactic level: sentence level. We use Yelp review sentiment analysis dataset for this experiment. The Yelp dataset includes 500,000 training reviews, 20,000 validation reviews and 40,000 testing reviews. To evaluate on the larger semantic unit, we treat each sentence as a token, which will be read sequentially by the RNN encoder. The performance is provided in Figure 4. We observe that our proposed model achieves superior performance while being significantly faster.
We summarize the obtained performance improvements in Table 1. On four different datasets and for three different syntactic levels we observe significant speedups when using the proposed techniques for skimming, rereading and early stopping, while maintaining the accuracy. A partial reading model which has the same computational cost achieves results that are less accurate, which illustrates the benefits of a flexible model.
Finally, we compare our model to a recently published baseline (Yu et al., 2017), which only implements the skipping actions with k  {1, 2, ..., K} but without early stopping or rereading. Results in Table 2 show that our model is much more efficient than their LSTM-skip model at the same-level of accuracy. These results demonstrated that our proposed rereading and skimming mechanisms are effective on a variety of seq2scalar tasks including sentiment analysis and topic classification. Also it is effective on different level of semantics: character-level, word-level or even sentence-level. With the help of those two mechanisms, we could achieve both higher accuracy and faster speed.
6

Under review as a conference paper at ICLR 2018

Syntactic-level Dataset Accuracy FLOPs (Yu et al., 2017) FLOPS (Ours)

Word

IMDB 88.82%

57.80%

29.33%

Character AG News 88.6%

87.68%

57.55%

Table 2: Summary of our results: We compare our model to (Yu et al., 2017), showing the relative FLOPs necessary to achieve the same accuracy on two datasets used in both theirs and this paper.
STOP
What a great dinner! I had the pasta trio special. They walk around with three pastas and just re-fill your plate. I even wanted one of them without an ingredient , and they did not bat an eye. They even had the pasta prepared the same way to refill me! I haven't been back, and I kick myself for it on my way out of town every time I visit there!
Figure 5: Sentiment analysis example: The color scheme shows the spectrum from positiveness (green) to negativeness (red) output by the LSTM model. Our model is able to read the first two sentences and get the correct prediction (positive), while the full-reading LSTM model gets confused and outputs the wrong label (negative).

3.2 CASE STUDIES
To obtain a more detailed understanding of our model, we first show the actions taken by our model on a sentiment analysis example (Figure 5), on which the LSTM full-reading model failed to give the right classification. We show the degree of positiveness given by LSTM model encoded in color, from green representing positiveness to brown representing negativeness.
The paragraph starts with a sentence with strong positiveness of a dinner, then followed by a few sentence on some confusing description on the dinner. Many trivial or even negative words show up in the explanation. As a result, the output of the full-reading model gradually changes from positive to negative and finally results in a negative signal. Importantly, after our model reads the first two sentences, the policy module decides that it is confident enough to make a decision yielding the correct answer.
Next we illustrate how the rereading and skimming actions are useful to identify important information in the text. As shown in Figure 6, our model first reads a key word "stake" and is confident that the document is about money. Then it skims a few irrelevant tokens to read about "buying stake in Biotechnology" the following two tokens. The phrase "5 percent stake" showed up twice. Our model consider it to be important so it re-reads this token. At this time, the model basically knows this text is about business with a reasonable confidence. Then it skips to read about "collaboration deal" and stops to make a confident prediction.
4 RELATED WORK
The idea of improving time efficiency with adaptive computation has been studied extensively recently and throughout the years (e.g., Schwing et al., 2011; Weiss & Taskar, 2013). For example, the adaptive computation time algorithm (Graves, 2016) on recurrent neural networks is similar to our work. However their model only implements the early stopping action, which can be easily trained by augmenting the loss with computational cost metrics. Spatially adaptive computation time (Figurnov et al., 2016) was proposed for image classification and object detection tasks. Different from our approach, the actions defined in their models are very simple. In contrast, in our model, the combinatorial complexity of actions make it more powerful but much harder to train. We adopt the efficient policy gradient methods and provide an efficient algorithm for end-to-end training.
The idea of learning visual attention with neural networks (Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014), which combine RNN or LSTM models with Reinforcement learning is closely related to our work. However, their decision making process is continues and can be sampled from a continuous distribution, while our model's actions are discrete, so it is more difficult to learn the policy when the sequences are long.

7

Under review as a conference paper at ICLR 2018

Pfizer buys 5 percent stake in Medarex BOSTON (CBS.MW) - Pharmaceutical powerhouse Pfizer is buying a 5 percent stake in biotechnology researcher Medarex under their newly signed collaboration deal, according to Medarex Chief Executive Donald Drakeman.

Pfizer buys 5 percent stake

Skip
Powerhouse Pfizer is buying a
Scan

5 percent stake in biotechnology Reread 5 percent stake in biotechnology

Skip
signed collaboration deal, according to

Stop Output

Figure 6: Topic classification example: Our model skips irrelevant chunks and reread an important phrase with "stake." It also early stops after reading "collaboration deal" and outputs the right classification (Business), but the full-reading model is fooled to output a wrong label (Technology).

Attention mechanism applied to text are related. ReasoNet (Shen et al., 2017) trains a policy network with two actions, continue and stop, which is similar to our proposed model. However their work is mainly evaluated on the question answering task and they use early stopping to increase accuracy. Our model's target is to minimize calculate cost while maintaining the same accuracy, so as to realize fast processing. Choi et al. (2016) focus on an approach to improve efficiency of the question answering task. They pursue a hierarchical approach for joint training. Rationalizing Neural Predictions (Lei et al., 2016) also uses attention mechanisms to select some fragments from a given document. But those fragments are used to facilitate text understanding rather than speeding up document processing. Hahn & Keller (2016) use a similar action space. In contrast to our work they use unsupervised training and mainly focus on action re-read, since their main target is to improve the performance of seq2seq tasks like language modelling. They skim words in order to not confuse the model rather than materializing computational benefits.
Other ways to reduce the evaluation time for new examples have been considered. Bengio et al. (2015) propose a scheme to selectively activate parts of the network. Bolukbasi et al. (2017) present two schemes to adaptively utilize the network during evaluation: The first is to choose components of a deep network, the second is a network selection system that adaptively selects the network to be evaluated for each example. But both of these mostly focus on model efficiency not on data efficiency.
Closely related is work by Yu et al. (2017). The authors train their policy network end-to-end with reinforcement learning. In contrast to their work which only implemented the skipping mechanism, our model has two other human-like mechanisms, rereading and early-stopping, thus leading to further improved efficiency and accuracy. Furthermore, we hardly rely on many hyper-parameters and only use a simple reward structure consisting of classification accuracy and computational cost. Finally, we illustrate the advantage of the actor-critic formulation (Konda & Tsitsiklis, 2000) to reduce the variance and improve performance.
5 CONCLUSIONS
We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. By mimicking human fast reading, we introduce a policy module to decide what token to read next (e.g., rereading the current token, reading the next one, or skipping the next few tokens) or whether the model should stop reading and form a decision. To encourage fast and accurate reading, we incorporate both classification accuracy and the computational cost as a reward function to score classification or other actions made by the agent during reading. An endto-end training algorithm based on the policy gradient method backpropagates the reward signal into both the policy module (also including the classification policy) and the recurrent encoder. We demonstrate the efficacy of the proposed approach on four different datasets and demonstrate improvements for both accuracy and computational performance.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention. arXiv preprint arXiv:1412.7755, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for efficient inference. In International Conference on Machine Learning, pp. 527­536, 2017.
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Hierarchical question answering for long documents. arXiv preprint arXiv:1611.01839, 2016.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493­2537, 2011.
Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. arXiv preprint arXiv:1612.02297, 2016.
Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.
Michael Hahn and Frank Keller. Modeling human reading with neural attention. arXiv preprint arXiv:1608.05604, 2016.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pp. 1693­1701, 2015.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, volume 3, pp. 413, 2013.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pp. 1008­1014, 2000.
Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Berant. Learning recurrent span representations for extractive question answering. arXiv preprint arXiv:1611.01436, 2016.
9

Under review as a conference paper at ICLR 2018
Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. arXiv preprint arXiv:1606.04155, 2016.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142­150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.
Tomas Mikolov, Martin Karafia´t, Lukas Burget, Jan Cernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances in neural information processing systems, pp. 2204­2212, 2014.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.
Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685, 2015.
Cicero D Santos and Bianca Zadrozny. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1818­1826, 2014.
A. G. Schwing, C. Zach, Y. Zheng, and M. Pollefeys. Adaptive Random Forest ­ How many "experts" to ask before making a decision? In Proc. CVPR, 2011.
Fabrizio Sebastiani. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1­47, 2002.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In AAAI, pp. 3776­3784, 2016.
Pierre Sermanet, Andrea Frome, and Esteban Real. Attention for fine-grained categorization. arXiv preprint arXiv:1412.7054, 2014.
Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1047­1055. ACM, 2017.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the conference on empirical methods in natural language processing, pp. 151­161. Association for Computational Linguistics, 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.
D. Weiss and B. Taskar. Learning Adaptive Value of Information for Structured Prediction. In Proc. NIPS, 2013.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merrie¨nboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Adams Wei Yu, Hongrae Lee, and Quoc V Le. Learning to skim text. arXiv preprint arXiv:1704.06877, 2017.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pp. 649­657, 2015.
10

