Under review as a conference paper at ICLR 2018
GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL
Anonymous authors Paper under double-blind review
ABSTRACT
Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.
1 INTRODUCTION
The goal of reinforcement learning is to learn an optimal policy that lets an agent achieves the maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning has been shown to be effective in solving challenging artificial intelligence tasks such as playing games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine et al., 2016).
Reinforcement learning methods can be categorized into three categories: value-base, policy-based, and actor-critic methods. Value-based methods learn an optimal policy by firstly learning a value function that estimates the expected return. Then, they infer an optimal policy by choosing actions that maximize the learned value function. Choosing actions in this way requires solving a maximization problem which is not trivial for continuous controls. While extensions to continuous controls were considered recently, they are restrictive since specific structures of the value function are assumed (Gu et al., 2016; Amos et al., 2017).
On the other hand, policy-based methods, also called policy search methods (Deisenroth et al., 2013), learn a parameterized policy maximizing a sample approximation of the expected return without learning the value function. For instance, policy gradient methods such as REINFORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability of observing high sample returns increases. Compared with value-based methods, policy search methods are simpler and naturally applicable to continuous problems. Moreover, the sample return is an unbiased estimate of the expected return and methods such as policy gradients are guaranteed to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999). However, sample returns usually have high variances and this makes such policy search methods converge too slowly.
Actor-critic methods combine the advantages of value-based and policy search methods. In these methods, the parameterized policy is called an actor and the learned value-function is called a critic. The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low variance estimate of the expected return, these methods often converge much faster than policy search methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsitsiklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schulman et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their approaches to learn the actor are different, they share a common property that they only use the value
1

Under review as a conference paper at ICLR 2018

of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not utilize the second-order information of the critic.
In this paper, we show that the second-order information of the critic is useful for learning the actor. To this end, we propose a novel actor-critic method for continuous controls which we call guide actor-critic (GAC). The main idea of our method is to firstly learn a non-parameterized Gaussian policy that locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. This Gaussian policy is obtained in a closed form by using Taylor's approximations of the critic. Then, the policy is used as a guide for learning a parameterized policy by supervised learning. Our analysis shows that learning the mean of the Gaussian policy is equivalent to performing a second-order optimization step in the action space where the curvature matrix is given by the KL constraint and Hessians of the critic. Furthermore, we establish a connection between GAC and DPG where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

We consider discrete-time Markov decision processes (MDPs) with continuous state space S  Rds and continuous action space A  Rda . We denote the state and action at time step t P N by st

and at, respectively. The initial state s1 is determined by the initial state density s1 ,, p1psq.

At time step t, the agent in state st takes an action at according to a policy at ,, pa|stq and

obtains a reward rt " rpst, atq. Then, the next state st`1 is determined by the transition function

st`1 ,, pps1|st, atq. A trajectory  " ps1, a1, r1, s2, . . . q gives us cumulative rewards or returns

defined

as

8
t"1



t´1

rpst

,

atq

where

the

discount

factor

0







1

assigns

different

weights

to

rewards received at different time steps. The expected returns of  can be expressed through the

action-value function which is defined as

«8 ff Qps, aq " Epat|stqt2,ppst`1|st,atqt1 ÿ t´1rpst, atq|s1 " s, a1 " a ,
t"1

(1)

where Ep r¨s denotes the expectation over the density p and the subscript t  1 indicates that the expectation is taken over the densities at time steps t  1. We can define the expected returns as

«8 ff J pq " Epps1q,pat|stqt1,ppst`1|st,atqt1 ÿ t´1rpst, atq " Eppsq,pa|sq rQps, aqs ,
t"1

(2)

where ppsq is the stationary state density which satisfies pps1q "  ppsqpa|sqpps1|s, aqdsda.

The goal of reinforcement learning is to find an optimal policy that maximizes the expected returns.

The policy search approach (Deisenroth et al., 2013) parameterizes  by a parameter  and finds < which maximizes the expected returns:

< " argmax Ep psq,pa|sq rQ ps, aqs .


(3)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by

 Ð  ` Ep psq,pa|sq r log pa|sqQ ps, aqs ,

(4)

where   0. In policy search, the action-value function is commonly estimated by the sample

returns:

Q ps, aq

«

1 N

N
n,t"1



t´1

rpst,n

,

at,n

q

obtained by

collecting

N

trajectories

using

 .

However, sample returns often yield high variance estimates which lead to slow convergence.

An alternative approach is to estimate the action-value function by a critic denoted by Qpps, aq whose parameter is learned such that Qpps, aq « Q ps, aq. The policy gradient theorem (Sutton et al., 1999) shows that the critic can be used instead of the sample returns. This theorem leads to the actor-critic method (Sutton et al., 1999) which updates the policy parameter by

"i  Ð  ` Ep psq,pa|sq  log pa|sqQpps, aq .

(5)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

2

Under review as a conference paper at ICLR 2018

The gradient in Eq.(5) often has less variance than that of Eq.(4) which leads to faster convergence2. A large class of existing actor-critic methods are based on this method (Peters & Schaal, 2008; Mnih et al., 2016). As shown in these papers, these methods only use values of the critic for learning the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that uses the first-order information of the critic. DPG updates a deterministic policy psq by

"i  Ð  ` Ep psq psqaQpps, aq|a"psq .

(6)

A method related to DPG is the stochastic value gradients method (Heess et al., 2015) that is able to learn a stochastic policy but it requires learning a model of the transition function.

As shown above, existing actor-critic methods only use up to the first-order information of the critic when learning the actor and ignore higher-order ones. In the next section, we describe our proposed method that also uses the second-order information of the critic when learning the actor.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method. Unlike existing methods that directly learn the parameterized actor from the critic, GAC firstly learns a guide actor that locally maximizes the critic by using the second-order Taylor's approximation of the critic. This step is described in Sections 3.1 to 3.4. Then, we use supervised learning to learn the parameterized actor. This step is described in Section 3.5. The pseudo-code is provided in Appendix B.

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our first goal is to learn a guide actor that maximizes the critic. However, greedy maximization should be avoided since the critic is a noisy estimate of the expected return and a greedy actor may change too abruptly across learning iterations. Such a behavior is undesirable in real-world problems, especially in robotics (Deisenroth et al., 2013). Instead, we maximize the critic with additional constraints:

"i

max
~

Eppsq,~pa|sq Qpps, aq ,

subject to Eppsq rKLp~pa|sq||pa|sqqs  ,

Eppsq rHp~pa|sqqs  ,

Eppsqrpa|sq " 1,

(7)

where ~pa|sq is the guide actor to be learned, pa|sq is the current actor that we want to improve upon. The state distribution ppsq is induced by a behavioral policy pa|sq which we define as a mixture of all past actors. The objective function differs from the ones in Eq.(3) in two important aspects. First, we maximize for a policy function ~ and not for the policy parameter. This is more advantageous than optimizing for a policy parameter since the policy function can be obtained in a closed form, as will be shown in the next subsection. Second, the expectation is defined over a state distribution of the behavioral policy and this gives us off-policy methods with better data efficiency. The first constraint is the Kullback-Leibler (KL) divergence constraint where KLpppxq||qpxqq " Eppxq rlog ppxq ´ log qpxqs. The second constraint is the Shannon entropy constraint where Hpppxqq " ´Eppxq rlog ppxqs. The KL constraint is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Abdolmaleki et al., 2015; Schulman et al., 2015a). The entropy constraint is crucial for maintaining stochastic behavior and preventing premature convergence (Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017). The final constraint ensures that the guide actor is a proper probability distribution. The KL bound  0 and the entropy bound  are hyper-parameters which control exploration-exploitation trade-off of the method. In practice, we fix the value of and adaptively reduce the value of  based on the current actor's entropy, as suggested by Abdolmaleki et al. (2015). More details of these tuning parameters are given in Appendix C.

2This gradient is biased. However, it is unbiased under some regularity conditions (Sutton et al., 1999).

3

Under review as a conference paper at ICLR 2018

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

~¸

<
~pa|sq 9 pa|sq <`< exp

Qpps, aq < ` <

,

(8)

where <  0 and <  0 are dual variables corresponding to the KL and entropy constraints,

respectively. The dual variable corresponding to the probability distribution constraint is contained in the normalization term and is determined by < and <. These dual variables are obtained by

minimizing the dual function:

« ~ ¸ff

 
gp, q "  ´  ` p ` qEppsq log pa|sq ` exp

Qpps, aq `

da .

(9)

All derivations and proofs are given in Appendix A. The solution in Eq.(8) tells us that the guide ac-
tor is obtained by weighting the current actor with Qpps, aq. If we set Ñ 0 then we have ~ «  and the actor is not updated. On the other hand, if we set Ñ 8 then we have ~pa|sq9 exppQpps, aq{<q which is a softmax distribution where < is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ~pa|sq and evaluating gp, q are intractable for an arbitrary pa|sq. We overcome this issue by employing two assumptions. First, we assume that the actor is the Gaussian distribution:

pa|sq " N pa|psq, psqq,

(10)

where the mean psq and covariance psq are functions parameterized by a policy parameter

. Second, we assume that Taylor's approximation of Qpps, aq is locally accurate up to the second-

order. More concretely, the second-order Taylor's approximation using an arbitrary action a0 is

given by

Qpps,

aq

«

Qpps, a0q

`

pa

´

a0qJg0psq

`

1 2 pa

´

a0 qJ H 0 psqpa

´

a0q

`

Op}a}3q,

(11)

where g0psq " aQpps, aq|a"a0 and H0psq " a2 Qpps, aq|a"a0 are the gradient and Hessian of the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is

sufficiently small, we can rewrite Taylor's approximation at a0 as

Qp0ps, aq

"

1 2

aJH

0

psqa

`

aJ0

psq

`

0psq,

(12)

where

0psq

"

g0psq

´

H 0 psqa0

and

0psq

"

1 2

a0JH

0psqa0

´

aJ0 g0psq

`

Qpps, a0q.

Note

that

H0psq, 0psq, and 0psq depend on the value of a0 and do not depend on the value of a. This

dependency is explicitly denoted by the subscript.

Substituting the Gaussian distribution and Taylor's approximation into Eq.(8) yields another Gaus-

sian distribution ~pa|sq " N pa|`psq, `psqq where the mean and covariance are given by

`psq " F ´1psqLpsq, `psq " p< ` <qF ´1psq.

(13)

The matrix F psq P Rda^da and vector Lpsq P Rda are defined as

F psq " <´1psq ´ H0psq, Lpsq " <´ 1psqpsq ` 0psq. The dual variables < and < are obtained by minimizing the following dual function:

(14)

gpp, q " 

´



`

p

`

qEp psq

« log

d

|2p

`

qF

´ 1psq|


|2psq| `

ff

`

1 2 Ep psq

"L psqJ F

´ 1psqLpsq

´

psqJ´ 1psqpsq

`

const,

(15)

where F psq and Lpsq are defined similarly to F psq and Lpsq but with  instead of <. The term

0psq is contained in the normalization term of the guide actor and in the constant term.

The practical advantage of using the Gaussian distribution and Taylor's approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrixvector products. The expectation over ppsq can be approximated by e.g., samples drawn from a replay buffer (Mnih et al., 2015). We require inverting F psq to evaluate the dual function. However, these matrices are computationally cheap to inverse when the dimension of actions is small.

4

Under review as a conference paper at ICLR 2018

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimate of the true action-value function. In this
case, the quality of a guide actor mostly depends on the accuracy of Taylor's approximation. Since
Taylor's approximation is accurate locally, the action a0 should be in a local vicinity of a. However, we did not directly use any individual a to compute the guide actor, but we weight pa|sq by the critic (see Eq.(8)). Thus, the action a0 should be similar to actions sampled from pa|sq. Based on this observation, we propose two approaches for obtaining accurate Taylor's approximation.

Taylor's approximation around the mean. In this approach, we perform Taylor's approximation

using the mean of pa|sq. More specifically, we use a0 " Epa|sq ras " psq for Eq.(12). In this case, we can show that the mean update in Eq.(13) corresponds to performing a second-order

optimization step in the action space to maximize Qpps, aq:

`psq

"

 psq

`

F

´1 

psqa

Qpps,

aq|a" psq ,

(16)

where F  psq " <´1psq ´ H psq and H psq " 2aQpps, aq|a"psq. This equivalence can be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that

the guide actor maximizes the critic by taking a step in the action space similarly to the Newton

method (Nocedal & Wright, 2006). However, the main difference lies in the curvature matrix where

the Newton method uses Hessians H psq but GAC uses a damped Hessian F  psq. The damping term <´ 1psq corresponds to the effect of the KL upper-bound and can be viewed as a trustregion that controls the update step-size. This damping term is particularly important since Taylor's

approximation is accurate locally and we should not take a large step in each update.

Expectation of Taylor's approximations. Instead of using a0 " psq, We may perform Taylor's approximation at samples a0 ,, pa|sq and compute its expectation. More concretely, we define

Qrps, aq to be an expectation of Qp0ps, aq over pa0|sq:

Qrps, aq

"

1 2

aJE

pa0 |sq

rH

0psqs

a

`

aJ E pa0 |sq

r0psqs

`

E

pa0 |sq

r0psqs

.

(17)

Note that Epa0|sqrH0psqs " Epa0|sqr2aQpps, aq|a"a0 s and the expectation is computed w.r.t. the distribution  of a0. We use this notation to avoid confusion even though pa0|sq and pa|sq are the same distribution. When Eq.(17) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Epa0|sqrH0psqa0s " Epa0|sqrH0psqsEpa0|sqra0s, we can show that the mean update corresponds to the following second-order optimization step:

`psq

"

 psq

`

E pa0|sq

rF

0psqs´1

E pa|sq

" aQpps,

i aq

,

(18)

where Epa0|sq rF 0psqs " <´1psq ´ Epa0|sq rH0psqs. Interestingly, the mean is updated using averaged gradients and Hessians. We believe that this can be advantageous for avoiding local

optima. Note that Qrps, aq can be used even when the independence assumption does not hold.

In the remainder, we use F psq to denote both of F  psq and Epa0|sq rF 0psqs, and use Hpsq to
denote both of H psq and Epa0|sqrH0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor's approximation by a0 " psq, and we use GAC-1 to refer to GAC with Taylor's approximation by a0 ,, pa|sq where we use a single sample for computational efficiency.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(13) indicates that F psq " <´ 1psq ´ Hpsq needs to be positive definite. Since <´1psq is positive definite by construction, we need Hpsq to be negative semidefinite. However, this is not guaranteed in general and Hpsq can be indefinite. To overcome this issue, we propose to use the following Hessian approximation:

Hpsq " ´aQpps, aqaQpps, aqJ ` 2a exppQpps, aqq expp´Qpps, aqq.

(19)

The proof is given in Appendix A.3. The first term is always negative semi-definite while the second

term is indefinite. Therefore, a negative semi-definite approximation of Hessians can be obtained as

"i

H0psq « ´ aQpps, aqaQpps, aqJ

,

a"a0

which is closely related to the Gauss-Newton approximation (Nocedal & Wright, 2006).

(20)

5

Under review as a conference paper at ICLR 2018

3.5 LEARNING PARAMETERIZED ACTOR
The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below, we discuss two supervised learning approaches that are attractive due to their connections to an existing method. However, more sophisticated approaches can be used too.

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a

natural choice for the parameterized actor is again a parameterized Gaussian distribution with a

state-dependent mean and covariance: pa|sq " N pa|psq, psqq. The parameter  can be learned by minimizing the expected KL divergence to the guide actor:

LKLpq " Eppsq rKL ppa|sq||~pa|sqqs

"

,, TrpF psqpsqq

Ep psq

< ` <

 ´ log |psq|

`

LWpq < ` <

`

const,

(21)

"i where LWpq " Eppsq }psq ´ `psq}F2 psq is the weighted-mean-squared-error (WMSE)

which only depends on  of the mean function. The const term does not depend on .

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients

(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, we can show that

 LW pq 2

"

" i" i ´Eppsq psqaQpps, aq|a"psq ` Eppsq psqaQpps, aq|a"`psq

"i

` <Eppsq





psq´1

psqH

´1 

psqa

Qpps,

aq|a"

psq

"i

´ <Eppsq





psq´1

psqH

´1 

psqa

Qpps,

aq|a"`

psq

.

(22)

The proof is given in Appendix A.4. The negative of the first term is precisely equivalent to DPG.

Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded as updating the mean parameter with biased DPG where the bias terms depend on <. We can verify

that aQpps, aq|a"`psq " 0 when < " 0 and this is the case of Ñ 8. Thus, all bias terms vanish when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,

unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is flexible, we observe that learning

performance is sensitive to the initial parameter of the covariance function. For practical pur-

poses, we propose to use a parametrized Gaussian distribution with state-independent covariance:

pa|sq " N pa|psq, q. This class of policy subsumes deterministic policies with additive independent Gaussian noise for exploration. To learn , we minimize the mean-squared-error (MSE):

LMpq

"

1 2

Ep

psq

"}

psq

´

`psq}22 .

(23)

For the covariance, we use an average of the guide covariances:  " p< ` <qEppsq "F ´1psq. For computational efficiency, we execute a single gradient update in each learning iteration instead

of optimizing this loss function until convergence.

Similarly to the above analysis, the gradient of the MSE w.r.t.  can be expanded and rewritten into

"´

¯i

LMpq " Eppsq psqH´1psq aQpps, aq|a"psq ´ aQpps, aq|a"`psq

(24)

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set < " 0 and

Hpsq " ´I where I is an identity matrix. We can also substitute these values back into Eq.(16).

Then, we can interpret DPG as a method that performs first-order optimization in the action space:

`psq " psq ` aQpps, aq|a"psq,

(25)

and then uses the gradient in Eq.(24) to update the policy parameter. This interpretation shows that

DPG is indeed a first-order method that only uses the first-order information of the critic for the

actor learning. Therefore in principle, GAC, which uses the second-order information, should learn

faster than DPG.

6

Under review as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, performances of actor-critic methods also depend on the accuracy of the critic.

We assume that the critic Qp ps, aq is represented by neural networks with a parameter . We adopt the approach proposed by Lillicrap et al. (2015) with some adjustments to learn . More concretely,
we use gradient descent to minimize the squared Bellman error with a slowly moving target critic:

,, ´

¯2

 Ð  ´  Eppsq,pa|sq,pps1|s,aq Qp ps, aq ´ y ,

(26)

where   0 is the step-size. The target value y " rps, aq ` Epa1|s1qrQp¯ ps1, a1qs is computed by

target critic Qp¯ ps1, a1q which has parameter ¯ updated by ¯ Ð   ` p1 ´  q¯. The expectation for

the squared play buffer.

error is approximated The expectation over

using mini-batch samples tpsn pa1|s1q is approximated using

, an, rn, samples

stn1aqn1u,Nnm"u1Mmd"r1aw,,nfropma1a|srn1eq-

for each sn1 . We do not use a target actor to compute y since the KL upper-bound already constraints

the actor update and a target actor will further slow it down. Note that we are not restricted to this

policy evaluation method and more efficient methods such as Retrace (Munos et al., 2016) can also

be used.

4 RELATED WORK

Besides the connections to DPG, GAC also relates to existing methods as follows. A similar op-

timization problem to Eq.(7) was considered by the model-free trajectory optimization (MOTO)

method Akrour et al. (2016). GAC can be viewed as an extension of MOTO with two signif-

icant novelties. First, MOTO learns a sequence of time-dependent log-linear Gaussian policies

tpa|sq " N pa|Bts ` bt, tq, while GAC learns a log-nonlinear Gaussian policy. Second, MOTO

learns

a time-dependent critic

given

by

Qptps, aq

"

1 2

aJC

ta

`

aJDts

`

aJct

` tpsq

and per-

forms policy update with these functions. In contrast, our method learns a more complex critic and

performs Taylor's approximation in each training step.

Beside MOTO, the optimization problem also resembles that of trust region policy optimization (TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

"i

max
1

Ep

psq,1

pa|sq

Qpps, aq

, subject to Ep psq rKLppa|sq||1 pa|sqqs 

,

(27)

where Qpps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b). There are two major differences between the two problems beside the state distributions, the entropy constraint and direction of KL constraints. First, TRPO optimizes for the policy parameter while we optimize for the guide actor. Second, TRPO solves the optimization problem by conjugate gradient with a quadratic approximation of the KL divergence, while we solve the optimization problem in a closed form with a quadratic approximation of the critic.

The idea of firstly learn a non-parameterized policy and then later learn a parameterized policy by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun, 2013). However, GPS learns the guide policy by trajectory optimization methods such as iterative linear-quadratic Gaussian regulator (Li & Todorov, 2004) which require a model of transition function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENT RESULTS
We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics simulator (Todorov et al., 2012). Both actor and critic use neural networks with two hidden layers of 300 and 400 units, as described in Appendix C. The state space is defined as a joint space of the agent for all tasks. However, an extension to pixel state space is straightforward by using convolutional neural networks (Goodfellow et al., 2016). We compare GAC-0 and GAC-1 against deep DPG (DDPG) (Lillicrap et al., 2015), Q-learning with normalized advantage function (Q-NAF) (Gu et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on 9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing methods and they clearly outperform the other methods in Half-Cheetah.

7

Under review as a conference paper at ICLR 2018

GAC-1

GAC-0

DDPG

QNAF

TRPO

1000 800 600 400 200
0 0

Time ste5p0s (x1000)

100

(a) Inverted-Pend.

8000 6000 4000 2000
0 0 100 200Time30st0eps 4(x0100005) 00 600 700
(b) Inv-Double-Pend.

10 20 30 40 50
0 100 200Time30st0eps 4(x0100005) 00 600 700
(c) Reacher

120 100 80 60 40 20
0
0

100 200Time30st0eps 4(x0100005) 00 600 700

(d) Swimmer

6000 5000 4000 3000 2000 1000
0
0

100 200Time30st0eps 4(x0100005) 00 600 700

(e) Half-Cheetah

1000

500

0

500

1000 0

100 200Time30st0eps 4(x0100005) 00 600 700

(f) Ant

3500 3000 2500 2000 1500 1000 500
0 0

100 200Time30st0eps 4(x0100005) 00 600 700

(g) Hopper

4000 3500 3000 2500 2000 1500 1000 500
0 0

100 200Time30st0eps 4(x0100005) 00 600 700

(h) Walker2D

1600 1400 1200 1000 800 600 400 200
00

100 200Time30st0eps 4(x0100005) 00 600 700

(i) Humanoid

Figure 1: Expected returns averaged over 5 trials (4 trials for Walker2D and 2 trials for Humanoid at the time of submission). The x-axis indicates training time steps. The y-axis indicates averaged return and higher is better. More clear figures are provided in Appendix C due to space limitation.

The performance of GAC-0 and GAC-1 are comparable on these tasks, except on Humanoid where GAC-1 seems to learn faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance approximation but this could help escape local optima. We believe GAC-S that uses S  1 samples for the averaged Taylor's approximation should outperform both GAC-0 and GAC-1. While this is computationally expensive, we can use parallel computation to reduce the computation time.
The expected returns of both GAC-0 and GAC-1 have high fluctuations on the Hopper and Walker2D tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can learn good policies for these tasks in the middle of learning. However, the policies quickly diverge to poor ones and then they are quickly improved to be good policies again. We believe that this happens because the step size F ´1psq " `<´1 ´ Hpsq´1 of the guide actor in Eq. (16) can be very large near local optima for Gauss-Newton approximation. That is, the gradients near local optima are small and this makes the approximation Hpsq " aQpps, aqaQpps, aqJ small as well. If <´1 is also relatively small then the matrix F ´1psq can be very large. Thus, under these conditions, GAC may use too large step sizes to compute the guide actor and this results in high fluctuations of performance. We expect that this scenario can be avoided by adding a regularization constant to the Gauss-Newton approximation.
6 CONCLUSION AND FUTURE WORK
Actor-critic methods are appealing for real-world problems due to its good data efficiency and learning speed. However, existing actor-critic methods do not use second-order information of the critic. In this paper, we established a framework that distinguishes itself from existing work by utilizing Hessians of the critic for actor learning. Within our framework, we proposed a practical method that uses Gauss-Newton approximation instead of the Hessians. We showed through experiments that our method is promising and thus the framework should be further investigated.
Our analysis showed that the proposed method is closely related to DPG. However, DPG was also shown to be a limiting case of the stochastic policy gradients when the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our framework has a connection to the stochastic policy gradients as well, and finding such a connection is one of our future work.
Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our method achieves state-of-the-art performances. However, its performance can still be improved in many directions. For instance, we may impose a KL constraint for a parameterized actor to improve its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efficient policy evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´is Paulo Reis, and Gerhard Neumann. Model-based relative entropy stochastic search. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 3537­3545, 2015.
Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-free trajectory optimization for reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 2961­2970, 2016.
Brandon Amos, Lei Xu, and J. Zico Kolter. Input convex neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 146­155, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1­142, 2013.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249­256, Chia Laguna Resort, Sardinia, Italy, 13­15 May 2010. PMLR.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 2829­2838, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1352­1361, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2944­2952, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
Vijay R. Konda and John N. Tsitsiklis. On actor-critic algorithms. SIAM J. Control Optim., 42(4): 1143­1166, April 2003. ISSN 0363-0129.
Sergey Levine and Vladlen Koltun. Guided policy search. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1­9, 2013.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17:39:1­39:40, 2016.
Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In ICINCO 2004, Proceedings of the First International Conference on Informatics in Control, Automation and Robotics, Setu´bal, Portugal, August 25-28, 2004, pp. 222­229, 2004.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.
9

Under review as a conference paper at ICLR 2018
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, February 2015. ISSN 00280836.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1928­1937, New York, New York, USA, 20­22 Jun 2016. PMLR.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient offpolicy reinforcement learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1046­1054, 2016.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization, second edition. World Scientific, 2006.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180­1190, 2008.
Jan Peters, Katharina Mu¨lling, and Yasemin Altun. Relative entropy policy search. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region policy optimization. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 1889­1897. JMLR.org, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438, 2015b.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 387­395, 2014.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation and machine learning. MIT Press, 1998.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 December 4, 1999], pp. 1057­1063, 1999.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026­5033, 2012.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229­256, 1992. doi: 10.1007/BF00992696.
10

Under review as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
~
subject to

"i Eppsq,~pa|sq Qpps, aq ,
Eppsq rKLp~pa|sq||pa|sqqs  , Eppsq rHp~pa|sqqs  , Eppsqrpa|sq " 1,

(28)

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian of this optimization problem is

"i Lpr, , , q " Eppsq,~pa|sq Qpps, aq ` p ´ Eppsq rKLp~pa|sq||pa|sqqsq

` pEppsq rHp~pa|sqqs ´ q ` pEppsqrpa|sq ´ 1q,

where , , and  are the dual variables. Then, by taking derivative of L w.r.t. r we obtain

,, ´

 ¯

Br L " Eppsq Qpps, aq ´ p ` q log rpa|sq `  log pa|sq da ´ p `  ´ q.

(29) (30)

We set this derivation to zero in order to obtain

,, ´

 ¯

0 " Eppsq Qpps, aq ´ p ` q log rpa|sq `  log pa|sq da ´ p `  ´ q

" Qpps, aq ´ p ` q log rpa|sq `  log pa|sq ´ p `  ´ q.

Then the solution is given by

~¸


rpa|sq " pa|sq ` exp

Qpps, aq `

exp

^ ´



` 

 `

´ 





~¸


9 pa|sq ` exp

Qpps, aq `

.

(31)
(32) (33)

To obtain the dual function gp, q, we substitute the solution to the constraint terms of the Lagrangian and this gives us

"i Lp, , q " Eppsq,rpa|sq Qpps, aq

«ff

´ p ` qEppsq,rpa|sq

Qpps, aq `

`



 `



log

 pa|sq

´



` `

´ 



`

Ep psq,rpa|sq

rlog

 pa|sqs

`



` Ep ,rpa|sq

´

1

`



´ .

After some calculation, we obtain

(34)

Lp, , q "  ´  ` Eppsq r `  ´ s

« ~ ¸ff

 
"  ´  ` p ` qEppsq pa|sq ` exp

Qpps, aq `

da

" gp, q,

(35)

where

in

the

second

line

we

use

the

fact

that

expp´

`´ `

q

is

the

normalization

term

of

rpa|sq.

11

Under review as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor's

approximation is performed with a0 " Epa|sq ras " psq. Recall that Taylor's approximation with  is given by

Qpps, aq

"

1 2

aJ

H



psqa

`

aJ psq

`

 psq,

(36)

where  psq " aQpps, aq|a"psq ´ H psqpsq. By substituting  psq into Lpsq " <´ 1psqpsq ´ psq, we obtain

Lpsq " <´1psqpsq ` aQpps, aq|a"psq ´ H psqpsq " p<´ 1psq ´ H psqqpsq ` aQpps, aq|a"psq

" F psqpsq ` aQpps, aq|a"psq. Therefore, the mean update is equivalent to

(37)

`psq " F ´1psqLpsq

" psq ` F ´1psqaQpps, aq|a"psq,

(38)

which is a second-order optimization step with a curvature matrix F psq " <´1psq ´ H psq.

For the case when ta0u ,, pa0|sq " N pa0|psq, psqq are used for averaged Taylor's approximation, we obtain

"i Lpsq " <´1psqpsq ` Epa0|sq aQpps, aq|a"a0 ´ Epa0|sq rH0psqa0psqs . (39)

Then, by assuming that Epa0|sq rH0psqa0psqs " Epa0|sq rH0s Epa0|sq ra0s, we obtain "i
Lpsq " <´ 1psqpsq ` Epa0|sq aQpps, aq|a"a0 ´ Epa0|sq rH0s Epa0|sq ra0s

"i " <´ 1psqpsq ` Epa0|sq aQpps, aq|a"a0 ´ Epa0|sq rH0s psq

"i " p<´1psq ´ Epa0|sq rH0psqsqpsq ` Epa0|sq aQpps, aq|a"a0

"i " F psqpsq ` Epa0|sq aQpps, aq|a"a0 .

(40)

Therefore, we have a second-order optimization step "i
`psq " psq ` F ´1psqEpa0|sq aQpps, aq|a"a0 ,

(41)

where F ´1psq " <´ 1psq ´ Epa0|sq rH0psqs is a curvature matrix. As described in the main paper, this interpretation is only valid when the assumption Epa0|sq rH0psqa0psqs "
Epa0|sq rH0s Epa0|sq ra0s holds. However, we can still use the expectation of Taylor's approximation to perform policy update regardless of this assumption.

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION
Let f ps, aq " exppQpps, aqq, then the Hessian Hpsq " a2 Qpps, aq can be expressed as Hpsq " a ra log f ps, aqs " a "af ps, aqf ps, aq´1 " af ps, aq `af ps, aq´1J ` 2af ps, aqf ps, aq´1 " af ps, aqf f ps, aq´1 paf ps, aqqJ ` 2af ps, aqf ps, aq´1 " ´af ps, aqf ps, aq´2 paf ps, aqqJ ` a2 f ps, aqf ps, aq´1 " ´ `af ps, aqf ps, aq´1 `af ps, aqf ps, aq´1J ` 2af ps, aqf ps, aq´1 " ´a log f ps, aqa log f ps, aqJ ` 2af ps, aqf ps, aq´1 " ´aQpps, aqaQpps, aqJ ` 2a exppQpps, aqq expp´Qpps, aqq,

(42)

12

Under review as a conference paper at ICLR 2018

which concludes the proof.
Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on Qpps, aq so that Hessians are always negative semi-definite. In literature, there exists two special structures that satisfies this requirement.
Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic function with a negative curvature:

QpNAFps, aq

"

1 2

pa

´

bpsqqJ

W

psqpa

´ bpsqq

` V psq,

(43)

where a negative-definite matrix-valued function W psq, a vector-valued function bpsq and a scalarvalued function V psq are parameterized functions whose their parameters are learned by policy evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative definite Hessians can be simply obtained as Hpsq " W psq. However, a significant disadvantage of NAF is that it assumes the action-value function is quadratic regardless of states and this is generally not true for most reward functions. Moreover, the Hessians become action-independent even though the critic is a function of actions.
Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions are always negative semi-definite, we may use ICNNs to represent a negative critic and directly use its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We first consider the weight mean-squared-error loss function where the guide actor is N pa|`psq, `psqq and the current actor is N pa|psq, psqq. Taylor's approximation of
Qpps, aq at a0 " psq is

Qpps, aq

"

1 2

aJ

H



psqa

`

aJ psq

`

 psq.

(44)

By assuming that H psq is strictly negative definite3, we can take a derivative of this approxima-

tion

w.r.t.

a

and

set

it

to

zero

to

obtain

a

"

H

´1 

psqa

Qpps,

aq

´

H

´1 

psq

psq.

Replacing

a

by

psq and `psq yields

 psq

"

H

´1 

psqa

Qpps,

aq|a" psq

´

H

´1 

psq

psq,

`psq

"

H

´1 

psqa

Qpps,

aq|a"`psq

´

H

´1 

psq

psq.

(45) (46)

Let the weight mean-squared-error be defined as

"i LWpq " Eppsq }psq ´ `psq}F2 psq .

(47)

3This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approximation.
13

Under review as a conference paper at ICLR 2018

We consider its gradient w.r.t.  as follows:

LWpq " 2Eppsq "psqF psq `psq ´ `psq

" 2Eppsq "psqp<´ 1psq ´ H psqq `psq ´ `psq

" 2Eppsq "psqp<´ 1psq ´ H psqq

´ ¯i

^

H

´1 

psqa

Qpps,

aq|a" psq

´

H

´1 

psqa

Qpps,

aq|a"`psq

"´

¯i

" 2<Eppsq





psq´ 1

psqH

´1 

psq

aQpps, aq|a"psq ´ aQpps, aq|a"`psq

"´

¯i

` 2Eppsq psq aQpps, aq|a"`psq ´ aQpps, aq|a"psq

" i" i " ´2Eppsq psqaQpps, aq|a"psq ` 2Eppsq psqaQpps, aq|a"`psq

"i

` 2<Eppsq





psq´1

psqH

´1 

psqa

Qpps,

aq|a"

psq

"i

´ 2<Eppsq





psq´1

psqH

´1 

psqa

Qpps,

aq|a"`

psq

.

(48)

This concludes the proof for the gradient in Eq.(22). Note that we should not directly replace the mean functions in the weight mean-square-error by Eq.(45) and Eq.(46) before expanding the gradient. This is because the analysis would require computing the gradient w.r.t.  inside the Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean `psq is considered as a constant w.r.t.  similarly to an output function in supervised learning. That is, there are no computation graph connection between `psq and .

The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be defined as

LMpq

"

1 2

Ep

psq

"}

psq

´

`psq}22 .

(49)

Its gradient w.r.t.  is given by

LMpq " Eppsq "psq `psq ´ `psq

"´

¯i

" Eppsq psqH´1psq aQpps, aq|a"psq ´ aQpps, aq|a"`psq ,

(50)

which concludes the proof for the gradient in Eq.(24).

To show that aQpps, aq|a"`psq " 0 when < " 0, we directly substitute < " 0 into `psq "

`´1psq

´

H



psq´1

´<´1psq

psq

`



¯ psqq

and

this

yield

`psq

"

´H

´1 

psq



psq.

(51)

Since

`psq

"

H

´1 

psqa

Qpps,

aq|a"`psq

´

H

´1 

psq

psq

from

Eq.(46)

and

the

Hessians

are

non-zero, it has to be that aQpps, aq|a"`psq " 0. This is intuitive since without the KL constraint, the mean of the guide actor always be at the optima of the second-order Taylor's approximation and

the gradients are zero at the optima.

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is defined as

QpNAFps, aq

"

1 2

pa

´

bpsqqJ

W

psqpa

´ bpsqq

` V psq,

(52)

where W psq is a negative definite matrix. Gu et al. (2016) proposed to perform Q-learning with NAF by using the fact that argmaxa QpNAFps, aq " bpsq and maxa QpNAFps, aq " V psq.

Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This can be shown by using NAF as a critic instead of performing Taylor's approximation of the critic.

14

Under review as a conference paper at ICLR 2018

Firstly, we expand the quadratic term of NAF as follows:

QpNAFps, aq

"

1 2 pa

´

bpsqqJW psqpa

´

bpsqq

`

V

psq

"

1 2

aJ

W

psqa

´

aJW psqbpsq

`

1 2

bpsqJ

W

psqbpsq

`

V

psq

"

1 2

aJ

W

psqa

`

aJpsq ` psq,

(53)

where

psq

"

´W psqbpsq and

psq

"

1 2

bpsqJW

psqbpsq

`

V

psq.

By

substituting

the

quadratic

model obtained by NAF into the GAC framework, the guide actor is now given by ~pa|sq "

N pa|`psq, `psqqq with

`psq " p<´1psqq ´ W psqq´1p<´1psqpsq ´ W psqbpsqq

(54)

`psq " p< ` <qp<´1psqq ´ W psqq.

(55)

To obtain Q-learning with NAF, we set < " 0, i.e., we perform a greedy maximization where with

the KL upper-bound approaches infinity, and this yields

`psq " ´W psq´1p´W psqbpsqq " bpsq,

(56)

which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a special case of GAC when the NAF model is used and the KL upper-bound approaches infinity.

A.6 RELATION TO MAXIMUM ENTROPY REINFORCEMENT LEARNING

The entropy constraint in the optimization problem in Eq.(7) suggests that GAC is closely

related to the maximum entropy reinforcement learning (MaxEnt RL) framework. The

MaxEnt RL aims to maximize an expected returns with an additional entropy bonus:

8
t"1

Ep psq

rrpst, atq

`

Hppat|stqqs

where





0

is

a

trade-off

parameter.

The

optimal

policy

is given by (Haarnoja et al., 2017)

M< axEntpa|sq

"

exp

^

Qs<oftps,

aq ´ 

Vs<oftpsq



9

exp

^

Q<softps, 

aq



,

(57)

where the Vs<oftpsq and Q<softps, aq are the optimal soft state-value and action-value functions, respectively. The soft value functions of a policy  are defined as

Vsoftpsq

"

  log

exp ^ Qsoftps, aq  da, 

(58)

«8 ff Qsoftps, aq " rps, aq ` Epat|stq,ppst`1|st,atq ÿ tprpst, atq ` Hpp¨|stqqq
t"1

(59)

The optimal policy and the soft state-value function closely resemble the guide actor in Eq.(8) and the log-integral term in Eq.(9), respectively, except for the definition of action-value functions and

the weight pa|sq ` . To learn the optimal policy of MaxEnt-RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute the soft value functions and approximates the intractable policy using a separate policy function. The proposed method largely differs from soft Q-learning since we use Taylor's approximation to convert the intractable integral into more convenient matrix-vector products.

B PSEUDO-CODE OF GAC
The pseudo-code of GAC is given in Algorithm 1.

C EXPERIMENT DETAILS
C.1 IMPLEMENTATION
We try to follow the network architecture proposed by the authors of each baseline method as close as possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network

15

Under review as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input: Initial actor pa|sq " N pa|psq, q, critic Qp ps, aq, target critic network
Qp¯ ps, aq, KL bound , entropy bound , learning rates 1, 2, and data buffer D " H. 2: for t " 1, . . . , Tmax do 3: procedure COLLECT TRANSITION SAMPLE
4: Observe state st and sample action at ,, N pa|pstq, q. 5: Execute at, receive reward rt and next state s1t. 6: Add transition tpst, at, rt, st1 qu to D. 7: end procedure
8: procedure LEARN 9: Sample N mini-batch samples tpsn, an, rn, sn1 quNn"1 uniformly from D. 10: procedure UPDATE CRITIC 11: Sample actions tan1 ,mumM"1 ,, N pa|psnq, q for each sn. 12: Compute yn, update  by, e.g., Adam, and update ¯ by moving average:

yn

"

rn

1 `M

M
ÿ Qp¯ ps1n, a1n,mq,
m"1



Ð



´

1 1 N

N
ÿ 
n"1

´ Qp psn, anq

´

¯2 yn

,

¯ Ð   ` p1 ´  q¯.

(60)
(61) (62)

13: end procedure
14: procedure LEARN GUIDE ACTOR 15: Compute an,0 for each sn by an,0 " psnq or an,0 ,, N pa|psnq, q. 16: Compute g0psq " aQppsn, aq|a"an,0 and H0pssq " g0psnqg0psnqJ. 17: Solve for p<, <q " argmin0,0 gp, q by a non-linear optimization method. 18: Compute the guide actor rpa|snq " N pa|`psnq, `psnqq for each sn. 19: end procedure
20: procedure UPDATE PARAMETERIZED ACTOR
21: Update policy parameter by, e.g., Adam, to minimize the MSE:

1  Ð  ´ 2 N

N
ÿ }psnq ´ `psnq}22.

n"1

22: Update policy covariance by averaging the guide covariances:

(63)

23: end procedure 24: end procedure 25: end for 26: Output: Learned actor pa|sq.

1  Ð N `psnq.

(64)

and the critic network. For both networks the first layer has 400 hidden units and the second layer has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the relu activation function except for the output of the actor network where we use the tanh activation function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average step for target networks is set to  " 0.001. The maximum size of the replay buffer is set to 1000000. The mini-batches size is set to N " 256. The weights of the actor and critic networks are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The initial covariance  in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process with noise parameters  " 0.15 and  " 0.2 for exploration .
16

Under review as a conference paper at ICLR 2018

1000

800

Averaged return

600

400 200
0 0

Time ste5p0s (x1000)

GAC-1 GAC-0 DDPG QNAF TRPO
100

Figure 2: Performance averaged over 5 trials on the Inverted Pendulum task.

For TRPO, we use the implementation publicly available at https://github.com/openai/ baselines. We also use the provided network architecture and hyper-parameters except the batch size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is fixed to " 0.0001. The entropy lower-bound  is adjusted heuristically by

 " maxp0.99pE ´ E0q ` E0, E0q,

(65)

where E « Eppsq rHppa|sqqs denotes the expected entropy of the current policy and E0 denotes the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming (SLSQP) method with an initial values  " 0.05 and  " 0.05. The number of samples for computing the target critic value is M " 10.

C.2 ENVIRONMENTS AND RESULTS
We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space and the reward function as provided and did not perform any normalization or gradient clipping. The maximum time horizon in each episode is set to 1000. The discount factor  " 0.99 is only used for learning and the test returns are computed without it.
Experiments are repeated for 5 times with different random seeds. The figures below show the results averaged over 5 trials except for Walker2D (4 trials) and Humanoid (2 trials) at the time of submission. The y-axis indicates the averaged test returns where the test returns in each trial are computed once every 5000 training time steps by executing 10 test episodes without exploration. The error bar indicates standard error. The performance of each method is denoted by the following colors, red: GAC-1, blue: GAC-0, black: DDPG, green: QNAF, and purple: TRPO.

17

Under review as a conference paper at ICLR 2018

Averaged return

8000

6000

4000 2000
0 0

GAC-1 GAC-0 DDPG QNAF TRPO 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 3: Performance averaged over 5 trials on the Inverted Double Pendulum task.

10

20

30
40
50 0

GAC-1 GAC-0 DDPG QNAF TRPO 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 4: Performance averaged over 5 trials on the Reacher task.

18

Averaged return

Under review as a conference paper at ICLR 2018

Averaged return

GAC-1

120 GAC-0

DDPG

100

QNAF TRPO

80

60

40

20

0

0 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 5: Performance averaged over 5 trials on the Swimmer task.

6000 GAC-1 GAC-0
5000 DDPG QNAF
4000 TRPO 3000 2000 1000
0 0 100 200 Tim3e0s0teps (4x010000) 500 600 700
Figure 6: Performance averaged over 5 trials on the Half-Cheetah task.
19

Averaged return

Under review as a conference paper at ICLR 2018

Averaged return

1000

500

0

500 GAC-1

GAC-0

1000

DDPG QNAF

TRPO

0 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 7: Performance averaged over 5 trials on the Ant task.

3500

GAC-1 GAC-0

3000

DDPG QNAF

2500 TRPO

2000

1500

1000

500

0 0 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 8: Performance averaged over 5 trials on the Hopper task.

20

Averaged return

Under review as a conference paper at ICLR 2018

Averaged return

4000

GAC-1 GAC-0

3500 DDPG

3000

QNAF TRPO

2500

2000

1500

1000

500

0 0 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 9: Performance averaged over 4 trials on the Walker2D task.

1600 GAC-1

1400

GAC-0 DDPG

1200

QNAF TRPO

1000

800

600

400

200

0 0 100 200 Tim3e0s0teps (4x010000) 500 600 700

Figure 10: Performance averaged over 2 trials on the Humanoid task.

21

Averaged return

