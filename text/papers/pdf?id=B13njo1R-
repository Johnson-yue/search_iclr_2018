Under review as a conference paper at ICLR 2018
PROGRESSIVE REINFORCEMENT LEARNING WITH DISTILLATION FOR MULTI-SKILLED MOTION CONTROL
Anonymous authors Paper under double-blind review
ABSTRACT
Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems, including agents that can move with skill and agility through their environment. An open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. We extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies, as evaluated in the domain of simulated bipedal locomotion across different classes of terrain. We also introduce an input injection method for augmenting an existing policy network to exploit new input features. Lastly, our method uses transfer learning to assist in the efficient acquisition of new skills. The combination of these methods allows a policy to be incrementally augmented with new skills. We compare our progressive learning and integration via distillation (PLAID) method against two alternative baselines.
1 INTRODUCTION
As they gain experience, humans develop rich repertoires of motion skills that are useful in different contexts and environments. Recent advances in reinforcement learning provide an opportunity to understand how motion repertoires can best be learned, recalled, and augmented. Inspired by studies on the development and recall of movement patterns useful for different locomotion contexts (Roemmich and Bastian, 2015), we develop and evaluate an approach for learning multi-skilled movement repertoires. In what follows, we refer to the proposed method as PLAID: Progressive Learning and Integration via Distillation.
For long lived applications of complex control tasks a learning system may need to acquire and integrate additional skills. Accordingly, our problem is defined by the sequential acquisition and integration of new skills. Given an existing controller that is capable of one-or-more skills, we wish to: (a) efficiently learn a new skill or movement pattern in a way that is informed by the existing control policy, and (b) to reintegrate that into a single controller that is capable of the full motion repertoire. This process can then be repeated as necessary. In the process of acquiring a new skill, we also allow for a control policy to be augmented with additional inputs, without adversely impacting its performance. This is a process we refer to as input injection.
Understanding the time course of sensorimotor learning in human motor control is an open research problem (Wolpert and Flanagan, 2016) that exists concurrently with recent advances in deep reinforcement learning. Issues of generalization, context-dependent recall, transfer or "savings" in fast learning, forgetting, and scalability are all in play for both human motor control models and the learning curricula proposed in reinforcement learning. While the development of hierarchical models for skills offers one particular solution that supports scalability and that avoids problems related to forgetting, we eschew this approach in this work and instead investigate a progressive approach to integration into a control policy defined by a single deep network.
Distillation refers to the problem of combining the policies of one or more experts in order to create one single controller that can perform the tasks of a set of experts. It can be cast as a supervised regression problem where the objective is to learn a model that matches the output distributions of all expert policies (Parisotto et al., 2015; Teh et al., 2017; Rusu et al., 2015). However, given a new task for which an expert is not given, it is less clear how to learn the new task while successfully integrating this new skill in the pre-existing repertoire of the control policy for an agent. One well-known technique in machine learning to significantly improve sample efficiency across similar tasks is to use Transfer Learning (TL) (Pan and Yang, 2010), which seeks to reuse knowledge learned from solving a previous task to efficiently learn a new task. However, transferring knowledge from previous tasks to new tasks may not be straightforward; there can be negative transfer wherein a previously-trained model can take longer to learn a new task via fine-tuning than would a randomly-initialized model (Rajendran et al., 2015). Additionally, while learning a new skill, the control policy should not forget how to perform old skills.
1

Under review as a conference paper at ICLR 2018

The core contribution of this paper is a method (PLAID) to repeatedly expand and integrate a motion repertoire. The main building blocks consist of policy transfer and multi-task policy distillation, and the method is evaluated in the context of a continuous motor control problem, that of robust locomotion over distinct classes of terrain. We evaluate the method against two alternative baselines. We also introduce input injection, a convenient mechanism for adding inputs to control policies in support of new skills, while preserving existing capabilities.

2 RELATED WORK
Transfer learning and distillation are of broad interest in machine learning and RL (Pan and Yang, 2010; Taylor and Stone, 2009; Teh et al., 2017). Here we outline some of the most relevant work in the area of Deep Reinforcement Learning (DRL) for continuous control environments.
Distillation Recent works have explored the problem of combining multiple expert policies in the reinforcement learning setting. A popular approach uses supervised learning to combine each policy by regression over the action distribution. This approach yields model compression (Rusu et al., 2015) as well as a viable method for multi-task policy transfer (Parisotto et al., 2015) on discrete action domains including the Arcade Learning Environment (Bellemare et al., 2013). We adopt these techniques and extend them for the case of complex continuous action space tasks and make use of them as building block.
Transfer Learning Transfer learning exploits the structure learned from a previous task in learning a new task. Our focus here is on transfer learning in environments consisting of continuous control tasks. The concept of appending additional network structure while keeping the previous structure to reduce catastrophic forgetting has worked well on Atari games (Rusu et al., 2015; Parisotto et al., 2015; Rusu et al., 2016; Chen et al., 2015) Other methods reproduce data from all tasks to reduce the possibility of forgetting how to perform previously learned skills e.g, (Shin et al., 2017; Li and Hoiem, 2016). Recent work seeks to mitigate this issue using selective learning rates for specific network parameters (Kirkpatrick et al., 2017). A different approach to combining policies is to use a hierarchical structure (Tessler et al., 2016). In this setting, previously-learned policies are available as options to execute for a policy trained on a new task. However, this approach assumes that the new tasks will be at least a partial composition of previous tasks, and there is no reintegration of newly learned tasks. A recent promising approach has been to apply metalearning to achieve control policies that can quickly adapt their behavior according to current rewards (Finn et al., 2017). This work is demonstrated on parameterized task domains.
Hierarchical RL further uses modularity to achieve transfer learning for robotic tasks (Tessler et al., 2016) This allows for the substitution of network modules for different robot types over a similar tasks (Devin et al., 2017). Other methods use Hierarchical Reinforcement Learning (HRL) as a method for simplifying a complex motor control problem, defining a decomposition of the overall task into smaller tasks (Kulkarni et al., 2016; Heess et al., 2016; Peng et al., 2017) While these methods examine knowledge transfer, they do not examine the reintegration of policies for related tasks and the associated problems such as catastrophic forgetting. Recent work examines learned motions that can be shaped by prior mocap clips (Merel et al., 2017), and that these can then be integrated in a hierarchical controller.

3 FRAMEWORK

In this section we outline the details of the Reinforcement Learning (RL) framework. We also give an introduction to the concepts of TL and distillation.

3.1 REINFORCEMENT LEARNING

Leveraging the framework of reinforcement learning, we frame the problem as a Markov Decision Processes
(MDP): at each time step t, the world (including the agent) is in a state st  S, wherein the agent is able to perform actions at  A, sampled from a policy (st, at) = p(at|st) and resulting in state st+1  S according to transition probabilities T (st, at, st+1). Performing action at from state st produces a reward rt; the expected cumulative reward earned from following some policy  may then be written as:

J () = Er0,...,rT

T
trt
t=0

(1)

where T is the time horizon, and  is the discount factor, defining the planning horizon length.

2

Under review as a conference paper at ICLR 2018

The agent's goal is to learn an optimal policy, , maximizing J(). If the policy has parameters , then the goal may be reformulated to identify the optimal parameters  :

 = arg max J ((·|))


(2)

Our policy models a Gaussian distribution with a mean state dependent mean, µt (st). Thus, our stochastic policy may be formulated as follows:

at  (at | st, ) = N (µ(st | µ), )  = diag{i2}

(3)

where  is a diagonal covariance matrix with entries i2 on the diagonal, similar to (Peng et al., 2017).

To optimize our policy, we use stochastic policy gradient methods, which are well-established family of
techniques for reinforcement learning (Sutton et al., 2000). The gradient of the expected reward with respect to the policy parameters,  J((·|)), is given by:

 J ((·|)) = d(s)  log((a, s|))A(s, a) da ds
SA

(4)

where d = S

T t=0

tp0(s0)(s0



s

|

t,

0)

ds0

is

the

discounted

state

distribution,

p0(s)

represents

the

initial state distribution, and p0(s0)(s0  s | t, 0) models the likelihood of reaching state s by starting

at state s0 and following the policy (a, s|) for T steps (Silver et al., 2014). A(s, a) represents an

advantage function (Schulman et al., 2016). In this work, we use the Positive Temporal Difference (PTD)

update proposed by (Van Hasselt, 2012) for A(s, a):

A(st, at) = I [t > 0] =

1, 0,

t > 0 otherwise

(5)

t = rt + V(st+1) - V(st)

(6)

where V(s) = E

T t=0

trt

|

s0

=

s

is the value function, which gives the expected discounted cumu-

lative reward from following policy  starting in state s. PTD has the benefit of being insensitive to the

advantage function scale. Furthermore, limiting policy updates in this way to be only in the direction of ac-

tions that have a positive advantage has been found to increase the stability of learning (Van Hasselt, 2012).

Because the true value function is unknown, an approximation V(· | v) with parameters v is learned,

which is formulated as the regression problem:

minimize E
st ,rt ,st+1

1 2

(yt

-

V

(s

|

v ))2

,

yt = rt + V(st+1 | v)

(7)

3.2 POLICY DISTILLATION
Given a set of expert agents that have solved/mastered different tasks we may want to combine the skills of these different experts into a single multi-skilled agent. This process is referred to as distillation. Distillation does not necessarily produce an optimal mix of the given experts but instead tries to produce an expert that best matches the action distributions produced by all experts. This method functions independent of the reward functions used to train each expert. Distillation also scales well with respect to the number of tasks or experts that are being combined.

3.3 TRANSFER LEARNING
Given an expert that has solved/mastered a task we want to reuse that expert knowledge in order to learn a new task efficiently. This problem falls in the area of Transfer Learning (Pan and Yang, 2010). Considering the state distribution expert is skilled at solving, (Di the source distribution) it can be advantageous to start learning a new, target task i+1 with target distribution Di+1 using assistance from the expert. The agent learning how to solve the target task with domain Di+1 is referred to as the student. When the expert is used to assist the student in learning the target task it can be referred to as the teacher. The success of these methods are dependent on overlap between the Di and Di+1 state distributions.

3

Under review as a conference paper at ICLR 2018

4 PROGRESSIVE LEARNING
Although we focus on the problem of being presented with tasks sequentially, there exist other methods for learning a multi-skilled character. We considered 3 overall integration methods for learning multiple skills, the first being a controller that learns multiple tasks at the same time (MultiTasker), where a number of skills are learned at the same time. It has been shown that learning many tasks together can be faster than learning each task separately (Parisotto et al., 2015). The curriculum for using this method is shown in Figure 1a were during a single RL simulation all tasks are learned together. It is also possible to learn each task separately but in parallel and then combine the resulting policies Figure 1b. We attempted to evaluate this method as well but we found that learning many skills from scratch was challenging, we were only able to get fair results for the flat task. Also, when a new task is to be learned with this model it would occur outside of the original parallel learning, leading to a more sequential method. The last version learns each task sequentially using TL from the previous, most skilled policy Figure 1c. This method works well for both combining learned skills and learning new skills.

L0 L1 0 . . . L-1
L

1

L0 1

L1 2

0 . . . . . .

L-1



D +1

L +1

(a) MultiTasker

L1 2

0 L0 1

D

^2

(b) Parallel Learning and Distillation

...

. . . L-1



L

D .^. . D ^ D

+1 ^+1

(c) PLAID

Figure 1: Different curriculum learning process. The red box with a D in it denotes a distillation step that combines policies. Each gray box denotes one iteration of learning a new policy. The larger red boxes with an Lterrain-type denotes a learning step where a new skill is learned.

4.1 PROGRESSIVE LEARNING AND INTEGRATION VIA DISTILLATION
In this section, we detail our proposed learning framework for continual policy transfer and distillation (PLAID). In the acquisition (TL) step, we are interested in learning a new task i+1. Here transfer can be beneficial if the task structure is somewhat similar to previous tasks i. We adopt the TL strategy of using an existing policy network and fine-tuning it to a new task. Since we are not concerned with retaining previous skills in this step, we can update this policy without concern for forgetting. As the agent learns it will develop more skills and the addition of every new skill can increase the probability of transferring knowledge to assist the learning of the next skill.
In the integration (distillation) step, we are interested in combining all past skills (0, . . . , i) with the newly acquired skill i+1. Traditional approaches have used policy regression where data is generated by collecting trajectories of the expert policy on a task. Training the student on these trajectories does not always result in robust behaviour. This poor behaviour is caused by the student experiences a different distribution of trajectories than the expert during evaluation. To compensate for this distribution difference, portions of the trajectories should be generated by the student. This allows the expert to suggest behavior that will pull the state distribution of the student closer to the expert's. This is a common problem in learning a model to reproduce a given distribution of trajectories (Ross et al., 2010; Bengio et al., 2015; Martinez et al., 2017; Lamb et al., 2016). We use a method similar to the DAGGER algorithm (Ross et al., 2010) which is useful for distilling policies (Parisotto et al., 2015). As our RL algorithm is an actor-critic method, we also perform regression on the critic by fitting both in the same step.
4.2 HIGH LEVEL EXPERIMENT DESIGN
The results presented in this work cover a range of tasks that share a similar action space and state space. Our focus is to demonstrate continual learning between related tasks. In addition, the conceptual framework allows for extensions that would permit differing state spaces, described later in Section: 5.2.

4

Under review as a conference paper at ICLR 2018
5 RESULTS
In this experiment, our set of tasks consists of 5 different terrains that a 2D humanoid walker learns to traverse. A bipedal character is trained to navigate multiple types of terrain including flat in (Figure 2a), incline (Figure 2b), steps (Figure 2c), slopes (Figure 2d), gaps (Figure 2e) and a combination of all terrains mixed (Figure 2f) on which agents are trained. The goal in these tasks is to maintain a consistent forward velocity traversing various terrains, while also matching a motion capture clip of a natural human walking gait on flat ground, similar to (Peng and van de Panne, 2016). The 2D humanoid receives as input both a character and (eventually) a terrain state representation, consisting of the terrains heights of 50 equallyspaced points in front of the character. The action space is 11-dimensional, corresponding to the joints. Reasonable torque limits are applied, which helps produce more natural motions and makes the control problem more difficult. A detailed description of the experimental setup is included in Section: 8.4. The tasks are presented to the agent sequentially and the goal is to progressively learn to traverse all terrain types.

(a) flat

(b) incline

(c) steps

(d) slopes

(e) gaps Figure 2: The environments used to evaluate PLAID.

(f) mixed

We evaluate our approach against two baselines. First, we compare the above learning curriculum from learning new tasks in PLAID with learning new tasks from randomly initialized controller (Scratch). This will demonstrate that knowledge from previous tasks can be effectively transferred after distillation steps. Second, we compare to the MultiTasker to demonstrate that iterated distillation is effective for the retention of learned skills. The MultiTasker is also used as a baseline for comparing learning speed. The results of the PLAID controller are displayed in the accompanying Video 1

5.1 TRANSFER LEARNING
First, the pd-biped is trained to produce a walking motion on flat ground (flat). In Figure 3a PLAID is compared to the two baselines for training on incline. The Scratch method learns the slowest as it is given no information about how to perform similar skills. The first MultiTasker for the incline task is initialized from a terrain injected controller that was trained to walk on flat ground. Any subsequent MultiTasker is initialized from the final MultiTasker model of the preceding task. This controller has to learn multiple tasks together, which can complicate the learning process, as simulation for each task is split across the training and the overall RL task can be challenging. This is in contrast to using PLAID, that is also initialized with the same policy trained on flat, that will integrate skills together after each new skill is learned.
In Figure 3b the MultiTasker is learning the new task (steps) with similar speed to PLAID. However, after adding more tasks the MultiTasker is beginning to struggle in Figure 3c and fails in in Figure 3d, with the number of tasks it must learn at the same time. While PLAID learns the new tasks faster and is able to integrate the new skill required to solve the task robustly.

1https : //www.dropbox.com/s/kbb4145yd1s9s3p/P rogresiveLearning.mp4?dl = 0 5

Under review as a conference paper at ICLR 2018

(a) incline

(b) steps

(c) slopes

Reward

2500

MultiTasker

Gaps Scratch

Transfer

Distill

2000

1500

1000

500

0
0 100000 200000 300000 400000 500000 600000 Iterations
(d) gaps

Figure 3: TL comparison over each of the environments. (a) Shows the benefit of using TL when learning a new task/skill, incline, when the controller has some knowledge of flat. (b) TL for both PLAID and MultiTasker is similar (c) PLAID is showing faster learning after adding an additional skill. (d) PLAID still is showing faster learning after adding an additional skill and MultiTasker failing to learn the new task. The distiller initializes its policy with the most recently learned expert. The MultiTasker is also initialized from the most recent expert but alternates between each environment during training. The learning for PLAID is split into two steps, with TL (in green) going first followed by the distillation part (in red). Using TL assists in the learning of new tasks.

5.2 INPUT INJECTION
An appealing property of using distillation in PLAID is that the combined policy model need not resemble that of the individual expert controllers. For example, two different experts lacking state features and trained without a local map of the terrain can be combined into a single policy that has new state features for the terrain. These new terrain features can assist the agent the task domain in which it operates.
We introduce the idea of input injection for this purpose. We augment a policy with additional input features while allowing it to retain its original functional behaviour similar to (Chen et al., 2015). This is achieved by adding additional inputs to the neural network and initializing the connecting layer weights and biases to 0. By only setting the weights and biases in the layer connecting the new features to the original network to 0, the gradient can still propagate to any lower layers which are initialized random without changing the functional behavior. This performed when distilling the flat and incline experts.
5.3 DISTILLING MULTIPLE POLICIES
Training over multiple tasks at the same time my help the agent learn skills quicker, but this may not scale with respect to the number of tasks. When training the MultiTasker over two or even three tasks (Figure 4a) the method displays good results, however when learning a fourth or more tasks the method struggles, as shown in Figure 4b and 4b. Part of the reason for this struggle is when new tasks are added the MultiTasker has to make trade-offs between more tasks to maximizes. As more tasks are added, this trade-off becomes increasingly complex resulting in the MultiTasker favouring easier tasks. Using PLAID to combine the skills of many policies appears to scale better with respect to the number of skills being integrated. This is likely because distillation is a semi-supervised method which is more stable than the un-supervised RL solution. This can be seen in Figure 4d, 4e and especially in 4f where PLAID combines the skills faster and can find higher value policies in practice. PLAID also presents zero-shot training on tasks which it has never trained. In Figure 5 this generalization is shown as the agent navigate across the mixed environment.
There are some indications that distillation is hindering training during the initial few iterations. We are initializing the network used in distillation with the most recently learning policy after TL. The large change in the initial state distribution from the previous seen distribution during TL could be causing larger gradients to appear, disrupting some of the structure learned during the TL step, shown in Figure 4d and 4e. There also might not exist a smooth transition in policy space between the newly learned policy and the previous policy distribution.
6 DISCUSSION
MultiTasker vs PLAID: The MultiTasker may be able to produce a policy that has higher overall average reward, but in practice constraints can keep the method from combining skills gracefully. If the reward functions are different between tasks, the MultiTasker can favor this task with higher rewards, as these tasks may receive higher advantage. It is also a non-trivial task to normalize the reward functions for each task in order to combine them. The MultiTasker may also favor tasks that are easier than other tasks in general. We have shown that the distiller scales better with respect to the number of tasks than the MultiTasker. We expect PLAID would further outperform the MultiTasker if the tasks were more difficult and the reward functions dissimilar.
6

Under review as a conference paper at ICLR 2018

(a) MultiTasker on 3 tasks

(b) MultiTasker on 4 tasks

(c) MultiTasker on 5 tasks

(d) PLAID on 3 tasks

(e) PLAID on 4 tasks

(f) PLAID on 5 tasks

Figure 4: These figures show the average reward a particular policy achieves over a number of tasks. After learning an expert for flat + incline a new steps task is trained. Figure (a) shows the performance for the MultiTasker and figure (c) for the distiller. The distiller learns the combined tasks fast, however the MultiTasker achieves marginally better average reward over the tasks Figures (b,e) show the performance of an expert on flat + incline + steps trained to learn the new task slopes for the MultiTasker and distiller. Last the MultiTasker (c) and PLAID (f) are trained on gaps.

(a) (b) (c) (d) (e) (f) (g)
Figure 5: Still frame shots of the pd-biped traversing the mixed environment.
In our evaluation we compare the number of iterations PLAID uses to the number the MultiTasker uses on only the new task, which is not necessarily fair. The MultiTasker gains its benefits from training on the other tasks together. If the idea is to reduce the number of simulation samples that are needed to learn new tasks then the MultiTasker would fall far behind. Distillation is also very efficient with respect to the number of simulation steps needed. Data could be collected from the simulator in groups and learned from in many batches before more data is needed as is common for behavioral cloning. We expect another reason distillation benefits learning multiple tasks is that the integration process assists in pulling policies out of the local minima RL is prone to.
Transfer Learning: Because we are using an actor-critic learning method, we also studied the possibility of using the value functions for TL. We did not discover any empirical evidence that this assisted the
7

Under review as a conference paper at ICLR 2018
learning process. When transferring to a new task, the state distribution has changed and the reward function maybe be completely different. This makes it unlikely that the value function will be accurate on this new task. In addition, value functions are in general easier and faster to learn than policies, implying that value function reuse is less important to transfer. We also find that helpfulness of TL depends on not only the task difficulty task but the reward function as well. Two tasks may overlap in state space but the area they overlap could be easily reachable. In this case TL may not give significant benefit because the overall RL problem is easy. The greatest benefit is gained from TL when the state space that overlaps for two tasks is difficult to reach and in that difficult to reach area is where the highest rewards are achieved.
6.1 LIMITATIONS:
Once integrated, the skills for our locomotion tasks are self-selecting based on their context, i.e., the knowledge of the upcoming terrain. It may be that other augmentation and distillation strategies are better for situations where the current reward function or a one-hot vector is used to select the currently active expert. In our transfer learning results we could be over fitting the initial expert for the particular task it was learning. Making it more challenging for the policy to learning a new task, resulting in negative transfer. After learning many new tasks the previous tasks may not receive a large enough potion of the distillation training process to preserve the experts skill well enough. How best to chose which data should be trained on next to best preserve the behaviour of experts is a general problem with multi-task learning. Distillation treats all tasks equally independent of their reward. This can result in very low value tasks, receiving potentially more distribution than desired and high value tasks receiving not enough. We have not needed the use a one-hot vector to indicate what task the agent is performing. We want the agent to be able to recognize which task it is being given but we do realize that some tasks could be too similar to differentiate, such as, walking vs jogging on flat ground.
6.2 FUTURE WORK:
It would be interesting to develop a method to prioritize tasks during the distillation step. This could assist the agent with forgetting issues or help with relearning tasks. While we currently use the Mean Squared Error (MSE) to pull the distributions of student policies in line with expert polices for distillation, better distance metrics would likely be helpful. Previous methods have used KL Divergence in the discrete action space domain where the state-action value function encodes the policy, e.g., as with Deep Q-Network (DQN). In this work we do not focus on producing the best policy from a mixture of experts, but instead we match the distributions from a number of experts. The difference is subtle but in practice it can be more difficult to balance many experts with respect to their reward functions. It could also be beneficial to use a KL penalty while performing distillation, i.e., something similar to the work in (Teh et al., 2017) in order to keep the policy from changing too rapidly during training.
7 CONCLUSION
We have proposed and evaluated a method for the progressive learning and integration (via distillation) of motion skills. The method exploits transfer learning to speed learning of new skills, along with input injection where needed, as well as continuous-action distillation, using DAGGER-style learning. This compares favorably to baselines consisting of learning all skills together, or learning all the skills individually before integration. We believe that there remains much to learned about the best training and integration methods for movement skill repertoires, as is also reflected in the human motor learning literature.
REFERENCES
Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253­279.
Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. (2015). Scheduled sampling for sequence prediction with recurrent neural networks. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28, pages 1171­1179. Curran Associates, Inc.
Chen, T., Goodfellow, I., and Shlens, J. (2015). Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641.
Devin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine, S. (2017). Learning modular neural network policies for multi-task and multi-robot transfer. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 2169­2176. IEEE.
8

Under review as a conference paper at ICLR 2018
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400.
Heess, N., Wayne, G., Tassa, Y., Lillicrap, T. P., Riedmiller, M. A., and Silver, D. (2016). Learning and transfer of modulated locomotor controllers. CoRR, abs/1610.05182.
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521­3526.
Kulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenenbaum, J. (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing Systems 29, pages 3675­3683.
Lamb, A. M., ALIAS PARTH GOYAL, A. G., Zhang, Y., Zhang, S., Courville, A. C., and Bengio, Y. (2016). Professor forcing: A new algorithm for training recurrent networks. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems 29, pages 4601­4609. Curran Associates, Inc.
Li, Z. and Hoiem, D. (2016). Learning without forgetting. CoRR, abs/1606.09282.
Martinez, J., Black, M. J., and Romero, J. (2017). On human motion prediction using recurrent neural networks. CoRR, abs/1705.02445.
Merel, J., Tassa, Y., Srinivasan, S., Lemmon, J., Wang, Z., Wayne, G., and Heess, N. (2017). Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201.
Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345­1359.
Parisotto, E., Ba, J. L., and Salakhutdinov, R. (2015). Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342.
Peng, X. B., Berseth, G., Yin, K., and Van De Panne, M. (2017). Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (TOG), 36(4):41.
Peng, X. B. and van de Panne, M. (2016). Learning locomotion skills using deeprl: Does the choice of action space matter? CoRR, abs/1611.01055.
Rajendran, J., Lakshminarayanan, A. S., Khapra, M. M., Prasanna, P., and Ravindran, B. (2015). Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain. arXiv preprint arXiv:1510.02879.
Roemmich, R. T. and Bastian, A. J. (2015). Two ways to save a newly learned motor pattern. Journal of neurophysiology, 113(10):3519­3530.
Ross, S., Gordon, G. J., and Bagnell, J. A. (2010). No-regret reductions for imitation learning and structured prediction. CoRR, abs/1011.0686.
Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu, K., and Hadsell, R. (2015). Policy distillation. arXiv preprint arXiv:1511.06295.
Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016). Progressive Neural Networks. arXiv.
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations (ICLR 2016).
Shin, H., Lee, J. K., Kim, J., and Kim, J. (2017). Continual learning with deep generative replay. arXiv preprint arXiv:1705.08690.
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic policy gradient algorithms. In ICML.
Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057­1063.
9

Under review as a conference paper at ICLR 2018
Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633­1685.
Teh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R. (2017). Distral: Robust multitask reinforcement learning. arXiv preprint arXiv:1707.04175.
Tessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2016). A Deep Hierarchical Approach to Lifelong Learning in Minecraft. arXiv, pages 1­6.
Van Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In Reinforcement Learning, pages 207­251. Springer.
Wolpert, D. M. and Flanagan, J. R. (2016). Computations underlying sensorimotor learning. Current opinion in neurobiology, 37:7­11.
8 APPENDIX
8.1 NETWORK MODELS
We used two different Network models for the experiments in this paper. The first model is a blind model that does not have any terrain features. The blind policy is a Neural Network with 2 hidden layers (512 × 256) with ReLU activations. The output layer of the policy network has linear activations. The network used for the value function has the same design except there is 1 output on the final layer. This design is used for the flat and incline tasks.
We augment the blind network design by adding features for terrain to create an agent with sight. This network with terrain features has a single convolution layer with 8 filters of width 3. This constitutional layer is followed by a dense layer of 32 units. The dense layer is then concatenated twice, once along each of the original two hidden layers in the blind version of the policy.
8.2 HYPER PARAMETERS AND TRAINING
The policy network models a Gaussian distribution by outputting a state dependant mean. We use a state independent standard deviation that normalized with respect to the action space and multiplied by 0.1. We also use a version of epsilon greedy exploration where with probability an exploration action is generated. For all of our experiments we linearly anneal from 0.2 to 0.1 in 100, 000 iterations and leave it from that point on. Each training simulation takes approximately 5 hours across 8 threads. For network training we use Stochastic Gradient Decent (SGD) with momentum. During the distillation step we use gradually anneal the probability of selecting an expert action from 1 to 0 over 10, 000 iterations.
For the evaluation of each model on a particular task we use the average reward achieved by the agent over at most 100 seconds of simulation time. We average this over running the agent over a number of randomly generated simulation runs.
8.3 INPUT FEATURE INJECTION
In order to add input features to network we construct a new network. This new network has a portion of it that is the same design as the previous network plus additional parameters. First initialize the new network with random parameters. Then we copy over the values from the previous network into the new one for the portion of the network design the matches the old. Then the weight for the layers that connect the old portion of the network to the new are set to 0. This will allow the network to preserve the previous distribution it modeled. Having the parameters from the old network will also help generate gradients to train the new 0 valued network parameters.
8.4 AGENT DESIGN
The agent used in the simulation models the dimensions and masses of the average adult. The size of the character state is 50 parameters that include the relative position and velocity of the links in the agent. The action space consists of 11 parameters that indicate target joint positions for the agent. The target joint positions (pd-targets) are turned into joint torques via proportional derivative controllers at each joint.
The reward function for the agent consists of 3 primary terms. The first is a velocity term the rewards the agent for going at velocity of 1 m/s The second term is the difference between the pose of the agent and the current pose of a kinematic character controlled via a motion capture clip. The difference between the agent and the clip consists of the rotational difference between each corresponding joint and the difference
10

Under review as a conference paper at ICLR 2018
in angular velocity. The angular velocity for the clip is approximated via finite differences between the current pose of the clip and it's last pose. The last term is an L2 penalty on the torques generated by the agent to help reduce spastic motions. We also impose torque limits on the joints to reduce unrealistic behaviour, limits: Hips 150, knees 125, ankles 100, shoulders 100, elbows 75 and neck 50 N/m. Terrain Types All terrain types are randomly generated per episode, except for the flat terrain. The incline terrain is slanted and the slant of the terrain is randomly sampled between 20 and 25 degrees. The steps terrain consists of flat segments with widths randomly sampled from 1.0 m to 1.5 m followed by sharp steps that have randomly generated heights between 5 cm and 15 cm. The slopes terrain is randomly generated by updating the slope of the previous point in the ground with a value sampled from -20 and 20 degrees to generate a new portion of the ground every 10 cm. The gaps terrain generate gaps of width 25 - 30 cm separated by flat segments of widths sampled from 2.0 m to 2.5 m. The mixed terrain is a combination of the above terrains where a portion is randomly chosen from the above terrain types. 8.5 MULTITASKER In certain cases the MultiTasker can learn new task faster than PLAID. In Figure 6a we present the MultiTasker and compare it to PLAID. In this case the MultiTasker splits its training time across multiple tasks, here we compare the two methods with respect to the time spent learning on the single new task. This is a good baseline to compare our method against but in some way this is not fair. If the real measure of how efficient a learning method is the number of simulation samples that are needed to learn then the MultiTasker would fall far behind because it needs to train across all tasks to gain the benefits of improving a single task.
(a) Figure 6: (a) Shows that the MultiTasker can learn faster on steps, flat and incline than PLAID (expert) learning the single task steps with TL.
11

