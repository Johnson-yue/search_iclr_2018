Under review as a conference paper at ICLR 2018
CENSORING REPRESENTATIONS WITH MULTIPLEADVERSARIES OVER RANDOM SUBSPACES
Anonymous authors Paper under double-blind review
ABSTRACT
In practice, often there are explicit constraints on the representations that are acceptable in a machine learning application in the real world; for example, representations of data must not contain identifying information so as to avoid privacy issues. This paper improves the performance of the recently proposed adversarial feature learning approach (AFL) for incorporating such explicit constrains, by introducing the concept of the vulnerableness of the adversary. In AFL, the censoring representation is done by training the networks to deceive the adversary that try to predict the sensitive information from the network, and the success of the AFL relies on the choice of the adversary. This motivate the use of high capacity networks as an adversary for improving the performance; however, this approach does not work well in practice as reported in this paper. Instead of the capacity of networks, this paper proposes to consider the vulnerableness in design of the adversary, i.e., the adversary should be designed not to be easily fooled. We also propose a method multiple adversaries over random subspaces (MARS) that instantiate the concept, and provides empirical validations on the efficacy of the proposed method compared to the various baselines, indicating the importance of the proposed concept. This is significant because it gives new implications about designing the adversary, which is important to improve the performance of AFL framework.
1 INTRODUCTION
Since its invention over 10 years ago (Hinton et al., 2006), deep neural networks (DNN) have shown significant performance improvements in various fields. When we apply DNN or more generally machine learning techniques to real-world data, one of the key challenges is how to systematically incorporate the desired constraints into the learned representations in a controllable manner. For example, when these techniques are applied to the data that contain a lot of user information (such as images with user name (Edwards & Storkey, 2016) or data of wearables (Iwasawa et al., 2017)), the desired representations should not contain user information that may result in privacy issues. Moreover, for legal and ethical reasons, machine learning algorithms have to make fair decisions, which do not rely on sensitive variables such as gender, age, or race (Louizos et al., 2016; Edwards & Storkey, 2016). This requires removal of information related to specific factors (such as user ID, race etc.) from the representation; this is called censoring representations in this paper.
One of the recently proposed approaches for censoring representation is adversarial feature learning (AFL) (Edwards & Storkey, 2016; Iwasawa et al., 2017; Xie et al., 2017), which employs the adversarial training framework to constrain the representations (Figure 1-a). Specifically, AFL considers an adversarial classifier who attempts to predict sensitive variables from the representations of a DNN, and simultaneously trains the DNN to deceive the classifier. By alternatively or jointly (using gradient reversal layer proposed by Ganin & Lempitsky (2015)) training the adversary and DNN in such a manner, AFL ensures that there is little or no information about the sensitive variables in the representations.
Although some previous studies report significant performance improvements of the AFL in the context of censoring representations, the success of the AFL depends on the choice of the adversarial classifier. In other words, AFL can only eliminate the information that the adversary can correctly predict. For example, if we use a logistic regression as the adversarial classifier, AFL can only
1

Under review as a conference paper at ICLR 2018

X R Classifier

S0 ~ S1 ~

Encoder E

M L (Y, R) Discriminator

S0 ~

X

S1 ~

R1 R2 M L (Y, R)
E D1 V (S, R1)

... ... ... ...
... ... ...

SL ~

D V (S, R) SL ~

D2 V (S, R2) MARS

(a) AFL with an adversary

(b) Multiple Adversaries over Random Subspaces

Figure 1: Multiple Adversaries over Random Subspaces (MARS) for censoring representations: (a)AFL with an adversary, (b) AFL with MARS. MARS is designed to have diverse-view and to be less vulnerable to meaningless update of the encoder E.
eliminate the information that is linearly separated in the representation spaces and cannot eliminate any non-linear dependency. This motivates the use of high-capacity networks as an adversary, such as DNNs for removing complex information more correctly; however, this approach does not work well in practice as reported in (Iwasawa et al., 2017) and this paper. This implies that only the capacity of the adversary is not a determinant factor of the success of adversarial feature learning; it is still unclear what is required for improving the quality of AFL.
In this paper, we propose to consider the vulnerableness in the design of the adversary, and verify the importance of vulnerableness by proposing multiple-adversaries over random subspace (MARS) that instantiate the concept. The term vulnerableness indicates how the adversarial classifier is easy to be fooled when updating the target networks (encoder in Figure 1-a) to deceive the adversary. We assume that the AFL will fail if an adversary is too easy to be fooled, even if it have enough capacity for predicting the sensitive variables. The proposed method, MARS, is designed to be less vulnerable, by incorporating multiple adversaries where each adversary tries to predict sensitive variables from a different subset of the representations. This makes adversary have diverse view and therefore less vulnerable to the update of the encoder as the encoder need to deceive a set of diverse adversaries. In this paper, we validates the importance of vulnerableness for improving the performance of AFL, by empirically showing that (1) MARS archives better performance compared to baselines (that uses a single adversary and multiple adversaries over the entire representation spaces), and (2) MARS improve the vulnerableness compared to the baseline.
The primary contributions of this paper are as follows:

· This is the first study that introduces the concept of vulnerableness in AFL, and empirically validates the importance of the concept by showing the superior performance of the proposed method that instantiate the concept of vulnerableness. This is significant because it gives implications about designing the adversary in AFL, which is important in practice.
· The proposed method achieved significant performance gain in the task of censoring representations. The empirical validation using three user-anonymization tasks shows that the proposed method allows the learning of significantly more anonymized representations with negligible performance degradation. Specifically, the probability of correctly predicting the user ID from learned representations is 0.124 and 0.112 points better on average than that of a single adversary and multiple adversaries over entire representation spaces, respectively.

2 PROBLEM DEFINITION AND RELATED WORKS

2.1 PROBLEM DEFINITION: CENSORING REPRESENTATIONS
Censoring representation means obtaining unbiased features. Here, unbiased features are features that are less affected by S, where S is a random variable that we want to remove from the data for some reason. One typical reason is related to fairness or privacy, which requires the output of neural networks not to be affected by unfair information or not contain user information.
It should be noted that poor design of the censoring procedure significantly reduces the utility of data. For example, the output of random mapping frand apparently has no information about S,

2

Under review as a conference paper at ICLR 2018
but it also gives no information about target Y . Alternatively, as a more realistic example, a neural network with limited capacity possibly acquires less information about S, but it may also result in poorer performance. Therefore, the primary goal of censoring representation is to obtain an encoder E that reduces information about S, while maintaining information about Y . Formally, the task can be written as a joint optimization problem of the loss:
J = L(E(X), Y ) - V (E(X), S),
where X indicates the input random variable, E is an encoder that transforms X to representation R,  is the weighting parameter, and V and L are loss functions that represent how much information about S and Y is present, respectively. Note that S can be any form of variable such as binary variable, categorical variable, or continuous variable. In this paper, we especially consider a special variant of censoring representation tasks, where we learn E with deep neural networks and S is the user ID (anonymization tasks).
2.2 RELATED WORKS: ADVERSARIAL FEATURE LEARNING
A recently proposed approach for the censoring representation task is adversarial feature learning (AFL). In AFL, V (E(X), S) is measured by an external neural network D, i.e., if the external networks can accurately predict S from R, AFL regards that R has too much information about S and if it is difficult to accurately predict S, AFL regards that R has little or no information about S. The external network D is called discriminator or adversary in this context. The information is used to update the weights of the encoder E so that the updated representations have less information about S. Formally, AFL solves the joint optimization problems:
min max E[- log qM (y|h = E(x)) +  log qD(s|h = E(x))],
E,M D
where M : R  Y is the label classifier, D : R  S is the adversarial classifier (see Figure 1), and qM and qD are conditional distributions parameterized by M and D, respectively. The optimization procedure is called adversarial training because E and D are in an adversarial relationship (E tries to minimize of log-likelihood qD and D tries to maximize it).
To the best of the authors knowledge, the adversarial training framework was first introduced by (Schmidhuber, 1992), and later re-invented by (Goodfellow et al., 2014) in the context of an imagegeneration task. In the context of censoring representations, Edwards & Storkey (2016) first proposed the use of adversarial training to remove sensitive information from the representations. They show its efficacy for fair classification tasks (S is binary) compared to an LFR (Learned Fair Representation) proposed by Zemel et al. (2013) that regularizes the l1 distance between the distributions for data with different S. Iwasawa et al. (2017) first applied the adversarial training to learn anonymized representations on the data of wearables, where S is categorical. More recently, Xie et al. (2017) first introduced the notion of adversarial feature learning and showed superior performances compared to variational fair auto-encoder (Louizos et al., 2016).
Although AFL already shows state-of-the-art performance to learn unbiased representations, how to improve its performance is still a challenge, which is tackled in this paper. This work is motivated by previous studies conducted by Iwasawa et al. (2017). They reported that simply using highcapacity networks as an adversary (such as deep neural networks) sometimes fails to improve the performance; on the other hand, the leaned representations are still highly biased if the adversary does not have enough capacity. Our work can be regarded as introduction of the novel design consideration (i.e., vulnerableness) of adversary for improving the quality of AFL and validation of it by proposing a method that instantiate the concept, which is not done in the previous works.
It is worth mentioning that the concept of multiple adversaries itself is already proposed and verified in the context of the image generation with adversarial training (Durugkar et al., 2017), and this paper does not insist that using multiple adversaries is the primal novelty. From the methodological perspective, the proposed method (MARS) can be seen as an extension of the concept of multiple adversaries by introducing the concept of diversity, and we verify that this extension is important in the context of censoring representations. Though the primary focus of this paper is limited to the context of adversarial feature learning, we further discuss the applicability of the proposed method on the other application of adversarial training, such as image/text generation (Goodfellow et al., 2014; Zhang et al., 2017) or domain adaptation(Ganin & Lempitsky, 2015); this is done at the end of this paper.
3

Under review as a conference paper at ICLR 2018

3 MULTIPLE-ADVERSARIES OVER RANDOM SUBSPACES
The proposed method, multiple-adversaries over random subspaces (MARS), considers multiple adversaries where each adversary is in charge of different subsets of features. The development of MARS is motivated by the assumption that the adversary should not be vulnerable and it could be achieved by enforcing the adversary to have diverse view. Figure 1 shows the comparison of overall architectures of the proposed method between AFL with an adversary, and with multiple adversaries over random subspaces (i.e., proposed method). Since the primary difference between naive AFL and MARS is how to build the adversary (training adversaries) and how to use the information of adversary (training encoders), the explanation is focused on the part.

3.1 TRAINING ADVERSARIES

There are many possible options to select a subspace; however, this paper specifically considers
the case where each subspace is randomly selected. Suppose that n is the number of dimensions of representation R  Rn, and K is the number of adversaries. In MARS, each adversary Dk is trained to predict S over a randomly selected subset of features Rk, whose dimension is mk and mk < n. Each adversary is trained to maximize the expected log-likelihood. Formally, the optimization problem for Dk is as follows

max
Dk

E[log

qDk (s|hk

=

Subk(E(x)))],

(1)

where Subk is a function that return the subset of R, which is fixed before the training. Specifically,
each Subk determine whether to remove the i-th dimension of R with the probability of . The Dk is the parameter of Dk.

3.2 TRAINING ENCODER

The adversaries are then integrated and a single prediction is made to train the encoder. Formally, the probability distribution of qD(s = si|h = E(x)) is parameterized by an ensembled adversary D:

1 K qD(s = si|h = E(x)) = K qDk (s|hk),
k

(2)

which means, the predictions of adversaries Dk are integrated by averaging the predictions. Note that integration operator F is not essential to be the averaging, it may be the max operator or the
soften version of the max operator proposed in (Durugkar et al., 2017).

The encoder is trained for minimizing the log-likelihood of Eq.2 and negative log-likelihood qM (y|h). Formally,

1 K

min
E ,M

E[-

log

qM

(y|h)

+



log

K

k

qDk (s|hk)],

(3)

where E and M are the parameter of E and M respectively. The overall algorithm is shown in Algorithm 1.

Algorithm 1 Optimization of the proposed model
Require: parameter ,  Initialize Subk for all k  1 · · · K based on  Initialize neural networks {E, M, D1, · · · DK } while training() do Update weights of Dk for all k  1 · · · K (eq. 1) Update weights of E and M (eq. 3) end while

4

Under review as a conference paper at ICLR 2018
4 EXPERIMENTS
4.1 DATASETS
We used two datasets to demonstrate the efficacy of MARS. Both datasets are related to human activity recognition using the data of wearables, where privacy issues have recently been pointed out by (Iwasawa et al., 2017). In both datasets, the task is to learn anonymized representations (R that does not contain user-identifiable information), while maintaining classification performance.
The opportunity recognition dataset (Sagha et al., 2011) consists of sensory data regarding human activity in a breakfast scenario, and the task is to predict the activity performed by a user. A wide variety of body-worn, object-based, and ambient sensors are used (see Figure 1 in Sagha et al. (2011) for more details). Each record consists of 113 real-value sensory readings, excluding time information. We considered two recognition tasks: gesture recognition (Opp-G) and locomotion recognition (Opp-L). Opp-G requires the recognition of 18 class activities 1, while Opp-L requires the recognition of 4 class locomotion-activities, i.e., stand, walk, sit, and lie. Given a sampling frequency of 30 Hz, the sliding window procedure with 30 frames and a 50% overlap produced 57,790 samples. We used data of subjects 2­4 as training/validation sets (90% for training and 10% for validation) and that of subject 1 as test sets.
The USC-HAD dataset (Zhang & Sawchuk, 2012) is a relatively new benchmark dataset, which contains a relatively large number of subjects (14 subjects: 7 males and 7 females). There are 12 activity classes corresponding to the most basic and common activities of people's daily lives 2. MotionNode, which is a 6-DOF inertial measurement unit specially designed for human motion sensing applications, is used to record the outputs from accelerometers that record 6 real sensory values. The sliding window procedure, using 30 frames and a 50% overlap, produced 172,169 samples. We used data of subjects 1­10 as training/validation sets (90% for training and 10% for validation) and that of subjects 11­14 as test sets.
4.2 EXPERIMENTAL SETTING
In all experiments, we parameterized the encoder E by convolutional neural networks (CNN) with three convolution-ReLU-pooling repeats followed by one fully connected layer and M by a logistic regression, in accordance with a previous study (see Figure 2 in Iwasawa et al. (2017) for more details). Every component of the network was trained with the Adam algorithm (learning rate is set to 0.0001) (Kingma & Ba, 2015) (150 epochs). In the optimization, we used the annealing heuristics to weighting parameter  in accordance with (Iwasawa et al., 2017); we linearly increased  during 15 epochs to 135 epochs of training.
To demonstrate the efficacy of the proposed method, we compare the following four methods and its variants: (1) None: w/o adversary (correspond to normal CNN), (2) Adv: w/ a single adversary, (3) MA: w/ multiple adversaries where each adversary tries to predict from the entire space, and (4) MARS: w/ multiple adversaries where each adversary tries to predict from a different subspace of the entire space. Each adversary is parametrized by multi-layer perceptron (MLP) with 800 hidden units. If we need to express the type of adversary, we denote it by a suffix. For example, Adv-LR means an adversary is parametrized by a logistic regression and MA-LR means that each adversary is parametrized by a logistic regression. The hyper-parameter  is set to 1.0, 1.0, 0.1 for Opp-G, OppL, and USC-HAD respectively. Without mentioning otherwise, we set the number of adversaries K as 10 for MA and MARS, and  for determine size of the subset as 0.8.
Following previous works, we evaluated the level of anonymization by training a classifier feva that tries to predict S over learned representations. This means that we regarded that a learned representation is anonymized well if we could not build an accurate classifier over the representations. Unless mentioned otherwise, we use a multi-layer perceptron with 800 hidden units as feva. We later discuss how the choice of classifier for feva and D affects the results by performing empirical evaluations.
1 open door 1, open door 2, close door 1, close door 2, open fridge, close fridge, open dishwasher, close dishwasher, open drawer 1, close drawer 1, open drawer 2, close drawer 2, open drawer 3, close drawer 3, clean table, drink from cup, toggle switch, and Null
2walking forward, walking left, walking right, walking upstairs, walking downstairs, running forward, jumping, sitting, standing, sleeping, elevator up, elevator down
5

Under review as a conference paper at ICLR 2018

Opp-G 1.0

Opp-L 1.0

USC-HAD 1.0

0.8 0.8 0.8

0.6 0.6 0.6

0.4 0.4 0.4

0.2 None Adv-LR Adv

MA MARS

0.2 None Adv-LR Adv MA MARS

0.2 None Adv-LR Adv

(a) Accuracy on predicting s (lower is better)

MA MARS

Opp-G 1.0

Opp-L 1.0

USC-HAD 1.0

0.8 0.8 0.8

0.6 0.6 0.6

0.4 0.4 0.4

0.2 None Adv-LR Adv MA MARS

0.2 None Adv-LR Adv MA MARS
(b) Accuracy on predicting y

0.2 None Adv-LR Adv MA MARS

Figure 2: Comparison of different representations on three anonymization tasks. Anonymized representation should lead to low accuracy on predicting user ID (a) while keeping accuracy on predicting Y (b). The dotted line of each figure shows the performance of None (w/o adversarial training).

4.3 EXPERIMENTAL RESULTS

The results for the three tasks are shown in Figure 2. The first row shows the performance when predicting S (lower is better), and the second row shows the performance when predicting y (higher is better). Each column corresponds to different tasks. The results show that the proposed method, MARS, has the best performance regarding the anonymization level while maintaining the classification performance. Specifically, in terms of the average of all three tasks, MARS's accuracy on S is 0.743 compared to 0.917, 0.908, 0.867 and 0.855 achieved by None, Adv-LR, Adv and MA respectively. Although the classification performance drops slightly for USC-HAD compared to None (from 0.537 to 0.496), it is still better than MA (0.478) that is the 2nd best baseline in accuracy on S; there is no performance degradation for the other two tasks.
Table 1 lists the user classification accuracy when different adversarial classifiers D and evaluator feva are used. In addition to the representations shown in Figure 2, we compared a logistic repression (Adv-LR), DNNs with 400-200 hidden units, ensemble of DNNs (MA-DNN), and ensemble of DNNs over random subspaces (MARS-DNN) for E. Adv2 and Adv5 correspond to the case where we train an adversary with MLP 2 iterations or 5 iterations against one iteration for training encoder E. Adv0.1 and MA0.1 correspond to the results of smaller weighting parameter  (1/10 of the default). For evaluator feva, we tested LR, multi-layer perceptron with 50 hidden units (MLP1), and multi-layer perceptron with 800 hidden units (MLP2). The best performance in each condition (each combination of datasets and feva) is highlighted in underline, and the best performance without significantly harming Y -accuracy (specifically, 5% or less degradation against None) is highlighted in bold. The asterisk denotes that the method droped the performance on Y more than 5%.
We can make the following observations. (1) In general, the representations learned with adversary/adversaries tend to more anonymized; however, a stronger evaluator still accurately predicts S. (2) For almost all conditions, the proposed methods (MARS or MARS-DNN) show the best performance. Note that although MV-DNN sometimes show better performances than MARS, it significantly reduces classification accuracy for Y . Especially, in the case when the evaluator is DNN, only the proposed method can give meaningful performance gain without any performance degradation. (3) Just increasing the iterations of training adversary doesn't pay off, as shown in the poor or no performance improvements of Adv2 or Adv5 against Adv.
In addition to the quantitative evaluation, we also give qualitative evaluation by visualizing the learned representations using t-SNE (Maaten & Hinton, 2008) (Figure 3). Specifically, we randomly sample 1,000 examples from the validation sets, and reduce the dimension of data by using principal component analysis over the output of the E to speed up the computations and suppress some noise. The number of components is selected such that the amount of variance that needs to be explained is greater than 95%. The selected number of components is mentioned on top of each

6

Under review as a conference paper at ICLR 2018

Table 1: Performance comparison against various feva: (a, b, c) Accuracy on predicting s in Opp-G, Opp-L, and USC-HAD datasets respectively, and (d)Accuracy on predicting y in each datasets.

None Adv-LR
Adv Adv2 Adv5 Adv0.1 Adv-DNN
MA MA0.1 MA-DNN
MARS MARS-DNN

(a) Opp-G

LR
0.806 0.704 0.663 0.792 0.721 0.525 0.793 0.578 0.602 0.526 0.494 0.571

MLP1
0.936 0.934 0.852 0.940 0.913 0.766 0.930 0.805 0.791 0.755 0.589 0.767

MLP2
0.971 0.975 0.934 0.977 0.967 0.944 0.976 0.932 0.935 0.924 0.777 0.916

DNN
0.968 0.970 0.981 0.975 0.969 0.945 0.977 0.973 0.939 0.962 0.817 0.939

None
Adv-LR
Adv Adv2 Adv5 Adv0.1 Adv-DNN
MA
MA0.1 MA-DNN
MARS
MARS-DNN

(c) USC-HAD

LR
0.655 0.345 0.325 0.503 0.465 0.435 0.344 0.450 0.334 0.315 0.296 0.364

MLP1
0.741 0.651 0.513 0.614 0.561 0.608 0.592 0.500 0.615 0.446 0.520 0.498

MLP2
0.809 0.779 0.703 0.728 0.676 0.742 0.731 0.695 0.747 0.634 0.669 0.655

DNN
0.808 0.799 0.762 0.747 0.697 0.786 0.776 0.726 0.787 0.701 0.727 0.680

None Adv-LR
Adv Adv2 Adv5 Adv0.1 Adv-DNN
MA MA0.1 MA-DNN
MARS MARS-DNN

(b) Opp-L

LR
0.839 0.745 0.795 0.814 0.803 0.557 0.791 0.651 0.583 0.792 0.515 0.500

MLP1
0.937 0.921 0.915 0.926 0.919 0.778 0.912 0.839 0.749 0.896 0.638 0.615

MLP2
0.970 0.963 0.962 0.969 0.967 0.941 0.969 0.939 0.905 0.952 0.791 0.781

DNN
0.966 0.961 0.959 0.971 0.971 0.954 0.970 0.952 0.910 0.962 0.850 0.866

(d) Accuracy on predicting y

None Adv-LR
Adv Adv2 Adv5 Adv0.1 Adv-DNN
MA MA0.1 MA-DNN
MARS MARS-DNN

Opp-G
0.796 0.809 0.786 0.767 0.778 0.809 0.784 0.799 0.814 0.777 0.803 0.807

Opp-L
0.828 0.829 0.810 0.811 0.814 0.818 0.814 0.818 0.799 0.814 0.824 0.815

USC
0.537 0.534 0.540 0.446 0.427 0.532 0.532 0.478 0.529 0.500 0.496 0.511

Ave
0.720 0.724 0.712 0.675 0.673 0.720 0.710 0.698 0.714 0.697 0.708 0.711

#Component=146 #Component=170 #Component=107

#Component=34

#Component=4

(a) None

(b) Adv-LR

(c) Adv

(d) MA

(e) MARS

Figure 3: t-SNE visualizations of samples in the Opp-G (original dimension is 400).

figure. Each color (and marker) denotes different users (|S|=3). The results show that the representations learned with multiple adversaries (MA and MARS) provide qualitatively more plausive results; we can see that the data for different users lie in almost similar places in the representations of MA or MARS; however, the representations of None, Adv-LR, and Adv are partially clustered based on the users. Both MA and MARS give representations that are less influenced by the users; however, the representations of MARS seem to lie in more low-dimensional manifold spaces visually and require significantly small number of components to represent the learned information, which possibly suggests that MARS eliminates unimportant information to a greater extent.
Figures 4-a and b compares the performance between different K and . Note that  = 0.0 means the method using all feature spaces, which is actually MA. The results shows that accuracy on predicting S is more sensitive to K than that on predicting Y , as shown in Figure 4-a and b. Specifically, we can see the parameter sensitivity curve is almost similar shape (reverse bell curve) among , though the peak is different depending on .
Figures 4-c and d shows the performance of D and the log-likelihood of each discriminator qDk during training. In Figure 4-d, each line represent the loss of Dk before the update of encoder, and dotted lines represent the change of the loss by the update. The shaded area represents the minimum and maximum loss, which means that larger shaded area indicates the higher variety of the Dk. The results demonstrate that: (1) Higher  tends to gives the higher accuracy of D, (2) The influences of

7

Under review as a conference paper at ICLR 2018

Accuracy on S Accuracy on Y Accuracy of D -logqDk(s|hk)

1.0

0.9

0.8

0.7 0.62

0.0 0.5 0.2 0.8
4 6 8 10 12 #Adversary

14

(a) Accuracy on s

0.9

0.8

0.7

0.6 0.52

0.0 0.5 0.2 0.8
4 6 8 10 12 #Adversary

14

(b) Accuracy on y

0.9

0.7

0.5

0.3 0.10

0.0 0.5 0.2 0.8
20 40 60 80 100 120 140
#Epch

(c) Accuracy of D

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.00

0.0 0.8
20 40 60 80 100 120 140
#Epch

(d) - log qDk

Figure 4: Performance comparisons with different hyper-parameters: (a, b) Predicting accuracy on s and y with different K and , (c) The performance of overall discriminator D during training, (d) The negative log-likelihood of each discriminator qDk during training.
the update to each adversary is more varied when  = 0.8 compared to  = 0.0, which implies the adversaries of higher  have more diversity and it make the D less vulnerable against the update.

5 DISCUSSION AND CONCLUSION

This study proposed MARS, which incorporates multiple adversaries where each adversary has a different role, and conducted empirical validations on the efficacy of the proposed method for obtaining censoring representations, specifically user-anonymization for the data of wearables (Iwasawa et al., 2017). Figure 2 shows that MARS gives anonymized features compared to Adv (which relies on a single adversary) and MA (which relies on multiple adversaries over the entire spaces), while maintaining the utility of data. Table 1 compares more variants of each method, and shows the efficacy of the proposed method against many evaluators. In addition to the quantitative evaluations, Figure 3 qualitatively shows that the proposed method provides well anonymized representations. Figure 4-d shows that the each adversary in MARS have diverse role, resulting MARS more robust to the update of E as a whole. All these results show that the proposed method is more effective in removing the influence of a specific factor (user in experiments) compared to the previous methods; this proves the underlying assumption of this study, i.e., the adversary needs to be less vulnerable, and this can be achieved by incorporating random subspace methods.

One of the reason why MARS works well is that the adversary is designed to have diverse-views

by incorporating random subspace methods, resulting the encoder need to be stronger to deceive the

adversary. It is worth mentioning that the capacity or accuracy of the adversary is not the only a

definitive factor that determine the success of the adversarial feature learning, as shown by the supe-

rior

performance

of

MARS

over

MA

that

have

1 1-

times

larger

capacity

of

MARS.

As

mentioned

in the related work section, such knowledge is important to design the adversary in practice, and is

not mentioned in prior studies of adversarial feature learning.

Although this paper focused on the case where the subsets are randomly selected and fixed, this might not be necessary. One of the possible extensions is to determine subsets with more sophisticated ways (e.g., by performing clustering or soft-clustering on the representation spaces after few training iterations), or to learn how to select the subset itself by adding the criterion regarding diversity of adversaries. Also, it might be possible to realize the diversity of adversaries by methods other than subspace selection. One possible way is to constrain weights of two adversaries so that they are orthogonal view, which is used in semi-supervised learning using co-training (Saito et al., 2017), or it might be worth a try to adding different noises for each adversary.

It might be worth mentioning about applicability and implications of MARS for other applications of adversarial training, such as image generation. From the perspective of the applicability, the MARS itself does not rely on any domain-specific settings and is therefore general enough for many applications based on adversarial training. For example, we can build multiple-adversaries upon the subset of feature spaces (maybe not on the image spaces). This makes discriminator have diverseview, so it might be useful for preventing mode collapse that is one of the well-known problems in image-generation with adversarial training. In the context of image-generation, Generative Multi Adversarial Networks proposed by Durugkar et al. (2017), which also use multiple adversaries, shows that multiple adversary is useful for generating better images, and for avoiding mode collapse. It might be interesting to see if enhancing the diversity of discriminators by preparing asymmetric adversaries as with this paper helps to generate better image or to avoid mode collapse better.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the International Conference on Machine Learning (ICML), 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pp. 2672­2680, 2014.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527­1554, 2006.
Yusuke Iwasawa, Kotaro Nakayama, Ikuko Eguchi Yairi, and Yutaka Matsuo. Privacy issues regarding the application of dnns to activity-recognition using wearables and its countermeasures by use of adversarial training. In Proceedings of the IJCAI, pp. 1930­1936, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair auto encoder. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579­2605, 2008.
Hesam Sagha, Sundara Tejaswi Digumarti, Jose´ del R Milla´n, Ricardo Chavarriaga, Alberto Calatroni, Daniel Roggen, and Gerhard Tro¨ster. Benchmarking classification techniques using the opportunity human activity dataset. In Proceedings of the INSS, pp. 36­40. IEEE, 2011.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
Ju¨rgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863­879, 1992.
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning (ICML), pp. 325­333, 2013.
Mi Zhang and Alexander A Sawchuk. USC-HAD: a daily activity dataset for ubiquitous activity recognition using wearable sensors. In Proceedings of the International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp), pp. 1036­1043. ACM, 2012.
Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching for text generation. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
9

