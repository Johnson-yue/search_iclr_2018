Under review as a conference paper at ICLR 2018

UCB EXPLORATION VIA Q-ENSEMBLES
Anonymous authors Paper under double-blind review

ABSTRACT
We show how an ensemble of Q-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the Q-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark.

1 INTRODUCTION
Deep reinforcement learning seeks to learn mappings from high-dimensional observations to actions. Deep Q-learning (Mnih et al. (2015)) is a leading technique that has been used successfully, especially for video game benchmarks. However, fundamental challenges remain, for example, improving sample efficiency and ensuring convergence to high quality solutions. Provably optimal solutions exist in the bandit setting and for small MDPs, and at the core of these solutions are exploration schemes. However these provably optimal exploration techniques do not extend to deep RL in a straightforward way.
Bootstrapped DQN (Osband et al. (2016)) is a previous attempt at adapting a theoretically verified approach to deep RL. In particular, it draws inspiration from posterior sampling for reinforcement learning (PSRL, Osband et al. (2013); Osband and Van Roy (2016)), which has near-optimal regret bounds. PSRL samples an MDP from its posterior each episode and exactly solves Q, its optimal Q-function. However, in high-dimensional settings, both approximating the posterior over MDPs and solving the sampled MDP are intractable. Bootstrapped DQN avoids having to establish and sample from the posterior over MDPs by instead approximating the posterior over Q. In addition, bootstrapped DQN uses a multi-headed neural network to represent the Q-ensemble. While the authors proposed bootstrapping to estimate the posterior distribution, their empirical findings show best performance is attained by simply relying on different initializations for the different heads, not requiring the sampling-with-replacement process that is prescribed by bootstrapping.
In this paper, we design new algorithms that build on the Q-ensemble approach from Osband et al. (2016). However, instead of using posterior sampling for exploration, we use the uncertainty estimates from the Q-ensemble. Specifically, we propose the UCB exploration strategy. This strategy is inspired by established UCB algorithms in the bandit setting and constructs uncertainty estimates of the Qvalues. In this strategy, agents are optimistic and take actions with the highest UCB. We demonstrate that our algorithms significantly improve performance on the Atari benchmark.

2 BACKGROUND

2.1 NOTATION

We model reinforcement learning as a Markov decision process (MDP). We define an MDP as

(S, A, T, R, p0, ), in which both the state space S and action space A are discrete, T : S × A × S 

R+ is the transition distribution, R : S × A  R is the reward function, and   (0, 1] is a discount

factor, and p0 is the initial state distribution. We denote a transition experience as  = (s, a, r, s )

where s  T (s |s, a) and r = R(s, a). A policy  : S  A specifies the action taken after observing

a state. We denote the Q-function for policy  as Q(s, a) := E optimal Q-function corresponds to taking the optimal policy

 t=0

trt|s0

=

s, a0

=

a

.

The

Q(s, a) := sup Q(s, a)


1

Under review as a conference paper at ICLR 2018
and satisfies the Bellman equation
Q(s, a) = Es T (·|s,a) r +  · max Q(s , a ) .
a
2.2 EXPLORATION IN REINFORCEMENT LEARNING
A notable early optimality result in reinforcement learning was the proof by Watkins and Dayan Watkins (1989); Watkins and Dayan (1992) that an online Q-learning algorithm is guaranteed to converge to the optimal policy, provided that every state is visited an infinite number of times. However, the convergence of Watkins' Q-learning can be prohibitively slow in MDPs where greedy action selection explores state space randomly. Later work developed reinforcement learning algorithms with provably fast (polynomial-time) convergence (Kearns and Singh (2002); Brafman and Tennenholtz (2002); Strehl et al. (2006)). At the core of these provably-optimal learning methods is some exploration strategy, which actively encourages the agent to visit novel state-action pairs. For example, R-MAX optimistically assumes that infrequently-visited states provide maximal reward, and delayed Q-learning initializes the Q-function with high values to ensure that each state-action is chosen enough times to drive the value down.
Since the theoretically sound RL algorithms are not computationally practical in the deep RL setting, deep RL implementations often use simple exploration methods such as -greedy and Boltzmann exploration, which are often sample-inefficient and fail to find good policies. One common approach of exploration in deep RL is to construct an exploration bonus, which adds a reward for visiting state-action pairs that are deemed to be novel or informative. In particular, several prior methods define an exploration bonus based on a density model or dynamics model. Examples include VIME by Houthooft et al. (2016), which uses variational inference on the forward-dynamics model, and Tang et al. (2016), Bellemare et al. (2016), Ostrovski et al. (2017), Fu et al. (2017). While these methods yield successful exploration in some problems, a major drawback is that this exploration bonus does not depend on the rewards, so the exploration may focus on irrelevant aspects of the environment, which are unrelated to reward.
2.3 BAYESIAN REINFORCEMENT LEARNING
Earlier works on Bayesian reinforcement learning include Dearden et al. (1998; 1999). Dearden et al. (1998) studied Bayesian Q-learning in the model-free setting and learned the distribution of Qvalues through Bayesian updates. The prior and posterior specification relied on several simplifying assumptions, some of which are not compatible with the MDP setting. Dearden et al. (1999) took a model-based approach that updates the posterior distribution of the MDP. The algorithm samples from the MDP posterior multiple times and solving the Q values at every step. This approach is only feasible for RL problems with very small state space and action space. Strens (2000) proposed posterior sampling for reinforcement learning (PSRL). PSRL instead takes a single sample of the MDP from the posterior in each episode and solves the Q values. Recent works including Osband et al. (2013) and Osband and Van Roy (2016) established near-optimal Bayesian regret bounds for episodic RL. Sorg et al. (2012) models the environment and constructs exploration bonus from variance of model parameters. These methods are experimented on low dimensional problems only, because the computational cost of these methods is intractable for high dimensional RL.
2.4 BOOTSTRAPPED DQN
Inspired by PSRL, but wanting to reduce computational cost, prior work developed approximate methods. Osband et al. (2014) proposed randomized least-square value iteration for linearly-parameterized value functions. Bootstrapped DQN Osband et al. (2016) applies to Q-functions parameterized by deep neural networks. Bootstrapped DQN (Osband et al. (2016)) maintains a Q-ensemble, represented by a multi-head neural net structure to parameterize K  N+ Q-functions. This multi-head structure shares the convolution layers but includes multiple "heads", each of which defines a Q-function Qk.
Bootstrapped DQN diversifies the Q-ensemble through two mechanisms. The first mechanism is independent initialization. The second mechanism applies different samples to train each Q-function. These Q-functions can be trained simultaneously by combining their loss functions with the help of a
2

Under review as a conference paper at ICLR 2018

random mask m  R+K

L=

 Bmini

K k=1

mk

·

(Qk (s,

a;

)

-

yQk )2,

where yQk is the target of the kth Q-function. Thus, the transition  updates Qk only if mk is nonzero. To avoid the overestimation issue in DQN, bootstrapped DQN calculates the target value yQk using the approach of Double DQN (Van Hasselt et al. (2016)), such that the current Qk(·; t) network determines the optimal action and the target network Qk(·; -) estimates the value

yQk

=

r

+



max Qk(s
a

, argmax Qk(s
a

, a; t); -).

In their experiments on Atari games, Osband et al. (2016) set the mask m = (1, . . . , 1) such that all {Qk} are trained with the same samples and their only difference is initialization. Bootstrapped DQN picks one Qk uniformly at random at the start of an episode and follows the greedy action at = argmaxa Qk(st, a) for the whole episode.

3 APPROXIMATING BAYESIAN Q-LEARNING WITH Q-ENSEMBLES

Ignoring computational costs, the ideal Bayesian approach to reinforcement learning is to maintain a posterior over the MDP. However, with limited computation and model capacity, it is more tractable to maintain a posterior of the Q-function. In this section, we first derive a posterior update formula for the Q-function under full exploration assumption and this formula turns out to depend on the transition Markov chain (Section 3.1). The Bellman equation emerges as an approximation of the log-likelihood. This motivates using a Q-ensemble as a particle-based approach to approximate the posterior over Q-function and an Ensemble Voting algorithm (Section 3.2).

3.1 BAYESIAN UPDATE FOR Q

An MDP is specified by the transition probability T and the reward function R. Unlike prior works outlined in Section 2.3 which learned the posterior of the MDP, we will consider the joint distribution over (Q, T ). Note that R can be recovered from Q given T . So (Q, T ) determines a unique MDP. In this section, we assume that the agent samples (s, a) according to a fixed distribution. The corresponding reward r and next state s given by the MDP append to (s, a) to form a transition  = (s, a, r, s ), for updating the posterior of (Q, T ). Recall that the Q-function satisfies the Bellman equation

Q(s, a) = r + Es T (·|s,a)

 max Q(s , a )
a

.

Denote the joint prior distribution as p(Q, T ) and the posterior as p~. We apply Bayes' formula to

expand the posterior:

p~(Q, T | ) = p( |Q, T ) · p(Q, T ) Z( )

p(Q, T ) · p(s |Q, T, (s, a)) · p(r|Q, T, (s, a, s )) · p(s, a) =,
Z( )

(1)

where Z( ) is a normalizing constant and the second equality is because s and a are sampled

randomly from S and A. Next, we calculate the two conditional probabilities in (1). First,

p(s |Q, T, (s, a)) = p(s |T, (s, a)) = T (s |s, a),

(2)

where the first equality is because given T , Q does not influence the transition. Second,

p(r|Q, T, (s, a, s )) = p(r|Q, T, (s, a))

= 1{Q(s,a)=r+·Es T (·|s,a) maxa Q(s ,a )} := 1(Q, T ),

(3)

where 1{·} is the indicator function and in the last equation we abbreviate it as 1(Q, T ). Substi-
tuting (2) and (3) into (1), we obtain the joint posterior of Q and T after observing an additional

randomly sampled transition 

p~(Q, T | ) = p(Q, T ) · T (s |s, a) · p(s, a) · 1(Q, T ).
Z( )

(4)

We point out that the exact Q-posterior update (4) is intractable in high-dimensional RL due to the large space of (Q, T ).

3

Under review as a conference paper at ICLR 2018

3.2 Q-LEARNING WITH Q-ENSEMBLES

In this section, we make several approximations to the Q-posterior update and derive a tractable algorithm. First, we approximate the prior of Q by sampling K  N+ independently initialized Q-functions {Qk}kK=1. Next, we update them as more transitions are sampled. The resulting {Qk} approximate samples drawn from the posterior. The agent chooses the action by taking a majority vote
from the actions determined by each Qk. We display our method, Ensemble Voting, in Algorithm 1.

We derive the update rule for {Qk} after observing a new transition  = (s, a, r, s ). At iteration i, given Q = Qk,i the joint probability of (Q, T ) factors into

p(Qk,i, T ) = p(Q, T |Q = Qk,i) = p(T |Qk,i).

(5)

Substitute (5) into (4) and we obtain the corresponding posterior for each Qk,i+1 at iteration i + 1 as

p~(Qk,i+1, T | )

=

p(T |Qk,i)

·

T (s |s, a) Z( )

·

p(s, a)

·

1(Qk,i+1, T ).

(6)

p~(Qk,i+1| ) = p~(Qk,i+1, T | )dT = p(s, a) · p~(T |Qk,i,  ) · 1(Qk,i+1, T )dT.
TT
We update Qk,i to Qk,i+1 according to
Qk,i+1  argmax p~(Qk,i+1| ).
Qk,i+1

(7) (8)

We first derive a lower bound of the the posterior p~(Qk,i+1| ):

p~(Qk,i+1| ) = p(s, a) · ET p~(T |Qk,i,) 1(Qk,i+1, T )

=

p(s,

a)

·

ET p~(T |Qk,i, )

lim
c+

exp

- c[Qk,i+1(s, a) - r -  Es T (·|s,a) max Qk,i+1(s , a )]2
a

=

p(s,

a)

·

lim
c+

ET p~(T

|Qk,i, )

exp

- c[Qk,i+1(s, a) - r -  Es T (·|s,a) max Qk,i+1(s , a )]2
a

 p(s, a) · lim exp
c+

- c ET p~(T |Qk,i,)[Qk,i+1(s, a) - r -  Es

T (·|s,a) max Qk,i+1(s
a

, a )]2

= p(s, a) · 1 .ET p~(T |Qk,i,)[Qk,i+1(s,a)-r- Es T (·|s,a) maxa Qk,i+1(s ,a )]2=0

(9)

where we apply a limit representation of the indicator function in the third equation. The fourth equation is due to the bounded convergence theorem. The inequality is Jensen's inequality. The last equation (9) replaces the limit with an indicator function.

A sufficient condition for (8) is to maximize the lower-bound of the posterior distribution in (9) by ensuring the indicator function in (9) to hold. We can replace (8) with the following update

Qk,i+1  argmin ET p~(T |Qk,i, )
Qk,i+1

Qk,i+1(s, a) -

r +  · Es

T

(·|s,a)

max
a

Qk,i+1(s

,a )

2.

(10)

However, (10) is not tractable because the expectation in (10) is taken with respect to the posterior

p~(T |Qk,i,  ) of the transition T . To overcome this challenge, we approximate the posterior update by reusing the one-sample next state s from  such that

Qk,i+1  argmin Qk,i+1(s, a) - r +  · max Qk,i+1(s , a ) 2.

Qk,i+1

a

(11)

Instead of updating the posterior after each transition, we use an experience replay buffer B to store
observed transitions and sample a minibatch Bmini of transitions (s, a, r, s ) for each update. In this case, the batched update of each Qk,i to Qk,i+1 becomes a standard Bellman update

Qk,i+1  argmin E(s,a,r,s )Bmini Qk,i+1(s, a) -
Qk,i+1

r +  · max Qk,i+1(s , a )
a

2.

(12)

For stability, Algorithm 1 also uses a target network for each Qk as in Double DQN in the batched update. We point out that the action choice of Algorithm 1 is exploitation only. In the next section,
we propose two exploration strategies.

4

Under review as a conference paper at ICLR 2018

Algorithm 1 Ensemble Voting
1: Input: K  N+ copies of independently initialized Q-functions {Qk}kK=1. 2: Let B be a replay buffer storing transitions for training
3: for each episode do do 4: Obtain initial state from environment s0 5: for step t = 1, . . . until end of episode do 6: Pick an action according to at = MajorityVote({argmaxa Qk(st, a)}Kk=1) 7: Execute at. Receive state st+1 and reward rt from the environment 8: Add (st, at, rt, st+1) to replay buffer B 9: At learning interval, sample random minibatch and update {Qk} 10: end for
11: end for

4 UCB EXPLORATION STRATEGY USING Q-ENSEMBLES

In this section, we propose optimism-based exploration by adapting the UCB algorithms (Auer et al. (2002); Audibert et al. (2009)) from the bandit setting. The UCB algorithms maintain an upper-confidence bound for each arm, such that the expected reward from pulling each arm is smaller than this bound with high probability. At every time step, the agent optimistically chooses the arm with the highest UCB. Auer et al. (2002) constructed the UCB based on empirical reward and the number of times each arm is chosen. Audibert et al. (2009) incorporated the empirical variance of each arm's reward into the UCB, such that at time step t, an arm At is pulled according to

At = argmax r^i,t + c1 ·
i

V^i,t log(t) ni,t

+

c2

·

log(t) ni,t

where r^i,t and V^i,t are the empirical reward and variance of arm i at time t, ni,t is the number of times arm i has been pulled up to time t, and c1, c2 are positive constants.

We extend the intuition of UCB algorithms to the RL setting. Using the outputs of the {Qk} functions, we construct a UCB by adding the empirical standard deviation ~(st, a) of {Qk(st, a)}Kk=1 to the empirical mean µ~(st, a) of {Qk(st, a)}kK=1. The agent chooses the action that maximizes this UCB

at  argmax µ~(st, a) +  · ~(st, a) ,
a

(13)

where   R+ is a hyperparameter.

We present Algorithm 2, which incorporates the UCB exploration. The hyperparemeter  controls the degrees of exploration. In Section 5, we compare the performance of our algorithms on Atari games using a consistent set of parameters.

Algorithm 2 UCB Exploration with Q-Ensembles
1: Input: Value function networks Q with K outputs {Qk}kK=1. Hyperparameter . 2: Let B be a replay buffer storing experience for training.
3: for each episode do 4: Obtain initial state from environment s0 5: for step t = 1, . . . until end of episode do 6: Pick an action according to at  argmaxa µ~(st, a) +  · ~(st, a) 7: Receive state st+1 and reward rt from environment, having taken action at 8: Add (st, at, rt, st+1) to replay buffer B 9: At learning interval, sample random minibatch and update {Qk} according to (12) 10: end for
11: end for

5 EXPERIMENT
In this section, we conduct experiments to answer the following questions: 5

Under review as a conference paper at ICLR 2018
1. does Ensemble Voting, Algorithm 1, improve upon existing algorithms including Double DQN and bootstrapped DQN?
2. is the proposed UCB exploration strategy of Algorithm 2 effective in improving learning compared to Algorithm 1?
3. how does UCB exploration compare with prior exploration methods such as the count-based exploration method of Bellemare et al. (2016)?
We evaluate the algorithms on each Atari game of the Arcade Learning Environment (Bellemare et al. (2013)). We use the multi-head neural net architecture of Osband et al. (2016). We fix the common hyperparameters of all algorithms based on a well-tuned double DQN implementation, which uses the Adam optimizer (Kingma and Ba (2014)), different learning rate and exploration schedules compared to Mnih et al. (2015). Appendix A tabulates the hyperparameters. The number of {Qk} functions is K = 10. Experiments are conducted on the OpenAI Gym platform (Brockman et al. (2016)) and trained with 40 million frames and 2 trials on each game.
We take the following directions to evaluate the performance of our algorithms:
1. we compare Algorithm 1 against Double DQN and bootstrapped DQN,
2. we isolate the impact of UCB exploration by comparing Algorithm 2 with  = 0.1, denoted as ucb exploration, against Algorithm 1.
3. we compare Algorithm 1 and Algorithm 2 with the count-based exploration method of Bellemare et al. (2016).
4. we aggregate the comparison according to different categories of games, to understand when our methods are suprior.
Figure 1 compares the normalized learning curves of all algorithms across Atari games. Overall, Ensemble Voting, Algorithm 1, outperforms both Double DQN and bootstrapped DQN. With exploration, ucb exploration improves further by outperforming Ensemble Voting.
In Appendix B, we tabulate detailed results that compare our algorithms, Ensemble Voting and ucb exploration, against prior methods. In Table 2, we tabulate the maximal mean reward in 100 consecutive episodes for Ensemble Voting, ucb exploration, bootstrapped DQN and Double DQN. Without exploration, Ensemble Voting already achieves higher maximal mean reward than both Double DQN and bootstrapped DQN in a majority of Atari games. ucb exploration achieves the highest maximal mean reward among these four algorithms in 30 games out of the total 49 games evaluated. Figure 2 displays the learning curves of these five algorithms on a set of six Atari games. Ensemble Voting outperforms Double DQN and bootstrapped DQN. ucb exploration outperforms Ensemble Voting.
In Table 3, we compare our proposed methods with the count-based exploration method A3C+ of Bellemare et al. (2016) based on their published results of A3C+ trained with 200 million frames. We point out that even though our methods were trained with only 40 million frames, much less than A3C+'s 200 million frames, UCB exploration achieves the highest average reward in 28 games, Ensemble Voting in 10 games, and A3C+ in 10 games. Our approach outperforms A3C+.
Finally to understand why and when the proposed methods are superior, we aggregate the comparison results according to four categories: Human Optimal, Score Explicit, Dense Reward, and Sparse Reward. These categories follow the taxonomy in Table 1 of Ostrovski et al. (2017). Out of all games evaluated, 23 games are Human Optimal, 8 are Score Explicit, 8 are Dense Reward, and 5 are Sparse Reward. The comparison results are tabulated in Table 4, where we see ucb exploration achieves top performance in more games than Ensemble Voting, Double DQN, and Bootstrapped DQN in the categories of Human Optimal, Score Explicit, and Dense Reward. In Sparse Reward, both ucb exploration and Ensemble Voting achieve best performance in 2 games out of total of 5. Thus, we conclude that ucb exploration improves prior methods consistently across different game categories within the Arcade Learning Environment.
6

Under review as a conference paper at ICLR 2018

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
0.0

Average Normalized Learning Curve

0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

bootstrapped dqn ucb exploration ensemble voting double dqn

Figure 1: Comparison of algorithms in normalized learning curve. The normalized learning curve is calculated as follows: first, we normalize learning curves for all algorithms in the same game to the interval [0, 1]; next, average the normalized learning curve from all games for each algorithm.

60000 DemonAttack

3000

Enduro

16000 Kangaroo

50000 40000

2500 14000 12000
2000 10000

30000 1500 8000

20000 1000 6000

4000

10000

500 2000

000 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

Frames

1e7

Frames

1e7

16000 Riverraid 25000 Seaquest 20000 UpNDown

14000 12000

20000

15000

10000 8000 6000

15000 10000

10000

4000 5000 5000 2000

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

double dqn
bootstrapped dqn ensemble voting ucb exploration

Figure 2: Comparison of UCB Exploration and Ensemble Voting against Double DQN and Bootstrapped DQN.

7

Under review as a conference paper at ICLR 2018
6 CONCLUSION
We proposed a Q-ensemble approach to deep Q-learning, a computationally practical algorithm inspired by Bayesian reinforcement learning that outperforms Double DQN and bootstrapped DQN, as evaluated on Atari. The key ingredient is the UCB exploration strategy, inspired by bandit algorithms. Our experiments show that the exploration strategy achieves improved learning performance on the majority of Atari games.
REFERENCES
Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. Exploration­exploitation tradeoff using variance estimates in multi-armed bandits. Theor. Comput. Sci., 410(19):1876­1902, 2009.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Mach. Learn., 47(2-3):235­256, 2002.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, pages 1471­1479, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res., 47:253­279, 2013.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. J. Mach. Learn. Res., 3(Oct):213­231, 2002.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian Q-learning. In AAAI/IAAI, pages 761­768, 1998.
Richard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In UAI, pages 150­159, 1999.
Justin Fu, John D Co-Reyes, and Sergey Levine. EX2: Exploration with exemplar models for deep reinforcement learning. arXiv preprint arXiv:1703.01260, 2017.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME: Variational information maximizing exploration. In NIPS, pages 1109­1117, 2016.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Mach. Learn., 49(2-3):209­232, 2002.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning. arXiv preprint arXiv:1607.00215, 2016.
Ian Osband, Dan Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via posterior sampling. In NIPS, pages 3003­3011, 2013.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. arXiv preprint arXiv:1402.0635, 2014.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In NIPS, pages 4026­4034, 2016.
8

Under review as a conference paper at ICLR 2018
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
Jonathan Sorg, Satinder Singh, and Richard L Lewis. Variance-based rewards for approximate bayesian reinforcement learning. arXiv preprint arXiv:1203.3518, 2012.
Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model-free reinforcement learning. In ICML, pages 881­888. ACM, 2006.
Malcolm Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943­950, 2000. Yi Sun, Faustino Gomez, and Jürgen Schmidhuber. Planning to be surprised: Optimal Bayesian
exploration in dynamic environments. In ICAGI, pages 41­51. Springer, 2011. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. # Exploration: A study of count-based exploration for deep reinforcement learning. arXiv preprint arXiv:1611.04717, 2016. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Qlearning. In AAAI, pages 2094­2100, 2016. Christopher JCH Watkins and Peter Dayan. Q-learning. Mach. Learn., 8(3-4):279­292, 1992. Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989.
9

Under review as a conference paper at ICLR 2018

A HYPERPARAMETERS
We tabulate the hyperparameters in our well-tuned implementation of double DQN in Table 1:

hyperparameter

value

total training frames 40 million

descriptions Length of training for each game.

minibatch size

32

Size of minibatch samples for each parameter update.

replay buffer size

1000000

The number of most recent frames stored in replay buffer.

agent history length 4

The number of most recent frames concatenated as input to the Q network. Total number of iterations = total training frames / agent history length.

target network update 10000 frequency

The frequency of updating target network, in the number of parameter updates.

discount factor

0.99

Discount factor for Q value.

action repeat

4

Repeat each action selected by the agent this many times. A value of 4 means the agent sees every 4th frame.

update frequency

4

The number of actions between successive parameter updates.

optimizer

Adam

Optimizer for parameter updates.

1 0.9 2 0.99
10-4

Adam optimizer parameter. Adam optimizer parameter. Adam optimizer parameter.

learning rate schedule

 10-4

t  106



Interp(10-4, 5  10-5) otherwise

 5  10-5

t > 5  106

Learning rate for Adam optimizer, as a function of iteration t.

exploration schedule

 Interp(1, 0.1) t < 106 

Interp(0.1, 0.01) otherwise

 0.01

t > 5  106

Probability of random action in greedy exploration, as a function of the iteration t .

replay start size

50000

Number of uniform random actions taken before learning starts.

Table 1: Double DQN hyperparameters. These hyperparameters are selected based on performances
of seven Atari games: Beam Rider, Breakout, Pong, Enduro, Qbert, Seaquest, and Space Invaders. Interp(·, ·) is linear interpolation between two values.

10

Under review as a conference paper at ICLR 2018

B RESULTS TABLES

Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar Ice Hockey Jamesbond Kangaroo Krull Kung Fu Master Montezuma Revenge Ms Pacman Name This Game Pitfall Pong Private Eye Qbert Riverraid Road Runner Robotank Seaquest Space Invaders Star Gunner Tennis Time Pilot Tutankham Up N Down Venture Video Pinball Wizard Of Wor Zaxxon
Times best

Bootstrapped DQN 1445.1 430.58 2519.06 3829.0 1009.5
1314058.0 795.1
26230.0 8006.58 28.62 85.91 400.22 5328.77 2153.0 110926.0 9811.45 -10.82 1314.31 21.89 33.57 1284.8 7652.2 227.5
-4.62 594.5 8186.0 8537.52 24153.0
2.0 2508.7 8212.4 -5.99 21.0 1815.19 10557.25 11528.0 52489.0 21.03 9320.7 1549.9 20115.0 -15.11 5088.0 167.47 9049.1 115.0 364600.85 2860.0 592.0
1

Double DQN 2059.7 667.5 2820.61 7639.5 1002.3
1982677.0 789.9
24880.0 7743.74
30.92 94.07 467.45 5177.51 3260.0 124456.0 23562.55 -14.58 1439.59 23.69 32.93 529.2 12030.0 279.5 -4.63 594.0 7787.0 8517.91 32896.0
4.0 2498.1 9806.9 -7.57 20.67 788.63 6529.5 11834.7 49039.0
29.8 18056.4 1917.5 52283.0 -14.04 5548.0 223.43 11815.3
96.0 374686.89
3877.0 8903.0
7

Ensemble Voting 2282.8 683.72 3213.58 8740.0 1149.3
1786305.0 869.4 27430.0 7991.9 32.92 94.47 426.78 6153.28 3544.0
126677.0 30004.4 -11.94 1999.88
30.02 33.92 1196.0 10993.2 371.5 -1.73 602.0 8174.0 8669.17 30988.0
1.0 3039.7 9255.1 -3.37 21.0 1845.28 12036.5 12785.8 54768.0 31.83 20458.6 1890.8 41684.0 -11.63 6153.0 208.61 19528.3 78.0 343380.29 5451.0 3901.0
9

UCB-Exploration 2817.6 663.8 3702.76 8732.0 1007.8
2016145.0 906.9
26770.0 9188.26
38.06 98.08 411.31 6237.18 3677.0 127754.0 59861.9 -4.08 2752.55 29.71 33.96 1903.0 12910.8 318.0 -4.71 710.0 14196.0 9171.61 31291.0 4.0 3425.4 9570.5 -1.47 20.95 1252.01 14198.25 15622.2 53596.0 41.04 24001.6 2626.55 47367.0 -7.8 6490.0 200.76 19827.3 67.0 372564.11 5873.0 3695.0
30

Table 2: Comparison of maximal mean rewards achieved by agents. Maximal mean reward is calculated in a window of 100 consecutive episodes. Bold denotes the highest value in each row.

11

Under review as a conference paper at ICLR 2018

Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar Ice Hockey Jamesbond Kangaroo Krull Kung Fu Master Montezuma Revenge Ms Pacman Name This Game Pitfall Pong Private Eye Qbert Riverraid Road Runner Robotank Seaquest Space Invaders Star Gunner Tennis Time Pilot Tutankham Up N Down Venture Video Pinball Wizard Of Wor Zaxxon
Times Best

Ensemble Voting 2282.8 683.72 3213.58 8740.0 1149.3
1786305.0 869.4 27430.0 7991.9 32.92 94.47 426.78 6153.28 3544.0
126677.0 30004.4 -11.94 1999.88
30.02 33.92 1196.0 10993.2 371.5 -1.73 602.0 8174.0 8669.17 30988.0
1.0 3039.7 9255.1 -3.37 21.0 1845.28 12036.5 12785.8 54768.0 31.83 20458.6 1890.8 41684.0 -11.63 6153.0 208.61 19528.3 78.0 343380.29 5451.0 3901.0
10

UCB-Exploration 2817.6 663.8 3702.76 8732.0 1007.8
2016145.0 906.9 26770.0 9188.26 38.06 98.08 411.31 6237.18 3677.0
127754.0 59861.9
-4.08 2752.55 29.71 33.96 1903.0 12910.8 318.0
-4.71 710.0 14196.0 9171.61 31291.0
4.0 3425.4 9570.5 -1.47 20.95 1252.01 14198.25 15622.2 53596.0 41.04 24001.6 2626.55 47367.0
-7.8 6490.0 200.76 19827.3 67.0 372564.11 5873.0 3695.0
28

A3C+ 1848.33 964.77 2607.28 7262.77 2257.92 1733528.71 991.96 7428.99 5992.08
68.72 13.82 323.21 5338.24 5388.22 104083.51 19589.95 -8.88 749.11 29.46 27.33 506.61 5948.40 246.02 -7.05 1024.16 5475.73 7587.58 26593.67 142.50 2380.58 6427.51 -155.97 17.33 100.0 15804.72 10331.56 49029.74 6.68 2274.06 1466.01 52466.84 -20.49 3816.38 132.67 8705.64 0.00 35515.92 3657.65 7956.05
10

Table 3: Comparison of Ensemble Voting, UCB Exploration, both trained with 40 million frames and A3C+ of Bellemare et al. (2016), trained with 200 million frames

12

Under review as a conference paper at ICLR 2018

Category Human Optimal Score Explicit Dense Reward Sparse Reward

Total 23 8 8 5

Bootstrapped DQN 0 0 0 1

Double DQN 3 2 1 0

Ensemble Voting 5 1 1 2

UCB-Exploration 15 5 6 2

Table 4: Comparison of each method across different game categories. The Atari games are separated into four categories: human optimal, score explicit, dense reward, and sparse reward. In each row, we present the number of games in this category, the total number of games where each algorithm achieves the optimal performance according to Table 2. The game categories follow the taxonomy in Table 1 of Ostrovski et al. (2017)

C INFOGAIN EXPLORATION

In this section, we also studied an "InfoGain" exploration bonus, which encourages agents to gain information about the Q-function and examine its effectiveness. We found it had some benefits on top of Ensemble Voting, but no uniform additional benefits once already using Q-ensembles on top of Double DQN. We describe the approach and our experimental findings here.

Similar to Sun et al. (2011), we define the information gain from observing an additional transition

n as

Ht|1,...,n-1 = DKL(p~(Q|1, . . . , n)||p~(Q|1, . . . , n-1))

where p~(Q|1, . . . , n) is the posterior distribution of Q after observing a sequence of transitions

(1, . . . , n). The total information gain is

H1,...,N =

N
n=1 Hn|1,...,n-1 .

(14)

Our Ensemble Voting, Algorithm 1, does not maintain the posterior p~, thus we cannot calculate (14)

explicitly. Instead, inspired by Lakshminarayanan et al. (2016), we define an InfoGain exploration bonus that measures the disagreement among {Qk}. Note that

H1,...,N + H(p~(Q|1, . . . , N )) = H(p(Q)),

where H(·) is the entropy. If H1,...,N is small, then the posterior distribution has high entropy and high residual information. Since {Qk} are approximate samples from the posterior, high entropy of the posterior leads to large discrepancy among {Qk}. Thus, the exploration bonus is monotonous with respect to the residual information in the posterior H(p~(Q|1, . . . , N )). We first compute the
Boltzmann distribution for each Qk

PT,k(a|s) =

exp Qk(s, a)/T , a exp Qk(s, a )/T

where T > 0 is a temperature parameter. Next, calculate the average Boltzmann distribution

1 PT,avg = K ·

K
k=1 PT,k(a|s).

The InfoGain exploration bonus is the average KL-divergence from {PT,k}kK=1 to PT,avg

bT(s)

=

1 K

·

K
DK L [PT,k ||PT,avg ].
k=1

(15)

The modified reward is r^(s, a, s ) = r(s, a) +  · bT(s),
where   R+ is a hyperparameter that controls the degree of exploration.

(16)

The exploration bonus bT(st) encourages the agent to explore where {Qk} disagree. The temperature parameter T controls the sensitivity to discrepancies among {Qk}. When T  +, {PT,k} converge to the uniform distribution on the action space and bT(s)  0. When T is small, the differences among {Qk} are magnified and bT(s) is large.

We display Algorithrim 3, which incorporates our InfoGain exploration bonus into Algorithm 2. The hyperparameters , T and  vary for each game.

13

Under review as a conference paper at ICLR 2018
Algorithm 3 UCB + InfoGain Exploration with Q-Ensembles 1: Input: Value function networks Q with K outputs {Qk}Kk=1. Hyperparameters T, , and . 2: Let B be a replay buffer storing experience for training. 3: for each episode do 4: Obtain initial state from environment s0 5: for step t = 1, . . . until end of episode do 6: Pick an action according to at  argmaxa µ~(st, a) +  · ~(st, a) 7: Receive state st+1 and reward rt from environment, having taken action at 8: Calculate exploration bonus bT(st) according to (15) 9: Add (st, at, rt +  · bT(st), st+1) to replay buffer B 10: At learning interval, sample random minibatch and update {Qk} 11: end for 12: end for
C.1 PERFORMANCE OF UCB+INFOGAIN EXPLORATION We demonstrate the performance of the combined UCB+InfoGain exploration in Figure 3 and Figure 3. We augment the previous figures in Section 5 with the performance of ucb+infogain exploration, where we set  = 0.1,  = 1, and T = 1 in Algorithm 3. Figure 3 shows that combining UCB and InfoGain exploration does not lead to uniform improvement in the normalized learning curve. At the individual game level, Figure 3 shows that the impact of InfoGain exploration varies. UCB exploration achieves sufficient exploration in games including Demon Attack and Kangaroo and Riverraid, while InfoGain exploration further improves learning on Enduro, Seaquest, and Up N Down. The effect of InfoGain exploration depends on the choice of the temperature T. The optimal temperature parameter varies across games. In Figure 5, we display the behavior of ucb+infogain exploration with different temperature values. Thus, we see the InfoGain exploration bonus, tuned with the appropriate temperature parameter, can lead to improved learning for games that require extra exploration, such as ChopperCommand, KungFuMaster, Seaquest, UpNDown.
14

Under review as a conference paper at ICLR 2018

0.8 Average Normalized Learning Curve

0.7

0.6

0.5 ucb+infogain exploration, T=1

0.4

bootstrapped dqn ensemble voting

ucb exploration

0.3 double dqn

0.2

0.1

0.0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

Figure 3: Comparison of all algorithms in normalized curve. The normalized learning curve is calculated as follows: first, we normalize learning curves for all algorithms in the same game to the interval [0, 1]; next, average the normalized learning curve from all games for each algorithm.

60000 DemonAttack

3000

Enduro

16000 Kangaroo

50000 40000

2500 14000 12000
2000 10000

30000 1500 8000

20000 1000 6000

4000

10000

500 2000

000 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

Frames

1e7

Frames

1e7

16000 Riverraid 30000 Seaquest 25000 UpNDown

14000 12000 10000 8000 6000 4000 2000

25000 20000 15000 10000 5000

20000 15000 10000 5000

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

ucb exploration
bootstrapped dqn
ensemble voting ucb+infogain exploration, T=1 double dqn

Figure 4: Comparison of algorithms against Double DQN and bootstrapped DQN.

15

Under review as a conference paper at ICLR 2018

C.2 UCB+INFOGAIN EXPLORATION WITH DIFFERENT TEMPERATURES

3000 Alien

700 Amidar 30000 BattleZone

2500 600 25000
2000 500 20000 400
1500 15000 300
1000 200 10000
500 100 5000

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

4500 ChopperCommand

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

35 Freeway

0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

-2 IceHocke6

4000 30 -4 3500 3000 25 -6
2500 20 -8

2000 15 -10
1500 10 -12 1000 500 5 -14

0 0 -16

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

Frames

1e7

Frames

1e7

700 Jamesbond 30000 KungFuMaster 30000

Seaquest

600 25000 25000
500 20000 20000 400
15000 15000 300 200 10000 10000
100 5000 5000

000

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frame0

1e7

Frames

1e7

Frames

1e7

2500 SpaceInvaders

7000

TimePilot

25000 UpNDown

6000 2000 20000
5000
1500 4000 15000

1000 3000 10000
2000 500 5000
1000

000

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

Frames

1e7

Frames

1e7

Frames

1e7

ucb-exploration
ucb+infogain exploration, T=1 ucb+infogain exploration, T=50 ucb+infogain exploration, T=100

Figure 5: Comparison of UCB+InfoGain exploration with different temperatures versus UCB exploration.

16

