Under review as a conference paper at ICLR 2018
DEPENDENT BIDIRECTIONAL RNN WITH SUPER-LONG SHORT-TERM MEMORY
Anonymous authors Paper under double-blind review
ABSTRACT
In this work, we first conduct mathematical analysis on the memory decay of three RNN cells; namely, the simple recurrent neural network (SRN), the long shortterm memory (LSTM) and the gated recurrent unit (GRU). Based on the analysis, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell. Next, we present an efficient RNN architecture, called the dependent bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-out (SISO) problem. Finally, the superior performance of the DBRNN architecture with the SLSTM cell is demonstrated by experimental results.
1 INTRODUCTION
The recurrent neural network (RNN) has proved to be an effective solution for natural language processing (NLP) through the advancement in the last three decades (Elman, 1990; Jordan, 1997). At the cell level of a RNN, the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (GRU) (Cho et al., 2014) are often adopted by a RNN as its low-level building cell. Being built upon these cells, various RNN architectures have been proposed to solve the sequence-in-sequence-out (SISO) problem. To name a few, there are the bidirectional RNN (BRNN) (Schuster & Paliwal, 1997), the encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015; Bahdanau et al., 2015) and the deep RNN (Pascanu et al., 2014). Although the LSTM and the GRU were designed to enhance the memory length of RNNs and avoid the gradient vanishing/exploding issue (Hochreiter & Schmidhuber, 1997; Razvan et al., 2013; Bengio et al., 1994), a good understanding of their memory length is still lacking. The first objective of this research is to analyze the memory length of three RNN cells ­ the simple RNN (SRN) (Elman, 1990; Jordan, 1997), the long short-term memory (LSTM) and the gated recurrent unit (GRU). This will be conducted in Sec. 2. Based on this understanding, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell in Sec. 3.
As to the macro RNN architecture, one popular choice is the BRNN. Since the elements in BRNN output sequences should be independent of each other (Schuster & Paliwal, 1997), the BRNN cannot be used to solve dependent output sequence problem alone. Nevertheless, most language tasks do involve dependent output sequences. The second choice is the encoder-decoder system, where the attention mechanism has been introduced (Vinyals et al., 2015; Bahdanau et al., 2015) to improve its performance furthermore. As shown later in this work, the encoder-decoder system is not an efficient learner. Here, to take advantages of both the encoder-decoder and the BRNN and overcome their drawbacks, we propose a new architecture called the dependent bidirectional recurrent neural network (DBRNN), which will be elaborated in Sec. 4. Furthermore, we conduct a series of experiments on the part of speech (POS) tagging and the dependency parsing (DP) problems in Sec. 5 to demonstrate the superior performance of the DBRNN architecture with the SLSTM cell. Finally, concluding remarks are given and future research direction is pointed out in Sec. 6.
2 MEMORY ANALYSIS OF SRN, LSTM AND GRU
For a large number of NLP tasks, we are concerned with finding semantic patterns from the input sequence. It was shown by Elman (1990) that the RNN builds an internal representation of semantic patterns. The memory of a cell characterizes its ability to map an input sequence of certain length
1

Under review as a conference paper at ICLR 2018

into such a representation. It was reported by Gers et al. (2000) that a SRN only memorized sequences of length between 3-5 units while a LSTM could memorize sequences of length longer than 1000 units. In this section, we study the memory of the SRN, LSTM and GRU.

2.1 MEMORY OF SRN

The SRN is described by the following two equations:

ht = Whht-1 + WinXt, Yt = f (ht),

(1) (2)

where subscript t is the index of the time unit, Wh  RN×N is the weight matrix for hidden state vector ht-1  RN , Win  RN×M is the weight matrix of input vector Xt  RM , Yt  RN and f (·) is an element-wise non-linear function. Usually, f (·) is a hyperbolic-tangent or a sigmoid function. Note that we can account for the bias terms by augmenting vectors ht and Xt with one more dimension and adjusting weight matrices Wh and Win accordingly in a straightforward manner. Thus, although the system of equations given in Eqs. (1) and (2) is simple, it is generic.

By induction, ht can be rewritten as

t

ht = Whth0 +

Wht-k Win Xk ,

k=1

(3)

where h0 is the initial internal state of the SRN. Typically, we assume an initial rest state by setting it to be a column vector of zeros; i.e., h0 = 0. Then, Eq. (3) becomes

t

ht =

Wht-k Win Xk .

k=1

(4)

Let max be the largest singular value of Wh. Then, we have

||Wht-kXk||  ||Wh|||t-k|||Xk|| = m|t-axk|||Xk||.

(5)

Hence, the contribution of Xk, k < t, to ht decays in form of m|t-axk|. We conclude that SRN's memory decays exponentially with its memory length |t - k|. Clearly, the memory will explode if
|max| > 1. This means that Wh cannot be chosen arbitrarily.

2.2 MEMORY OF LSTM

Figure 1: The diagram of a LSTM cell.
By following the work of Hochreiter & Schmidhuber (1997), we plot the diagram of a LSTM cell in Fig. 1. In this figure, ,  and  denote the hyperbolic tangent function, the sigmoid function and the multiplication operation, respectively. All of them operate in an element-wise fashion. The
2

Under review as a conference paper at ICLR 2018

LSTM has an input gate, an output gate, a forget gate and a constant error carousal (CEC) module. Mathematically, the LSTM cell can be written as

ht = (Wf It)ht-1 + (WiIt)(WinIt), Yt = (WoIt)(ht),

(6) (7)

where ht  RN , column vector It  R(M+N) is a concatenation of the current input, Xt  RM , and the previous output, Yt-1  RN (i.e., ItT = [XtT , YtT-1]). Furthermore, Wf , Wi, Wo and Win are weight matrices for the forget gate, the input gate, the output gate and the input, respectively.
Their dimensions are defined by their corresponding inputs and outputs.

Under the assumption h0 = 0, the hidden state vector of the LSTM can be derived by induction as

tt

ht =

(Wf Ij) (WiIk)(WinIk),

(8)

k=1 j=k+1

forget gate

where denotes the element-wise multiplication. By setting f (·) in Eq. (2) to a hyperbolic-tangent function, we can compare outputs of the SRN and the LSTM below:

SRN: LSTM:

YtSRN = YtLST M =

t
 Wht-kWinXk ,
k=1

tt

(WoIt)

(Wf Ij)

k=1 j=k+1

 (Wi Ik )(Win Ik )

.

(9) (10)

We see from the above that Wht-k and and the LSTM, respectively.

forget gate

t j=k+1

(Wf Ij)

play

the

same

memory

role

for

the

SRN

If Wf in Eq. (10) is selected such that

min |(Wf Ij)|  max, max  [0, 1),

then

t
(Wf Ij )  m|t-axk|.
j=k+1

(11)

As given in Eqs. (5) and (11), the impact of input Ik on output Yt in the LSTM lasts longer than that of input Xk in the SRN. This is the case if an appropriate weight matrix, Wf , of the forget gate is selected.

2.3 MEMORY OF GRU

The GRU was originally proposed for neural machine translation (Cho et al., 2014). It provides an effective alternative for the LSTM. Its operations can be expressed by the following four equations:

zt = (WzXt + Uzht-1), rt = (WrXt + Urht-1), h~t = (W Xt + U (rt  ht-1)), ht = ztht-1 + (1 - zt)h~t,

(12) (13) (14) (15)

where Xt, ht, zt and rt denote the input vector, the hidden state vector, the update gate vector and the reset gate vector, respectively, and Wz, Wr, W , are trainable weight matrices. Its hidden state is also its output, which is given in Eq. (15). If we simplify the GRU by setting Uz, Ur and U to zero matrices, then we can obtain the following simplified GRU system:

zt = (WzXt), h~t = (W Xt), ht = ztht-1 + (1 - zt)h~t.

(16) (17) (18)

3

Under review as a conference paper at ICLR 2018

For the simplified GRU with the initial rest condition, we can derive the following by induction:

tt

ht =

(WzXj) (1 - (WzXk))(W Xk).

k=1 j=k+1

update gate

(19)

By comparing Eqs. (8) and (19), we see that the update gate of the simplified GRU and the forget gate of the LSTM play the same role. One can control the memory decay behavior of the GRU by choosing the weight matrix, Wz, of the update gate carefully.

3 SUPER-LONG SHORT-TERM MEMORY (SLSTM)
As discussed above, the LSTM and the GRU have longer memory by introducing the forget and the update gates, respectively. However, their memory still fades quickly since their memory decay rate is proportional to the product of a sequence of small numbers. In this section, we attempt to design super-long short-term memory (SLSTM) cells and propose two new cell models:
· SLSTM-I: the super-long short-term memory (SLSTM) with input weight vector ci  RN , i = 1, · · · , t - 1, where weights ci and cj (with i = j) are independent.
· SLSTM-II: the SLSTM-I with no forget gate.
These two cells are depicted in Figs. 2 (a) and (b), respectively.

(a)
(b) Figure 2: The diagrams of (a) the SLSTM-I cell and (b) the SLSTM-II cell.
4

Under review as a conference paper at ICLR 2018

The SLSTM-I cell can be described by

ht = (Wf It)ht-1 + ct(WiIt)(WinIt), Yt = (WoIt)(ht + b).

With h0 = 0, we can get

tt

ht = ck

(Wf Ij) (WiIk)(WinIk),

k=1 j=k+1

(20) (21)
(22)

Yt = (WoIt) ht + b ,

(23)

where b  RN is a trainable bias vector. The SLSTM-II cell can be written as ht = ht-1 + ct(WiIt)(WinIt), Yt = (WoIt)(ht + b).
It can be further simplified as
t
ht = ck(WiIk)(WinIk),
k=1
Yt = (WoIt) ht + b .

(24) (25)
(26) (27)

As shown above, we introduce weight vector, ci, i = 1, · · · , t - 1, to the SLSTM-I and the SLSTMII to increase or decrease the impact of input Ii in the sequence. The major difference between the SLSTM-I and the SLSTM-II is that fewer parameters are used in the SLSTM-II than those in the
SLSTM-I. The numbers of parameters used by different RNN cells are compared in Table 1, where Xt  RM , Yt  RN and t = 1, · · · , S.

Table 1: Comparison of Parameter Numbers.

Cell LSTM GRU SLSTM-I SLSTM-II

Number of Parameters 4N (M + N + 1) 3N (M + N + 1)
4N (M + N + 1) + N (S + 1) 3N (M + N + 1) + N (S + 1)

4 PROPOSED DEPENDENT BRNN (DBRNN) ARCHITECTURE

We investigate the macro RNN architecture and propose the dependent BRNN (DBRNN) in this section. Our proposal is inspired by the pros and cons of two RNN architectures ­ the bidirectional RNN (BRNN) architecture (Schuster & Paliwal, 1997) and the encoder-decoder architecture (Cho et al., 2014). In the following, we will first examine the BRNN and the encoder-decoder in Sec. 4.1 and, then, propose the DBRNN in Sec. 4.2.

4.1 BRNN AND ENCODER-DECODER

Most NLP problems can be formulated as an estimation problem. The probabilistic model for the BRNN can be written as

Y^tf = argmax P (Yt|{Xi}ti=1),
Yt
Y^tb = argmax P (Yt|{Xi}Si=t),
Yt
Y^t = argmax P (Yt|Y^tf , Y^tb),
Yt

(28) (29) (30)

5

Under review as a conference paper at ICLR 2018

where P is the probability density function, S is the length of the input sequence. f and b denote forward and backward respectively. So Y^tf and Y^tb are the forward and the backward predictions of Yt, respectively. As shown in Eq. (30), the final prediction of the BRNN is the pooling of different expert opinions - one is from the forward prediction while the other is from the backward
prediction. Due to this bidirectional design, the BRNN can fully utilize the information of the entire
input sequence to predict each individual output element. On the other hand, the BRNN does not utilize the predicted output in predicting Yt. This makes elements in the predicted sequence {Y^t}tS=1 independent of each other.

This problem can be handled using the encoder-decoder model in form of

Y^t = argmax P (Yt|{Y^i}it=-11, {Xi}Si=1).
Yt

(31)

However, unlike the BRNN, the encoder-decoder architecture does not pool expert opinions, making it vulnerable to previous erroneous predictions in the forward path. Recently, the BRNN has been introduced in the encoder by Bahdanau et al. (2015), yet this design still does not address the erroneous prediction problem.

4.2 DBRNN ARCHITECTURE AND TRAINING

Being motivated by observations in Sec. 4.1, we propose the DBRNN architecture to fulfill the following objectives:

Y^tf = argmax P (Yt|{Y^if }ti=-11, {Xi}Si=1),
Yt
Y^tb = argmax P (Yt|{Y^ib}iS=t+1, {Xi}iS=1),
Yt
Y^t = argmax P (Yt|Y^tf , Y^tb)  argmax P (Yt|{Y^i}Si=1, {Xi}iS=1).
Yt Yt

(32) (33) (34)

The DBRNN architecture is shown in Fig. 3. It consists of a lower and an upper BRNN branches. The lower one pools the expert opinions from the input while the upper one pools the forward and backward predictions from the output. At each time step, the input to the forward and the backward parts of the upper BRNN is the concatenated forward and backward outputs from the lower BRNN branch.

Figure 3: The DBRNN architecture.

The DBRNN architecture can be described mathematically as follows. We use X = {x1, ..., xS}, xi  RM , and Y = {y1, ..., yS}, yi  RN , to denote the input and output sequences, respectively, where N , S and M are the same as those in Table 1. Let C(·) be the cell function. The input, X, is

fed into the forward and backward RNN of the lower BRNN branch as

Ztf = Clf (xt, hfl(t-1)),

Ztb = Clb(xt, hlb(t+1)),

Zt =

Ztf Ztb

,

(35)

6

Under review as a conference paper at ICLR 2018

where h denotes the cell hidden state and l denotes the lower BRNN. The final output, Zt, of the lower BRNN is the concatenation of the output, Ztf , of the forward RNN and the output, Ztb, of the backward RNN. Similarly, the upper BRNN generates the final output Yt as

Ytf = Cuf (Zt, huf(t-1)), Ytb = Cub (Zt, hub (t+1)), Yt = W f Ytf + W bYtb,

(36)

where u denotes the upper BRNN, Yt is a linear combination of Ytf and Ytb, and W f and W b are trainable weight matrices. To generate forward prediction Y^tf and backward prediction Y^tb, the forward and backward paths of the upper BRNN branches are trained separately with the tar-
get sequence and the reversed target sequence, respectively. The results of the upper forward and
backward RNN are then combined to generate the final result.

There are three errors: prediction error of Y^tf denoted by ef , prediction error of Y^tb denoted by eb and prediction error of Y^t denoted by e. To train this network, ef is back propagated through time to the upper forward RNN and the lower BRNN, eb is back propagated through time to the upper backward RNN and the lower BRNN, and e is back propagated through time to the entire
architecture.

It is worthwhile to compare the DBRNN and the solution in Cheng et al. (2016). Both of them have
a bidirectional design for the output. However, there exist three main differences. First, the DBRNN
is a general design for the sequence-in-sequence-out (SISO) problem without being restricted to dependency parsing. The target sequences in training Y^tf , Y^tb and Y^t are the same for the DBRNN. In contrast, the solution in Cheng et al. (2016) has different target sequences. Second, the attention
mechanism is used by Cheng et al. (2016) but not in the DBRNN. Third, The encoder-decoder
design is adopted by in Cheng et al. (2016) but not in the DBRNN.

5 EXPERIMENTS
5.1 EXPERIMENTAL SETUP
We conduct experiments on two problems: part of speech (POS) tagging and dependency parsing (DP). The POS tagging task is an easy one which requires shorter memory while the DP task needs much longer memory and has more complex relations between the input and the output.
In the experiments, we compare the performance of four RNN architectures under two scenarios: 1) It = Xt, and 2) ItT = [XtT , YtT-1]. The four RNN architectures are the basic one-directional RNN (basic RNN), the BRNN, the sequence-to-sequence (a variation of encoder-decoder) RNN, and the DBRNN with four cell designs (LSTM, GRU, SLSTM-I and SLSTM-II). When It = Xt, we do not include the GRU cell since it inherently demands ItT = [XtT , YtT-1]. For the DBRNN, we show the results for Yt (denoted by "DBRNN combined") and Ytf (denoted by "DBRNN forward"), which is the prediction from the forward path of the upper BRNN branch. We do not include the result for Ytb, which is the prediction from the backward path of the upper BRNN branch since the performance of the backward RNN path of the upper BRNN branch is poorer.
The training dataset used for both problems are from the Universal Dependency 2.0 English branch (UD-English). It contains 12543 sentences and 14985 unique tokens. The test dataset for both experiments is from the test English branch (gold, en.conllu) of CoNLL 2017 shared task development and test data.
In the experiment, the lengths of the input and the target sequences are fixed. Sequences longer than the maximum length will be truncated. If the sequence is shorter than the fixed length, a special pad symbol will be used to pad the sequence. The input to the POS tagging and the DP problems are the stemmed and lemmatized sequences (column 3 in CoNLL-U format). The target sequence for POS tagging is the universal POS tag (column 4). The target sequence for DP is the interleaved dependency relation to the headword (relation, column 8) and its position (column 7). As a result, the length of the actual target sequence (rather than the preprocessed fixed-length sequence) for DP is twice of the length of the actual input sequence.
The input is first fed into a trainable embedding layer (Bengio et al., 2003) before it is sent to the actual network. Table 2 shows the detailed network and training specifications. It is important

7

Under review as a conference paper at ICLR 2018

Table 2: Network and training details

Input/output sequence fixed length Number of RNN layers Embedding layer vector size Number of RNN cells Batch size Training steps Learning rate Training optimizer Maximum gradient norm

100/100 1 512 512 20
14000 0.5
AdaGrad(Duchi, 2011) 5

to point out that we do not finetune network parameters or apply any engineering trick for the best possible performance since our main goal is to compare the performance of the LSTM, GRU, SLSTM-I and SLSTM-II four cells under various macro-architectures.

5.2 EXPERIMENTAL RESULTS
The results of the POS tagging problem with It = Xt and ItT = [XtT , YtT-1] are shown in Tables 3 and 4, respectively. Among all possible combinations, the DBRNN with the LSTM cell has the highest accuracy (89.16%) for It = Xt while the DBRNN with the GRU cell has the (89.74%) has the highest accuracy for ItT = [XtT , YtT-1].

Table 3: POS tagging test accuracy, It = Xt (%)

BASIC RNN BRNN (Schuster & Paliwal, 1997) Seq2seq (Sutskever et al., 2014) DBRNN Combined DBRNN Forward

LSTM 85.38 88.49 25.83 89.16 88.93

SLSTM-I 85.30 82.84 24.87 83.69 83.54

SLSTM-II 84.35 79.14 31.43 81.08 81.08

Table 4: POS tagging test accuracy, ItT = [XtT , YtT-1] (%)

BASIC RNN BRNN Seq2seq DBRNN Combined DBRNN Forward

LSTM 86.98 88.94 24.73 89.67 89.46

GRU 87.09 89.26 33.79 89.74 89.53

SLSTM-I 85.57 83.48 34.09 84.25 84.05

SLSTM-II 85.56 82.57 52.96 84.41 84.44

The results of the DP problem with It = Xt and ItT = [XtT , YtT-1] are shown in Tables 5 and 6, respectively. The SLSTM-I and SLSTM-II cells perform better than the LSTM and the GRU cells. Among all possible combinations, the DBRNN combined with SLSTM-I has the best performance. It has an accuracy of 54.39% and 60.30% for the former and the latter, respectively. Also, the basic RNN often outperforms BRNN for the DP problem as shown in Tables 5 and 6. This can be explained by that the basic RNN can access the entire input sequence when predicting the latter half of the output sequence since the target sequence is twice as long as the input. The other reason is that the BRNN can easily overfit when predicting the headword position.
8

Under review as a conference paper at ICLR 2018

Table 5: DP test accuracy, It = Xt (%)

BASIC RNN BRNN Seq2seq DBRNN Combined DBRNN Forward

LSTM 15.14 14.74 24.37 25.26 25.71

SLSTM-I 38.36 39.24 30.04 54.39 53.67

SLSTM-II 42.52 35.78 35.67 53.80 52.61

Table 6: DP test accuracy, ItT = [XtT , YtT-1] (%)

BASIC RNN BRNN Seq2seq DBRNN Combined DBRNN Forward

LSTM 44.12 32.46 27.67 56.89 58.30

GRU 47.49 27.83 29.94 51.32 53.17

SLSTM-I 54.52 54.14 40.85 60.30 59.81

SLSTM-II 56.02 47.72 48.73 58.28 58.05

We see from Tables 3 - 6 that the two DBRNN architectures outperform other architectures in both POS tagging and DP problems regardless of used cells. This shows the superiority of introducing the expert opinion pooling from both the input and the predicted output.
Furthermore, the proposed SLSTM-I and SLSTM-II outperform the LSTM and the GRU by a significant margin for complex language tasks. This demonstrates that the scaling factor in the SLSTM-I and the SLSTM-II does help the network retain longer memory. For the POS tagging problem, the SLSTM-I and the SLSTM-II do not perform as well as the GRU or the LSTM. This is probably due to the shorter memory requirement of this simple task. The SLSTM cells are over-parameterized and, as a result, they converge slower and tend to overfit the training data.
The performance of the SLSTM-I and the SLSTM-II cells is close except for the sequence-tosequence architecture, where the SLSTM-II cell performs particularly well. This has to do with the removing of the forget gate in the SLSTM-II. The hidden state ht of the SLSTM-II is more expressive in representing patterns over a longer distance. Since the sequence-to-sequence design relies on the expressive power of the hidden state, the SLSTM-II does have an advantage.
We compare the convergence behavior of It = Xt and ItT = [XtT , YtT-1] with the LSTM, the SLSTM-I and the SLSTM-II cells for the DP problem in Fig. 4. We see that the SLSTM-I and the SLSTM-II do not behave very differently between It = Xt and ItT = [XtT , YtT-1] as the LSTM does. This shows the effectiveness of the SLSTM-I and the SLSTM-II design regardless of the input. More performance comparison will be provided in the Appendix.

(a) (b) (c) Figure 4: Training perplexity of the basic RNN with It = Xt and ItT = [XtT , YtT-1] for the DP problem.
9

Under review as a conference paper at ICLR 2018
6 CONCLUSION AND FUTURE WORK
The memory decay behavior of the LSTM and the GRU was investigated and explained by mathematical analysis. Although the memory of the LSTM and the GRU fades slower than that of the SRN, it may not be long enough for complicated language tasks such as dependency parsing. To enhance the memory length, two cells called the SLSTM-I and the SLSTM-II were proposed. Furthermore, we introduced a new RNN architecture called the DBRNN that has the merits of both the BRNN and the encoder-decoder. It was shown by experimental results that the DBRNN with SLSTM-I and SLSTM-II outperforms other designs by a significant margin for complex language tasks. The DBRNN design is superior for both simple and complex language tasks. There are interesting issues to be further explored. For example, is the SLSTM cell also helpful in more sophisticated RNN architectures such as the deep RNN? Is it possible to make the DBRNN deeper and better? They are left for future study.
REFERENCES
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of The International Conference on Learning Representations, 2015.
Yoshua Bengio, Patrice Simard, and Paolo Frasoni. Learning long-term dependencies with gradient descent is difficult. Neural Networks, 5:157­166, 1994.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, pp. 1137­1155, 2003.
Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao, and Li Deng. Bi-directional attention with agreement for dependency parsing. In Proceedings of The Empirical Methods in Natural Language Processing (EMNLP 2016), 2016.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. In Proceedings of The Empirical Methods in Natural Language Processing (EMNLP 2014), 2014.
Duchi. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, pp. 2121­2159, 2011.
Jeffrey Elman. Finding structure in time. Cognitive Science, 14:179­211, 1990.
F. A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural Computation, pp. 2451­2471, 2000.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735­ 1780, 1997.
Michael Jordan. Serial order: A parallel distributed processing approach. Advances in Psychology, 121:471­495, 1997.
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv:1312.6026, 2014.
Pascanu Razvan, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of The International Conference on Machine Learning (ICML 2013), pp. 1310­1318, 2013.
Mike Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural networks. Signal Processing, 45:2673­2681, 1997.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, pp. 3104­3112, 2014.
O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. Advances in Neural Information Processing Systems, pp. 2773­2781, 2015.
10

Under review as a conference paper at ICLR 2018
APPENDIX: MORE EXPERIMENTAL RESULTS
In the appendix, we provide more experimental results to shed light on the convergence performance in the training of various architectures with different cells for the POS tagging and the DP tasks. First, we compare the training perplexity between It = Xt and ItT = [XtT , YtT-1] for various architectures with the LSTM, the SLSTM-I and the SLSTM-II cells in Figs. 5 - 8. Then, we examine the training perplexity with ItT = [XtT , YtT-1] for various architectures with different cells in Figs. 9 - 12.
(a) (b) (c) Figure 5: The training perplexity of the BRNN architecture with It = Xt and ItT = [XtT , YtT-1] for the DP task.
(a) (b) (c) Figure 6: The training perplexity of the DBRNN architecture with It = Xt and ItT = [XtT , YtT-1] for the DP task.
(a) (b) (c) Figure 7: The training perplexity of the sequence-to-sequence architecture with It = Xt and ItT = [XtT , YtT-1] for the DP task.
11

Under review as a conference paper at ICLR 2018
(a) (b) (c) Figure 8: The training perplexity of the sequence-to-sequence architecture with It = Xt and ItT = [XtT , YtT-1] for the POS tagging task.
(a) (b)
(c) (d) Figure 9: The training perplexity of different architectures with the LSTM (top left), its zoom-in (top right) and the GRU (bottom left), its zoom-in (bottom right) for the POS tagging.
12

Under review as a conference paper at ICLR 2018
(a) (b) Figure 10: The training perplexity of different architectures with the LSTM (left) and the GRU (right) cells for the DP task.
(a) (b)
(c) (d) Figure 11: The training perplexity for the basic RNN (top left), the BRNN (top right), the sequenceto-sequence (bottom left) and the DBRNN architectures (bottom right) for the DP task.
13

Under review as a conference paper at ICLR 2018
(a) (b) Figure 12: The training perplexity of the sequence-to-sequence architecture with the SLSTM-II cell for the POS tagging (left) and for the DP (right) tasks.
14

