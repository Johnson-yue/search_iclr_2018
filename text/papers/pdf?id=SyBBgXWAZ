Under review as a conference paper at ICLR 2018
OPTIMAL TRANSPORT MAPS FOR DISTRIBUTION PRESERVING OPERATIONS ON LATENT SPACES OF GENERATIVE MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. In this paper, we show that the latent space operations used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on. To address this, we propose to use distribution matching transport maps to ensure that such latent space operations preserve the prior distribution, while minimally modifying the original operation. Our experimental results validate that the proposed operations give higher quality samples compared to previous approaches.
1 INTRODUCTION & RELATED WORK
Generative models such as Variational Autoencoders (VAEs) (Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have emerged as popular techniques for unsupervised learning of intractable distributions. In the framework of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), the generative model is obtained by jointly training a generator G and a discriminator D in an adversarial manner. The discriminator is trained to classify synthetic samples from real ones, whereas the generator is trained to map samples drawn from a fixed prior distribution to synthetic examples which fool the discriminator. Variational Autoencoders (VAEs) (Kingma & Welling, 2013) are also trained for a fixed prior distribution, but this is done through the loss of an autoencoder that minimizes the variational lower bound of the data likelihood. For both VAEs and GANs, using some data X we end up with a trained generator G, that is supposed to map latent samples z from the fixed prior distribution to outputs samples G(z) which (hopefully) have the same distribution as the data
However, there is the concern of over-fitting: if the model has enough parameters and is improperly trained, it could just learn to memorize the training examples, without learning a meaningful representation of the data. Therefore, a desirable property of the generator is to learn a smooth representation of the data space, where we can transition between output samples in a realistic manner, without leaving the "data manifold". Therefore, it is a common practice in the literature of generative models to explore how the output samples G(z) of the learned models behave under various arithmetic operations on the latent samples z.
In this paper, we show that the operations typically used so far, such as linear interpolation (Goodfellow et al., 2014), spherical interpolation (White, 2016), vicinity sampling and vector arithmetic (Radford et al., 2015), cause a distribution mismatch between the latent prior distribution and the results of the operations. This is problematic, since the generator G was trained on a fixed prior and expects to see inputs with statistics consistent with that distribution.
To address this, we propose to use distribution matching transport maps, to obtain analogous latent space operations (e.g. interpolation, vicinity sampling) which preserve the prior distribution of the latent space.
1

Under review as a conference paper at ICLR 2018
In Figure 1 we showcase how our proposed technique gives an interpolation operator which avoids distribution mismatch when interpolating between samples of a uniform distribution.
points from prior midpoint linear midpoint matched (ours)

(a) Between eight pairs of points drawn from a uniform prior (b) we show the trajectories of linear interpolation and our matched interpolation, as well as the associated midpoints.

(b) The uniform prior dis-(c) linear midpoint distri- (d) matched midpoint dis- (e) spherical midpoint dis-

tribution.

bution

tribution (ours)

tribution of White (2016)

Figure 1: We show an example distribution mismatch induced by previous interpolation schemes when using a uniform prior in two dimensions (i.e. on [-1, 1]2). Our matched interpolation is a
minimal modification of the linear trajectory which traverses through the space such that all points
along the path are distributed identically to the prior.

1.1 GENERATIVE MODELS AND SAMPLE OPERATIONS
In the literature there are dozens of papers that use sample operations to explore the learned models. Bengio et al. (2013) use linear interpolation between neighbors in the latent space to study how well deep vs shallow representations can disentangle the latent space of Contractive Auto Encoders (CAEs) (Rifai et al., 2011).
In the seminal GAN paper of Goodfellow et al. (2014), the authors use linear interpolation between latent samples to visualize the transition between outputs of a GAN trained on MNIST. Dosovitskiy et al. (2015) linearly interpolate the latent codes of an auto encoder trained on a synthetic chair dataset.
Radford et al. (2015) also linearly interpolate between samples to evaluate the quality of the learned representation. Furthermore, motivated by the semantic word vectors of Mikolov et al. (2013), they explore using vector arithmetic on the samples to change semantics such as adding a smile to a generated face.

2

Under review as a conference paper at ICLR 2018

Operation

Expression

2-point interpolation n-point interpolation Vicinity sampling Analogies

y = tz1 + (1 - t)z2 , t  [0, 1]

y=

n i=1

tizi

with

i ti = 1

yj = z1 + uj for i = 1, · · · , k

y = z3 + (z2 - z1)

Table 1: Examples of interesting sample operations which need to be adapted if we want the distribution of the result y to match the prior distribution. For interpolation and vicinity sampling, we want the distribution to be identical to the prior distribution.

Reed et al. (2016) and Brock et al. (2016) use also interpolation in recent works ignoring the distribution mismatch.
1.2 DISTRIBUTION MISMATCH AND RELATED APPROACHES
While there are numerous works performing operations on samples, most of them have ignored the problem of distribution mismatch, such as the one presented in Figure 1d. Kingma & Welling (2013) and Makhzani et al. (2015) sidestep the problem when visualizing their models, by not performing operations on latent samples, but instead restrict the latent space to 2-d and uniformly sample the percentiles of the distribution on a 2-d grid. This way, the samples have statistics that are consistent with the prior distribution. However, this approach does not scale up to higher dimensions - whereas the latent spaces used in the literature can have hundreds of dimensions.
Related to our work, White (2016) experimentally observe that there is a distribution mismatch between the distance to origin for points drawn from uniform or Gaussian distribution and points obtained with linear interpolation, and propose to use a so-called spherical linear interpolation to reduce the mismatch, obtaining higher quality interpolated samples. However, the proposed approach has no theoretical guarantees.
In this work, we propose a generic method to fully preserve the desired prior distribution when using sample operations. The approach works as follows: we are given a `desired' operation, such as linear interpolation y = tz1 + (1 - t)z2, t  [0, 1]. Since the distribution of y does not match the prior distribution of z, we search for a warping f : Rd  Rd, such that y~ = f (y) has the same distribution as z. In order to have the modification y~ as faithful as possible to the original operation y, we use optimal transform maps (Santambrogio, 2015; Villani, 2003; 2008) to find a minimal modification of y which recovers the prior distribution z.
This is illustrated in Figure 1a, where each point y~ of the matched curve is obtained by warping a corresponding point y of the linear trajectory, while not deviating too far from the line.

2 FROM DISTRIBUTION MISMATCH TO OPTIMAL TRANSPORT

With an implicit models such as GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling, 2013), we use the data X , drawn from an unknown random variable x, to learn a generator G : Rd  Rd with respect to a fixed prior distribution pz, such that G(z) approximates x. Once the model is trained, we can sample from it by feeding latent samples z through G.
We now bring our attention to operations on latent samples z1, · · · , zk from pz, i.e. mappings

 : Rd × · · · × Rd  Rd.

(1)

We remind the reader that samples of a random variable are also random variables, which we can then realize into to vectors z1, · · · , zk  Rd. Thus, y = (z1, · · · , zk) is a random variable (commonly referred to as a statistic) which can be numerically evaluated on a realization y = (z1, ·, zk).
Therefore, our analysis on operations on samples is done through the underlying random variable y. The same treatment is typically used to analyze other statistics over random variables, such as the sample mean, sample variance and test statistics.

3

Under review as a conference paper at ICLR 2018

In Table 1 we show example operations which have been commonly used in the literature. As discussed in the Introduction, such operations can provide valuable insight into how the trained generator G changes as one creates related samples y from some source samples. The most common such operation is the linear interpolation, which we can view as a operation

yt = tz1 + (t - 1)z2,

(2)

where z1, z2 latent samples from the prior pz and yt is parameterized by t  [0, 1].

Now, assume z1 and z2 are i.i.d, and let Z1, Z2  Z be their (scalar) first components. Then the first component of yt is Yt = tZ1 + (t - 1)Z2, and we can compute:
V ar[Yt] = V ar[tZ1 + (1 - t)Z2] = t2V ar[Z1] + (1 - t)2V ar[Z2] = (1 + 2t(t - 1))V ar[Z]. (3)

Since (1 + 2t(t - 1)) = 1 for all t ]0, 1[, it is in general impossible for yt to have the same distribution as z, which means that distribution mismatch is inevitable when using linear interpolation.
A similar analysis reveals the same for all of the operations in Table 1.

This is problematic: since the generator G is only trained for samples from pz, we have no guarantees on how suitable yt is as an input for G.

This leaves us with a dilemma: we have various intuitive operations (see Table 1) which we would want to be able to perform on samples, but their resulting distribution happens to be inconsistent with the distribution we trained G for.

2.1 ON THE POSSIBLY PARADOXICAL RELATIONSHIP BETWEEN THE LIKELIHOOD AND THE
LOSS

It is obvious that we cannot expect a model G to perform well outside the domain it was trained on, but what if we are still within the support of the original training distribution?
Since we might train on a distribution with infinitely large support (e.g. gaussian in Rd), but visit some regions with vanishingly small probability, clearly we cannot expect G to perform well on the entire support.
But we could use a uniform distribution for z (say on [-1, 1]d), such that every infinitesimal region of the support is sampled with equal probability. Now, assuming the trained model is smooth, we might assume that this ensures the model performs similarly well everywhere in the support.

Unfortunately however, this is not guaranteed and turns out also to not be the case in practice.

For a concrete example, suppose we want to learn an unknown mapping f : Rd  R over a prior distribution pz using linear regression with respect to an l2 loss:

H = arg min Ezpz [l2(f (z), H z)],
H

(4)

with l2(f (z), H z) = f (z) - H z 2.

Now, consider the case where f and pz(z) are spherically symmetric, such that f (z) = g( z ) and pz(z) = s( z ) (e.g. f (z) = sin( z and z drawn from an isotropic gaussian, i.e. pz(z)  N (0, I) with I as identity. In this case, due to symmetry, it is easy to see that H = 0, and thus the pointwise loss is l2 = g( z ) 2.

Since we make no assumptions on g (except measurability), at a given distance z = r, we can have an arbitrary relationship between the likelihood of a point in the prior, s(r), and the corresponding pointwise loss of the optimal regressor. We can therefore make the following observations:

· A uniform prior (e.g. uniform on the sphere {z  Rd : z < R}), where s(r) is constant, does not give any guarantee of a uniformly distributed pointwise loss. Thus, even for a uniform prior, we cannot expect the model to perform equally well everywhere on the support.
· We can not expect the loss l2 to be low for regions that have a high likelihood. E.g. for g = s the inverse relationship holds. The intuition for this is that minimizing l2(z) may be `harder' for regions of high probability, such that the resulting loss will still be higher than elsewhere in the space, even though regions of high probability are `more important' since we are minimizing the expected loss.

4

Under review as a conference paper at ICLR 2018

A particular example is when z is drawn from an isotropic Gaussian z  N (0, I) and g(r) = 1/r2.

In this case, if we look at the midpoint of linear interpolation, y

=

1 2

z1

+

1 2

z2,

we

have

y



N (0,

1 2

I).

Now,

both z

and

y

have spherically symmetric

distributionswith likelihoods

decaying

exponentally with r. However, the "radius" of y is smaller by a factor of 12 compared to z, which

means that points drawn from y will have a higher likelihood when evaluated at the density of pz,

i.e.

Ey[pz(y)]  Ez[pz(z)].

(5)

So while the points of y are on average 'high likelihood' points in the distribution of z, we have for

the loss l2:

Ey[l2(y, f (y))] = Ey[g( y )]  Ez[g( z )] = Ey[l2(y, f (y))]

(6)

.

So therefore, it is clear that in the general case, we have no guarantees on the relationship between
the likelihood of points in the training distribution pz and the expected loss when using a different distribution y, even in the case where y is concentrated at regions of 'high probability' in pz.

While we have made these observations based on a simple setting (linear regression, l2 loss and symmetry constraints), we will give a heuristic reasoning why this might be happening in practice,
while experimentally also showing that this is the case.

To this end, in the next section we will give a short example of the curse of dimensionality and discuss how the points of the prior distribution pz are concentrated in a thin spherical shell if it has i.i.d components.

We note that the analysis of the next section can bee seen as a more rigorous version of an observation made by White (2016), who experimentally shows that there is a significant difference between the average norm of the midpoint of linear interpolation and the points of the prior, for uniform and Gaussian distributions.

2.2 ON THE CURSE OF DIMENSIONALITY AND GEOMETRIC OUTLIERS

Suppose our latent space has a prior with z = [Z1, · · · , Zd]  [-1, 1]d with i.i.d entries Zi  Z. In this case, we can look at the squared norm

d
z 2 = Zi2.
i=1

(7)

From the Central Limit Theorem (CLT), we know that as d  ,

1 d( d

z

2 - µZ2 )  N (0, Z2 2 ),

(8)

in distribution. Thus, assuming d is large enough such that we are close to convergence, we can approximate the distribution of z 2 as N (dµZ2 , dZ2 2 ). In particular, this implies that almost all points lie on a relativelythin spherical shell, since the mean grows as dµZ2 whereas the standard deviation grows only as dZ2 .

Now

consider

an

operator

such

as

the

midpoint

of

linear

interpolation,

y

=

1 2

z1

+

1 2

z2

,

with

com-

ponents Y (i)

=

1 2

Z1(i)

+

1 2

Z2(i)

.

Furthermore, let's assume Z is symmetric around 0, such that

E[Z] = 0.

In this case, we can compute:

E[(Y

(i))2]

=

V

1 ar[ 2 Z1

+

1 2 Z2]

=

1 V
2

ar[Z ]

=

1 2

µZ2 2

V

ar[(Y (i))2]

=

V

1 ar[( 2 Z1

+

1 2

Z2

)2

]

=

1 V
4

ar[Z 2 ]

=

1 4

Z2

2

.

(9) (10)

Thus, the distribution of

y

2

can

be

approximated

with

N

(

1 2

dµZ2 ,

1 4

dZ2 2 ).

Therefore, y also mostly lies on a spherical shell, but with a different radius than z. In fact, the
shells will intersect at regions which have a vanishing probability for large d. In other words, when looking at the squared norm y 2, y 2 is a (strong) out-lier with respect to the distribution of z 2.

5

Under review as a conference paper at ICLR 2018

While we already made the point that likelihood is not a guaranteed indicator for a good pointwise loss, the norm is a scalar statistic which one can imagine that a learned generator G specializes with respect to. In Figure 3 in the experiments, we illustrate how dramatically different the distributions are, having almost no common support, when using a uniformly distributed prior space in 100 dimensions.

2.3 DISTRIBUTION MATCHING WITH OPTIMAL TRANSPORT

We have seen that distribution mismatch appears with common operators, such as linear interpolation. Furthermore, we have seen that the result can cause a dramatic increase in the expected loss for the generator G, because of the mismatched training distribution.
In order to address this, we propose a simple and intuitive strategy for constructing distribution preserving operators, via optimal transport:
Strategy 1 (Optimal Transport Matched Operations).

1. We construct an 'intuitive' operator y = (z1, · · · , zk).
2. We analytically (or numerically) compute the resulting (mismatched) distribution py
3. We search for a minimal modification y~ = f (y) (in the sense that Ey[c(y~, y)] is minimal with respect to a cost c), such that distribution is brought back to the prior, i.e. py~ = pz.

The cost function in step 3 could e.g. be the euclidean distance c(x, y) = x - y , and is used to measure how faithful the modified operator, y~ = f (k(z1, · · · , zk)) is to the original operator k. Finding the map f which gives a minimal modifications can be challenging, but fortunately it is a well studied problem from optimal transport theory. We refer to the modified operation y~ as the distribution matching of y, with respect to the cost c and prior distribution pz.
For completeness, we introduce key concept of optimal transport theory in a simplified setting, i.e. assuming probability distributions are in euclidian space and skipping measure theoretical formalism. We refer to Villani (2003; 2008) and Santambrogio (2015) for a thorough and formal treatment of optimal transport.

The problem of step (3) above was first posed byMonge (1781) and can more formally be stated as: Problem 1 (Santambrogio (2015) Problem 1.1). Given probability distributions px, py, with domains X , Y respectively, and a cost function c : X × Y  R+, we want to minimize

inf Expx [c(x, f (x))] f : X  Y, f (x)  py

(MP)

We refer to the minimizer f X  Y of (MP) (if it exists), as the optimal transport map from px to py with respect to the cost c.

However, the problem remained unsolved until a relaxed problem was studied by Kantorovich (1942):
Problem 2 (Santambrogio (2015) Problem 1.2). Given probability distributions px, py, with domains X , Y respectively, and a cost function c : X × Y  R+, we want to minimize

inf E(x,y)px,y [c(x, y)] (x, y)  px,y, x  px, y  py

(KP)

We refer to the joint distribution px,y which minimizes (KP) as the optimal transport plan from px to py with respect to the cost c.

The key difference is to relax the deterministic relationship between x and f (x) to a joint probability distribution px,y with marginals px and py for x and y. In the case of Problem 1, the minimization might be over the empty set since it is not guaranteed that there exists a mapping f such that f (x)  y. In contrast, for Problem 2, one can always construct a joint density px,y with px and py as marginals, such as the trivial construction where x and y are independent, i.e. px,y(x, y) := px(x)py(y).
We note that we can treat a joint density px,y(x, y) over X × Y as a stochastic mapping from X to Y, where given a fixed x, we obtain values f (x) by sampling from the conditional distribution, i.e.

6

Under review as a conference paper at ICLR 2018

f (x)  py|x. Thus, we can view the Problem KP as a relaxation of Problem MP where f is allowed to be a stochastic mapping.

While the relaxed problem of Kantorovich, (KP) is much more studied in the optimal transport literature, for our purposes of constructing operators it is desirable for the mapping f to be deterministic as in (MP).

To this end, we will choose the cost function c such that the two problems coincide and where we can find an analytical solution f or at least an efficient numerical solution.

In particular, we note that most operators in Table 1 are all pointwise, such that if the points zi have i.i.d. components, then the result y will also have i.i.d. components.

Now, if we combine this with a constraint the cost c to be additive over the components of x, y, we obtain the following simplification:

Theorem 1. Suppose px and py have i.i.d components and c over X × Y = Rd × Rd decomposes

as

d

c(x, y) = C(x(i), y(i)), .

(11)

i=1

Then, the minimization problems (MP) and (KP) turn into d identical scalar problems for the distributions pX and pY of the components of x and y:

inf EXpX [C(X, T (X))] T : R  R, T (X)  pY

(MP-1-D)

inf E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY ,

(KP-1-D)

such that an optimal transport map T for (MP-1-D) gives an optimal transport map f for (MP)

by pointwise application of T , i.e. f (x)(i) := T (x(i)), and an optimal transport plan pX,Y for

(KP-1-D) gives an optimal transport plan px,y(x, y) :=

d i=1

pX,Y

(x(i),

y(i))

for

(KP).

Proof. See Appendix.

Fortunately, under some mild constraints, the scalar problems have a known solution:

Theorem 2 (Theorem 2.9 in Santambrogio (2015)). Let h : R  R+ be convex and suppose the cost C takes the form C(x, y) = h(x - y). Given an continuous source distribution pX and a target distribution pY on R having a finite optimal transport cost in (KP-1-D), then

TXmon Y (x) := FY[-1](FX (x)),

(12)

defines an optimal transport map from pX to pY for (MP-1-D), where FX (x) :=

x -

pX

(x

)dx

itshethpeseCuudmo-uilnavteivreseForfeqFuYe.nFceuyrtDheisrtmriobruet,iothne(jCoDinFt )diosftrXibuatniodnFoY[f-(1X](,yT)Xm:=on Yin(fX{t))deRfi|nFeYs

(t) an

 t} is optimal

transport plan for (KP-1-D).

The mapping TXmon Y (x) in Theorem 2 is non-decreasing and is known as the monotone transport map from X to Y . It is easy to verify that TXmon Y (X) has the distribution of X, in particular FX (X)  Uniform(0, 1) and if U  Uniform(0, 1) then FY[-1](U )  Y . These two properties happen to uniquely define the monotone transport map,

Lemma 1 (Theorem 2.5 in Santambrogio (2015)). Suppose a mapping g(x) is non-decreasing and maps a continious distribution pX to a distribution pY , i.e.

g(X)  Y,

(13)

then g is the monotone transport map TXmon Y .

According to Lemma 1, an alternative way of computing TXmon Y is to find some g that is nondecreasing and transforms pX to pY .

Now, combining Theorems 1 and 2, we obtain a concrete realization of the Strategy 1 outlined above. We choose the cost c such that it admits to Theorem 1, such as c(x, y) := x - y 1, and use an

7

Under review as a conference paper at ICLR 2018

operation that is pointwise, we just need to compute the monotone transport map in (12). That is,
if z has i.i.d components with distribution pZ, we need to compute the compnent distribution pY of the result y of the operation, the CDFs FZ , FY and obtain

TYmonZ (y) := FZ[-1](FY (y))

(14)

as the component-wise modification of y, i.e. y~(i) := TYmonZ (y(i)).

In the next sections, we illustrate this for a few examples, in particlar for linear interpolation and
vicinity sampling, using a uniform or a gaussian prior. We picked the examples where we can analytically compute the uniform transport map, but note that it is also easy to compute FZ[-1] and (FY (y)) numerically, since one only needs to estimate CDFs in one dimension.

Since the components of all random variables in these examples are i.i.d, for such a random vector x we will implicitly write X for a scalar random variable that has the distribution of the components of x.

2.4 EXAMPLE 1:UNIFORM LINEAR INTERPOLATION

Now suppose z has uniform components Z  Uniform(-1, 1). In this case, pZ(z) = 1/2 for -1 < z < 1.

Now let yt = tz1 + (1 - t)z2 denote the linear interpolation between two points z1, z2. We then obtain the component distribution pYt as the convolution of ptZ and p(1-t)Z . Due to symmetry we can assume that t > 1/2, since pYt = pY1-t .
First we note that ptZ = 1/(2t) for -t < z < t and p(1-t)Z = 1/(2(1-t)) for -(1-t) < z < 1-t. We then obtain, via convolution:



0





1

y + 1 

pYt (y)

=

2(1

-

t)(2t)

2 - 2t -y + 1



0

if y < -1 if - 1 < y < -t + (1 - t) if - t + (1 - t) < y < t - (1 - t) if t - (1 - t) < y < 1 if 1 < y

(15)

(16)

Now, we obtain the CDF FYt by computing

y

FYt (y) =

pYt (y )dy

-



0



1

    

1 2

(y

+

1)(y

+

1)

=

2(1

- t)(2t)

2(1 - 2(1 - 

t)(y + t) t)(3t - 1)

+

(-

1 2

y2

+

y

+

1 2

(2t

- 1)2

-

(2t

- 1))

2(1 - t)(2t)

(17)
if y < -1 if - 1 < y < 1 - 2t if 1 - 2t < y < 2t - 1 if 2t - 1 < y < 1 if 1 < y
(18)

Now

since

pZ (z)

=

1/2

for

|z|

<

1,

we

have

FZ (z)

=

1 2

z

+

1 2

for

|z|

<

1.

This

gives

FZ[-1](p)

=

2(p

-

1 2

).

Now, we just compose the two mappings to obtain TYmton Z (y) = FZ[-1](FYt (y)).

3 EXAMPLE 2: UNIFORM VICINITY SAMPLING

Let z again have uniform components on [-1, 1]. For vicinity sampling, we want to obtain new points z1, ·, zk which are close to z. We thus define

zi := z + ui,

(19)

8

Under review as a conference paper at ICLR 2018

where ui also has uniform compnents, such that each coordinate of zi differs at most by from z. By identifying tZi = tZ + (1 - t)Ui with t = 1/(1 + ), we see that tZi has identical distribution to the linear interpolation Yt in the previous example. Thus gt(Zi) := TYmton Z (tZi) will have the distribution of Z, and by Lemma1 is then the monotone transport map from Zi to Z.

3.1 EXAMPLE 3: GAUSSIAN LINEAR INTERPOLATION, VICINITY SAMPLING AND ANALOGIES

Suppose z has components Z  N (0, 2). In this case, we can compute linear interpolation as

before, yt = tz1 + (1 - t)z2. Since the sum of gaussians is gaussian, we get, Yt  N (0, t22 + (1 - t)22). Now, it is easy to see that with a proper scaling factor, we can adjust the variance of

Yt

back to

2.

That is,


t2

1 +(1-t)2

Yt



N (0, 2),

so by

Lemma

1 gt(y)

:=

1 y
t2 +(1-t)2

is the

monotone transport map from Yt to Z.

By adjusting the vicinity sampling operation to

zi := z + ei,

(20)

where

ei



N (0, 1),

we

can

similarly

find

the

monotone

transport

map

g

(y)

=

1 1+

2 y.

Another operation which has been used in the literature is the "analogy", where from samples z1, z2, z3, one wants to apply the difference between z1 and z2, to z3. The transport map is then g(y) = 1 y
3

4 EXPERIMENTS

4.1 COMPARISON OF DISTRIBUTIONS
To validate the correctness of the matched operators obtained above, we numerically simulate the distributions for toy examples, as well as prior distributions typically used to train Generative Models.
For Figure 1, we sample 1 million pairs of points uniformly on [-1, 1]2, and estimate numerically the midpoint distribution of linear interpolation, our proposed matched interpolation and the spherical interpolation of White (2016). It is reassuring to see that the matched interpolation gives midpoint which are also uniformly distributed. In contrast, the linear interpolation condenses more towards the origin, forming a pyramid-shaped distribution (the result of convolving two boxes in 2-d). Since the spherical interpolation of White (2016) follows a great circle with varying radius between the two points, we see that the resulting distribution has a 'hole' in it since it always circles around the origin.
For Figure 3, we sample 100000 points in d = 100 dimensions, using either i.i.d. uniform components on [-1, 1] or Gaussian N (0, 1). We see there is a dramatic difference between the prior and We also show the spherical (SLERP) interpolation of White (2016) which has a matching first moment, but otherwise also induces a distribution mismatched. In contrast, our matched interpolation, which we derive in the next section, fully preserves the prior distribution and perfectly aligns.

4.2 QUALITATIVE RESULTS
In this section we will present some concrete examples for the differences in generator output dependent on the exact operation used to traverse the latent space. To this end, the output for linear interpolation, SLERP (spherical linear interpolation) and our proposed matched transport interpolation will be compared.
We used DCGAN models trained for LSUN and an icon dataset (https://data.vision.ee.ethz.ch/cvl/lld/) to qualitatively evaluate.
We begin with the classic example of 2-point interpolation: Figure 2 shows three examples each for an interpolation between 2 points in latent space. Each example is first done via linear interpolation, then SLERP and finally matched interpolation.

9

Under review as a conference paper at ICLR 2018
It is immediately obvious that linear interpolation produces inferior results with generally more blurry, less saturated and less detailed output images. SLERP and matched interpolation are slightly different, however it is not obvious which is superior.

(a) Icon dataset

(b) LSUN dataset

Figure 2: 2-point interpolation: Each example shows linear, SLERP and transport matched interpolation from top to bottom respectively. Both outputs are produced with DCGAN trained on 100-dimensional, uniformly distributed latent vectors. The output resolution for the icon dataset 2a is 32, for 2b 64 pixel.

An even stronger effect can be seen when we do 4-point interpolation, showcased in Figures 4 (LSUN) and 5 (icons). Because the outputs where generated at a higher resolution, we can observe a slight loss in detain for the SLERP version here as well. Most details are lost and artifacts start do dominate the image towards the midpoint for linear interpolation. The increase in artifacts is also very apparent in figures

0.25 0.2 0.15 0.1 5 · 10-2

prior midpoint linear midpoint matched (ours) midpoint spherical

·10-2 6 5 4 3 2 1

prior midpoint linear midpoint matched (ours) midpoint spherical

py2 py2

0 5 10 15 20 25 30 35 40 45 50 y2
(a) uniform distribution

0 20 40 60 80 100 120 140 160 180 200
y2
(b) Gaussian distribution

Figure 3: We the distribution of the squared norm y 2 of midpoints for two prior distributions in 100 dimensions: (a) components uniform on [-1, 1] and (b) components Gaussian N (0, 1), for linear interpolation, our proposed matched interpolation and the spherical interpolation of White (2016). Both linear interpolation and spherical interpolation introduce a distribution mismatch, whereas our proposed matched interpolation preserves the prior distribution for both priors.

10

Under review as a conference paper at ICLR 2018

(a) Linear interpolation

(b) spherical interpolation

(c) our matched interpolation

Figure 4: Interpolation from 4 sampled points from LSUN dataset. The 4 sampled points are in the corners while the middle sampled point is in the middle of the 5 × 5 grid for each of the interpolation
methods considered: linear, SLERP and our proposed uniform-matched interpolation.

(a) Linear interpolation

(b) SLERP interpolation

(c) our matched interpolation

Figure 5: Interpolation between 4 random points on the icon dataset, at 32x32 pixel. Similar effects as for the LSUN example are noticable.

5 CONCLUSIONS
We have shown that the common latent space operations used for Generative Models induce distribution mismatch from the prior distribution the models were trained for. This problem has been ignored by the literature so far, partially due to belief that this should not be a problem for uniform priors. However, our statistical and experimental analysis shows that the problem is real, with the operations used so far producing significantly lower quality samples compared to their inputs. To address the distribution mismatch, we propose to use optimal transport to minimally modify (in l1 distance) the operations such that they fully preserve the prior distribution. We give analytical formulas of the resulting (matched) operations for various examples, which are easily implemented. The matched operators give a significantly higher quality samples compared to the originals, having the potential to become standard tools for evaluating and exploring generative models.

11

Under review as a conference paper at ICLR 2018

(a) Linear interpolation

(b) SLERP interpolation

(c) matched interpolation

Figure 6: Midpoints sampling for linear SLERP and uniform-matched interpolation when starting from the same pairs of samples points on icon dataset.

(a) Linear interpolation

(b) SLERP interpolation

(c) matched interpolation

Figure 7: Midpoints sampling for linear SLERP and uniform-matched interpolation when starting from the same pairs of samples points on LSUN.

REFERENCES

Yoshua Bengio, Gre´goire Mesnil, Yann Dauphin, and Salah Rifai. Better mixing via deep representations. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 552­560, 2013.

Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.

Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox. Learning to generate chairs with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1538­1546, 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.

Leonid Vitalievich Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk SSSR, volume 37, pp. 199­201, 1942.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Dis-

tributed representations of words and phrases and their compositionality.

In

C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger

(eds.), Advances in Neural Information Processing Systems 26, pp. 3111­3119.

Curran Associates, Inc., 2013.

URL http://papers.nips.cc/paper/

12

Under review as a conference paper at ICLR 2018
5021-distributed-representations-of-words-and-phrases-and-their-compositionality. pdf. Gaspard Monge. Me´moire sur la the´orie des de´blais et des remblais. Histoire de l'Acade´mie Royale des Sciences de Paris, 1781. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016. Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 833­840, 2011. Filippo Santambrogio. Optimal transport for applied mathematicians. Birka¨user, NY, 2015. Ce´dric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003. Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. Tom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016.
13

Under review as a conference paper at ICLR 2018

6 APPENDIX

6.1 PROOF OF THEOREM 1

Proof. We will show it for the Kantorovich problem, the Monge version is similar. Starting from (KP), we compute

inf E(x,y)px,y [c(x, y)] (x, y)  px,y, x  px, y  py

= inf

d
E(x,y)px,y [ C(x(i), y(i))] (x, y)  px,y, x  px, y  py
i=1

= inf

d
E(x,y)px,y [C(x(i), y(i))] (x, y)  px,y, x  px, y  py
i=1

d
 inf E(x,y)px,y [C(x(i), y(i))] (x, y)  px,y, x  px, y  py
i=1

d

= inf E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY
i=1

(21) (22) (23) (24) (25)

= d · inf E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY ,

(26) (27)

where the inequality in (24) is due to each term being minimized separately.

Now let Pd(X, Y ) be the set of joints px,y with px,y(x, y) =

d i=1

pX,Y

(x(i), y(i))

where

pX,Y

has marginals pX and pY . In this case Pd(X, Y ) is a subset of all joints px,y with marginals px and

py, where the pairs (x(1), y(1)), . . . , (x(d), y(d))) are constrained to be i.i.d. Starting again from

(23) can compute:

d
inf E(x,y)px,y [C(x(i), y(i))] (x, y)  px,y, x  px, y  py
i=1

 inf

d
E(x,y)px,y [C(x(i), y(i))] px,y  Pd(X, Y )
i=1

= inf

d
E(x,y)px,y [C(x(i), y(i))] px,y  Pd(X, Y )
i=1

d

= inf

E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY

i=1

(28) (29) (30)

= d · inf E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY ,

(31) (32)

where the inequality in (28) is due to minimizing over a smaller set.

Since the two inequalities above are in the opposite direction, equality must hold for all of the expressions above, in particular:

inf E(x,y)px,y [c(x, y)] (x, y)  px,y, x  px, y  py

(33)

= d · inf E(X,Y )pX,Y [C(X, Y )] (X, Y )  pX,Y , X  pX , Y  pY

(34)

Thus, (KP) and (KP-1-D) equal up to a constant, and minimizing one will minimize the other, and

the minimization of the former can be done over pX,Y with px,y(x, y) =

d i=1

pX,Y

(x(i)

,

y(i)

).

14

