Under review as a conference paper at ICLR 2018
TOWARDS INTERPRETABLE CHIT-CHAT: OPEN DOMAIN DIALOGUE GENERATION WITH DIALOGUE ACTS
Anonymous authors Paper under double-blind review
ABSTRACT
Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.
1 INTRODUCTION
Conversational agents are becoming ubiquitous recently. Through human-machine conversation, such agents either help users complete specific tasks (Young et al., 2013) or engage them in social chat (Vinyals & Le, 2015). Depending on application scenarios, various conversational agents have been designed including chatbots, personal assistants, and automated customer service, etc.
Traditional research on conversational agents focuses on task-oriented dialogue systems (Young et al., 2013) where task specific dialogue acts are handcrafted in a form of slot-value pairs which make the conversation in such systems interpretable and controllable but suffer from scalability problems. To escape from the limitation, recent interest of research moves to end-to-end dialogue learning without any assumptions on dialogue acts. Most of the effort is paid to non-task-oriented chit-chat (Vinyals & Le, 2015), and there are also a few studies on task-oriented dialogues (Bordes & Weston, 2017; Eric & Manning, 2017). Without dialogue acts, these work directly constructs a response by learning from large scale data with neural networks, and thus enjoys good scalability to new domains. On the other hand, due to the absence of dialogue acts, it is hard to interpret the emergence of a response to a dialogue context and predict where the conversation will flow to.
In this work, we aim to achieve scalability, interpretability, and controllability in non-task-oriented dialogues. To this end, we introduce dialogue acts into open domain dialogue generation. Open domain dialogue generation has been widely applied to chatbots which aim at engaging users by keeping conversation going, but is never in the harness of dialogue acts. Existing work concentrates on generating relevant and diverse responses for a context. However, it is not clear if relevance and diversity are sufficient to engagement in interactions. Therefore, we investigate the following problems: (1) if we can properly design dialogue acts that enjoy both scalability and capability of interpreting engagement in human-human open domain conversation; (2) how to learn a dialogue generation model with the dialogue acts; and (3) how the model performs in practice and if the performance can be explained by the dialogue acts.
To examine how people engage in social chat, we establish a general dialogue act taxonomy for open domain conversation by extending the existing work with high-level dialogue acts regarding to conversational context. The taxonomy, when applied to real data, gives rise to an interesting finding that in addition to replying with relevance and diversity , people are used to driving their social chat by constantly switching to new contexts and properly asking questions. Such behaviors are less explored before, and thus are difficult for the existing end-to-end learning methods to
1

Under review as a conference paper at ICLR 2018

imitate. To mimic human behaviors, we propose jointly modeling dialogue act selection and response generation in open domain dialogue generation. The dialogue model is specified with neural networks. We propose learning from human-human interactions by fitting the model to large scale real world dialogues tagged with a dialogue act classifier and further optimizing the policy of act selection for long-term conversation through a reinforcement learning approach. Our model enjoys several advantages over the existing models: (1) the dialogue acts provide interpretation to response generation from a discourse perspective; (2) the dialogue acts enhance diversity of responses by expanding the search space from language to act × language; and (3) the dialogue act selection is compatible with post-engineering work (e.g., combination with rules), and thus allows engineers to flexibly control their systems. Evaluation results on large scale test data indicate that our model can significantly outperform state-of-the-art methods in terms of quality of generated responses regarding to given contexts and lead to long-term conversation in both machine-machine simulation and human-machine conversation in a way similar to how human behave in their interactions.

Our contributions in this work include: (1) proposal of open domain dialogue generation with dialogue acts; (2) design of dialogue acts that represent human behavior regarding to conversational context and insights from analysis of human-human interactions with the design; (3) joint modeling of dialogue act selection and response generation in open domain dialogue generation; (4) proposal of a supervised learning approach and a reinforcement learning approach for model optimization; (5) empirical verification of the effectiveness of the model through human annotations, machinemachine simulation, and human-machine conversation.

Dialogue Acts
Context Maintain Statement (CM.S)
Context Maintain Question (CM.Q)
Context Maintain Answer (CM.A) Context Switch Statement (CS.S) Context Switch Question (CS.Q)
Context Switch Answer (CS.A)
Others (O)

Table 1: Definition of dialogue acts.

Definitions

Examples

A user or a bot aims to maintain the current conversational context (e.g., topic) by giving information, suggesting something, or commenting on the previous utterances, etc.

"there are many good places in Tokyo." after "I plan to have a tour in Tokyo this summer.".

A user or a bot asks a question in the current

context. Questions cover 5W1H and yes-no "where are you going to stay in

with various functions such as context clarifica- Tokyo?" after "I plan to have a tour

tion, confirmation, knowledge acquisition, and in Tokyo this summer.".

rhetorical questions, etc.

A response or an answer to the previous utter- "this summer." after "when are

ances in the current context.

you going to Tokyo?".

Similar to CM.S, but the user or the bot tries to switch to a new context (e.g., topic) by bringing in new content. A user or a bot tries to change the context of conversation by asking a question.
The utterance not only replies to the previous turn, but also starts a new topic.
greetings, thanks, and requests, etc..

"I plan to study English this summer." after "I plan to have a tour in Tokyo this summer.". "When will your summer vacation start?" after "I plan to have a tour in Tokyo this summer." "I don't know because I have to get an A+ in my math exam." after "when are you going to Tokyo?". "thanks for your help."

2 DIALOGUE ACTS FOR OPEN DOMAIN CONVERSATION
We first define dialogue acts, and then describe the data for learning and the insights we obtain from the data. Finally, we elaborate how we build the classifier with neural networks.
2.1 DEFINITION OF DIALOGUE ACTS

Our dialogue acts are inherited from the existing work on 1-on-1 live chats and twitter (Kim et al., 2010; Ivanovic, 2005). Similar to (Oraby et al., 2017) , we organize the original 12 acts in (Ivanovic, 2005) into high-level dialogue acts: "statement" and "expressive" are merged as "statement"; "yesno question" and "open question" are combined as "question"; "yes-answer", "no-answer", and "response-ack" are collapsed as "answer"; and other tags are treated as "others". On top of these acts, we further define two high-level dialogue acts that describe how people behave regarding to conversational context in their interactions. As will be seen later, the extension may bring us further insights on engagement in social chat. Details of the dialogue acts are described in Table 1.

2

Under review as a conference paper at ICLR 2018

The high-level dialogue acts in Table 1 are generally applicable to open domain dialogues from various sources in different languages such as Twitter, Reddit, Facebook, Weibo (www.weibo. com), and Baidu Tieba (https://tieba.baidu.com/), etc. One can extend the taxonomy by defining finer-grained dialogue acts and learn their generation models with the approaches described later. Existing work does not have dialogue acts regarding to conversational context. Therefore, it is not clear how these dialogue acts depict human behavior in interactions, and there are no large scale data available for learning dialogue generation with the dialogue acts either. To resolve these problems, we build a data set.
2.2 DATA SET

We crawled 30 million dyadic dialogues (conversations between two people) from Baidu Tieba. Baidu Tieba is the largest Reddit-like forum in China which allows users to post and comment on others' post. Two people can communicate with each other through one posting a comment and the other one replying to the comment. Data in Baidu Tieba covers a large variety of topics, and thus can be viewed as a simulation of open domain conversation in a chatbot. We randomly sample 9 million dialogues as a training set, 90 thousand dialogues as a validation set, and 1000 dialogues as a test set. These data are used to learn a dialogue generation model later. We employ the Standford Chinese word segmenter1 to tokenize utterances in the data. Table 2 reports statistics of the data.

Table 2: Statistics of the experimental data sets

train val test

# dialogues

9M 90k 1000

Min. # turns per dialogue

3

5

5

Max. # turns per dialogue 50 50 50

Avg. # turns per dialogue 7.68 7.67 7.66

Avg. # words per utterance 15.81 15.89 15.74

For dialogue act learning, we randomly sample 500 dialogues from the training set and recruit 3 native speakers to label dialogue acts2 for each utterance according to the definitions in Table 1.
Table 3 shows a labeling example from one annotator. Each utterance receives 3 labels, and the
Fleiss' kappa of the labeling work is 0.45, indicating moderate agreement among the labelers.

Table 3: An example of dialogue with labeled acts.
Turns A: The Great Wall of China is beautiful! B: Did you see the sunset on the Great Wall? A: Yes, it's the most beautiful scenery. B: It was very crowded when I visited there last time A: I only stayed there for a while. Too many vistors!
2.3 INSIGHTS FROM THE LABELED DATA

Dialogue Acts CM.S CM.Q CM.A CS.S CM.S

The frequencies of the dialogue acts in terms of percentages of the total number of utterances in the labeled data are CM.S 55.8%, CM.Q 11.7%, CM.A 12.2%, CS.S 12.4%, CS.Q 4.8%, CS.A 2%, and O 1.1%. In addition to the numbers, we also get further insights from the data that are instructive to our dialogue generation learning:
Context switch is a common skill to keep conversation going. In fact, we find that 78.2% dialogues contain at least one CS.* act. The average number of turns of dialogues that contain at least one CS.* is 8.4, while the average number of turns of dialogues that do not contain a CS.* is 7. When dialogues are shorter than 5 turns, only 47% of them contain a CS.*, but when dialogues exceeds 10 turns, more than 85% of them contain a CS.*. Because there are no specific goals in their conversations, people seldom stay long in one context. The average number of turns before context switch is 3.39. We also observed successive context switch in many dialogues (43.7%). The numbers suggest dialogue generation with smooth context switch and moderate context maintenance.
Question is an important building block in open domain conversation. In fact, 13.9% CM.* are CM.Q and the percentage is even higher in CS.* which is 20.27%. People need to ask questions in
1https://nlp.stanford.edu/software/tokenizer.shtml 2By default, the first utterance in a dialogue is labeled as CM.* except clear opening expressions such as "hello" or "morning", because the first utterance often follows the context of a post.

3

Under review as a conference paper at ICLR 2018

Decoder wi ,1
v '1

wi , 2 v '2

wi ,3 Softmax
v '3

wi , 4 v '4

wi ,5
v '5 ...

Attention

Encoder v1 v2 v3 v4 v5

w w wai

wi 1,1

i 1, 2

i 1,3

i 1, 4

(a) generation network

v6
wi1,5

1,1 1,2 1,3 1,4 2,1 2,2 2,3 2,4 3,1 3,2 3,3 3,4 4,1 4,2 4,3 4,4

1 2 3 4

1 2 3

4

1 2 3 4

MLP

0.01 CS.A 0.05 CS.Q 0.20 CS.S 0.05 CM.A 0.18 CM.Q 0.50 CM.S 0.01 Other

CM.S

(b) policy network

Figure 1: Policy network and generation network.
order to maintain contexts. The average number of turns of contexts with questions (i.e., consecutive CM.* with at least one CM.Q) is 3.92, while the average number of turns of contexts without questions is only 2.95. The observation indicates that a good dialogue model should be capable of asking questions properly, as suggested by Li et al. (2017a). A further step to study human's questioning behavior is to look into types and functions of questions. We leave it as future work.
The observations raise new challenges that are difficult for the existing end-to-end methods to tackle (e.g., smoothly interleaving context blocks with switch actions), and thus encourage us to create a new model. Note that these observations may relate to dialogue scenarios (e.g., chatting online instead of face-to-face) and cultures, but we ignore these factors and just study how to learn the conversational patterns from the data with a principled approach. The learning approach is generally applicable to other data. To perform learning, we need to build a classifier that can automatically tag large scale dialogues with dialogue acts.

2.4 DIALOGUE ACT CLASSIFICATION

We aim to learn a classifier c from DA = {di}iN=1 where di = {(ui,1, ai,1), . . . , (ui,ni , ai,ni )} represents a dialogue with ui,k the k-th utterance and ai,k the labeled dialogue act. Given a new dialogue d = {u1, . . . , un}, c can sequentially tag the utterances in d with dialouge acts by taking
ui, ui-1, and the predicted ai-1 as inputs and outputting a vector c(ui, ui-1, ai-1) where the j-th
element representing the probability of ui being tagged as the j-th dialogue act.

We parameterize c(·, ·, ·) using neural networks. Specifically, ui and ui-1 are first processed by

bidirectional recurrent neural networks with gated recurrent units (biGRUs) (Chung et al., 2014)

respectively. Then the last hidden states of the two biGRUs are concatenated with an embedding of

ai-1 and fed to a multi-layer perceptron (MLP) to calculate a dialogue act distribution. Formally,

suppose

that

ui

=

(wi,1, . . . , wi,n)

where

wi-,j is

the embedding of - -

the

j-th

word,

then

the

j-th

hidden state -

of

the

biGRU

is

given

by

hi,j

=

[ h i,j ;

h i,j] where

h i,j

is the j-th state of a forward - -

GRU, h i,j is the j-th state of a backward GRU, and [·; ·] is a concatenation operator. h i,j and h i,j

are calculated by

- -

- -

h i,j = fGRU( h i,j-1, wi,j ); h i,j = fGRU( h i,j+1, wi,j ).

(1)

Similarly, we have hi-1,j as the j-th hidden state of ui-1. Let e(ai-1) be the embedding of ai-1, then c(ui, ui-1, ai-1) is defined by a two-layer MLP:

c(ui, ui-1, ai-1) = fMLP([hi,n; hi-1,n; e(ai-1)]),

(2)

where we pad zeros for u0 and a0 in c(u1, u0, a0). We learn c(·, ·, ·) by minimizing cross entropy with DA. Let pj(ai) be the probability of ai being the j-th dialogue act and c(ui, ui-1, ai-1)[j] be
the j-th element of c(ui, ui-1, ai-1), then the objective function of learning is formulated as

N ni 7
- pj (ai,k) log(c(ui,k, ui,k-1, ai,k-1)[j]).
i=1 k=1 j=1

(3)

We randomly split the labeled dialogues as 400/30/70 dialogues with 3280/210/586 utterances for training/validation/test. Details of learning are given in Appendix 7.2. The learned classifier achieves an accuracy of 70.1% on the test data. We employ it to tag the training, validation, and test sets in Table 2.

4

Under review as a conference paper at ICLR 2018

3 DIALOGUE GENERATION MODEL

We present dialogue generation learning using large scale dialogues tagged with dialogue acts. Then, we show model optimization with reinforcement learning for long-term conversation.

3.1 SUPERVISED LEARNING

We aim to learn a dialogue generation model g from D = {di}Ni=1 where di = {(ui,1, ai,1), . . . , (ui,ni , ai,ni )} refers to a human-human dialogue with ui,k the k-th utterance and ai,k the dialogue act tagged by the classifier in Section 2.4. Given si = {(u1, a1), . . . , (ui-1, ai-1)} as a new dialogue session, g(si) can generate a response as the next turn of the dialogue.
Our dialogue model consists of a policy network and a generation network. A dialogue act is first selected from the policy network according to the conversation history, and then a response is generated from the generation network based on the conversation history and the dialogue act. Formally, the dialogue model can be formulated as

g(si) = pr(ri|si, ai)pa(ai|si),

(4)

where pa is the policy network and pr is the generation network.

Figure 1(b) shows the architecture of the policy network. The utterance sequence and the act se-
quence are encoded with a hierarchical encoder and a GRU encoder respectively. Then, the last
hidden states of the two encoders are concatenated and fed to an MLP to calculate a probability distribution of dialogue acts for the next turn. Formally, uj  si, uj is first transformed to hidden vectors {huj,k}nk=j 1 through a biGRU parameterized as Equation (1). Then, {hju,nj }ji-=11 is processed by a GRU parameterized as tk = fGuRU(tk-1, huk,nk ). In parallel, {a1, . . . , ai-1} is transformed to {hak}ik-=11 by hka = fGaRU(hka-1, e(ak)). pa(ai|si-1) is then defined by

pa(ai|si) = max fMLP([ti-1; hia-1])[j], j

(5)

where fMLP([ti-1; hia-1])[j] is the j-th element of the output of the MLP, and ai is set as the dialogue act corresponding to the maximal fMLP([ti-1; hia-1])[j].
We build the generation network in a sequence-to-sequence framework. Here, we simplify pr(ri|si, ai) as pr(ri|ai, ui-1, ui-2) since decoding natural language responses from long conversation history is challenging. Figure 1(a) illustrates the architecture of the generation network. The only difference from the standard encoder-decoder architecture with an attention mechanism is that in encoding, we concatenate ui-1 and ui-2, and attach ai to the top of the long sentence as a special word. The technique here is similar to that in zero-shot machine translation (Johnson et al., 2016). More formulation details can be found in Appendix 7.1.

The dialogue model is then learned by minimizing the negative log likelihood of D:

N ni
- [log(pr(ui,k|di,<k, ai,k)) + log(pa(ai,k|di,<k))],
i=1 k=1

(6)

where di,<k = {(ui,1, ai,1), . . . , (ui,k-1, ai,k-1)}. Through supervised learning, we fit the dialogue model to human-human interactions in order to learn their conversational patterns. However, supervised learning does not explicitly encourage long-term conversation. This motivates us to further optimize the model through a reinforcement learning approach.

3.2 REINFORCEMENT LEARNING

We optimize the dialogue model through self-play (Li et al., 2016b; Lewis et al., 2017) where we let two models learned with the supervised approach talk to each other in order to improve their performance. In the simulation, a dialogue is initialized with a message sampled from the training set. Then, the two models continue the dialogue by alternately taking the conversation history as an input and generating a response (top one in beam search) until T turns (T = 20 in our experiments).
To speed up training and avoid generated responses diverging from human language, we fix the generation network and only optimize the policy network by reinforcement learning. Thus, the policy in

5

Under review as a conference paper at ICLR 2018

learning is naturally defined by the policy network pa(ai|si) with si = {(u1, a1), . . . , (ui-1, ai-1)} a state and ai an action. We define a reward function r(ai, si) as

r(ai, si) = E[len(ai, si)] + E[rel(ai, si)],

(7)

where E[len(ai, si)] is the expected conversation length after taking action ai under state si,

E[rel(ai, si)] is the expected response relevance within the conversation,  = 0.67, and

 = 0.33. Through Equation (7), we try to encourage actions that can lead to long (mea-

sured by E[len(ai, si)]) and reasonable (measured by E[rel(ai, si)]) conversations. To estimate E[len(ai, si)] and E[rel(ai, si)], we fix (si, ai) and construct a dialogue set {di,j}jN=1 (N = 10 in

our experiments) by sampling after (si, ai) with self-play. j, di,j = (si, uj,i+1, . . . , uj,ni,j ) where k, uj,i+k is randomly sampled from the top 20 results of pr conditioned on a dialogue act sampled

according to pa for that turn. A simulated dialogue is terminated if (1) cosine(e(ui-1), e(ui)) > 0.9

&& cosine(e(ui)), e(ui+1)) > 0.9, or (2) cosine(e(ui-1), e(ui+1)) > 0.9, or (3) the length of the dialogue reaches T , where e(·) denotes the representation of an utterance given by the encoder of

pr. Condition (1) means three consecutive turns are (semantically) repetitive, and Condition (2)

means one agent gives repetitive responses in two consecutive turns. Both conditions indicate a high

probability that the conversation falls into a bad infinite loop. E[len(ai, si)] and E[rel(ai, si)] are

then estimated by

1N

1 N 1 ni,j

E[len(ai, si)]

=

N

ni,j ;
j=1

E[rel(ai, si)]

=

N

j=1

ni,j

m(di,j<k, uj,k),
k=1

(8)

where di,j<k = (u1, . . . , uj,k-1), and m(·, ·) is the dual LSTM model proposed in (Lowe et al., 2015) which measures the relevance between a response and a context. We train m(·, ·) with the

30 million crawled data through negative sampling. The objective of learning is to maximize the

expected future reward:

T

J () = E[ r(ai, si)].

(9)

i=1

The gradient of the objective is calculated by Reinforce algorithm (Williams, 1992):
TT
J  log(pa(at|st)) (r(ai, si) - bt) ,

(10)

t=1

i=t

where

the

baseline

bt

is

empirically

set

as

1 |A|

atA r(at, st) with A the space of actions.

4 EXPERIMENT

4.1 EXPERIMENT SETUP

Our experiments are conducted with the data in Table 2. The following methods are employed as baselines: (1) S2SA: sequence-to-sequence with attention (Bahdanau et al., 2015) in which utterances in contexts are concatenated as a long sentence. We use the implementation with Blocks (https://github.com/mila-udem/blocks); (2) HERD: the hierarchical encoder-decoder model in (Serban et al., 2016) implemented with the source code available at (https://github.com/julianser/hed-dlg-truncated); (3) VHERD: the hierarchical latent variable encoder-decoder model in (Serban et al., 2017) implemented with the source code available at (https://github.com/julianser/hed-dlg-truncated); and (4) RL-S2S: dialogue generation with reinforcement learning (Li et al., 2016b). We implement the algorihtm by finishing the code at (https://github.com/liuyuemaicha/ Deep-Reinforcement-Learning-for-Dialogue-Generation-in-tensorflow). Dull responses are defined as in (Li et al., 2016b) and listed in Appendix 7.3.
All baseline models are implemented with the recommended configurations in the existing literatures. We denote our Dialogue Act aware Generation Model with only Supervised Learning as SL-DAGM, and the full model (supervised learning + reinforcement learning) as RL-DAGM. Implementation details are given in Appendix 7.3.
4.2 RESPONSE GENERATION FOR GIVEN CONTEXTS

The first experiment is to check if the proposed models can generate high-quality responses regarding to given contexts. To this end, we take the last turn of each test dialogue as ground truth, and feed the previous turns as a context to different models for response generation. Top one responses from

6

Under review as a conference paper at ICLR 2018

beam search (beam size= 20) of different models are collected, randomly shuffled, and presented to 3 native speakers to judge their quality. Each response is rated by the three annotators under the following criteria: +2: the response is not only relevant and natural, but also informative and interesting; +1: the response can be used as a reply, but might not be informative enough (e.g.,"Yes, I see" etc.); 0: The response makes no sense, is irrelevant, or is grammatically broken.

Table 4: Evaluation Results

(a) Human annotations. Ratios are calculated by (b) Average dialogue length in machine-machine and

combining labels from the three judges.

human-machine conversations.

S2SA HERD VHERD RL-S2S SL-DAGM RL-DAGM

0 0.478 0.447 0.349 0.393 0.279 0.341

1 0.478 0.456 0.471 0.462 0.475 0.386

2 0.044 0.097 0.180 0.142 0.244 0.273

Kappa 0.528 0.492 0.494 0.501 0.508 0.485

RL-S2S SL-DAGM RL-DAGM

Machine-Machine 4.36 7.36 7.87

Human-Machine 4.54 5.24 5.58

Table 4(a) summarizes the annotation results. Improvements from our models over the baseline methods are statistically significant (t-test, p-value < 0.01). In addition to human annotations, we also compare different models on automatic metrics including BLEU, embedding based metrics (Liu et al., 2016), and distinct n-grams (Li et al., 2015). The conclusion is that our models are comparable with the state-of-the-art model on BLEU and embedding based metrics, but are significantly better than all models on distinct n-grams. Details can be found in Table 7 in Appendix 7.4. Both SLDAGM and RL-DAGM can significantly improve response diversity with both informative content and rich patterns under the context switch and the questioning mechanisms. This is supported by the much more +2 responses from the two models in Table 4(a) and the significant improvement on distinct n-grams in Table 7. On the other hand, due to the diversity, responses from our models may diverge from the ground truth sometimes. This is why improvements on other automatic metrics are not significant. To further explain the advantages of our models, we show an example in Table 5. In addition to responses from SL-DAGM and RL-DAGM, we also show responses from other reasonable but not selected acts. With dialogue acts, responses from our models become really rich, from confirmation (CM.Q) to an open question (CS.Q) and to a long informative statement (CS.S). More importantly, the dialogue acts let us know why we have such responses: both SL-DAGM and RL-DAGM try to switch to new topics (e.g., Xiamen, noodle, and plan etc.) in order to continue the conversation. One can also change the flow of the conversation by picking responses from other dialogue acts. The example demonstrates that in addition to good performance, our models enjoy good interpretability and controllability as well. We show more such examples in Appendix 7.5.
4.3 ENGAGEMENT TEST
Secondly, we study conversation engagement with the proposed models. Experiments are conducted through machine-machine simulation and human-machine conversation. In both experiments, we compare SL-DAGM and RL-DAGM with RL-S2S, as RL-S2S is the only baseline optimized for future success. Responses from all models are randomly sampled from the top 5 beam search results. Average length of dialogues is employed as an evaluation metric.
Machine-machine simulation is conducted in a way similar to (Li et al., 2016b) in which we let two bots equipped with the same model talk with each other in 1000 simulated dialogues. Each dialogue is initialized with the first utterance of a test example, and terminated according to the termination conditions for reward estimation in Section 3.2. In human-machine conversation, we recruit 5 native speakers as testers and ask them to talk with the bots equipped with the three models. Every time, a bot is randomly picked for a tester, and the tester does not know which model is behind. Every tester finishes 100 dialogues with each bot. To make a fair comparison, we let the bots start dialgoues. A starting message in a dialogue is randomly sampled from the test data and copied 3 times for all the 3 bots (a tester can skip the message if he/she cannot understand it). A dialogue is terminated if (1) the tester thinks the conversation cannot be continued (e.g., due to bad relevance or repetitive content etc.); or (2) the bot gives repetitive responses in two consecutive turns (measured by cosine(e(ui-1), e(ui+1)) > 0.9). Dialogue acts in human turns are tagged by the classifier in Section 2.4. The evaluation metric is calculated with the total 500 dialogues for each model.
Table 4(b) reports the evaluation results. In both experiments, SL-DAGM and RL-DAGM can lead to longer conversations, and the improvements from both models over the baseline are statistically significant (t-test, p-value < 0.01). Improvements in human-machine conversation are smaller than

7

Under review as a conference paper at ICLR 2018

Table 5: An example of response generation. Utterances in the context are split by "".

Context

Responses

S2SA: I think so

?      

HERDYou are already in vacation? VHERDHaha. RL-S2SI am having lunch now.

SL-DAGM                  

Have dinner together?  how

OK. I am just back from Xiamen, and want to have noodle

about lunch?  I can only have my

in cafeteria. (CS.S)

lunch at company.  Then I cannot

RL-DAGM What are you going to do

join you because I am in my vaca-

for your vacation? (CS.Q)

tion. CM.QYou are already in vacation?

CM.SI thought you were at work.

those in machine-machine simulation, indicating the gap between the simulation environment and the real conversation environment and encouraging us to consider online optimization in humanmachine conversations in the future. RL-DAGM is better than SL-DAGM in both experiments, indicating the efficacy of reinforcement learning. The reason that our models are better is that they captured conversational patterns in human-human interactions and obtained further optimization through reinforcement learning, while the baseline does not explicitly model the patterns. First, the models can pro-actively switch contexts. In machine-machine simulation, 65.4% (SL) and 94.4% (RL) dialogues contain at least one CS.*; and in human-machine conversation, the two percentages are 38.1% (SL) and 48.1% (RL) respectively. More interestingly, in machine-machine simulation, average lengths of dialogues without CS.* are only 4.78 (SL) and 2.67 (RL) respectively which are comparable with or even worse than RL-S2S, while average lengths of dialogues with CS.* are 8.66 (SL) and 8.18 (RL) respectively. The results demonstrate the importance of context switch for engagement in open domain conversation and one signficant effect of RL is promoting context switch in interactions. Second, the models can drive conversations by asking questions. In machinemachine simulation, 36.5% (SL) and 32.4% (RL) dialogues contain at least one question. The percentages in human-machine conversation are 17.7% (SL) and 22.3% (RL) respectively. We give more analysis in Appendix 7.6.

5 RELATED WORK
Existing dialogue models are either built for open domain conversation or for specific task completion. Regarding to the former, a common practice is to learn a generation model in an end-to-end way. On top of the basic sequence-to-sequence with attention architecture (Vinyals & Le, 2015; Shang et al., 2015), various extensions have been proposed to tackle the "safe response" problem (Li et al., 2015; Mou et al., 2016; Xing et al., 2017); to model complicated structures of conversation contexts (Serban et al., 2016; Sordoni et al., 2015); to bias responses to some specific persona or emotions (Li et al., 2016a; Zhou et al., 2017); and to pursue better optimization strategies (Li et al., 2017b; 2016b). On the other line of research, POMDP (Young et al., 2013) breaks down the development of task-oriented dialogue systems into natural language understanding (Yao et al., 2014; Henderson et al., 2014), dialogue management (Mrksic´ et al., 2016), and response generation (Wen et al., 2015). Recently, researchers also consider learning task-oriented dialogue models in an endto-end way (Wen et al., 2016; 2017; Bordes & Weston, 2017). In this work, we introduce dialogue acts into open domain dialogue generation. Different from those in task-oriented dialogue systems, the dialogue acts are generally designed for explaining engagement in social chat. To the best of our knowledge, we are the first who model open domain dialogue generation with dialogue acts. Before us, some researchers have proposed analyzing open domain dialogues with dialogue acts (Kim et al., 2010; Oraby et al., 2017; Ivanovic, 2005; Wallace et al., 2013; Wu et al., 2005; Ritter et al., 2010). These work, however, stops at performing utterance classification or clustering. Our dialogue act design is inspired by these work, but we not only exploit the dialogue acts to explain open domain dialogues, but also conduct dialogue generation with the dialogue acts.

6 CONCLUSION
We study open domain dialogue generation with generally designed dialogue acts that can describe human behavior in social interactions. To mimic such behavior, we propose jointly modeling dialogue act selection and response generation, and perform both supervised learning with a learned dialogue act classifier and reinforcement learning for long-term conversation. Empirical studies on response generation for given contexts, machine-machine simulation, and human-machine conversation show that the proposed models can significantly outperform state-of-the-art methods.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.
Antoine Bordes and Jason Weston. Learning end-to-end goal-oriented dialog. ICLR, 2017.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Mihail Eric and Christopher D Manning. Key-value retrieval networks for task-oriented dialogue. arXiv preprint arXiv:1705.05414, 2017.
Matthew Henderson, Blaise Thomson, and Steve Young. Word-based dialog state tracking with recurrent neural networks. In SIGDIAL, pp. 292­299, 2014.
Edward Ivanovic. Dialogue act tagging for instant messaging chat sessions. In Proceedings of the ACL Student Research Workshop, pp. 79­84. Association for Computational Linguistics, 2005.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vie´gas, Martin Wattenberg, Greg Corrado, et al. Google's multilingual neural machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558, 2016.
Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. Classifying dialogue acts in one-on-one live chats. 2010.
Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-toend learning of negotiation dialogues. In EMNLP, pp. 2433­2443, 2017.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. NAACL, 2015.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A persona-based neural conversation model. ACL, 2016a.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016b.
Jiwei Li, Alexander H Miller, Sumit Chopra, Jason Weston, et al. Learning through dialogue interactions by asking questions. ICLR, 2017a.
Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. EMNLP, 2017b.
Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. EMNLP, 2016.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909, 2015.
Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. arXiv preprint arXiv:1607.00970, 2016.
Nikola Mrksic´, Diarmuid O Se´aghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. Neural belief tracker: Data-driven dialogue state tracking. arXiv preprint arXiv:1606.03777, 2016.
Shereen Oraby, Pritam Gundecha, Jalal Mahmud, Mansurul Bhuiyan, and Rama Akkiraju. How may i help you?: Modeling twitter customer serviceconversations using fine-grained dialogue acts. In Proceedings of the 22nd International Conference on Intelligent User Interfaces, pp. 343­355. ACM, 2017.
9

Under review as a conference paper at ICLR 2018
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pp. 311­318. Association for Computational Linguistics, 2002.
Alan Ritter, Colin Cherry, and Bill Dolan. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 172­180. Association for Computational Linguistics, 2010.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle Pineau. End-to-end dialogue systems using generative hierarchical neural network models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA., pp. 3776­3784, 2016.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, pp. 3295­3301, 2017.
Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation. Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1577­1586, 2015.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of conversational responses. arXiv preprint arXiv:1506.06714, 2015.
Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.
Byron C Wallace, Thomas A Trikalinos, M Barton Laws, Ira B Wilson, and Eugene Charniak. A generative joint, additive, sequential model of topics and speech acts in patient-doctor communication. In EMNLP, pp. 1765­1775, 2013.
Tsung-hsien Wen, Pei-hao Su, V David, and Steve Young. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In In EMNLP. Citeseer, 2015.
Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562, 2016.
Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and Steve Young. Latent intention dialogue models. arXiv preprint arXiv:1705.10229, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Tianhao Wu, Faisal M Khan, Todd A Fisher, Lori A Shuler, and William M Pottenger. Posting act tagging using transformation-based learning. In Foundations of data mining and knowledge discovery, pp. 319­331. Springer, 2005.
Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. Topic aware neural response generation. In AAAI, pp. 3351­3357, 2017.
Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geoffrey Zweig, and Yangyang Shi. Spoken language understanding using long short-term memory neural networks. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pp. 189­194. IEEE, 2014.
Stephanie Young, Milica Gasic, Blaise Thomson, and John D Williams. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160­1179, 2013.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. Emotional chatting machine: Emotional conversation generation with internal and external memory. arXiv preprint arXiv:1704.01074, 2017.
10

Under review as a conference paper at ICLR 2018

7 APPENDIX

7.1 GENERATION NETWORK

Suppose that xi = word, then the k-th

[ai; ui-1; ui-2 hidden state of

] = (wi,1, . the encoder

.. is

g, wivie,nnib)ywvhi,ekre=w[i-v,ki,iks;

the embedding -v i,k] where

of

the

k-th

-v i,k = fGeRU(-v i,k-1, wi,k); -v i,k = fGeRU(-v i,k+1, wi,k)

(11)

Positions of u-1 and u0 in x1 and x2 are padded with zeros. Let ri = (wi,1, . . . , wi,T ), then in decoding the j-th word wi,j, {vi,1, . . . , vi,ni } is summarized as a context vector ci,j through an attention mechanism:

ni
ci,j = j,kvi,k; j,k =
k=1

exp(ej,k) ;

ni m=1

exp(ej,m

)

ej,k

=

v

tanh(W[vi,k; vi,j-1]),

(12)

where v and W are parameters, and vi,j-1 is the (j - 1)-th hidden state of the decoder GRU in

which vi,j is calculated by

vi,j = fGdRU(vi,j-1, wi,j-1, ci,j ).

(13)

The generation probability of wi,j is then defined as

pr(wi,j |wi,<j , xi) = I(wi,j ) sof tmax(wi,j-1, vi,j ),

(14)

where I(wi,j) is a vector with only one element 1 indicating the index of wi,j in the vocabulary. pr(ri|ai, ui-1, ui-2) is finally defined as

T
pr(ri|ai, ui-1, ui-2) = pr(wi,1|xi) pr(wi,j |wi,<j , xi).
j=2

(15)

7.2 DETAILS OF LEARNING THE DIALOGUE ACT CLASSIFIER
We randomly split the 500 labeled dialogues as 400, 30, and 70 dialogues for training, validation, and test respectively. Utterances in the three sets are 3280, 210, and 586 respectively. In training, we represent dialogue acts as probability distributions by averaging the labels given by the three annotators. For example, if an utterance is labeled as "CM.S", "CM.S", and "CS.S", then probability distribution is (0.67, 0, 0, 0.33, 0, 0, 0). In test, we predict the dialogue act of an utterance ui by arg maxj g(ui, ui-1, ai-1)[j]. To avoid overfitting, we pre-train word embeddings using word2vec3 with an embedding size of 200 on the 30 million data and fix them in training. We set the embedding size of the dialogue acts and the hidden state size of the biGRUs as 100, and the dimensions of the first layer and the second layer of the MLP as 200 and 7 respectively. We optimize the objective function (3) using back-propagation and the parameters are updated by stochastic gradient descent with AdaDelta algorithm Zeiler (2012). The best performing model on the validation data is picked up for test.

7.3 DETAILS OF IMPLEMENTATION OF THE DIALOGUE GENERATION MODEL
In learning of the generation network, we set the size of word embedding as 620 and the size of hidden vectors as 1024 in both the encoder and the decoder. Both the encoder vocabulary and the decoder vocabulary contain 30, 000 words. Words out of the vocabularies are replaced by a special token "UNK". We employ AdaDelta algorithm Zeiler (2012) to train the generation network with a batch size 128. We set the initial learning rate as 1.0 and reduce it by half if perplexity on validation begins to increase. We will stop training if the perplexity on validation keeps increasing in two successive epochs.
In learning of the policy network, we set the size of word embedding, the size of dialogue act, and the size of hidden states of the biGRU as 100. There are 50 neurons in the first layer of the MLP and 7 neurons in the second layer of the MLP. Vectors in the policy network have smaller sizes than
3https://code.google.com/archive/p/word2vec/

11

Under review as a conference paper at ICLR 2018

those in the generation network because the complexity of dialogue act prediction is much lower than language generation.
In reinforcement learning, the size of mini-batch is 60 and learning rate is fixed as 0.05. To estimate the reward, we train a dual LSTM (Lowe et al., 2015) with the size of word embedding and the size of hidden states as 100. Responses from the simulated dialogues are generated with a beam size 20.
In RL-S2S, we define 8 responses as dull responses according to the frequency of responses in the training set. Table 6 gives the responses.

Table 6: Dull responses for learning RL-S2S.

No. Chinese responses English translations

1 

I do not know.

2  I think you are right.

3 

Are you a man or a woman?

4 

I see.

5 

I do not know either.

6 

You are right.

7  I think so.

8 

OK.

7.4 AUTOMATIC EVALUATION FOR RESPONSE QUALITY
We evaluate the performance of different models in terms of response quality for given contexts by comparing the generated responses with the ground truth using automatic metrics. These metrics include (1) BLEU (Papineni et al., 2002) which measures term overlap of two responses; (2) embedding based metrics (Liu et al., 2016) such as Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) which measure similarity of two responses in a semantic space; and (3) ratios of distinct unigrams (distinct-1) and bigrams (distinct-2) in the generated responses which are employed in (Li et al., 2015) to measure response diversity.

Table 7: Automatic evaluation results. Numbers in bold mean that improvement from the model on

that metric is statistically significant over the baseline methods (t-test, p-value < 0.01).

BLEU-1 BLEU-2 Average Extrema Greedy Distinct-1 Distinct-2

S2SA

4.67 1.18 21.45 16.68 21.53 0.033 0.069

HERD

3.70 1.06 16.87 13.58 20.15 0.062 0.139

VHERD

6.10 1.76 20.83 16.17 21.36 0.079 0.225

RL-S2S

5.57 1.83 20.72 16.73 20.64 0.100 0.213

SL-DAGM 6.23

2.07 20.68 16.42 21.52 0.200

0.466

RL-DAGM 6.77

2.12 21.18 16.97 21.76 0.223

0.503

Table 7 summarizes the results. We can see that our models are significantly better than all baselines on distinct-1 and distinct-2. This is because we search a response not only from a language space, but also from an act space. The dimension of dialogue acts provides further variations to the generated responses. On the other hand, the diversity of responses makes them diverge from the ground truth. That is why there is no significant improvement on other metrics.

7.5 MORE EXAMPLES OF RESPONSE GENERATION
We compare SL-DAGM and RL-DAGM with baseline models in terms of response quality for given contexts with more examples in Table 8.

7.6 FURTHER ANALYSIS ON ENGAGEMENT TEST
Table 9 gives some examples on machine-machine simulation. Unlike the dialogues from RL-S2S which quickly converge to loops, dialogues from our models move forward under the management of the dialogue acts. The dialogue acts let us know why such responses are generated and make the

12

Under review as a conference paper at ICLR 2018

Table 8: More examples of response generation. Utterances in the context are split by "".

Context

Responses

   
 

S2SA: I think so HERDHaha. VHERDSo you are praising me? RL-S2SI am a good boy too.

Your have dark circles under your eyes. Always stay up late?  I
never stay up late after I graduate from my university.  Good boy! Early to bed, early to rise.  Yes.

SL-DAGM But I often have poor sleep because of jet lag. (CS.S) RL-DAGM Then when do you get up everyday? (CS.Q)
CM.QAre you going to sleep? CM.SI sleep early as well.

S2SA: I have sent you an email.

?    800 

HERDCould you please add me in your WeChat?
VHERDAdd me in your WeChat. RL-S2SI have sent you an email.

How about renting an apartment together?  How much? 800.  Then add me as a contact in
WeChat.

SL-DAGMI want to play board games with you. (CS.S)
RL-DAGM       When can I contact you? (CM.Q)

CS.QWhich district do you live in?

CM.SGive me your WeChat ID.

Dialogue Length

16 RL-DAGM 16 SL-DAGM 16

RL-S2S

14 14 14

12 12 12

10 10 10

888

666

444

222

01

234 tester ID

5

01

234 tester ID

5

01

234 tester ID

5

Figure 2: Evaluation results on human-machine conversation in terms of different testers.
simulated dialogues closer to human dialogues by moderate context continuation and jumping out of the contexts at proper timing. Table 10 and Table 11 show some examples from the test of humanmachine conversation. We denote a machine turn as "M" and a human turn as "H". After each example, we give the reason of termination in which "EOD-H" means the dialogue is terminated by the tester and "EOD-R" means the dialogue is terminated by the repetition check with the next generated turn attached. Compared to dialogues with the baseline, dialogues with our models can go deeper with much richer content, although a side-effect is that sometimes responses from CS.* might be against common sense (e.g., the first example of SL-DAGM). This sheds light on our future direction to further improve the generation network with knowledge. In addition to the qualitative results, we also show quantitative results of human-machine conversation test in terms of different testers in Figure 2. Although there exists variance among the testers, the overall trend is consistent with the numbers in Table 4(b).

13

Under review as a conference paper at ICLR 2018

Table 9: Comparison of simulated dialogues from different models.

RL-S2S

SL-DAGM

RL-DAGM

A:  B:  A:  B:  A: 
A: Hello, Wuxi girl! B: I come from Wuxi. How about you? A: I come from Shanghai. B: I come from Shanghai too. A: I come from Shanghai too.

A: (CM.S) B: (CM.Q) A: (CM.A) B: (CM.S) A:         (CS.Q)
A: Hello, Wuxi girl! (CM.S) B: You also come from Wuxi? (CM.Q) A: Yes, I am. (CM.A) B: I am a college student in Wuxi. (CM.S) A: Is there any delicious food in Wuxi? (CS.Q)

A: (CM.S) B: (CS.Q) A:          (CS.S) B: (CM.S) A:          (CM.S)
A: Hello, Wuxi girl! (CM.S) B: Hello, you also in Jiangsu province? (Wuxi is a city in Jiangsu) (CS.Q) A: I am not born in Jiangsu, but I go to college in Jiangsu. (CS.S) B: Many people in Jiangsu are natives. (CM.S) A: But most people around me are not natives. (CM.S)

A:  . B:  A:  B:  
A: I can learn English in Singapore and Malaysia. B: Are there any interesting places in Singapore? A: Are there any interesting places? B: Are there any interesting places in Singapore? I want to have a tour in Singapore.

A:  (CM.S) B: (CS.Q) A: (CM.S) B: (CM.S) A: (CS.Q)
A: I can learn English in Singapore and Malaysia. (CM.S) B: What is your English name? (CS.Q) A: I think I can write it out. (CM.S) B: What is your family name? (CM.S) A: When shall I write my English name? (CS.Q)

A:  (CM.S) B:          (CM.Q) A: (CM.A) B: (CS.Q) A:         (CM.Q)
A: I can learn English in Singapore and Malaysia. (CM.S) B: You also want to go there to learn English? (CM.Q) A: Yep. (CM.A) B: What are you major in? (CS.Q) A: What majors do we have? (CM.Q)

14

Under review as a conference paper at ICLR 2018

Table 10: Example 1 of human-machine conversation. "M" means a machine turn, and "H" means

a human turn.

RL-S2S

SL-DAGM

RL-DAGM

M:  H:  M:  H:  M:  H:  EOD-R ()
M: I am full with only soup. H: Then you can lose weight. M: Is it tough to lose weight? H: It's indeed tough when you have to diet. M: I have to diet. H: Then diet? EOD-R (I have to diet as well.)

M: (CM.S) H: (CS.S) M:          (CM.S) H: (CM.A) M: (CS.S) H: (CM.S) M:         (CM.S) EOD-H
M: I am full with only soup. (CM.S) H: Then you can lose weight. (CS.S) M: I really want to get a little fatter. (CM.S) H: I am jealous of you! (CM.A) M: I am trying to gain weight. (CS.S) H: Then have more ice cream. (CM.S) M: I even ate Song of Ice and Fire.(CM.S) EOD-H

M: (CM.S) H: (CS.S) M: (CS.Q) H: (CM.A) M: (CS.Q) H: (CM.A) M: (CM.S) H:          (CM.S) M:  (CS.Q) H: (CM.A) EOD-R ( )
M: I am full with only soup. (CM.S) H: Then you can lose weight. (CS.S) M: Do you eat albumen powder? (CS.Q) H: Nope. (CM.A) M: What is enzyme powder? (CS.Q) H: Something can make you slim. (CM.A) M: I want to try it! (CM.S) H: Go ahead, and do not give up. (CM.S) M: Can I get enzyme powder in pharmacy? How much is it? (CS.Q) H: Yes you can. It's not expensive. (CM.A) EOD-R (Can I get it in pharmacy? How much?)

15

Under review as a conference paper at ICLR 2018

Table 11: Example 2 of human-machine conversation. "M" means a machine turn, and "H" means

a human turn.

RL-S2S

SL-DAGM

RL-DAGM

M:  H:  M:  H:  EOD-R 
M: Light makeup looks good for you. H: Well, you are right. M: Thank you. Let's do it together! H: Do it together! EOD-R (Do it together!)

M:        (CM.S) H: (CM.A) M: (CS.S) H: (CM.S) M: (CM.S) H: (CM.S) M:  (CM.S) H: (CM.S) M: (CM.Q) H: (CM.A) EOD-R()
M: Light makeup looks good for you. (CM.S) H: Well, you are right. (CM.A)
M: Only use foundation and lipstick. (CS.S) H: Recommend me a brand of foundation. (CM.S) M: Shiseido is good. (CM.S)
H: That is great. (CM.S)
M: Because I used to try it for sometime, and feel good. (CM.S) H: Sounds good. (CM.S)
M: How many times have you used it? (CM.Q) H: Never. (CM.A)
EOD-R (How many times have you used it?)

M:        (CM.S) H: (CM.A) M:          (CS.Q) H: (CM.A) M: (CS.Q) H: (CM.S) EOD-R ()
M: Light makeup looks good for you. (CM.S) H: Well, you are right. (CM.A) M: Does heavy makeup make your complexion look bad? (CS.Q) H: Nope. (CM.S) M: Then is there any side-effect? (CS.Q) H: My skin will become worse. (CM.S) EOD-R (is there any side-effect?)

16

