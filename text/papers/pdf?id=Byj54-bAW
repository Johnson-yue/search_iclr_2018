Under review as a conference paper at ICLR 2018
A TENSOR ANALYSIS ON DENSE CONNECTIVITY VIA CONVOLUTIONAL ARITHMETIC CIRCUITS
Anonymous authors Paper under double-blind review
ABSTRACT
Several state of the art convolutional networks rely on inter-connecting different layers to ease the flow of information and gradient between their input and output layers. These techniques have enabled practitioners to successfully train deep convolutional networks with hundreds of layers. Particularly, a novel way of interconnecting layers was introduced as the Dense Convolutional Network (DenseNet) and has achieved state of the art performance on relevant image recognition tasks. Despite their notable empirical success, their theoretical understanding is still limited. In this work, we address this problem by analyzing the effect of layer interconnection on the overall expressive power of a convolutional network. In particular, the connections used in DenseNet are compared with other types of inter-layer connectivity. We carry out a tensor analysis on the expressive power inter-connections on convolutional arithmetic circuits (ConvACs) and relate our results to standard convolutional networks. The analysis leads to performance bounds and practical guidelines for design of ConvACs. The generalization of these results are discussed for other kinds of convolutional networks via generalized tensor decompositions.
1 INTRODUCTION
Recently, densely connected networks such as FractalNet (Larsson et al., 2016), ResNet (He et al., 2016), and DenseNet (Huang et al., 2016), have obtained state of the art performance on large problems where highly deep network configurations are used. Adding dense connections between different layers of a network virtually shortens its depth, thus allowing a better flow of information and gradient through the network. This makes possible the training of highly deep models. Models with these types of connections have been successfully trained with hundreds of layers. More specifically, DenseNets have achieved state of the art performance on the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets, using models of up to 1 thousand layers in depth. Nevertheless, whether these connections provide a fundamental enhancement on the expressive power of a network, or just improve the training of the model, is still an open question. In Huang et al. (2016), DenseNet models with 3 times less parameters than its counterpart (ResNets) were able to achieve the same performance on the ImageNet challenge. Moreover, a theoretical understanding of why the connections used by DenseNets lead to better performance compared with FractalNets or ResNets is still pending.
Despite the popularity of these models, there are few theoretical frameworks explaining the power of these models and providing insights to their performance. In Cohen et al. (2016a), the authors considered convolutional networks with linear activations and product pooling layers, called convolutional arithmetic circuits (ConvACs), and argued for the expressiveness of deep networks using a tensor based analysis. This analysis has been extended to rectifier based convolutional networks via generalization of the tensor product Cohen & Shashua (2016a). In Cohen & Shashua (2016a), it was shown that ConvACs enjoy a greater expressive power than rectifier based models despite the popularity of rectifier based networks in practice. Indeed the empirical relevance of ConvAC was demonstrated through an architecture called SimNets Cohen et al. (2016b). In addition, the generative ConvAC of Sharir et al. (2016) achieved state of the art performance in classification of images with missing pixels. These results served as motivation for the works of Cohen & Shashua (2016b); Cohen et al. (2017); Levine et al. (2017); Sharir & Shashua (2017), where different aspects of ConvACs were studied from a theoretical perspective.
1

Under review as a conference paper at ICLR 2018

In Cohen & Shashua (2016b) the inductive bias introduced by pooling geometries was studied. Later, Levine et al. (2017) makes use of the quantum entanglement measure to analyze the inductive bias introduced by the correlations among the channels of ConvACs. Moreover, Sharir & Shashua (2017) generalizes the convolutional layer of ConvACs by allowing overlapping receptive fields, in other words permitting stride values lower than the convolution patch size. These locally overlapping connections led to an enhancement on the expressive capacity of ConvACs. The notion of inter-layer connectivity for ConvACs was addressed by Cohen et al. (2017) in the context of sequential data processing, such as audio and text related tasks. In that work, the expressive capabilities of interconnecting processing blocks from a sequence was studied. Nevertheless, these types of interconnections are related to the sequential nature of the problem and different from the ones used in ResNet, FractalNet and DenseNet.
In this work, we extend the tensor analysis framework of Cohen et al. (2016a) to obtain insightful knowledge about the effect of dense connections, from the kind used in DenseNets, FractalNet and ResNet, on the expressiveness of deep ConvACs. We study the expressive capabilities provided by different types of dense connections. Moreover, from these results we derive performance bounds and practical guidelines for selection of the hyperparameters of a deep ConvAC, such as layer widths and the topology of dense connections. These results serve as the first step into understanding dense connectivity in rectifier networks as well, since they can be further extended to include rectifier linear units, in the same spirit as the generalization of the tensor products done by Cohen & Shashua (2016a).
The remainder of this paper is organized as follows. In Section 2, we introduce the notation and basic concepts from tensor algebra. In Section 3, we present the tensor representation of ConvACs as introduced by Cohen et al. (2016a), and later in Section 4, we obtain tensor representations for densely connected ConvACs. In Section 5, performance bounds and design guidelines are derived for densely connected ConvACs.

2 PRELIMINARIES

The term tensor refers to a multi-dimensional array, where the order of the tensor corresponds
to the number of indexes required to access one of its entries. For instance, a vector is a tensor
of order 1 while a matrix is a tensor of order 2. In general a tensor A of order N requires N
indexes (d1, . . . , dN ) to access one of its elements. For the sake of notation, given I  N, we use the expression [I] to denote the set {1, 2, . . . , I}. In addition, the (d1, . . . , dN )-th entry of a given tensor of order N and size M1 × M2 × · · · × MN is denoted as Ad1,...,dN , where di  [Mi] for all i  [N ]. Moreover, for the particular case of tensors of order N with symmetric sizes M1 = M2 = · · · = MN = M , we use (RM )N as shorthand notation for RM×···×M . A crucial operator in tensor analysis is the tensor product , since it is necessary for defining the rank of a tensor. For two tensors B  RM1×···×Mp and C  R ,Mp+1×···×Mp+q the tensor product is defined such that B  C  RM1×···×Mp+q and (B  C)d1,...,dp+q = B Cd1,...,dp dp+1,...,dp+q for all (d1, . . . , dp+q). In tensor algebra, a tensor A  RM1×M2×···×MN is said to have rank 1 if it can
be expressed as A = v(1)  · · ·  v(N), where v(i)  RMi for all i  [N ]. Moreover, any tensor A  RM1×M2×···×MN can be expressed as a sum of rank-1 tensors, that is

Z
A = vz(1)  · · ·  vz(N) ,
z=1

(1)

where Z  N is sufficiently large and vz(i)  RMi for i  [N ]. Note that this statement is

trivial for Z =

N i=1

Mi

.

On the other hand, when Z is the minimum number such that (1)

is satisfied, the rank of the tensor is defined to be rank(A) = Z and (1) becomes equivalent

to the well known CANDECOMP/PARAFAC (CP) decomposition of A. Another operator, that

is on the core of the former works of Cohen & Shashua (2016a); Cohen et al. (2016a); Levine

et al. (2017), is the matricization operator. The operator [A] denotes the matricization of a tensor

A  RM1×···×MN of order N . This matricization of the tensor A re-orders its elements into a matrix

[A]  RM1·M3···MN-1×M2·M4···MN with Ady1,...,dN in the row 1 +

Ni=/12(d2i-1 - 1)

N/2 j=i+1

M2j-1

and column 1 +

N/2 i=1

(d2i-1

-

1)

N/2 j=i+1

M2j

.

This is operator is of great use since it enjoys

properties such as [A  B] = [A] [B] and rank(A)  rank([A]), where denotes the Kronecker

2

Under review as a conference paper at ICLR 2018

product of two matrices. Note that, since the Kronecker product is multiplicative in the rank, we have that rank(A  B)  rank([A  B]) = rank([A])rank([B]) which is a central property of this theoretical analysis framework.
3 CONVOLUTIONAL ARITHMETIC CIRCUITS AS TENSOR DECOMPOSITIONS
conv pool

M ZZ

fd (xi)

pool(z) =

N i=1

conv(i,

z)

conv(i, z) = az,i, i

hy(X) = ay, pool(:)

i = [f1 (xi), . . . , fM (xi)] (a)

···

M

r0 r0

rL-1

rL-1

fd (xi) conv0(j, ) = a0,j, , j

poolL-1() = j {1,2} convL-1(j , )

i = [f1 (xi), . . . , fM (xi)]

hy(X) = aL,1,y, poolL-1(:)

pool0(j, ) = j {2j-1,2j} conv0(j , )

(b)

Figure 1: (a) Example of a shallow (i.e., L = 1) convolutional arithmetic circuit. (b) Example of a deep convolutional arithmetic circuit.

A ConvAC is a convolutional neural network that utilizes linear activation functions with product
pooling, unlike most popular convolutional networks which make use of rectifier activations with max or average pooling. Moreover, the input of the network is modeled by X = (x1, . . . , xN )  (Rs)N , where xi  Rs denotes the vectorization of the i-th patch of the input image. For this analysis, it is assumed that a set of M features is obtained from every patch, that is fd (xi)  R for all i  [N ], d  [M ]. These features are selected from a given parametric family F = {f : Rs  R :   }, such as Gaussian kernels, wavelet functions, or learned features. Then, to determine whether an input X belongs to a class belonging to the set Y, the network evaluates the some score functions hy(X)  R and decides for the class y  Y such that

hy(X) = max hy(X) .
yY

Using this formulation, in Figure 1(a) we observe an example of a single hidden layer ConvAC, while in Figure 1(b) we observe the general case of a deep arithmetic circuit of L layers. As shown by Cohen et al. (2016a), any score function of a ConvAC can be expressed as an homogeneous polynomial with degree N on the input features of the form

MN

hy(X) =

Ayd1 ,...,dN

fdi (xi) ,

d1,...,dN =1

i=1

(2)

where Ayd1,...,dN  R are the polynomial coefficients stored in the grid-tensor Ay  (RM )N .
In other words, a score function hy(X) is a polynomial of M N variables fd (xi)  R for all i  [N ], d  [M ], degree N , and M N polynomial coefficients stored in the grid-tensor Ay.

3

Under review as a conference paper at ICLR 2018

For the special case of a shallow ConvAC with 1 × 1 convolutions and Z hidden units1, shown in Figure 1(a), the score functions are computed from the weight vectors az,i [az1,i, . . . , azM,i]T  RM and ay [ay1, . . . , ayZ ]T  RZ for all i  [N ] and z  [Z]. This leads to the score function

Z Z NM

hy(X) = ay, pool(:) = ayz pool(z) = ayz

azd,ifd (xi) .

z=1

z=1 i=1 d=1

(3)

The first step of the tensor analysis framework is to obtain an expression (in terms of the network

Ipnaroatmheertewrsoradzys,aonbdtaaidzn,iin)gofthteheexgprrieds-steionnsofrorAAy ythtahtatretrparnessfeonrtms sth(i2s)cionntocr(e3te).

network architecture. This expression was

already obtained in Cohen & Shashua (2016a) as

Z
Ay = ayz az,1  az,2  · · ·  az,N ,
z=1

(4)

where  denotes the tensor product. Note that (4) is in the form of a standard CP decomposition of the grid tensor Ay. This implies that the rank of Ay is bounded by rank(Ay)  Z. Moreover, the
obtained results where generalized in Cohen et al. (2016a) for the case of a deep ConvAC with size-2 pooling windows2, thus L = log2N hidden layers as shown in Figure 1(b), leading to a grid-tensor given by the hierarchical tensor decomposition

r0

1,j, =

a1,j, a0,2j-1,  a0,2j,

=1

...

rl-1

l,j, =

al,j, l-1,2j-1,  l-1,2j,

=1

...

rL-1

Ay = L,1,1 =

aL,1,yL-1,1,  L-1,2, ,

=1

(5)

where r0, . . . , rL-1  N are the number of channels in the hidden layers, {a0,j,  RM }j[N],[r0] are the weights in the first hidden convolutions, {al,j,  RM }j[N/2l],[rl] are the weights of the hidden layers, and aL,1,y  RrL-1 stores the weights corresponding to the output y in the output
layer.

4 DENSELY CONNECTED ARITHMETIC CIRCUITS
The recent empirical success of densely connected networks (DenseNets), presented by Huang et al. (2016), has served as motivation for our theoretical analysis on dense connectivity. Dense connectivity in a convolutional neural network refers to the case when a number k  N (known as growth rate) of previous layers serve as input of the forthcoming layer. More precisely, in Huang et al. (2016), a DenseNet performs this via concatenation along the feature dimension of the current layer inputs with the preceding layer features. Note that these feature must have compatible sizes along the spatial dimension for the concatenation to be possible. To address this issue, Huang et al. (2016) proposed to group blocks of the same spatial dimensions into a dense block, as shown in Figure 2. These dense blocks do not contain operations such as pooling, that alter the spatial dimensions of the input features. Moreover, in the DenseNet architecture the layers that perform the pooling operation are called transition layers, since they serve as transition between dense blocks. For example, in Figure 2 we depict a dense block of 4 layers with growth rate k = 2, followed by a transition layer.
1We must mention that the generalization to w × w convolutions is straightforward and was already covered by Cohen & Shashua (2016a).
2Note that the generalization to different pooling sizes is straight forward and was done by Cohen & Shashua (2016a).

4

Under review as a conference paper at ICLR 2018
conv + act-fun concat pool

Dense Block

Transition Layer

Figure 2: Example of a dense block of size 4 with growth rate k = 2.

In the original DenseNet these transition layers included one convolution layer before the pooling operation. Nevertheless, for this work we consider transition layers composed of only pooling operations. Note that this does not affect the generality of the model, since avoiding dense connections on the convolutional layer preceding the transition layer is equivalent to including a convolution in that transition layer3.
In the case of ConvACs, any dense block of size greater than 1 can be represented as a dense block of size 1, since the activation function is the linear function (the non-linearity comes from the product pooling operator in the transition layer). Therefore, for ConvACs, it is only reasonable to analyze dense blocks of size 1. Note that, if we only allow dense connections between hidden layers within a dense block, a ConvAC is limited to a maximum growth rate of k = 1. In order to analyze the effect of broader connectivity we extend the concept of growth rate by allowing dense connections between dense blocks. With proper pooling, outputs of hidden layers belonging to different dense blocks can also be concatenated along the feature dimension. In the reminder of this paper we refer to the dense connections between hidden layers of the same block as intra-block connections, while the connections between hidden layers of different blocks as inter-block connections.

4.1 DENSE INTRA-BLOCK CONNECTIONS

In this section we analyze the effect of intra-block connections. We first start by constructing a densely connected version of a single hidden layer ConvAC. The resulting network with growth rate k = 1 is shown in Figure 3(a). In the same manner as in (3), this architecture leads to the score function

Z NM

Z+M

N

hy(X) = azy

adz,ifd (xi) +

azy fz-Z (xi) .

z=1 i=1 d=1

z=Z+1 i=1

(6)

Then, we present the following proposition regarding shallow ConvACs with dense connections of growth rate k = 1.

Proposition 1 The network's function of a densely connected shallow ConvAC shown in (6) corresponds to the grid tensor

Z

Ay = azyaz,1  az,2  · · ·  az,N + Sdiag

z=1

N

azy+Z

M z=1

,

(7)

where aZy +1,

SdiagN ayz+Z . . . , aZy +M in its

M z=1



(RM

diagonal.

)N

denotes

the

super-diagonal

tensor

of

order

N

with

Proof See appendix B.1.

Note that the rank of this tensor is now bounded by rank(Ay)  Z + M instead of Z, but adding these dense connections increases the number of parameters of the network from M N Z + Z to
3This would effectively reduce the dense block size by 1.

5

Under review as a conference paper at ICLR 2018

conv concat
pool

MM

M

ZZ

hy(X) = ay, pool(:)

i fd (xi)

pool(z) =

N i=1

conv(i,

z)

conv(i, z) = az,i, i

(a)

M M r0

r0 r1 r1

M

r0

r0

r1 r1

r2 r2

(b) M

M

M

r0

r0

r1 r1

r2 r2

(c)

Figure 3: (a) Example of a shallow (L = 1) convolutional arithmetic circuit with one intra-block connection. (b) Example of a 3 layered (L = 3) convolutional arithmetic circuit with multiple intrablock connections. (c) Example of a 3 layered (L = 3) convolutional arithmetic circuit with one inter-block connection.

M N Z +Z +M . Then, for large values of N , there is no clear advantage on using dense connections on a shallow ConvAC. Nevertheless, in Section 5 we show that dense connections are capable of increasing the expressive power of deep ConvACs, specially for large values of N .

We now generalize the obtained results for the case of a L-layered dense arithmetic circuit, with growth rare k = 1, as the one in Figure 3(b). Similarly to (5), the obtained grid tensor has the hierarchical decomposition given by

r0

1,j, =

a1,j, a0,2j-1,  a0,2j, + Sdiag

=1

2

r1

2,j, =

a2,j, 1,2j-1,  1,2j, + Sdiag

=1

22

a1,+j,r0 a2,+j,r1

M =1 r0 =1

...

rL-1

Ay = L,1,1 =

aL,1,yL-1,1,  L-1,2, + Sdiag

=1

2L

aL+,j,rL-1

rL-2
.
=1

(8) (9)

From this result we observe that inter block connections account for virtually increasing the width of the network's hidden layers from rl to r~l rl + rl-1 for all l = 0, 1, . . . , L - 1, where r-1

6

Under review as a conference paper at ICLR 2018

M . Note that this increased width comes at the expense of increasing the network's parameters. Moreover, in Section 5 we discuss whether increasing the network's width via intra block dense connections leads to an enhancement in its overall expressive power.

4.2 DENSE INTER-BLOCK CONNECTIONS

In this section we study broader connectivity via dense inter-block connections. As discussed in Sec-
tion 4, proper pooling of the preceding features must take place before the concatenating them into
the current layer. Since this type of connections have not been considered in the former DenseNets,
we propose 3 possible ways of realizing such connections (via product, average, or max pooling). For a ConvAC with pooling window size wpool, an inter block connection that connects block l  [L] with block p  [L] is said to be of jump length Ljump  [L - 1] if p = l + Ljump. An example of an inter block connection of jump length Ljump = 1 can be seen in Figure 3(c). To perform this inter block connections, the sizes along the spatial dimensions of preceding features must be reduced by Ljumpwpool, before concatenating them along the feature dimension of layer l. This spatial size reduction may be realized via pooling of the preceding features with window size Ljumpwpool. When using a pooling layer the size along the feature dimension remains unchanged. Moreover, the type of
pooling employed (product, average, or maximum) affects the expressive potential of the resulting
ConvAC. Furthermore, the following proposition addresses the effect that adding dense inter block
connections, via average pooling, has on the network function of a ConvAC.

Proposition 2 Adding inter block connections via average pooling of jump length Ljump  1 to a standard ConvAC with grid-tensor Ay  (RM )N leads to a network function of the form

MN

hy(X) =

Ady1 ,...,dN

fdi (xi) + g(X) ,

d1,...,dN =1

i=1

where g(X) contains polynomial terms on fd (xi) for d  [M ], i  [N ] of degree lower than N .

Remark 1 This result is also valid when the connections are done by addition instead of concatenation, as it is done in ResNet and FractalNet.

Proof See appendix B.2.

From this proposition we conclude that adding inter block connections average pooling does not alter

the grid tensor Ay, instead these connections account for extra polynomial terms of degree strictly

less than N . Note that, for the special case where the input features belong to an exponential kernel

family, such as F = {f(x) = eTx : Rs  R :   } or F = {f(x) = e -x p : Rs  R :   } where · p denotes the p norm with p  N, the number of polynomial terms is equivalent

to the number of exponential basis that the network function can realize. Therefore, the another

valid measure of expressiveness is the number of polynomial terms a ConvAC is able to realize.

Given a certain ConvAC topology, the number of polynomial terms can be computed inductively

by expanding the polynomial products of every layer via generalized binomial expansions. Such

an analysis is left for future contributions. Moreover, if we perform this connections via product

poling, the features to be concatenated correspond to polynomial terms of the same order. This leads

to a generalization of the intra-block connections from 4.1, leading to virtually increased widths

r~l

rl +

Ljump q=1

rl-1-q .

Finally, we leave the analysis of inter-block connections via maximum

pooling for future work and consider only product pooling inter-block connections in the remainder

of this paper.

5 PRACTICAL IMPLICATIONS
For the sake of comparison, let us assume networks with hidden layer widths rl decaying (or increasing) at an exponential rate of   R. Formally, this is rl = rl-1  N, thus rl = ()lr for all l = 0, 1, . . . , L - 1, where r r0. To shorten the notation, we denote as (L, r, , k) to a ConvAC with of exponential width decay   R, length L  N, initial with r  N and growth-rate k  N. A growth-rate of k = 0 refers to a standard ConvAC with no dense connections.

7

Under review as a conference paper at ICLR 2018

Definition 1 Suppose that the weights of a (L, r, , k) ConvAC, with L, k  N and r,   R, are
randomly drawn according to some continuous non-vanishing distribution. Then, this (L, r, , k)
ConvAC is said to have weak dense gain Gw  R if, with probability p > 0, we obtain score functions that cannot be realized by a (L, r , , 0) ConvAC with r < Gwr. When p = 1, this (L, r, , k) ConvAC is said to have a strong dense gain Gs = Gw  R.

Using this definition we present a bound for the weak dense gain Gw in the following theorem.

Theorem 5.1 Given M  N, any a (L, r, , k) ConvAC with L > 1, r  M,   1, k > 0 has a

dense

gain

is

bounden

by

Gw



M r

.

Proof See appendix B.3.

This general bound may serve as guideline for tayloring M and the widths r0, . . . , rL-1 such that we exploit the expressiveness added by dense connections.

Theorem 5.2 For the particular case of theorem 5.1 when k = 1, the weak dense gain is bounded

by Gw  min

1

+

1 

,

M r

.

Proof See appendix B.3.

Using this result, we able able to quantify the expressive gain provided by dense inter block connec-

tions.

If

a

ConvAC

has

a

dense

gain

Gw

=

(1

+

1 

)

that

is

already

close

to

the

general

bound

from

Theorem 5.1 it is less encouraging to include broader dense connections, since it would increase the

number of parameters of the model while there is no room for a significant expressive gain increase.

In this scenario, connections as the ones in ResNet and FractalNet may result more beneficial since

they do not increase the size of the model, while at the same time enhancing its trainability.



Theorem 5.3

For the particular case of theorem 5.1 when k

=

1, if r



1 1+

M , then the bound

of

theorem

5.2

is

achieved

with

equality

and

strong

dense

gain

Gs

=

1

+

1 

.

Proof See appendix B.3.

This last theorem shows that there exist a regime where this bounds can be achieved with strong dense gain. Whether this is true outside this regime is still an open question, since further knowledge about the rank of random tensors is limited. Moreover, these theorems does not consider the additional amount of parameters added by dense connections. We complete our analysis by addressing this issue in the following proposition.

Proposition 3 Let Pdense  N be the additional number of parameters that are added to a

(L, r, , 0) ConvAC when we introduce dense connections of growth-rate k > 0. In the same man-

ner, let Pstand  N be the number of parameters that are added to a (L, r, , 0) ConvAC when we increase its initial width r by a factor G  R. Then the ratio between Pdense and Pstand is greater

than

Pstand  Pdense r

(G - 1)M

(G2 - 1)

k q=1

-1-q

L l=1

(

2 2

)l

+

k q=1

-q

.

Proof See appendix B.4.

The factor G from this proposition directly relates to the dense gain of a ConvAC, thus this ratio may be used to decide whether is interesting to add dense connections to a model (we want this ratio to be as large as possible). Finally Theorems 5.1 and 5.2 directly bound this ratio, which give the practitioner a guideline to decide which connections (if any) should be added to a given model.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report, pp. 05­02, 2005. Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor de-
compositions. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 955­963, New York, New York, USA, 20­22 Jun 2016a. PMLR. URL http://proceedings.mlr.press/v48/cohenb16.html. Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling geometry. CoRR, abs/1605.06743, 2016b. URL http://arxiv.org/abs/1605.06743. Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 698­728, Columbia University, New York, New York, USA, 23­26 Jun 2016a. PMLR. URL http:// proceedings.mlr.press/v49/cohen16.html. Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016b. Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with mixed tensor decompositions. CoRR, abs/1703.06846, 2017. URL http://arxiv.org/ abs/1703.06846. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016. Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entanglement: Fundamental connections with implications to network design. CoRR, abs/1704.01552, 2017. URL http://arxiv.org/abs/1704.01552. Or Sharir and Amnon Shashua. On the expressive power of overlapping operations of deep networks. arXiv preprint arXiv:1703.02065, 2017. Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tractable generative convolutional arithmetic circuits. CoRR, abs/1610.04167, 2016. URL http://arxiv.org/abs/1610. 04167.
9

Under review as a conference paper at ICLR 2018

A PRELIMINARY LEMMAS
Lemma 1 Given Z  N, let A  (RM )P be a random tensor of even order P  2 such that
Z
A = a(z1)  · · ·  a(zP ) ,
z=1
where az(k)  RM are randomly drawn from a non-vanishing continuous distribution for all k  [P ] and z  [Z]. Then, if Z  M P/2 we have that rank(A) = rank([A]) = Z with probability 1. This lemma also holds when for a subset Z  [Z] we have that a(zk) = azez  RM for all z  Z, where az  R are randomly drawn from a non-vanishing continuous distribution.
Proof Using the definition of the matricization operator, we get that the matricization A is

Z
[A] = (a(z1)
z=1

az(3) · · ·
a~(zodd)

a(zP -1))(a(z2)

a(z4) · · ·
a~(zeven)

az(P ))T  RM (P/2)×M (P/2) .

(10)

Note that, from this expression, it is straight forward to see that the rank of [A] is always less or equal than Z.

Let Z~



[M P/2] be the subset Z~

=

{z~

:

z~

=

M

(P /2) -1 M -1

(z

-

1)

+

1

:

z



[Z]} and U



RM(P/2)×M(P/2) be a permuted version of [A] such that the first Z rows of U correspond to the

rows z~  Z~ of [A], and the first Z columns of U correspond to the columns z~  Z~ of [A]. Since

permuting the rows and the columns of a matrix does not alter its rank, we have that U has the same

rank as [A]. Now, let us partition U into blocks as

U=

P W

Q Z

,

where P is of size Z-by-Z, and Q, W, Z have matching dimensions. Note that, if rank(P) = Z then rank(U)  Z, which leads to Z  rank(U) = rank([A])  Z, thus rank([A]) = Z.
Therefore, it is sufficient to show that rank(P) = Z with probability 1 to conclude this proof. To that end, let us define the mapping from x  RMP Z to P = P(x) as

x

a1(1)T , . . . , a(Z1)T , a1(2)T , . . . , aZ(P )T

T
.

Note that this definition of x implies that az(i) = a(zi)(x) for all z  [Z] and i  [P ]. Therefore, since [A] is computed as in (10), we have that [A] = [A](x), thus Q = Q(x) and P = P(x). Now, det P(x) is a polynomial on x, then it either vanishes in a set of measure zero or its the
zero-polynomial (see Caron & Traynor (2005)).

If we set x to be equal to some x0  RMP Z such that a(zi) = ez for all z  [Z] and i  [P ], we

have that a~(zodd) = a~z(even) = ez~  RMP/2 with z~

(

P /2-1 n=0

M n)(z

-

1)

+

1

=

M

(P /2) -1 M -1

(z

-

1) + 1. Therefore, since [A] is calculated as in (10) and a~z(odd)a~z(even)T is now a matrix with 1 on

the entry (z~, z~) and zero elsewhere, we have that [A](x0) is a diagonal matrix with ones on the

diagonal elements z~  Z~ and zero elsewhere. This leads to P(x0) = IZ which has a determinant

det P(x0) = 0. Finally, since there exist x0 such that the polynomial det P(x0) is not zero, we

conclude that det P(x) is not the zero-polynomial, which means that det P(x) = 0 with probability

1, thus proving this lemma.

Lemma 2 Let A  (RM )P and B  (RM )P be random tensors of even order P  2 and Z  N be tensors such that

Z1
A = a(z1)  · · ·  az(P ) ,
z=1

Z2
B = bz(1)  · · ·  bz(P ) ,
z=1

10

Under review as a conference paper at ICLR 2018

where a(zi)  RM and b(zi)  RM are randomly drawn from a non-vanishing continuous distribution. Then, if Z1  M P/2 and Z2  M P/2, we have that rank (A  B) = Z1Z2 with probability 1.

Proof Let C  (RM )2P be a random tensor defined as C = A  B. Therefore, we may express C as
ZZ
C = a(q1)  · · ·  a(qP )  bz(1)  · · ·  bz(P ) .
z=1 q=1
Then, we define rank-1 tensors C(q,z) to be C(q,z) = aq(1)  · · ·  aq(P )  bz(1)  · · ·  bz(P ) to get

Z

C=

C(q,z) .

q,z=1

Since C is now expressed as a sum of Z1Z2 rank-1 tensors, we have that rank(C)  Z1Z2.
Since Z1  M P/2 and Z2  M P/2 we may use Lemma 1, this leads to rank([A]) = Z1 and rank([B]) = Z2 with probability 1. Finally we, use the properties of the Kronecker product to obtain the rank of the matricization C as rank([C]) = rank([A  B]) = rank([A])rank([B]), leading to

rank([C]) = Z1Z2  Z1Z2 = rank([C])  rank(C)  Z1Z2  rank(C) = Z1Z2

with probability 1, thus proving the Lemma.

Lemma 3 Let A  (RM )P and B  (RM )P be tensors of order P > 2 and Z  N be tensors such that

Z1
A = a(z1)  · · ·  a(zP ) ,
z=1

Z2
B = b(z1)  · · ·  bz(P ) ,
z=1

where a(zi)  RM and b(zi)  RM are randomly drawn from a non-vanishing continuous distribution. Then, if Z1 + Z2  M P/2, we have that rank (A + B) = Z1 + Z2 with probability 1.

Proof Let C  RM×···×M be a tensor of order P defined as C = A + B. Therefore, we may express

C as
Z1 Z2 Z1+Z2

C = a(z1)  · · ·  az(P ) + b(z1)  · · ·  b(zP ) =

cz(1)  · · ·  cz(P ) ,

z=1

z=1

z=1

where

cz(i)

a(zi) bz(i)

0 < z  Z1

.

Z1 < z  Z1 + Z2

Since Z1 + Z2  M P/2 we may use Lemma 1, leading to rank (C) = Z1 + Z2 with probability 1, thus proving this Lemma.

Corollary 1 Let A, B, C be tensors of the same size with ranks ZA rank (A), ZB and ZC rank (C). Then, the following statements hold true.
rank (A + B + C) = ZA + ZB + ZC  rank (A + B) = ZA + ZB rank ((A + B)  C) = (ZA + ZB)ZC  rank (A  C) = ZAZC .

rank (B),

11

Under review as a conference paper at ICLR 2018

B DEFERRED PROOFS

B.1 PROOF PROPOSITION 1

Proof We reformulate this (6) to have the same form as (3). To that end we define azd,i, for z =

Z + 1, . . . , Z + M , to be adz,i = 1 if z - Z = d and zero otherwise. This definition of azd,i leads to

M d=1

adz,ifd (xi)

=

fz-Z (xi)

for

z

=

Z

+

1,

.

.

.

,

Z

+

M.

Using

this

relation

we

get

Z NM

Z +M

NM

hy(X) = azy

adz,ifd (xi) +

ayz

azd,ifd (xi)

z=1 i=1 d=1

z=Z+1 i=1 d=1

Z+M N M

= ayz

adz,ifd (xi) ,

z=1 i=1 d=1

which has the same form as (3). Therefore, as done in (4), we obtain the grid tensor for this archi-

tecture as

Z+M

Ay =

azyaz,1  az,2  · · ·  az,N

z=1

Z Z+M

= azyaz,1  az,2  · · ·  az,N +

azyaz,1  az,2  · · ·  az,N

z=1

z =Z +1

Z Z+M

= azyaz,1  az,2  · · ·  az,N +

ayz ez-Z  ez-Z  · · ·  ez-Z

z=1

z =Z +1

Z

= ayz az,1  az,2  · · ·  az,N + Sdiag

z=1

N

azy+Z

M z=1

,

thus proving this proposition.

B.2 PROOF PROPOSITION 2

Proof Given x

[f1 (x1), · · · , fM (x1), f2 (x1), · · · , fM (xN )]T  RMN , the output of

the l-th layer of a (L, r, , 0) ConvAC can be stored into the vectors of mappings l,j(x)

[1l,j(x), . . . , rl,lj(x)]T  Rrl for j  [N/2l] and l  [L]. Moreover, since the entries of these vectors are the result of l - 1 convolution-pooling layers with product pooling of window size 2, all

the mappings 1l,j(x) can be expressed as a sum of polynomial terms on x of degree 2l.

Now, let the coefficient vectors al,j, [al1,j, , . . . , alr,lj, ]T  Rrl for j  [N/2l] and   [rl+1], be the weight vectors for the convolution of the l-th layer. To shorten the notation we use al,j,, l,j =

rl d=1

ald,j, dl,j

as shorthand

for

the convolution between

these vectors.

Then,

the

outputs

the the

layer l of this ConvAC are given by l+1,j  Rrl+1 with l+1,j = al,2j-1, , l,2j-1 al,2j, , l,2j

for j  [N/2l+1],   [rl+1]. If we recursively calculate these out vectors up to the L-th layer we

obtain the score functions hystand(x) L,1(x) = 1L,1(x)  R.

We now consider the effecct of adding dense connections via average pooling from some k  N

preceding layers l - 1, . . . , l - k. To this end, let r~l =

k q=1

rl-q

be

the

total

size

along

the

feature

dimesnion of the vectors to be concatenated. In addition, let l,j(x) [1l,j(x), . . . , rl~,lj(x)]T  Rr~l be the vectors of mappings of the corresponding preceeding features at the layer l for j  [N/rl].

In order to compute the convolutions of this layer, an additional vector of coefficients is required as

bl,j, [b1l,j, , . . . , brl~,lj, ]T  Rr~l . Then, the outputs of the l-th layer of this (L, r, , k) ConvAC are the denoted as the vectors ~l,j(x) [~1l,j(x), . . . , ~rl,lj+1 (x)]T  Rrl+1 for j  [N/2l+1] where

~l+1,j =

al,2j-1, bl,2j-1,

,

l,2j-1 l,2j-1

al,2j, bl,2j,

,

l,2j l,2j

.

12

Under review as a conference paper at ICLR 2018

From this expression it follows

~l+1,j = ( al,2j-1, , l,2j-1 + bl,2j-1, , l,2j )( al,2j, , l,2j + bl,2j, , l,2j ) = l+1,j + l+1,j ,

where

l+1,j = bl,2j-1, , l,2j-1 al,2j, , l,2j + al,2j-1, , l,2j-1 bl,2j, , l,2j + bl,2j-1, , l,2j-1 bl,2j, , l,2j .

Note that the entries of l,j(x) are assumed to come from preceding layers with an appropiate average pooling. Since performing avergae pooling does not increase the degree of the polynomial terms involved (only product pooling does) and the jump length Ljump is at least 1, the entries of l,j(x) have at most polynomial degree 2l-1, which is strictly less than the degree of the entries of l,j(x) (i.e., 2l). Therefore, from the obtained expression of l+1,j we observe that it has polynomials withb degree no greater than 2l + 2l-1, while the entries of l+1,j have a strictly higher degree of 2l + 2l = 2l+1.
Moreover, since al,j, , l,j + l,j can be expressed as

al,j, , l,j + l,j =

al,j, al,j,

,

l,j l,j

(11)

we can make use of the obtained results in an unductive manner up to the L-th layer, thus leading to
hdyense(x) = hystand(x) + g(x) ,
where g(x) contains polynomial terms of x of order strictly less than N , thus proving this theorem. Note that this result also applies to additive and resudial connections, as de ones used in ResNet and FractalNet, since they can be expressed as in (11).

B.3 PROOF OF THEOREMS 5.1 TO 5.3

Proof Given M  N, a (L, r, , 0) ConvAC with L > 1, r  M,   1 has a grid tensor Asytand  (RM )N . For the forthcoming analysis let us assume r0  M . This assumption is done, so that we can write min{r0, M } = r0, merely for notation purposes since we show that this does not affect
the generality of the results. Using this assumption, we upper bound the rank of the grid tensor as

rank 1,j, rank 2,j,

r0

= rank

a1,j, a0,2j-1,  a0,2j,  min{r0, M } = r0

=1

r1 r1
 rank 1,2j-1,  1,2j,  rank 1,2j-1, rank 1,2j,

=1
...

=1

= r1r02

rl-1

rl-1

rank l,j, 

rank l-1,2j-1,  l-1,2j, 

rank l-1,2j-1, rank l-1,2j, .

=1

=1

(12)

It was shown in Cohen & Shashua (2016a) that, when the weights are independently generated from some continuous distribution, we have that rank 1,j, = min{r0, M } with probability 1. Note
that, the bounds obtained for r0 values greater than M is the same as for r0 = M , thus implying that the assumption of r0  M does not affect the generality of the results. Finally, by induction up
to the L-th layer, we obtain a bound for the grid tensor rank as

rL-1

L-1

rank (Asytand) = rank L,1,1 

rank L-1,1, rank L-1,2, =

r .2L-l-1
l

=1

l=0

(13)

Since we assumed networks with hidden layer widths rl decaying (or increasing) at an exponential rate of   R. Formally, this is rl = rl-1  N, thus rl = ()lr for all l = 0, 1, . . . , L - 1, where

13

Under review as a conference paper at ICLR 2018

r r0. Therefore, we may simplify the obtained bound to

L-1

rank (Aystand) 

(()lr)2L-l-1 = ()

rL-1
l=0

l2L-l-1

L-1 l=0

2L-l-1

=

()2L-1-Lr2L-1 .

l=0

We (L,

this r, ,

analysis by k) ConvAC

proving with k

Theorem 5.1. To > 0, while Asytand

that end let is the grid

Aydense be tensor of a

the (L,

grid tensor of a dense r , , 0) ConvAC with

r  R. As discussed in Section 4, this dense version of the former (L, r, , 0) ConvAC is equiva-

lent to virtually increasing the widths of the ConvAC, which translates extra additive terms in the ex-

pressions from 12. Moreover, using corollary 1 we observe that, if the ranks of the tensors l,j, are

additive and multiplicative up to rank(Aydense) > rank(Asytand), so they are up to rank(Asytand). A weak dense gain value Gw  R is achieved when there is a set of functions realized by the (L, r, , k)

ConvAC that cannot be us assume the best case

realized scenario

wbyhe(rLe,rran, k,(A0)ydCenosen)vAreCacuhnelsetshserm=axiGmwurm.

To bound this possible rank,

gain, from

let the

size of Adyense this can be at most rank(Adyense) = M 2L-1. As discussed, would imply that the

ranks of the tensors l,j, are additive up to M 2L-1 for both ConvACs. Therefore, Asytand achieves

its maximum rank able to realize the

given by rank functions of a

(Aystand) = (L, r, , k)

()2L-1-L(r )2L-1. Then, a ConvAC when rank (Aystand)

(L, r , , 0) ConvAC is = rank (Adyense), thus

()2L-1-L(r )2L-1 = M 2L-1. Finally, since r = Gwr, this leads to a maximum value of

Gw

=

ML  2L-1
r



M r

,

which proves Theorem 5.1.

For Theorem 5.2 we use consider the particular case of k = 1, which yields a core tensor given by the hierarchical tensor decomposition from (9). We use the same assumption of r0  M and define the virtually increased widths r~l rl + rl-1  N for l = 1, . . . , L - 1 and r~0 M . This leads to

rank rank

1,j, = rank

r0 +M
a1,j, a0,2j-1,  a0,2j,
=1

 min{r0 + M, M } = r~0

r1 +r0

r~1

2,j, 

rank 1,2j-1,  1,2j, 

rank 1,2j-1, rank 1,2j,

=1
...

=1

= r~1r~02

rl-1 +rl-2

r~l-1

rank l,j, 

rank l-1,2j-1,  l-1,2j, 

rank l-1,2j-1, rank l-1,2j,

=1

=1

and

L-1

rank (Aydens) = rank L,1,1 

r~2L-l-1
l

.

(14)

l=0

Note that for rl = rl-1  N (  R), we get virtually increased widths r~l = (1 + )lr =



1

+

1 

l r, for all l = 0, 1, . . . , L - 1, leading to

L-1
rank (Adyens) 
l=0

((1 + 1/))l r

2L-l-1
= ((1 + 1/))

rL-1
l=0

l2L-l-1

L-1 l=0

2L-l-1

= ((1 + 1/))2L-1-L r2L-1 .

(15)

As in for the proof of Theorem 5.1, the maximum dense gain Gw is obtained when rank(Aydense) reaches the maximum possible rank. In this particular case, this corresponds to rank(Adyense) = min ((1 + 1/))2L-1-L r2L-1, M 2L-1 . Furthermore, for obtaining rank (Asytand) = rank (Adyense) a gain of

Gw  min

1M 1+ ,
 r

14

Under review as a conference paper at ICLR 2018

is required, thus proving Theorem 5.2.

Finally, for proving Theorem 5.3 we show that this bound can be attained with prob-
ability 1. Note that this bound is achieved when the inequalities from (12) hold with equality for all l  [L]. The first inequality of (12) holds when the tensors l-1,2j-1,1  l-1,2j,1 , · · · , l-1,2j-1,rl-1  l-1,2j,rl-1 are additive on the rank. This can
be proven to be true with probability 1 when

rl-1
rank l-1,2j-1,  l-1,2j,  M 2l-1

(16)

=1

by applying Lemma 3. In the same manner, the second inequality of (12) holds when the tensor pairs l-1,2j-1,, l-1,2j, are multiplicative in the tensor rank. We may use Lemma 2 to prove this is the case with probability 1 if we can bound

rank l-1,2j-1,  M 2l-2 .

(17)

In summary, if equations (16) and (17) hold, we may use Lemmas 3 and 2 to prove that (12) reaches

equality with probability 1, thus implying that (13) also reaches equality everywhere outside a set

Lebesgue measure zero. It is straight forward to see that, if (16) holds, so does (17).



For

the

case

of

a

network

with

exponential

width

decay



and

r



1 

M we have that

rl-1
rank l-1,2j-1,  l-1,2j,  ()2l-1-lr2l-1  ()2l r2l  ()2l
=1

1 M

2l
= M 2l-1 ,



(18)



thus

(16)

holds.

Therefore,

within

this

regime

of

r



1 

M , we may ensure that the tensor rank is

additive and multiplicative with probability 1.

Now we apply the same reasoning for a densely connected arithmetic circuit of L layers and width

decay



such

that r



1 1+

M

=

1 (1+1/)

M . In the same manner as for the standard ConvAC

we bound

rl-1
rank l-1,2j-1,  l-1,2j,  ((1 + 1/))2l-1-l r2l-1  ((1 + 1/))2l r2l

=1

 ((1 + 1/))2l

1

 M

2l
= M 2l-1 ,

(1 + 1/)

which enables us to make use of Lemmas 3 and 2 to prove that (15) holds with equality everywhere

except

from

a

set

of

Lebesgue

measure

zero.

Note

that,

for

r



1 1+

M

<

1 

M , we have that

both equations (13) and (15) reach equality with probability 1, thus proving Theorem 5.3.

B.4 PROOF PROPOSITION 3

Proof Let P (L, r, , k)  N be the number of parameters of a (L, r, , k) ConvAC. A standard (L, r, , 0) ConvAC is composed of the weights {a0,j,  RM }j[N],[r0] in the first hidden con-
volutions, {al,j,  RM }j[N/2l],[rl] in the hidden layers, and aL,1,y  RrL-1 in the weights corresponding to the output y in the output layer. Therefore, this ConvAC has a number of weights

LN

P (L, r, , 0) = M N r0 +

2l rlrl-1 + Y rL

l=1

When adding dense connections of growth-rate k, we need additional weights for the convolution

between the preceding layers and the current layer. Therefore, at the l-th layer we have an extra

k q=1

N 2l

rlrl-1-q

weights,

which

leads

to

LN

L kN

P (L, r, , k) = M N r0 + 2l rlrl-1 +

2l rlrl-1-q + Y rL.

l=1 l=1 q=1

15

Under review as a conference paper at ICLR 2018

By definition, we have that Pstd = P (L, Gr, , 0) - P (L, Gr, , 0) and Pdense = P (L, Gr, , k) - P (L, r, , 0), thus yielding

Pstd = (G - 1)(M N r0 + Y rL) + (G2 - 1)

L

N 2l rlrl-1

l=1

= (G - 1)(M N r + LY r) + (G2 - 1)

L

N 2l

2l-1r2

l=1

= (G - 1)(M N + LY )r + (G2 - 1)N -1r2 L ( 2 )l 2
l=1

and

k
Pdense =

L

N 2l

rlrl-1-q

=

N r2

k

-1-q

L ( 2 )l . 2

q=1 l=1

q=1

l=1

Finnaly, we use these expressions to compute the ratio of interest as

Pstd Pdense

=

(G - 1)(M N + LY )r + (G2 - 1)N -1r2

N r2

k q=1

-1-q

L l=1

(

2 2

)l

L l=1

(

2 2

)l

(G - 1)(M N + LY )

= Nr

k q=1

-1-q

L l=1

(

2 2

)l

+

(G2 - 1)

k q=1

-1-q



r

(G - 1)M

(G2 - 1)

k q=1

-1-q

L l=1

(

2 2

)l

+

k q=1

-q

which proves this proposition.

16

