Under review as a conference paper at ICLR 2018
PER-WEIGHT CLASS-BASED LEARNING RATES VIA ANALYTICAL CONTINUATION
Anonymous authors Paper under double-blind review
ABSTRACT
We study the problem of training deep fully connected neural networks. Despite much progress in the design of activation functions, novel normalization techniques, and various skip-connection techniques, such networks remain challenging to train due to vanishing or exploding gradients. Our method is based on employing a different class-dependent learning rate to each network weight. Since the learning rates are hyperparameters and not part of the network, we perform an analytical continuation of the network, and create a generalized network. Following this reparameterization, the set of per-class per-weight learning rates are being manipulated during the training iterations. Our results show that the new algorithm leads to improved classification accuracy for both classical and modern activation functions.
1 INTRODUCTION
The success of deep neural networks in computer vision, voice recognition and synthesis, and NLP is undeniable. What is common in all of these application domains is the use of weight sharing, either spatially, in CNNs, or through time, in recurrent architectures such as RNN or LSTM. It is far less clear whether vanilla fully connected networks demonstrate the same advantage over alternative machine learning methods, such as the gradient boosted trees algorithm (Friedman, 2000), which seems to be the method of choice in many machine learning competitions.
Indeed, learning deep, fully-connected, neural networks is prone to converge to poor local minima. This can be mitigated by advanced optimization methods, by the usage of normalization, and by carefully designing the activation functions. In this work, we propose a new alternative, which is to manipulate the learning rate of each neuron during training in a manner that is class dependent.
The learning rate of each individual weight is based on the label of the learned sample. In other words, the label is used not only for the loss, but also to control the weight updates during training. What makes our work different than an ineffective brute force attempt to learn a predefined subnetwork per class, is the ability to share activations based on patterns that emerge during training. The class-based differentiation occurs due to local modifications of the learning rates, and does not follow a predetermined or a global program.
The influence of each sample on the individual weights is controlled by a set of hyperparameters. The optimization of these hyperparameters is usually not clear. We introduce a method that is based on the idea of network lifting, i.e., we construct a generalized network that contains both the original network's parameters and hyperparameters as its own parameters. The learned hyperparameters are not used for inference. Therefore, the resulting network is as fast in its inference time as a regular feed-forward network.
2 RELATED WORK
Fully Connected Networks are typically shallow, as a result of vanishing or exploding gradients. This challenge is often tackled by normalization schemes, such as Batch Normalization (Ioffe & Szegedy, 2015), Layer Normalization (Ba et al., 2016) and Weight Normalization (Salimans & Kingma, 2016). These methods aim to have the output or input of each layer follow a zero-mean and a standard deviation of 1 distribution.
1

Under review as a conference paper at ICLR 2018

Very recently, a new type of activation function named SELU Klambauer et al. (2017) was constructed with a specific goal of creating networks that are self-normalized (do not require added normalization). This is done by using exponential linear units (Clevert et al., 2015) with pre-calculated coefficients such that the normalization property stays intact during the network's forward propagation.
Our method also works well with all other activation functions we have experimented with, including the time tested tanh non-linearity, and the common ReLU activation. However, acknowledging the experimental success of SELU, we focus on demonstrating that our method is able to improve the results of fully connected networks with SELU activations, and this is evaluated where SELU does best.
Somewhat related is the task of learning the network's structure during training, e.g., (Saxena & Verbeek, 2016; Wen et al., 2016; Liu et al., 2015; Feng & Darrell, 2015; Lebedev & Lempitsky, 2016). This differs from our work, in which the network structure remains fixed, except that during the weight update step of the training phase, different parts of the network change at a difference pace, depending on the class of the training sample.

3 METHOD

In order to add class dependency to our weights, we modify the weight update procedure of the gradient descent algorithm. Specifically, we learn a set of per-class learning rates for each of the network's weights.
The parametric form of the modified weight update, for every single network weight w, is given by

w



w

-

(µwT y)

L(w, x, w

y)

(1)

where  is a global learning rate, µw  Rk is a vector of learned parameters, k being the number of classes, y = y1, y2, . . . , yk  Y = (0, 1)k is a one-hot encoding vector that has the value of 1 for the class of sample x  X , and L(w, x, y) is the network loss for the training sample (x, y) as a function of w.

Given µw for all weights w in the network, the update rule is well-defined, and clearly class dependent. Since {µw}w are external to the network, and do not take part in its optimization, one cannot
update these weights by using a direct gradient method.

In order to derive the update step for these weights, we consider an analytical continuation of our
network, where a "handle" zw is placed. In other words, we define an extended, sample-dependent, network with parametrized weights zw (, x, y), such that at zw(0, x, y) = w for all pairs (x, y)  X × Y. Specifically, we define z to be (omitting for brevity w, x and y)

z

()

=

w

-

(µwT y)

L w

.

(2)

Note that for  1, L (z (), x, y) L (w, x, y), since L (z (), x, y) represents the original network after one step of SGD.
When the class label is not j, i.e., yj = 0, the jth weight of µw, denoted by µjw does not play any role, since in that case z () is independent of it. When yj = 1, the gradient descent update role of µwj is proportional to the square of the derivative of the loss by the specific weight w. Both these cases stem from the chain rule:

L(z(), x, y) L z L µjw = z µjw = z

-yj L w

(3)

A Taylor expansion w.r.t  can be performed on Eq. 3. We assume  1 and consider the elements up to order O 2 . Then, we can write it as

L L w L ==
z w z w

1+

µwT

L y
zw

L = + O () .
w

(4)

2

Under review as a conference paper at ICLR 2018

The gradient of the loss w.r.t to µwj then follows immediately:

L µwj

= -yj

L w

2
+ O 2

 -yj

L w

2

Given the gradient of the loss w.r.t µwj , we obtain the update rule

µwj  µwj + µyj

L w

2
,

(5) (6)

where in our experiments we set the learning rate of µw using cross validation.
The initialization of each of µwj is set to 1, which results in the conventional gradient descent. However, at each optimization step, µwj can only increase, since a positive element is added to it. Noting that unlike the network weights, µ are specific for the specific training landscape, one can reset these after each update. However, since we learn them by a gradient descent update rule, we opt to accumulate the updates for exactly half an epoch, after which we reset all µjw to be 1.

4 EXPERIMENTS

We evaluate our method on the UCI repository on datasets that contain more than 1000 samples. These are exactly the datasets where SELU was shown to allow fully connected neural networks to outperform other learning methods1. Moreover, instead of performing cross validation over the architectures, we select the architecture pointed to by Klambauer et al. (2017), after extensive cross validation experiments. These hyperparameters, which are listed in Tab. 1, were extracted from the authors' github repository2. Following that work, all experiments were also run for 100 epochs, however, we differ in that no early stopping is employed. In some experiments, Klambauer et al. (2017) report using a high learning rate of 0.1 for some of the models. However, in our runs these models diverged. We, therefore, decided to use a learning rate of 0.01 for all experiments.
The choice of using the hyperparameters that provided the best SELU outcomes clearly challenges us, since we compete against SELU specifically where it excels. Therefore, due to the diminishing returns principle, our ability to greatly outperform is reduced. It is also likely that the results of our algorithm would improve, given the chance to select different hyperparameters.
During our experiments we set µ = 105 for SELU and tanh, and µ = 104 for ReLU. If the network did not converge, we reduced it to by a factor of 10. For computational reasons, we limited the number of layers with our per-weight/per-class learning rates to be the bottom 16 layers, whereas any additional layer is a regular fully connected one. In addition, the topmost (representation) layer is not manipulated, since softmax is applied instead of the activation function. Therefore, for example, 8 or 16 layers architectures employ specific learning rates to all layers, except the top one, whereas in our experiments, 32 layer architectures would have such layers only for the bottom 16 layers. The reason that the bottom layers were selected is that the lower layers, which are further away from the loss, would benefit more from the additional class-based information.
The loss function is generally non-convex, however around the minima, we can treat it locally as a convex function. Given a convex function, following the regular gradients would bring us to the minima, and adaptive learning rates are no longer needed. Moreover, the µjw parameters change in a cyclic manner every half an epoch, which might hinder the last steps of convergence. We, therefore, introduce a cutoff of 10, 20 or 30 epochs, after which we stop using our local learning rates, and return to using the normal gradients. We use cross-validation in order to decide on the position of this cutoff for each of the datasets. Except for the clear rule for selecting µ above, this binary choice is the only added hyperparameter parameter introduced by our method that is not set to a single fixed value throughout the experiments.
The right way to use the UCI datasets for comparing algorithms is debated in the literature (Ferna´ndez-Delgado et al., 2014; Klambauer et al., 2017; Wainberg et al., 2016). We follow
1The SELU work had an exceptionally high number of validating experiments, beyond what we could clone in reasonable time using our resources.
2 https://github.com/gklambauer/SelfNormalizingNetworks/tree/master/ Hyperparameters/UCI

3

Under review as a conference paper at ICLR 2018

Table 1: The hyperparameters used during our experiments for each of the UCI datasets. Shown are the number of layers, the number of hidden neurons per layer, the architecture, and the dropout constant. Conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer, according to the geometric progression. Rectangular ones have a fixed number of hidden neurons per layer.

dataset
Abalone Adult Bank Car Cardiotocography 10clases Cardiotocography 3clases Chess krvk Chess krvkp Connect 4 Contrac Hill Valley Image Segmentation Led Display Letter Magic Miniboone Molec biol splice Mushroom Nursery Oocytes merluccius nucleus 4d Oocytes merluccius states 2f Optical Ozone Page blocks Pendigits Plants margin Plants shape Plants texture Ringnorm Semeion Spambase Statlog german credit Statlog image Statlog landsat Statlog shuttle Steel plates Thyroid Titanic Twonorm Wall following Waveform Waveform Noise Wine quality red Wine quality white Yeast

# layers
8 8 3 4 16 4 32 8 8 16 32 3 3 4 3 4 8 16 3 8 8 8 3 16 4 4 8 2 4 16 3 2 3 16 3 16 3 8 8 4 4 2 32 32 3

#hidden
512 512 512 512 512 256 1024 512 1024 512 256 512 512 512 512 1024 256 256 1024 1024 256 256 512 256 256 256 256 512 256 1024 512 256 1024 1024 512 1024 1024 256 256 256 256 1024 1024 1024 256

Architecture
Conic Conic Conic Rectangular Conic Rectangular Rectangular Rectangular Rectangular Rectangular Conic Conic Conic Rectangular Conic Rectangular Rectangular Rectangular Conic Conic Rectangular Rectangular Conic Conic Rectangular Rectangular Rectangular Rectangular Conic Rectangular Rectangular Rectangular Conic Rectangular Rectangular Rectangular Conic Conic Conic Conic Rectangular Conic Rectangular Rectangular Conic

-Dropout
0 0.05 0.05
0 0 0.05 0 0.05 0 0.05 0.05 0.05 0.05 0.05 0 0 0.05 0.05 0 0 0 0.05 0.05 0 0.05 0.05 0 0.05 0 0.05 0 0.05 0 0 0 0 0.05 0 0 0 0.05 0.05 0 0 0.05

4

Under review as a conference paper at ICLR 2018
the splitting of train set and test set suggested by Ferna´ndez-Delgado et al. (2014). For validation we leave out a random subset of 15% of the training set. The same train, validation, and test splits are used across the various methods. This protocol follows the one of Klambauer et al. (2017). However, different validation splits are used, and our SELU results, which employ the public PyTorch implementation release with PyTorch 0.20, do not fully match the numbers reported by Klambauer et al. (2017).
Nothing in the proposed method is tailored toward SELU or any other activation function. Our method is general and could work well with any other activation function. Therefore, in addition to the SELU experiments, we also experiment with other activation functions. These experiments employ exactly the same architectures used per dataset for SELU. The only architectural difference between these experiments and the SELU ones is in the use of a regular Dropout instead of the AlphaDropout prescribed by Klambauer et al. (2017).
The results are reported in Tab. 2 As can be seen, using the SELU activation together with our purposed method benefits 21 experiments out of 45, where only 14 experiments suffer a loss in performance. In 10 datasets the results are identical. Since, in many cases, the results are similar, we applied McNemar's test in order to compare the algorithms. The test is applied for correct/incorrect classification of test samples. In 13 cases, our method outperforms the baseline SELU method with a p-value smaller than 0.05. In 6 of the datasets the baseline method outperforms at that significance level.
Two common activation functions are ReLU and the hyperbolic tangent (tanh). When using the ReLU activation function, 23 datasets benefit from using our method, whereas 11 datasets have suffered a performance reduction. In 11 datasets, the results are identical. When using tanh as an activation function, in 15 datasets an improvement is seen and only 9 datasets suffer a loss in performance. In 21 datasets the results are identical.
An interesting question to ask is whether the different learning rates per class lead to per-class specialization of the neurons. In order to answer this, we mark a neuron as class-specific if its activations across the test samples have a Wilcoxon ranksum p-value lower than 10-7 (the low threshold is taken to mitigate the multiple hypothesis situation). In other words, for each class, we separate the activations of each neuron to those obtained for the class and to those obtained for all other classes, and compute the Wilcoxon ranksum p-value for the difference between the two distributions. In Tab. 3 we report the mean over all classes of the ratio of neurons which are found to be "class-specialized". As can be seen, there is little difference for the baseline networks, in comparison to the situation when applying our method.
5 CONCLUSIONS
We employ the technique of analytical continuation in order to control the learning rate hyperparameter during training. We chose to manipulate this parameter in a per-class per-neuron fashion. Other options include per sample manipulation, per-layer, grouping based on side-information and many combinations of these. In addition, the technique can be used in order to dynamically control other hyperparameters, e.g., introducing per-weight regularization terms or determining class-based trade-off parameters.
Our experimental results show that compared with the SELU technique, exactly where it was shown to excel, our method leads to an improvement in accuracy nearly two out of three times. The situation is similar for other classical and modern activation functions. Being able to win convincingly in a diverse set of experiments, with variations in both datasets and activation functions, indicates that our method is a useful tool to add to one's toolbox. The underlying technique of analytical continuation can also be used to create many other such tools.
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
5

Under review as a conference paper at ICLR 2018

Table 2: All datasets from the UCI Repository with over 1000 samples. For each activation function, we run the baseline network and the network with our per-class learning rate method. The bold fonts mark the best performance per each pair for matching experiments.

dataset
Abalone Adult Bank Car Cardiotocography 10clases Cardiotocography 3clases Chess krvk Chess krvkp Connect 4 Contrac Hill Valley Image Segmentation Led Display Letter Magic Miniboone Molec biol splice Mushroom Nursery Oocytes merluccius nucleus 4d Oocytes merluccius states 2f Optical Ozone Page blocks Pendigits Plants margin Plants shape Plants texture Ringnorm Semeion Spambase Statlog german credit Statlog image Statlog landsat Statlog shuttle Steel plates Thyroid Titanic Twonorm Wall following Waveform Waveform Noise Wine quality red Wine quality white Yeast

SELU
65.18 81.50 89.25 96.18 82.41 88.62 79.51 98.00 84.00 53.40 51.49 75.24 70.00 96.40 86.77 92.77 80.75 100.00 99.68 75.34 91.39 96.86 96.77 96.75 98.64 79.12 62.62 77.35 96.81 89.20 92.57 74.00 95.50 89.35 99.93 68.97 97.88 78.45 96.84 88.75 86.10 84.50 54.44 53.61 60.65

SELU+ class lr
65.33 81.36 89.38 96.18 79.77 88.71 81.69 98.00 85.00 51.36 51.49 75.24 70.60 96.47 87.36 93.03 80.75 100.00 99.65 75.73 91.98 96.91 96.69 96.56 98.67 79.38 61.88 77.35 96.68 88.57 92.57 74.60 95.58 87.96 99.93 68.04 98.09 78.45 96.86 88.67 85.80 84.72 54.82 54.19 60.51

ReLU
64.51 81.81 88.89 95.95 75.82 89.28 76.58 97.93 84.61 50.68 50.83 74.29 69.00 96.20 86.94 92.15 77.99 100.00 99.31 78.86 91.98 97.12 96.77 96.86 99.23 76.00 45.87 77.85 96.46 84.30 93.26 75.00 95.67 88.41 99.84 62.78 97.61 78.45 96.65 86.73 86.20 85.20 56.32 54.84 60.38

ReLU+ class lr
65.33 81.82 89.25 96.06 74.13 89.28 77.02 98.19 84.48 50.95 50.83 74.29 69.20 96.25 86.35 92.32 77.68 100.00 99.31 76.32 91.78 97.17 97.16 96.56 99.23 76.63 46.00 77.85 96.57 78.77 93.26 75.80 95.67 88.59 99.87 65.15 97.56 78.45 96.65 86.95 86.24 85.40 56.20 55.37 60.24

tanh
64.99 81.81 89.16 93.40 80.34 91.72 78.49 98.00 85.80 50.27 55.45 73.33 70.00 96.40 86.76 92.55 81.07 100.00 99.34 81.02 92.56 96.91 96.69 96.86 99.17 78.87 58.00 78.72 91.86 89.70 93.09 74.00 95.50 87.64 99.89 71.75 98.04 78.45 97.22 87.98 86.00 85.00 57.07 53.61 62.80

tanh+ class lr
64.99 81.86 88.63 93.40 80.53 90.40 80.27 98.50 85.85 50.27 50.83 73.33 70.00 96.45 86.21 92.55 81.38 100.00 99.35 80.63 92.56 96.96 96.69 96.49 99.17 79.00 58.00 78.72 91.24 89.70 92.87 74.60 95.50 87.64 99.88 71.75 98.04 78.45 97.24 88.01 86.72 85.12 57.07 53.61 62.80

J. Feng and T. Darrell. Learning the structure of deep convolutional networks. In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 2749­2757, Dec 2015. doi: 10.1109/ ICCV.2015.315.
6

Under review as a conference paper at ICLR 2018

Table 3: Mean % of neurons with class specialization based on Wilcoxon rank- sum test (p < 10-7).

dataset
Abalone Adult Bank Car Cardiotocography 10clases Cardiotocography 3clases Chess krvk Chess krvkp Connect 4 Contrac Hill Valley Image Segmentation Led Display Letter Magic Miniboone Molec biol splice Mushroom Nursery Oocytes merluccius nucleus 4d Oocytes merluccius states 2f Optical Ozone Page blocks Pendigits Plants margin Plants shape Plants texture Ringnorm Semeion Spambase Statlog german credit Statlog image Statlog landsat Statlog shuttle Steel plates Thyroid Titanic Twonorm Wall following Waveform Waveform Noise Wine quality red Wine quality white Yeast

SELU
70.63 84.08 44.14 22.29 43.15 58.33 56.42 82.81 56.73 23.95 0.07 6.34 41.50 70.87 81.12 95.83 66.73 92.80 62.15 9.29 60.14 71.55 11.59 60.65 79.26 0.00 0.00 0.00 64.84 51.01 70.05 5.47 64.80 76.50 50.22 36.94 45.65 73.49 88.28 59.23 83.04 71.66 11.80 14.64 26.56

SELU+ class lr
69.59 85.03 49.61 22.36 43.04 58.27 56.37 82.86 64.11 23.60 0.05 6.34 41.60 70.69 81.38 96.26 66.50 92.75 61.98 9.43 62.66 71.62 11.72 60.27 79.19 0.00 0.00 0.00 65.53 51.02 69.66 5.47 64.65 76.11 50.28 36.97 45.52 73.24 88.96 58.28 82.68 71.83 11.98 15.58 26.77

ReLU
73.67 84.77 42.84 34.61 56.77 61.82 73.64 81.25 75.71 36.92 0.00 6.06 41.09 72.36 81.12 95.43 68.16 95.29 62.13 18.14 69.56 71.28 10.68 76.90 79.04 0.00 0.00 0.00 74.41 55.63 71.35 5.27 64.82 80.95 49.04 49.45 45.72 82.18 91.31 64.43 80.11 71.12 25.45 35.79 24.80

ReLU+ class lr
72.95 84.93 43.75 34.63 55.45 61.69 73.36 81.64 76.01 36.57 0.00 6.06 41.09 72.36 82.55 96.00 68.00 95.04 62.25 15.15 69.61 71.16 10.68 76.26 79.03 0.00 0.00 0.00 75.10 56.09 71.35 5.27 64.69 81.01 48.86 49.46 45.55 82.13 91.50 64.67 79.75 71.12 23.97 33.38 24.69

tanh
73.61 82.19 42.71 27.23 47.10 58.04 61.74 80.64 61.28 18.28 0.37 7.05 42.10 73.04 80.99 96.34 62.13 93.90 64.24 13.76 65.56 71.26 12.76 64.50 80.94 0.00 0.00 0.00 64.75 42.84 71.81 4.49 67.39 74.86 51.57 42.66 45.72 75.44 91.31 67.65 80.92 71.66 13.70 19.38 25.33

tanh+ class lr
73.32 81.28 43.88 27.21 47.89 58.11 61.82 80.52 60.86 18.14 0.44 7.05 42.07 72.99 80.73 96.58 62.08 94.21 64.29 13.76 65.54 71.31 13.41 64.48 80.83 0.00 0.00 0.00 64.45 42.85 72.01 4.49 67.38 75.29 51.49 42.31 45.54 75.63 91.36 67.55 80.96 71.64 13.51 20.77 25.27

7

Under review as a conference paper at ICLR 2018
Manuel Ferna´ndez-Delgado, Eva Cernadas, Sene´n Barro, and Dinani Amorim. Do we need hundreds of classifiers to solve real world classification problems? J. Mach. Learn. Res., 15(1):3133­ 3181, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id= 2627435.2697065.
Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2000.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Gu¨nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in neural information processing systems (NIPS), 2017.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806­814, 2015.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
Shreyas Saxena and Jakob Verbeek. Convolutional Neural Fabrics. In Advances in Neural Information Processing Systems (NIPS), Barcelona, Spain, December 2016. URL https: //hal.inria.fr/hal-01359150.
Michael Wainberg, Babak Alipanahi, and Brendan J. Frey. Are random forests truly the best classifiers? Journal of Machine Learning Research, 17(110):1­5, 2016. URL http://jmlr.org/ papers/v17/15-374.html.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074­2082, 2016.
8

