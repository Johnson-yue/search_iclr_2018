Under review as a conference paper at ICLR 2018

SMOOTH LOSS FUNCTIONS FOR DEEP TOP-K CLASSIFICATION
Anonymous authors Paper under double-blind review

ABSTRACT

Human labeling of data constitutes a long and expensive process. As a consequence,

many classification tasks entail incomplete annotation and incorrect labels, while

being built on a restricted amount of data. In order to handle the ambiguity and the

label noise, the performance of machine learning models is usually assessed with

top-k error metrics rather than top-1. Theoretical results suggest that to minimize

this error, various loss functions, including cross-entropy, are equally optimal

choices of learning objectives in the limit of infinite data. However, the choice of

loss function becomes crucial in the context of limited and noisy data. Besides, our

empirical evidence suggests that the loss function must be smooth and non-sparse

to work well with deep neural networks. Consequently, we introduce a family of

smoothed loss functions that are suited to top-k optimization via deep learning. The

widely used cross-entropy is a special case of our family. Evaluating our smooth

loss functions is computationally challenging: a na¨ive algorithm would require

O(

C k

) operations, where C is the number of classes. Thanks to a connection to

polynomial algebra and a divide-and-conquer approach, we provide an algorithm

with a time complexity of O(kC). Furthermore, we present a novel and error-

bounded approximation to obtain fast and stable algorithms on GPUs with single

floating point precision. We compare the performance of the cross-entropy loss and

our margin-based losses in various regimes of noise and data size. Our investigation

reveals that our loss provides on-par performance with cross-entropy for k = 1,

and is more robust to noise and overfitting for k = 5.

1 INTRODUCTION
In machine learning many classification tasks present inherent label confusion. The confusion can originate from a variety of factors, such as incorrect labeling, incomplete annotation, or some fundamental ambiguity that obfuscate the ground truth label even to a human expert. For example, consider the images from the ImageNet data set (Russakovsky et al., 2015) in Figure 1, which illustrate the aforementioned factors. To mitigate these issues, one may require the model to predict the k most likely labels, where k is typically very small compared to the total number of labels. Then the prediction is considered incorrect if all of its k labels differ from the ground truth, and correct otherwise. This is commonly referred to as the top-k error. Learning such models is a longstanding task in machine learning, and many loss functions for top-k error have been suggested in the literature.
In the context of correctly labeled large data, deep neural networks trained with cross-entropy have shown exemplary capacity to accurately approximate the data distribution. An illustration of this phenomenon is the performance attained by deep convolutional neural networks on the ImageNet challenge. Specifically, state-of-the-art models trained with cross-entropy yield remarkable success on the top-5 error, although cross-entropy is not tailored for top-5 error minimization. This phenomenon can be explained by the fact that cross-entropy is top-k calibrated for any k (Lapin et al., 2016), an asymptotic property which is verified in practice in the large data setting. However, in cases where only a limited amount of data is available, learning large models with cross-entropy can be prone to over-fitting on incomplete or noisy labels.
To alleviate the deficiency of cross-entropy, we present a new family of top-k classification loss functions for deep neural networks. Taking inspiration from multi-class SVMs, our loss creates a margin between the correct top-k predictions and the incorrect ones. Our empirical results show
1

Under review as a conference paper at ICLR 2018

Figure 1: Examples of images with label confusion, from the validation set of ImageNet. The top-left image is incorrectly labeled as "red panda", instead of "giant panda". The bottom-left image is labeled as "strawberry", although the categories "apple", "banana" and "pineapple" would be other valid labels. The center image is labeled as "indigo bunting", which is only valid for the lower bird of the image. The right-most image is labeled as a cocktail shaker, yet could arguably be a part of a music instrument (for example with label "cornet, horn, trumpet, trump"). Such examples motivate the need to predict more than a single label per image.

that in their original form, such loss functions do not perform well in combination with deep neural

networks. We believe that the reason for this is the lack of smoothness and the sparsity of the

derivatives that are used in backpropagation. In order to overcome this difficulty, we smooth the loss

with a temperature parameter. The evaluation of the smooth function and its gradient are challenging,

as smoothing increases the na¨ive time complexity from O(C) to O(

C k

).

With a connection to

polynomial algebra and a divide-and-conquer method, we present an algorithm with O(kC) time

complexity and training time comparable to cross-entropy in practice. We provide insights for

numerical stability of the forward pass. To deal with instabilities of the backward pass, we derive a

novel and inexpensive approximation for which we prove an error bound. Our investigation reveals

that our top-k loss outperforms cross-entropy in the presence of noisy labels or in the absence of large

amounts of data. We further confirm that the difference of performance reduces with large correctly

labeled data, which is consistent with known theoretical results.

2 RELATED WORK
Top-k Loss Functions. The majority of the work on top-k loss functions has been applied to shallow models: Lapin et al. (2016) suggest a convex surrogate on the top-k loss; Fan et al. (2017) select the k largest individual losses in order to be robust to data outliers; Chang et al. (2017) formulate a truncated re-weighted top-k loss as a difference-of-convex objective and optimize it with the Concave-Convex Procedure (Yuille & Rangarajan, 2002); and Yan et al. (2017) propose to use a combination of top-k classifiers and to fuse their outputs.
Closest to our work is the extensive review of top-k loss functions for computer vision by Lapin et al. (2017). The authors conduct a study of a number of top-k loss functions derived from cross-entropy and hinge losses. Interestingly, they prove that for any k, cross-entropy is top-k calibrated, which is a necessary condition for the classifier to be consistent with regard to the theoretically optimal top-k risk. In other words, cross-entropy satisfies an essential property to perform the optimal top-k classification decision for any k in the limit of infinite data. This may explain why cross-entropy performs well on top-5 error on large scale data sets. While thorough, the experiments are conducted on linear models, or pre-trained deep networks that are fine-tuned. For a more complete analysis, we wish to design loss functions that allow for the training of deep neural networks from a random initialization.
Smoothing. Smoothing is a helpful technique in optimization. In work closely related to ours, Lee & Mangasarian (2001) show that smoothing a binary SVM with a temperature parameter improves the theoretical convergence speed of their algorithm. Schwing et al. (2012) use a temperature parameter to smooth latent variables for structured prediction. Lapin et al. (2017) apply Yoreau-Mosida regularization to smooth their top-k surrogate losses.

2

Under review as a conference paper at ICLR 2018

Smoothing has also been applied in the context of deep neural networks. In particular, Zheng et al. (2015) and Clevert et al. (2016) both suggest to modify the non-smooth ReLU activation to improve the training. Gulcehre et al. (2017) suggest to introduce "mollifyers" to smooth the objective function by gradually increasing the difficulty of the optimization problem. Chaudhari et al. (2017) add a local entropy term to the loss to promote solutions with high local entropy. These smoothing techniques are used to speed up the optimization or improve generalization. In this work, we show that smoothing is necessary for the neural network to perform well in combination with our loss function. We hope that this insight can also help the design of losses for tasks other than top-k error minimization.

3 TOP-K SVM

3.1 BACKGROUND: MULTI-CLASS SVM
In order to build an intuition about top-k losses, we start with the simple case k = 1, namely multi-class classification, where the output space is defined as Y = {1, ..., C}. We suppose that a vector of scores per label s  RC, and a ground truth label y  Y are both given. The vector s is the output of the model we wish to learn, for example a linear model or a deep neural network. The
notation 1 will refer to the indicator function over boolean statements (1 if true, 0 if false).

Prediction. The prediction is given by any index with maximal score: P (s)  arg max s.

(1)

Loss. The classification loss incurs a binary penalty by comparing the prediction to the ground truth label. Plugging in equation (1), this can also be written in terms of scores s as follows:

(s, y)

1(y

=

P (s))

=

1(max jY

sj

>

sy ).

(2)

Surrogate. The loss of equation (2) is not amenable to optimization, as it is not even continuous in s. To overcome this difficulty, a typical approach in machine learning is to resort to a surrogate loss that provides a continuous upper bound on . Crammer & Singer (2001) suggest the following upper bound on the loss, known as the multi-class SVM loss:

l(s, y) = max max {sj + 1} - sy, 0 .
j Y \{y}

(3)

In other words, the surrogate loss is zero if the ground truth score is higher than the largest other score by a margin of at least one. Otherwise it incurs a penalty which is linear in the difference.

Rescaling. Note that the value of 1 as a margin is an arbitrary choice, and can be changed to  for any  > 0. This simply entails that we consider the cost  of a misclassification to be  instead of 1.

3.2 TOP-K CLASSIFICATION
We now generalize the above framework to top-k classification, where k  {1, ..., C - 1}. We use the following notation: for p  {1, ..., C}, s[p] refers to the p-th largest element of s, and s\p to the vector (s1, ..., sp-1, sp+1, ..., sC )  RC-1 (that is, the vector s with the p-th element omitted). Y(k) is the set of k-tuples with k distinct elements of Y. Note that we use a bold font for a tuple y¯  Y(k) in order to distinguish it from a single label y¯  Y.

Prediction. Given the scores s  RC, the top-k prediction consists of any set of labels corresponding to the k largest scores:

Pk(s)  y¯  Y(k) :  i  {1, .., k}, sy¯i  s[k] .

(4)

Loss. The loss depends on whether y is part of the top-k prediction, which is equivalent to comparing the k-largest score with the ground truth score:

k(s, y) 1(y / Pk(s)) = 1(s[k] > sy).

(5)

Again, such a binary loss is not suitable for optimization. Thus we introduce a surrogate loss.

3

Under review as a conference paper at ICLR 2018

Surrogate. As pointed out in Lapin et al. (2015), there is a natural extension of the previous multi-class case:

lk(s, y) max s\y + 1 [k] - sy, 0 .

(6)

This loss creates a margin between the ground truth and the k-th largest score, irrespectively of the values of the (k - 1)-largest scores. Note that we retrieve the formulation of Crammer & Singer (2001) for k = 1.

Difficulty of the Optimization. The surrogate loss lk of equation (6) suffers from two disadvantages that make it difficult to optimize: (i) it is not a smooth function of s ­ it is continuous but not differentiable ­ and (ii) its weak derivatives have at most two non-zero elements. Indeed at most two elements of s are retained by the (·)[k] and max operators in equation (6). All others are discarded and thus get zero derivatives. When lk is coupled with a deep neural network, the model typically yields poor performance, even on the training set. Similar difficulties to optimizing a piecewise linear loss have also been reported by Li et al. (2017) in the context of multi-label classification. We illustrate this in the next section.
We postulate that the difficulty of the optimization explains why there has been little work exploring the use of SVM losses in deep learning (even in the case k = 1), and that this work may help remedy it. We propose a smoothing that alleviates both issues (i) and (ii), and we present experimental evidence that the smooth surrogate loss offers better performance in practice.

3.3 SMOOTH SURROGATE LOSS

Reformulation. We introduce the following notation: given a label y¯  Y, Yy¯(k) is the subset of tuples from Y(k) that include y¯ as one of their elements. For y¯  Y(k) and y  Y, we further define
k(y¯, y) 1(y / y¯). Then, by adding and subtracting the k - 1 largest scores of s\y as well as sy,
we obtain:

lk(s, y) = max s\y + 1 [k] - sy, 0 ,

  

  

= max k(y¯, y) + sj - max

sj .

y¯ Y (k)

jy¯  y¯Yy(k) jy¯ 

(7)

We give a more detailed proof of this in Appendix A.1. Since the margin can be rescaled without loss

of generality, we rewrite lk as:

  



1

1 

lk(s, y)

=

max
y¯ Y (k)

k(y¯, y) 

+

k

jy¯

sj 

-

max
y¯ Yy(k)

k

jy¯

sj . 

(8)

Smoothing. In the form of equation (8), the loss function can be smoothed with a temperature parameter  > 0:

11

Lk, (s, y) =  log

exp  k(y¯, y) + k sj

y¯ Y (k)

jy¯

-  log

exp
y¯ Yy(k)

1 k sj .
jy¯
(9)

Note that we have changed the notation to use Lk, to refer to the smooth loss. In what follows, we first outline the properties of Lk, and its relationship with cross-entropy. Then we show the empirical advantage of Lk, over its non-smooth counter-part lk.

Properties of the Smooth Loss. The smooth loss Lk, has a few interesting properties. First, for any  > 0, Lk, is infinitely differentiable and has non-sparse gradients. Second, under mild conditions, when   0+, the non-maximal terms become negligible, therefore the summations collapse to maximizations and Lk,  lk in a pointwise sense (Proposition 2 in Appendix A.2). Third, Lk, is an upper bound on lk if and only if k = 1 (Proposition 3 in Appendix A.3), but Lk, is, up to a scaling factor, an upper bound on k (Proposition 4 in Appendix A.4). This makes it a valid surrogate loss for the minimization of k.

4

Under review as a conference paper at ICLR 2018
Relationship with Cross-Entropy. We have previously seen that the margin can be rescaled by a factor of  > 0. In particular, if we scale  by   0+ and choose a temperature  = 1, it can be seen that L1,1 becomes exactly the cross-entropy loss for classification. In that sense, Lk, is a generalization of the cross-entropy loss to: (i) different values of k  1, (ii) different values of temperature and (iii) higher margins with the scaling  of . For simplicity purposes, we will keep  = 1 in this work.
Experimental Validation. In order to show how smoothing helps the training, we train a DenseNet 40-12 on CIFAR-100 from Huang et al. (2017) with the same hyper-parameters and learning rate schedule. The only difference with Huang et al. (2017) is that we replace the cross-entropy loss with L5, for different values of  . We plot the top-5 training error in Figure 2 (for each curve, the value of  is held constant during training):

(a) Top-5 training error for different values of  on (b) Visualization of derivatives for k = 1 and var-

CIFAR-100. The dashed line y = 0.95 represents the ious  : each bar gives the value of the derivative

base error for random predictions. The successive (y-axis) for a given label (x-axis). These are the

drops in the curves correspond to the decreases of the derivatives obtained with the synthetic data s =

learning rate at epochs 150 and 225.

(0.5, 0.8, 0.3, 0, -0.5) and ground truth y = 1.

Figure 2: Influence of  on the optimization. We confirm that smoothing helps the training of a neural network in Figure 2a, where a large enough value of  greatly helps the performance on the training set. In Figure 2b, we visualize how the case  = 0 yields sparse derivatives, while increasing  gives more information about all labels.

We remark that the network exhibits good accuracy when  is high enough (0.01 or larger). For  too small, the model fails to converge to a good critical point. When  is positive but small, the function is smooth but the gradients are numerically sparse (see Figure 2b), which suggests that the smoothness property is not sufficient and that non-sparsity is a key factor here.

4 COMPUTATIONAL CHALLENGES AND EFFICIENT ALGORITHMS

4.1 CHALLENGE

Experimental evidence suggests that it is beneficial to use Lk, rather than lk to train a neural network.

Moreover, at first glance, Lk, may appear prohibitively expensive to compute. Specifically, there

are summations over Y(k) and Yy(k), which have a cardinality of

C k

and

C k-1

respectively. For

instance for ImageNet, we have k = 5 and C = 1, 000, which amounts to

C k

8.1012 terms to

compute and sum over for each single sample, thereby making the approach practically infeasible.

This is in stark contrast with lk, for which the most expensive operation is to compute the k-th largest score of an array of size C, which can be done in O(C). To overcome this computational challenge,

we will now reframe the problem and reveal its exploitable structure.

5

Under review as a conference paper at ICLR 2018

For a vector e  RC and i  {1, .., C}, we define i(e) as the sum of all products of i distinct elements of e. Explicitly, i(e) can be written as i(e) = 1j1<...<jiC ej1 ...eji . The terms i are known as the elementary symmetric polynomials. We further define 0(e) = 1 for convenience.
We now re-write Lk, using the elementary symmetric polynomials, which appear naturally when separating the terms that contain the ground truth from the ones that do not:

Lk, (s, y) =  log

exp (k(y¯, y)/ ) exp(sj/k )

y¯ Y (k)

jy¯

-  log

exp(sj/k ) ,

y¯Yy(k) jy¯

=  log

exp(sj/k ) + exp (1/ )

exp(sj/k )

y¯Yy(k) jy¯

y¯Y(k)\Yy(k) jy¯

-  log

exp(sj/k ) ,

y¯Yy(k) jy¯

=  log exp(sy/k )k-1(exp(s\y/k )) + exp (1/ ) k(exp(s\y/k ))

(10)

-  log exp(sy/k )k-1(exp(s\y/k )) .
Note that the application of exp to vectors is meant in an element-wise fashion. The last equality of equation (10) reveals that the challenge is to efficiently compute k-1 and k, and their derivatives for the optimization.
While there are existing algorithms to evaluate the elementary symmetric polynomials, they have been designed for computations on CPU with double floating point precision. For the most recent work, see Jiang et al. (2016). To efficiently train deep neural networks with Lk, , we need algorithms that are numerically stable with single floating-point precision and that exploit GPU parallelization. In the next sections, we design algorithms that meet these requirements.

4.2 FORWARD COMPUTATION

We consider the general problem of efficiently computing (k-1, k). Our goal is to compute k(e), where e  Rn and k n. Since this algorithm will be applied to e = exp(s\y/k ) (see equation (10)), we can safely assume ei = 0 for all i  1, n .
The main insight of our approach is the connection of i(e) to the polynomial:

P (X + e1)(X + e2)...(X + en). Indeed, if we expand P to 0 + 1X + ... + nXn, Vieta's formula gives the relationship:
i  0, n , i = n-i(e).

(11) (12)

Therefore, it suffices to compute the coefficients n-k to obtain the value of k(e). To compute the expansion of P , we can use a divide-and-conquer approach with polynomial multiplications when
merging two branches of the recursion.

This method computes all (i)1in instead of the only (i)k-1ik that we require. Since we do

not need i(e) for i > k, we can avoid computations of all coefficients for a degree higher than n - k. For ImageNet, we have k = 5 and n = 1, 000, therefore we have to compute coefficients up

to a degree 995 instead of 1,000, which is a small improvement. To turn k n to our advantage,

n
we notice that i(e) = n(e)n-i(1/e). Moreover, n(e) = ei can be computed in O(n).

Therefore we introduce the polynomial:

i=1

11

1

Q n(e)(X + e1 )(X + e2 )...(X + en ).

(13)

6

Under review as a conference paper at ICLR 2018

Then if we expand Q to 0 + 1X + ... + nXn, we obtain with Vieta's formula again:

i  0, n , i = n(e)n-i(1/e) = i(e).

(14)

Subsequently, in order to compute k(e), we only require the k first coefficients of Q, which is very efficient when k is small in comparison with n. This results in a time complexity of O(kn) (Proposition 5 in Appendix B). Moreover, there are only O(log(n)) levels of recursion, and since every level can have its operations parallelized, the resulting algorithm scales very well with n when
implemented on a GPU. The algorithm is described in Algorithm 1: step 2 initializes the polynomials
for the divide and conquer method. While the polynomial has not been fully expanded, steps 5-6
merge branches by performing the polynomial multiplications (which can be done in parallel). Step
10 adjusts the coefficients using equation (14). We point out that we could obtain an algorithm with a time complexity of O(n log(k)2) if we were using Fast Fourier Transform for polynomial multiplications in steps 5-6. Since we are interested in the case where k is small (typically 5), such
an improvement is negligible.

Algorithm 1 Forward Pass

Require: e  (R+ )n, k  N 1: t  0 2: Pi(t)  (1, 1/ei) for i  1, n 3: p  n

4: while p > 1 do

5: ...
6:
7:

P1(t+1)  P1(t)  P2(t)
P((pt-+11))//2  Pp(-t)1  Pp(t) tt+1

8: p  (p - 1)//2

9: end while
n
10: P (t+1)  P (t) × ei

i=1

11: return P (t+1)

Initialize

n

polynomials

to

X

+

1 ei

(encoded

by

coefficients)

Number of polynomials

Merge branches with polynomial multiplications

Polynomial multiplication up to degree k

Polynomial multiplication up to degree k

Update number of polynomials

Recover i(e) = n-i(1/e)n(e)

Obtaining numerical stability in single float precision requires special attention: the use of exponentials with a temperature parameter potentially arbitrarily small is fundamentally unstable. In Appendix C.1, we describe how operating in the log-space and using the sum-log-exp trick alleviates this issue.

4.3 BACKWARD COMPUTATION

A side effect of this procedure is that a large number of buffers are allocated for automatic differentiation: for each addition in log-space, we apply log and exp operations, each of which needs to store values for the backward pass. This results in a significant amount of time spent on memory allocations, which become the time bottleneck. To avoid this, we exploit the structure of the problem and design a backward algorithm that relies on the results of the forward pass. By avoiding the memory allocations and considerably reducing the number of operations, the backward pass is then sped up by one to two orders of magnitude and becomes negligible in comparison to the forward pass. We describe our efficient backward pass in more details below.

First, we introduce the notation for derivatives:

For j  1, n , i  1, j(i)

i(e) . ej

(15)

We now observe that:

j(i) = i-1(e\j ).

(16)

In other words, equation (16) states that j(i), the derivative of i(e) with respect to ej, is the sum of product of all (i - 1)-tuples that do not include ej. One way of obtaining i-1(e\j) is to compute a forward pass for e\j, which we would need to do for every j  1, n . To avoid such expensive

7

Under review as a conference paper at ICLR 2018

computations, we remark that i(e) can be split into two terms: the ones that contain ej (which can expressed as eji-1(e\j)) and the ones that do not (which are equal to i(e\j) by definition). This gives the following relationship:

i(e\j ) = i(e) - ej i-1(e\j ).

(17)

Simplifying equation (17) using equation (16), we obtain the following induction for the derivatives:

j(i) = i-1(e) - ej j(i-1).

(18)

Since the (i(e))1ik have been computed during the forward pass, we can initialize the induction
with j(1) = 1 and iteratively compute the derivatives j(i) for i  2 with equation (18). This is summarized in Algorithm 2.

Algorithm 2 Backward Pass

Require: e, (j(e))1jk, k  N

(j(e))1jk have been computed in the forward pass

1: i(1) = 1 for i  1, n 2: for j  1, k do

3: j(i) = i-1(e) - ej j(i-1) for i  1, n

4: end for

Algorithm 2 is subject to numerical instabilities (Proposition C.2 in Appendix C.2). In order to avoid these, one solution is to use equation (16) for each unstable element and therefore require additional forward passes. To avoid this inefficiency, we derive a novel and error-bounded approximation, which simply requires us to compute the additional (k + 1)-th coefficient during the forward pass. Note that the time complexity of the forward pass remains unchanged since O((k + 1)C) = O(kC). Once this additional coefficient is obtained from the forward pass, applying our approximation to each unstable element of the backward pass has a constant time complexity (exactly four floating-point operations) instead of a full forward pass of time complexity O(kC). For details, we refer the reader to Appendix C.2.
5 EXPERIMENTS
Theoretical results suggest that Cross-Entropy (CE) is an optimal classifier in the limit of infinite data, by accurately approximating the data distribution. In practice, the presence of label noise makes the data distribution more complex to estimate when only a finite number of samples is available. For these reasons, we explore the behavior of CE and Lk, when varying the amount of label noise and the training data size. For the former, we introduce label noise in the CIFAR-100 data set (Krizhevsky, 2009), in a way that would not perturb the top-5 error of a perfect classifier. For the latter, we vary the training data size on subsets of the ImageNet data set (Russakovsky et al., 2015).
In all the following experiments, the temperature parameter is fixed throughout the training and is set to one, unless explicitly stated otherwise. This choice is discussed in the next section. The algorithms are implemented in PyTorch and will be made publicly available. Experiments on CIFAR-100 are performed on a single Nvidia Titan Xp card, and the experiments on ImageNet make use of two Nvidia Titan Xp cards.
5.1 CIFAR-100 WITH NOISE
Data set. In this experiment, we investigate the impact of label noise on CE and L5,1. The CIFAR100 data set contains 60,000 RGB images, with 50,000 samples for training-validation and 10,000 for testing. There are 20 "coarse" classes, each consisting of 5 "fine" labels. For example, the coarse class "people" is made up of the five fine labels "baby", "boy", "girl", "man" and "woman". In this set of experiments, the images are centered and normalized channel-wise before they are fed to the network. We use the standard data augmentation technique with random horizontal flips and random crops of size 32 × 32 on the images padded with 4 pixels on each side.
We introduce noise in the labels as follows: with probability p, each fine label is replaced by a fine label from the same coarse class. This new label is chosen at random and may be identical to
8

Under review as a conference paper at ICLR 2018

the original label. Note that all instances generated by data augmentation from a single image are assigned the same label. The case p = 0 corresponds to the original data set without noise, and p = 1 to the case where the label is completely random (within the fine labels of the coarse class). With this method, a perfect top-5 classifier would still be able to achieve 100 % accuracy by systematically predicting the five fine labels of the unperturbed coarse label.

Methods. To evaluate our loss functions, we use the architecture DenseNet 40-40 from Huang et al. (2017), and we use the same hyper-parameters and learning rate schedule as in Huang et al. (2017). The temperature parameter is fixed to one. When the level of noise becomes non-negligible, we empirically find that CE suffers from over-fitting and significantly benefits from early stopping ­ which our loss does not need. Therefore we help the baseline and hold out a validation set of 5,000 images, on which we monitor the accuracy across epochs. Then we use the model with the best top-5 validation accuracy and report its performance on the test set. Results are averaged over three runs with different random seeds.

Table 1: Testing performance on CIFAR-100 with different levels of label noise. With noisy labels, L5,1 consistently outperforms CE on both top-5 and top-1 accuracies, with improvements increasingly significant with the level of noise. For reference, a model making random predictions would obtain 1% top-1 accuracy and 5% top-5 accuracy.

Noise Level 0.0 0.2 0.4 0.6 0.8 1.0

Top-1 Accuracy (%) CE L5,1 76.68 69.33 68.20 71.30 61.18 70.02 52.50 67.97 35.53 55.85 14.06 15.28

Top-5 Accuracy (%) CE L5,1 94.34 94.29 87.89 90.59 83.04 87.39 79.59 83.86 74.80 79.32 67.70 72.93

Results. As seen in Table 1, L5,1 outperforms CE on the top-5 testing accuracy when the labels are noisy, with an improvement of over 5% in the case p = 1. When there is no noise in the labels, CE provides better top-1 performance, as expected. It also obtains a better top-5 accuracy, although by a very small margin. Interestingly, L5,1 outperforms CE on the top-1 error when there is noise, although L5,1 is not a surrogate for the top-1 error. For p = 0.8, L5,1 still yields an accuracy of 55.85%, as compared to 35.53% for CE. This suggests that when the provided label is only informative about top-5 predictions (because of noise or ambiguity), it is preferable to use L5,1.
5.2 IMAGENET
Data set. As shown in Figure 1, the ImageNet data set presents different forms of ambiguity and noise in the labels. It also has a large number of training samples, which allows us to explore different regimes up to the large-scale setting. Out of the 1.28 million training samples, we use subsets of various sizes and always hold out a balanced validation set of 50,000 images. We then report results on the 50,000 images of the official validation set, which we use as our test set. Images are resized so that their smaller dimension is 256, and they are centered and normalized channel-wise. At training time, we take random crops of 224 × 224 and randomly flip the images horizontally. At testing time, we use the standard ten-crop procedure (Krizhevsky et al., 2012).
We report results for the following subset sizes of the data: 64k images (5%), 128k images (10%), 320k images (25%), 640k images (50%) and finally the whole data set (1.28M - 50k = 1.23M images for training). Each strict subset has all 1,000 classes and a balanced number of images per class ­ except for the largest subset, which has a slight unbalance alike the full ImageNet data set.
Methods. In all following experiments, we train a ResNet-18 (He et al., 2016) following the protocol of the ImageNet experiment in Huang et al. (2017). Namely, we optimize the model with Stochastic Gradient Descent with a batch-size of 256, for a total of 90 epochs. We use a momentum of 0.9. The learning rate is initially set to 0.1 and is then divided by ten at epochs 30 and 60. For both methods, training on the whole data set takes about a day and a half (it is only 10% longer with L5,1
9

Under review as a conference paper at ICLR 2018

than with CE). As previously, the validation top-5 accuracy is monitored at every epoch, and we use the model with best top-5 validation accuracy to report its test error.

Probabilities for Multiple Crops. Using multiple crops requires a probability distribution over labels for each crop. Then this probability is averaged over the crops to compute the final prediction. The standard method is to use a softmax activation over the scores. We believe that such an approach is only grounded to make top-1 predictions. The probability of a label y¯ being part of the top-5 prediction should be marginalized over all combinations of 5 labels that include y¯ as one of their elements. This can be directly computed with our algorithms to evaluate k and its derivative. We refer the reader to Appendix D for details. All the reported results of top-5 error with multiple crops are computed with this method. This provides a systematic boost of at least 0.2% for all loss functions. In fact, it is more beneficial to the CE baseline, by up to 1% in the small data setting.

Table 2: Top-5 accuracy (%) on ImageNet using training sets of various sizes. Results are reported on the official validation set, which we use as our test set.

% Data Set
100% 50% 25% 10% 5%

Number of Images 1.23M 640k 320k 128k 64k

CE
90.48 87.56 82.43 71.39 59.47

L5,1
89.56 87.14 82.79 72.24 60.81

Results. The results of Table 2 confirm that L5,1 offers better performance on top-5 error than CE when the amount of training data is restricted. As the data set size increases, CE is able to perform
better top-5 predictions. In the following section, we investigate the performance of CE and L5,1 using the full data set in more detail.

6 ERROR ANALYSIS

Influence of the Margin. Recall that CE is equivalent to using L1,1 without any margin. Therefore, there are two major differences between CE and L5,1: (i) they have different margins, and (ii) they are not optimizing for the same value of k in the top-k error. A natural question that arises is whether
it is beneficial to add a margin or not. To answer this, we disentangle the influence of the two
aforementioned factors by varying them independently. In other words, we compare the performance of Lk,1 for k  {1, 5}, with a margin (denoted by Lk,1(1)) and without (denoted by Lk,1(0)) ­ with this notation, CE corresponds to L1,1(0). We use the same experimental protocol as in the previous section, which yields the following results:

CE=L1,1(0) L1,1(1) L5,1(0) L5,1(1)

Top-1 Accuracy (%) 71.69 71.94 63.66 63.05

Top-5 Accuracy (%) 90.48 90.42 89.27 89.56

Table 3: Impact of k and of the margin on the performance of the losses. For k  {1, 5}, we report in bold font the surrogate with best top-k accuracy. In both cases, using a margin improves the top-k accuracy being optimized.

Two useful facts emerge through these experiments. First, for both values of k, we confirm that using a margin increases the top-k error being optimized by the surrogate. In particular, L1,1 gives an improvement over the top-1 error of CE. Second, somewhat surprisingly, the top-1 surrogates provide better top-5 accuracy than top-5 surrogates. This raises the question of whether the use of a top-1 surrogate results in better features, which we investigate next.
10

Under review as a conference paper at ICLR 2018

Quality of features. We use a model pre-trained with CE and learn the classifier layer with L5,1, and vice-versa. This results in the matrix presented below:

Classifier

CE L5,1

Features

CE L5,1

90.48 90.05

89.44 89.56

Table 4: Top-5 accuracy (%) for cross-combinations of classifiers and features.

In Table 4, we observe that using the features learned from CE does not help the performance of L5,1. More surprisingly perhaps, CE performs better than L5,1 itself when using features learned with L5,1.
Surrogate Approximation. A more careful analysis reveals that the temperature parameter is playing a crucial role in this specific situation. Indeed, when we evaluate the objective function with the loss function l5, we obtain a numerical value of 0.83 for the model trained with L5,1 and 0.73 for the one trained with CE. While L5, would be a perfect approximation of l5 in the limit   0, it appears that the temperature of 1 is too high for this instance of problem.
Improved Results based on Error Analysis. Using the above analysis, we wish to adapt the temperature parameter to more accurately optimize for l5. Since the model trained with CE has obtained a better objective value for l5, we start with it and further optimize it with L5, while progressively decreasing the temperature  from 1 to 0 (details in Appendix E.3). This successfully reduces the objective function for l5 from 0.73 to 0.52, and results in an increase of the testing accuracy from 90.48% to 90.62%. In previous work, similar fine-tuning on ImageNet has been attempted for different top-k surrogates (Lapin et al., 2017). According to the authors, these experiments did not not compare favorably to additional training with cross-entropy. In our setting, however, the training procedure indicates that CE would not benefit from such supplementary training. Nonetheless, fine-tuning with our top-k surrogate results in a performance improvement.
7 CONCLUSION
This work has introduced a new family of loss functions that allow for direct minimization of the top-k error. We have experimentally shown that non-sparsity is essential for loss functions to work well with deep neural networks. Thanks to a connection to polynomial algebra and a novel approximation, we have shown how to efficiently evaluate the smooth loss and its gradient.
The experiments have shown that our smooth top-5 loss function is more robust to noise and overfitting than cross-entropy when the amount of training data is limited. In the context of large data, we have shown that using a margin improves results for the error being optimized. Furthermore, we have demonstrated that careful optimization of the non-smooth top-k surrogate leads to improved results. Making this optimization more accurate with an annealing of the temperature is an interesting direction for future research.
Our remark about the necessity for smoothing the loss in deep learning is not specific to top-k error, and we hope that this insight will help the design of other loss functions. In particular, structured prediction problems could benefit from smoothed SVM surrogate losses. How to efficiently compute such smooth functions could open interesting research problems.

11

Under review as a conference paper at ICLR 2018
REFERENCES
Xiaojun Chang, Yao-Liang Yu, and Yi Yang. Robust top-k multiclass SVM for visual category recognition. International Conference on Knowledge Discovery and Data Mining, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-SGD: Biasing gradient descent into wide valleys. International Conference on Learning Representations, 2017.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). International Conference on Learning Representations, 2016.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2001.
Yanbo Fan, Siwei Lyu, Yiming Ying, and Bao-Gang Hu. Learning with average top-k loss. Neural Information Processing Systems, 2017.
Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. Mollifying networks. International Conference on Learning Representations, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Conference on Computer Vision and Pattern Recognition, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. Conference on Computer Vision and Pattern Recognition, 2017.
Hao Jiang, Stef Graillat, Roberto Barrio, and Canqun Yang. Accurate, validated and fast evaluation of elementary symmetric functions and its application. Applied Mathematics and Computation, 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Neural Information Processing Systems, 2012.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Top-k multiclass SVM. Neural Information Processing Systems, 2015.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Loss functions for top-k error: Analysis and insights. Conference on Computer Vision and Pattern Recognition, 2016.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.
Yuh-Jye Lee and Olvi L Mangasarian. SSVM: A smooth support vector machine for classification. Computational optimization and Applications, 2001.
Yuncheng Li, Yale Song, and Jiebo Luo. Improving pairwise ranking for multi-label image classification. Conference on Computer Vision and Pattern Recognition, 2017.
PyTorch. http://pytorch.org/.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015.
Alexander G. Schwing, Tamir Hazan, Marc Pollefeys, and Raquel Urtasun. Efficient structured prediction with latent variables for general graphical models. International Conference on Machine Learning, 2012.
Caixia Yan, Minnan Luo, Huan Liu, Zhihui Li, and Qinghua Zheng. Top-k multi-class svm using multiple features. Information Sciences, 2017.
Alan L. Yuille and Anand Rangarajan. The concave-convex procedure (CCCP). Neural Information Processing Systems, 2002.
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural networks using softplus units. International Joint Conference on Neural Networks, 2015.
12

Under review as a conference paper at ICLR 2018

A SURROGATE LOSSES

In this section, we fix C the number of classes. We let  > 0 and k  {1, ..., C - 1}. The results assume that lk has the same rescaling as in equation (8):

lk(s, y) = max

1 k s\y + 1

[k]

-

1 k sy,

0

.

(19)

A.1 REFORMULATION

Proposition 1. We can equivalently re-write lk as:

  



1

1 

lk(s, y)

=

max
y¯ Y (k)

k(y¯, y) 

+

k

jy¯

sj 

-

max
y¯ Yy(k)

k

jy¯

sj . 

(20)

Proof.

lk(s, y) = max

1 k s\y + 1

[k]

-

1 k sy,

0

,

= max

1 k s\y + 1

[k]

-

1 k sy,

0

  

+

1 k

k-1

s[j]

+

1 k sy

-

1 k

k-1

s[j]

+

1 k sy

,

j=1

j=1







 = max


1 k s\y + 1

[k]

+

1 k

k-1
s[j],
j=1

1 k

k-1
s[j]
j=1

+

1 k sy

-

1 k

k-1
s[j]
j=1

+

1 k sy ,



  



=


max max y¯ Y (k) \Yy(k)

 1


+

1 k

jy¯

 sj , max
 y¯Yy(k)

1 k

jy¯

 sj


-

max
y¯ Yy(k)

1 k

jy¯

 sj ,


  

=

max
y¯ Y (k)

 k(y¯, y)


+

1 k

jy¯

 sj


-

max
y¯ Yy(k)

1 k

jy¯

 sj .


(21)

A.2 POINTWISE CONVERGENCE

Lemma 1. Let n  2 and e  Rn. Assume that the largest element of e is greater than its second

n

largest

element:

e[1]

>

e[2].

Then

lim
 0



log

 >0

exp(ei/ )
i=1

= e[1].

Proof. For simplicity and without loss of generality, we suppose that the elements of e are sorted in

descending order. Then for i  {2, ..n}, we have ei - e1  e2 - e1 < 0 by assumption, and thus



i



{2, ..n},

lim
 0

exp((ei

-

e1)/ )

=

0.

Therefore:

 >0

nn

lim
 0

exp((ei - e1)/ ) =

lim
 0

exp((ei

-

e1)/ )

=

1.

 >0 i=1

i=1  >0

(22)

And thus:

n

lim  log
 0

exp((ei - e1)/ ) = 0.

 >0

i=1

(23)

13

Under review as a conference paper at ICLR 2018

The result follows by noting that:

nn

 log

exp(ei/ ) = e1 +  log

exp((ei - e1)/ ) .

i=1 i=1

(24)

11 Proposition 2. Assume that s[k-1] > s[k] and that s[k] > s[k+1] or k sy > 1 + k s[k]. Then

lim
 0

Lk, (s, y)

=

lk(s, y).

 >0

11

1

Proof.

From s[k]

> s[k+1] or

k sy

> 1+

k

s[k],

one

can

see

that

max
y¯ Y (k)

k(y¯, y) + k jy¯ sj

is a

1

strict maximum. Similarly, from s[k-1] > s[k], we have that max
y¯ Yy(k)

k jy¯ sj

is a strict maximum.

Since Lk, can be written as:

Lk, (s, y) =  log

exp

y¯ Y (k)

1 k(y¯, y) + k sj /
jy¯

-  log

exp

y¯ Yy(k)

1 k sj /
jy¯

,

(25)

the result follows by two applications of Lemma 1.

A.3 BOUND ON NON-SMOOTH FUNCTION

Proposition 3. Lk, is an upper bound on lk if and only if k = 1.

Proof. Suppose k = 1. Let s  RC and y  Y. We introduce y = arg max 1(y¯, y) + sy¯. Then
y¯Y
we have:

l1(s, y) = 1(y, y) + sy - sy, =  log(exp((1(y, y) + sy )/ ) -  log exp(sy/ ),
  log( exp((1(y¯, y) + sy¯)/ ) -  log exp(sy/ ) = L1, (s, y).

(26)

y¯Y

Now suppose k  2. We construct an example (s, y) such that Lk, (s, y) < lk(s, y). For simplicity, we set y = 1. Then let s1 = , si =  for i  {2, ..., k + 1} and si = - for i  {k + 2, ..., C}. Assuming infinite values simplifies the analysis, and by continuity of Lk, and lk, the proof will hold
for real values sufficiently small.

We

further

assume

that

1

+

1 k

(

-

)

>

0.

Then

can

write

lk (s,

y)

as:

lk (s,

y)

=

1

+

1 (
k

-

).

(27)

Exploiting the fact that exp(si/ ) = 0 for i  k + 2, we have:

And:

exp((1 + sj)/k ) = exp
y¯Y(k)\Yy(k) jy¯

1+ 

,

exp
y¯ Yy(k)

1  + (k - 1) k sj / = k exp k .
jy¯

(28) (29)

14

Under review as a conference paper at ICLR 2018

This allows us to write Lk, as:

 + (k - 1) Lk, (s, y) =  log k exp k + exp



exp

1+ 

=  log 1 +

k exp

+(k-1) k

 ,



=  log 1 +

exp

1 

,

k exp

- k

=  log

1 1 + exp

1 (1

+

1 (

-

))

k k

1+ 
.

-  log k exp  + (k - 1) , k (30)

We

introduce

x

=

1

+

1 k

(

-

).

Then

we

have:

1x

Lk, (s, y) =  log

1 + exp k



,

(31)

And:

lk(s, y) = x.

(32)

For

any

value

x

>

0,

we

can

find

(,

)



R2

such

that

x

=

1

+

1 k

(

-

)

and

our

hypotheses

are

all verified. Consequently, we only have to prove that there exists x > 0 such that:

1x (x)  log 1 + exp

- x < 0.

k

(33)

We show that lim (x) < 0, which will conclude the proof.
x

1x (x) =  log 1 + exp

- x,

k

1x =  log 1 + exp

-  log(exp( x )),

k



=  log exp

-x

1 +

  log( 1 ) < 0 when x   since k  2.

k

k

(34)

A.4 BOUND ON PREDICTION LOSS

Lemma 2. Let (p, q)  N2 such that p  q and q  1. Then

q p



1 q

q p+1

.

Proof.

q p q p+1

(q - p - 1)!(p + 1)! = (q - p)!p! ,

(p + 1) = q-p .

(35)

This is a monotonically increasing function of p  0, therefore it is achieving its minimum for p = 0:

q p q p+1

=

(p q

+ -

1) p



1 .
q

(36)

15

Under review as a conference paper at ICLR 2018

Lemma 3. Assume that y / Pk(s). Then we have:





1 exp  sj  

exp  sj  .

k k

k

y¯ Yy(k)

jy¯

y¯ Y (k) \Yy(k)

jy¯

(37)

Proof. The idea of the proof is to show that we can upper bound the score of elements of Yy(k) by elements of Y(k)\Yy(k) by replacing the ground truth score by one of the k largest scores (since y / Pk(s)). There are not enough elements in a single instance of Y(k)\Yy(k) to do that for every element of Yy(k), but Lemma 2 bounds the amount of repetitions of Y(k)\Yy(k) needed.

Proposition 4. Lk, is, up to a scaling factor, an upper bound on the prediction loss k: Lk, (s, y)  (1 -  log(k))k(s, y).

(38)

Proof. Suppose that k(s, y) = 0. Then the inequality if trivial because Lk, (s, y)  0.
We now assume that k(s, y) = 1. Then there exist at least k higher scores than sy. To simplify indexing, we introduce Zy(k) = Y(k)\Yy(k) and Tk the set of k labels corresponding to the k-largest scores. By assumption, y / Tk since y is misclassified. We then write:

exp ((y¯, y)/ ) uj = exp (1/ )

uj +

uj .

y¯ Y (k)

jy¯

y¯Zy(k) jy¯

y¯Yy(k) jy¯

(39)

Thanks to Lemma 3, we have:

1

uj  k

uj .

y¯Zy(k) jy¯

y¯Yy(k) jy¯

(40)

Injecting this back into (49):

exp ((y¯, y)/ )

uj



(1

+

1 k

exp

(1/ ))

uj ,

y¯ Y (k)

jy¯

y¯Yy(k) jy¯

(41)

And back to the original loss:

1

Lk, (s, y)   log

(1 + exp (1/ )) k

uj -  log

uj ,

y¯Yy(k) jy¯

y¯Yy(k) jy¯

=  log(1 + 1 exp (1/ ))   log( 1 exp (1/ )) =  log( 1 ) + 1 = 1 -  log(k). kkk (42)

B TIME COMPLEXITY
Lemma 4. Let P and Q be two polynomials of degree p and q. The time complexity of obtaining the first r coefficients of P Q is O(min{r, p} min{r, q}).
Proof. The multiplication of two polynomials can be written as the convolution of their coefficients, which can be truncated at degree r for each polynomial. Proposition 5. The time complexity of Algorithm 1 is O(kC).
16

Under review as a conference paper at ICLR 2018

Proof. Let N = log2(C), or equivalently C = 2N . With the divide-and-conquer algorithm, the complexity of computing the k first coefficients of P can be written as:

T (k, C) = 2T (k, C ) + min{k, C}2. 2

(43)

Indeed we decompose P = Q1Q2, with each Qi of degree C/2, and for these we compute their k

first

coefficients

in

T

(

C 2

).

Then

given

the

k

first

coefficients

of

Q1

and

Q2,

the

k

first

coefficients

of

P are computed in O(min{k, C}2) by Lemma 4. Then we can write:

C T (k, C) = 2T k,
2 CC 2T k, = 4T k, 24
...

+ min{k, C}2, C2
+ 2 min k, , 2

(44)

2N -1 T

C k, 2N-1

=

2N T (k, 1) +2N-1 min

C k, 2N-1

2
.

2N O(1)=O(C)

N -1
By summing these terms, we obtain T (k, C) = 2N T (k, 1) + 2j min
j=0

C k, 2j

2
.

CC

2n0

Let n0  N such that 2n0+1 < k  2n0 . In loose notation, we have k C = O(1). Then we can

write:

N -1
2j min
j=0

C k, 2j

2 n0
= 2j min

C k, 2j

2
+

N -1

2j min

j=0

j =n0 +1

n0 N -1

= 2j k2 +

2j

C2 ,

2j

j=0

j =n0 +1

= (2n0+1 - 1)k2 + C2(2-n0-1 - 2-N ),

C k, 2j

2
,

(45)

= O(kC).

Thus finally:

N -1
T (k, C) = 2N T (k, 1) + 2j min
j=0
= O(C) + O(kC), = O(kC).

C k, 2j

2
,

(46)

C NUMERICAL STABILITY

C.1 FORWARD PASS
In order to ensure numerical stability of the computation, we maintain all computations in the log space: for a multiplication exp(x1) exp(x2), we actually compute and store x1 + x2; for an addition exp(x1) + exp(x2) we use the "log-sum-exp" trick: we compute m = max{x1, x2}, and store m + log(exp(x1 - m) + exp(x2 - m)), which guarantees stability of the result. These two operations suffice to describe the forward pass.

C.2 BACKWARD PASS Proposition 6. The backward recursion of Algorithm 2 is unstable when ej

1 and ej

max{ep}.
p=j

17

Under review as a conference paper at ICLR 2018

n
Proof. To see that, assume that when we compute ( ep) - ej, we make a numerical error in
p=1
the order of (e.g 10-5 for single-precision floats). With the numerical errors, we obtain approximate ^ as follows:

^j(1) = 1,
n
^j(2) = 1(e) - ej ^j(1) = ep - ej = j(2) + O( ),
p=1
^j(3) = 2(e) - ej ^j(2) = 2(e) - ej (j(2) + O( )) = j(3) + O(ej )),

(47)

...

Since ej

^j(k) = k(e) - ej ^j(k-1) = ... = j(k) + O(ejk-1 )). 1, we quickly obtain unstable results.

Proposition 7. We introduce the following approximation to the gradient:

~j(k)

k (e) ej

-

k+1(e) ej2

.

(48)

Computing ~j(k) with equation (56) is numerically stable, and requires four floating point operations. Proposition 8. If the following hypothesis is verified:

  (0, 1) : k+1(e) < , ej k (e)

(49)

then the relative error of using the approximation of Proposition (7) can be bounded by:

j(k) - ~j(k) j(k)

< 1-

(= + o( ) when  0).

(50)

Intuition To begin with, let us see why hypothesis (57) makes sense. this hypothesis means

that the p = j,

terms that do not contain

ep ej

=

O(), with 

=

o(1)

ej are negligible and ej = O(1).

in comparison to the other ones. Assume that This is the case of the instability shown before.

Then we can write:

k+1(e) = k+1(e) , ejk(e) ejk(e)

=

k+1 k

(e)/ekj +1 (e)/ejk

,

O(k+1) = O(k) ,

(51)

= O(),

= o(1).

Subsequently, hypothesis (57) is satisfied in the instability case presented previously. Furthermore, we

can

show

with

similar

reasoning

that

k (e) ej

=

O(1)

and

k+1 (e) ej2

=

O(),

therefore

the

approximated

gradient

-k (e)
ej

k+1 (e) ej2

is stable to compute.

This means that we simply compute one more

coefficient (k+1(e)) during the forward pass, in order to obtain a fast and stable backward pass.

Proof. We now prove that equation (56) is true when assuming hypothesis (57). First, we note that:

k+1(e\j ) < ej k(e\j ) + k+1(e\j ) ej ej

(because every ep > 0),

= k+1(e) , ej

(52)

< k(e) (by assumption of 57).

18

Under review as a conference paper at ICLR 2018

Then we can write:

j(k) = k-1(e\j ),

1 =
ej

k(e) - k(e\j) ,

1 =
ej

k (e)

-

1 ej

k+1(e) - k+1(e\j )

=

k (e) ej

-

k+1(e) e2j

+

k+1(e\j e2j

)

.

,

Therefore we have:

j(k) -

k (e) ej

-

k+1(e) ej2

And finally:

=

k+1(e\j ej2

)

,

< k(e) (by equation (60)). ej

j(k) -

k (e) ej

-

k+1(e) ej2

j(k)

<

k (e) ej j(k)

,

= k(e) , k(e) - k(e\j) 1
=. 1 - k(e\j ) k (e)

Since

x



1 1-x

is

strictly

increasing

on

[0,

1)

and

:

0  k(e\j ) = k(e\j )ej < k+1(e) < < 1,

k (e)

k(e)ej k(e)ej

we have: and thus:

11 1 - k(e\j) < 1 - ,
k (e)

j(k) -

k (e) ej

-

k+1(e) ej2

j(k)

< 1- .

(53)
(54)
(55)
(56) (57) (58)

D PROBABILITY FOR TOP-K PREDICTION

We consider the probability of label i being part of the final top-k prediction. To that end, we marginalize over all k-tuples that contain i as one of their element. Then the probability of selecting label i for the top-k prediction can be written as:

pi(k) 

exp( sj).

y¯ Yi(k)

jy¯

(59)

19

Under review as a conference paper at ICLR 2018

Proposition 9. The unnormalized probability can be computed as:

pi(k)



d log i(exp(s)) . dsi

(60)

Proof.

pi(k)  exp(si)k-1(exp(s\i)),

=

exp(si)

di(exp(s)) d exp(si)

,

= di(exp(s)) . dsi

(61)

Finally we can rescale the unnormalized probability by k(exp(s)) since it is independent of i. We

obtain:

p^i(k)



1 k(exp(s))

di(exp(s)) dsi

=

d log i(exp(s)) . dsi

(62)

NB. We prefer to use d log i(exp(s)) rather than di(exp(s)) for stability reasons. Once the dsi dsi
unnormalized probabilities are computed, they can be normalized by simply dividing by their sum.

E EXPERIMENTS: SUPPLEMENTARY DETAILS

E.1 CIFAR-100
In the main paper, we report the average of the scores for clarity purposes. Here, we also detail the standard deviation of the scores for completeness.

Table 5: Testing performance on CIFAR-100 with different levels of label noise. We indicate the mean and standard deviation (in parenthesis) for each score.

Noise Level 0.0 0.2 0.4 0.6 0.8 1.0

Top-1 Accuracy (%) CE L5,1 76.68 (0.38) 69.33 (0.27) 68.20 (0.50) 71.30 (0.79) 61.18 (0.97) 70.02 (0.40) 52.50 (0.27) 67.97 (0.51) 35.53 (0.79) 55.85 (0.80) 14.06 (0.13) 15.28 (0.39)

Top-5 Accuracy (%) CE L5,1 94.34 (0.09) 94.29 (0.10) 87.89 (0.08) 90.59 (0.08) 83.04 (0.38) 87.39 (0.23) 79.59 (0.36) 83.86 (0.39) 74.80 (0.15) 79.32 (0.25) 67.70 (0.16) 72.93 (0.25)

E.2 FEATURES TRANSFER
The fully connected layer has its parameters reset to random values, while all other layers keep their values from the pre-trained model (and remain fixed throughout the training). The learning rate is initialized at 0.1, and is divided by ten every 10 epochs. The whole training has a budget of 40 epochs. We report the testing performance of the model with best top-5 validation accuracy.
E.3 ANNEALING OF THE TEMPERATURE
The temperature successively takes the values 1, 0.1, 0.01, 0.001 and 0 (which corresponds to the non-smooth loss). For each value of the temperature, the learning rate is set to 0.01 for 10 epochs, then 0.001 for 5 epochs and finally 0.001 for 5 epochs. We report the testing performance of the model with best top-5 validation accuracy.
20

Under review as a conference paper at ICLR 2018 E.4 EVALUATION OF THE OBJECTIVE FUNCTION We compare the value of the objective function for different models. To avoid any randomness, this is evaluated on the whole training set, where we take a single centered crop for each image.
21

