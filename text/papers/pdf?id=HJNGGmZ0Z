Under review as a conference paper at ICLR 2018
WHAT ARE IMAGE CAPTIONS MADE OF?
Anonymous authors Paper under double-blind review
ABSTRACT
This paper focuses on the `image' side of image captioning. We investigate why end-to-end neural image captioning systems seemingly work so well, and how they can be improved by better utilizing different image representations in an informed manner. In this paper, we study the properties of different types of image representations and how they affect the performance of end-to-end image captioning models. Our empirical analysis provides interesting insights into the representational properties and suggests that the model implicitly learns a `visualsemantic' sub-space. We also provide insights into the generalization capabilities of the model. Our analysis specially focuses on interpreting the discriminative quality of the feature representations, some properties of the induced space and uniqueness of generated image captions. Our results suggest that explicitly modeling the presence of objects and basic object interactions in necessary for tasks that require semantic understanding and better generalization.
1 INTRODUCTION
Image description generation, or image captioning (IC), is the task of automatically generating a textual description for a given image at sentence-level ("a man riding a bike on the street"). The generated text is expected to describe, in a single sentence, what is visually depicted in the image, for example the entities/objects present in the image, their attributes, the actions/activities performed, entity/object interactions (including quantification), the location/scene, etc.
Significant progress has been made with end-to-end approaches to tackling the problem, where large-scale parallel image­description datasets such as Flickr30k (Young et al., 2014) and MSCOCO (Chen et al., 2015b) are used to train a CNN-RNN based neural network IC system (Vinyals et al., 2016; Karpathy & Fei-Fei, 2015; Xu et al., 2015). Such systems have demonstrated impressive performance in the COCO captioning challenge1 according to automatic metrics, seemingly even surpassing human performance in many instances (e.g. CIDEr score > 1.0 vs. human's 0.85) (Chen et al., 2015a). However, in reality, the performance of end-to-end systems is still far from satisfactory according to metrics based on human judgement2. Thus, despite the progress, this task is currently far from being a solved problem.
Our empirical analysis suggests that the apparently strong performance of end-to-end IC systems could potentially be attributed to the fact that they are exploiting the distributional similarity in the image feature space. This observation is in line with reports in previous work (Karpathy, 2016; Vinyals et al., 2016). In addition, some previous work has also alluded to the nature of the dataset being repetitive and simplistic, with an almost constant and predictable linguistic structure (Lebret et al., 2015; Devlin et al., 2015; Vinyals et al., 2016).
In this paper, we focus on understanding what makes neural end-to-end IC systems seemingly perform well, at least according to automatic metrics. This is important to gain insights into how image captioning models can be improved in an informed manner. As end-to-end models are essentially language models conditioned on images, recent work has concentrated on text side of image captioning, by optimizing the language modelling capabilities of the RNN (Rennie et al., 2016; Liu et al., 2017) to improve its performance on automatic metrics. We, however, focus on exploring the image side of image captioning. There have been efforts that attempt to improve image captioning by utilizing or modeling images more effectively, for example by using attention over mid-level image
1http://cocodataset.org/#captions-challenge2015 2http://cocodataset.org/#captions-leaderboard
1

Under review as a conference paper at ICLR 2018

features (Xu et al., 2015) and high-level object proposals (Anderson et al., 2017). Nevertheless, such methods are more complex. Instead, we direct our attention on exploring the simpler yet effective CNN-RNN model, and investigate how images can be better utilized without relying on the more complicated IC architectures.
Many interesting questions can be raised from the image aspect of image captioning: (i) what makes a good representation? (ii) What happens to the representation when training an end-to-end IC system? (iii) What could be the ideal representation for the IC task? Our paper takes some steps in this direction and, as mentioned, addresses this problem in the context of a simple CNN-RNN model that performs comparably to more complex models.
Therefore, in this paper, we study different image representations and how they affect the performance of end-to-end IC models. Our results suggest that explicitly modeling the presence of objects and basic object interactions such as quantification in an image is necessary for tasks that require semantic understanding and better generalization.
Overall, the study suggests that regardless of the representation used, end-to-end IC models learn and exploit image similarity spaces rather than performing actual image understanding.
This study is in line with the recent work that explore understanding of deep learning models and the representational interpretations (Papernot et al., 2016; Szegedy et al., 2013; Sturm, 2014) and works the have tried to delve into the image captioning task (Devlin et al., 2015; Vinyals et al., 2016).
Outline: We first describe the modeling framework, followed by description of the representations. In the following sections we will explore the experiments and the representational probes.

2 MODEL SETTING
We base our implementation on the end-to-end approach by Karpathy & Fei-Fei (2015) and Vinyals et al. (2015). We use the LSTM (Hochreiter & Schmidhuber, 1997) based language model as described in (Zaremba et al., 2014).
To condition the image information, we first perform a linear projection of the image representation followed by a non-linearity as shown below:
Imfeat = (W ·Im)
Here, Im  Rd is the d-dimensional initial image representation, W  Rn×d is the linear transformation matrix,  is the non-linearity. We use the Exponential Linear Units (Clevert et al., 2015) as the non-linear activation in all our experiments. Following Vinyals et al. (2015), we initialize the LSTM based caption generator with the projected image information.
Training and Inference The sentence generator is trained to generate sentences conditioned on the image representation. We train the model by minimizing the cross-entropy, that is, the sentencelevel loss corresponds to the sum of the negative log likelihood of the correct word at each time step:

Pr(S|Imfeat; ) = log(Pr(wi|wi-1..w0; Imfeat)
i

(1)

where Pr (S|Imfeat; ) is the sentence level loss conditioned on the image feature Imfeat and Pr(wt) is the probability of the word at time step t. This is trained with standard teacher forcing as described in Sutskever et al. (2014) where the correct word information is fed to the next state in the
LSTM.

Inference is usually done using approximate techniques like beam search and sampling methods (Karpathy & Fei-Fei, 2015; Vinyals et al., 2015). In this paper, as we are mainly interested in the studying effect of different image representations, we focus on the language output that the models can most confidently produce. Therefore, in order to isolate any other variables from the experiments, we generate captions using a greedy arg max based approach for consistency (unless stated otherwise, we always use greedy decoding).

2

Under review as a conference paper at ICLR 2018
3 IMAGE REPRESENTATIONS
Pre-trained Image Networks: We use VGG19: VGGNet (Simonyan & Zisserman, 2015), ResNet152: ResNet (He et al., 2016) pre-trained on the Imagenet (Russakovsky et al., 2015). Further, we also explore Places365-ResNet152: (Zhou et al., 2014) trained a CNN on the Places2 dataset (Zhou et al., 2017). We investigate if scene-specific categories are useful for IC and MMT, despite these networks not predicting object-specific categories. We also make use of Hybrid1365ResNet152: (Zhou et al., 2014) which is a CNN trained on the concatenation of the ILSVRC and Places2 datasets that predicts both object and scene classes. We further make use of the YOLO (Redmon & Farhadi, 2016), a state-of-the art, object detection system and make use of two pretrained models a) YOLOCoco: a model that is trained on the MSCOCO dataset and b) YOLO9k: this pretrained model can detect over 9000 object categories.
In what follows, we describe different types of image representations explored in this paper.
We condition the LSTM with a 300-dimensional vector with random values sampled uniformly between [0, 1)3. This feature essentially gives us a worst case image feature and provides an artificial lower bound.
Penultimate layer from a pre-trained image network (Penultimate): Most previous attempts to build IC systems use the output of the penultimate layer of a CNN pre-trained on the ILSVRC dataset. Previous work motivates using `off-the-shelf' feature extractors in the framework of transfer learning (Razavian et al., 2014; Donahue et al., 2014). These features have often been used for various Image Captioning tasks (Mao et al., 2014; Karpathy & Fei-Fei, 2015; Xu et al., 2015; Gao et al., 2015; Vinyals et al., 2015; Donahue et al., 2015) and has been shown to produce state-of-theart results on the Image Captioning tasks.
We extract ­ for each image ­ the penultimate layer of these pre-trained networks: (i) for VGG19, we extract the fully-connected (fc7) layer and represent the image as a 4,096-dimensional vector; (ii) for ResNet152, Places365-ResNet152 and Hybrid1365-ResNet152, we use the 2,048-dimensional (pool5) layer of ResNet-152 as features.
Class prediction vector (Softmax): We investigate higher-level image representations, where each element in the vector is an estimated posterior probability of object categories. Again, the categories may not directly correspond to the captions in the dataset. While there are alternative methods that work by fine-tuning the image network on a new set of object classes extracted in ways that are directly relevant to the captions (Fang et al., 2015; Wu et al., 2016), we study the impact of the standard prediction vectors on the task of IC. The idea is to explore the usefulness of the pretrained image networks as is for the task, in line with the usual approaches that use the previously mentioned penultimate layer of the network. More specifically, we explore: (i) pre-trained versions of VGG19 and ResNet152, where we obtain the 1000-dimensional posterior distribution over the ILSVRC object classes; (ii) Places365-ResNet152 and Hybrid-ResNet152, where we obtain a 365 and 1365-dimensional posterior distribution over the scene categories and scene and object categories, respectively.
Object-Class Word Embeddings: Here we experiment with a method that utilizes the averaged word representations of top-k predicted object classes. We first obtain predictions of the images on the object categories (synsest-level information about objects) using the pre-trained image network ResNet152. We then select the objects that have a probability score > 5% and use the 300dimensional pre-trained word2vec (Mikolov et al., 2013) representations4 to obtain the averaged vector over all retained object categories. This is motivated by the central observation that averaged word embeddings can represent semantic-level properties and are useful for classification tasks (Arora et al., 2016).
Bag of objects ground truth annotations: The MSCOCO dataset contains region annotations for instances of 80 pre-defined object categories that appear in each image (car, cat). We note that
3We also tried using 1,000-dimensions, yielding similar results, albeit with a very slightly poorer performance.
4https://code.google.com/archive/p/word2vec/
3

Under review as a conference paper at ICLR 2018
these were annotated independently of the image captions, i.e. people writing the captions were not shown the annotations and had no knowledge of the 80 categories. As such, there is no direct correspondence between the region annotations and image captions.
We use these gold annotations to investigate whether such a sparse and high-level information is helpful for IC. We represent each image as a sparse 80-dimensional `bag of objects' vector, where each element represents the ground truth frequency of occurrence of each object category in the image. We explore two different settings a) a multi-one-hot vector that captures only the presence or absence of the object category, b) a multi-frequency-hot that captures the counts of objects. With this, we are intending to examine the hypothesis of `objects' with the former and partially `object interactions' with the latter.
Pseudo-random Vectors: To probe the capacity of the model to discern image representations, we examine a type of representation where similar images are represented using similar random vectors. We obtain it by using the information from the bag of ground truth annotation. Formally, this is, Im = oObjects f × ro; where, ro  Rd is an object specific random vector and f is a scalar representing counts of the objects. In the binary case, f = 1.
4 EXPERIMENTS AND RESULTS
We now report results using the representations and investigate the effect of the representations.
Datasets Training: We use the most widely used evaluation setup for Image Captioning ­ MSCOCO Chen et al. (2015b). The dataset consists of 82,783 images for training, with five captions per image, thus totaling 413,915 captions in total. The validation set consists of 40,504 images and 202,520 captions. We perform model selection on a 5000-image development set and report the results on a 5000-image test set using standard, publicly available5 splits of the MSCOCO validation dataset as in previous work Karpathy & Fei-Fei (2015); Vinyals et al. (2016). Out of Domain Dataset: We used the Flicr30k dataset with 1000-test image samples 6.
FOIL Dataset: We use the recently proposed FOIL dataset (Shekhar et al., 2017) that contains captions with carefully placed mistakes, where one of the words is replaced with a similar, but wrong word that can be spotted by having access to good image information. We focus on the task of identifying if the caption is foiled given the image information. We use the proposed train and test splits as mentioned.
Evaluation Metrics We evaluated system outputs using the standard evaluation metrics for Image Captioning using the most common metrics - BLEU Papineni et al. (2002), which is computed from 1-gram to 4-gram precision scores, Meteor Lavie (2014) and CIDEr Vedantam et al. (2015). All these metrics are based on some form of n-gram overlap between the system output and the reference captions (i.e. no image information is used). For each system generated caption, we have five references that we compare against. We used the publicly available cocoeval script for evaluation.7
Model Settings and Hyperparameters We use a single hidden layer LSTM with 128dimensional word embeddings and 256-dimensional hidden dimensions. As training vocabulary we retain only words that appear at least twice.
4.1 RESULTS
We report results in Table 1. We observe that the random image embeddings clearly are not providing any useful information and are performing poorly, as expected. The Softmax representations with similar sets of object classes (VGG19, ResNet152, and Hybrid1365-ResNet152) have very similar performance. However, the Places365-ResNet representations perform poorly. We note that the
5http://cs.stanford.edu/people/karpathy/deepimagesent 6the samples were obtained from http://staff.fnwi.uva.nl/d.elliott/wmt16/splits.zip 7https://github.com/pdollar/coco
4

Under review as a conference paper at ICLR 2018

Representation

B-1 B-2 B-3 B-4 M C

Random

0.48 0.24 0.11 0.07 0.11 0.07

Penultimate Softmax

VGG19

0.62 0.43 0.29 0.19 0.20 0.61

ResNet152

0.62 0.43 0.29 0.19 0.20 0.62

Places365-ResNet152 0.60 0.41 0.28 0.19 0.19 0.56

Hybrid1365-ResNet152 0.60 0.41 0.27 0.18 0.19 0.60

VGG19 (fc7)

0.65 0.46 0.32 0.22 0.21 0.69

ResNet152 (Pool5) 0.66 0.48 0.33 0.23 0.22 0.74

Places365-ResNet152 0.61 0.41 0.27 0.19 0.19 0.55

Hybrid1365-ResNet152 0.65 0.46 0.32 0.23 0.22 0.72

Embeddings

Top-k

0.62 0.42 0.28 0.19 0.20 0.63

BOO

Gold-Binary Gold-Counts YOLO-Coco
YOLO-9k

0.65 0.47 0.32 0.22 0.22 0.75 0.66 0.48 0.33 0.23 0.22 0.80 0.65 0.46 0.32 0.22 0.22 0.75 0.64 0.44 0.30 0.20 0.20 0.67

Pseudo-random Pseudo-random

Binary Counts

0.65 0.47 0.33 0.22 0.22 0.74 0.65 0.46 0.31 0.20 0.21 0.78

Table 1: Results on IC test set, where we vary only the image representation and keep other parameters constant. The captions are generated with beam = 1
.

posterior distribution may not directly correspond to captions as there are many words and concepts that are not contained in the set of object classes. Our results differ from those by Wu et al. (2016) and Yao et al. (2016) where the object classes have been fine-tuned to correspond directly to the caption vocabulary. We posit that the degradation in performance is due to spurious probability distributions over object classes for similar looking images.
The performance of the Pool5 image representations shows a similar trend for VGG19, ResNet152, and Hybrid1365-ResNet152. ResNet152 is slightly better in performance. The Places365-ResNet representation performs poorly. We posit that the representations from the image network trained on object classes rather than scene classes are able to capture more fine-grained image details from the images, whereas the image network trained with scene-based classes captures more coarse-grained information.
The performance of the averaged top-k word embeddings is similar to that of the Softmax representation. This is interesting, since the averaged word representational information is mostly noisy: we combine top-k synset-level information into one single vector, however, it still performs competitively.
We observe that the performance of the Bag of Objects sparse 80-dimensional annotation vector is better than all other image representations judging by the CIDEr score. We remark here again, that this is despite the fact that the annotations may not directly correspond to the semantic information in the image or the captions. The sparse representational information is indicative of presence of only a subset of potentially useful objects. We notice two distinct patterns, a marked difference with binary and count based representations. This takes us back to the motivation that image captioning ideally requires the information about objects, interaction of the objects with the attributes. Although, our representation is really sparse on the object interactions, it captures the basic concept of the presence of more than one object of the same kind. Thus, providing some kind of extra information. A similar trend is observed by Yin & Ordonez (2017), although in their models they further try to learn interactions using a specified object RNN.
We also notice that predicted objects using YOLOCoco performs better than the YOLO9k. This is probably expected as the YOLOCoco was trained on the same dataset hence obtaining better object proposals. We also observed a lot of out of training data vocabulary objects being predicted for the test set (around 20%).
The most surprising result is the performance of the pseudo-random vectors. We notice that both the pseudo-random binary and the pseudo-random count based vectors perform almost as good as the gold objects. This suggests that the conditioned RNN is able to remove noise and learn some sort of a common 'visual-semantic' subspace.
5

Under review as a conference paper at ICLR 2018

Method B-1 B-2 B-3 B-4 M C

PCA ICA PPCA

0.65 0.47 0.33 0.23 0.22 0.73 0.65 0.47 0.33 0.23 0.22 0.73 0.66 0.48 0.33 0.23 0.22 0.73

Table 2: Factored Representations

Model B-1 B-2 B-3 B-4 M C Pool5 0.60 0.41 0.26 0.17 0.14 0.29
SC 0.62 0.42 0.28 0.18 0.17 0.35 TDBU 0.60 0.40 0.26 0.17 0.17 0.34
Table 3: Performance on Flickr dataset

4.2 FACTORIZING THE REPRESENTATION
With encouraging results using the pseudo-random vectors, and the results using very sparse, partially related to captions, object categories based representations, we evaluate the relevance of initial representations. Especially, we are interested in evaluating if the performance of the model significantly degrades if we reduce the dimensionality of the initial representation. We experiment with three exploratory factor analysis based methods ­ Principal Component Analysis(PCA) (Halko et al., 2011), Probabilistic Principal Component Analysis(PPCA) (Tipping & Bishop, 1999) and Independent Component Analysis(ICA) (Hyva¨rinen et al., 2004). In all cases, we obtain 80-dimensional representations using the three methods on the resnet's pool5 based 2048 dimensional representations We summarize our experiment in the Table 2. We observe that, the representations obtained by all the factor models seem to retain the necessary representational power to produce appropriate captions equivalent to the original representation.
4.3 INTERPRETING IMAGE REPRESENTATIONS
We first investigate the differences in image representations with respect to their ability to group and distinguish between semantically related images. For this, we selected three categories from MSCOCO ("dog", "person", "toilet") and also pairwise combinations of these ("dog+person", "dog+toilet", "person+toilet"). Up to 25 images were randomly selected for each of these six groups (single category or pair) such that the images are annotated with only the associated categories. Each group is represented by the average image feature of these images. Figure 1 shows the cosine distances between each group, for each of our image representations. The Bag of Objects model clusters these groups the best, as expected (e.g. the average image representation of "dog" correlates with images containing "dog" as a pair like "dog+person" and "dog+toilet"). The Softmax models seem to also to exhibit semantic clusters, although to a lesser extent. This can be observed with "person", where the features are not semantically similar to any other groups. The most likely reason is that there is no "person" category in ILSVRC. Also, Place365 and Hybrid1365 Softmax (Figure 1c) also showed very strong similarity for images containing "toilet", where or not they contain "dog" or "person", possibly because they capture scene features. On the other hand, Pool5 features seem to result in images that are more similar to each other than Softmax overall.
4.4 ANALYZING THE TRANSFORMED REPRESENTATION
Taking forward the suggestion that the conditioned RNN is learning some sort of a common `visualsemantic' space, we explore the difference in representations in the initial representational space and the transformed representational space. The transformation is learned jointly as a subtask of the image captioning. To visualize the two representational spaces, we use Barnes-Hut t-SNE (Maaten & Hinton, 2008) to compute a 2-dimensional embedding over the test split.
In general, we found that images are initially clustered by visual similarity (Pool5) and semantic similarity (Softmax, Bag of Objects). After transformation, we observe that some linguistic information from the captions has resulted in different types of clusters.
Figure 2 highlights some interesting observations of the changes in clustering across three different representations. For Pool5, images seem to be clustered by their visual appearance, for example snow scenes in Figure 2a, regardless of the subjects in the images (people or dogs). After transformation, separate clusters seem to be formed for now scenes involving a single person, groups of people, and dogs. Interestingly, images of dogs in fields and snow scenes are also drawn closer together.
Softmax (Figure 2b) shows many small, isolated clusters before transformation. After transformation, bigger clusters seem to be formed ­ suggesting that the captions have again drawn related images together despite being different in the Softmax space.

6

Under review as a conference paper at ICLR 2018

For bag of objects (Figure 2c), objects seem to be clustered by co-occurrence of object categories, for example toilets/bathrooms and kitchens are clustered since they share sinks. Toilets and kitchens seem to be further apart in the transformed space.
A similar observation was made by Vinyals et al. (2016) in which the authors observe that the endto-end based image captioning models are capable of performing retrieval tasks with comparable performance to the task specific models that are trained with the ranking loss.

(a) Bag of objects

(b) ResNet152 Softmax (c) Places365 Softmax

(d) Hybrid1365 Softmax

(e) Embeddings

(f) ResNet152 Pool5

(g) Places365 Pool5

(h) Hybrid1365 Pool5

(i) Pseudorandom

Figure 1: The cosine distance matrix between six groups (three MSCOCO categories and pairwise combinations of the three categories) from the train dataset. Each group is represented by the average image feature of 25 randomly selected images from the category or combination of categories.

 



(a) Pool5

(b) Softmax

(c) Bag of objects

Figure 2: Visualization of the t-SNE projection of initial representational space (above) vs. the transformed representational space (below). See main text for a more detailed discussion.

7

Under review as a conference paper at ICLR 2018
4.5 DOMAIN DEPENDENCY
We now evaluate the performance of one of our models and two of the state-of-the-art models that are trained on the MSCOCO dataset, but we will evaluate on the test portion of the Flickr30k dataset. We evaluated the captions generated by our model with resnet 152-dimensional representation and two other pretrained models a) Self-Critical(SC) (Rennie et al., 2016) based on self critical sequence training that uses reinforcement learning using metrics. and b) Bottom Up and Top Down(TDBU) (Anderson et al., 2017) based on top-down and bottom up attention using object region proposals. Both the state-of-the-art models are much more complex than the image conditioned RNN based language model. Our results are summarized in Table 3.
We observe that the scores drop by a large margin. A similar observation was made by Vinyals et al. (2016), however, they alluded to the linguistic mismatch. To further analyze, we probed the out of training vocabulary words in the Flickr30k testset and observed that it was around 8.6% which seems to be usual unseen rate. While, the typical sentences on the Flickr30k are structurally different and are typically long, the images seem to have similar distribution of objects as the ones in MSCOCO. We used Flickr30k Entities Plummer et al. (2015) dataset to evaluate if there are samples where no visual words were detected. To perform this experiment, we extracted gold visual words for each of the caption and evaluated if the generated captions contain at least one visual word. However, we observed that around 24.3% of the captions did not correspond to any gold visual word. This further elicits the importance of the information of objects and object interactions with attributes and environment for generation of appropriate descriptions.
4.6 FOILED CAPTIONS
We now switch to a slightly different task, we focus on one of the tasks as suggested in the Shekhar et al. (2017) on detecting if a caption for a given image is foiled. For this task, we utilize simple bag-of-words based linguistic features and four different image representational features - a) gold bag-of-objects (binarized and with counts) and b) predicted bag-of-objects (binarized and with counts) obtained from the pre-trained YOLOcoco model. We use multilayer perceptron with a single 100-dimensional hidden layer and with ReLU activation to be consistent to the models reported in Shekhar et al. (2017). We summarize the results in Table 4. Gold BOO binary and Gold BOO counts correspond to the models that use ground truth based bag-of-object information.  indicates results that are directly obtained from Shekhar et al. (2017). The results indicate that simple models with object based information are able to predict the captions being foiled or not with a very high degree of accuracy. However, models that make use of CNN-based features seem to fail at this particular task. We posit that in this case, explicit information that elucidates the presence and absence of objects seem to capture necessary information for the resolution of the task. We also observe that the performance of the predicted-objects from YOLOcoco perform as good as the ground truth bag-of-object models.
4.7 UNIQUENESS OF CAPTIONS
We now study the ability of representations to produce unique captions for every distinct image. We use the validation portion of the MSCOCO dataset that contains 40,504 images and produce captions with four types of distinct image representations. We report the results in Table 5. We observe that in almost all cases, the produced representations are far from unique. In most cases, there is a significant portion of the captions that are repeated. This is also observed by Devlin et al. (2015) on different test splits, but using retrieval-based and pipeline based and retrieval based methods for IC.
We further analyze the captions and provide details in the appendix.
5 DISCUSSION
Our experiments probed the contribution of various types of image representations and provided some insights into the induced semantic space. We observed that the conditional RNN based language model was capable of making sense of noisy information and correctly clustering these. We further evaluated the generalization of these kinds of models and observed that the models seem to
8

Under review as a conference paper at ICLR 2018

Image Feats
Blind CNN+LSTM
Gold BOO binary Gold BOO counts Predicted BOO binary Predicted BOO counts
Human (majority) Human (unanimity)

Overall
55.62 61.07
95.83 96.18 94.94 95.14
92.89 76.32

Correct
86.20 89.16
96.30 96.23 95.68 95.82
91.24 73.73

Foil
25.04 32.98
95.36 96.14 94.23 94.48
94.52 78.90

Model
Bag of Objects Top-k Class Softmax ResNet Pool5 Human

Unique (%)
29.5 29.0 28.7 28.8 99.4

Table 4: Results of performance on FOIL

Table 5: Unique captions with beam = 1.

be highly constrained to the linguistic structure on which it was trained. This was consistent both in simple image conditioned RNN based language models as well as more complex models. Unfortunately, the current sets of task datasets do not always reflect the paucity of information content in the image representation. We, along with previous work in the field, continue to obtain repeated captions for similar sets of images. Our empirical probes motivate the usefulness of explicit object information and the interactions between objects. As future work, we are interested in exploring more complex models that use attention based models and models based on exploiting latent spaces.
REFERENCES
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and vqa. arXiv preprint arXiv:1707.07998, 2017.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In ICLR, 2016.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015a. URL http://arxiv.org/abs/1504.00325.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015b.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. Language models for image captioning: The quirks and what works. arXiv preprint arXiv:1505.01809, 2015.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pp. 647­655, 2014.
Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, pp. 2625­2634, 2015.
Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Dolla´r, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, et al. From captions to visual concepts and back. In CVPR, pp. 1473­1482, 2015.
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a machine? dataset and methods for multilingual image question. In NIPS, pp. 2296­2304, 2015.
Nathan Halko, Per-Gunnar Martinsson, Yoel Shkolnisky, and Mark Tygert. An algorithm for the principal component analysis of large data sets. SIAM Journal on Scientific computing, 33(5): 2580­2594, 2011.
9

Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Aapo Hyva¨rinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John Wiley & Sons, 2004.
Andrej Karpathy. Connecting Images and Natural Language. PhD thesis, Department of Computer Science, Stanford University, 2016.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, pp. 3128­3137, 2015.
Michael Denkowski Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. ACL 2014, pp. 376, 2014.
Remi Lebret, Pedro Pinheiro, and Ronan Collobert. Phrase-based image captioning. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2085­2094, Lille, France, 07­09 Jul 2015. PMLR.
Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Improved image captioning via policy gradient optimization of spider. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9:2579­2605, 2008.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv preprint arXiv:1412.6632, 2014.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pp. 3111­3119, 2013.
Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel. Cleverhans v1. 0.0: An adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pp. 311­318, 2002.
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In ICCV, pp. 2641­2649, 2015.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In CVPR Workshops, pp. 512­519, 2014.
Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.
Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. arXiv preprint arXiv:1612.00563, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211­252, 2015. doi: 10.1007/s11263-015-0816-y.
Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. Foil it! find one mismatch between image and language caption. arXiv preprint arXiv:1705.01359, 2017.
10

Under review as a conference paper at ICLR 2018
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Bob L Sturm. A simple method to determine if a music information retrieval system is a horse. IEEE Transactions on Multimedia, 16(6):1636­1644, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, pp. 3104­3112, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, 61(3):611­622, 1999.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pp. 4566­4575, 2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR, pp. 3156­3164, 2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. PAMI, 2016.
Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton van den Hengel. What value do explicit high level concepts have in vision to language problems? In CVPR, pp. 203­212, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, volume 14, pp. 77­81, 2015.
Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. Boosting image captioning with attributes. arXiv preprint arXiv:1611.01646, 2016.
Xuwang Yin and Vicente Ordonez. Obj2text: Generating visually descriptive language from object layouts. arXiv preprint arXiv:1707.07102, 2017.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2: 67­78, 2014.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In NIPS, pp. 487­495. 2014.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. PAMI, 2017.
11

Under review as a conference paper at ICLR 2018

Representation Bag of objects VGG19 softmax ResNet softmax Places365 softmax Hybrid1365 softmax VGG19 fc7 ResNet pool5 Places365 pool5 Hybrid1365 pool5 Embeddings

CIDEr () 2.78 (+0.00) 3.14 (+0.36) 3.67 (+0.89) 2.00 (-0.77) 0.01 (-2.77)

Caption a bird is perched on a branch in the sun . a owl is perched on a branch of a tree . a owl is perched on a branch in a tree . a bear is sitting on a branch in the wilderness . a giraffe standing in a field of grass .

0.18 (-2.59) 0.38 (-2.40) 0.34 (-2.43) 3.03 (+0.26) 2.38 (-0.40)

a black and white image of a bird sitting on a window sill . a large black bear standing in a forest . a giraffe standing in the middle of a forest . a bird is perched on a branch in a tree . a bird sitting on a branch in a window .

(a) Bag of objects: bird (1)

Representation Bag of objects VGG19 softmax ResNet softmax Places365 softmax Hybrid1365 softmax VGG19 fc7 ResNet pool5 Places365 pool5 Hybrid1365 pool5 Embeddings

CIDEr () 0.09 (+0.00) 0.00 (-0.09) 0.00 (-0.09) 0.06 (-0.03) 0.00 (-0.09)
0.73 (+0.63) 0.01 (-0.08) 0.00 (-0.09) 0.01 (-0.08) 0.01 (-0.09)

Caption a large airplane flying through a blue sky . a man in a baseball cap and sunglasses is holding a baseball bat . a man is holding a baseball bat in a batting cage . a dog is standing in the grass with a ball in its mouth . a man holding a tennis racquet on a tennis court .
a plane is sitting on a runway with a few people . a train is on the tracks in a city . a giraffe standing in a fenced in enclosure . a man holding a baseball bat standing next to home plate . a baseball player holding a bat on a field .

(b) Bag of objects: airplane (1)

Representation Bag of objects VGG19 softmax ResNet softmax Places365 softmax Hybrid1365 softmax VGG19 fc7 ResNet pool5 Places365 pool5 Hybrid1365 pool5 Embeddings

CIDEr () 0.01 (+0.00) 0.04 (+0.04) 0.00 (-0.00) 0.13 (+0.12) 0.06 (+0.05)
0.24 (+0.24) 0.08 (+0.08) 0.10 (+0.09) 0.05 (+0.04) 0.00 (-0.01)

Caption a man wearing a suit and tie standing in front of a building . a woman in a pink wig and a pink dress . a man in a suit and tie is smiling . a woman with a red polka dotted dress tie . a woman in a red dress is talking on a cell phone .
a woman with a cell phone in her hand . a woman in a red shirt and tie . a woman is holding a cell phone to her ear . a woman in a dress shirt and tie holding a parasol . a man wearing a tie and a shirt and a tie .

(c) Bag of objects: person (1), tie (1)

Figure 3: Example outputs from our system with different representations, the sub-captions indicate the annotation along with the frequency in braces. We also show the CIDEr score and the difference in CIDEr score relative to the Bag of Objects representation.

A ANALYSIS ON GENERATED CAPTIONS
Here, we provide a qualitative analysis of different image representations presented and gain some insights into how they contribute to the the IC task. The Bag of Objects representation led to a strong performance in IC despite being extremely sparse and low-dimensional (80 dimensions). Analyzing the test split, we found that each vector consists of only 2.86 non-zero entries on average (standard deviation 1.8, median 2). Thus, with the minimal information being provided to the generator RNN, we find it surprising that it is able to perform so well.
We compare the output of the remaining models against the Bag of Objects representation by investigating what each representation adds to or subtracts from this simple, yet strong model. We start by selecting images (from the test split) annotated with the exact same Bag of Objects representation ­ which should result in the same caption. For our qualitative analysis, several sets of one to three MSCOCO categories were manually chosen. For each set, images were selected such that there is exactly one instance of each category in the set and zero for others. We then shortlisted images where the captions generated by the Bag of Objects model produced the five highest and five lowest CIDEr scores (ten images per set). We then compare the captions sampled for each of the other representations.
Figure 3 shows some example outputs from this analysis. In Figure 3a, Bag of Objects achieved a high CIDEr score despite only being given "bird" as input, mainly by `guessing' that the bird will
12

Under review as a conference paper at ICLR 2018 be perching/sitting on a branch. The object-based Softmax (VGG and ResNet) models gave an even more accurate description as "owl" is the top-1 prediction of both representations (96% confidence for VGG, 77% for ResNet). Places365 predicted "swamp" and "forest". The Penultimate features on the other hand struggled with representing the images correctly. In Figure 3b, Bag of Objects struggled with lack of information (only "airplane" is given), the Softmax features mainly predicted "chainlink fence", Places365 predicted "kennel" (hence the dog description), and it most likely that Penultimate has captured the fence-like features in the image rather than the plane. In Figure 3c, the Softmax features generally managed to generate a caption describing a woman despite not explicitly containing the `woman' category. This is because other correlated categories were predicted, such as "mask", "wig", "perfume", "hairspray" and in the case of Places365 "beauty salon" and "dressing room".
13

