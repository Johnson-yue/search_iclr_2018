Under review as a conference paper at ICLR 2018

UNDERSTANDING DEEP NEURAL NETWORKS WITH RECTIFIED LINEAR UNITS
Anonymous authors Paper under double-blind review

ABSTRACT

In this paper we investigate the family of functions representable by deep neural

networks (DNN) with rectified linear units (ReLU). We give an algorithm to train

a ReLU DNN with one hidden layer to global optimality with runtime polynomial

in the data size albeit exponential in the input dimension. Further, we improve

on the known lower bounds on size (from exponential to super exponential) for

approximating a ReLU deep net function by a shallower ReLU net. Our gap the-

orems hold for smoothly parametrized families of "hard" functions, contrary to

countable, discrete families known in the literature. An example consequence of

our gap theorems is the following: for every natural number k there exists a func-

tion representable by a ReLU DNN with k2 hidden layers and total size k3, such

that any ReLU DNN with at most total nodes. Finally, we construct

k hidden layers will a family of Rn  R

require

at

least

1 2

piecewise linear

kk+1 - 1 functions

for n  2 (also smoothly parameterized), whose number of affine pieces scales

exponentially with the dimension n at any fixed size and depth. To the best of our

knowledge, such a construction with exponential dependence on n has not been

achieved by previous families of "hard" functions in the neural nets literature.

This construction utilizes the theory of zonotopes from polyhedral theory.

1 INTRODUCTION
Deep neural networks (DNNs) provide an excellent family of hypotheses for machine learning tasks such as classification. Neural networks with a single hidden layer of finite size can represent any continuous function on a compact subset of Rn arbitrary well. The universal approximation result was first given by Cybenko in 1989 for sigmoidal activation function (Cybenko, 1989), and later generalized by Hornik to an arbitrary bounded and nonconstant activation function Hornik (1991). Furthermore, neural networks have finite VC dimension (depending polynomially on the number of edges in the network), and therefore, are PAC (probably approximately correct) learnable using a sample of size that is polynomial in the size of the networks Anthony & Bartlett (1999). However, neural networks based methods were shown to be computationally hard to learn (Anthony & Bartlett, 1999) and had mixed empirical success. Consequently, DNNs fell out of favor by late 90s.
Recently, there has been a resurgence of DNNs with the advent of deep learning LeCun et al. (2015). Deep learning, loosely speaking, refers to a suite of computational techniques that have been developed recently for training DNNs. It started with the work of Hinton et al. (2006), which gave empirical evidence that if DNNs are initialized properly (for instance, using unsupervised pre-training), then we can find good solutions in a reasonable amount of runtime. This work was soon followed by a series of early successes of deep learning at significantly improving the state-of-the-art in speech recognition Hinton et al. (2012). Since then, deep learning has received immense attention from the machine learning community with several state-of-the-art AI systems in speech recognition, image classification, and natural language processing based on deep neural nets Hinton et al. (2012); Dahl et al. (2013); Krizhevsky et al. (2012); Le (2013); Sutskever et al. (2014). While there is less of evidence now that pre-training actually helps, several other solutions have since been put forth to address the issue of efficiently training DNNs. These include heuristics such as dropouts Srivastava et al. (2014), but also considering alternate deep architectures such as convolutional neural networks Sermanet et al. (2014), deep belief networks Hinton et al. (2006), and deep Boltzmann machines Salakhutdinov & Hinton (2009). In addition, deep architectures based on new non-saturating activation functions have been suggested to be more effectively trainable ­ the most successful and

1

Under review as a conference paper at ICLR 2018

widely popular of these is the rectified linear unit (ReLU) activation, i.e., (x) = max{0, x}, which is the focus of study in this paper.
In this paper, we formally study deep neural networks with rectified linear units; we refer to these deep architectures as ReLU DNNs. Our work is inspired by these recent attempts to understand the reason behind the successes of deep learning, both in terms of the structure of the functions represented by DNNs, Telgarsky (2015; 2016); Kane & Williams (2015); Shamir (2016), as well as efforts which have tried to understand the non-convex nature of the training problem of DNNs better Kawaguchi (2016); Haeffele & Vidal (2015). Our investigation of the function space represented by ReLU DNNs also takes inspiration from the classical theory of circuit complexity; we refer the reader to Arora & Barak (2009); Shpilka & Yehudayoff (2010); Jukna (2012); Saptharishi (2014); Allender (1998) for various surveys of this deep and fascinating field. In particular, our gap results are inspired by results like the ones by Hastad Hastad (1986), Razborov Razborov (1987) and Smolensky Smolensky (1987) which show a strict separation of complexity classes. We make progress towards similar statements with deep neural nets with ReLU activation.

1.1 NOTATION AND DEFINITIONS

We extend the ReLU activation function to vectors x  Rn through entry-wise operation: (x) =

(max{0, x1}, max{0, x2}, . . . , max{0, xn}). For any (m, n)  N, of affine and linear transformations from Rm  Rn, respectively.

let

Amn

and

Lnm

denote

the

class

Definition 1. [ReLU DNNs, depth, width, size] For any number of hidden layers k  N, input and output dimensions w0, wk+1  N, a Rw0  Rwk+1 ReLU DNN is given by specifying a sequence of k natural numbers w1, w2, . . . , wk representing widths of the hidden layers, a set of k affine transformations Ti : Rwi-1  Rwi for i = 1, . . . , k and a linear transformation Tk+1 : Rwk  Rwk+1 corresponding to weights of the hidden layers. Such a ReLU DNN is called a (k + 1)-layer ReLU DNN, and is said to have k hidden layers. The function f : Rn1  Rn2 computed or
represented by this ReLU DNN is

f = Tk+1    Tk  · · ·  T2    T1,

(1.1)

where  denotes function composition. The depth of a ReLU DNN is defined as k + 1. The width

of a ReLU DNN is max{w1, . . . , wk}. The size of the ReLU DNN is w1 + w2 + . . . + wk.

Definition 2. We denote the class of Rw0  Rwk+1 ReLU DNNs with k hidden layers of widths {wi}ki=1 by F ,{wi}ik=+01 i.e.

F{wi }ik=+01

:= {Tk+1    Tk  · · ·    T1 :

Ti



Awi
wi-1

i



{1,

...

,

k},

Tk+1



Lwk+1
wk

}

(1.2)

Definition 3. [Piecewise linear functions] We say a function f : Rn  R is continuous piecewise linear (PWL) if there exists a finite set of polyhedra whose union is Rn, and f is affine linear over

each polyhedron (note that the definition automatically implies continuity of the function because the affine regions are closed and cover Rn, and affine functions are continuous). The number of pieces of f is the number of maximal connected subsets of Rn over which f is affine linear (which

is finite).

Many of our important statements will be phrased in terms of the following simplex.

Definition 4. Let M > 0 be any positive real number and p  1 be any natural number. Define the

following set:

Mp := {x  Rp : 0 < x1 < x2 < . . . < xp < M }.

2 EXACT CHARACTERIZATION OF FUNCTION CLASS REPRESENTED BY RELU DNNS

One of the main advantages of DNNs is that they can represent a large family of functions with a relatively small number of parameters. In this section, we give an exact characterization of the functions representable by ReLU DNNs. Moreover, we show how structural properties of ReLU DNNs, specifically their depth and width, affects their expressive power. It is clear from definition that any function from Rn  R represented by a ReLU DNN is a continuous piecewise linear (PWL) function. In what follows, we show that the converse is also true, that is any PWL function

2

Under review as a conference paper at ICLR 2018

is representable by a ReLU DNN. In particular, the following theorem establishes a one-to-one correspondence between the class of ReLU DNNs and PWL functions.
Theorem 2.1. Every Rn  R ReLU DNN represents a piecewise linear function, and every piecewise linear function Rn  R can be represented by a ReLU DNN with at most log2(n + 1) + 1 depth.

Proof Sketch: It is clear that any function represented by a ReLU DNN is a PWL function. To

see the converse, we first note that any PWL function can be represented as a linear combination of

piecewise linear convex functions. More formally, by Theorem 1 in (Wang & Sun, 2005), for every piecewise linear function f : Rn  R, there exists a finite set of affine linear functions 1, . . . , k and subsets S1, . . . , Sp  {1, . . . , k} (not necessarily disjoint) where each Si is of cardinality at
most n + 1, such that
p

f = sj
j=1

max
iSj

i

,

(2.1)

where sj  {-1, +1} for all j = 1, . . . , p. Since a function of the form maxiSj i is a piecewise linear convex function with at most n + 1 pieces (because |Sj|  n + 1), Equation (2.1) says

that any continuous piecewise linear function (not necessarily convex) can be obtained as a linear

combination of piecewise linear convex functions each of which has at most n + 1 affine pieces.

Furthermore, Lemmas D.1, D.2 and D.3 in the Appendix (see supplementary material), show that

composition, addition, and pointwise maximum of PWL functions are also representable by ReLU

DNNs.

In

particular,

in

Lemma

D.3

we

note

that

max{x, y}

=

x+y 2

+

|x-y| 2

is

implementable

by

a

two layer ReLU network and use this construction in an inductive manner to show that maximum of

n + 1 numbers can be computed using a ReLU DNN with depth at most log2(n + 1) .

While Theorem 2.1 gives an upper bound on the depth of the networks needed to represent all continuous piecewise linear functions on Rn, it does not give any tight bounds on the size of the networks that are needed to represent a given piecewise linear function. For n = 1, we give tight
bounds on size as follows:

Theorem 2.2. Given any piecewise linear function R  R with p pieces there exists a 2-layer DNN with at most p nodes that can represent f . Moreover, any 2-layer DNN that represents f has size at least p - 1.

Finally, the main result of this section follows from Theorem 2.1, and well-known facts that the
piecewise linear functions are dense in the family of compactly supported continuous functions and the family of compactly supported continuous functions are dense in Lq(Rn) (Royden & Fitzpatrick, 2010)). Recall that Lq(Rn) is the space of Lebesgue integrable functions f such that |f |qdµ < , where µ is the Lebesgue measure on Rn (see Royden Royden & Fitzpatrick (2010)).
Theorem 2.3. Every function in Lq(Rn), (1  q  ) can be arbitrarily well-approximated in the Lq norm (which for a function f is given by ||f ||q = ( |f |q)1/q) by a ReLU DNN function with at most log2(n + 1) hidden layers. Moreover, for n = 1, any such Lq function can be arbitrarily well-approximated by a 2-layer DNN, with tight bounds on the size of such a DNN in terms of the
approximation.

Proofs of Theorems 2.2 and 2.3 are provided in Appendix A. We would like to remark that a weaker version of Theorem 2.1 was observed in (Goodfellow et al., 2013, Proposition 4.1) (with no bound on the depth), along with a universal approximation theorem (Goodfellow et al., 2013, Theorem 4.3) similar to Theorem 2.3. The authors of Goodfellow et al. (2013) also used a previous result of Wang (Wang, 2004) for obtaining their result. In a subsequent work Boris Hanin (Hanin, 2017) has, among other things, found a width and depth upper bound for ReLU net representation of positive PWL functions on [0, 1]n. The width upperbound is n+3 for general positive PWL functions and n + 1 for convex positive PWL functions. For convex positive PWL functions his depth upper bound is sharp if we disallow dead ReLUs.

3 BENEFITS OF DEPTH
Success of deep learning has been largely attributed to the depth of the networks, i.e. number of successive affine transformations followed by nonlinearities, which is shown to be extracting

3

Under review as a conference paper at ICLR 2018

hierarchical features from the data. In contrast, traditional machine learning frameworks including support vector machines, generalized linear models, and kernel machines can be seen as instances of shallow networks, where a linear transformation acts on a single layer of nonlinear feature extraction. In this section, we explore the importance of depth in ReLU DNNs. In particular, in Section 3.1, we provide a smoothly parametrized family of R  R "hard" functions representable by ReLU DNNs, which requires exponentially larger size for a shallower network. Furthermore, in Section 3.2, we construct a continuum of Rn  R "hard" functions representable by ReLU DNNs, which to the best of our knowledge is the first explicit construction of ReLU DNN functions whose number of affine pieces grows exponentially with input dimension. The proofs of the theorems in this section are provided in Appendix B.

3.1 CIRCUIT LOWER BOUNDS FOR R  R RELU DNNS

In this section, we are only concerned about R  R ReLU DNNs, i.e. both input and output dimensions are equal to one. The following theorem shows the depth-size trade-off in this setting.

Theorem 3.1. For every pair of natural numbers k  1, w  2, there exists a family of hard

functions representable by a R  R (k + 1)-layer ReLU DNN of width w such that if it is also

representable by a (k + 1)-layer ReLU DNN for any k  k, then this (k + 1)-layer ReLU DNN

has size at least

1 2

k

wk k

- 1.

In fact our family of hard functions described above has a very intricate structure as stated below. Theorem 3.2. For every k  1, w  2, every member of the family of hard functions in Theorem 3.1 has wk pieces and this family can be parametrized by

(wM-1 × wM-1 × . . . × Mw-1),

M >0

k times

(3.1)

i.e., for every point in the set above, there exists a distinct function with the stated properties.

The following is an immediate corollary of Theorem 3.1 by choosing the parameters carefully.

Corollary 3.3. For every k  N and > 0, there is a family of functions defined on the real line such that every function f from this family can be represented by a (k1+ ) + 1-layer DNN with size

k2+

and

if

f

is

represented

by

a

k +1-layer

DNN,

then

this

DNN

must

have

size

at

least

1 2

k

·

kk

- 1.

Moreover, this family can be parametrized as, M>0Mk2+ -1.

A particularly illuminative special case is obtained by setting = 1 in Corollary 3.3:

Corollary 3.4. For every natural number k  N, there is a family of functions parameterized by the

set M>0kM3-1 such that any f from this family can be represented by a k2 + 1-layer DNN with

k3

nodes,

and

every

k

+

1-layer

DNN

that

represents

f

needs

at

least

1 2

kk+1

-

1

nodes.

We can also get hardness of approximation versions of Theorem 3.1 and Corollaries 3.3 and 3.4, with the same gaps (upto constant terms), using the following theorem.

Theorem 3.5. For every k  1, w  2, there exists a function fk,w that can be represented by a (k + 1)-layer ReLU DNN with w nodes in each layer, such that for all  > 0 and k  k the

following holds:

1

inf
gGk ,

|fk,w(x) - g(x)|dx > ,
x=0

where Gk , is the family of functions representable by ReLU DNNs with depth at most k + 1, and

size at most k

wk/k (1-4)1/k 21+1/k

.

4

Under review as a conference paper at ICLR 2018

The depth-size trade-off results in Theorems 3.1, and 3.5 extend and improve Telgarsky's theorems from (Telgarsky, 2015; 2016) in the following three ways:

(i) If we use our Theorem 3.5 to the pair of neural nets considered by Telgarsky in Theorem 1.1 in Telgarsky (2016) which are at depths k3 (of size also scaling as k3) and k then for this purpose of approximation in the 1-norm we would get a size lower bound for the shallower net which scales as (2k2 ) which is exponentially (in depth) larger than the lower bound of (2k) that Telgarsky can get for this scenario.

(ii) Telgarsky's family of hard functions is parameterized by a single natural number k. In

contrast, we show that for every pair of natural numbers w and k, and a point from the set

in equation 3.1, there exists a "hard" function which to be represented by a depth k network

would

need

a

size

of

at

least

w

k k

k

.

With

the

extra

flexibility

of

choosing

the

parameter

w,

for the purpose of showing gaps in representation ability of deep nets we can shows size

lower bounds which are super-exponential in depth as explained in Corollaries 3.3 and 3.4.

(iii) A characteristic feature of the "hard" functions in Boolean circuit complexity is that they are usually a countable family of functions and not a "smooth" family of hard functions. In fact, in the last section of Telgarsky (2015), Telgarsky states this as a "weakness" of the state-of-the-art results on "hard" functions for both Boolean circuit complexity and neural nets research. In contrast, we provide a smoothly parameterized family of "hard" functions in Section 3.1 (parametrized by the set in equation 3.1). Such a continuum of hard functions wasn't demonstrated before this work.

We point out that Telgarsky's results in (Telgarsky, 2016) apply to deep neural nets with a host of different activation functions, whereas, our results are specifically for neural nets with rectified linear units. In this sense, Telgarsky's results from (Telgarsky, 2016) are more general than our results in this paper, but with weaker gap guarantees. Eldan-Shamir (Shamir, 2016; Eldan & Shamir, 2016) show that there exists an Rn  R function that can be represented by a 3-layer DNN, that takes exponential in n number of nodes to be approximated to within some constant by a 2-layer DNN. While their results are not immediately comparable with Telgarsky's or our results, it is an interesting open question to extend their results to a constant depth hierarchy statement analogous to the recent result of Rossman et al (Rossman et al., 2015). We also note that in last few years, there has been much effort in the community to show size lowerbounds on ReLU DNNs trying to approximate various classes of functions which are themselves not necessarily exactly representable by ReLU DNNs (Yarotsky, 2016; Liang & Srikant, 2016; Safran & Shamir, 2017).

3.2 A CONTINUUM OF HARD FUNCTIONS FOR Rn  R FOR n  2

One measure of complexity of a family of Rn  R "hard" functions represented by ReLU DNNs is the asymptotics of the number of pieces as a function of dimension n, depth k + 1 and size s of the ReLU DNNs. More precisely, suppose one has a family H of functions such that for every n, k, s  N the family contains at least one Rn  R function representable by a ReLU DNN with depth at most k + 1 and size at most s. The following definition formalizes a notion of complexity for such a H.

Definition pieces (see

5 (compH(n, Definition 3)

k, s)). The of a Rn 

measure compH(n, k, s) R function from H that

is defied as the maximum number of can be represented by a ReLU DNN

with depth at most k + 1 and size at most s.

Similar measures have been studied in previous works Montufar et al. (2014); Pascanu et al.

(2013); Raghu et al. (2016). The best known families H are the ones from Corollary 5 of (Mont-

ufar et al., 2014) and a mild generalization of Theorem 1.1 of (Telgarsky, 2016) to k layers of

ReLU

activations

with

width

w;

these

constructions

achieve

compH(n, k, s)

=

O( )(s/k)kn
nn(k-1)

and

compH(n, k, s)

=

O

(

sk kk

),

respectively.

In

comparison

to

these,

we

give

the

first

construction

that,

for any fixed k and s, achieves an exponential dependence on n. In particular, we construct a class

of functions for which compH(n, k, s) = (sn). Moreover, for fixed n, k, s, our functions are

smoothly parameterized. In what follows, we first give a few structural definitions and lemmas,

which will be used in constructing our class of hard functions. The main result of this section is

given by Theorem 3.9.

5

Under review as a conference paper at ICLR 2018

(a)

H

1 2

,

1 2

N

1

(b)

H

1 2

,

1 2

 Z(b1,b2,b3,b4)

(c)

H

1 2

,

1 2

,

1 2

 Z(b1,b2,b3,b4)

Figure

1:

We

fix

the

a

vectors

for

a

two

hidden

layer

R



R

hard

function

as

a1

=

a2

=

(

1 2

)



11

Left: A specific hard function induced by 1 norm: ZONOTOPE22,2,2[a1, a2, b1, b2] where

b1 = (0, 1) and b2 = (1, 0). Note that in this case the function can be seen as a composi-

tion of Ha1,a2 with 1-norm N 1 (x) := x 1 = Z((0,1),(1,0)). Middle: A typical hard function

ZONOTOPE22,2,4[a1, a2, c1, c2, c3, c4]

with

generators

c1

=

(

1 4

,

1 2

),

c2

=

(-

1 2

,

0),

c3

=

(0,

-

1 4

)

and

c4

=

(-

1 4

,

-

1 4

).

Note

how

increasing

the

number

of

zonotope

generators

makes

the

function

more complex. Right: A harder function from ZONOTOPE23,2,4 family with the same set of gen-

erators c1, c2, c3, c4 but one more hidden layer (k = 3). Note how increasing the depth make the

function more complex. (For illustrative purposes we plot only the part of the function which lies

above zero.)

Definition 6. Let b1, . . . , bm  Rn. The zonotope formed by b1, . . . , bm  Rn is defined as

Z(b1, . . . , bm) := {1b1 + . . . + mbm : -1  i  1, i = 1, . . . , m}.

The set of vertices of Z(b1, . . . , bm) will be denoted by vert(Z(b1, . . . , bm)). The support function Z(b1,...,bm) : Rn  R associated with the zonotope Z(b1, . . . , bm) is defined as

Z(b1,...,bm)(r)

=

max
xZ (b1 ,...,bm )

r, x

.

The following results are well-known in the theory of zonotopes (Ziegler, 1995).

Theorem 3.6. The following are all true.

1. | vert(Z(b1, . . . , bm))|  (m - 1)n-1. The set of (b1, . . . , bm)  Rn × . . . × Rn such that this DOES NOT hold at equality is a 0 measure set.

2. Z(b1,...,bm)(r) = maxxZ(b1,...,bm) r, x = maxxvert(Z(b1,...,bm)) r, x , and Z(b1,...,bm) is therefore a piecewise linear function with | vert(Z(b1, . . . , bm))| pieces.

3. Z(b1,...,bm)(r) = | r, b1 | + . . . + | r, bm |.
Definition 7 (extremal zonotope set). The set S(n, m) will denote the set of (b1, . . . , bm)  Rn × . . . × Rn such that | vert(Z(b1, . . . , bm))| = (m - 1)n-1. S(n, m) is the so-called "extremal zonotope set", which is a subset of Rnm, whose complement has zero Lebesgue measure in Rnm.
Lemma 3.7. Given any b1, . . . , bm  Rn, there exists a 2-layer ReLU DNN with size 2m which represents the function Z(b1,...,bm)(r).
Definition 8. For p  N and a  pM , we define a function ha : R  R which is piecewise linear over the segments (-, 0], [0, a1], [a1, a2], . . . , [ap, M ], [M, +) defined as follows: ha(x) = 0 for all x  0, ha(ai) = M (i mod 2), and ha(M ) = M - ha(ap) and for x  M , ha(x) is a linear continuation of the piece over the interval [ap, M ]. Note that the function has p + 2 pieces, with the leftmost piece having slope 0. Furthermore, for a1, . . . , ak  pM , we denote the composition of the functions ha1 , ha2 , . . . , hak by
Ha1,...,ak := hak  hak-1  . . .  ha1 .
Proposition 3.8. Given any tuple (b1, . . . , bm)  S(n, m) and any point

(a1, . . . , ak) 

(Mw-1 × wM-1 × . . . × Mw-1),

M >0

k times

6

Under review as a conference paper at ICLR 2018

the function ZONOTOPEkn,w,m[a1, . . . , ak, b1, . . . , bm] := Ha1,...,ak  Z(b1,...,bm) has (m - 1)n-1wk pieces and it can be represented by a k + 2 layer ReLU DNN with size 2m + wk.

Finally, we are ready to state the main result of this section. Theorem 3.9. For every tuple of natural numbers n, k, m  1 and w  2, there exists a family of Rn  R functions, which we call ZONOTOPEkn,w,m with the following properties:

(i) Every f  ZONOTOPEkn,w,m is representable by a ReLU DNN of depth k + 2 and size 2m + wk, and has (m - 1)n-1wk pieces.

(ii) Consider any f  ZONOTOPEnk,w,m. If f is represented by a (k + 1)layer DNN for any k  k, then this (k + 1)-layer DNN has size at least

max

1 2

(k

wk kn

)

·

(m

-

1)(1-

1 n

)

1 k

-1

,

k
kw k
n1/k

.

(iii) The family ZONOTOPEnk,w,m is in one-to-one correspondence with

S(n, m) ×

(Mw-1 × Mw-1 × . . . × wM-1) .

M >0

k times

Lets choose w  N such that s  wk-1 + w(k - 1) and set m = wk-1. Now we recall the

ZadneOfidNnwiOtiooTunOroPhfaEcrodnkm,wfup,mnHc(htniao,vnke,fasm)sginliyvpeainettcaaeitnst.shTecoobmtehgpeiHnbn(enisnt,gkoof, sfo)tuhr=eksneoc(wtsilonen)d.gaenIn,dsosuetchehetrehxwaptoofrondresntththiiaesl

choice of m functions in dependence

on the input dimension of the number of affine pieces in a neurally representable function has not

been demonstrated in previous constructions.

4 TRAINING 2-LAYER Rn  R RELU DNNS TO GLOBAL OPTIMALITY

In this section we consider the following empirical risk minimization problem. Given D data points

(xi, yi)  Rn × R, i = 1, . . . , D, find the function f represented by 2-layer Rn  R ReLU DNNs

of width w, that minimizes the following optimization problem

min 1 D

Df F{n,w,1}

i=1

(f (xi), yi)

 min 1 D

DT1Anw, T2Lw1

i=1

T2((T1(xi))), yi

(4.1)

where : R × R  R is a convex loss function (common loss functions are the squared loss, (y, y ) = (y - y )2, and the hinge loss function given by (y, y ) = max{0, 1 - yy }). Our main result of this section gives an algorithm to solve the above empirical risk minimization problem to global optimality.

Theorem 4.1. There exists an algorithm to find a global optimum of Problem 4.1 in time O(2w(D)nwpoly(D, n, w)). Note that the running time O(2w(D)nwpoly(D, n, w)) is polynomial in the data size D for fixed n, w.

Proof Sketch: A full proof of Theorem 4.1 is included in Appendix C. Here we provide a sketch of the proof. When the empirical risk minimization problem is viewed as an optimization problem in the space of weights of the ReLU DNN, it is a nonconvex, quadratic problem. However, one can instead search over the space of functions representable by 2-layer DNNs by writing them in the form similar to (2.1). This breaks the problem into two parts: a combinatorial search and then a convex problem that is essentially linear regression with linear inequality constraints. This enables us to guarantee global optimality.
Let T1(x) = Ax + b and T2(y) = a · y for A  Rw×n and b, a  Rw. If we denote the i-th row of the matrix A by ai, and write bi, ai to denote the i-th coordinates of the vectors b, a respectively, due to homogeneity of ReLU gates, the network output can be represented as
ww
f (x) = ai max{0, ai · x + bi} = si max{0, a~i · x + ~bi}.
i=1 i=1
where a~i  Rn, ~bi  R and si  {-1, +1} for all i = 1, . . . , w. For any hidden node i  {1 . . . , w}, the pair (a~i, ~bi) induces a partition Pi := (P+i , P-i ) on the dataset, given by P-i = {j :

7

Under review as a conference paper at ICLR 2018

Algorithm 1 Empirical Risk Minimization

1: function ERM(D) 2: S = {+1, -1}w
3: Pi = {(P+i , P-i )}, i = 1, . . . , w 4: P = P1 × P2 × · · · × Pw

Where D = {(xi, yi)}Di=1  Rn × R All possible instantiations of top layer weights
All possible partitions of data into two parts

5: count = 1

6: for s  S do

7: for {(P+i , P-i )}wi=1  P do 

D



  

minimize:

(si(a~i · xj + ~bi), yj )

8:

 

a~,~b

loss(count) =

j=1 i:jP+i

Counter





  

subject

to:



a~i · xj + ~bi  0 j  P-i a~i · xj + ~bi  0 j  P+i

9: count + +

10: end for

11: OPT = argmin loss(count)

12: end for 13: return {a~}, {b~}, s corresponding to OPT's iterate

14: end function

a~i · xj + b~i  0} and P+i = {1, . . . , D}\P-i . Algorithm 1 proceeds by generating all combinations of the partitions Pi as well as the top layer weights s  {+1, -1}w, and minimizing the loss

D j=1

i:jP+i (si(a~i · xj + ~bi), yj) subject to the constraints a~i · xj + ~bi  0 j  P-i and

a~i · xj + ~bi  0 j  P+i which are imposed for all i = 1, . . . , w, which is a convex program.

Algorithm 1 implements the empirical risk minimization (ERM) rule for training ReLU DNN with one hidden layer. To the best of our knowledge there is no other known algorithm that solves the ERM problem to global optimality. We note that due to known hardness results exponential dependence on the input dimension is unavoidable Blum & Rivest (1992); Shalev-Shwartz & BenDavid (2014); Algorithm 1 runs in time polynomial in the number of data points. To the best of our knowledge there is no hardness result known which rules out empirical risk minimization of deep nets in time polynomial in circuit size or data size. Thus our training result is a step towards resolving this gap in the complexity literature.

A related result for improperly learning ReLUs has been recently obtained by Goel et al (Goel et al., 2016). In contrast, our algorithm returns a ReLU DNN from the class being learned. Another difference is that their result considers the notion of reliable learning as opposed to the empirical risk minimization objective considered in (4.1).

5 DISCUSSION
The running time of the algorithm that we give in this work to find the exact global minima of a two layer ReLU-DNN is exponential in the input dimension n and the number of hidden nodes w. It is unlikely, due to complexity theory beliefs such as P = N P , that the exponential dependence on n can be removed, if one requires the global optimality guarantee; see the book by Shalev-Schwartz and Ben-David Shalev-Shwartz & Ben-David (2014) and Blum & Rivest (1992); DasGupta et al. (1995). However, it is not clear to us whether the globally optimal algorithms have to necessarily be exponential in the number of hidden nodes. We are not aware of any complexity results which would rule out the possibility of an algorithm which trains to global optimality in time that is polynomial in the data size and the number of hidden nodes, assuming that the input dimension is a fixed constant. Resolving this dependence on network size would be another step towards clarifying the theoretical complexity of training ReLU DNNs and is a good open question for future research, in our opinion. Perhaps an even better breakthrough would be to get optimal training algorithms for DNNs with two or more hidden layers and this seems like a substantially harder nut to crack. It would also be a significant breakthrough to get gap results between consecutive constant depths or between logarithmic and constant depths.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Eric Allender. Complexity theory lecture notes. https://www.cs.rutgers.edu/ ~allender/lecture.notes/, 1998.
Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999.
Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.
Avrim L. Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. Neural Networks, 5(1):117­127, 1992.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303­314, 1989.
George E. Dahl, Tara N. Sainath, and Geoffrey E. Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 8609­8613. IEEE, 2013.
Bhaskar DasGupta, Hava T. Siegelmann, and Eduardo Sontag. On the complexity of training neural networks with continuous activation functions. IEEE Transactions on Neural Networks, 6(6): 1490­1504, 1995.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th Annual Conference on Learning Theory, pp. 907­940, 2016.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.
Benjamin D. Haeffele and Rene´ Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. arXiv preprint arXiv:1708.02691, 2017.
Johan Hastad. Almost optimal lower bounds for small depth circuits. In Proceedings of the eighteenth annual ACM symposium on Theory of computing, pp. 6­20. ACM, 1986.
Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82­97, 2012.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527­1554, 2006.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251­257, 1991.
Stasys Jukna. Boolean function complexity: advances and frontiers, volume 27. Springer Science & Business Media, 2012.
Daniel M. Kane and Ryan Williams. Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits. arXiv preprint arXiv:1511.07860, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Quoc V. Le. Building high-level features using large scale unsupervised learning. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 8595­8598. IEEE, 2013.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Shiyu Liang and R Srikant. Why deep neural networks for function approximation? 2016.
9

Under review as a conference paper at ICLR 2018
Jiri Matousek. Lectures on discrete geometry, volume 212. Springer Science & Business Media, 2002.
Guido F. Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in neural information processing systems, pp. 2924­2932, 2014.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098, 2013.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Alexander A. Razborov. Lower bounds on the size of bounded depth circuits over a complete basis with logical addition. Mathematical Notes, 41(4):333­338, 1987.
Benjamin Rossman, Rocco A. Servedio, and Li-Yang Tan. An average-case depth hierarchy theorem for boolean circuits. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pp. 1030­1048. IEEE, 2015.
H.L. Royden and P.M. Fitzpatrick. Real Analysis. Prentice Hall, 2010. Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural
networks. In International Conference on Machine Learning, pp. 2979­2987, 2017. Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep boltzmann machines. In International Confer-
ence on Artificial Intelligence and Statistics (AISTATS), volume 1, pp. 3, 2009. R. Saptharishi. A survey of lower bounds in arithmetic circuit complexity, 2014. Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Over-
feat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR 2014). arXiv preprint arXiv:1312.6229, 2014. Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint arXiv:1609.01037, 2016. Amir Shpilka and Amir Yehudayoff. Arithmetic circuits: A survey of recent results and open questions. Foundations and Trends R in Theoretical Computer Science, 5(3­4):207­388, 2010. Roman Smolensky. Algebraic methods in the theory of lower bounds for boolean circuit complexity. In Proceedings of the nineteenth annual ACM symposium on Theory of computing, pp. 77­82. ACM, 1987. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929­1958, 2014. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014. Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101, 2015. Matus Telgarsky. benefits of depth in neural networks. In 29th Annual Conference on Learning Theory, pp. 1517­1539, 2016. Shuning Wang. General constructive representations for continuous piecewise-linear functions. IEEE Transactions on Circuits and Systems I: Regular Papers, 51(9):1889­1896, 2004. Shuning Wang and Xusheng Sun. Generalization of hinging hyperplanes. IEEE Transactions on Information Theory, 51(12):4425­4431, 2005. Dmitry Yarotsky. Error bounds for approximations with deep relu networks. arXiv preprint arXiv:1610.01145, 2016. Gu¨nter M. Ziegler. Lectures on polytopes, volume 152. Springer Science & Business Media, 1995.
10

Under review as a conference paper at ICLR 2018

A EXPRESSING PIECEWISE LINEAR FUNCTIONS USING RELU DNNS

Proof of Theorem 2.2. Any continuous piecewise linear function R  R which has m pieces can be specified by three pieces of information, (1) sL the slope of the left most piece, (2) the coordinates of the non-differentiable points specified by a (m - 1)-tuple {(ai, bi)}im=-11 (indexed from left to right) and (3) sR the slope of the rightmost piece. A tuple (sL, sR, (a1, b1), . . . , (am-1, bm-1) uniquely specifies a m piecewise linear function from R  R and vice versa. Given such a tuple, we construct a 2-layer DNN which computes the same piecewise linear function.
One notes that for any a, r  R, the function

f (x) =

0 r(x - a)

xa x>a

(A.1)

is equal to sgn(r) max{|r|(x - a), 0}, which can be implemented by a 2-layer ReLU DNN with size 1. Similarly, any function of the form,

g(x) =

t(x - a) 0

xa x>a

(A.2)

is equal to - sgn(t) max{-|t|(x - a), 0}, which can be implemented by a 2-layer ReLU DNN with size 1. The parameters r, t will be called the slopes of the function, and a will be called the breakpoint of the function.If we can write the given piecewise linear function as a sum of m
functions of the form (A.1) and (A.2), then by Lemma D.2 we would be done. It turns out that such a decomposition of any p piece PWL function h : R  R as a sum of p flaps can always be arranged where the breakpoints of the p flaps all are all contained in the p - 1 breakpoints of h. First, observe that adding a constant to a function does not change the complexity of the ReLU
DNN expressing it, since this corresponds to a bias on the output node. Thus, we will assume
that the value of h at the last break point am-1 is bm-1 = 0. We now use a single function f of the form (A.1) with slope r and breakpoint a = am-1, and m - 1 functions g1, . . . , gm-1 of the form (A.2) with slopes t1, . . . , tm-1 and breakpoints a1, . . . , am-1, respectively. Thus, we wish to express h = f + g1 + . . . + gm-1. Such a decomposition of h would be valid if we can find values for r, t1, . . . , tm-1 such that (1) the slope of the above sum is = sL for x < a1, (2) the slope of the above sum is = sR for x > am-1, and (3) for each i  {1, 2, 3, .., m - 1} we have bi = f (ai) + g1(ai) + . . . + gm-1(ai). The above corresponds to asking for the existence of a solution to the following set of simultaneous
linear equations in r, t1, . . . , tm-1:

m-1

sR = r, sL = t1 + t2 + . . . + tm-1, bi =

tj(aj-1 - aj) for all i = 1, . . . , m - 2

j=i+1

It is easy to verify that the above set of simultaneous linear equations has a unique solution. Indeed,
r must equal sR, and then one can solve for t1, . . . , tm-1 starting from the last equation bm-2 = tm-1(am-2 - am-1) and then back substitute to compute tm-2, tm-3, . . . , t1. The lower bound of p - 1 on the size for any 2-layer ReLU DNN that expresses a p piece function follows from
Lemma D.6.

One can do better in terms of size when the rightmost piece of the given function is flat, i.e., sR = 0. In this case r = 0, which means that f = 0; thus, the decomposition of h above is of size p - 1. A similar construction can be done when sL = 0. This gives the following statement which will be useful for constructing our forthcoming hard functions.
Corollary A.1. If the rightmost or leftmost piece of a R  R piecewise linear function has 0 slope, then we can compute such a p piece function using a 2-layer DNN with size p - 1.

Proof of theorem 2.3. Since any piecewise linear function Rn  R is representable by a ReLU DNN by Corollary 2.1, the proof simply follows from the fact that the family of continuous piecewise linear functions is dense in any Lp(Rn) space, for 1  p  .

11

Under review as a conference paper at ICLR 2018

B BENEFITS OF DEPTH
B.1 CONSTRUCTING A CONTINUUM OF HARD FUNCTIONS FOR R  R RELU DNNS AT
EVERY DEPTH AND EVERY WIDTH
Lemma B.1. For any M > 0, p  N, k  N and a1, . . . , ak  Mp , if we compose the functions ha1 , ha2 , . . . , hak the resulting function is a piecewise linear function with at most (p + 1)k + 2 pieces, i.e.,
Ha1,...,ak := hak  hak-1  . . .  ha1
is piecewise linear with at most (p + 1)k + 2 pieces, with (p + 1)k of these pieces in the range [0, M ] (see Figure 2). Moreover, in each piece in the range [0, M ], the function is affine with minimum value 0 and maximum value M .

Proof. Simple induction on k.

Proof of Theorem 3.2. Given k  1 and w  2, choose any point

(a1, . . . , ak) 

(wM-1 × Mw-1 × . . . × wM-1) .

M >0

k times

By Definition 8, each hai , i = 1, . . . , k is a piecewise linear function with w + 1 pieces and the leftmost piece having slope 0. Thus, by Corollary A.1, each hai , i = 1, . . . , k can be represented by a 2-layer ReLU DNN with size w. Using Lemma D.1, Ha1,...,ak can be represented by a k + 1 layer DNN with size wk; in fact, each hidden layer has exactly w nodes.

Proof of Theorem 3.1. Follows from Theorem 3.2 and Lemma D.6.

1 0.8 0.6 0.4 0.2
0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2
1 0.8 0.6 0.4 0.2
0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2
1 0.8 0.6 0.4 0.2
0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2
Figure 2: Top: ha1 with a1  21 with 3 pieces in the range [0, 1]. Middle: ha2 with a2  11 with 2 pieces in the range [0, 1]. Bottom: Ha1,a2 = ha2  ha1 with 2 · 3 = 6 pieces in the range [0, 1]. The dotted line in the bottom panel corresponds to the function in the top panel. It shows that for every piece of the dotted graph, there is a full copy of the graph in the middle panel.

Proof of Theorem 3.5. Given k  1 and w  2 define q := wk and sq := ha  ha  . . .  ha where

k times

a

=

(

1 w

,

2 w

,

.

.

.

,

w-1 w

)



1q-1.

Thus,

sq

is

representable

by

a

ReLU

DNN

of

width

w

+1

and

depth

k + 1 by Lemma D.1. In what follows, we want to give a lower bound on the 1 distance of sq from

any continuous p-piecewise linear comparator gp : R  R. The function sq contains

q 2

triangles

of

width

2 q

and

unit

height.

A

p-piecewise

linear

function

has

p-1

breakpoints

in

the

interval

[0,

1].

So that in at least

wk 2

- (p - 1) triangles, gp has to be affine. In the following we demonstrate that

12

Under review as a conference paper at ICLR 2018

inside any triangle of sq, any affine function will incur an

1

error

of

at

least

1 2wk

.

2i+2

wk |sq(x) - gp(x)|dx =

x=

2i wk

2 wk
x=0

sq(x) - (y1 + (x - 0) ·

y2 - y1

2 wk

-0

)

dx

=

1 wk
x=0

xwk - y1 -

wkx 2

(y2

-

y1)

dx +

2 wk

x=

1 wk

2 - xwk - y1 -

wkx 2

(y2

-

y1)

dx

=

1 wk

1 z=0

z

-

y1

-

z 2

(y2

-

y1)

dz

+

1 wk

2 z=1

2

-

z

-

y1

-

z 2

(y2

-

y1)

dz

=

1 wk

-3

+

y1

+

2

2y12 + y1 -

y2

+

y2

+

2(-2 + y1)2 2 - y1 + y2

The

above

integral

attains

its

minimum

of

1 2wk

at

y1

=

y2

=

1 2

.

Putting

together,

swk - gp 1 

wk 2

- (p - 1)

·

1 2wk



wk

- 1 - 2(p - 1) 4wk

=

1 4

-

2p - 1 4wk

Thus, for any  > 0,

p



wk

-

4wk  2

+

1

=

2p

-

1



(

1 4

-

)4wk

=

1 4

-

2p - 1 4wk





=

The result now follows from Lemma D.6.

swk - gp 1  .

B.2 A CONTINUUM OF HARD FUNCTIONS FOR Rn  R FOR n  2
Proof of Lemma 3.7. By Theorem 3.6 part 3., Z(b1,...,bm)(r) = | r, b1 | + . . . + | r, bm |. It suffices to observe
| r, b1 | + . . . + | r, bm | = max{ r, b1 , - r, b1 } + . . . + max{ r, bm , - r, bm }.

Proof of Proposition 3.8. The fact that ZONOTOPEnk,w,m[a1, . . . , ak, b1, . . . , bm] can be represented by a k + 2 layer ReLU DNN with size 2m + wk follows from Lemmas 3.7 and D.1. The number of pieces follows from the fact that Z(b1,...,bm) has (m - 1)n-1 distinct linear pieces by parts 1. and 2. of Theorem 3.6, and Ha1,...,ak has wk pieces by Lemma B.1.
Proof of Theorem 3.9. Follows from Proposition 3.8.

C EXACT EMPIRICAL RISK MINIMIZATION

Proof of Theorem 4.1. Let : R  R be any convex loss function, and let (x1, y1), . . . , (xD, yD)  Rn × R be the given D data points. As stated in (4.1), the problem requires us to find an affine transformation T1 : Rn  Rw and a linear transformation T2 : Rw  R, so as to minimize the empirical loss as stated in (4.1). Note that T1 is given by a matrix A  Rw×n and a vector b  Rw so that T (x) = Ax + b for all x  Rn. Similarly, T2 can be represented by a vector a  Rw such that T2(y) = a · y for all y  Rw. If we denote the i-th row of the matrix A by ai, and write bi, ai to denote the i-th coordinates of the vectors b, a respectively, we can write the function represented by this network as

ww
f (x) = ai max{0, ai · x + bi} = sgn(ai) max{0, (|ai|ai) · x + |ai|bi}.
i=1 i=1

In other words, the family of functions over which we are searching is of the form

w
f (x) = si max{0, a~i · x + ~bi}
i=1

(C.1)

13

Under review as a conference paper at ICLR 2018

where a~i  Rn, bi  R and si  {-1, +1} for all i = 1, . . . , w. We now make the following observation. For a given data point (xj, yj) if a~i · xj + ~bi  0, then the i-th term of (C.1) does not contribute to the loss function for this data point (xj, yj). Thus, for every data point (xj, yj), there exists a set Sj  {1, . . . , w} such that f (xj) = iSj si(a~i · xj + ~bi). In particular, if we are given the set Sj for (xj, yj), then the expression on the right hand side of (C.1) reduces to a linear function of a~i, ~bi. For any fixed i  {1, . . . , w}, these sets Sj induce a partition of the data set into two parts. In particular, we define P+i := {j : i  Sj} and P-i := {1, . . . , D} \ P+i . Observe now that this partition is also induced by the hyperplane given by a~i, ~bi: P+i = {j : a~i · xj + ~bi > 0} and P+i = {j : a~i · xj + ~bi  0}. Our strategy will be to guess the partitions P+i , P-i for each i = 1, . . . , w, and then do linear regression with the constraint that regression's decision variables a~i, ~bi induce the guessed partition.
More formally, the algorithm does the following. For each i = 1, . . . , w, the algorithm guesses a partition of the data set (xj, yj), j = 1, . . . , D by a hyperplane. Let us label the partitions as follows (P+i , P-i ), i = 1, . . . , w. So, for each i = 1, . . . , w, P+i  P-i = {1, . . . , D}, P+i and P-i are disjoint, and there exists a vector c  Rn and a real number  such that P-i = {j : c · xj +   0} {an+d1P, -+i 1=}w{. j : c · xj +  > 0}. Further, for each i = 1, . . . , w the algorithm selects a vector s in
For a fixed selection of partitions (P+i , P-i ), i = 1, . . . , w and a vector s in {+1, -1}w, the algorithm solves the following convex optimization problem with decision variables a~i  Rn, ~bi  R for i = 1, . . . , w (thus, we have a total of (n + 1) · w decision variables). The feasible region of the optimization is given by the constraints

a~i · xj + ~bi  0 j  P-i a~i · xj + ~bi  0 j  P+i

(C.2)

which are imposed for all i = 1, . . . , w. Thus, we have a total of D · w constraints. Subject to

these constraints we minimize the objective

D j=1

i:jP+i (si(a~i · xj + ~bi), yj ). Assuming the

loss function is a convex function in the first argument, the above objective is a convex function.

Thus, we have to minize a convex objective subject to the linear inequality constraints from (C.2).

We finally have to count how many possible partitions (P+i , P-i ) and vectors s the algorithm has to search through. It is well-known Matousek (2002) that the total number of possible hyperplane

partitions i = 1, . . .

of a , w,

set we

of size have a

D in total

Rn is at most 2 of at most Dnw

DpnartitioDnns.wThheenreevaerren2wv2e.cTtohrussswinit{h-a 1g,u+es1s}fwor.

each This

gives us a total of 2wDnw guesses for the partitions (P+i , P-i ) and vectors s. For each such guess,

we have a convex optimization problem with (n + 1) · w decision variables and D · w constraints,

which can be solved in time poly(D, n, w). Putting everything together, we have the running time

claimed in the statement.

The only

above holds

argument holds for n  2. For

only n=

for n  2, 1, a similar

since we algorithm

used

the

inequality

2

D n

can be designed, but one

 Dn which which uses the

characterization achieved in Theorem 2.2. Let : R  R be any convex loss function, and let (x1, y1), . . . , (xD, yD)  R2 be the given D data points. Using Theorem 2.2, to solve problem (4.1)

it suffices to find a R  R piecewise linear function f with w pieces that minimizes the total loss.

In other words, the optimization problem (4.1) is equivalent to the problem

D
min (f (xi), yi) : f is piecewise linear with w pieces .
i=1

(C.3)

We now use the observation that fitting piecewise linear functions to minimize loss is just a step
away from linear regression, which is a special case where the function is contrained to have exactly
one affine linear piece. Our algorithm will first guess the optimal partition of the data points such that all points in the same class of the partition correspond to the same affine piece of f , and then do linear regression in each class of the partition. Altenatively, one can think of this as guessing the interval (xi, xi+1) of data points where the w - 1 breakpoints of the piecewise linear function will lie, and then doing linear regression between the breakpoints.

14

Under review as a conference paper at ICLR 2018

More formally, we parametrize piecewise linear functions with w pieces by the w slope-intercept
values (a1, b1), . . . , (a2, b2), . . . , (aw, bw) of the w different pieces. This means that between breakpoints j and j + 1, 1  j  w - 2, the function is given by f (x) = aj+1x + bj+1, and the first and last pieces are a1x + b1 and awx + bw, respectively.

Define I to be the set of all (w - 1)-tuples (i1, . . . , iw-1) of natural numbers such that 1  i1  . . .  iw-1  D. Given a fixed tuple I = (i1, . . . , iw-1)  I, we wish to search through all piece-

wise linear . . . , (xiw-1

functions whose breakpoints, , xiw-1+1). Define also S =

in order, appear in the {-1, 1}w-1. Any S

intervals  S will

(xi1 , xi1+1), (xi2 , xi2+1), have the following inter-

pretation: if Sj = 1 then aj  aj+1, and if Sj = -1 then aj  aj+1. Now for every I  I

and S  S, requiring a piecewise linear function that respects the conditions imposed by I and

S is easily seen to be equivalent to imposing the following linear inequalities on the parameters

(a1, b1), . . . , (a2, b2), . . . , (aw, bw):

Sj (bj+1 - bj - (aj - aj+1)xij )  0 Sj (bj+1 - bj - (aj - aj+1)xij+1)  0
Sj (aj+1 - aj )  0

(C.4)

Let the set of piecewise linear functions whose breakpoints satisfy the above be denoted by PWLI1,S for I  I, S  S.

Given a particular I  I, we define

D1 := {xi : i  i1}, Dj := {xi : ij-1 < i  i1} j = 2, . . . , w - 1, . Dw := {xi : i > iw-1}

Observe that

Dw
min{ (f (xi)-yi) : f  PWLI1,S} = min{
i=1 j=1

(aj ·xi +bj -yi )
iDj

: (aj, bj) satisfy (C.4)} (C.5)

The right hand side of the above equation is the problem of minimizing a convex objective subject to

linear constraints. Now, to solve (C.3), we need to simply solve the problem (C.5) for all I  I, S 

S and pick the minimum. Since |I| =

D w

= O(Dw) and |S| = 2w-1 we need to solve O(2w · Dw)

convex optimization problems, each taking time O(poly(D)). Therefore, the total running time is

O((2D)w poly(D)).

D AUXILIARY LEMMAS
Now we will collect some straightforward observations that will be used often. The following operations preserve the property of being representable by a ReLU DNN. Lemma D.1. [Function Composition] If f1 : Rd  Rm is represented by a d, m ReLU DNN with depth k1 + 1 and size s1, and f2 : Rm  Rn is represented by an m, n ReLU DNN with depth k2 + 1 and size s2, then f2  f1 can be represented by a d, n ReLU DNN with depth k1 + k2 + 1 and size s1 + s2.
Proof. Follows from (1.1) and the fact that a composition of affine transformations is another affine transformation.
Lemma D.2. [Function Addition] If f1 : Rn  Rm is represented by a n, m ReLU DNN with depth k + 1 and size s1, and f2 : Rn  Rm is represented by a n, m ReLU DNN with depth k + 1 and size s2, then f1 + f2 can be represented by a n, m ReLU DNN with depth k + 1 and size s1 + s2.
Proof. We simply put the two ReLU DNNs in parallel and combine the appropriate coordinates of the outputs.
15

Under review as a conference paper at ICLR 2018

Lemma D.3. [Taking maximums/minimums] Let f1, . . . , fm : Rn  R be functions that can each be represented by Rn  R ReLU DNNs with depths ki + 1 and size si, i = 1, . . . , m. Then the function f : Rn  R defined as f (x) := max{f1(x), . . . , fm(x)} can be represented by a ReLU DNN of depth at most max{k1, . . . , km} + log(m) + 1 and size at most s1 + . . . sm + 4(2m - 1). Similarly, the function g(x) := min{f1(x), . . . , fm(x)} can be represented by a ReLU DNN of depth at most max{k1, . . . , km} + log(m) + 1 and size at most s1 + . . . sm + 4(2m - 1).

Proof. We prove this by induction on m. The base case m = 1 is trivial. For m  2, consider

magn1m2ad:x=s,{izkmem12s,a.ax.t<{.mf, 1kom,s.tm2.sw.1h},+ef+n.m2.ml.os}g(am2nm2d2)+,g)24g1(:+2=an1m2mdanag-dx2 {m1cfa)anaxm2n{bdke+s1rme2,mp.2.r+e.+1s,,1ef.n+m.te.}.d,..k.bBm+yy}sRt+mheeL+lUion4gd(D(u2Ncmt2mNi2osn)-oh+fy1pd)1o,eprrtheethsseppssieesaccttt(iismvvieenollcyyset.,

Therefore, the function G : Rn  R2 given by G(x) = (g1(x), g2(x)) can be implemented by a

ReLU DNN with depth at most max{k1, . . . , km} +

log(

m 2

)

+ 1 and size at most s1 + . . . +

sm + 4(2m - 2).

We now show how to represent the function T : R2  R defined as T (x, y) = max{x, y} =

x+y 2

+

|x-y| 2

fact that f =

by a 2-layer ReLU DNN T  G and Lemma D.1.

with

size

4

­

see

Figure

3.

The

result

now

follows

from

the

Input x1 Input x2

1

1

1 2

-1

-1 -1

-

1 2

+x1 +x2
2

|x1 -x2 | 2

1

2

1

1

1 2

-1

Figure

3:

A

2-layer

ReLU

DNN

computing

max{x1, x2}

=

x1 +x2 2

+

|x1 -x2 | 2

Lemma D.4. Any affine transformation T : Rn  Rm is representable by a 2-layer ReLU DNN of

size 2m.

Proof. Simply use the fact that T = (I    T ) + (-I    (-T )), and the right hand side can be represented by a 2-layer ReLU DNN of size 2m using Lemma D.2.
Lemma D.5. Let f : R  R be a function represented by a R  R ReLU DNN with depth k + 1 and widths w1, . . . , wk of the k hidden layers. Then f is a PWL function with at most 2k-1 · (w1 + 1) · w2 · . . . · wk pieces.

1 0.8 0.6 0.4 0.2
0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2
0.6 0.4 0.2
0 -0.2 -0.4
-0.2 0 0.2 0.4 0.6 0.8 1 1.2
Figure 4: The number of pieces increasing after activation. If the blue function is f , then the red function g = max{0, f + b} has at most twice the number of pieces as f for any bias b  R.
16

Under review as a conference paper at ICLR 2018

Proof. We prove this by induction on k. The base case is k = 1, i.e, we have a 2-layer ReLU DNN. Since every activation node can produce at most one breakpoint in the piecewise linear function, we can get at most w1 breakpoints, i.e., w1 + 1 pieces.
Now for the induction step, assume that for some k  1, any R  R ReLU DNN with depth k + 1 and widths w1, . . . , wk of the k hidden layers produces at most 2k-1 · (w1 + 1) · w2 · . . . · wk pieces.
Consider any R  R ReLU DNN with depth k + 2 and widths w1, . . . , wk+1 of the k + 1 hidden layers. Observe that the input to any node in the last layer is the output of a R  R ReLU DNN with depth k + 1 and widths w1, . . . , wk. By the induction hypothesis, the input to this node in the last layer is a piecewise linear function f with at most 2k-1 · (w1 + 1) · w2 · . . . · wk pieces. When we apply the activation, the new function g(x) = max{0, f (x)}, which is the output of this node, may have at most twice the number of pieces as f , because each original piece may be intersected by the x-axis; see Figure 4. Thus, after going through the layer, we take an affine combination of wk+1 functions, each with at most 2 · (2k-1 · (w1 + 1) · w2 · . . . · wk) pieces. In all, we can therefore get at most 2·(2k-1 ·(w1 +1)·w2 ·. . .·wk)·wk+1 pieces, which is equal to 2k ·(w1 +1)·w2 ·. . .·wk ·wk+1, and the induction step is completed.

Lemma D.5 has the following consequence about the depth and size tradeoffs for expressing functions with agiven number of pieces.

Lemma D.6. Let f : R  R be a piecewise linear function with p pieces. If f is represented by a

ReLU

DNN

with

depth

k

+

1,

then

it

must

have

size

at

least

1 2

kp1/k

-

1.

Conversely,

any

piecewise

linear function f that represented by a ReLU DNN of depth k + 1 and size at most s, can have at

most

(

2s k

)k

pieces.

Proof. Let widths of the k hidden layers be w1, . . . , wk. By Lemma D.5, we must have

2k-1 · (w1 + 1) · w2 · . . . · wk  p.

(D.1)

By the AM-GM inequality, minimizing the size w1 + w2 + . . . + wk subject to (D.1), means setting

w1 + 1

=

w2

=

...

=

wk .

This implies that w1 + 1

=

w2

=

...

=

wk



1 2

p1/k

.

The first

statement follows. The second statement follows using the AM-GM inequality again, this time with

a restriction on w1 + w2 + . . . + wk.

17

