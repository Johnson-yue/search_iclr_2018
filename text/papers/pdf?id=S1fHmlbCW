Under review as a conference paper at ICLR 2018
NEURAL NETWORKS FOR IRREGULARLY OBSERVED CONTINUOUS-TIME STOCHASTIC PROCESSES
Anonymous authors Paper under double-blind review
ABSTRACT
Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly. In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L2(R). Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed. As we obtain a family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform. Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization. We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.
INTRODUCTION
The predominant assumption made in deep learning for time series analysis is that observations are made regularly, with the same duration of time separating each successive timestamps (Cho et al., 2014; Graves et al., 2013; Sutskever et al., 2014; LeCun & Bengio, 1995; van den Oord et al., 2016; Bakshi & Stephanopoulos, 1993). However, this assumption is often inappropriate, as many real-world time series are observed irregularly and are, occasionally, event-driven (e.g., financial data, social networks, internet-of-things).
One common approach in working with irregularly observed time series is to interpolate the observations to realign them to a regular time-grid. However, interpolation schemes may result in spurious statistical artifacts, as shown in (Huth & Abergel, 2014; Belletti et al., 2017). Fortunately, procedures for working with irregularly observed time series in their unaltered form have been devised, notably in the field of Gaussian-processes and kernel-learning (Huth & Abergel, 2014; Belletti et al., 2017) and more recently in deep learning (Neil et al., 2016).
In this article, we investigate the underlying representation of time series data as it is processed by a neural network. Our objective is to identify a class of neural networks that provably guarantee information preservation for certain irregularly observed signals. In doing so, we must analyze neural networks from a frame theoretic perspective, which has enabled a clear understanding of the impact discrete sampling has on representations of continuous-time signals (Benedetto & Heller, 1990; Benedetto, 1992; Benedetto & Ferreira, 2003; Feichtinger & Gröchenig, 1994; Gröchenig, 1992; Mallat, 2008).
Although frame theory has historically been studied in the linear setting, recent work by Sun & Tang (2017) has related frames with non-linear operators in Banach space, to what can be interpreted as non-linear frames. Here, we extend this generalization frames to characterize entire families of neural networks. In doing so, we can show that the composition of certain non-linear neural layers (i.e., convolutions and fully-connected layers) form non-linear frames in L2(R), while others do not (i.e., recurrent layers).
Moreover, frame theory can be used to analyze randomly-observed time series. In particular, when observations are made according to a family of self-exciting point processes known as Hawkes processes (Daley & Vere-Jones, 2007). We prove that such processes, under certain assumptions of stability, almost surely yield non-linear frames on a class of band-limited functions. That is to say,
1

Under review as a conference paper at ICLR 2018

that despite having discrete and irregular observations, the signal of interest can still be smoothly recovered.
As we obtain a family of convolutional neural networks that constitute non-linear frames, we show that under certain conditions, such networks can efficiently divide the time-axis of a time series into orthogonal sub-spaces of different temporal scale and localization. Namely, we optimize the weights of our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform (Mallat, 1989). Our numerical experiments on synthetic data highlight this unique capacity that allows neural networks to learn sparse representations of signals in L2(R), and how such a property is particularly powerful when training parsimoniously parameterized auto-encoders. Such auto-encoders learn optimal ways of compressing certain classes of input signals.
Finally, we show that that the ability of these networks to divide time series into a set sub-spaces, corresponding to different temporal scales and localization, can be composed with existing predictive frameworks to improve both accuracy and efficiency. This is demonstrated on real-world video classification and financial forecasting tasks.

CONTRIBUTIONS & ORGANIZATION
1. Neural representations of L2(R): We introduce the article with a theoretical analysis of the sufficient conditions on neural networks that enable smoothly recoverable representations of signals in L2(R) and prove that, under certain assumptions, this property holds true in the irregularly observed setting.
2. Orthogonal representations in time: We proceed to show that by enforcing certain constraints on convolutional filters, we can guarantee that the representation that the neural network produces only depends on the coordinates of the input signal in an learned orthonormal basis.
3. Numerical experiments: Finally, we evaluate the resulting constrained convolutional neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.

NOTATION · L2(R) is the space of square-integrable real-valued functions defined on R and equipped with the norm induced by the inner product f, g  L2(R)  tR f (t)g(t)dt. · Ld2(R) is the space of square-integrable d-dimensional vector-valued functions defined on R and equipped with the norm induced by the inner product f, g  L2d(R)  tR f (t)T g(t)dt. · l2(R) is the space of square-integrable real-valued sequences indexed by Z and equipped with the norm induced by the inner product (x), (y)  l2(Z)  nZ xnyn. · ld2(R) is the space of square-integrable d-dimensional vector-valued sequences indexed by Z and equipped with the norm induced by the inner product (x), (y)  ld2(Z)  nZ xTn yn.
Note that the inner products of the spaces we consider are Hilbert spaces on the classes of equivalent functions for the Lebesgue measure.

· F T [] denotes the Fourier transform and z denotes the complex conjugate of z  C. Recall

the Fourier transform nZ e-2inxn.

of

a

sequence

(xn)



Z

is

given

at

any

frequency



by

F

T

[(xn)]()

=

· For a function f  t  R  f (t)  Rd, f ( - h) is short-hand to denote f  t  R  f (t - h)  Rd, likewise, f (  ) is used to denote f  t  R  f (t )  Rd.

· For a set A, () denotes the sequence (n)  AZ.

· ()[ 2] denotes (2n)nZ. That is, the dilation of a sequence by a factor of 2. · For two vectors (w0, w1)  R1d × Rd2, their concatenation is denoted by [w0, w1]  Rd1+d2 .

2

Under review as a conference paper at ICLR 2018

1 HOMEOMORPHIC NON-LINEAR ENCODINGS OF CONTINUOUS-TIME SIGNALS

We begin by investigating sufficient conditions on composite functions that guarantee such functions produce discretized representations of continuous-time signals that can be smoothly reconstructed.

1.1 FRAMES

To do so, we must leverage frame theory (Benedetto & Ferreira, 2003), a theory developed to precisely to characterize the suitable properties for linear representations of irregularly observed signals. Intuitively, a frame is a representation of a signal that enables signal recovery in a smooth manner (i.e., suitable for the representation to be homeomorphic).
Formally (Benedetto & Ferreira, 2003), we define a frame as an operator from L2(R) to l2(Z) that is characterized by a family of functions (Sn)nZ in L2(R) (i.e., the atoms of the frame).

Definition 1.1 Linear frame of L2(R): A linear operator corresponding to the family (S)  L2(R)Z,
F(S)  f  L2(R)  (< f, Sn >)nZ  l2(Z)
is a frame of L2(R) if and only if there exist two real-valued constants 0 < A  B such that

f  L2(R), A f

2 2



F(S)(f )

2 2

B

f

22.

Representations provided by frames depend smoothly on their inputs. Moreover, a direct consequence of the definition above is that a frame is invertible in a smooth manner on its image.

There are many examples of frames. For now, we provide two concrete examples from (Benedetto

& Ferreira, 2003; Mallat, 2008). Recall the definition of the Haar function as t  R  WHaar(t) =

1 if t  [0, 1 2), -1 if t  [1 2, 1), 0 otherwise, the set of dilations and translations of WHaar, namely

WHaar sampling

 -2lt 2l

t,lZ constitutes a frame of

frequency

1 t

,

the

set

of

translated

L2(R). Shannon's sampling theorem shows that, with

functions

 (s

sin(t t

t)

)nZ

is

also

a

frame

for

the

set

of

functions in L2(R) whose Fourier transforms is supported on the interval

-

1 t

,

1 t

. In both cases,

the atoms of the frame are orthonormal families of functions ­ it is trivial to prove that A = B = 1.

While the first frame works for the entire space of square integrable functions, the second only applies

to the sub-space of band-limited signals.

Proposition 1.1 Left inverses of Frames: If F(S) is a frame of L2(R) then F(S) has a left inverse

F+

=

(F(S)F(S))-1F(S)

that

is

1 A

Lipschitz,

where

F(S)

is

the

adjoint

of

F(S).

This fundamental proposition is proven in (Benedetto & Ferreira, 2003; Mallat, 2008). As our goal is to find the conditions for non-linear representations of L2(R) to be homeomorphic, we unfortunately
can not leverage properties in the linear setting. Therefore, we must adopt an alternative definition. Let a non-linear frame be an operator from L2(R) to l2(Z) that is characterized by a family of functions (Sn)nZ in L2(R) and a family of non-linear real valued functions (n)nZ defined over R (or Rd as a trivial extension).

Definition 1.2 Non-linear frame of L2(R): A non-linear discrete representation scheme

F(S),()  f  L2(R)  (n(< f, Sn >))n  l2(Z)

is a non-linear frame of L2(R) if there exist two real-valued constants 0 < A  B such that

f, g  L2(R),

A

f

-g

2 2



F(S),()(f ) - F(S),()(g)

2 2



B

f -g

2 2

.

It is worth noting that a linear frame (in the standard definition of the term) is still a frame in this non-linear setting.

Proposition 1.2 Smoothness of signal recovery: A non-linear frame is invertible on its image of

L2(R)

and

the

inverse

is

1 A

Lipschitz.

3

Under review as a conference paper at ICLR 2018

Proof 1.1 Consider f, g  L2(R) such that F(S),()(f ) = F(S),()(g), then, as A > 0, f - g 2 = 0

and therefore f and g are in same equivalence class in L2(R). Therefore, F(S),() is injective and

if we consider (x), (y)  F(S),()(L2(R)) and if we denote by fx the only element in L2(R) such

that F(S),()(fx) = (x), and fy the only element in L2(R) such that F(S),()(fy) = (y) then, by

definition of a non-linear frame,

fx - fy

2 2



1 A

x-y

2 2

.



In a later section, we will show that smooth signal recovery is crucial for non-linear signal approximations (consisting of a finite number of coefficients) to remain stable during reconstruction. However in order to show this, we must first explore the sufficient conditions on non-linear operators to produce non-linear frames. We start by introducing several definitions on multivariate real-valued functions.

Definition 1.3 Bi-Lipschitz-Invertible (BLI) operators: An operator   l2(Z)  l2(Z) such that

 0 < A < B s.t. (xn), (yn)  L2(Z),

A

x-y

2 2



(x) - (y)

2 2

B

x-y

22,

is a BLI operator and we refer to (A, B) as the framing constants of .

Theorem 1.1 BLI operators and linear frames: Let (l)l=1...L be a collection of BLI operators with framing constants ((Al, Bl))l=1...L and F a frame on L2(R) with framing constants A0 and B0. The composite operator fL      f1  F is a non-linear frame of L2(R) with framing constants lL=0 A0 and Ll=0 B0.

Proof 1.2 The proof of the theorem is immediate but we use it to expose how our careful choice of
the definition of non-linear frames is leveraged. First let us recall that injectivity is preserved by
composition. Then, we initiate an immediate proof by induction with a simple remark: consider two functions f, g  L2(R)

A1A0

f

-g

2 2

 A1

F(f ) - F(g)

2

 B1

F(f ) - F(g)

2 2

 B1B0

f

-g

2 2

To conclude the proof, a similar statement can then be made if we compose 1  F by 2, . . . L. 

This proof allows us to make guarantees about operator pipelines while relying on conditions that are simple to verify. We can now use the theory we have established to analyze the representational properties of neural networks; in particular, convolutional neural networks (CNN) and recurrent neural networks (RNN).

1.2 SUFFICIENT CONDITIONS ON CNNS TO PRODUCE NON-LINEAR FRAMES
Here, we study representational properties of recent CNN architectures (Chollet, 2016; Kaiser et al., 2017; Lebedev et al., 2014) that rely on depth-wise separable convolutions. We show that by enforcing certain constraints on the structure of temporal filters, we obtain a network that is, provably, a non-linear frame. Here we trade off expressiveness for representational guarantees as we impose constraints on network parameters.
In depth-wise separated convolution stacks (Chollet, 2016; Kaiser et al., 2017) a temporal convolution is applied before a depth-wise linear transform and finally a leaky ReLU layer. We assume that the depth-wise linear operators being learned are all full rank (or full column rank if they increase the number of dimensions of the representation). Such an assumption makes sense for CNNs being trained by a stochastic optimization method with non-pathological data-sets.
Inspired by the multi-scale parsing enabled by the discrete wavelet transform or dyadic wavelet transform we employ time domain convolutions that are conjugate mirror filters (Mallat, 2008). Such time domain filters constitute a decomposition filter bank consisting of cascading convolutions. The decomposition filter bank admits a dual reconstruction filter bank thereby guaranteeing injectivity.
Definition 1.4 Element-wise Leaky ReLU (LReLU): Consider 0 <  << 1, LReLU applies a piecelinear function element-wise as
LReLU  (xn)  ld2(Z)  (max(xn, xn))nZ  ld2(Z).

4

Under review as a conference paper at ICLR 2018

Definition 1.5 Depth-wise fully connected layer (DFC): Consider two integers di and do, A  Rdo,di and b  Rdo
DF CA,b  (xn)  ld2i (Z)  (LReLU (A × xn + b))nZ  ld2o (Z).
Lemma 1.1 Full column rank DFC (FDFC) layers are BLI: The function
w  Rdi  LReLU (Aw + b)  Rdo
with di  do and A is full column rank is left invertible. Also, the left inverse is Lipschitz as 0 < m  M  R such that
w  Rdi , m w - w 2  LReLU (Aw + b) - LReLU (Aw + b) 2  M w - w 2.
Proof 1.3 As LReLU is strictly increasing and continuous therefore it is invertible and as A is full column rank it admits a left inverse which proves the first part of the lemma. We finish the proof by using the fact that linear functions in vector spaces of finite dimensions are Lipschitz, the fact that LReLU and its inverse are Lipschitz, and the fact that Lipschitz-ness is preserved by composition. 

Let us now study the representational properties of time domain convolution layers whose filters are constrained in the Frequency domain.

Definition 1.6 Reconstructible convolution layer (RConv): Consider two convolution filters h, g  l2(Z) such that there exist h, g  l2(Z) and

  R, F T [h] × F T [h]() + F T [g] × F T [g]]() = 2.

(1)

The following convolution is a Reconstructible convolution layer:

RConvl,h,g  (xn)  ld2(Z)  ([h  xn, g  xn])nZ  l22×d(Z).

Later on, we show that entire families of such h, g  l2(Z) exist under some conditions on h, g. In particular Eq. (4) will provide simple sufficient conditions on h and g for Eq. (1) to hold.

Lemma 1.2 Temporal convolutions allowing reconstruction: Consider two temporal convolution

filters hl, h~l such that their Fourier transforms satisfy (1). If xl0+1 = hl  xl and xl1+1 = gl  xl then

xl

=

1 2

h[ -1]  xl0+1 + g[ -1]  x1l+1

(where [ -1] means that we iterate in reverse order on the

filter weights) which proves the pair of convolution filters constitutes an invertible operator. Also,

 0 < m  M such that

m

xl

2 2



xl0+1

2 2

+

xl1+1

2 2



M

xl

22.

Proof 1.4 By definition h[ -1]  x0l+1 +g[ -1]  x1l+1 = h[ -1]  h  xl + g~[ -1]  g  xl, therefore, if we recall that the Fourier Transform diagonalizes convolutions and turns time reversal into complex conjugacy, we have

F T h[ -1]  xl0+1 + g[ -1]  xl1+1 = F T h × F T [h] + F T [g] × F T [g] ×F T xl = 2F T xl

with condition (1). Also with the Plancherel formula

h  xl

2 2

=

F T [h  xl]

2 2

=

F T [h]

2 2

×

F T [xl]

2 2

=

h

2 2

xl

22.

and finally,

F T [xl]

2 2

=

1 4

F T [h[

-1]  xl0+1 + g[

-1]  xl1+1]

2 2



1 4

F T [h[ -1]  xl0+1

2 2

+

F T [g[ -1]  x1l+1]

2 2

which concludes the proof with

m=

h

2 2

+

4

g

2 2

and

M

=

h

2 2

+

g

2 2

.

We represent the recomposition operation introduced in Lemma 1.2 in . The next proposition shows how the convolutions RConv and non-linearity LReLU can be interleaved to produce non-linear frames.

Proposition 1.3 Compositions of FDFC and RConv layers are BLI: The composite function from ld2i (Z) onto ld2o (Z)
F DF CAL,bL  RConvL,hl,gl      F DF CA1,b1  RConv1,h1,g1
is BLI.

5

Under review as a conference paper at ICLR 2018

Decomposition

Recomposition

time depth

depth

time

(xnl )

g

Al,0x +bl,0

LReLU

(xln+1,0)

LReLU-1

Al+,0(x-b)

~g[::-1] 1/2

h

Al,1x +bl,1

LReLU

(xln+1,1)

LReLU-1

A+ (x-b)
l,1

~h[::-1] 1/2

(xnl )

Figure 1: Decomposition and recomposition principle for depth-wise separable convolutions. We denote the Moore-Penrose inverse of the full column rank matrix A by A+.

Proof 1.5 Let us recall again that injectivity is stable by composition of operators. It is also clear that non-linear framing conditions remain true as composite bi-Lipschitz functions. 

With the proposition above it is now trivial to prove the theorem below.

Theorem 1.2 Compositions of FDFC and dilated RConv layers are non-linear frame: If F is a frame of L2(R) then the composite function from L2di (R) onto ld2o (Z)
F DF CAL,bL  RConvL,hL,gL      F DF CA1,b1  RConv1,h1,g1  F is a non-linear frame.

Now, we expose the framing properties of RNNs (for an introduction on RNNs we refer the reader
to (Bengio et al., 1994)). For the vast majority of popular recurrent architectures (for instance, LSTMs,
GRUs (Hochreiter & Schmidhuber, 1997; Cho et al., 2014)) the use of bounded output layers leads to
saturation and vanishing gradients. With such vanishing gradients, it is possible to find series of input sequences that diverge in l2(Z) while their outputs through the RNN are a Cauchy sequence.

Proposition 1.4 Saturating RNNs do not provide non-linear frames: Let us consider a RNN F(S),()  f  L2(R)  (n(< f, Sn >))n  l2(Z)
where F(S) is a given linear frame of L2(R). If there exists a sequence (vk)kZ  F(S)(L2(R))Z such that ((n(vk))nZ)kN is a Cauchy sequence in l2(Z) while vk 2  +, then F(S),() is not a linear frame.

Proof 1.6 With the assumption on (vk)kZ  F(S)(L2(R))Z, there cannot exist A > 0 such that

k0, k1  Z, A

vk0 - vk1

2 2



(n(< vk0 , Sn >))nZ - (n(< vk1 , Sn >))nZ

2 2

as the sequence (vk) would then be Cauchy and therefore converge as l2(Z) is complete for the l2 norm. 

Such a proposition highlights a key difference between the representational ability of RNNs and CNNs. We explore representations of irregularly sampled data through the lens of non-linear frames.

1.3 IRREGULAR SAMPLING
We now show that even when signals are irregularly observed by a random sampling process, that particular neural networks can still, almost surely produce a homeomorphic representation.
Sampling by Hawkes processes is a common assumption in finance, seismology, and social media analytics (Ogata, 1988; Bacry et al., 2015; Belletti et al., 2017; Yang & Zha, 2013; Li & Zha, 2013; Zhao et al., 2015). We use (ItN ) to denote the canonical filtration associated with the stochastic process (Nt)tR. We recommend (Daley & Vere-Jones, 2007) for a more thorough introduction to this concept. As a simplification, we denote ItN to be the information generated by (Ns)s<t.
Definition 1.7 Random sampling Hawkes process: A stochastic point process (Nt) (Daley & VereJones, 2007) defines a random measure over the axis of time and is defined by stochastic local Poisson

6

Under review as a conference paper at ICLR 2018

intensities for (Nt)

E t = limdt0+

N (t + dt) - N (t) ItN dt

.

For a Hawkes process characterized by   t  R  (t)  0, t < 0, (t) = 0, µ  0, we assume

t = µ +   dNt.

(2)

In other words, t is the number of observations per unit of time expected given the events that occurred until time t. Intuitively, if t is higher, then it is more likely for observations to be available shortly after the time t.
As in (Belletti et al., 2017; Bacry et al., 2015), Hawkes processes can be used to model the random observation time of a continuous-time series in a setting where in,formation is observed asynchronously in an event-driven manner across multiple channels (the extension to multi-variate point processes is immediate).

Proposition 1.5 Sampling density of stable Hawkes processes: If the Hawkes process (2) is stable (i.e. tR (t)dt < 1), then almost surely

tlim+

Nt t

=

µ .
1 - tR (t)dt

A complete proof of the ergodic behavior of stable Hawkes processes is provided in (Daley & Vere-Jones, 2007). Now, given an asymptotic Nyquist sampling rate for a random sampling scheme, the following lemma delineates which frames can still be used for signal recovery. In particular, we can no longer recover all signals in an unambiguous manner. Hence, exact recovery is only possible for band-limited functions (i.e. functions whose Fourier transform has bounded support).

Theorem 1.3 Irregular sampling frames: If a sequence of time-stamps have the property that

nlim± tn

=

±

and

nlim+

tn n

>

2R,

then (e-2itn) is a frame F of L2[-R, R] (where the real axis represents sampling frequencies) with left inverse F +. Considering R1 > R and S  L2(R) such that

F T [S]  < +, support(F T [S])  [-R1, R1] and   [-R, R], F T [S]() = 1, (3)

then f  L2(R) s.t. support(F T [f ])  [-R, R], f =
where xn(f ) = R=1-R1 F +(F T [f ]1[-R1,R1])()e2itnd.

xn(f )S( - tn)

Complete proof is given in (Benedetto & Heller, 1990; Benedetto, 1992); the theorem is regarded as the fundamental theorem of frame analysis for irregularly observed signals. We now leverage the fundamental properties that were obtained in the deterministic setting and extend them to provide guarantees under random sampling schemes.

Proposition 1.6 Under Hawkes process random sampling, framing is preserved almost surely: Let (tn)nZ be a family of sampling time-stamps generated by a stable Hawkes process whose intensity follows the dynamics described in (2), denote
R= 1 µ , 2 1 - tR (t)dt
and let S be a frame operator abiding by conditions (3), then almost surely the frame is injective when translated by the irregular random time-stamps (tn)nZ.
Proof 1.7 The proposition is a direct consequence of Prop. 1.5 and Theorem 1.3. 

7

Under review as a conference paper at ICLR 2018

Theorem 1.4 Recovery of randomly observed band-limited signals: Let (tn)nZ be a family of

sampling time-stamps generated by a stable Hawkes process whose intensity follows the dynamics

described

in

(2)

Consider

R

=

1 2

µ 1-tR (t)dt

,

let

F(S)

be

a

frame

operator

with

atoms

(S( - tn))nZ

given by (3). A composite function from L2di (R) onto ld2o (Z)

F DF CAL,bL  RConvL,hL,gL      F DF CA1,b1  RConv1,h1,g1  F(S(-tn))nZ

is almost surely a non-linear frame over the set of functions in L2(R) whose Fourier Transform has its support included in [-R, R]. In particular such a representation is invertible on its image by a
Lipschtiz inverse.

Proof 1.8 Previously, we proved Theorem 1.2 on the preservation of framing properties by composition with FDFC and RConv layers. In Prop. 1.6 we proved that F(S(-tn))nZ is almost surely a frame of the subset of L2(R) of functions with band-limit [-R, R]. 

One concern, however, is that the theorems we developed assumed observations on the entire real axis
are available as well as infinite representations indexed by Z. In particular, a theory of framing for
band-limited functions is useful but only applies to periodic functions. Bounded support function of L2(R) are part of the many examples that are not band-limited Mallat (2008); Benedetto & Ferreira
(2003).

1.4 FINITE APPROXIMATIONS OF NON BAND-LIMITED SIGNALS

In our objective to develop theoretical statements that can be leveraged in practice (i.e., when computing with finite time and memory), we must now extend our analysis to (1) functions observed on compact intervals and (2) finite approximations of signals. The following statements show how the requirements of Lipschitz-ness in non-linear frames provide guarantees on the impact of approximation errors associated with finite representation of continuous-time signals.

The theorems above can be employed for irregularly observed functions that are periodic and band-limited (Benedetto & Ferreira, 2003; Benedetto, 1992; Mallat, 2008). However, since we hope to develop a representational theory that is applicable to non-stationary signals, we must also consider non-periodic functions. In the appendix, we show how wavelet decomposition can efficiently approximate certain classes of functions that are smooth and not band-limited.

WasitPhrojlWologg22((NN ))(fsc)atlhese

of decomposition and O(N ) Wavelet approximation error

scalars representing the approximation is O(2-N) in L2 norm for the space

of of

f 

Lipstichz functions (see Definition 4.1 in appendix). As we employ functions that are BLI the impact

of the approximation error remains controlled.

Proposition 1.7 Approximate representation: Let L 1 be a BLI function from l2(Z) onto l2(Z). Let ProjW, l be the projection operator from L2([0, 1]) onto a Wavelet basis of l scales,

L  1  PWlog2(N)(f ) - L  1P+W(f )

2 2

= O(2-N2).

In other words, the numerical representation can be arbitrarily close to the true representation of
smooth, continuous-time functions with compact support. Indeed, if W is a wavelet basis, then Proj+W(f ) = f . The argument stresses the critical role of our assumption of the Lipschitz-ness of frames and the BLI functions which guarantees that representations based on approximations can be
arbitrarily accurate.

2 ORTHOGONAL MULTI-RESOLUTION CONVOLUTIONS IN TIME
So far, we have focused on sufficient conditions to make accurate representation of continuous-time signals possible as they are observed randomly and as the corresponding observation are processed non-linearly. We now show that additional conditions on time-domain convolutional filter banks further guarantee that the representation is minimal (i.e., produces orthogonal outputs).

8

Under review as a conference paper at ICLR 2018

2.1 MULTI-RESOLUTION REPRESENTATION
As our goal is to obtain different representations of a time series while avoiding redundancy, let us introduce multi-resolution approximations (Mallat, 2008).
Definition 2.1 Multi-resolution approximation: A family (Hl)lZ of closed sub-spaces of L2(R) is a multi-resolution approximation if the family is nested (l  Z, Hl  Hl+1); dense (lZHl = L2(R)); separated (lZHl = {0}); causal (l  Z, g()  Hl  g( 2)  Hl+1); stable by translation ((t, l)  Z, g()  Hl  g( - 2lt)  Hl). In addition we require that (H0) there exists an orthonormal family (S( - n))nZ such that span((S()nZ)) = Hl, i.e. S( - n) is a Riesz basis of H0 with scaling function S.
Such Riesz basis is proven in (Mallat, 2008); the family of Haar wavelets is merely an example. General conditions for a function S  L2(R) to be a scaling function are given by the following theorem.

Theorem 2.1 Conjugate mirror temporal convolution layer (CMConv): (Mallat, 1989) Let S and W in l2(Z) be two filters convolution filters such that



F T [S](0) = 2,

(4)

  [-1 2, 1 2], F T [S]() 2 + F T [S]( + 1 2) 2 = 2, W n = (-1)1-nS -n.

(5) (6)

We

further

assume

that

inf

f

[-

1 4

,

1 4

]

F T [S](f )

>

0.

The

inverse

Fourier

Transform

of





p+=1

F T [S](2-p) 2

is

a

scaling

function

S

of

L2(R)

for

a

multi-resolution

approximation.

Moreover,

the

Wavelet

function

W

defined

as

the

inverse

Fourier

transform

of

1 2

F

T

[W

]F

T

[S

]

is

such

that

for any scale l

Wl,n

=

1 W
2l

 -2ln 2l

n  Z is an orthonormal basis Wl defined as the orthogonal

complement of Hl in Hl+1. In particular, (Wl,n)lZ,nZ is an orthonormal basis of L2(R).

2.2 DEPTH-WISE SEPARABLE CONVOLUTIONS AND MULTI-RESOLUTION APPROXIMATIONS

We now show how depth-wise separable convolutions with scaling and wavelet filters quickly come with guarantees of orthogonality.

In the following we consider an input space with d input channels and a series of affine operators with increasing output dimensions (dl)Ll=1. We denote FS(-tn)nZ by F to simplify notations.

Theorem 2.2 Conjugate mirror convolutions and FDFC: Consider two convolution filters S

and

W ,

if

S , W

respect

the

conditions

Eq.

(4)

and

inf

[-

1 4

,

1 4

]

F T [S]()

>

0,

then

S , W

constitute a pair of RConv filters. Consider the function which to f  L2d(R) associates (l(f ))l=1...L:

f  (< f, S(  - n) >)nZ (W)[2]

BLI

. . . . . . BLI (1(f )n)nZ


(S )[2]

(W )[2]

BLI

. . . BLI (2(f )n)nZ

 ...



(S )[2]



BLI (L(f )n)nZ

The representation (l(f ))l=1...L is a non-linear frame that only depends on the coordinates of f in an orthonormal basis of L2(R).

Proof 2.1 We start the proof by showing that (xn)  l2(Z)  ([(W  x)2n, (S  x)2n])  l2(Z) is a RConv reconstructible convolution layer. Based on Eq. (4), as W n = (-1)1-nS-n we have

F

T

[W

]()

=

ei F

T

[S ](

+

1 2

)

and

then

as





[-1

2, 1

2],

F T [S]() 2 +

F T [S]( +

9

Under review as a conference paper at ICLR 2018

1 2) 2 = 2 the first part of the proof is concluded. The second part of the proof utilizes

the fact that the cascading convolutions above compute a Discrete Wavelet Transform Mallat

(2008). Therefore, (1(f )n)nZ = (1(< f, W1,n >)n)nZ, (2(f )n)nZ = (2(< f, W2,n >

)n)nZ, . . . , (L-1(f )n)nZ = (L-1(< f, WL-1,n >)n)nZ and (1(f )n)nZ = (L(< f, SL-1,n >

)n)nZ where

Sl,n

=

1 S
2l

-2l n 2l

nZ .

The cascaded time domain convolutions being computed yield the coordinates of f in an orthonormal basis. Therefore, as the orthogonal CNN grows deeper it can only yield novel orthogonal information about the input signal that is informative of its properties on a particular temporal scale. Such is the nature of our efficiency claim for the neural networks we consider.

2.3 TRAINING OF RCONV LAYERS BY ALTERNATING SGD STEPS AND DC PROGRAMMING:

As

noted

in

Mallat

(2008)

the

constraint

inf

[-

1 4

,

1 4

]

,

F T [S]()

>

0.

is

always

met

in

practice,

which our numerical experiments confirm. The two convolution filters is (1) that W = (-1)1-nS[

critical constraints on -1] and F T [S](0)

thedesign of the = 2, which is

temporal trivial to

enforce, and (2) that   R,

F T [S]() 2 +

F

T

[S ](

+

1 2

)

2

=

2.

In our implementation we approximate the constraint by computing the Fast Fourier Transform of the filter, since it is defined discretely in time by a finite set of weights. Therefore, we interleave the normal training step of S with solving the following following minimization problem

1

min

S RH

=0,

1 N -1

F T [S]() 2

+

F T [S](

+

1) 2 2

-2

where H is the number of free parameters we allow in our temporal convolution filter. Such an

optimization problem can be rewritten as a difference of convex (DC) functions (as x = max(0, x) -

min(0, x), s



F T [S]() 2 +

F T [S]( +

1 2

)

2

-

2

is

clearly

convex

and

convexity

is

stable

by

composition by a non-decreasing function) and an adapted solver (Tao & An, 1998) can then take

advantage of the particular structure of the problem to find an optimal solution rapidly.

3 NUMERICAL EXPERIMENTS
Here, we show that the sufficient conditions for neural networks to yield non-linear frames are computationally tractable. The following experiments explore the empirical properties of such neural networks compared to various baselines.
3.1 SYNTHETIC COMPRESSION EXPERIMENT
In our first numerical experiment, we generate regularly sampled non-stationary stochastic processes, characterized by a random mixture of Gabor functions (Mallat, 2008) and step functions. As shown in Figure 2, the resulting signals are highly irregular, lack permanent seasonality, and have compact support. The objective here is to devise a procedure to train conjugate mirror (convolutional) filters with stochastic optimization methods to progressively improve representational power.
We train a 16 parameter filter S to optimally conduct the following compression (i.e., auto-encoding) task. The pair of filters specified in Eq. (4) are employed as in Theorem 2.2 to produce the coordinates of the input signal in the wavelet basis corresponding to the (learned) filters S and W . The input signals are uni-variate with 128 observations each.The encoding, therefore, initially consists of 128 scalar values, of which, only the 64 with higher magnitude are selected ­ all other values are set to 0.
An inverse Discrete Wavelet Transform is then employed to reconstruct the input signal. The quality of this reconstruction is measured by the squared L2 loss, which penalizes discrepancies between the input signal and its reconstruction. To train this model, we use a stochastic optimization algorithm, RMSProp, to minimize the aforementioned loss. We train for 2,500 iterations with a learning rate of 10-3. This optimization is interleaved with a constraint enforcing program that enforces Eq. (4) every 100 iterations. Figure 2 shows that this procedure progressively improves the randomly-initialized filters and significantly out-performs an LSTM-based auto-encoder model.

10

Under review as a conference paper at ICLR 2018

Squared L2 loss Value Value Squared L2 loss
Value Value Square L2 loss

Original signal
2 1 0 1

2

0

20

40

6T0 ime

80

Original signal

3

2

1

0

1

2

0

20

40

6T0 ime

80

LSTM encoding compression loss
25000 20000 15000 10000
0 500 Tr1a00i0ning st1e50p0 2000 2500

LSTM compressed signal
1.0 0.5 0.0 0.5

100 120

Time0 20 40 60 80 100 120

100 120

3 2 1 0 1 20

Learned Wavelet basis compressed signal Time20 40 60 80 100 120

Wavelet encoding compression loss
15000 10000

Violation of conjugacy constraint
1.5 1.0

5000 0.5

0

0

500 T1r0a00ining s15te00p 2000

2500

0.0 0

500 T1r0a00ining s1t5e00p 2000

2500

Figure 2: For a ratio of compression of 2, the learned wavelet basis auto-encoder outperforms an LSTM-based auto-encoder with the same number of parameters.

Model

Hit@1 Precision at equal recall

Baseline LSTM 0.645

0.573

LSTM on 4 Wavelets 0.798

0.648

Figure 3: Top: An LSTM model trained on a wavelet-transformed YouTube-8M data-set achieves comparable results against the baseline while using half the number of parameters. Bottom: Table evaluating the wavelet-transformed LSTM model against the results from Abu-El-Haija et al. (2016) on held-out data.

3.2 MULTI-SCALE VIDEO CLASSIFICATION
We further show that a wavelet representation can be composed with classical recurrent architecture (in regularly observed settings) to mitigate the effect of noisy data. This is particularly useful for LSTM networks Hochreiter & Schmidhuber (1997), since hyperbolic tangent layers tend to saturate in the the presence of high-magnitude perturbations.
The YouTube-8M data-set contains millions of YouTube videos and their associated genres (AbuEl-Haija et al., 2016). Because the frames in each video are pre-featurized (i.e., a time series of featurized frames), models designed for this data-set must solely leverage the temporal structure in the data. This has enabled the authors of the paper to achieve state-of-the-art results in video classifications using a 2-layer LSTM model.
In our experiment, we train a similar model to learn on a multi-scale wavelet representation of data. This representation separates the original time series into d scales, varying from fine to coarse. Each of the d time series in this multi-scale representation are fed into a similar 2-layer LSTMs with d2 times fewer parameters. The outputs of each LSTM, are then concatenated before the final soft-max layer. We provide a model diagram detailing these components in the appendix. Our experimental results in Figure 3 indicate that this multi-scale representation greatly improves the performance of recurrent neural networks while using far fewer parameters.
11

Under review as a conference paper at ICLR 2018

Number of shares exchanged L2 loss relative to baseline L1 loss relative to baseline

5000 AAPL traded volume 20151030

4000

3000

2000

1000

0 11:00:04

11:O00:0b9 se1r1v:0a0:1t4ion 1t1:i0m0:1e9 sta11m:00:p24

11:00:29

0.984 Trade volume prediction

0.982

0.980

0.978

0.976

0.974 Standard LSTM

0.972

Wavelet Net transform width 2 Wavelet Net transform width 4

0.970
0 10000 20000 T3r0a0i0n0 in4g00 0it0era50t0i0o0n 60000 70000 80000

0.902 Trade volume prediction

0.900

0.898

0.896 0.894 0.892
0

Standard LSTM Wavelet Net transform width 2 Wavelet Net transform width 4
10000 20000 T3r0a0i0n0 in4g00 0it0era50t0i0o0n 60000 70000 80000

Figure 4: Predicting trade volume of a 15 minute window given the previous 15 minutes of observa-
tions. We evaluate the performance of neural networks against a simple averaging model. Even with
L1 regularization, the LSTM does not have the same predictive power as the neural network specified in Theorem 2.2 with L = 2 or L = 4.

3.3 MULTI-SCALE FINANCIAL FORECASTING
In 2015, an astounding medium volume of 40 million shares of AAPL (Apple Inc.) were traded each day. With the price of each share at approximately 100 USD, each 15-minute trading period represents an exchange of 142 million USD. Trades are highly irregular events characterized by an instantaneous exchange of shares between actors. Forecasting trade volume at a very fine resolution is essential in leveraging arbitrage opportunities. However, the noisy nature of financial markets makes this task incredibly challenging (Abergel et al., 2012).
Using AAPL price data from 2015, we train two neural networks: a standard LSTM and a wavelet transform network, to predict the next 15 minutes of trading given the 15 minutes that have just elapsed. On average, the duration between time-stamps was 907ms (25th percentile: 200ms, median: 220ms, 75th percentile: 1800ms).
After the first scale projection onto a Haar wavelet basis (Mallat, 2008) is produced (with a characteristic resolution  = 8 seconds), both the wavelet transform network (with M = 8) and the LSTM make predictions with this first scale as input. Each model is evaluated by the L2 loss against a baseline predicting a constant trading volume equal to the average over the previous 15 observed minutes. Notice that in Figure 4, the LSTM struggles with the noisiness of the data, whereas the wavelet transform network is robust, and manages to improve the prediction performance by a half-percent. This half-percent represents 50 thousand USD of exchanged volume over a 15 minute period.

4 CONCLUSION
In this article, we analyze neural networks from a frame theoretic perspective. In doing so, we come to the conclusion that by considering time series as an irregularly observed continuous-time stochastic processes, we are better able to devise robust and efficient convolutional neural networks. By leveraging recent contributions to frame theory, we prove properties about non-linear frames that allow us to make guarantees over an entire class of convolutional neural networks. Particularly regarding their capacity to produce discrete representations of continuous time signals that are both injective and bi-Lipschitz. Moreover, we show that, under certain conditions, these properties almost certainly hold, even when the signal is irregularly observed in an event-driven manner. Finally, we show that bounded-output recurrent neural networks do not satisfy the sufficient conditions to yield non-linear frames.
This article is not limited to the theoretical statements it makes. In particular, we show that we can to build a convolutional neural network that effectively computes a Discrete Wavelet Transform. The network's filters are dynamically learned while being constrained to produce outputs that preserve both orthogonality and the properties associated with non-linear frames. Our numerical experiments on real-world prediction tasks further demonstrate the benefits of such neural networks. Notably, their ability to produce compact representations that allow for efficient learning on latent continuous-time stochastic processes.

12

Under review as a conference paper at ICLR 2018
REFERENCES
Frédéric Abergel, Jean-Philippe Bouchaud, Thierry Foucault, Charles-Albert Lehalle, and Mathieu Rosenbaum. Market microstructure: confronting many viewpoints. John Wiley & Sons, 2012.
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.
Emmanuel Bacry, Iacopo Mastromatteo, and Jean-François Muzy. Hawkes processes in finance. Market Microstructure and Liquidity, 1(01):1550005, 2015.
Bhavik R Bakshi and George Stephanopoulos. Wave-net: A multiresolution, hierarchical neural network with localized learning. AIChE Journal, 39(1):57­81, 1993.
Francois Belletti, Evan Sparks, Alexandre Bayen, and Joseph Gonzalez. Random projection design for scalable implicit smoothing of randomly observed stochastic processes. In Artificial Intelligence and Statistics, pp. 700­708, 2017.
John J Benedetto. Irregular sampling and frames. wavelets: A Tutorial in Theory and Applications, 2: 445­507, 1992.
John J Benedetto and Paulo JSG Ferreira. Modern sampling theory. Sankhya: The Indian Journal of Statistics, 65(Part 4):849­851, 2003.
John J Benedetto and William Heller. Irregular sampling and the theory of frames, i. Note di Matematica, 10(suppl. 1):103­125, 1990.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157­166, 1994.
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
François Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure. Springer Science & Business Media, 2007.
Hans G Feichtinger and Karlheinz Gröchenig. Theory and practice of irregular sampling. Wavelets: mathematics and applications, 1994:305­363, 1994.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645­6649. IEEE, 2013.
Karlheinz Gröchenig. Reconstruction algorithms in irregular sampling. Mathematics of computation, 59(199):181­194, 1992.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Nicolas Huth and Frédéric Abergel. High frequency lead/lag relationships--empirical facts. Journal of Empirical Finance, 26:41­58, 2014.
Lukasz Kaiser, Aidan N Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. arXiv preprint arXiv:1706.03059, 2017.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014.
13

Under review as a conference paper at ICLR 2018
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.
Liangda Li and Hongyuan Zha. Dyadic event attribution in social networks with mixtures of hawkes processes. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pp. 1667­1672. ACM, 2013.
Stephane Mallat. A wavelet tour of signal processing: the sparse way. Academic press, 2008. Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation.
IEEE transactions on pattern analysis and machine intelligence, 11(7):674­693, 1989. Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network
training for long or event-based sequences. In Advances in Neural Information Processing Systems, pp. 3882­3890, 2016. Yosihiko Ogata. Statistical models for earthquake occurrences and residual analysis for point processes. Journal of the American Statistical association, 83(401):9­27, 1988. Qiyu Sun and Wai-Shing Tang. Nonlinear frames and sparse reconstructions in banach spaces. Journal of Fourier Analysis and Applications, 23(5):1118­1152, 2017. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014. Pham Dinh Tao and Le Thi Hoai An. A dc optimization algorithm for solving the trust-region subproblem. SIAM Journal on Optimization, 8(2):476­505, 1998. Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR abs/1609.03499, 2016. Shuang-Hong Yang and Hongyuan Zha. Mixture of mutually exciting processes for viral diffusion. In International Conference on Machine Learning, pp. 1­9, 2013. Qingyuan Zhao, Murat A Erdogdu, Hera Y He, Anand Rajaraman, and Jure Leskovec. Seismic: A self-exciting point process model for predicting tweet popularity. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1513­1522. ACM, 2015.
14

Under review as a conference paper at ICLR 2018

CONVERGENCE OF WAVELET APPROXIMATIONS IN HOLDER FUNCTIONAL
SPACES

We rely on Wavelet approximations (Mallat, 2008) to represent signals that are not band-limited.

Given a certain Wavelet function W



L2(R) and Wl,

=

1 2l

W

(

- 2l

)

a

series

of

dilated

and

translated versions of W , a Wavelet approximation expresses a function f  L2(R) as

L-1

f  PWL (f )

< f, Wl, > Wl, .

l=0  Z

Under

some

conditions

on

W

(Mallat,

2008),

the

family

Wl,

=

1 2l

W

(

- 2l

)

can

be

orthonormal

and every function f  L2(R) can be written in the limit as f = lZ Z < f, Wl, > Wl, . A

Wavelet function is defined as the high frequency mirror of a low frequency Scale function whose

unit translations constitute a set of orthonormal atoms for a frame of L2(R) (i.e. a Riesz basis of

L2(R)).

In the following we consider functions with bounded support and restrict our study to functions defined on the interval [0, 1] to simplify notations. A change of variable can immediately be employed to generalize the statements below to any bounded support function.

Definition 4.1 Holder space of  Lipschitz functions: A function f  L2([0, 1]) is uniformly  Lipschitz if there exists M > 0 such that for all s in [0, 1] there exists a polynomial ps of degree  such that
t  [0, 1], f (t) - pv(s)  K t - s .

In other words we consider functions defined on compacts that can be well approximated by polynomial splines and therefore have a certain degree of smoothness.

Proposition 4.1 Wavelet approximation of smooth functions with compact support: If f  L2([0, 1]) is  Lipschitz there exist 0 < m  M such that the approximation error of the wavelet decomposition with wavelet function W and scale function S is

log2(N ) 2l-1

log2(N )-1



f-  l=L+1

< f, Wl, > Wl, +
 =0

 =0

< f, Slog2(N), > Slog2(N), 

2
= O(2-N2)
2

The proposition above, proven in (Mallat, 2008) helps us examine how such an approximation affects the representations we employ.

15

Under review as a conference paper at ICLR 2018

NEURAL ARCHITECTURES EMPLOYED IN THE VIDEO CLASSIFICATION
EXPERIMENT
The present section of the appendix describes in detail the architectures employed in our Youtube8 video classification experiments. In Figure 5 we present the baseline architecture in the form of a block-diagram. In Figure 6 we expose our architectural choices which split the input into independent sub-spaces of representation that all correspond to different characteristic scales of variation of the input signal.
Label

Softmax layer

1024 features
1024 features

Weighted sum along time axis (linearly increasing from 1/T to T)
2 layer stacked LSTM with 1024 hidden units in each layer
Channel wise time domain dyadic wavelet transform (cascaded dilated convolutions by Daubechies 4 Scale and Wavelet filters)

1024 features
Pre-trained Inception network

Input data
Computed by Abu-El-Haija et al. (2016)

Sequence of images

t

Figure 5: Baseline architecture from (Abu-El-Haija et al., 2016).

16

Under review as a conference paper at ICLR 2018

1024 features
Concatenate in depth
4 x 256 features
4 x (1024 to 256 to 256) independent 2 layer LSTMs
4 x 1024 features

Label Softmax layer Weighted sum along time axis (linearly increasing from 1/T to T)
2 layer stacked LSTM with 256 hidden units in each layer 2 layer stacked LSTM with 1024 hidden units in each layer 2 layer stacked LSTM with 1024 hidden units in each layer 2 layer stacked LSTM with 1024 hidden units in each layer
Channel wise time domain dyadic wavelet transform (cascaded dilated convolutions by Daubechies 4 Scale and Wavelet filters)

1024 features
Pre-trained Inception network

Scale 3 averages Scale 3
details Scale 2 details Scale 1 details
Input data
Computed by Abu-El-Haija et al. (2016)

Sequence of images

t

Figure 6: The architecture we propose for the Youtube video classification task that leverages a multi-resolution approximation computed by a wavelet convolution stack.

17

