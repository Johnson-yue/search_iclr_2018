Under review as a conference paper at ICLR 2018
LEARNING APPROXIMATE CLOSED-LOOP POLICIES FOR MARKOV POTENTIAL GAMES
Anonymous authors Paper under double-blind review
ABSTRACT
Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We consider a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications, where the agents share or compete for some common resource, the state-action sets are continuous, rewards might be nonconvex functions, and there might be coupled constraints. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided open-loop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider policies that depend on the current state and where agents adapt to stochastic transitions. Following state-of-the-art results for single-agent problems obtained with deepreinforcement-learning, we consider the agents' policies belong to some parametric class (e.g., deep neural networks). We provide sufficient and necessary, easily verifiable conditions for a stochastic game to be an MPG, and show that a closedloop Nash equilibrium can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP--which is a single-objective problem--is usually much simpler than solving the original set of coupled OCPs that form the game--which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no Nash equilibrium belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning (DRL) algorithm and learn a set of policies (one per agent) that closely approximates an exact variational Nash equilibrium of the game.
1 INTRODUCTION
In a noncooperative stochastic dynamic game, the agents compete in a time-varying environment, which is characterized by a discrete-time dynamical system equipped with a set of states and a state-transition probability distribution. Each agent has an instantaneous reward function, which can be stochastic and depends on agents' actions and current system state. We consider that both the state and action sets are subsets of real vector spaces and subject to coupled constraints, as usually required by engineering applications.
A dynamic game starts at some initial state. Then, the agents take some action and the game moves to another state and gives some reward values to the agents. This process is repeated at every time step over a (possibly) infinite time horizon. The aim of each agent is to find the policy that maximizes its expected long term return given other agents' policies. Thus, a game can be represented as a set of coupled optimal-control-problems (OCPs), which are difficult to solve in general.
OCPs are usually analyzed for two cases namely open loop (OL) or closed loop (CL), depending on the information that is available to the agents when making their decisions. In the OL analysis, the action is a function of time, so that we find an optimal sequence of actions that will be executed in
1

Under review as a conference paper at ICLR 2018
order, without feedback after any action. In the CL setting, the action is a mapping from the state, usually referred as feedback policy or simply policy, so the agent can adapt its actions based on feedback from the environment (the state transition) at every time step. For deterministic systems, both OL and CL solutions can be optimal and coincide in value. But for stochastic system, an OL strategy consisting in a precomputed sequence of actions cannot adapt to the stochastic dynamics so that it is unlikely to be optimal. Thus, CL are usually preferred over OL solutions.
For dynamic games, the situation is more involved than for OCPs, see, e.g., Basar and Olsder (1999). In an OL dynamic game, agents' actions are functions of time, so that an OL equilibrium can be visualized as a set of state-action trajectories. In a CL dynamic game, agents' actions depend on the current state variable, so that, at every time step, they have to consider how their opponents would react to deviations from the equilibrium trajectory that they have followed so far, i.e., a CL equilibrium might be visualized as a set of trees of state-action trajectories. The sets of OL and CL equilibria are generally different even for deterministic dynamic games (Kydland, 1975; Fudenberg and Levine, 1988).
There is a class of games, named Markov potential games (MPGs), for which the OL analysis shows that NE can be found by solving a related single OCPs (see González-Sánchez and Hernández-Lerma (2013); Zazo et al. (2016) for recent surveys on MPGs). Thus, the benefit of MPGs is that solving a single OCP is generally simpler than solving a set of coupled OCPs. Nevertheless, none previous study has provided a practical method for finding CL solutions.
In this work, we tackle the CL analysis of MPGs. We assume that the agents' policies lie in a parametric set. This assumption makes derivations simpler, allowing us to prove that, under some potentiality conditions on the reward functions, a game is an MPG. Therefore, similar to the OL case, we show that the Nash equilibrium (NE) for the approximate game can be found as an optimal policy of a related OCP. This is a practical approach for approximating NE, since even when this assumption does not hold, if the parametric family is expressive enough to represent the complexities of the problem under study, we can expect than (under some continuity assumptions) small deviations in the parametric policies should translate to small perturbations in the value functions, so that the parametric solution will approximate an equilibrium of the original MPG well. In addition, we remark that this parametric policy assumption has been widely used for learning the solution of single-agent OCPs with continuous state-action sets (see, e.g., Konda and Tsitsiklis (2003); Melo and Lopes (2008); Powell and Ma (2011); Van Hasselt (2012); Lillicrap et al. (2015); Heess et al. (2015); Schulman et al. (2015)). Here, we show that the same idea can be extended to MPGs in a principled manner.
Moreover, once we have formulated the related OCP, we can apply reinforcement learning techniques to find an optimal solution. Some recent works have applied deep reinforcement learning (DRL) to cooperative stochastic games (Foerster et al., 2017; Sunehag et al., 2017), where all agents share a common objective, which is a particular case of MPGs. In this work, we consider a practical approach that applies to both cooperative and noncooperative MPGs.
Summary of contributions. We provide sufficient and necessary conditions on the agents' reward function for a stochastic game to be an MPG. Then, we show that a closed-loop Nash equilibrium can be found (or at least approximated) by solving a related optimal control problem (OCP) that is similar to the MPG but with a single-objective reward function. We provide two ways to obtain the reward function of this OCP: i) computing the line integral of a vector field composed of the partial derivatives of the agents' reward, which is theoretically appealing since it has the form of a potential function but difficult to obtain for complex parametric policies; ii) and as a separable term in the agents' reward function, which can be obtained easily by inspection for any arbitrary parametric policy. We illustrate the proposed approach by applying DRL to a noncoooperative Markov game that models a communications engineering application (in addition, we illustrate the differences with the previous standard approach by solving a classic resource sharing game analytically in the appendix).
2 PROBLEM SETTING FOR CLOSED LOOP MPG
Let N {1, . . . , N } denote the set of agents. Let ak,i be the real vector of length Ak that represents the action taken by agent k  N at time i, where Ak  RAk is the set of actions of agent k  N . Let A kN Ak denote the set of actions of all agents that is the Cartesian product of every agent's
2

Under review as a conference paper at ICLR 2018

action space, such that A  RA, where A = kN Ak. The vector that contains the actions of all agents at time i is denoted ai  A. Let X  RS denote the set of states of the game, such that xi is a real vector of length S that represents the state of the game at time i, with components xi(s):

xi (xi(s))Ss=1  X.

(1)

Note that the dimensionality of the state set can be different from the number of agents (i.e., S = N ).

State transitions are determined by a probability distribution over the future state, conditioned on the

current state-action pair: xi+1  px(·|xi, ai); where we use boldface notation for denoting random variables. State transitions can be equivalently expressed as a function, f : X × A ×   X, that

depends on some random variable i  , with distribution p(·|xi, ai), such that

xi+1 = f (xi, ai, i).

(2)

We include a vector of C constraint functions, g (gc)cC=1, where gc : X × A  R; and define the constraint sets for i = 0: C0 A  {a0 : g(x0, a0)  0}; and for i = 0, . . . , : Ci {X  {xi :
xi = f (xi-1, ai-1, i-1)}} × A  {(xi, ai) : g(xi, ai)  0}, which determine the feasible states
and actions. The instantaneous reward of each agent, rk,i, is also a random variable conditioned on the current state-action pair: rk,i  pri (·|xi, ai). Given random variable k,i  k with distribution pk (·|xi, ai), we define reward function rk : X × A × k  R for every agent k  N :

rk,i = rk(xi, ai, k,i).

(3)

We assume that i and k,i are independent of each other and of any other j and k,j, at every time step j = i, given xi and ai.

Let k : X  Ak and  : X  A denote the policy for agent k and all agents, respectively, such that:

ak,i = k(xi), ai = (xi), and  (k)kN .

(4)

Let k and  = kN k denote the policy spaces for agent k and for all agents, respectively, such that k  k and   . Note that (X) = A. Introduce also -k : X  A-k as the policy of all
agents except that of agent k. Then, by slightly abusing notation, we write:  = (k, -k), k  N .

The general (i.e., nonparametric) stochastic game with Markov dynamics consists in a multiobjective variational problem with design space  and objective space RN , where each agent aims to find a

stationary policy that maximizes its expected discounted cumulative reward:



G1 : k  N

maximize
k k
s.t.

E irk (xi, k(xi), -k(xi), i)
i=0
xi+1 = f (xi, (xi), i),

(5)

g(xi, (xi))  0.

Similar to static games, since there might not exist a policy that maximizes every agent's objective,

we will rely on Nash equilibrium (NE) as solution concept. But rather than trying to find a variational

NE solution for (5), we propose a more tractable approximate game by constraining the policies to

belong to some finite-dimensional parametric family.

Introduce the set of parametric policies, w, as a finite-dimensional function space with parameter

w  W  RW : w {(·, w) : w  W}. Note that for a given w, the parametric policy is still

a mapping from states to actions: (·, w) : X  A. Let wk  Wk  RWk denote the parameter

vector of length Wk for the parametrized policy k, so that it lies in the finite-dimensional space

kw {k(·, wk) : wk  Wk}, such that w

kN wk , W

kN Wk, W

kN Wk, and

w (wk)kN , (·, w) (k(·, wk))kN .

(6)

Let w-k denote the parameters of all agents except that of agent k, so that we can also write:

w = (wk, w-k) , (·, w) = (·, (wk, w-k)) = (k(·, wk), -k(·, w-k)) .

(7)

In addition, we use wk( ) to denote the -th component of wk, such that wk (wk( ))W=k1.

By constraining the policy of G1 to lie in kw, we obtain a multiobjective optimization problem with design space W:



maximize G2 : wkWk

k  N

s.t.

E irk (xi, k(xi, wk), -k(xi, w-k), k,i)
i=0
xi+1 = f (xi, (xi, (wk, w-k)), i),

(8)

g(xi, (xi, (wk, w-k)))  0.

3

Under review as a conference paper at ICLR 2018

The solution concept in which we are interested is the parametric closed loop Nash equilibrium (PCLNE), which consists in a parametric policy for which no agent has incentive to deviate unilaterally.

Definition 1 A parametric closed-loop Nash equilibrium (PCL-NE) of G2 is a vector w = wk, w-k  RW that satisfies:


E irk xi, k(xi, wk), -k(xi, w-k), k,i
i=0 
 E irk xi, k(xi, wk), -k(xi, w-k), k,i , k  N , x0 = x0  X,
i=0
wk  wk  Wk : ai = k(xi, wk), -k(xi, w-k)  C0, (xi, ai)  Ci, i = 1, . . . ,  .

(9)

Since G2 is similar to G1 but with an extra constraint on the policy set, loosely speaking, we can see a PCL-NE as a projection of some NE of G1 onto the manifold spanned by parametric family of choice. Hence, if the parametric family has arbitrary expressive capacity (e.g., a neural network with enough neurons in the hidden layers), we can expect that the resulting PCL-NE evaluated on G1 will approximate arbitrarily close the performance of an exact variational equilibrium.
We consider the following general assumptions.

Assumption 1 The state and parameter sets, X and W, are nonempty and convex.

Assumption 2 The reward functions rk are twice continuously differentiable in X × W, k  N .

Assumption 3 The state-transition function, f , and constraints, g, are continuously differentiable in X × W, and satisfy some regularity conditions (e.g., Mangasarian-Fromovitz).

Assumption 4 sets {a0  C0,

The reward (xi, ai)  Ci

f:unEc[trikon(sxir,kaai,rekp,ir)o]per,Ba}nid=t0hearree

exists a scalar nonempty and

B such that bounded k

the level  N.

Assumptions 1­2 usually hold in engineering applications. Assumption 3 ensures the existence of
feasible dual variables, which is required for establishing the optimality conditions. Assumption 4
will allow us to ensure the existence of PCL-NE. We say that rk is proper if: i) E [rk(xi, ai, k)] > - for at least one (xi, ai)  Ci, and ii) E [rk(xi, ai, k,i)] < , a0  C0, (xi, ai)  Ci (i = 1, . . . , ).

3 STANDARD APPROACH TO CLOSED-LOOP MARKOV GAMES

In this section, we review the standard approach for tackling CL dynamic games (González-Sánchez and Hernández-Lerma, 2013). For simplicity, we consider deterministic game and no constraints:

Gstd : k  N

maximize
k k
s.t.


irk (xi, k(xi), -k(xi))
i=0
xi+1 = f (xi, (xi)).

(10)

First, it inverts f to express the policy in reduced form, i.e., as a function of current and future states:

(xi) = h(xi, xi+1).

(11)

This implicitly assumes that such function h : X × X  A exists, which might not be the case if f is not invertible. Next, k is replaced with (11) in each rk:

rk (xi, k(xi), -k(xi)) = rk (xi, h(xi, xi+1)) rk (xi, xi+1) ,

(12)

where rk : X×X  R is the reward in reduced-form. Then, the Euler equation (EE) and transversality condition (TC) are obtained from rk for all k  N and used as necessary optimality conditions:

xi rk (xi-1, xi) + xi rk (xi, xi+1) = 0

lim
i

xi

xi rk (xi-1, xi)

=

0

(EE), (TC).

(13) (14)

4

Under review as a conference paper at ICLR 2018

When rk are concave for all agents, and X  R+ (i.e., X = {xi : xi  0, xi  RS}), these optimality conditions become sufficient for Nash equilibrium (González-Sánchez and Hernández-Lerma, 2013, Theorem 4.1). Thus, the standard approach consists in guessing parametric policies from the space of functions , and check whether any of these functions satisfies the optimality conditions. We illustrate this procedure with a well known resource-sharing game named "the great fish war" due to Levhari and Mirman (1980), with Example 1 in Appendix A.
Although the standard approach sketched above (see also Appendix A) has been the state-of-the-art for the analysis of CL dynamic games, it has some drawbacks: i) The reduced form might not exist; ii) constraints are not handled easily and we have to rely in ad hoc arguments for ensuring feasibility; iii) finding a specific parametric form that satisfies the optimality conditions can be extremely difficult since the space of functions is too large; and iv) the rewards have to be concave for all agents in order to guarantee that any policy that satisfies the conditions is an equilibrium.
In order to overcome these issues, we propose to first constrain the set of policies to some parametric family, and then derive the optimality conditions for this parametric problem; as opposed to the standard approach that first derives the optimality conditions of G1, and then guesses a parametric form that satisfies them. Based on this insight, we will introduce MPG with parametric policies as a class of games that can be solved with standard DRL techniques by finding the solution of a related (single-objective) OCP. We explain the details in the following section.

4 CLOSED LOOP MARKOV POTENTIAL GAMES

In this section, we extend the OL analysis of Zazo et al. (2016) to the CL case. We define MPGs with CL information structure; introduce a parametric OCP; provide verifiable conditions for a parametric approximate game to be an MPG in the CL setting; show that when the game is an MPG, we can find a PCL-NE by solving the parametric OCP with a specific objective function; and provide a practical method for obtaining such objective function.
First, we define MPGs with CL information structure and parametric policies as follows.

Definition 2 Given a policy family (·, w)  w, game (8) is called an MPG if there is a function J : X × W ×   R, named the potential, that satisfies the following condition k  N :



E i (rk(xi, k(xi, wk), -k(xi, w-k), k,i) - rk(xi, k(xi, vk), -k(xi, w-k), k,i))

i=0



= E i J (xi, k(xi, wk), -k(xi, w-k), i) - J (xi, k(xi, vk), -k(xi, w-k), i) ,

i=0

xi  X, wk, vk  Wk.

(15)

Definition 2 means that there exists some potential function, J, shared by all agents, such that if some agent k changes its policy unilaterally, the change in its reward, rk, equals the change in J.

The main contribution of this paper is to show that when (8) is a MPG, we can find one PCL-NE by solving a related parametric OCP. The generic form of such parametric OCP is as follows:



maximize E
wW

iJ (xi, (xi, w), i)

P1 :

i=0

s.t. xi+1 = f (xi, (xi, w), i),

g(xi, (xi, w))  0.

(16)

where we replaced the multiple objectives (one per agent) with the potential J as single objective. This is convenient since solving a single objective OCP is generally much easier than solving the Markov game. However, we still have to find out how to obtain J. The following Theorem formalizes the relationship between G2 and P1 and shows one way to obtain J (proof in Appendix C).

Theorem 1 Let Assumptions 1­4 hold. Let the reward functions satisfy the following k, j  N :

E wj [xi rk(xi, (xi, w), k,i)] = E [wk [xi rj (xi, (xi, w), j,i)]] ,

(17)

5

Under review as a conference paper at ICLR 2018

E [xi [xi rk(xi, (xi, w), k,i)]] = E [xi [xi rj (xi, (xi, w), j,i)]] , E wj [wk rk(xi, (xi, w), k,i)] = E wk wj rj (xi, (xi, w), j,i) ,

(18) (19)

where the expected value is taken component-wise. Then, game (8) is an MPG that has a PCL-NE equal to the solution of OCP (16). The potential J that is the instantaneous reward for the OCP is

given by line integral:

J (xi, (xi, w), i)

1
=
0 kN

S rk (z), k((z), wk), -k(((z), w-k), k,i dm(z)

m=1

xi(m)

dz

+ Ak rk xi, k(xi, k(z)), -k(xi, w-k), k,i dk, (z) dz,

=1 ak,i( )

dz

(20)

where (z) (k(z))Sm=1 and (z) (k(z))kN are piecewise smooth paths in X and W, respectively, with components k(z) (k, (z))W=k1, such that the initial and final state-action conditions are given by ((0), (0)) and ((1) = xi, (1) = w).

From (20), we can see that J is obtained through the line integral of a vector field with components the partial derivatives of the agents' rewards (see Appendix C), and so the name potential function. Note also that Theorem 1 proves that any solution to P1 is also a PCL-NE of G2, but we remark that there may be more equilibria of the game that are not solutions to P1 (see Appendix C).
The usefulness of Theorem 1 is that, once we have the potential function, we can formulate and solve the related OCP for any specific parametric policy family. This is a considerable improvement over the standard approach. On one hand, if the chosen parametric policy contains the optimal solution, then we will obtain the same equilibrium as the standard approach. On the other hand, if the chosen parametric family does not have the optimal solution, the standard approach will fail, while our approach will always provide a solution that is an approximation (a projection over w) of an exact variational equilibrium. Moreover, as mentioned above, we can expect that the more expressive the parametric family, the more accurate the approximation to the variational equilibrium. In Appendix B, we show how to to solve "the great fish war" game with the proposed framework, yielding the same solution as with the standard approach, with no loss of accuracy.

Although expressing J as a line integral of a field is theoretically appealing, if the parametric family is involved--as it is usually the case for expressive policies like deep neural-networks--then (20) might be difficult to evaluate. The following results show how to obtain J easily by visual inspection.

First, the following corollary follows trivially from (17)­(19) and shows that cooperative games, where all agents have the same reward, are MPGs, and the potential equals the reward:

Corollary 1 Cooperative games, where all agents have a common reward, such that rk(xi, (xi, w), k,i) = J (xi, (xi, w), i), k  N ,
are MPGs; and the potential function (20) equals the common reward function in (21).

(21)

Second, we address noncooperative games, and show that the potential can be found as a separable

term that is common to all agents' reward functions. Interestingly, we will also show that a game is

an MPG in the CL setting if and only if all agents' policies depend on disjoint subsets of components

of the state vector. the policy of agent

More k and

fionrtrmoadlulyc,eiantnroewduscteatXe kveacstotrh,exske,taonfdslteattexv-ekc,tiobrecothmepvoencetonrtsotfhcaotminpflouneennctes

that do not influence the policy of agent k:

xk,i (xi(m))mXk , x-k,i (xi(l))l/Xk .

(22)

In addition, introduce Xkr as the set of components of the state vector that influence the reward of agent k directly (not indirectly through any other agent's policy), and define the state vectors:

xkr,i (xi(m))mXkr , x-r k,i (xi(l))l/Xkr .

(23)

Finally, introduce the union of these two subsets, Xk = Xk  Xkr, and its corresponding vectors:

xk,i (xi(m))mXk , x-k,i (xi(m))m/Xk .

(24)

6

Under review as a conference paper at ICLR 2018

The following theorem allows us to obtain the potential function by inspection, and states that an MPG requires that all agents have disjoint state-component subsets (proof in Appendix D).

Theorem 2 Let Assumptions 1­4 hold. Suppose the reward function of every agent can be expressed

as the sum of a term common to all agents plus another term that depends neither on its own state-components, nor on its policy parameter:

rk xkr,i, (xk,i, wk), (x- k,i, w-k), k,i = J (xi, (xi, w), i) + k x-r k,i, (x- k,i, w-k), i , k  N

(25)

Then, game (8) is an MPG if and only if:

E xk,i k x-r k,i, u-k (x-k,i), i = 0.

(26)

Moreover, if (26) holds, then the common term in (25), J, equals the potential function (20).

Note that (26) holds in the following cases: i) when k = 0, as the cooperative case described in Corollary 1; ii) when k does not depend on the state, k : Wk  R, as in "the great fish war"
example described in Appendix B; or iii) when all agents have disjoint state-component subsets:

Xk  Xj = , (k, j)  {N × N : k = j}

(27)

In order to apply Theorems 1 and 2, we are implicitly assuming that there exists solution to the OCP. We finish this section, by showing that this is actually the case in our setting (proof in Appendix E).

Proposition 1 Under Assumption 4, OCP (16) has nonempty solution set.

5 EXPERIMENT

In this section, we show how to use the proposed MPGs framework to learn an equilibrium of a communications engineering application. We extend the Medium Access Control (MAC) game presented in Zazo et al. (2016) to stochastic dynamics and rewards (where previous OL solutions would fail), and use the Trust Region Policy Optimization (TRPO) algorithm (Schulman et al., 2015), which is a reliable reinforcement learning method policy search method that approximates the policy with a deep-neural network, to learn a policy that is a PCL-NE of the game.

We consider a MAC uplink scenario with N = 4 agents, where each agent is a user that sets its
transmitter power aiming to maximize its data rate and battery lifespan. If multiple users transmit
at the same time, they will interfere with each other and decrease their rate, using their batteries inefficiently, so that they have to find an equilibrium. Let xk,i  [0, Bk,max] Xk denote the battery level for each agent k  N , which is discharged proportionally to the transmitted power, Let ak,i  [0, Pk,max] Ak be the transmitted power for the k-th user, where constants Pk,max and Bk,max stand for the maximum allowed transmitter power and battery level, respectively. The system state is the vector with all user's battery levels: xi = (xk,i)kN  X; such that S = N and all state vector components are unshared, i.e., X = kN Xk  RN , and Xk = {k}. We remark that although each agent's battery depletion level depends directly on its action and its previous battery
level only, it also depends indirectly on the strategies and battery levels of the rest of agents. The
game can be formalized as follows:

Gmac : k  N

maximize
wk A
s.t.


i log 1 + i=0 1 +

|hk,i|2 (xk,i, wk) jN :j=k |hj |2 (xj,i, wj )

+ xk,i

xk,i+1 = xk,i - i(xk,i, wk), xk,0 = Bk,max

0  (xk,i, wk)  Pk,max, 0  xk,i  Bk,max, i = 0, . . . , 

(28)

where hk is the random fading channel coefficient for user k,  is the weight for the battery reward term, and  is the discharging factor.

First of all, note that each agent's policy and reward depend only on its own battery level, xk,i. Therefore, we can apply Theorem 2 and establish that the game is a MPG, with potential function:

J (xi, (xi, w)) = log 1 + |hk,i|2(xk,i, wk) +  xk,i

kN

kN

(29)

7

Under review as a conference paper at ICLR 2018
40
30
20 10 Vtrpo
Vcvx 0
50 100 150 200 250
Figure 1: Results for the MAC game (28) obtained with TRPO and the averaged solutions given by the convex optimization solver).
Thus, we can formulate OCP (16) with single objective given by (29).
Since the battery level is a positive term in the reward, the optimal policy will make the battery deplete in finite time (formal argument can be derived from transversality condition (55)). Moreover, since k,i  0, the episode gets into a stationary (i.e., terminal) state once the battery has been depleted. We have chosen the reward to be convex. The reason is that we can solve the finite time-horizon convex OCP exactly with a convex optimization solver, e.g., CVX (Grant and Boyd, 2014), and use the result as a baseline for comparing with the solution learned by a DRL algorithm. Nevertheless, standard solvers do not allow to include random variables. To surmount this issue, we generated 100 independent sequences of samples of hk,i and k,i for all k  N and length T = 100 time steps each, and obtain two solutions with them. We set |hk,i|2 = |hk|2 vk,i, where vk,i is uniform in [0.5, 1], |h1|2 = 2.019, |h2|2 = 1.002, |h3|2 = 0.514 and |h4|2 = 0.308; and k,i is uniform in [0.7, 1.3]. The first solution is obtained by averaging the sequences, and building a deterministic convex problem with the average sequence, which yielded an optimal value Vcvx = 33.19. We consider Vcvx to be an estimator of the optimal value of the stochastic OCP. The second solution is obtained by building 100 deterministic problems, solving them, and averaging their optimal values, which yielded an optimal value Vavg,cvx = 34.90. We consider Vavg,cvx to be an upper bound estimate of the optimal value of the stochastic OCP (Jensen's inequality). The batteries depleted at a level xT < 10-6 in all cases, concluding that time horizon of T = 100 steps is valid. We remark that these solutions required complete knowledge of the game.
When we have no prior knowledge of the dynamics and rewards, the proposed approach allows as to learn a PCL-NE of (28) by using any DRL method that is suitable for continuous state and actions, like TRPO (Schulman et al., 2015), DDPG (Lillicrap et al., 2015) or A3C (Mnih et al., 2016). DRL methods learn by interacting with a black-box simulator, such that at every time step i, agents observe state xi, take action ai = w(xi) and observed the new stochastic battery levels and reward values, with no prior knowledge of the reward or state-dynamic functions.
As a proof of concept, we perform simulations with TRPO, approximating the policy with a neural network with 3 hidden layers of size 32 neurons per layer and RELU activation function, and an output layer that is the mean of a Gaussian distribution. Each iteration of TRPO uses a batch of size 4000 simulation steps (i.e., tuples of state transition, action and rewards). The step-size is 0.01. Figure 1 shows the results. After 400 iterations, TRPO achieves an optimal value Vtrpo = 32.34, which is 97.44% of our estimate Vcvx, and 92.7% of the upper bound Vavg,cvx.
6 CONCLUSIONS
We have extended previous results on MPGs with constrained continuous state-action spaces providing practical conditions and a detailed analysis of Nash equilibrium with parametric policies, showing that a PCL-NE can be found by solving a related OCP. Having established a relationship between a MPG and an OCP is a significant step for finding an NE, since we can apply standard optimal control and reinforcement learning techniques. We illustrated the theoretical results by applying TRPO (a well known DRL method) to an example engineering application, obtaining a PCL-NE that yields near optimal results, very close to an exact variational equilibrium.
8

Under review as a conference paper at ICLR 2018
REFERENCES
T. Apostol. Calculus: Multi-variable Calculus and Linear Algebra, with Applications to Differential Equations and Probability. Wiley, 1969.
T. Basar and G. J. Olsder. Dynamic Noncooperative Game Theory. Society for Industrial and Applied Mathematics, 1999.
D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, 3rd edition, 2007.
J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. arXiv preprint 1705.08926, 2017.
D. Fudenberg and D. K. Levine. Open-loop and closed-loop equilibria in dynamic games with many players. Journal of Economic Theory, 44(1):1­18, 1988.
D. González-Sánchez and O. Hernández-Lerma. Discrete­Time Stochastic Control and Dynamic Potential Games: The Euler­Equation Approach. Springer, 2013.
M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1. http://cvxr.com/cvx, Mar. 2014.
N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems 28 (NIPS), pages 2926­2934. 2015.
V. R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143­1166, Apr. 2003.
F. Kydland. Noncooperative and dominant player solutions in discrete dynamic games. International Economic Review, pages 321­335, 1975.
D. Levhari and L. J. Mirman. The great fish war: An example using a dynamic Cournot-Nash solution. The Bell Journal of Economics, 11(1):322­334, 1980.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint 1509.02971v1, 2015.
F. S. Melo and M. Lopes. Fitted natural actor-critic: A new algorithm for continuous state-action MDPs. In Machine Learning and Knowledge Discovery in Databases, volume 5212, pages 66­81. Springer, 2008.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proc. Int. Conf. on Machine Learning (ICML), pages 1928­1937, 2016.
W. B. Powell and J. Ma. A review of stochastic algorithms with continuous value function approximation and some new approximate policy iteration algorithms for multidimensional continuous applications. Journal of Control Theory and Applications, 9(3):336­352, 2011.
A. P. Sage and C. C. White. Optimum Systems Control. Prentice-Hall, 2nd ed. edition, 1977.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint 1506.02438, 2015.
P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for cooperative multi-agent learning. CoRR, abs/1706.05296, 2017.
H. Van Hasselt. Reinforcement learning in continuous state and action spaces. In Reinforcement Learning, pages 207­251. Springer, 2012.
S. Zazo, S. V. Macua, M. Sánchez-Fernández, and J. Zazo. Dynamic potential games with constraints: Fundamentals and applications in communications. IEEE Transactions on Signal Processing, 64 (14):3806­3821, July 2016.
9

Under review as a conference paper at ICLR 2018

A EXAMPLE: THE "GREAT FISH WAR" GAME ­ STANDARD APPROACH

Let us illustrate the standard approach described in Section 3 with a well known resource-sharing game named "the great fish war" due to Levhari and Mirman (1980). We follow (González-Sánchez and Hernández-Lerma, 2013, Sec. 4.2).

Example 1. Let xi be the stock of fish at time i, in some fishing area. Suppose there are N countries obtaining reward from fish consumption, so that they aim to solve the following game:

maximize

Gfish :

k k

k  N

s.t.


i log (k(xi))
i=0



(30)

xi+1 = xi - k(xi) , xi  0, k(xi)  0, i = 0, . . . , ,

kN

where x0  0 and 0 <  < 1 are given.

In order to solve Gfish, let us express each agent's action as:

k(xi) = xi - xi1+/1 -

j (xi ),

jN :j=k

(31)

so that the rewards can be also expressed in reduced form, as required by the standard-approach:



rk(xi) = log xi - x1i+/1 -

j(xi) .

jN :j=k

(32)

Thus, the Euler equations for every agent k  N and all t = 0, . . . ,  become:

xi-1

-x1i /-1/ - xi1/ - jN :j=k j (xi-1)

+



1 xi

- -

jN :j=k j (xi)/xi x1i+/1 - jN :j=k j (xi)

=

0.

(33)

Now, the standard method consists in guessing a family of parametric functions that replaces the policy, and checking whether such parametric policy satisfies (33) for some parameter vector. Let us try with policies that are linear mappings of the state:

k(xi) = wkxi. By replacing (34) in (33), we obtain the following set of equations:


(34)

 1 + wk - wj = 1 - wj, k  N .

jN

jN

(35)

Fortunately, it turns out that (35) has solution (which might not be the case for other policy parametrization), with parameters given by:

1 - 

wk

=



+ N (1 -

, )

k  N .

(36)

Since 0 <  < 1 and 0   < 1, it is apparent that wk > 0 and the constraint k(xi)  0 holds for all xi  0. Moreover, since kN wk < 1, we have that xi+1  0 for any x0  0. In addition, since xi is a resource and the actions must be nonnegative, it follows that limi xi = 0 (there is no reason to save some resource). Therefore, the transversality condition holds. Since the rewards are
concave, the states are non-negative and the linear policies with these coefficients satisfy the Euler
and transversality equations, we conclude that they constitute an equilibrium (González-Sánchez and
Hernández-Lerma, 2013, Theorem 4.1).

B EXAMPLE: "GREAT FISH WAR" GAME ­ PROPOSED APPROACH
In this section, we illustrate how to apply the proposed approach with the same "the great fish war" example, obtaining the same results as with the standard approach.
Example 2. Consider "the great fish war" game described in Example 1. In order to use our approach, we replace the generic policy with the specific policy mapping of our preference. We

10

Under review as a conference paper at ICLR 2018

choose the linear mapping, k(xi) = wkxi, to be able to compare the results with those obtained with the standard approach. Thus, we have the following game:

Gfish,w : k  N

maximize
wk Wk
s.t.


i log (wkxi)
i=0



xi+1 = xi -

wkxi ,

kN

xi  0, wkxi  0, i = 0, . . . , .

(37)

Let us verify conditions (68)­(69). For all k, j  N we have:

rk (xi, (xi, w)) = log (wkxi) ,
xi rk (xi, (xi, w)) = 1/xi, wk rk (xi, (xi, w)) = 1/wk, xi xi rk (xi, (xi, w)) = xi xi rj (xi, (xi, w)) = -1/xi2, wj xi rk (xi, (xi, w)) = wk xi rj (xi, (xi, w)) = 0, wj wk rk (xi, (xi, w)) = wk wj rj (xi, (xi, w)) = 0.

(38) (39) (40) (41) (42) (43)

Since conditions (68)­(69) hold, we conclude that (37) is an MPG. By applying the line integral (20), we obtain:

J (xi, wi) = log(xi) + log (wk) .
kN

(44)

Now, we can solve OCP (16) with potential function (44). For this particular problem, it is easy to solve the KKT system in closed form. Introduce a shorthand:

w wk.
kN
The Euler-Lagrange equation (63) for this problem becomes: i + ixi (1 - w) - i-1xi = 0.

(45) (46)

The optimality condition (65) with respect to the policy parameter becomes:

i - ixi (1 - w)-1 wk = 0.

(47)

Let us solve for i in (47):

i

i

=

xi

(1

-

w)-1

. wk

(48)

Replacing (48) and the state-transition dynamics in (46), we obtain the following set of equations:

 (1 + wk - w) = 1 - w, k  N .

(49)

Hence, the parameters can be obtained as:

1 - 

wk

=



+ N (1 -

, )

k  N .

(50)

This is exactly the same solution that we obtained in Example 1 with the standard approach. We remark that for the standard approach, we were able to obtain the policy parameters since we put the correct parametric form of the policy in the Euler equation. If we had used another parametric family without a linear term, the Euler equations (33) might have no solution and we would have got stuck. In contrast, with our approach, we could freely choose any other form of the parametric policy, and always solve the KKT system of the approximate game. Broadly speaking, we can say that the more expressive the parametric family, the more likely that the optimal policy of the original game will be accurately approximated by the optimal solution of the approximate game.

11

Under review as a conference paper at ICLR 2018

C PROOF OF THEOREM 1

Proof: The proof mimics the OL analysis from Zazo et al. (2016). Let us build the KKT systems
for the game and the OCP with parametric policies. For game (8), each agent's Lagrangian is given k  N by



Lk x0:, w, k,0:,0:, k,0:, µk,0: = E

i rk (xi, (xi, w), k,i)

i=0

+ k,i (f (xi, (xi, w), i) - xi+1) + µk,i g (xi, (xi, w)) , (51)

where k,i (k,s,i)sS=1  RS and µk,i (µk,c,i)Cc=1  RC are the vectors of multipliers at time i (which are random since they depend on i and xi), and we introduced:

x0: (xi)i=0 , a0: (ai)i=0 , k,0: (k,i)i=0 , µk,0: (µk,i)i=0 .

(52)

Introduce a shorthand for the instantaneous Lagrangian of agent k:

k xi, xi+1, w, k,i,i, k,i, µk,i E rk (xi, (xi, w), k,i)

+ k,i (f (xi, (xi, w), i) - xi+1) + µk,i g (xi, (xi, w)) . (53)

The discrete time stochastic Euler-Lagrange equations applied to each agent's Lagrangian are different from the OL case studied in Zazo et al. (2016) (see also (Sage and White, 1977, Sec. 6.1)), since we only take into account the variation with respect to the state:

E xi k(xi, xi+1, w, k,i, i, k,i, µk,i) + xi k (xi-1, xi, w, k,i-1, i-1, k,i-1, µk,i-1) = 0S , i = 1, . . . , ,
where 0S denotes the vector of length S. The transversality condition is given by

(54)

lim E
i

xk,ixk,i k (xi, xi+1, w, k,i, i, k,i, µk,i)

= 0S.

(55)

In addition, we have an optimality condition for the policy parameter wk:

E wk k (xi, xi+1, w, k,i, i, k,i, µk,i) = 0Wk .

(56)

From these first-order optimality conditions, we obtain the KKT system for every agent k  N and all time steps i = 1, . . . , :

E xi rk (xi, (xi, w), k,i) + k,i f (xi, (xi, w), i)

+ xi µk,i g (xi, (xi, w)) - k,i-1 = 0Sk , (57)

lim E
i

xk,ixi

rk (xi, (xi, w), k,i) + k,i f (xi, (xi, w), i)

+ xi µk,i g (xi, (xi, w)) = 0Sk ,

(58)

E wk rk (xi, (xi, w), k,i) + k,i f (xi, (xi, w), i)

+ wk µk,i g (xi, (xi, w)) = 0Wk , xi+1 = f (xi, (xi, w), i) , g (xi, (xi, w))  0C ,
µk,i  0C , µk,i g (xi, (xi, w)) = 0,

(59) (60) (61)

where k,i-1 is considered deterministic since it is known at time i.

Now, we derive the KKT system of optimality conditions for the OCP (16). The Lagrangian for (16) is given by:



LOCP x0:, w,k,0:, 0:, 0:, 0: = E

i J (xi, (xi, w), i)

i=0

+ i (f (xi, (xi, w), i) - xi+1) + i g (xi, (xi, w)) ,

(62)

12

Under review as a conference paper at ICLR 2018

where i (k,s,i)sS=1  RS and i (k,c,i)Cc=1  RC are the corresponding multipliers, which are random variables since they depend on i and xi. By taking the discrete time stochastic EulerLagrange equations and the optimality condition with respect to the policy parameter for the OCP, we obtain are a KKT system for the OCP: i = 1, . . . , :

E xi J (xi, (xi, w), i) + i f (xi, (xi, w), i)

+ xi i g (xi, (xi, w)) - i-1 = 0Sk ,

(63)

lim E
i

xi

xi

J (xi, (xi, w), i) + i f (xi, (xi, w), i)

+ xi i g (xi, (xi, w)) = 0Sk ,

(64)

E w J (xi, (xi, w), i) + i f (xi, (xi, w), i)

+ wk i g (xi, (xi, w)) = 0A, xi+1 = f (xi, (xi, w), i) , g (xi, (xi, w))  0C ,
i  0C , i g (xi, (xi, w)) = 0,
where i-1 is known at time i and includes the multipliers related to xi-1.

(65) (66) (67)

By comparing (57)­(61) and (63)­(67), we conclude that both KKT systems are equal if the following holds k  N and i = 1, . . . , :

E [xi rk (xi, (xi, w), k,i)] = E [xi J (xi, (xi, w), i)] , E [wk rk (xi, (xi, w), k,i)] = E [wk J (xi, (xi, w), i)] ,
k,i = i , µk,i = i.

(68) (69) (70)

Since Assumption 4 ensures existence of primal variable for the OCP, Assumption 3 guarantee the existence of dual variables that satisfy its KKT system. By applying (70) and replacing the dual variables of the KKT of the game with the OCP dual variables for every agent, we obtain a system of equations where the only unknowns are the user strategies. This system is similar to the OCP in the primal variables. Therefore, the OCP primal solution also satisfies the KKT necessary conditions of the game. Moreover, from the potentiality condition, it is straightforward to show that this primal solution of the OCP is also a PCL-NE of the MPG (see also (Zazo et al., 2016, Theorem 1)).

Introduce the following vector field:

F (xi, w, i) (xi,w)J (xi, (xi, w), i) .

(71)

Since F is conservative by construction (Apostol, 1969, Theorems 10.4, 10.5 and 10.9), conditions

(68)­(69) are equivalent to (17)­(19) and we can calculate a potential J through line integral (20).

D PROOF OF THEOREM 2

Proof: We can rewrite game (8) by making explicit that the actions result from the policy mapping, which yields an expression that reminds the OL problem but with extra constraints:

G3 : k  N

maximize

wkWk, {ak,i}0 

 i=0

Ak

s.t.


E irk xkr,i, ak,i, a-k,i, k,i
i=0
ak,i = (xk,i, wk), a-k,i = (x- k,i, w-k),
xi+1 = f (xi, ai, i),

(72)

g(xi, ai)  0,

where it is clear that: ai (ak,i, a-k,i) = (xi, w) Rewrite also OCP (16) with explicit dependence on the actions:

P2 :

maximize

wW, {ai}0

 i=0

A

s.t.


E iJ (xi, ai, i)
i=0
ai = (xi, w),

(73)

xi+1 = f (xi, ai, i),

g(xi, ai)  0.

13

Under review as a conference paper at ICLR 2018

By following the Euler-Lagrange approach described in Theorem 1, we have that the KKT systems for game and OCP are equal if the dual variables are equal (including new extra dual variables for the equality constraints that relate the action and the policy) and the following first-order conditions hold k  N and i = 1, . . . , :

E xkr,i rk xkr,i, ak,i, a-k,i, k,i E ak,i rk xrk,i, ak,i, a-k,i, k,i

= E xrk,i J (xi, ai, i) , = E ak,i J (xi, ai, i) .

(74) (75)

The benefit of this reformulation is that the gradient in (74) is taken with respect to the components
in Xkr only (instead of the whole set X ), at the cost of replacing (69) with the sequence of conditions (75). We have to realize that ak,i is indeed a function of variables xk,i and wk. In order to understand the influence of this variable change, we use the identity ak,i = wk (xk,i) and apply the chain rule to both sides of (75), obtaining:

E xk,i rk = E xkr,i rk E wk rk = E ak,i rk

xk,i xrk,i + E ak,i rk wk ak,i,

xk,i ak,i,

(76) (77)

E xk,i J = E xrk,i J xk,i xkr,i + E ak,i J xk,i ak,i,

(78)

E [wk J ] = E ak,i J wk ak,i.

(79)

From (74)­(75), it is clear that the right side of (76) and (78) are equal. Similarly, from (75), the right side of (77) and (79) are equal, so that their left side must be also equal. Hence, we can replace (75) with the two following conditions:

E xk,i rk xrk,i, wk xk,i , a-k,i, k,i E wk rk xrk,i, wk xk,i , a-k,i, k,i

= E xk,i J xi, wk xk,i , a-k,i, i , = E wk J xi, wk xk,i , a-k,i, i .

Moreover, we can combine (74) and (80) in one single equation:

(80) (81)

E xk,i rk xkr,i, wk xk,i , a-k,i, k,i = E xk,i J xi, wk xk,i , a-k,i, i By using the identity a-k,i = w-k (x- k,i) in (81)­(82), we have:

.

(82)

E xk,i rk xrk,i, wk xk,i , w-k x-k,i , k,i E wk rk xi, wk xk,i , w-k x-k,i , k,i

= E xk,i J (xi, u (xi) , i) , = E [wk J (xi, u (xi) , i)] .

(83) (84)

Note that under assumptions (25)­(26), conditions (83)­(84) are equivalent to (68)­(69), with potential function J equal to the objective of OCP (16).

E PROOF OF PROPOSITION 1

Proof: Once that Theorem 2 has shown that the individual rewards can be expressed in separable
form, it follows from the definition of proper function that: rk being proper implies that J is also proper. Since J is proper, it has nonempty level sets. Let B  R define a nonempty level set of J:

{a0  C0, (xi, ai)  Ci : E [J (xi, ai, i)]  B}i=0 .

(85)

Since  < 1, we have:



iE [J (xi, ai, i)]  B



i =

B 1-.

i=0 i=0

(86)

Hence, the following level sets are also nonempty:

(xi, ai) :



iE [J

(xi, ai, i)]



1

B -



i=0


.
i=0

(87)

14

Under review as a conference paper at ICLR 2018

In addition, since J is proper, it must be upper bounded, i.e., U  R, such that J  U . Then, we have:



iE [J (xi, ai, i)]  U



i = U . 1-

i=0 i=0

(88)

Since B  U , we have that

1

B -





U 1-.

(89)

Therefore, the level sets (87) are bounded.

From Assumption 2 the fact that J can be obtained from line integral (20), and fundamental theorem of calculus, we deduce that J is continuous. Therefore, we conclude that these level sets are also compact. Thus, we can use (Bertsekas, 2007, Prop. 3.1.7, see also Sections 1.2 and 3.6) to ensure existence of an optimal policy.

15

