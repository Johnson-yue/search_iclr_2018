Under review as a conference paper at ICLR 2018
EVOLUTIONARY EXPECTATION MAXIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables. While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data). Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM). We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step, the latent states are then optimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A) considering the bit-vectors of the latent states as genomes of individuals, and by (B) defining the fitness of the individuals as the (log) joint probabilities given by the used generative model. As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.
1 INTRODUCTION
Evolutionary algorithms (EA) have been introduced (e.g. Fogel et al., 1966; Rechenberg, 1965) as a technique for function optimization using methods inspired by biological evolutionary processes such as mutation, recombination, and selection. As such EAs are of interest as tools to solve Machine Learning problems, and they have been frequently applied to a number of tasks such as clustering (Pernkopf & Bouchaffra, 2005; Hruschka et al., 2009), reinforcement learning (Salimans et al., 2017), and hierarchical unsupervised (Myers et al., 1999) or deep supervised learning (e.g., Stanley & Miikkulainen 2002 and Suganuma et al. 2017; Real et al. 2017 for recent examples). In some of these tasks EAs have been investigated as alternatives to standard procedures (Hruschka et al., 2009), but most frequently EAs are used to solve specific sub-problems. For example, for classification with Deep Neural Networks (DNNs LeCun et al., 2015; Schmidhuber, 2015), EAs are frequently applied to solve the sub-problem of selecting the best DNN architectures for a given task (e.g. Stanley & Miikkulainen, 2002; Suganuma et al., 2017) or more generally to find the best hyper-parameters of a DNN (e.g. Loshchilov & Hutter, 2016; Real et al., 2017).
Inspired by these previous contributions, we here ask if EAs and learning algorithms can be linked more tightly. To address this question we make use of the theoretical framework of probabilistic generative models and expectation maximization (EM Dempster et al., 1977) approaches for parameter optimization. The probabilistic approach in combination with EM is appealing as it establishes a very general unifying framework able to encompass diverse algorithms from clustering and dimensionality reduction (Roweis, 1998; Tipping & Bishop, 1999) over feature learning and sparse coding (Olshausen & Field, 1997) to deep learning approaches (Patel et al., 2016). However, for most generative data models, EM is computationally intractable and requires approximations. Variational EM is a very prominent such approximation and is continuously further developed to become more
1

Under review as a conference paper at ICLR 2018

efficient, more accurate and more autonomously applicable. Variational EM seeks to approximately solve optimization problems of functions with potentially many local optima in potentially very high dimensional spaces. The key observation exploited in this study is that a variational EM algorithm can be formulated such that latent states serve as variational parameters. If the latent states are then considered as genomes of individuals, EAs emerge as a very natural choice for optimization in the variational loop of EM.

2 TRUNCATED VARIATIONAL EM

A probabilistic generative model stochastically generates data points y using a set of hidden (or latent) variables s. The generative process can be formally expressed in the form of joint probability p(s, y | ), where  are the model parameters. Given a set of N data points, y(1), . . . , y(N) = y(1:N), learning seeks to change the parameters  so that the data generated by the generative model becomes as similar as possible to the N real data points. One of the most popular approaches to achieve this goal is to seek maximum likelihood (ML) parameters , i.e., parameters that maximize the data log-likelihood for a given generative model:

L() := log(L()) = log ( p (yn, s | ))

(1)

n {s}

To efficiently find (approximate) ML parameters we follow Saul & Jordan (1996); Neal & Hinton
(1998); Jordan et al. (1999) who reformulated the problem in terms of a maximization of a lower bound of the log-likelihood, the free energy F (q, ). Free energies are given by

N
F (q(1:N), ) =

N
q(n)(s) log p(s, y (n) | ) + H(q(n)(s)) ,

(2)

n=1 {s}

n=1

where q(n)(s) are variational distributions, and where H(q) denotes the entropy of a distribution q. For the purposes of this study, we consider elementary generative models which are difficult to train because of exponentially large state spaces. These models serve well for illustrating the approach but we stress that any generative model which gives rise to a joint distribution p(s, y | ) can be trained with the approach discussed here as long as the latents s are binary.

In order to find approximate maximum likelihood solutions, distributions q(n)(s) are sought that approximate the intractable posterior distributions p(s | y (n), ) as well as possible, which results in the free-energy being as similar (or tight) as possible to the exact log-likelihood. At the same time variational distributions have to result in tractable parameter updates. Standard approaches include Gaussian variational distributions (e.g. Opper & Winther, 2005) or mean-field variational distributions (Jordan et al., 1999). If we denote the parameters of the variational distributions by , then a variational EM algorithm consists of iteratively maximizing F(, ) w.r.t.  in the variational E-step and w.r.t.  in the M-step. The M-step can hereby maintain the same functional form as for exact EM but the expectation values now have to be computed w.r.t. the variational distributions.

Instead of using parametric functions such as Gaussians or factored (mean-field) distributions, for
our purposes we choose truncated variational distributions defined as a function of a finite set of
states (Lu¨cke & Eggert, 2010; Sheikh et al., 2014; Shelton et al., 2017). These states will later serve as populations of evolutionary algorithms. If we denote Kn a population of hidden states for a given data point y (n), then variational distributions and their corresponding expectation values are given
by (e.g. Lu¨cke & Eggert, 2010; Sheikh et al., 2014):

qn(s | Kn, ) :=

p

(s p

| yn, ) (s | yn,

)

(s



Kn),

s Kn

p(s, yn | )g(s) g(s) qn = sKn p(s , yn | ) .
s Kn

(3)

where (s  Kn) is 1 if Kn contains the hidden state s, zero otherwise. If the set Kn contains all states with significant posterior mass, then (3) approximates expectations w.r.t. full posteriors very well. By inserting truncated distributions as variational distribution of the free-energy (2), it can be shown (Lu¨cke, 2016) that the free-energy takes a very compact simplified form given by:

F (K, ) = log ( p (yn, s | )), where K = (K1, . . . , KN ).
n sKn

(4)

2

Under review as a conference paper at ICLR 2018

As the variational parameters of the variational distribution (3) are now given by populations of hidden states, a variational E-step now consists of finding for each data point n the population Kn that maximizes sKn p (yn, s | ).

3 EVOLUTIONARY OPTIMIZATION

For the generative models considered here, each latent state s takes the form of a bit vector. Hence, each population Kn is a collection of bit vectors. Because of the specific form (4), the free-energy is
increased in the variational E-step if and only if we replace and individual s in population K(n) by a
new individual snew so far not in K(n) such that:

p(snew, yn | ) > p(s, yn | ) .

(5)

More generally, this means that the free energy is maximized in the variational E-step if we find for each n those S individuals with the largest joints p(s, yn | ), where p(s, yn | ) is given by the
respective generative model (compare Lu¨cke, 2016; Forster & Lu¨cke, 2017, for formal derivations).

Full maximization of the free-energy is often a computationally much harder problem than increas-
ing the free-energy; and in practice an increase is usually sufficient to finally approximately max-
imize the likelihood. As we increase the free-energy by applying (5) we can choose any fitness function F (s; yn, ) for an evolutionary optimization which fulfils the property:

F (snew; yn, ) > F (s; yn, )  p(snew, yn | ) > p(s, yn | ) .

(6)

Any mutations selected such that the fitness F (s; yn, ) increases will result in provably increased free-energies. Together with M-step optimizations of model parameters, the resulting variational EM algorithm will monotonously increase the free-energy. The freedom in choosing a fitness function satisfying (6) leaves us free to pick a form that enables an efficient parent selection procedure. More concretely (while acknowledging that other choices are possible) we define the fitness F (snew; yn, ) to be:

F (s) = F (s; yn, ) = logP (s; yn, ) - 2 min (logP (s; yn, ))
s

(7)

where logP is defined as the logarithm of the joint probability where summands that do not depend
on the state s have been elided. logP is usually more efficiently computable than the joint probabilities and has better numerical stability, while being a monotonously increasing function of the joints when the data-point yn is considered fixed. As we will want to sample states proportionally to their
fitness, an offset is applied to logP to make sure F always takes positive values. As previously mentioned, other choices of F are possible as long as (6) holds. From now on we will drop the argument yn or index n (while keeping in mind that an optimization is performed for each data point yn).

Our applied EAs then seek to optimize F (s) for a population of individual K (we also drop the index

n here). More concretely, given the current population K of unique individuals s, the EA iteratively

seeks a new set K with higher overall fitness. For our models, s are bit-vectors of length H, and

we usually require that populations K and K to have the same size as is customary for truncated

approximations (e.g. Lu¨cke & Eggert, 2010; Shelton et al., 2017). Our example algorithm includes

three common genetic operators, discussed in more detail below: parent selection, generation of

children by single-point crossover and stochastic mutation of the children. We repeat this process

over Ng generations in which subsequent iterations use the output of previous iterations as input population.

A) Parent selection

F (s)

B) Crossover

C) Mutation

sa : (1 · · · 1 1) sb : (0 · · · 0 1)

(1 · · · 0 1) (0 · · · 1 1)

(1 · · · 0 1) p1 p0
(0 · · · 1 1)

s sa sb

Figure 1: Components of the genetic algorithm.

3

Under review as a conference paper at ICLR 2018

Figure 2: A small Noisy-OR
model. Each observable yd is conditionally dependent
on all sh. The generative process first samples each
sh from a Bernoulli distribution; then each yd is sampled from a Bernoulli distribution
of parameter Nd(s), generating a data-point.

Parent Selection. This step selects Np parents from the population K. Ideally, the selection procedure should be balanced between exploitation of parents with high fitness (which will more likely produce children with high fitness) and exploration of mutations of poor performing parents (which might eventually produce children with high fitness while increasing population diversity). Diversity is crucial, as K is a set of unique individuals and therefore the improvement of the overall fitness of the population depends on generating different children with high fitness. In our numerical experiments we explored both fitness-proportional selection of parents (a classic strategy in which the probability of an individual being selected as a parent is proportional to its fitness) and random uniform selection of parents.

Crossover. During the crossover step, random pairs of parents are selected; then each pair is assigned a number c from 1 to H - 1 with uniform probability (this is the single crossover point); finally the parents swap the last H - c bits to produce the offspring. We denote Nc the number of children generated in this way. The crossover step can be skipped, making the EA more lightweight
but decreasing variety in the offspring.

Mutation. Finally, each of the Nc children undergoes one or more random bitflips to further increase offspring diversity. In our experiments we compare results of random uniform selection of the bits to flip with a more refined sparsity-driven bitflip algorithm. This latter bitflip schemes assignes to 0's and 1's different probabilities of being flipped in order to produce children with a sparsity compatible with the one learned by the model. In case the crossover step is skipped, a different bitflip mutation is performed on Nc identical copies of each parent.

A full run of the evolutionary algorithm

Algorithm 1: Evolutionary Expectation Maximization therefore produces NgNcNp children (or

choose initial model parameters  and initial sets K(n) repeat
for each data-point n do candidates = {}

new states s). Finally we compute the union set of the original population K with
all children and select the S fittest individuals of the union as the new population K .

for g = 0 to Ng do parents = select parents
children = mutation(crossover(parents)) candidates = candidates  children

The EEM Algorithm. We now have all elements required to formulate a learning algorithm with EAs as its integral part. Alg. 1 summarizes the essential computa-

K(n) = select best(K(n)  candidates) update  using M-steps with (3) and K(n)

tional steps. Note that this E-step can be trivially parallelized over data-points. Finally, it is worth pointing out that algo-

until F has increased sufficiently

rithm 1, by construction, never decreases

the free-energy.

4 THE GENERATIVE MODELS

We will use the EA formulated above as integral part of an unsupervised learning algorithm. The objective of the learning algorithm is the optimization of the log-likelihood 1. D denotes the number of observed variables, H the number of hidden units, and N the number of data points. Noisy-OR. The noisy-OR model is a highly non-linear bipartite data model with all-to-all connectivity among hidden and observable variables. All variables take binary values. The model assumes
4

Under review as a conference paper at ICLR 2018

a Bernoulli prior for the latents, and active latents are then combined via the actual noisy-OR rule.

p (s | ) = hsh (1 - h)1-sh
h
p (y | s, ) = Nd(s)yd (1 - Nd(s))1-yd where Nd(s) := 1 - (1 - Wdhsh)
dh

(8) (9)

In the context of the Noisy-OR model,  = {, W }, where  is the set of values h  [0, 1] representing the prior activation probabilities for the hidden variables sh and W is a D×H matrix of values Wdh  [0, 1] representing the probability that the latent sh activates the observable yd.
Section A of the appendix contains the explicit forms of the free energies and the M-step update rules for noisy-OR.

Binary Sparse Coding. As a second model and one for continuous data, we consider Binary Sparse Coding (BSC; Henniges et al., 2010). BSC differs from standard Sparse Coding in its use of binary latent variables. The latents are assumed to follow a univariate Bernoulli distribution which uses the same activation probability for each hidden unit. The combination of the latents is described by a linear superposition rule. Given the latents, the observables are independently and identically drawn from a Gaussian distribution:

H
p (s | ) = sh (1 - )1-sh ,
h=1

DH
p (y | s, ) = N (yd; Wdhsh, 2) .

d=1

h=1

(10)

The parameters of the model are  = (, W, 2), where W is a D × H matrix whose columns contain the weights associated with each hidden unit sh and where 2 determines the variance of the Gaussian. M-step update rules for BSC can be derived in close-form by optimizing the free
energy (2) wrt. all model parameters (compare, e.g., Henniges et al., 2010). We report the final
expressions in appendix B.

5 NUMERICAL EXPERIMENTS

We describe numerical experiments performed to test the applicability and scalability of EEM. Throughout the section, the different evolutionary algorithms are named by indicating which parent selection procedure was used ("fitparents" for fitness-proportional selection, "randparents" for random uniform selection) and which bitflip algorithm ("sparseflips" or "randflips"). We add "cross" to the name of the EA when crossover was employed.

5.1 ARTIFICIAL DATA

First we investigate EMM using artificial data where the ground-truth components are known. We
use the bars test as a standard setup for such purposes (Fo¨ldiak, 1990; Hoyer, 2003; Lu¨cke & Sahani, 2008). In the standard setup, Hgen/2 non-overlapping vertical and Hgen/2 non-overlapping horizontal bars act as components on D = Hgen × Hgen pixel images. N images are then generated by first selecting each bar with probability gen. The bars are then superimposed according to the
noisy-OR model (non-linear superposition) or according to the BSC model. In the case of BSC
Gaussian noise is then added.

Noisy-OR. Let us start with the standard bars test which uses a non-linear superposition (Fo¨ldiak,

1990) of 16 different bars (Spratling, 1999; Lu¨cke & Sahani, 2008), and a standard average crowd-

edness of two bars per images (gen

=

2 H gen

).

We apply EEM for noisy-OR using different con-

figurations of the EA. We use H = 16 generative fields. As a performance metric we here employ

reliability (compare, e.g., Spratling, 1999; Lu¨cke & Sahani, 2008), i.e., the fraction of runs whose

learned free energies are above a certain minimum threshold and which learn the full dictionary of

bars as well as the correct values for the prior probabilities .

Figure 3 shows reliabilities over 10 different runs for each of the EAs. On 8x8 images the more exploitative nature of "fitparents-sparseflips" is advantageous over the simpler and more explorative "randparents-randflips". Note that this is not necessarily true for lower dimensionalities or otherwise easier-to-explore state spaces, in which also a naive random search might quickly find high-fitness

5

Under review as a conference paper at ICLR 2018

Reliability

1.0 0.8 0.6 0.4 0.2 fitparefnittpsa-crreroan0snt.ss0d--rpcaranordsesfnl-itspsps-acrrosesrasfl-nispdpspaarsreefnfilttippsa-srraenndtsfl-isppsarseflips

Figure 3: Reliability for the listed EAs over 10 runs of EEM for noisy-OR on 8x8 bars images. In this figure, black bars indicate both priors and bars were recovered correctly, grey bars indicate bars were recovered but not priors. For all runs H = 16, N = 104, Ng = 2, Np = 8, Nc = 7, S = 120. Each run performed 100 iterations.

individuals. In this test the addition of crossover reduces the probability of finding all bars and leads to an overestimation of the crowdedness H.
After the initial verification on a standard bars test, we now make the component extraction problem more difficult by increasing overlap among the bars. A highly non-linear generative model such as noisy-OR is a good candidate to model occlusion effects in images. Figure 4 shows the results of training noisy-OR with EEM on a bars data-set in which the latent causes have sensible overlaps. The test parameters were chosen to be equal to those in (Lu¨cke & Sahani, 2008, Fig. 9). After applying EEM with noisy-OR (H = 32) to N = 400 images with 16 strongly overlapping bars, we observed that all Hgen = 16 bars were recovered in 13 of 25 runs, which is competitive especially when keeping in mind that no additional assumptions (e.g., compared to other models applied to this test) are used by EEM for noisy-OR.

Figure 4: Sample input (left) and learned generative fields (right) for a run on overlapping bars. Out of 25 runs, 13 recovered all 16 ground-truth generative components (14.92 recovered bars in average, median 16). As H = 32, the extra generative fields are used to explain common overlaps and noise.

BSC. Like for the non-linear generative model, we first evaluate EEM for the linear BSC model on

a bars test. For BSC, the bars are superimposed linearly (Henniges et al., 2010), which makes the

problem easier. As a consequence, standard bars test were solved with very high reliability using

EEM for BSC even if merely random bitflips were used for the EA. In order to make the task more

challenging, we therefore (A) increased the dimensionality of the data to D = 10 × 10 bars images,

(B) increased the number of components to Hgen = 20, and (C) increased the average number of

bars per data point from two (the standard setting) to five. We employed N = 5, 000 training data

points and tested the same five different configurations of the EA as were evaluated for noisy-OR.

We set the number of hidden units to H = Hgen = 20 and used S = 120 variational states. Per

data point and per iteration, in total 112 new states (Np = 8, Nc = 7, Ng = 2) were sampled to vary Kn. Per configuration of the EA, we performed 20 independent runs, each with 300 iterations.

The results of the experiment are depicted in Fig. 5. We observe that a basic approach such as

random uniform selection of parents and random uniform bitflips for the EA works well. However,

more sophisticated EAs improve performance. For instance, combining bitflips with crossover and

selecting parents proportionally to their fitness shows to be very benefical. The results also show

that sparseness-driven bitflips lead generally to very poor performance, even if crossover or fitness-

proportional selection of the parents is included. This effect may be explained with the initialization

of Kn.

The initial states

are

drawn from

a

Bernoulli distribution with parameter

1 H

which makes

it more difficult for sparseness-driven EAs to explore and find solutions with higher crowdedness.

Fig. 8 in appendix C depicts the averaged free energy values for this experiment.

6

Reliability

Under review as a conference paper at ICLR 2018
1.0 0.8 0.6 0.4 0.2 0.0 fitfiptapreanrretasn-ntssdp-pcaarrorseessnfl-tispsp-scarrosrfiesastfln-pisdpappsraaernrsetensfl-tcispr-orsassnodvfleirp-rsandflips

Figure 5: Reliability for the listed EAs over 20 runs of EEM for BSC on 10x10 bars images.

5.2 NATURAL IMAGE PATCHES
Next, we verify the approach on natural data. We use patches of natural images, which are known to have a multi-component structure, which are well investigated, and for which typically models with high-dimensional latent spaces are applied. The image patches used are extracted from the van Hateren image database (van Hateren & van der Schaaf, 1998).
Noisy-OR. First we consider raw images patches, i.e., images without substantial pre-processing which directly reflect light intensities. Such image patches were generated by extracting random square subsections of a single 255x255 image of overlapping grass wires (part of image 2338 of the database). We removed the brightest 1% pixels from the data-set, scaled each data-point to have gray-scale values in the range [0, 1] and then created data points with binary entries by repeatedly choosing a random gray-scale image and sampling binary pixels from a Bernoulli distribution with parameter equal to the gray-scale value of the original pixel (cfr. figure 6). Note that components in such light-intensity images can be expected to superimpose non-linearly because of occlusion, which motivates the application of a non-linear generative model such as noisy-OR. We employ the "fitparents-sparseflips" evolutionary algorithm that was shown to perform best on artificial data (3). Parameters were H = 100, S = 120, Ng = 2, Np = 8, Nc = 7. Figure 6 shows the generative fields learned over 200 iterations. EEM allows learning of generative fields resembling curved edges, in line with expectations and with the results obtained in (Lu¨cke & Sahani, 2008).

Figure 6: 50 generative fields learned by applying EEM ("fitparents-sparseflips") for noisy-OR to natural image patches. See Appendix F for a run at H = 200.
BSC. Finally, we consider pre-processed image patches using common whitening approaches as they are customary for sparse coding approaches (Olshausen & Field, 1997). We use N = 100, 000 patches of size D = 16 × 16, randomly picked from the whole data set. The highest 2 % of the amplitudes were clamped to compensate for light reflections and patches without significant structure were excluded for learning. ZCA whitening (Bell & Sejnowski, 1997) was applied retaining 95 % of the variance (we used the procedure of a recent paper Exarchakis & Lu¨cke, 2017). We trained the BSC model for 4,000 iterations using the "fitparents-cross-sparseflips" EA and employing H = 300 hidden units and S = 200 variational states. Per data point and per iteration, in total 360 new states (Np = 10, Nc = 9, Ng = 4) were sampled to vary Kn. The results of the experiment are depicted in Fig. 7. The obtained generative fields primarily take the form of Gabor functions with different locations, orientations, phase, and spatial frequencies. This is a typical outcome of sparse coding being applied to images. On average more than five units were activated per data point showing that the learned code makes use of the generative model's multiple causes structure. The generative fields converged faster than prior and noise parameters (similar effects are known from probabilistic PCA for the variance parameter). The finit slope of the free-energy after 4000 iterations is presumably due to these parameters still changing slowly.
7

Under review as a conference paper at ICLR 2018

A

F (, K) / N
H

B

-150

-200

-250 0

1000 2000 3000 4000
Iteration

C
4 2
Iteration0 0 1000 2000 3000 4000

D
0.6 0.5 0.4
0

1000 2000 3000 4000
Iteration

Figure 7: Results on training the BSC model on natural images using the "fitparents-crosssparseflips" EA. A 60 of the 300 generative fields obtained through training (see Appendix for all fields). B Evolution of the free energy per data point over iterations. C Evolution of the expected number of active hidden units per data point over iterations. D Evolution of the standard deviation over iterations.

6 DISCUSSION
The training of generative models is a very intensively studied branch of Machine Learning. If EM is applied for training, most non-elementary models require approximations. For this reason, sophisticated and mathematically grounded approaches such as sampling or variational EM have been developed in order to derive sufficiently precise and efficient learning algorithms.
Evolutionary algorithms (EAs) have also been applied in conjunction with EM. Pernkopf & Bouchaffra (2005), for instance, have used EAs for clustering with Gaussian mixture models (GMMs). However, the GMM parameters are updated by their approach relatively conventionally using EM, while EAs are used to select the best GMM models for the clustering problem (using a min. description length criterion). Such a use of EAs is similar to DNN optimization where EAs optimize DNN hyperparameters in an outer optimization loop (Stanley & Miikkulainen, 2002; Loshchilov & Hutter, 2016; Real et al., 2017; Suganuma et al., 2017, etc), while the DNNs themselves are optimized using standard error-minimization algorithms. Still other approaches have used EAs to directly optimize, e.g., a clustering objective. But in these cases EAs replace EM approaches for optimization (compare Hruschka et al., 2009). In contrast to all such previous applications, we have here shown that EAs and EM can be combined directly and intimately: Alg. 1 defines EAs as an integral part of EM, and as such EAs address the key optimization problem arising in the training of generative models.
We see the main contribution of our study in the establishment of this close theoretical link between EAs and EM. This novel link will make it possible to leverage an extensive body of knowledge and experience from the community of evolutionary approaches for learning algorithms. Our numerical experiments are a proof of concept which shows that EAs are indeed able to train generative models with large hidden spaces and local optima. For this purpose we used very basic EAs with elementary selection, mutation, cross-over operators.
EAs more specialized to the specific optimization problems arising in the training of generative models have great potentials in future improvements of accuracy and scalability, we believe. In our experiments, we have only just started to exploit the abilities of EAs for learning algorithms. Still, our results represent, to the knowledge of the authors, the first examples of noisy-OR or sparse coding models trained with EAs (although both models have been studied very extensively before). Most importantly, we have pointed out a novel mathematically grounded way how EAs can be used for generative models with binary latents in general. The approach here established is, moreover, not only very generically formulated using the models' joint probabilities but it is also very straightforward to apply.
8

Under review as a conference paper at ICLR 2018
REFERENCES
A. J. Bell and T. J. Sejnowski. The "independent components" of natural scenes are edge filters. Vision Research, 37(23):3327­38, 1997.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical Society B, 39:1­38, 1977.
G. Exarchakis and J. Lu¨cke. Discrete sparse coding. Neural Computation, 29:2979­3013, 2017.
L. J. Fogel, A. J. Owens, and M. J. Walsh. Artificial intelligence through simulated evolution. 1966.
Peter Fo¨ldiak. Forming sparse representations by local anti-hebbian learning. Biological cybernetics, 64(2):165­170, 1990.
D. Forster and J. Lu¨cke. Truncated variational EM for semi-supervised Neural Simpletrons. In IJCNN, pp. 3769­3776, 2017.
M. Henniges, G. Puertas, J. Bornschein, J. Eggert, and J. Lu¨cke. Binary sparse coding. In Proceedings LVA/ICA, LNCS 6365, pp. 450­57. Springer, 2010.
P. O. Hoyer. Modeling receptive fields with non-negative sparse coding. Neurocomputing, 52-54: 547­52, June 2003. ISSN 09252312. doi: 10.1016/S0925-2312(02)00782-8.
P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning Research, 5:1457­69, 2004.
E. R. Hruschka, R. JGB Campello, A. A. Freitas, et al. A survey of evolutionary algorithms for clustering. IEEE Trans. on Systems, Man, and Cybernetics, 39(2):133­155, 2009.
M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. Machine Learning, 37:183­233, 1999.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
I. Loshchilov and F. Hutter. CMA-ES for hyperparameter optimization of deep neural networks. In ICLR Workshop, pp. 513­520, 2016.
J. Lu¨cke. Truncated variational expectation maximization. arXiv preprint, arXiv:1610.03113, 2016.
J. Lu¨cke and J. Eggert. Expectation truncation and the benefits of preselection in training generative models. JMLR, 11:2855­900, 2010.
J. Lu¨cke and M. Sahani. Maximal causes for non-linear component extraction. Journal of Machine Learning Research, 9:1227­67, 2008.
J. W. Myers, K. B. Laskey, and K. A. DeJong. Learning Bayesian networks from incomplete data using evolutionary algorithms. In Proc. Annual Conference on Genetic and Evolutionary Computation, pp. 458­465, 1999.
R. Neal and G. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan (ed.), Learning in Graphical Models. Kluwer, 1998.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311­3325, 1997.
M. Opper and O. Winther. Expectation consistent approximate inference. JMLR, 6:2177­04, 2005.
A. B. Patel, T. Nguyen, and R. G. Baraniuk. A probabilistic theory of deep learning. In NIPS, pp. 2558­2566, 2016.
F. Pernkopf and D. Bouchaffra. Genetic-based em algorithm for learning gaussian mixture models. IEEE Trans. on Pattern Analysis and Machine Intelligence, 27(8):1344­1348, 2005.
E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-scale evolution of image classifiers. In ICML, pp. 2902­2911, 2017.
9

Under review as a conference paper at ICLR 2018
I. Rechenberg. Cybernetic solution path of an experimental problem. 1965. S. Roweis. EM algorithms for PCA and SPCA. NIPS, pp. 626­32, 1998. T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to
reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. L. K. Saul and M. Jordan. Exploiting tractable substructures in intractable networks. NIPS, pp.
486­492, 1996. J. Schmidhuber. Deep learning in neural networks. Neural networks, 61:85­117, 2015. A.-S. Sheikh, J. A. Shelton, and J. Lu¨cke. A truncated EM approach for spike-and-slab sparse
coding. Journal of Machine Learning Research, 15:2653­2687, 2014. J. A. Shelton, J. Gasthaus, Z. Dai, J. Lu¨cke, and A. Gretton. Gp-select: Accelerating em using
adaptive subspace preselection. Neural Computation, 29(8):2177­2202, 2017. M. W. Spratling. Pre-synaptic lateral inhibition provides a better architecture for self-organising
neural networks. Network: Computation in Neural Systems, 10:285 ­ 301, 1999. M. W. Spratling, K. De Meyer, and R. Kompass. Unsupervised learning of overlapping image
components using divisive input modulation. Computational Intelligence and Neuroscience, pp. 1­19, 2009. ISSN 1687-5265. K. O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computing, 10(2):99­127, 2002. ISSN 1063-6560. M. Suganuma, S. Shirakawa, and T. Nagao. A genetic programming approach to designing convolutional neural network architectures. In GECCO, pp. 497­504, 2017. M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society. Series B, 61, 1999. J. H. van Hateren and A. van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. Proceedings of the Royal Society of London B, 265: 359­66, 1998.
10

Under review as a conference paper at ICLR 2018

APPENDIX

A: NOISY-OR

The truncated free energy takes on the following form for Noisy-OR:

FNOR(K, ) = N log (1 - h) + log

exp F

h n sK(n)

F (s, ) := sh log
h
+ ydn log
d

h 1 - h
1 h(1 - Wdhsh) - 1

+ log(1 - Wdhsh)
h

The M-step equations for noisy-OR are obtained by taking derivatives of the free energy, equating them to zero and solving the resulting set of equations. We report the results here for completeness:

where

hnew

=

1 N

sh qn

n

Wdnhew = 1 +

n(ydn - 1) Ddh(s) qn n Cdh(s) qn

Ddh(s)

:=

Wdh(s)sh Nd(s)(1 - Nd(s))

(11) (12)

Cdh(s) := Wdh(s)Ddh(s)

(13)

Wdh(s) := (1 - Wdh sh )
h =h
The update rule for  is quite straightforward. The update equations for the weights Wdh, on the other hand, do not allow a closed form solution (i.e. no exact M-step equation can be derived). The rule presented here, instead, expresses each Wdnhew as a function of all current W ; this is a fixedpoint equation whose fixed point would be the exact solution of the maximization step. Rather than solving the equation numerically at each step of the learning algorithm, we exploit the fact that in practice one single evaluation of 13 is enough to (noisily, not optimally) move towards convergence. Since TV-EM is guaranteed to never decrease F, drops of the free-energy during training can only be ascribed to this fixed-point equation; this provides a simple mechanism to check and possibly correct for misbehaviors of 13 if needed.

B: M-STEP UPDATE RULES FOR BSC

The free energy for BSC follows from inserting (10) into (2). Update rules can be obtained by optimizing the resulting expression separately for the model parameters , 2 and W (compare,
e.g., Henniges et al., 2010). For the sake of completeness, we show the result here:

1N H

= N

sh qn

n=1 h=1

(14)

2 = 1 N || y (n) - W s ||2

ND

qn

n=1

(15)

11

Under review as a conference paper at ICLR 2018

W=

N

y (n)

s

T qn

n=1

N
ss T qn
n =1

-1

(16)

Exact EM can be obtained by setting qn to the exact posterior p(s | y (n), ). As this quickly becomes computational intractable with higher latent dimensionality, we approximate exact posteriors by truncated variational distributions (3). For BSC, the truncated free energy (4) takes the form



F (K, ) = - N D log 22 +N H log (1 - )+ log 

exp log p(y(n), s|)  (17)

2

n sKn

where

log

p(y,

s|)

=

-

1 22

(y

-

W

s)T

(y

-

W

s)

+

|s|

log

 1-

(18)

C: FURTHER EXPERIMENTAL RESULTS FOR BSC

-220

-240

F(, K) / N

-260

-280
-300
-320 0

randparents-randflips fitparents-crossover-randflips fitparents-sparseflips fitparents-cross-sparseflips randparents-cross-sparseflips
Iteration50 100 150 200 250 300

Figure 8: Results of the experiment with artificial data (10x10 bars) for the BSC model. Depicted is the evolution of the free energy for different EAs averaged over 20 independent runs. Dots and vertical errorbars show the mean and the standard deviation, respectively.

Figure 9: Full dictionary learned from natural images by the BSC model trained with the "fitparentscross-sparseflips" EA. Depicted is the dictionary at iteration 4,000. The generative fields are ordered according to their activation, starting with most active fields.
12

Under review as a conference paper at ICLR 2018

D: SPARSITY-DRIVEN BITFLIPS
When performing sparsity-driven bitflips, we flip each bit of a particular child s with probability p0 if it is 0, with probability p1 otherwise. We call pbf the average probability of flipping any bit in s. We impose the following constraints on p0 and p1:
· p1 = p0 for some constant  · the average number of on bits after mutation is set at s

which yield the following expressions for p0 and p1:



=

(H

-

|s|) · ((Hpbf ) - (s - (s - |s| + Hpbf )|s|

|s|))

p0

=

H

H pbf + ( - 1)|s|

p1 =  · p0

Trivially, random uniform bitflips correspond to the case p0 = p1 = pbf .

E: RELIABILITY OF EEM FOR NOISY-OR ON OVERLAPPING BARS
With respect to the tests shown in figure 4 and discussed in section 5.1, it is worth to spend a few more words on comparisons with the other algorithms shown (Lu¨cke & Sahani, 2008, Fig. 9). Quantitative comparison to NMF approaches, neural nets (DI Spratling et al., 2009), and MCA (Lu¨cke & Sahani, 2008) shows that EMM for noisy-OR performs well but there are also approaches with higher reliability. Of all the approaches which recover more than 15 bars on average, most require additional assumptions. E.g., all NMF approaches, non-negative sparse coding (Hoyer, 2004) and R-MCA2 require constraints on weights and/or latent activations. Only MCA3 does not require constraints and presumably neither DI. DI is a neural network approach, which makes the used assumptions difficult to infer. MCA3 is a generative model with a max-non-linearity as superposition model. For learning it explores all sparse combinations with up to 3 components. Applied with H = 32 latents, it hence evaluates more than 60000 states per data point per iteration for learning. For comparison, EEM for noisy-OR evaluates on the order of S = 100 states per data point per iteration.

F: HIGHER-SCALE NATURAL IMAGE PATCHES FOR NOISY-OR

Figure 10: Generative fields learned running EEM for noisy-OR ("fitparents-sparseflips") for 175 iterations with H = 200 latent variables. Learned crowdedness H was 1.6.

13

