Under review as a conference paper at ICLR 2018
DEEP FUNCTION MACHINES: GENERALIZED NEURAL NETWORKS FOR TOPOLOGICAL LAYER EXPRESSION
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper we propose a generalization of deep neural networks called deep function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly infinite) dimension and we show that a family of DFMs are invariant to the dimension of input data; that is, the parameterization of the model does not directly hinge on the quality of the input (eg. high resolution images). Using this generalization we provide a new theory of universal approximation of bounded non-linear operators between function spaces. We then suggest that DFMs provide an expressive framework for designing new neural network layer types with topological considerations in mind. Finally, we introduce a novel architecture, RippLeNet, for resolution invariant computer vision, which empirically achieves state of the art invariance.
1 INTRODUCTION
In recent years, deep learning has radically transformed a majority of approaches to computer vision, reinforcement learning, and generative models [Schmidhuber (2015)]. Theoretically, we still lack a unified description of what computational mechanisms have made these deeper models more successful than their wider counterparts. Substantial analysis by Shalev-Shwartz et al. (2011), Raghu et al. (2016), Poole et al. (2016) and many others gives insight into how the properties of neural architectures, like depth and weight sharing, determine the expressivity of those architectures. However, less studied is the how the properties of data, such as sample statistics or geometric structure, determine the architectures which are most expressive on that data.
Surprisingly, the latter perspective leads to simple questions without answers rooted in theory. For example, what topological properties of images allow convolutional layers such expressivity and generalizeability thereon? Intuitively, spatial locality and translation invariance are sufficient justifications in practice, but is there a more general theory which suggests the optimality of convolutions? Furthermore, do there exist weight sharing schemes beyond convolutions and fully connected layers that give rise to provably more expressive models in practice? In this paper, we will more concretely study the data-architecture relationship and develop a theoretical framework for creating layers and architectures with provable properties subject to topological and geometric constraints imposed on the data.
The Problem with Resolution. To motivate a use for such a framework, we consider the problem of learning on high resolution data. Computationally, machine learning deals with discrete data, but frequently this data is sampled from a continuous process. For example, audio is inherently a continuous function f : [0, tend]  R, but is sampled as a vector v  R44,100×t. Even in vision, images are generally piecewise smooth functions f : R2  R3, but are sampled as tensors v  Rx×y×c. Performing tractible machine learning as the resolution of data of this type almost always requires some lossy preprocessing like PCA or Discrete Fourier Analysis [Burch (2001)]. Convolutional neural networks avoid dealing therein by intutively assuming a spacial locality on these vectors. However, one wonders what is lost through the use of various dimensionality reduction and weight sharing schemes1.
1Note we do not claim that deep learning on high resolution data is currently intractible or ineffective. The problem of resolution is presented as an example in which topological constaints can be imposed on a type of data to yield new architecutres with desired, provable properties.
1

Under review as a conference paper at ICLR 2018

Figure 1: Left: A discrete vector v  Rl×w representation of an image. Right: The true continuous function f : R2  R from which it was sampled.
A key observation in discussing a large class of smooth functions is their simplicity. Although from a set theoretic perspective, the graph of a function consists of infiniteley many points, relatively complex algebras of functions can be described with symbolic simplicity. A great example are polynomials: the space of all square (x2) mononomials occupies a one-dimensional vector space, and one can generalize this phenomena beyond these basic families. Thus we will explore what results in embracing the assumption that a signal is really a sample from a continuous process, and utilize the analytic simplicity of certain smooth functions to derive new layer types.
Our Contribution. First, we extend neural networks to the infinite dimensional domain of continuous functions and define deep function machines (DFMs), a general family of function approximators which encapsolates this continuous relaxation and its discrete counterpart. Thereafer, we survey and refocus past analysis of neural networks with infinitely (and potentially uncountably) many nodes2 with respect to the expresiveness the maps that they represent. We show that DFMs not only admit most other infinite dimensional neural network generalizations in the literature but also provide the necessary language to solve two long standing questions of universal approximation raised following Stinchcombe (1999). With the framework firmly established, we then return to our motivating goal of provable deep learning and show that DFMs naturally give rise to neural networks which are provably invariant to the resolution of the input, and indeed that DFMs can be used more generally to construct architectures (e.g. those with convolutions) with provable properties given topological assumptions. Finally we experimentally verify such constructions by introducing a new type of layer, WaveLayers, apart from convolutions.

2 BACKGROUND

In order to propose deep function machines we must establish what it means for a neural network act directly on continuous functions. Recall the standardMcCulloch & Pitts (1943) feed-forward neural network.
Definition 2.1 (Discrete Neural Networks). We say N : Rn  Rm is a (discrete) feed-forward neural network iff for the following recurrence relation is defined for adjacent layers  ,

N : y = g W T y ; y0 := x

(2.1)

where W is a weight tensor and g is a non-polynomial activation function.

Suppose that we wish to map one space of functions to another with a neural network. Consider the model of N as the number of neurons for every layer becomes uncountable. The index for each neuron then becomes real-valued, along with the weight and input vectors. The process is roughly depicted in Figure 2. The core idea behind the derivation is that as the number of nodes in the network becomes uncountable we need apply a normalizing term to the contribution of each node in the evaluation of the following layer so as to avoid saturation. Eventually this process resembles Lebesgue integration.
More formally, let N be an L layer neural network as given in Definition 2.1. Without loss of generality we will examine the first layer, = 1. Let us denote  : X  R  R as some
2See related work.

2

Under review as a conference paper at ICLR 2018

Figure 2: Left: Resolution refinement of an input signal by simple functions. Right: An illustration of the extension of neural networks to infinite dimensions. Note that x  RN is a sample of f (N), a simple function with f (N) -   0 as N  . Furthermore, the process is not actually countable,
as depicted here.

arbitrary continuous input function for the neural network. Likewise consider a real-valued piecewise integrable weight function, w : R2  R, for a layer which is composed of two indexing variables3
u, v  E , E  R. In this analysis we will restrict the indices to lie in compact sets E , E .

If f is a simple function then for some finite partition of E , say u0 < · · · < un, then f =

n m=1

[um-1 ,um ] pn

where

for

all

u



[um-1,

um],

pn



(u).

Visually

this

is

a

piecewise

constant

function underneath the graph of . Suppose that some vector x is sampled from , then we can make

x a simple function by taking an arbitray parition of E so that: when u0 < u < u1, f (u) = x0, and

when u1 < u < u2, f (u) = x1, and so on. This simple function f is essentially piecewise constant

on intervals of uniform length so that on each interval it attains the value of the nth component, xn.

Finally if wv is some simple function approximating the v-th row of some weight matrix W in the same fashion, then wv · f is also a simple function. Therefore particular neural layer associated to f

(and thereby x) is

n

y1 = g(W T x) = g

Wmvxmµ([um-1, um]) = g

m=1

wv(u)f (u) dµ(u) ,
E

(2.2)

where µ is the Lebesgue measure on R.
Now suppose that there is a refinement of x; that is, returning to our original problem, there is a higher resolution sample of  say f (and thereby x ), so that it more closely approximates . It then follows that the cooresponding refined partition, u0 < · · · < uk, (where k > n), occupies the same E but individually, µ([um-1, um])  µ([um-1, um]). Therefore we weight the contribution of each xn less than each xn, in a measure theoretic sense.
Recalling the theory of simple functions without loss of generality assume , (·, ·)  0. Then we yield that if

Fv = {(wv, f ) : E  R | f, wv simple, 0  f  , 0  wv   (·, v)}

(2.3)

then it follows immediately that

sup wv(u)f (u) dµ(u) =  (u, v)(u) dµ(u).

(f,wv )Fv E

E

(2.4)

Therefore we give the following definition for infinite dimensional neural networks.
Definition 2.2 (Operator Neural Networks). We call O : L1(E )  L1(E ) an operator neural network parameterized by  if for two adjacent layers 

O : y (v) = g

y (u) (u, v) dµ(u) ; y0(v) = (v).

E

(2.5)

where E , E are locally compact Hausdorff mesure spaces and u  X, v  Y.

3It is no loss of generality to extend the results in this work to weight kernels indexed by arbitrary u, v  Rn, but we ommit this treatment for ease of understanding.

3

Under review as a conference paper at ICLR 2018

3 DEEP FUNCTION MACHINES

With operator neural networks defined, we endeavour to define a topologically inspired framework for developing expressive layer types. A powerful language of abstraction for describing feedforward (and potentially recurrent) neural network architectures is that of computational skeletons as introduced in Daniely et al. (2016). Recall the following definition.
Definition 3.1. A computational skeleton S is a directed asyclic graph whose non-input nodes are labeled by activations.

Daniely et al. (2016) provides an excellent account of how these graph structures abstract the many neural network architectures we see in practice. We will give these skeletons "flesh and skin" so to speak, and in doing so pursure a suitable generalization of neural networks which allows intermediate mappings between possibly infinite dimensional topological vector spaces. DFMs are that generalization.
Definition 3.2 (Deep Function Machines). A deep function machine D is a computational skeleton S indexed by I with the following properties:

· Every vertex in S is a topological vector space X where  I.

· If nodes  A  I feed into then the activation on is denoted y  X and is defined as

y =g T y
A
where T : X  X is some affine form called the operation of node .

(3.1)

To see the expressive power of this generalization, we propose several operations T that not only encapsulate ONNs and other abstractions on infinite dimensional neural networks, but also almost all feed-forward architectures used in practice.

3.1 GENERALIZED NEURAL LAYERS

Generalized neural layers are the basic units of the theory of deep function machines, and they can be used to construct architectures of neural networks with provable properties, such as the resolution invariance we seek. The most basic case is X = Rn and X = Rm, where we should expect a standard neural network. As either X or X become infinite dimensional we hope to attain models of functional MLPs from Rossi et al. (2002) or infinite layer neural networks from Globerson & Livni (2016) with universal approximation properties.
Definition 3.3 (Generalized Layer Operations). We suggest several natural generalized layer families T for DFMs as follows.

· T is said to be o-operational if and only if X and X are spaces of integrable functions over locally compact Hausdorff measure spaces, and

T [y ](v) = o(y )(v) = y (u) (u, v) dµ(u).
E
For example4, X , X = C(R), yields operator neural networks.

(3.2)

· T is said to be n-discrete if and only if X and X are finite dimensional vector spaces,

and

T [y ] = n(y ) = W T y .

(3.3)

For example, X = Rn, X = Rm, yields standard feed-forward neural networks.

· T is said to be f-functional if and only if X is some space of integrable functions as mentioned previously and X is a finite dimensional vector space, and

T [y ] = f(y ) =  (u)y (u) dµ(u)
E

(3.4)

4Nothing precludes the definition from allowing multiple functions as input, the operation must just be carried on each coordinate function.

4

Under review as a conference paper at ICLR 2018

N [0, 1]
n R6 n R12 n R3

O C([-1, 1])
o C([0, 1])
o C (R)

D1
[0, )
fff n
C(R) C(R) C(R)

od C(R) Rm

d Rn nn
Rm

of f

f

C (R)

Figure 3: Examples of three different deep function machines with activations ommited and T replaced with the actual type. Left: A standard feed forward binary classifier (without convolution), Middle: An operator neural network. Right: A complicated DFM with residues.

For example 5 X = C(R), X = Rn, yields functional MLPs.

· T is said to be d-defunctional if and only if X is a are finite dimensional vector space and X is some space of integrable functions.

Tl[y ](v) = d(y )(v) =  (v)T y

(3.5)

For example, X = Rn, X = C(R).

The naturality of the above layer operations come from their universality and generality.

3.2 RELATED WORK AND A UNIFIED VIEW OF INFINITE DIMENSIONAL NEURAL NETWORKS
Operator neural networks are just one of many instantiations of DFMs. Before we show universality results for deep function machines, it should be noted that there has been substantial effort in the literature to explore various embodiments of infinite dimensional neural networks. To the best of the authors' knowledge, DFMs provide a single unified view of every such proposed framework to date.
In particular, Neal (1996) proposed the first analysis of neural networks with countably infinite nodes, showing that as the number of nodes in discrete neural networks tends to infinity, they converge to a Gaussian process prior over functions. Later, Williams (1998) provided a deeper analysis of such a limit on neural networks. A great deal of effort was placed on analyzing covariance maps associated to the Guassian processes resultant from infinite neural networks with both sigmoidal and Gaussian activation functions. These results were based mostly in the framework of Bayesian learning, and led to a great deal of analyses of the relationship between non-parametric kernel methods and infinite networks, including Le Roux & Bengio (2007), Seeger (2004), Cho & Saul (2011), Hazan & Jaakkola (2015), and Globerson & Livni (2016).
Out of this initial work, Hazan & Jaakkola (2015) define hidden layer infinite layer neural networks with one or two layers which map a vector x  Rn to a real value by considering infinitely many feature maps w(x) = g ( w, x ) where w is an index variable in Rn. Then for some weight function u : Rn  R, the output of an infinite layer neural network is a real number u(w) w(x) dµ(w). This approach can be kernelized and therefore has resulted further theory by Globerson & Livni (2016) that aligns neural networks with Gaussian processes and kernel methods. Operatator neural networks differ significantly in that we let each w be freely paramaterized by some function  and require that x be a continuous function on a locally compact Hausdorf space. Additionally no universal approximation theory is provided for infinite layer networks directly, but is cited as following from the work of Le Roux & Bengio (2007). As we will see, DFMs will not only encapsulate (and benefit from) these results, but also provide a general universal approximation theory therefor.
5Note that y (u) is a scalar function and  is a vector valuued function of dimension dim(X ). Additionally this definiton can easily be extended to function spaces on finite dimensional vectorspaces by using the Kronecker product.

5

Under review as a conference paper at ICLR 2018

Table 1: Unification of Infinite Dimensional Neural Network Theory

Name
Infinite NNs
Functional MLPs

Form

b+

 j=1

vj

h(x;

uj

)

p i=1

ig

wi dµ

DFM N : Rn -n i=1R -n Rm
F : L1(R) -f Rp -n R

Continuous NNs
Nonparametric Continuous
NNs

1(u)g(x · 0(u)) du C : Rn -d L1([a, b]) -f Rm

1(u)g( x, u ) du

C : Rn -d L1(R) -f Rm

Infinite same as non-parametric

Layer NNs

continuous NNs

same as non-parametric continuous NNs

Authors
Neal (1996); Williams (1998)
Stinchcombe (1999); Rossi et al. (2002)
Le Roux & Bengio (2007)
Le Roux & Bengio (2007)
Globerson & Livni (2016);
Hazan & Jaakkola (2015)

Another variant of infinite dimensional neural networks, which we hope to generalize, is the func-

tional multilayer perceptron (Functional MLP). This body of work is not referenced in any of the

aforementioned work on infinite layer neural networks, but it is clearly related. The fundamental idea

is that given some f  V = C(X), where X is a locally compact Hausdorff space, there exists a

generalization of neural networks which approximates arbitrary continuous bounded functionals on

V (maps f  a  R). These functional MLPs take the form

p i=1

ig

i(x)f (x) dµ(x) . The

authors show the power of such an approximation using the functional analysis results of Stinchcombe

(1999) and additionally provide statistical consistency results defining well defined optimal parameter

estimation in the infinite dimensional case.

Stemming additionally from the initial work of Neal (1996), the final variant called continuous neural
networks has two manifestations: the first of which is more closely related to functional perceptrons
and the last of which is exactly the formulation of infintie layer NNs. Initially Le Roux & Bengio (2007) proposes an infinite dimensional neural network of the form 1(u)g(x · 0(u)) dµ(u) and shows universal approximation in this regime. Overall this formulation mimics multiplication by some weighting vector as in infinite layer NNs, except in the continuous neural formulation 0 can be parameterized by a set of weights. Thereafter, to prove connections between gaussian processes from a different vantage, they propose non-parametric continuous neural networks, 1(u)g(x · u) dµ(u), which are exactly infinite-layer neural networks.

In the view of deep function machines, the foregoing variants of infinite and semi-infinite dimensional neural networks are merely instantiations of different computational skeleton structures. A summary of the unified view is given in Table 1.

3.3 APPROXIMATION THEORY OF DEEP FUNCTION MACHINES
In addition to unification, DFMs provide a powerful language for proving universal approximation theorems for neural networks of any depth and dimension6. The central theme of our approach is that the approximation theories of any DFM can be factored through the standard approximation theories of discrete neural networks. In the forthcoming section, this principle allows us to prove two approximation theories which have been open questions since Stinchcombe (1999).
The classic results of Cybenko (1989), yields the theory for n-discrete layers. For f-functional layers, the work of Stinchcombe (1999) proved in great generality that for certain topologies on C(E ), two layer functional MLPs universally approximate any continuous functional on C(E ). Following Stinchcombe (1999), Rossi et al. (2002) extended these results to the case wherein multiple
6By dimension, we mean both infinite and finite dimensional neural networks.

6

Under review as a conference paper at ICLR 2018

o-operational layers prepended f-functional layers. We will show in particular that o-operational and similarly d-defunctional layers alone are dense in the much richer space of uniformly continuous bounded operators on function space. We give three results of increasing power, but decreasing transparency.
Theorem 3.4 (Point Approximation). Let [a, b]  R be a bounded interval and g : R  B  R be a continuous, bijective activation function. Then if  : E  R and f : E  B are L1(µ) integrable functions there exists a unique class of o-operational layers such that g  o[] = f.

Proof. We seek a class of weight kernels  so that that o[] = f. Let  (u, v) = (g-1)  (h((u), v)) h ((u), v) where (u) is the indefinite integral of . Define h so that
it satisfies the following two equivalent equations µ-a.e.

h((b), v) - h((a), v) = f (v) h(x, v)
(u) = 0 x x=(u),v=v,u{a,b}

(3.6)

The proof is completed in the appendix.

The statement of Theorem 3.4 is not itself very powerful; we merely claim that o-operational layers

can at least map any one function to any other one function. However, the proof yields insight

into what the weight kernels of o-operational layers look like when the single condition   f is

imposed. Therefrom, we conjecture but do not prove that a statstically optimal initialization for

training o-operational layers is given by satifying

(3.6) when 

=

1 n

m n=1

n,

f

=

1 n

m n=1

fn,

where the training set {(n, fn)} are drawn i.i.d from some distribution D.

Theorem 3.5 (Nonlinear Operator Approximation). Suppose that E1, E2 are bounded intervals in R.
For all , , if K : Lip(E1)  Lip(E3) is a uniformly continuous, nonlinear operator, then for every > 0 there exists a deep function machine

D : L1(E1) o L1(E2) o L1(E3)

(3.7)

such that D|Lip - K < .
With two layer operator networks universal, it remains to consider d-deconvolutional layers. Theorem 3.6 (Nonlinear Basis Approximation). Suppose I, E2, E3 are compact intervals, and let C(X) denote the set of analytic functions on X. If B : In  C(E3) is a continuous basis map to analytic functions then for every > 0 there exists a deep function machine

D : Rn d L1(E2) o L1(E3)

(3.8)

such that D|In - B < in the topology of uniform convergence.
To the best of our knowledge, the above are the first approximation theorems for nonlinear operators and basis maps on function spaces for neural networks. The proofs in the appendix roughly involve a factorization of arbitrary DFMs through approximation theories for n-discrete layers.
Essentially, the factorization works as follows. In both of the foregoing theorems we want to roughly approximate some nonlinear map K with a DFM D. We therefore define an operator |K|, called an affine projection, that takes functions, converts them into piecewise constant approximations, applies K, and then again converts the result to piecewise constant approximations. Since there are a finite number, say N and M , of pieces given in the input and the output of |K| respectively, we can define an operator K~ : RN  RM , called a lattice map, which in some sense reproduces |K|. We then show both Theorem 3.5 and Theorem 3.6 by approximating K~ with a discrete neural network, N , and chosing D to be such that its discreitzation is N . Surprisingly, this principle holds for any DFM structure and a large class of different K not just those which use piecewise constant approximations!

7

Under review as a conference paper at ICLR 2018

4 NEURAL TOPOLOGY FROM TOPOLOGY

As we have now shown, deep function machines can express arbitrarily powerful configurations of 'perceptron layer' mappings betwen different spaces. However, it is not yet theoretically clear how different configurations of the computaional skeleton and the particular spaces X do or do not lead to a difference in expressiveness of DFMs. To answer questions of structure, we will return to the motivating example of high-resolution data, but now in the language of deep function machines.

4.1 RESOLUTION INVARIANT NEURAL NETWORKS

If an an input (xj)  RN is sampled from an continuous function   C(E ), o-operational layers are a natural way of extending neural networks to deal directly with . As before, it is useful to think of each o as a continuous relaxation of a class of n, and from this perspective we can gain insight into the weight tensors of n-discrete layers as the resolution of x increases.
Theorem 4.1 (Invariance). If T is an o-operational layer with an integrable weight kernel (u, v) of O(1) parameters, then there is a unique fully connected n-discrete layer, N , with O(N ) parameters so that T [](j) = N (x)j for all , x as above.

Theorem 4.1 is a statement of variance in parameterization; when the input is a sample of a smooth signal, fully connected n-discrete layers are naively overparameterized.

Resolution Invariance Schema

 (u, v; w) =

n k=1

f

(u,

v;

wk )

DFMs therefore yield a simple resolution invariance scheme

parameterization

for neural networks. Instead of placing arbitrary restrictions on W like convolution or assuming that the gradient descent

O : C(E )

o

C(E )

will implicitly find a smooth weight matrix or filter W for n,

n-discrete inst.

we take W to be the discretization of a smooth  (u, v). An immediate advantage is that the weight surfaces,  (u, v), [O]n : RN

n

RM

of o-operational layers can be parameterized by dense fam-

ilies f (u, v; w), whose parameters w do not depend on the resolution of the input but on the complexity of the model being learnt.

Figure 4: DFM construction of resolution invariant n-discrete layer.

4.2 TOPOLOGICALLY INSPIRED LAYER PARAMETERIZATIONS

Furthermore, we can now explore new parameterizations by constructing weight tensors and thereby neural network topologies which approximate the action of the operator neural networks which most expressively fit the topological properties of the data. Generally new restrictions on the weights of discrete neural networks might be achieved as follows:

1. Given that the input data x is sampled from some f  F  {g : E0  R}, find a closed algebra of weight kernels so that 0  A0 is minimally parameterized and g  o[F ] is a sufficiently "rich" class of functions.
2. Repeat this process for each layer of a computational skeleton S and yield a DFM O.
3. Instantiate a deep function machine [O]n called the n-discrete instatiation of O consisting of only n-discrete layers by discretizing each o-operational layer through the resolution invariance schema sample:

W= w

ij

kt

, ,··· , , ,···

eu1 eu2

ev1 ev2

ij...kt...

(4.1)

where e denotes the cardinality of the sample along the -axis of E = [0, 1]dim(E ). This process is depicted in Figure 4.

This perspective yields interpretations of existing layer types and the creation of new ones. For example, convolutional n-discrete layers provably approximate o-operational layers with weight kernels that are solutions to the ultrahyperbolic partial differential equation.

8

Under review as a conference paper at ICLR 2018

Figure 5: The WaveLayer architecture of RippLeNet for MNIST. Bracketed numbers denote number of wave coefficients. The images following each WaveLayer are the example activations of neurons after training given by feeding 0 into RippLeNet.

Theorem 4.2 (Convolutional Neural Networks). Let N be an n-discrete convolutional layer such that n(x) = h x where is the convolution operator and h is a filter tensor. Then there is a o-operational layer, O with  (u1, . . . , un, v1, . . . , vn) such that

n 2 k=1 2uk

=

c2

n k=1

2 2vk

(4.2)

and its n-discrete instatiation is [O ]n = N .

Using Theorem 4.2 we therefore propose the following generalization of convolutional layers with weight kernels satisfying (4.2) whose n-discrete instantiation is resolution invariant.
Definition 4.3 (WaveLayers). We say that T is a WaveLayer if it is the n-discrete instantiation (via the resolution invariance schema) of an o-operational layer with weight kernel of the form

b
 (u, v) = s0 + si cos(wiT (u, v) - pi); si, pi  R, wi  R2dim(E ).
i=1

(4.3)

WaveLayers7 are named as such, because the kernels  are super position standing waves moving
in directions encoded by wi, offset in phase by pi. Additionally any n-discrete convolutional layer can be expressed by WaveLayers, setting the direction i of wi to i = /4. In this case, instead of learning the values h at each j, we learn si, wi, pi.

5 EXPERIMENTS
With theoretical guarantees given for DFMs, we propose a series of experiments to test the learnability, expressivity, and resolution invariance of WaveLayers.
RippLeNet. To find a baseline model, we performed a grid search on MNIST over various DFMs and hyperparameters, arriving at RippLeNet, an architecture similar to the classic LeNet-5 of LeCun et al. (1998) depicted in Figure 5. The model is the n-discrete instatiation of the following DFM

(5.1)
and consiststs of 5 successive wave layers with tanh activations and no pooling. We found that using ReLu often resulted in a failure of learning to converge. Changes in the activation shapes (eg. 24x24x2  10x10x2) are achieved merely by specifying the sample partition of E at each node of the DFM. The trainable parameters, in particular the magnitude of internal frequencies, were initialized at offsets proportional to the number of waves comprising each o-operational layer. Likewise, orientation of each wave was initialized uniformly on the unit spherical shell.
7Note: WaveLayers are not not the same as Fourier networks or FC layers with cos activation functions. It suffices to view wave layers as simply another way to reparameterize the weight matrix of n-discrete layers, and therefore the VC dimension of WaveLayers is less than that of normal fully connected layers. See the appendix.
9

Under review as a conference paper at ICLR 2018

Model Expressive Parameter Reduction. In Theorem 4.1, it was shown that DFMs in some sense parameterize the complexity of the model being learned, without constraints imposed by the form of data. RippLeNet benefits in that the sheer number of parameters (and therefore the variance) can be reduced until the model expresses the mapping to satisfactory error without concern for resolution variants.

We emprically verify this methodology by fixing the model architecture and increasing the number of waves per layer uniformly. This results in an exponential marginal utility on lowest error achieved after 80 epochs of MNIST with respect Figure 6: A plot of test error versus number of to the number of parameters in the model, shown trainable parameters for RippLeNet in comparison in Figure 6. The slight outperformance of early with early LeNet architectures. LeNet architectures, suggest that future work in optimizing WaveLayers might be fruitful in achieving state of the art parameter reduction.

Resolution Invariance. True resolution invariance has the desirable property of consistency. Principly, consistency requires that regardless of the the resolution complexity of data, the training time, paramerization, and testing accuracy of a model do not vary.

Figure 7: A plot of training time (normalized for each layer type with respect to the training time for 28 × 28 baseline) as the resolution of MNIST scales.

We test consistency for RippLeNet by fixing all aspects of initialization save for the input resolution of images. For each training run, we rescale MNIST using both bicubic and nearest neighbor scaling to square resolutions of sidelength R = {16, . . . , 36, 38, 46, 64, . . . }. In conjuction we compare the resolution consistency of fully connected (FC) and convolutional architectures. For FC models, the number of free parameters on the first layer is increased out of necessity. Likewise, the size of the input filters for convolutional models is varied. As shown in Figure 7, WaveLayers remain invariant to resolution changes in both multirun variance and normalized convergence iterations, whereas both FC and convolutional layers exhibit an increase in both measurements with resolution.

6 CONCLUSION

In this paper we proposed deep function machines, a novel framework for topologically inspired layer parameterization. We showed that given topological assumptions, DFMs provide theoretical tools to yield provable properties in neural neural networks. We then used this framework to derive WaveLayers, a new type of provably resolution invariant layer for processing data sampled from continuous signals such as images and audio. The derivation of WaveLayers was additionally accompanied by the proposal of several layer operations for DFMs between infinite and/or finite dimensional vector spaces. We for the first time proved a theory of non-linear operator and functional basis approximation for neural networks of infinite dimensions closing two long standing questions since Stinchcombe (1999). We then utilized the expressive power of such DFMs to arrive at a novel architecture for resolution invariant image processing, RippLeNet.

10

Under review as a conference paper at ICLR 2018
Future Work. Although we've layed the ground work for exploration into the theory of deep function machines, there are still many open questions both theoretically and empirically. The drastic outperformance in resolution variance of RippLeNet in comparision to traditional layer types suggests that new layer types via DFMs with provable properties in mind should be further explored. Furthermore a deeper analysis of existing global network topologies using DFMs may be useful given their expressive power.
REFERENCES
Carl Burch. A survey of machine learning. A survey for the Pennsylvania Governor's School for the Sciences, 2001.
Youngmin Cho and Lawrence K Saul. Analysis and extension of arc-cosine kernels for large margin classification. arXiv preprint arXiv:1112.3712, 2011.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2:303­3314, 1989.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. arXiv preprint arXiv:1602.05897, 2016.
Amir Globerson and Roi Livni. Learning infinite-layer networks: beyond the kernel trick. arXiv preprint arXiv:1606.05316, 2016.
Tamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from infinite neural networks. arXiv preprint arXiv:1508.05133, 2015.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. 2007.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115­133, 1943.
Radford M Neal. Bayesian learning for neural networks, volume 118. 1996.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems, pp. 3360­3368, 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Fabrice Rossi, Brieuc Conan-Guez, and François Fleuret. Theoretical properties of functional multi layer perceptrons. 2002.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85­117, 2015.
Matthias Seeger. Gaussian processes for machine learning. International Journal of neural systems, 14(02):69­106, 2004.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623­1646, 2011.
Maxwell B Stinchcombe. Neural network approximation of continuous functionals and continuous functions on compactifications. Neural Networks, 12(3):467­477, 1999.
Christopher KI Williams. Computation with infinite neural networks. Neural Computation, 10(5): 1203­1216, 1998.
11

Under review as a conference paper at ICLR 2018

7 APPENDIX A: ADDITIONAL TOY DEMONSTRATION

To provide a direct comparison between convolutional, fully connected, and WaveLayer operations on data with semicontinuity assumptions, we conducted several toy demonstrations. We construct a dataset of functions (similar to a dataset of audio waveforms) whose input pairs are Gaussian bump functions, Bµ(u) centered at different points in the interval µ  [0, 1]. The cooresponding "labels" or target outputs are squres of gaussian bump functions plus a linear form whose slopes are given by the position of the center, that is Bµ(u)2 + µ(u - µ). The desired map we wish to learn is then T : Bµ(u)  Bµ(u)2 + µ · (u - µ). To construct the actual dataset D, for a random sample of centers µ we sample the input output pairs over 100 evenly spaced sub-intervals. The resultant dataset is a list of N pairs of input/output vectors D = {(xi, yi)}iN=1 with x, y  R100.
We then train three different two layer DFMs with n-discrete convolutional, fully-connected, and WaveLayers respectively. The following three figures show the outputs of all three layer types as training progresses. In the first three quadrants, the output of each layer type on a particular example datapoint is shown along with that example's input/target functions, (xi, yi). The particular example shown is chosen randomly at the beginning of training. In the bottom right, the log training error over the whole dataset of each layer type is shown. A tick is the number of batches seen by the algorithm.

In initialization, the three layers exhibit predicted behavior despite artifacts towards the boundaries of the intervals. The convolutional layer, acts as a mullifer smoothing the input signal as its own kernel is Gaussian. The fully connected layer generates as predicted a normally distributed set of different output activations and does not regard the spatial locality (and thereby continuity) of the input. Finally the WaveLayer output exhibits behaviour predicted in Neal (1996), that is it limits towards a smoothed random walk over the input signal.

As training continues, both the convolutional and WaveLayer outputs preserve the continuity of

the input signal, and approximate the smoothness of the output signal as induced by their own

relation to ultrahyperbolic differential equations. Since the FC layer is not restricted to any given

topology, although it approximates the desired output signal closely in the L2 norm, it fails to

achieve smoothness as this regularization is not explicity coded. It is important to note that in this

example the WaveLayer output immediately surpasses the accuracy of the convolutional output

because the convolutional output only has bias units accross entire channels, whereas the bias units

of WaveLayers are themselves functions

n k=1

sk cos(wk

·

v

+

pk )

+

bk .

Therefore

WaveLayers

can impose hetrogenously accross their output signals, where as convolutional require much deeper

architectures to artifically generate such biases.

12

Under review as a conference paper at ICLR 2018
The results of this toy demonstration illustrate the intermediate flexibility of WaveLayers between purely fully connected and convolutional architectures. Satisfying the same differential equation (4.2), convolutional and WaveLayer architectures are regularized by spatial locality, but WaveLayers can in fact go beyond convolution layers and employ translational hetrogeneity. Although the purpose of this work is not to demonstrate the superiority of either convoluitional or WaveLayer architectures, it does open a new avenue of exploration in neural architecture design using DFMs to design layer types using topological constraints.
13

Under review as a conference paper at ICLR 2018

8 APPENDIX B: THEOREMS AND PROOFS

8.1 POINT APPROXIMATION
Theorem 8.1. Let [a, b]  R be a bounded interval and g : R  B  R be a continuous, bijective activation function. Then if  : E  R and f : E  B are L1(µ) integrable functions there exists a unique class of o-operational layers such that g  o[] = f.

Proof. We will give an exact formula for the weight function  cooresponding to o so that the formula is true. Recall that

y (v) = g

(u) (u, v) dµ(u) .

E

(8.1)

Then let  (u, v) = (g-1)  (h((u), v)) h ((u), v) where (u) is the indefinite integral of  and h : R × E  R is some jointly and seperately integrable function. By the bijectivity of g

onto its codomain,  exists. Now further specify h so that, h((u), v)

= f (v). Then by the

uE

fundamental theorem of (Lebesgue) calculus and chain rule,

g(o[](v)) = g

(g-1)  (h((u), v)) h ((u), v)(u) dµ(u)

E

= g g-1 (h((u), v))

= f (v)

uE

A generalization of this theorem to E  Rn is given by Stokes theorem.

(8.2)

8.2 DENSITY IN LINEAR OPERATORS
Theorem 8.2 (Approximation of Linear Operators). Suppose E , E are -compact, locally compact, measurable, Hausdorff spaces. If K : C(E )  C(E ) is a bounded linear operator then there exists an o-operational layer such that for all y  C(E ), o[y ] = K[y ].

Proof. Let t : C(E )  R be a linear form which evaluates its arguments at t  E ; that is, t(f ) = f (t). Then because t is bounded on its domain, t  K = K t : C(E )  R is a bounded linear functional. Then from the Riesz Representation Theorem we have that there is a unique regular
Borel measure µt on E such that

Ky (t) = K t y = y (s) dµt(s),
E
µt = K t

(8.3)

We will show that  : t  K t is continuous. Take an open neighborhood of K t, say V  [C(E )], in the weak* topology. Recall that the weak* topology endows [C(E )] with smallest collection of open sets so that maps in i(C(E ))  [C(E )] are continuous where i : C(E )  [C(E )] so that i(f ) = f^ =   (f ),   [C(E )]. Then without loss of generality

m

V=

f^-n1(Un )

n=1

where fn  C(E ) and Un are open in R. Now -1(V ) = W is such that if t  W then

K t 

m 1

f^-n1(Un ).

Therefore

for

all

fn

then

Kt(fn )

=

t(K[fn ])

=

K[fn ](t)



Un .

We would like to show that there is an open neighborhood of t, say D, so that D  W and (Z)  V .

First since all the maps K[fn ] : E

 R are continuous let D =

m 1

(K

[fn

])-1

(Un

)



E

.

Then if r  D, f^n [K r] = K[fn ](r)  Un for all 1  n  m. Therefore (r)  V and so

(D)  V .

14

Under review as a conference paper at ICLR 2018

As the norm ·  is continuous on [C(E )], and  is continuous on E , the map t  (t) is continuous. In particular, for any compact subset of E , say F , there is an r  F so that (r) is
maximal on F ; that is, for all t  F , µt  µr . Thus µt µr.

Now we must construct a borel regular measure  such that for all t  E , µt . To do so, we

will decompose E into a union of infinitely many compacta on which there is a maximal measure.

Since E

is a -compact locally compact Hausdorff space we can form a union E

=

 1

Un

of

precompacts Un with the property that Un  Un+1. For each n define n so that Un\Un-1 µt(n)

where µt(n) is the maximal measure on each compact cl(Un) as described in the above paragraph.

Finally let  =

 n=1

n.

Clearly



is

a

measure

since

every

n

is

mutually

singular

with

m

when

n = m. Additionally for all t  E , µt .

Next by the Lebesgue-Radon-Nikodym theorem, for every t there is an L1() function Kt so that dµt(s) = Kt(s) d(s). Thus it follows that

K y (t) = y (s)Kt(s) d(s)
E
= y (s)K(t, s) d(s) = o[y ](t).
E
By letting  = K we then have K = o up to a -null set and this completes the proof.

(8.4)

8.3 DENSITY IN NON-LINEAR OPERATORS
Theorem 8.3. Suppose that E1, E2 are bounded intervals in R. If K : Lip(E1)  Lip(E3) is a uniformly continuous, nonlinear operator. Then for every > 0 there exists a deep function machine

D : L1(E1) o L1(E2) o L1(E3)

(8.5)

such that D|Lip - K < .

We will first introduce some defintions which quantize uniformly continuous operators on function space.

Definition 8.4. Let P = p1 < · · · < pN be some partition of a compact interval E with N components. We call P : Lip(E)  RN and P : RM  Lip(E) affine projection maps if

P (f ) = (f (pi))Ni=1

N -1

P (v) = v 

Pi (x)

i=0

(vi+1 - vi) µ(Pi)

(t

-

pi)

+

vi

(8.6)

where Pi is the indicator function on Pi = [pi, pi+1) when i < N and PN-1 = [pN-1, pN ].
Definition 8.5. Let P, Q be partitions of E1, E3 of N, M components respectively. If K : Lip(E1)  Lip(E3), its affine projection, |K|, and its lattice map, K~ , are defined so that the following diagram commutes,

Lip(E1) P RN P Lip(E1)

|K| K~

K

Lip(E3) Q RM Q Lip(E3)

Lemma 8.6 (Strong Linear Approximation). If K : Lip(E1)  Lip(E3) is a uniformly continuous, nonlinear operator, then for every > 0 there exist partitions P, Q of E1, E3 so that K - |K| < .

Proof. To show the lemma, we will chase the commutative diagram above by approximation.

For any  > 0, approximates f

we claim ; that is,

that f-

there exists P  P  f

a P such L1(µ) <

that for any f . To see this,

 Lip(E1), the affine projection take P to be a uniform partition of

15

Under review as a conference paper at ICLR 2018

E1

with

p

:=

µ(Pi)

<

 µ(E1

)

.

Then

N -1
|f - P  P  f | dµ 
i=1

f (t) -
Pi

(f

(pi) - f (pi µ(Pi)

))

(t

-

pi

)

+

f

(pi

)

N -1

 |f (t) - f (pi) - (t - pi)| dµ(t)
i=1 Pi

N -1
 2|t - pi| dµ(t)  p2N < .
i=1 Pi

dµ(t)

Now by the absolute continuity of K, for every > 0 there is a  and therefore a partition P of E1 so that if f - P  P  f L1(µ) <  then K[f ] - K[P  P  f ] L1(µ) < /2. Finally let Q be a uniform partition of E3 so that for every   Lip(E3),  - Q  Q   L1(µ) < /2. It follows that for every f  Lip(E1)
K[f ] - |K|[f ] L1(µ)  K[f ] - K  P  P [f ] + K[f ] - Q  Q  K[f ]
< + =. 22
Therefore the affine projection of K approximates K. This completes the proof.

With the lemma given we will approximate nonlinear operators through an approximation of the affine approximation using n-discrete DFMs.

Proof of Theorem 3.5. Let > 0 be given. By Lemma 8.6 there exist partitions, P, Q, so that K - |K| < /2. The cooresponding lattice map K~ : RN  RM is therefore continuous. Since E1 is a compact interval, the image P [Lip(E1)] is compact and homeomorphic to the unit hypercube [0, 1]N . By the universal approximation theorem of Cybenko (1989), for every , there exists a deep function machine
N : RN n RJ n RM ,

so that K~ - N  < . Then, the continuity of the affine projection maps implies that there exist  such that Q  N  P - |K| < /2. Therefore the induced operator on N represents K; that is, Q  N  P - K < .
Let N be parameterized by W 1  RN×J and W 2  RJ×M . Let S be any uniform partition of an I = [0, 1] with J components. Then parameterize a deep function machine D with weight kernels

NJ

1(u, v) =

Sj×Pi (u, v)Wi1,j (u - pi),

i=1 j=1

M -1

J

2(v, x) =

Qk (x)

k=1

j=1

Wj2,k+1 - Wj2,k µ(Qk )

(x

-

qk )

+

Wj2,k

(v - sj),

where  is the dirac delta function. We claim that D = Q  N  P . Performing routine computations, for any f  Lip(E1),

D[f ] = T2  g 

f (u)1(u, v) dµ(u)

E1


NJ



= T2  g  

Sj×Pi (u, v)Wi1,j f (u)(u - pi) dµ(u)

E1 i=1 j=1


J



= T2  g   P (f )T Wj1Sj (v) := T2  g  h(v)

j=1

16

Under review as a conference paper at ICLR 2018

Thus, h is identical to the jth neuron of the first n-discrete layer in N  P when v = sj. Turning to T2 in D, we get that

D[f ] = 2(v, x)g(h(v)) dµ(v)
I

M -1

J

= Qk (x)

k=1

j=1

Wj2,k+1 - Wj2,k µ(Qk )

(x

-

qk )

+

Wj2,k

g(h(sj ))

M -1
= Qk (x) · g(P (f )T W 1)T
k=1

Wk2+1 - µ(Qk

Wk2 )

(x

-

qk

)

+

Wk2

= Q (g(P (f )T W 1)T W 2) = Q  N  P [f ].

Therefore D|Lip - K < and this completes the proof.

We will now prove a similar theorem for d-defunctional layers.
Theorem 8.7 (Nonlinear Basis Approximation). Suppose I, E2, E3 are compact intervals, and let C(X) denote the set of analytic functions on X. If B : In  C(E3) is a continuous basis map to analytic functions then for every > 0 there exists a deep function machine

D : Rn d L1(E2) o L1(E3)

(8.7)

such that D|In - B < in the topolopgy of uniform convergence.

Proof. Recall that the set of polynomials on E3, P, are a basis for the vector space C(E3). Therefore the map B has a decomposition through nmaots ,  so that the following diagram commutes


Rn B

1(R)

C  (E3 )

and  : (ai)i=1  angn where gn is the mononomial of degree n. The existence of  can be verified through a composition of the direct product of basis projections in C(E3) and B.

For each m maos factor

N into a

the projection image countable collection

in of

mthaepsm(th ic:ooRrndinatRe,)i=m1[so[Rthnat]]

= R and so again the

 i=1

i

=

.

We

will

approximate B by approximations of    via increasing products of i.

Define the aforementioned increasing product map (N) as

N

(N) = i ×

c0

i=1 N +1

where c0 is the constant map. Now with > 0 given, we wish to show that there exists an N so that   (N) - B < in the topology of uniform convergence.

To see this let PN  P denote the set of polynomials of degree at most n. Next we define a open 'mullification' of PN . In particular let

OPN ( ) = {f  C( ) | |f - g| < , g  PN } .

It C

is clear that OPN1 ( )  (E3) we have that {OPi

(O)P}iN=21(

) is

when N1  N2 an open cover of

and furthermore by the density of P in B[In]  C(E3). Since In is compact

B[In] is a compact subset of C(E3) and thus there is a finite index set I = {N1, . . . Nk} so that

tI OPt  B[In]. If N = max I then OPN ( )  B[In]. Therefore for every x  In we have

that   (N) - B < since   (N) is a polynomial of degree at most N.

17

Under review as a conference paper at ICLR 2018

Now we will filter the maps N := 1...N  (N) where N : Rn  RN through the universal approximation theory of standard discrete neural networks. Let N be a two n-discrete layer DFM so that N - N < . For convienience let N := n2  gn1 Then we can instantiate N as the DFM in (8.7) using the same method as in the proof of the nonlinear operator approximation theory above.
For the d-defunctional layer let Wj1k be the weight tensor of n1 in N so that n1(x)1j = k Wj1kxk. Then let the weight kernel for d be 1k(v) = (W·1k). and then d[x]|v=j = n1(x)j1. We will ommit the design of weight kernels for the o-operational layer, but this is not difficult to establish. All together we now have that via the approximation of N and equivalence of N and its instantiation in the statement of the theorem,
    o  g  d -   N  N < .
Finally we need deal with the basis map . On the compact set (N)[In] = N  N [In],  is a bounded linear operator and its composition,     o is also a bounded linear operator. Therefore by the bounded linear approximation theorem of o-operational layers, there is a o -     o < . Appending such o to d as above we achieve the approximation bound of the theorem. This completes the proof.

8.4 RESOLUTION INVAIRANCE
Theorem 8.8. If T is an o-operational layer with an integrable weight kernel (u, v) of O(1) parameters, then there is a unique n-discrete layer with with O(N ) parameters so that o[](j) = n[x]j for all indices j and for all , x as above.

Proof. Given some o, we will give a direct computation of the corresponding weight matrix of n. It follows that

o[](v) = (u)  (u, v) dµ(u)

E

N -1 n+1
= ((xn+1 - xn)(u - n) + xn)  (u, v) dµ(u)
nn

N -1

n+1

n+1

= (xn+1 - xn)

(u - n) (u, v) dµ(u) + xn

 (u, v) dµ(u)

nn

n

(8.8)

Now, let Vn(v) =

nn+1(u - n) (u, v) dµ(u) and Qn(v) =

n+1 n



(u, v)

dµ(u);

We

can

now

easily simplify (8.8) using the telescoping trick of summation.

N -1
o()[v] = xN VN-1(v) + xn (Qn(v) - Vn(v) + Vn-1(v)) + x1 (Q1(v) - V1(v))
n=2

(8.9)

Given indices in j  {1, · · · , M }, let W  RN×M so that Wn,j = (Qn(j) - Vn(j) + Vn-1(j), WN,j = VN-1(j), and W1,j = Q1(j) - V1(j). It follows that if W parameterizes some n, then n[x]j = o[](j) for every f sampled/approximated by x and . Furthermore, dim(W )  O(N ), and n is unique up to L1(µ) equivalence.

8.5 CONVOLUTIONAL NEURAL NETWORKS AND THE ULTRAHYPERBOLIC DIFFERENTIAL EQUATION

Proof. A general solution to (4.3) is of the form (u, v) = F (u - cv) + G(u + cv) where F, G are second-differentiable. Essentially the shape of  stays constant in u, but the position of  varies in v. For every h there exists a continuous F so that F (j) = hj, G = 0. Let (u, v) = F (u - cv) + G(u + cv). Therefore applying Theorem 4.1, to o parameterized by , we yield a weight matrix W so that

[o[](j) = (u) (F (u - cj) + 0) dµ(u) = (W x)j = (h x)j = n[x]j.
E0

(8.10)

18

Under review as a conference paper at ICLR 2018

This completes the proof.

Lip0

o o

Lip1 Lip1

o ··· o
oo oo
o ··· o

Lip4 Lip4

f
R10
f

9 APPENDIX C: VC DIMENSION OF DISCRETIZED OPERATOR NEURAL NETWORKS.
In order to calculate the VC dimension of DFMs contianing only discretized o-operational layers, denoted D, we have D  N, where N is the family of all DFMs with n-discrete skeletons whose per-node dimensionality is exactly that of the discretization D. Thus the VC dimension of F can be bounded by that of N, however more fine tuned estimate is both possible and essential.
Suppose that in designing some deep architecture, one wishes to keep VC dimension low, whilst increasing per-node activation dimensionality. In practice optimization in higher dimensions is easier when a low dimensional parameterization is embedded therein. For example, hyperdimensional computing, sparse coding, and convolutional neural networks naturally neccessitate high dimensional hidden spaces but benefit from regularized capacity. Since the dimensionality of the discretization O does not depend on the original dimensionality of the space, then the capacity of O depends directly on the "complexity" of the family of weight surfaces there endowed. It would therefore be convenient to answer the following question formally.
The VC Problem. Let W  L1(R2, µ) be some family of weight surfaces. Then induce OW , a family of discretized o-operational layers with OW := {[oW ]n}W W where [·]n denotes the discretization. What is V CDim(OW )?
Although in this work we do not directly attack this problem, a solution leads to another dimension of layer and architecture design beyond topological constraints. In practice, one would be able to choose which set of W to give a satisfactory generalizability condition on their learning problem.
10 APPENDIX D: ANALYTICAL DERIVATION OF CONTINUOUS ERROR BACKPROPAGATION FOR SEPERABLE WEIGHT KERNELS
With these theoretical guarantees given for DFMs, the implementation of the feedforward and error backpropagation algorithms in this context is an essential next step. We will consider operator neural networks with polynomial kernels. As aforementioned, in the case where a DFM has nodes with non-seperable kernels, we cannot give the guarntees we do in the following section. Therefore, a standard auto-differentiation set-up will suffice for DFMs with for example wave layers.
Feedforward propagation is straight forward, and relies on memoizing operators by using the separability of weight polynomials. Essentially, integration need only occur once to yield coefficients on power functions. See Algorithm 1.
10.0.1 FEED-FORWARD PROPAGATION
We will say that a function f : R2  R is numerically integrable if it can be seperated into f (x, y) = g(x)h(y). Theorem 10.1. If O is a operator neural network with L consecutive layers, then given any such that 0  < L, y is numerically integrable, and if  is any continuous and Riemann integrable input function, then O[] is numerically integrable.
19

Under review as a conference paper at ICLR 2018

Algorithm 1 Feedforward Propagation on F

Input: input function 

for l  {0, . . . , L - 1} do

for t  ZX do Calculate It = E y (j )jt dj .
end for

for s  ZY do

Calculate Cs =

ZX a

ka,sIa.

end for

Memoize y (j) = g

ZY b

jbCb

.

end for

The output is given by O[] = yL.

Proof. Consider the first layer. We can write the sigmoidal output of the ( )th layer as a function of the previous layer; that is,

y =g

w (j , j )y (jl) djl .

E

(10.1)

Clearly this composition can be expanded using the polynomial definition of the weight surface.

Hence

 ZY ZX



y = g  y (j )

kx2l,x2 jx2l jx2 dj 

E x2 x2l

 ZY = g  jx2
x2

ZX
kx2l ,x2
x2l


y (j )jx2l dj  ,
E

(10.2)

and therefore y is numerically integrable. For the purpose of constructing an algorithm, let Ix2 be the evaluation of the integral in the above definition for any given x2

It is important to note that the previous proof requires that y be Riemann integrable. Hence, with  satisfying those conditions it follows that every y is integrable inductively. That is, because y0 is integrable it follows that by the numerical integrability of all l, O[] = yL is numerically integrable.
This completes the proof.

Using the logic of the previous proof, it follows that the development of some inductive algorithm is possible.

10.0.2 CONTINUOUS ERROR BACKPROPAGATION

As is common with many non-convex problems with discretized neural networks, a stochastic gradient descent method will be developed using a continuous analogue to error backpropagation. We define the loss function as follows.

Definition 10.2. For a operator neural network O and a dataset {(n(j), n(j))} we say that the error for a given n is defined by

1 E=
2

(O(n) - n)2 djL
EL

(10.3)

This error definition follows from N as the typical error function for N is just the square norm of the difference of the desired and predicted output vectors. In this case we use the L2 norm on C(EL) in the same fashion.
We first propose the following lemma as to aid in our derivation of a computationally suitable error backpropagation algorithm.
Lemma 10.3. Given some layer, l > 0, in O, functions of the form  = g ly are numerically integrable.

20

Under review as a conference paper at ICLR 2018

Proof. If

then

 =g

y( -1)w( -1) dj
E( -1)

ZY( -1)

ZX( -1)



 =g 

jlb

ka( ,b-1)

y( -1)ja djl-2

ba

E( -1)

hence  can be numerically integrated and thereby evaluated.

(10.4) (10.5)

The ability to simplify the derivative of the output of each layer greatly reduces the computational time of the error backpropagation. It becomes a function defined on the interval of integration of the next iterated integral.
Theorem 10.4. The gradient, E(, ), for the error function (10.3) on some O can be evaluated numerically.

Proof. Recall that E over O is composed of kx,y for x  ZX , y  ZY , and 0  l  L. If we

show that

E  kx,y

can be numerically evaluated for arbitrary, l, x, y, then every component of E

is

numerically evaluable and hence E can be numerically evaluated. Given some arbitrary l in O, let

n = . We will examine the particular partial derivative for the case that n = 1, and then for arbitrary

n, induct over each iterated integral.

Consider the following expansion for n = 1,

E  1 kxL,-y n = kxL,-y 1 2

[O() - ]2 djL
E

= [O() - ] L

jLx-1jLy yL-1djL-1 djL

E E( -1)

= [O() - ] LjLy

jLx-1yL-1djL-1 djL

E E( -1)

Since the second integral in (10.6) is exactly IxL-1 from (??), it follows that

(10.6)

E  kx(n,y)

= IxL-1

[O() - ] LjLy djL
E

and clearly for the case of n = 1, the theorem holds.

(10.7)

Now we will show that this is all the case for larger n. It will become clear why we have chosen to include n = 1 in the proof upon expansion of the pratial derivative in these higher order cases.

Let us expand the gradient for n  {2, . . . , L}.

E kxL,-y n =

[O() - ]L
EL

wL-1L-1
EL-1

· · · wL-n+1)L-n+1)
EL-n+1)

n-1 iterated integrals

(10.8)

yL-njLa-njLb -n+1 djL-n . . . djL
EL-n
As aforementioned, proving the n = 1 case is required because for n = 1, (10.8) has a section of n - 1 = 0 iterated integrals which cannot be possible for the proceeding logic.

We now use the order invariance properly of iterated integrals (that is, A B f (x, y) dxdy = B A f (x, y) dydx) and reverse the order of integration of (10.8).
In order to reverse the order of integration we must ensure each iterated integral has an integrand which contains variables which are guaranteed integration over some region. To examine this, we propose the following recurrence relation for the gradient.

21

Under review as a conference paper at ICLR 2018

Let {Bs} be defined along L - n  s  L, as follows

BL = Bs =

[O() - ] LBL-1 djL,
EL
ZX ZY
 jajbB dj ,
E ab

(10.9)

BL-n =

jLx-njLy -n+1 djL-n

EL-n

such that

E  kx,y

= BL.

If we wish to reverse the order of integration, we must find a reoccurrence

relation on a sequence, {Bs} such that

E  kxL,-y n

=

BL-n

=

BL. Consider the gradual reversal of

(10.8).

Just as important as Clearly,

E =
kx,y

yL-njLx-n
EL-n

[O() - ]L
EL

wL-1L-1
EL-1

· · · jLy -n+1wL-n+1)L-n+1) djL-n+1 . . . djLdjL-n
EL-n+1)

(10.10)

is the first order reversal of (10.8). We now show the second order case with first weight function expanded.

E =
kx,y

yL-njLx-n
EL-n

ZY EL-n+1) b

ZX
ka,bjLa+-yn+1L-n+1)
a

[O() - ]L
EL

· · · jLb -n+2w(L-n+2)(L-n+2) djL-n+1 . . . djLdjL-n.
EL-n+1)

(10.11)

Repeated iteration of the method seen in (10.10) and (10.11), where the inner most integral is moved to the outside of the (L - s)th iterated integral, with s is the iteration, yields the following full reversal
of (10.8). For notational simplicity recall that l = L - n, then

E =
kx,y

y jlx
E

E

ZX
j a+y 
a

ZY ZX+2
ka,bjlb++2c +2
E +2 b c

ZY+2 ZX+3

kc,+d2jld++3e +3

E +3 d

e

ZYL-1
· · · kpL,-q 1jLq [O() - ]L
EL q

djL . . . djL-n.

(10.12)

Observing the reversal in (10.12), we yield the following recurrence relation for {Bs}. Bare in mind,

l

=

L

-

n,

x

and

y

still

correspond

with

E  kx,y

,

and

the

following

relation

uses

its

definition

on

s

for

cases not otherwise defined.

BL,t =

ZYL-1
ktL,b-1jLb [O() - ] L djL.
EL b

Bs,t =

ZY(s-1) ZX(s)

kt(,sb-1)jsa+b(s)Bs+1,a djs.

E(s) b

a

(10.13)

B= E kx,y = Bl =

ZX
ja+y Bl+2,a dj .
Ea
jlxy B dj .
E

22

Under review as a conference paper at ICLR 2018

Algorithm 2 Error Backpropagation

Input: input , desired , learning rate , time t. for  {0, . . . , L} do

Calculate  = g end for

E( -1) y( -1)w( -1) dj

For every t, compute BL,t from from (10.13). Update the output coefficient matrix kxL,-y 1 - IxL-1 EL [F () - ] LjLy djL  kxL,-y 1. for l = L - 2 to 0 do

If it is null, compute and memoize Bl+2,t from (10.13).

Compute but do not store B  R.

Compute

E  kx,y

=

Bl

from from (10.13).

Update the weights on layer l: kx,y(t)  kx,y

end for

Note that BL-n = BL by this logic.
With (10.13), we need only show that BL-n is integrable. Hence we induct on L - n  s  L over {Bs} under the proposition that Bs is not only numerically integrable but also constant.
Consider the base case s = L. For every t, because every function in the integrand of BL in (10.13) is composed of jL, functions of the form BL must be numerically integrable and clearly, BL  R.

Now suppose that Bs+1,t is numerically integrable and constant. Then, trivially, Bs,u is also numerically integrable by the contents of the integrand in (10.13) and Bs,u  R. Hence, the proposition that
s + 1 implies s holds for < s < L.

Lastly we must show that both B and Bl are numerically integrable. By induction Bl+2 must be

numerically integrable. Hence by the contents of its integrand B must also be numerically integrable

and

real.

As

a

result,

Bl

=

E  kx,y

is

real

and

numerically

integrable.

Since we have

shown

that

E  kx,y

is numerically

integrable,

E

must

therefore

be

numerically

evaluable as aforementioned. This completes the proof.

23

