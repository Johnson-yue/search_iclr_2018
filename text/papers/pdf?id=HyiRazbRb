Under review as a conference paper at ICLR 2018

DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRE-
SENTATION FROM RANDOM INITIALIZATION
Anonymous authors Paper under double-blind review

ABSTRACT

Auto-encoder is commonly used for unsupervised representation learning and for pre-training deeper neural networks. When its activation function is linear and the encoding dimension (width of hidden layer) is smaller than the input dimension, it is well known that auto-encoder is optimized to learn the principal components of the data distribution Oja (1982). However, when the activation is nonlinear and when the width is larger than the input dimension, auto-encoder behaves differently from PCA, with the ability to capture multi-modal aspects of the input distribution.

We provide a theoretical explanation for this empirically observed phenomenon,

when rectified-linear unit (ReLu) is adopted as the activation function and the

hidden-layer width is set to be large. In this case, we show that, with signifi-

cant probability, initializing the weight matrix of an auto-encoder by sampling

from a spherical Gaussian distribution followed by stochastic gradient descent

(SGD) training converges towards the ground-truth representation for a class of

sparse dictionary learning models. In addition, we can show that, conditioning

on

convergence,

the

expected

convergence

rate

is

O(

1 t

),

where

t

is

the

number

of updates. Our analysis quantifies how increasing hidden layer width helps the

training performance when random initialization is used, and how the norm of

network weights influence the speed of SGD convergence.

1 INTRODUCTION
Let x denote a vector in Rd. An auto-encoder can be decomposed into two parts, encoder and decoder. The encoder can be viewed as a composition function se  ae : Rd  Rk of a linear transformation ae : Rd  Rk, and a coordinate-wise activation function se : Rk  Rk. They are defined as
ae(x) := Wex + be with We  Rk×d, be  Rk and
se(y)j := s(yj) where s : R  R is typically a nonlinear function The decoder takes the output of encoder, se(ae(x))  Rk, and maps it back to Rd. Let xe := se(ae(x)). The decoding function, which we denote as x^, is defined as
x^(xe) := sd(Wdxe + bd) with Wd  Rd×k, bd  Rd, sd : Rd  Rd
Suppose the activation functions are fixed before training. One can intuitively view x^ as a reconstruction of the original signal/data using the hidden representation parameterized by (Wd, bd) and the code xe. The goal of training an auto-encoder is to learn the "right" network parameters, (We, be), (Wd, bd), so that x^ has low reconstruction error.
Weight tying A folklore knowledge when training auto-encoders is that, it usually works better if one sets Wd = WeT . This trick is called "weight tying", which is viewed as a way of regularizing the network parameters. With tied weights, the classical auto-encoder is simplified as
x^(se(ae(x))) = sd(W T se(W x + be) + bd)
1

Under review as a conference paper at ICLR 2018

In the rest of the manuscript, we focus on weight-tied auto-encoder with the following specific architecture:

x^(h(x)) = W T sReLu(W x + b) with sReLu(y)i := max{0, yi}

(1)

In the deep learning community, sReLu is commonly referred to as the rectified-linear (ReLu) activation. According to the above architecture, the reconstruction of x learned by our network can be parametrized by (W, b):
x^W,b(x) := W T sReLu(W x + b)
A classic measure of reconstruction error used by auto-encoders is the expected squared loss. Assuming that the data fed to the auto-encoder is i.i.d distributed according to an unknown distribution, i.e., x  p(x), the population expected squared loss is defined as

L(W, b) :=

1 2 Exp(x)

x - x^W,b(x)

2

(2)

With a distributional assumption, learning a "good representation" translates to inferring the parameters of distribution p(x) by adjusting the parameters (W, b) to minimize the squared loss function. The implicit hope is that the squared loss will provide information about what is a good representation. In other words, one hopes that squared loss will characterize what kind of network parameters are close to the latent distribution parameters.

In practice, auto-encoder, as with any other neural networks, is typically trained using gradientbased algorithms. From an optimization perspective, since the loss function is non-convex in W and is shown to have exponentially many local minima Safran & Shamir (2016), one would expect the algorithm to be stuck in local minima and only find sub-optimal solutions. However, in practice, they usually learn good representation of the data. Our goal is to demystify this phenomenon from a theoretical view.

Stochastic-gradient based training Stochastic gradient descent (SGD) is a scalable variant of gradient descent commonly used in deep learning. At every time step t, the algorithm evaluates a stochastic gradient of the population loss function (in our case, (2)) with respect to the network parameters (in our case, (W, b)) using back propagation by sampling one or a mini-batch of data points. The weight update has the following generic form

W t+1  W t - wt gt+1(W t),

with

Eg(W t) = L(W, b) (W t) W

and

bt+1

 bt - btgt+1(bt)

with

Eg(bt) =

L(W, b) (bt) b

where wt and bt are the learning rates for updating W and b respectively, typically set to be a small number or a decaying function of time t. The unbiased gradient estimate g(W t) and g(bt) can be

obtained by differentiating the empirical loss function defined on a single or a mini-batch of size m

1m m(W, b) := m

1 (x; W, b) :=
m

m

1 2

xi - x^W,b(xi)

2

i=1 i=1

Then the stochastic or mini-batch gradient descent update can be written as

(3)

W t+1

 Wt

- wt 

m(W, b) (W t) W

and

bt+1

 bt - bt 

m(W, b) (bt) b

(4)

Max-norm regularization A common trick called "max-norm regularization" Srivastava et al. (2014) or "weight clipping" is used in training deep neural networks. 1 In particular, after each step of stochastic gradient descent, the updated weights is forced to satisfy
max Wi, 2  c
i
1 The name max-norm regularization was originally proposed as a technique for low-rank matrix factorization Srebro & Shraibman (2005), where the definition is in fact not exactly the same as what is practiced in the deep learning community. We use the latter convention in our analysis.

2

Under review as a conference paper at ICLR 2018

Table 1: Organization of notations: the "parameters" are those whose value determine the perfor-

mance guarantee of auto-encoders; the "auxiliary" variables are only used to facilitate our analysis.

Model

Algorithm

Analysis

parameters

auxiliary parameters

auxiliary auxiliary

k(dictionary size) d (dimension)  (incoherence)  (inverse noise magnitude)

x
W Cj

c (norm control) c (learning rate to parameters) n (width of hidden layer)

Wt bt at(·)

o 
s,1, s,2 g(·)

for some constant c. This means the row norm of the weights can never exceed the prefixed constant
c. In practice, whenever Wi, 2 > c, the max-norm constraint is enforced by projecting the weights back to a ball of radius c.

2 PRELIMINARIES

In this section, we start by defining notations. Then we introduce a norm-controlled variant of SGD algorithm that operates on the auto-encoder architecture formalized in (1). Finally, we introduce assumptions on the data generating model.

General principle of notations We use the same notation for network parameters W, b, and for activation a(·), as in Section 1. We use s(·) as a shorthand for the ReLu activation function sReLu(·). We use capital letters, such as W , either to denote a matrix or an event, and lower case letters, such as x, for vectors. W T denotes the transpose of W . We use Ws, to denote the s-th row of W . When a matrix W is modified through time, we let W t denote the state of the matrix at time t, and Wst, for the state of the corresponding row. We use · for l2-norm of vectors and | · | for absolute value of real numbers. Matrix-vector multiplication between W and x (assuming their dimensions match)
is denoted by W x. Inner product of vectors x and y is denoted by x, y .

Organization of notations Throughout the manuscript, we introduce notations that can be divided into "model", "algorithm", and "analysis" categories according to their utility. They are organized in Table 1 to help readers interpreting our results. For example, If a reader is interested in knowing how to apply our result to parameter tuning in training auto-encoders, then she might ignore the auxiliary notations and only refer to algorithmic parameters and model parameters in Table 1, and examine how does the setting of the former is influenced by the latter in Theorem 1.

2.1 NORM-CONTROLLED SGD TRAINING
We assume that the algorithm has access to an endless stream of samples from an unknown distribution p(x), so that it evaluates stochastic gradients of the population squared-loss objective in (2) by randomly sampling data from an unknown distribution p(x). The norm-controlled SGD variant we analyze is presented in Algorithm 1 (it can be easily extended to the mini-batch SGD version, where for each update we sample more than one data points). It is almost the same as what is commonly used in practice: it random initializes the weight matrix by sampling unit spherical Gaussian, and at every step the algorithm moves towards the direction of the negative stochastic gradient with a linearly decaying learning rate.
However, there are two differences between Algorithm 1 and original SGD: first, we impose that the norm of the rows of W t be controlled; this is akin to the practical trick of "max-norm regularization" as explained in Section 1; second the update of bias is chosen differently than what is usually done in practice, which deserves additional explanation.

Comment on the setting of bias in Algorithm 1 The stochastic gradient of bias b with respect to squared loss in (2) can be evaluated by sampling a single data point and differentiate against the empirical loss in (3), can be derived as



(x; W, b) bj

= - s(aj) aj

r, Wj

(derivation can be found in (6) of the Appendix)

3

Under review as a conference paper at ICLR 2018

Algorithm 1 Norm-controlled SGD training

Input: width parameter n; norm parameter c; learning rate parameters c , to, ; total number of

iterations, tmax. Initialization of W o: For all s  [n],

Wso

c

z z

, where z  Rd, zi  N (0, 1)

Initialization of bo: Sample x  p(x); for all s  [n],

bFosind bbsssuch

that



(x;W b

o

,0)

(bs

)

=

0

(version used in analysis: bos  Ex x, Wso

(

1 c2

-

1))

while t  tmax do

W t+1



Wt

-

t 

(x,W t W

,bt

)

,

where

x



p(x)

Draw a fresh sample x  p; for all s  [n],

Find

bs

such

that



(x

;W b

t

,bt

)

(bs

)

=

0

bts+1  bs, or equivalently, bts+1 

x

1{at(x

)>0}, Wst+1

(

1 c2

- 1)

( version used in analysis bst+1



Ex

x

1{at(x

)>0}, Wst+1

(

1 c2

- 1) )

end while

Output: W ,tmax btmax

Since the gradient is noisy, the generic form of SGD suggests modifying btj using the update

bjt+1



btj

+

bt

 s(aj ) aj

r, Wj

for a small learning rate bt to mitigate noise. This amounts to stepping towards the negative gradient direction and move a little. On the other hand, since bj is a one-dimensional variable, we can directly find the next update bjt+1 as the point that sets the gradient to zero, that is, we find bj such that



(x

;W b

t,

bt)

(bj)

=

0

The closed form solution to this is to choose

bj = x 1{at(x )>0}, Wjt

1 ( Wjt

2 - 1)

This strategy, which is essentially Newton's algorithm, should perform better than gradient descent

if we have an accurate estimate of the true gradient, so it would likely benefit from evaluating the

gradient using a mini-batch of data. If, on the other hand, the gradient is very noisy, then this

method will likely not work as well as the original SGD update. Analyzing the evolvement of both W t and bt, which has dependent stochastic dynamic if we follow the original SGD update, would

be a daunting task. Thus, to simplify our analysis, we assume in our analysis that we have access to

Ex x 1{at(x )>0}, Wjt+1 (

1 Wjt+1

- 1)
2

The substitute of Wjt+1 for Wjt is to further simplify our analysis. In practice, this update can be implemented by first updating Wjt to Wjt+1, and then updating btj using Wjt+1.

2.2 A SIMPLE SPARSE DICTIONARY LEARNING MODEL

We assume that the data x we sample follows the dictionary learning model

x = (W )T s +

where W   Rd×k. k is the size of the dictionary, which we assume to be at least two (otherwise, the model becomes degenerate). The rows of W  are the dictionary items; Wj satisfies
Wj = 1, j  [k]

s  [k] is 1-sparse, with

1 P r(sj = 1) = k E = 0 and E[ T ] = I

4

Under review as a conference paper at ICLR 2018

Connection to clustering models Note that the data generating model can be equivalently viewed as a mixture model: for example, when sj = 1, it means x is of the form Wj + . When is Gaussian, the model coincides with mixture of Gaussians model, with the dictionary items being the latent locations of individual Gaussians. Thus, we adopt the concept from mixture models, and use x  Cj to indicate that x is generated from the j-th component of the distribution.
To enable our analysis, we have to impose that the noise has a bounded norm: let the incoherence between dictionary items be defined as  := maxj,i=j,i,j[k] | Wj , Wi |, we assume that the noise has bounded norm:
1 k-1 max = 2k(1 + ( - )2) for some  <  < 4k2 - 3k + 1

3 MAIN RESULTS

To formally study the convergence property of Algorithm 1, we need a measure to gauge the distance between the learned representation at time t, W t, and the ground-truth representation, W , which may have different number of rows. There are potentially different ways to go about this. The distance measure we use is

(W t, W ) := 1 k

min
s[n]

(Wst

, Wj )

with

(Wst

, Wj ) := 1 - (

j[k]

Wst Wst

, Wj )2

Note that (Wst , Wj ) is the squared sine of the angle between the two vectors, which decreases monotonically as their angle decreases, and equals zero if and only if the vectors align. Thus, mins[n] (Wst , Wj ) can be viewed as the angular distance from the best approximation in the learned hidden representations of the network, to the ground-truth dictionary item Wj . And (·, ·) measures this distance averaged over all dictionary items.

Our main result provides recovery and speed guarantee of Algorithm 1 under our data model.

Theorem 1. Suppose we have access to i.i.d. samples x  p(x), where the distribution p(x)

satisfies our model

assumption in Section 2.2.

Fix any 



(0,

n e

).

If we train auto-encoder with

norm-controlled SGD with random initialization as described in Algorithm 1, with the following

parameter setting

· The row norm of weights set to be Wst

=

c (s



[n], t) such that

1-(k-1)

1 2

-2k+



c2



2k

· If the bias update at t is chosen such that

bts+1

= Ex

x 1{at(x )>0}, Wst+1

1 ( c2

- 1)

· The learning rate of SGD is set to be t :=

c t+to

,

with

c

> 2kc and to 

192(c

)2B2(ln

n 

)2

(1

+

1 (-)2

)2

Then Algorithm 1 has the following guarantees

· The algorithm will be initialized successfully (see definition of successful initialization in Definition 1) with probability at least

1 - k exp

-n

1 [1
2

-

1

+

1 ( -

)2

]

· Conditioning on successful initialization, let  denote the sample space of all realizations of the algorithm's stochastic output, (W 1, W 2, . . . , ). Then at any time t, there exists a large subset of the sample space, F t  , with P r(F t)  1 - , such that

E[(W

t,

W

)|F

t]



(

to + 1 to + t +

1

)4

2(1

( - + (

)2 - )2)

+

(c

)2B 3

(1+

to

1 +

1

)

2c kc

+1

to

1 +t

+

1

5

Under review as a conference paper at ICLR 2018

Interpretation The first statement of the theorem suggests that the probability of successful ini-

tialization increases as the 1. width of hidden layer increases; 2. the magnitude of noise decreases

( increases); 3. the dictionary becomes more incoherent ( decreases); 4. the number of dictionary

items decreases. The second statement suggests that conditioning on a successful initialization, the

algorithm

will

have

expected

convergence

towards

W ,

measured

by

(·, ·),

of

order

O(

1 t

).

If

we

examine of form of bound on the convergence rate, we see that the rate will be dominated by the

second term, whose constant is heavily influenced by the choice of learning rate parameter c .

Explaining distributed sparse representation via gradient-based training The main advantage of gradient-based training of auto-encoders, as revealed by our analysis, is that it simultaneously updates all its neurons in parallel, in an independent fashion. During training, a subset of neurons will specialize at learning a single dictionary item: some of them will be successful while others may fail to converge to a ground-truth representation. However, since the update of each neuron is independent (in an algorithmic sense), when larger number of neurons are used (widening the hidden layer), it becomes more likely that each ground-truth dictionary will be learned by some neuron, even from random initialization.

4 RELATED WORKS
Our convergence analysis techniques are inspired by recent works that are at the intersection of stochastic optimization and unsupervised learning, where the problem is usually non-convex Ge et al. (2015); Balsubramani et al. (2013); Tang & Monteleoni (2017).
It has been noted since Oja (1982) that auto-encoders are closely related to online PCA algorithms when the width of hidden layer is smaller than the data dimension, that is, when k < d. This is not applicable to understand the performance of auto-encoders with large width, which seems to be preferred in practice. We argue that auto-encoders can be viewed as a generalized, multi-modal version of 1-PCA. This is observed from our analysis: the expected improvement of each neuron, Wjt, bears a striking similarity to that obtained for stochastic 1-PCA Balsubramani et al. (2013)
The training of auto-encoders also bears similarity with the stochastic k-means algorithm Tang & Monteleoni (2017): we may view each neuron as trying to learn a hidden dictionary item, or cluster center in k-means terminology. However, there is a key difference between k-means and autoencoders: the performance of k-means is highly sensitive to the number of clusters. If we specify the number of clusters, which corresponds to the network width n in our notation, to be larger than the true k, then running n-means will over-partition data from each component, and each learned center will not converge to the true component center (because they converge to the mean of the sub component). For auto-encoders, however, thanks to the independent update of neurons, even when n is much larger than k, the individual neurons can still converge to the true cluster center (dictionary item).

5 ANALYSIS
In our analysis, we define an auxiliary variable
(Wst , Wj) := 1 - (Wst , Wj)
Note that (·, ·) is the squared cosine of the angle between Wst and Wj, which increases as their angle decreases. Thus,  can be thought as as measuring the angular "closeness" between two vectors; it is always bounded between zero and one and equals one if and only if the two vectors align.
Our analysis can be divided into three steps. We first define what kind of initialization enables SGD to converge quickly to the correct solution, and show that when the number of nodes in the hidden layer is large, random initialization will satisfy this sufficient condition. Then we derive expected the per-iteration improvement of SGD, conditioning on the algorithm's iterates staying in a local neighborhood (Definition 4). Finally, we use martingale analysis to show that the local neighborhood condition will be satisfied with high probability. Piecing these elements together will lead us to the proof of Theorem 1, which is in the Appendix.

6

Under review as a conference paper at ICLR 2018

5.1 PART I: PERFORMANCE GUARANTEE OF INITIALIZATION

Covering guarantee from random initialization Intuitively, for each ground-truth dictionary item, we only require that at least one neuron is initialized to be not too far from it.
Definition 1. If the entries of W o is constructed by randomly sampling from N (0, 1) and rescaling the rows to have norm c > 0. Then we define the event of successful initialization as

Eo :=

j, max
i[k]

Wio , Wj

c

( - )2 1 - 2[1 + ( - )2]

Lemma 1. Suppose W o  Rn×d is constructed by drawing zi,j  N (0, 1) and setting Wio,j =

c

zij zij

, for all i  [n], j  [d]. Then for any   (0, 1)

P r{j



[k], max
i[n]

Wio

, Wj

c

1 - }  1 - k exp{-n()d-3}

Definition 2. Conditioning on Eo, we can map the rows of W o to an dictionary item Wj , j  [k], according to the following firing map 2

g : [n]  {0, 1, . . . , k} s.t. g(s) = j if Wso , Wj  c(1 - o) g(s) = 0 otherwise

with o := 1 -

1 1 + ( - )2

Note that some rows in W o may not be mapped to any dictionary item, in which case we let g(s) =
0. This means such neurons are not close (in angular distance) to any ground-truth after random initialization. Also note that for some rows Wso , there might exist multiple j  [k] such that g(s) = j according to our criterion in the definition. But when o is sufficiently small, we can show
that the assignment must be unique, in which case the mapping is well defined.

Lemma 2 (Uniqueness of firing). Suppose during training, the weight matrix has a fixed norm c.

At time t, for any row of weight matrix Wst , we denote by s,1

:= maxj

Wst c

, Wj

, and we let

W1

:= arg maxj

Wst c

, Wj

.

And we denote by s,2

:=

maxj[k],j=1 |

Wst c

, Wj

|, and W2

:=

arg maxj[k],j=1 |

Wst c

, Wj

|. Let  > 0 be the coherence parameter: maxi,j | Wj , Wi

|  .

Then for any  > , we have

s,1 

1 1 + ( - )2

=

{s,2 < s,1}

In our subsequent analysis, we will set s,1 

1 1+(-)2

,

so

that

the

uniqueness

of

firing

condition

holds. In this case, for any s  [n], with g(s) > 0, we simplify notations on measure of distance

and closeness as

ts := (Wst , Wg(s))

st := (Wst , Wg(s))

5.2 PART II: THE EVOLVEMENT OF WEIGHTS AND BIAS DURING SGD TRAINING
This section lower bounds the expected increase of st after each SGD update, conditioning on F t. We first show that conditioning on F t, the firing of a neuron s with g(s) = j, will indicate that the data indeed comes from the j-th component, which is characterized by event Et. Definition 3. At step t, we denote the event of correct firing of W t as
Et := {0  i  t, s s.t. x  Cg(s), Wsi , x + bsi > 0} {0  i  t, s s.t. g(s) > 0 and x  Cj, j = g(s), Wsi , x + bis < 0} 2Note that the firing map g is only defined for the sake of analysis; the algorithm does not have access to this information.

7

Under review as a conference paper at ICLR 2018

Definition 4. At step t, we denote the event of satisfying local condition of W t as

F t := {0  i  t, s  [n] s.t. g(s) > 0, Wsi , Wg(s)  c(1 - o)}

Lemma 3 (Correctness of firing). If at t  0, if j  [k], the network parameters (Wst , bst ) is

chosen that satisfies

bst =

1 Wst

E x1{ast-1>0}, Wst

(

1 Wst

2 - 1)

with Wst = c s.t. such that

1
1 2

- (k - 1) - 2k + 



c2



2k

Then for any t > 0, F t = Et.

Then we proceed to characterize the expected change of ts, conditioning on Et.
Theorem 2. Suppose Et holds, then after one step of stochastic gradient descent update on W t, W t+1 satisfies

version 1 version 2

E[ts+1|Et]



st {1 +

k

2t Wst

(1 - ts)} - (t)2B

ts+1

 st {1 +

k

2t Wst

(1 - st )} - tZ - (t)2B with E[Z|Et] = 0 and |Z|  B

for some constant B > 0 where B is a constant depending on the model parameter and the norm of rows of weight matrix.

5.3 PART III: CONVERGENCE OF MARTINGALES

By Theorem 2, pected increase

the sequence of the cosine

so, s1, . . of angle

. , st , . . . between

is a Wst

sub-martingale. One caveat of is and Wg(s) is conditional on Et,

that the

the excorrect

firing condition. So showing that the correct firing event indeed holds is crucial to our overall con-

vergence analysis. Since by Lemma 3, F t = Et, it suffices to show that F t holds. To this end,

note that F t's form a nested sequence

Fo  F1  ...Ft  ...

We denote the limit of this sequence as

F  := lim F t
t

So F  is the event that

{ Wst , Wj  c(1 - o), t  0, j  [k]s  [n] s.t. g(s) = j}

Theorem 3 shows that P r(F ) is in fact arbitrarily close to one, conditioning on Eo. We note that there is a line of recent work that analyze the convergence of SGD on non-convex functions

Balsubramani et al. (2013); Ge et al. (2015); Tang & Monteleoni (2017), where similar technical

difficulty arise: to show local improvement of the algorithm on a non-convex functions, one usually

needs to lower bound the probability of the algorithm entering a "bad" region, which can be saddle

points Ge et al. (2015); Balsubramani et al. (2013) or the part of solution space outside of a local

neighborhood Tang & Monteleoni (2017). Some variant of martingale concentration is usually used

for the

obtaining such result. Here, since event local neighborhood of Wg(s) defined

Ft as

can {y :

be equivalently interpreted as y, Wj  c(1 - o)}, we

Wst remains within employ a technique

similar to that in Tang & Monteleoni (2017) to show that F t holds with high probability for all t.

Theorem 3.

Fix any 



(0,

n e

).

Suppose

we

choose

t

=

c t+to

such that

c > 2kc

to



192(c

)2B2(ln

n )2(1 

+

(

1 -

)2

)2

Then conditioning on Eo, we have

P r(F ) = 1 - 

8

Under review as a conference paper at ICLR 2018
6 OPEN PROBLEMS
There are several interesting questions that are not addressed here. First, as noted in our discussion in Section 2, the update of bias as analyzed in our algorithm is not exactly what is used in original SGD. It would be interesting (and difficult) to explore whether the algorithm has fast convergence when bt is updated by SGD with a decaying learning rate. Second, our model assumption is rather strong, and it would be interesting to see whether similar results hold on a relaxed model, for example, where one may relax to 1-sparse constraint to m-sparse, or one may relax the finite bound requirement on the noise structure. Third, our performance guarantee of random initialization depends on a lower bound on the surface area of spherical caps. Improving this bound can improve the tightness of our initialization guarantee. Finally, it would be very interesting to examine whether similar result holds for activation functions other than ReLu, such as sigmoid function.
REFERENCES
Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pp. 3174­3182, 2013. URL http://papers.nips.cc/ paper/5132-the-fast-convergence-of-incremental-pca.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pp. 797­842, 2015. URL http://jmlr.org/ proceedings/papers/v40/Ge15.html.
Daniele Micciancio and Panagiotis Voulgaris. Faster exponential time algorithms for the shortest vector problem. In Proceedings of the Twenty-first Annual ACM-SIAM Symposium on Discrete Algorithms, SODA '10, pp. 1468­1480, Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics. ISBN 978-0-898716-98-6. URL http://dl.acm.org/citation. cfm?id=1873601.1873720.
Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3):267­273, Nov 1982. ISSN 1432-1416. doi: 10.1007/BF00275687. URL https: //doi.org/10.1007/BF00275687.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 774­782, 2016. URL http://jmlr.org/ proceedings/papers/v48/safran16.html.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th Annual Conference on Learning Theory, COLT'05, pp. 545­560, Berlin, Heidelberg, 2005. Springer-Verlag. ISBN 3-540-26556-2, 978-3-540-26556-6. doi: 10.1007/11503415 37. URL http://dx.doi.org/10.1007/11503415_37.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15 (1):1929­1958, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation. cfm?id=2627435.2670313.
Cheng Tang and Claire Monteleoni. Convergence rate of stochastic k-means. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1495­1503, Fort Lauderdale, FL, USA, 20­22 Apr 2017. PMLR. URL http://proceedings.mlr.press/ v54/tang17b.html.
9

Under review as a conference paper at ICLR 2018

7 APPENDIX

Derivation of stochastic gradients Upon receiving a data point x, the stochastic gradient with respect to W is a jacobian matrix whose (j, i)-th entry reads



(x; W, b)  wj  i

=

  wj  i

d i=1

1 2

[xi

-

k

wjis(

d

wjlxl + bj )]2

j=1

l=1

d kd

k

d

=

{xi
i=1

-

wjis(

j=1

l=1

wjlxl

+ bj )}{- wji

wjis(

j=1

l=1

wjlxl

+ bj)}

d kd

d

=

-

{xi
i=1

-

j=1

wjis(
l=1

wjlxl

+

bj )}

 wj  i

wjis(
l=1

wj  l xl

+

bj )

For i = i, the derivative of the second term can be written using the chain rule as

wji

  wj  i

d
s(
l=1

wj  l xl

+

bj )

=

wji

s(aj )  aj 

xi

where we let aj := wjlxl + bj , which is the activation of the j-th neuron upon receiving x in the hidden layer before going through the ReLu unit.

For i = i, the derivative of the second term can be written using product rule and chain rule as

wj  i

  wj  i

d
s(
l=1

wj  l xl

+

bj )

+

d
s(
l=1

wj  l xl

+

bj )

=

wj  i



s(aj  aj 

)

xi

+

s(aj )

Let r  Rd be the residual vector with ri := xi -

k j=1

wji

s(

d l=1

wjlxl

+

bj ).

Then

we

have



(x; W, b)  wj  i

=

d
-{
i=1

ri

wj

i



s(aj  aj 

)

xi

+ s(aj )ri }

In vector notation, the stochastic gradient of loss with respect to the j-th row of W can be written as



(x; W, b) Wj

= -( s(aj) aj

r, Wj

x + s(aj)r)

(5)

Similarly, we can obtain the stochastic gradient with respect to the j-th entry of the bias term as



(x; W, b) bj

= - s(aj) aj

r, Wj

Now

let

us

examine

the

terms

s(aj ) aj

and

r, Wj

. By property of ReLu function,

(6)

s(aj) = 0 if aj <= 0 aj 1 if aj > 0

Comment Mathematically speaking, the derivative of ReLu at zero does not exist. Here we follow
the convention used in practice by setting the derivative of ReLu at zero to be 0. In effect, the event {aj = 0} has zero probability, so what derivative to use at zero does not affect our analysis (as long as the derivative is finite).

Proof of main theorem. Consider any time t > 0. By Lemma 1, the probability of successfully initializing the network can be lower bounded by

P r(Eo)  1 - k exp

-n

1 [1
2

-

1

+

1 ( -

)2

]

10

Under review as a conference paper at ICLR 2018

Conditioning on Eo and applying Theorem 3, we get that P r(F t)  P r(F )  1 - 

Since F t = Et by Lemma 3, we can apply version 1 of Theorem 2 to get the expected increase in ts for any s such that g(s) > 0 as:

E[ts|F t-1]



st-1{1

+

k

2t-1 Wst-1

(1 - ts-1)} - (t-1)2B

Since by Eo, j  [k], there exists s  [n] such that g(s) = j. Let s(j) be any s  [n] such that g(s) = j. Then, the inequality above translates to

E[st(j)|F t-1]



st-(j1)

-

k

2t-1 Wst(j)

ts-(j1)(1 - st-(j1)) + (t-1)2B

=

ts-(j1)(1

-

to

t-1 +t-

) 1

+

(to

(c )2 +t-

1)2

B

where

2  t = 2c (1 - st(j)) = 2c (1 - st(j))  2c

k Ws(j)

kc kc

by our choice of c and by our assumption on the initial value os(j). Taking total expectation up to time t, conditioning on F t, and letting  denote a lower bound on t, we get

E[st(j)|F t]



E[st-(j1)|F t](1

-

to

t-1 +t-

) 1

+

(to

(c )2 +t-

1)2 B



E[st-(j1)|F t-1](1

-

to

 +t

-

) 1

+

(to

(c )2 +t-

1)2 B

where the last inequality is by the same argument as that in Lemma 6. This has the exact same form as in Lemma D.1 of ?. Applying it with ut := E[st(j)|Ft], a = , and b = (c )2B (note our t + to
matches their notion of t), we get

E [ts(j )

|Ft]



(

to + 1 to + t +

1

)

so(j)

+

(c 

)2B -1

(1

+

to

1 +

)+1 1

to

1 +t

+

1

By

the

upper

bound

on

t,

we

can

choose



as

small

as

2c kc

.

So

we

can

get

an

upper

expressed

in

algorithmic and model parameters as

E [st (j )

|Ft]



( to + 1 to + t +

1

)

2c kc

os(j)

+

(c )2B

2c kc

(1 + -1

1

) 2c kc

+1

1

to + 1

to + t + 1



(

to + 1

2c
) kc

to + t + 1

( - )2 2(1 + ( - )2)

+

(c )2B

2c kc

(1 + -1

to

1

) 2c kc

+1

+ 1 to

1 +t+1

because by Eo,

os(j)

=

1 - so(j)



1 - (1

-

( - )2 2(1 + ( - )2) )

=

( - )2 2(1 + ( - )2)

Finally,

E[(W t, W )|F t] = 1 k

E

min
s[n],g(s)=j

(Wst

, Wj

)



1 k

E(Wst(j) , Wj )

j[k]

j[k]

(

to + 1

2c
) kc

to + t + 1

( - )2 2(1 + ( - )2)

+

(c )2B

2c kc

(1 + -1

1

) 2c kc

+1

1

to + 1

to + t + 1



( to + 1 to + t +

)4 1

( - )2 2(1 + ( - )2)

+

(c

)2B 3

(1

+

to

1 +

2c
) kc 1

+1

to

1 +t

+

1

where the last inequality is by our requirement that c > 2kc.

11

Under review as a conference paper at ICLR 2018

7.1 PART I: PERFORMANCE GUARANTEE OF INITIALIZATION

Proof of Lemma 1. Let u =

z z

, where z

 Rd with zi

 N (0, 1).

We know that u is a random

vector on Sd-1, the d-dimensional unit sphere. For any fixed vector v  Sd-1, we have

P r( u, v  1 - h) = P r(u  Scap(v, h))

where Scap(v, h) is the surface of the spherical cap centered at v with height h. By property of

spherical Gaussian, we know that u is uniformly distributed on Sd-1. So we can directly calculate

the probability above as

P r(u



Scap(v, h))

=

µ(Scap(v, h)) µ(Sd-1)

where µ measures the area of surface. The latter ratio can be lower bounded (see Lemma 4 in the Appendix) as a function of d and h:

µ(Scap(v, h)) µ(Sd-1)



f (d, h)

=

(

h(2 - h))d-1 h
2d

Since for all row i  [n], their entries are Wio

=c

z z

, then for any ground-truth dictionary item

Wj , j  [k], we have

P r{max
i[n]

Wio , v

 c(1 - h)} = P r{i  [n], s.t. Wio , v

 c(1 - h)}

= 1 - P r{i  [n], Wio , v < c(1 - h)} = 1 - ni=1P r{ cu, v < c(1 - h)} = 1 - ni=1P r{ u, v < 1 - h}
 1 - ni=1(1 - f (d, h))  1 - exp-nf(d,h)

By union bound, this implies that

P r{j



[k], max
i[n]

Wio

, Wj

 c(1 - h)}  1 - k exp-nf(d,h)

Now by our choice of the form of lower bound on the inner product, we have

h=1- 1-

substituting this into the function f (d, h), we get a nice form

()d-1

=

()d-1 (1 +

1 - ) = ()d-3(1 +

1 - 1 -  1 - (1 - )

1 - )  ()d-3

Substituting this into the previous inequality written in terms of h completes the proof.

Lemma 4 (Lower bound on surface area of spherical cap). Let Sd-1 denote the d-dimensional

hypersphere, and let Scap(, h)  Sd-1 denote the surface of a hyper spherical cap centered at 

with height h. Let µ(·) denote measure of area of surface in d-dimensional Euclidean space. Let

f (h, d) := (

2h

-

h2)d-1

h 2d

,

then

µ(Scap(, h)) µ(Sd-1)



f (h, d)

Proof. Let Bd denote the d-dimensional unit ball in Euclidean space, and let Bcap(, h) denote the

spherical cap centered at  with height h. Then applying Lemma 4.1 of Micciancio & Voulgaris

(2010), we get

V

ol(Bcap)(, h) V ol(Bd)

>

d-1
2h - h2

h

2d

where V ol(·) denotes measure of volume in Rd. So this lower bounds the ratio between volumes between spherical cap and the unit ball. We show that we can use this to lower bound the ratio between surface areas between spherical cap and the unit ball. Since by ?, we know that the ratio between their area can be expressed exactly as

µ(Scap(, h)) µ(Sd-1)

=

1d 2 I2h-h2 (

- 2

1 ,

1 )
2

12

Under review as a conference paper at ICLR 2018

and the ratio between their volume

V

ol(Bcap)(, h) V ol(Bd)

=

1d 2 I2h-h2 (

+ 2

1 ,

1 )
2

where Ix(a, b) is the regularized incomplete beta function. By property of Ix(a, b),

Ix(a + 1, b) < Ix(a, b)

So we have

µ(Scap(, h)) µ(Sd-1)

>

µ(Bcap)(, h) µ(Bd)

>

2h -

d-1
h2

h

2d

Proof of Lemma 2. We have

cs,2 = | Wso , W2 | = |( W1 , Wso W1 + Wso - W1 , Wso W1 )T W2 |  cs,1 + Wso - W1 , Wso W1

Since We have

Wso - W1 , Wso W1 2 = c2 - c2s2,1 cs,2  cs,1 + c 1 - s2,1 = s,2  s,1 + 1 - s2,1

Since s,1 

1 1+(-)2

,

we

get

s,2  s,1 + 1 - s2,1  s,1 + s,1 ( - )2  s,1

7.2 PART II: THE EVOLVEMENT OF WEIGHTS AND BIAS DURING SGD TRAINING
Proof of Lemma 3. Conditioning on F t, we show that Et holds by induction on 0  i  t. We start by showing that Eo holds.

Base case In this case

bos = E x1{as-1>0}, Wso

1 ( c2

- 1)

=

E

x, Wso

1 ( c2 - 1)

Note that

E x1{as-1>0}, Wso = E x, Wso =

1 k

Wso , Wi

i[k]

Since F o holds, we know that the firing of neuron s is unique. Let s,1 and s,2 as defined in Lemma 2. Consider the case g(s) = j. In this case, Lemma 2 implies that

s,1

:=

max
j

Wso

, Wj

= Wso , Wj

So

Wso x + bso = (Wso )T (Wj

+

1 ) + ( c2 - 1)

1 k

Wso , Wi

i[k]

Now,

observe

that

1 c2

-

1

<

0

and

1 k

Wso , Wi



c

1 k

s,1

+

c

k

- k

1

s,2

i[k]

13

Under review as a conference paper at ICLR 2018

So we get,

Wso x + bos  cs,1 - c

+

1 ( c2

-

1 1)(c k s,1

+

k c

- k

1 s,2)

1 1 k-1  cs,1 - c + ( c2 - 1)(c k s,1 + c k s,1)

=

s,1{(1

-

k )c

- k

1

+

1

+

(k ck

-

1) }

-

c



s,1(1

-

)c

k

- k

1

-

c

which is bounded above zero since our model assumptions on and , we get

( (k

-

k 1)(1

-

)

)2



1 4k2(1 + (

- )2)

(k

-

k2 1)2(1

- )2

=

1 1 + ( - )2

1 4(k - 1)2(1 - )2



1 1 + ( - )2

=

(1 - o)2



s,1

Consider the case g(s) = j , j

=

j.

We

first

upper

bound

bo

in

this

case.

Since

1 c2

-1

<

0,

we

would like to lower bound

E x, Wso

=

1 k

Wso , Wi

i[k]



=

1 k

 

Wso

, Wj


 + Wso , Wi
i=j 





1 k

 

Wso

, Wj



-|
i=j

Wso , Wi

 |


1 k cs,1

-

k

- k

1 cs,2



1 k

cs,1(1

-

(k

-

1))

where s,1 = Wso , Wj by uniqueness according to Lemma 2 as well. On the other hand, Wso x  | Wso , Wj + |  c(s,2 + )

So

Wso x + bso  c(s,2 +

)

-

(1

-

1 c2

c ) k s,1(1

-

(k

-

1))

 c(s,1 +

)

-

(1

-

1 c2

c ) k s,1(1

-

(k

-

1))

=

cs,1(

-

(1

-

1 c2

1 )

-

(k k

-

1) )

+

c

Note

that



-

(1

-

1 c2

)

1-(k-1) k

<

0

by

our

upper

bound

assumption

on

c.

Furthermore,

since

s,1



1 - o

>

1 1 + ( - )2

=

(1 + ( - )2)



(1 -

1 c2

)(

1-(k-1) k

- )

where the last inequality is by our assumption on max ,  and c (see Lemma 8), which implies

Wso x + bso  0

Case 0 < i  t Suppose Ei holds, we show that Ei+1 holds for i  t - 1. Let x  Cj for any j  [k]. Since Ei holds, we know that

s s.t. g(s) > 0, asi > 0 iff g(s) = j

So for neurons Wsi+1 with g(s) > 0 and g(s) = j, we know that these neurons are not updated, that is,
Wsi+1 = Wsi and bis+1 = bis
This means the two conditions in Ei+1 on neurons g(s) = j = j. For neurons Wsi+1 with g(s) = j, bis+1 is set such that

bsi+1

=E

x1{asi >0}, Wsi+1

1 ( c2

- 1) =

E[x|x



Cj ], Wsi+1

1 ( c2

- 1)

=

Wsi+1, Wj

1 ( c2

-

1)

14

Under review as a conference paper at ICLR 2018

Consider the case x  Cj, we have

(Wsi+1)T x + bis+1 =

T

Wsi+1

+

1 c2

(Wsi+1)T

Wj



(1

-

1 o) c

-

c

where the last inequality is by assumption that F i+1 holds. Since we choose the norm so that

c2



2k



(1

+

(

1 -

)2)

= (1 - o)2  1 - o

It follows that (Wsi+1)T x + bis+1 > 0 holds for x s.t. x  Cj. Finally, consider the case x  Cj for j = j, j  [k]. Again, since F i+1 holds,

(Wsi+1)T Wj  1 - o =

1 1 + ( - )2

we can apply Lemma 2 to get

(Wsi+1)T x

+ bis+1  cs,2 +

T Wsi+1

+ (Wsi+1)T Wj

1 ( c2

- 1)

 cs,2 + c

+

1 cs,1( c2

-

1)



c(

-

(1

-



-

1 c2

)s,1)

Now, we show that

-

(1

-



-

1 c2

)s,1

<

0.

By our assumption on c,

1 c2

+

1 2

-

2k

+



1 - (k - 1)

+=

1 2

- 2k + 2 - 2(k - 1) 1 - (k - 1)

=

1 2

(1

-

k

+

)

+

1 2

k

-

1 2



-

2k

+

2

-

2(k

-

1)

1 - (k - 1)

=

1

-

3 2

(k

- 1) + 2(k - 1)



1

2 1 - (k - 1) 2

So which implies

1-

1 c2

-





1 2

2k(1

-

1 c2

- )2



k 2



1 2k

>

2k(1

1 + ( -

)2)

>

which in turn implies

2k

>

1

-



-

1 c2

Again, by our assumption on , combined with the inequality above, we get

s,1 > 1 - o =

1 1 + ( - )2 =

2k

which completes the proof.

>

1--

1 c2

Proof of Theorem 2. We start by proving version 1. The proof of version 2 follows directly. Let x  Cj for any j  [k]. We consider and any neuron s s.t. g(s) = j. Again, since Et holds,
ast > 0|x  Cj, g(s) = j

15

Under review as a conference paper at ICLR 2018

We now examine E[st+1|x  Cj, g(s) = j, Et]. To ease notation, we let w := Wst , w := Wj ,  := wt , r := rt, a := ats, and b := bst in the proof.

st+1

=

(

Wjt+1, w Wjt+1 2

)2

=

((w + [ r, w x + ar])T w)2 w + [ r, w x + ar] 2

The denominator reads

w 2 + 2[(rT w)(xT w) + a(rT w)] + 2[(rtw)2 x 2 + 2a(rT w)(xT r) + a2 r 2]

=

w 2(1 +

1 w

2 2[(rT w)(xT w) + a(rT w)]

+

1 w

2 2[(rT w)2

x

2 + 2a(rT w)(xT r) + a2

r

2])

Let A(w) := (rT w)(xT w) + a(rT w) and B(w) := (rT w)2 x 2 + 2a(rT w)(xT r) + a2 r 2.

11

w + [ r, w x + ar] 2 =

w 2(1 +

1 w

2

2A(w)

+

1 w

2

2B(w))

=

1-(

1 w

2

2A(w)

+

1 w

2

2B(w))

w

2{1 - (

1 w

2

2A(w)

+

1 w

2

2B(w))2}



1-(

1 w

2

2A(w)

+

1 w

2

2B(w))

w2

Let C(w) := (rT w)(xT w) + a(rT w). The numerator reads

(wT w + [(rT w)(xT w) + a(rT w)])2 = (wT w)2 + 2(wT w)C(w) + 2C(w)2

Putting these together,

tj+1 

1 w

2 {(wT w)2 + 2(wT w)C(w) + 2C(w)2}{1 - (

1 w 2 2A(w) +

1 w

2 2B(w))}

=

1 w

2 {(wT w)2

+ 2(wT w)C(w)

-

1 w

2 2A(w)(wT w)2}

(7)

+

1 w

2 {2C(w)2

-

42

A(w)C

(w w

)(wT
2

w

)3

-

23

A(w)C w

(w)
2

-

2

(wT

w)2B(w) w2

-23

B(w)C

(w w

)(wT
2

w

)

-

4

B(w)C(w) } w2

Bounding major direction of improvement Now let us focus on line (7), which can be viewed as the major direction of change guided by the stochastic gradient, while the rest terms can be viewed as noise. It equals

jt

+

(wT w)C(w) 2 w 2

-

1 w

2 tj2A(w)

=

tj

+

2 w

2

{(wT

w

)C

(w

)

-

tj

A(w)}

Note that both terms C(w) and A(w) are stochastic, since their value is determined by data we sampled from the model distribution. We continue with the key step of our analysis: bounding EC(w) and EA(w). Since

rtw = xT w - a w 2 and a = xT w + b

(rT w)(xT w) = (xT w - a w 2)(xT w) = (xT w - (xT w + b) w 2)(xT w) = wT xxT w - (xT w + b) w 2(xT w)
and a(rT w) = (xT w + b)(xT w - a(wT w))
So C(w) = (2 - w 2)(wT xxT w) - a(xT w)(wT w) - b w 2(xT w) + b(xT w) - ab(wT w)
= (2 - w 2)(wT xxT w) - (xT w)2(wT w) - b(xT w)(wT w) -b w 2(xT w) + b(xT w) - b(xT w)(wT w) - b2(wT w)

16

Under review as a conference paper at ICLR 2018

Since b is chosen such that b = (wT w)(

1 w2

- 1) conditioning on Et, after some calculation we

get

Ex{-b(xT w)(wT w) - b w 2(xT w) + b(xT w) - b(xT w)(wT w)|t}

= 2(wT w)3(1 -

1 w

2 ) + (wT w)(1 -

1 w

2 )(

w

2 - 1)

On the other hand, note that
Ex[wT xxT w|t] = wT ExxT w = wT E(w + )(w + )T w = wT (w(w)T + E T )w = wT (w(w)T + I)w = wT w + wT w

where the second inequality is by the condition that x is drawn from the j-th component, and fourth inequality is due to our model assumption on the covariance structure of . Similarly,

Ex[(xT w)2|t] = Ex[wT xxT w|t] = wT (w(w)T + I)w = (wT w)2 +  w 2

Therefore, we can bound the other part

Ex{(2 - w 2)(wT xxT w) - (xT w)2(wT w)|t} = (2 - w 2)(wT w)(1 + ) - [(wT w)3 + (wT w) w 2]

Combining, we get

Ex{C(w)|t} = (2 - w 2)(wT w)(1 + ) - [(wT w)3 + (wT w) w 2]

+2(wT w)3(1 -

1 w

2)

+

(wT w)(1

-

1 w

2 )(

w

2 - 1) - Ex[b2(wT w)|t]

Thus,

Ex

{C

(w

)|t

}

(wT w) w2

=

tj {(wT w)2[1 -

1 w

2 ] + 2 - 2

w

2+

1 w

}
2

-

Ex[b2|t]tj

Now, we turn to the term A(w), which equals

A(w) = ((x - aw)T w)(xT w + a) = [xT w - (xT w + b) w 2](2xT w + b) = (2 - 2 w 2)(xT w)2 + b(xT w)(1 - 3 w 2) - b2 w 2

So

Ex[A(w)|t] = (2 - 2 w 2)[(wT w)2 +  w 2]

+(wT w)2(1 - 3 w 2)(1 -

1 w

2 ) - Ex[b2

w

2|t]

And

Ex[A(w)|t]

jt w2

= (tj )2(

1 w

2

-

2+

w

2) + tj(2 - 2 w

2) - Ex[b2|t]jt

After more calculations, we get

Ex

{C

(w

)|t

}

(wT w) w2

-

Ex[A(w)|t]

tj w2

=

1 w

2 jt (1

-

tj )

So the expectation of line (7) is

ts + 2

1 w

2 ts(1

-

st )

Also, note that since the terms w, w, x are all bounded, the noise term must be bounded by some constant B1. Therefore, we can get a lower bound on increase of conditional expectation

E[ts+1|x  Cj, g(s) = j, Et]  st + 2

1 w

2 ts(1 - st ) -

2B1

17

Under review as a conference paper at ICLR 2018

Since

P r(x



Cj )

=

1 k

,

we

get

E[st+1|g(s)

=

j, Et]



1 k

{ts

+

2

1 w

2 ts(1 - ts) -

2B1}

k +

- k

1 E[st+1|g(s)

=

j,

Et, x



Cj

,

j

= j]

 ts + 2 k

1 w

2 st (1 - ts) - 2B1

where the last inequality holds because ast (x)  0 for x  Cj and g(s) = j (j = j ) by Et, and for neuron s such that ast (x)  0, the gradient evaluates to zero so those entries are not updated after t. Thus, since Et holds, we know that

Wst+1 = Wst |g(s) > 0, g(s) = j, and x  Cj

version 2 Let

Y :=

1 w

2 (wT w)C(w) -

1 w

4 A(w)(wT w)2

Then, from the proof of version 1, we know that

st+1  ts + 2tE[Y |Et, X  Cg(s)]P r(X  Cg(s)) - 2tE[Y |Et] + 2tY + (t)2B1

= ts + 2t

1 w

2 ts(1 - ts) + 2t(Y

- E[Y |Et]) + (t)2B1

So letting Z := Y - E[Y |Et], we know that E[Z|Et] = 0 Again, since the terms w, w, x are all bounded, there must exists B2  |Z|. Letting B := max{B1, B2} finishes the proof.

7.3 PART III: CONVERGENCE OF MARTINGALES

Proof outline of Theorem 3. Conditioning on Eo, note that F  can be written as the union of events
s[n],g(s)>0{ Wst , Wj  c(1 - o), t  0, j  [k] s.t. g(s) = j} We denote these individual events regarding neuron s by Fs, that is,
Fs := { Wst , Wj  c(1 - o), t  0, j  [k] s.t. g(s) = j}

tThhte=eu0pn(rFioosotnf\bsFotrusat+nted1g)yo,vwieshrteoarleslhnoewurtohnast.thSeinincedi(vFidsua)lcpcraonbabbeilpitayrtcitainonbeedloinwtoerabuonuinodnedofbdyisnjo, ianntdsutabksientgs Fst := { Wst , Wj  c(1 - o), 0  i  t, j  [k] s.t. g(s) = j}

We can define Est similarly, and note we can easily show that

Fst = Est

by going through Lemma 3 exactly the same. To lower bound P r(Fs), we upper bound the individual error probability P r(Fst \ Fst+1), and show that their summation vanishes for t  . The same approach is taken in Proposition 2 Tang & Monteleoni (2017), and our main idea is to adapt
the analysis there to our case. Thanks to the special form of inequality obtained in Theorem 2, we can neatly re-write the statement of its version 2 in terms of ts as

st+1

 ts{1 -

k

2t Wst

(1 - st )} + 2tZ + (t)2B

(8)

Essentially, the sufficient condition for the analysis of Proposition 2 Tang & Monteleoni (2017) to work are

· The expected decrease on the objective function st is of the form

st+1



st {1

-

t

t + to

}

+

2 t

c +

to

Z

+

( t

c +

to

)2B

(9)

18

Under review as a conference paper at ICLR 2018

· t  2, t  0 · The noise terms Z, B are bounded, and E[Z|F t] = 0.

By our choice of t, the relation in (8) satisfies the special form in (9). Since conditioning on F t,

we have

t

:=

2c (1 - st ) k Wst



2c os kc

and since conditioning on Eo,

os



1 + (1 - o)2 2



1 2

for g(s) > 0 and our choice of o in Eo. Since we also set

c  2kc

we get that t  2 always hold. On the other hand, here the noise terms are obviously bounded by our model assumption. Therefore, we only need to slightly adapt Proposition 2 Tang & Monteleoni (2017) for our purpose. The exact proof is available in the Appendix for completeness.

Complete proof of Theorem 3. We consider each Fst individually. Since the proof for each s is exactly the same, we abuse the notation F t to let it denote Fst for any fixed s. Similarly, we let t denote ts. Conditioning on F t, we know that  > 2 s.t.   t. By Lemma 5, for any  > 0, and any 0  i  t - 1, we have

E{ei+1 |F

i}



E {e{(1-

i to +i

)i |F

i}

exp{

(c

)2B

+ 2(c )2B2 }

(to + i)2 2(to + i)2

 E{e(1)i |F i-1} exp{ (c )2B + 2(c )2B2 } (to + i)2 2(to + i)2

where

(1)

=

(1

-

 to +i

),

and

the

second

inequality

is

by

Lemma

6.

For

k



1,

we

define

(0)

:= 

and

(k)

:= tk=1(1 -

to

+

(i

 -

t

+

1)

)(0)

We can similarly get, for k = 0, . . . , i,

E{e(k)i-k+1 |F i-k}  E{e(k+1)i-k |F i-(k+1)}

Since  > 0, k  1,

exp{

(k)(c )2B (to + i - k)2

+

((k))2(c )2B2 } 2(to + i - k)2

(k)

=

kt=1(1

-

to

+

 (i -

t

+

) 1)



( to

+i-k to + i

+

1 )

Since the bound is shrinking as  increases and   2,

(t0

(k) + i - k)2



( to

+i-k to + i

+ 1 )2 (to

 + i - k)2



4 (to + i)2

Recursively applying the relation until we get to the term

E{e(i)1 |F o}  E{e(i+1)o |F o} exp{ (c )2B

2(c )2B2 +}

(to + i)2 2(to + i)2

=

exp{(i+1)o}

exp{

(c )2B (to + i)2

+

2(c 2(to

)2B2 + i)2

}

Combining all these recursive inequalities with the bound on (k), we get

E{ei+1 |F i}



e(i+1) 0

i-1 4(c )2B

exp{ (
k=0

(to

+

i)2

+

42(c )2B2 2(to + i)2 )}



exp{( to to +

) 0 i

+

[(c

)2B

+

2(c )2B2 2

] (to

4i +

i)2

}

19

Under review as a conference paper at ICLR 2018

Let o := 1 - (1 - o)2. Then we can apply the conditional Markov's inequality, for any i > 0,

P r(F i \ F i+1) = P r(i+1 > o|F i)

=

P r(eii+1

>

eio |F i)



E[eii+1 |F i] ei o

Since event Eo

= {o



1+(1-o )2 2

}

implies

o

= 1 - o



1-(1-o )2 2

=

o 2

,

we

have

( to to +

) o i

-

o



o

-

o



o 2

Combining this with the upper bound on E[eii+1 |F i], we get

P r(F i \ F i+1)  exp

-i

{

o 2

-

(B

+

iB 2

2

)

4(c )2i (to + i)2

}

We

choose i

=

1 

ln

(i+1)2 

with 

=

o 4

,

and

show

that

o 2

- (B +

)iB2 4(c )2i
2 (to+i)2

is lower bounded

by .

Case 1:

B

>

i B 2 2

.

Since

to



32(c )2 o

B

,

we

get

1 2 o

- (B

+

iB2 2

)

4(c )2i (to + i)2





Case 2:

B



i B 2 2

.

We

get

1 2 o

-

(B

+

iB2 2

)

4(c )2i (to + i)2



2

-

iB2

4(c )2i (to + i)2

=

2

-

1 

ln

(1

+ 

i)2

4(c (to

)2B2i + i)2



2

-

1 

ln

(to

+ 

i)2

4(c

)2B2(to + (to + i)2

i)

Now we show Since

1 ln (to + i)2 4(c )2B2     to + i

to

+

i



to



192(c )2B2 o2

ln2

1 

ln

1 

 1, and obviously,

16(c )2B2 2



16(c )2B2

(

1 2

o

)2



1 3

,

we

can

apply

Lemma

??

with

b

=

2,

C

:=

16(c )2B2

(

1 2

o

)2

,

t

:=

to

+

i



(

3C b-1

ln

)1

2 b-1



,

and

get

4(c )2B2 2

ln

(to + i)2 

:=

2C ln t + C ln

1 

<

tb-1

=

to

+i

Or equivalently,

1 

ln

(to +i)2 

4(c )2B2 to +i



.

Thus, for both cases,

2 - (B

+

iB 2

2

)

4(c )2i (to + i)2

=



This implies

P r(F i

\ F i+1)



e-

1 

(ln

(1+i)2 

)

=

 (i + 1)2

20

Under review as a conference paper at ICLR 2018

Finally, we have


P r(i1F i \ F i+1)  P r(F i \ F i+1)  

i=1

Now recall that this holds for each s, that is, s  [n],

P r(Fs)  s

substituting

s

=

 k

for



in

the

proof

above,

and

taking

the

union

bound

completes

the

proof.

Lemma 5 (Inequality of moment generating function). Suppose the conditions of Theorem 2 hold. Then conditioning on Et (F t), we can upper bound the moment generating function of t+1 as

E[et+1 |F t]  exp{t(1 - t ) + (t)2B + 2 (t)2B2 }

t + to

2

Proof. We apply the result of Theorem 2. Rewriting version 2 of Theorem 2 using t := 1 - st , for any s  [n], we get

t+1

 t -

k

2t Wst

t(1 - t) + 2tZ + (t)2B

=

t{1 -

k

2t Wst

(1 - t)} + 2tZ + (t)2B

We

let

t

:=

2c k

(1-t Wst

)

.

Conditioning

on

F t,

the

moment

generating

function

of

the

t+1

is

upper

bounded by

E[et+1 |F t]  exp{t(1 - 2c (1 - t) ) + (t)2B}E exp{2tZ|F t} (t + to)k Wst
= exp{t(1 - t ) + (t)2B}E exp{2tZ|F t} t + to

By Theorem 2,

E{2tZ|F t} = 0 and |2tZ|  2tB

Applying Hoeffding's lemma for bounded random variable, we can bound its m.g.f. by

So, finally

E exp{2tZ|F t}  exp{ 2(t)2B2 } 2

E[et+1 |F t]  exp{t(1 - t ) + (t)2B + 2 (t)2B2 }

t + to

2

Lemma 6 (Lemma from Tang & Monteleoni (2017)). For any  > 0, E{et |F t}  E{et |F t-1}

7.4 TECHNICAL LEMMAS

Lemma 7 (Tang & Monteleoni (2017)).

For any fixed b



(1, 2].

If C



b-1 3

,





1 e

,

and

t



(

3C b-1

ln

1 

)

2 b-1

,

then

tb-1

- 2C

ln t - C ln

1 

>

0.

Proof.

Let

f

(t)

:=

tb-1

-

2C

ln

t

-

C

ln

1 

.

Taking derivative, we get f

(t)

=

(b

-

1)tb-2

-

2C t

0

when t



(

2C b-1

)

1 b-1

.

Since

ln

1 

3C b-1



3C b-1



1,

(ln

1 

)3C

2 b-1

b-1



(

2C b-1

)

1 b-1

,

it

suffices

to

show

f ((ln

1 

3C b-1

)

2 b-1

)

>

0 for

our statement

to hold.

f ((ln

1 

)3C

2 b-1

b-1

)

=

(ln

1 

3C b-1

)2

-

2C

ln{(ln

1 

)3C

2 b-1

b-1

}

-

C

ln

1 

=

(ln

1 

)2

9C 2 (b-1)2

-

4C b-1

ln(ln

1 

3C b-1

)

-

C

ln

1 

=

4C b-1

[

3 2

C

b-1

ln

1 

-

ln(

3C b-1

ln

1 

)]+C

ln

1 

[

3C (b-1)2

-1]

>

0,

where

the

first

term

is

greater

than

zero

because

x-ln(2x)

>

0 for x > 0, and the second term is greater than zero by our assumption on C.

21

Under review as a conference paper at ICLR 2018

Lemma 8. Suppose our model assumptions on parameters , c,  hold, and that our assumptions on the algorithmic parameter c in Theorem 1 holds, then

(1

+

(

-

)2)



(1

-

11 c2 )(

-

(k k

-

1)

-

)

Proof.

(1

-

1 c2

1 )(

-

(k k

-

1)

-

)

1/2 + k 1 - (k - 1) 1/2 + 2k

-

1 - (k - 1)

k

1 - (k - 1)

= 1 + 2k - 1/2 + 2k 2k 1 - (k - 1)

1 2k(1 - (k - 1)) - 1/2 - 2k =+
2k 2k(1 - (k - 1))

where

2k(1 - (k - 1)) - 1/2 - 2k

=

(2k

+

k

-

2k2

-

1 )

2

where the last term is greater than zero because

k-1

2k - 1/2

 < 4k2 - 3k + 1 < 2k2 - k

So the term

(1 -

1 1 - (k - 1) c2 )( k

- )



1 2k



max

(1 + ( - )2)

22

