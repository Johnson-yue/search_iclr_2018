Under review as a conference paper at ICLR 2018
LEARNING EFFICIENT TENSOR REPRESENTATIONS WITH RING STRUCTURE NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Tensor train (TT) decomposition is a powerful representation for high-order tensors, which has been successfully applied to various machine learning tasks in recent years. In this paper, we propose a more generalized tensor decomposition with ring structure network by employing circular multilinear products over a sequence of lower-order core tensors, which is termed as TR representation. Several learning algorithms including blockwise ALS with adaptive tensor ranks and SGD with high scalability are presented. Furthermore, the mathematical properties are investigated, which enables us to perform basic algebra operations in a computationally efficiently way by using TR representations. Experimental results on synthetic signals and real-world datasets demonstrate the effectiveness of TR model and the learning algorithms. In particular, we show that the structure information and high-order correlations within a 2D image can be captured efficiently by employing an appropriate tensorization and TR decomposition.
1 INTRODUCTION
Tensor decompositions aim to represent a higher-order (or multi-dimensional) data as a multilinear product of several latent factors, which attracted considerable attentions in machine learning (Yu & Liu, 2016; Anandkumar et al., 2014; Romera-Paredes et al., 2013; Kanagawa et al., 2016; Yang et al., 2017) and signal processing (Zhou et al., 2016) in recent years. For a dth-order tensor with "square" core tensor of size r, standard tensor decompositions are the canonical polyadic (CP) decomposition (Goulart et al., 2015) which represents data as a sum of rank-one tensors by O(dnr) parameters and Tucker decomposition (De Lathauwer et al., 2000; Xu et al., 2012; Wu et al., 2014; Zhe et al., 2016) which represents data as a core tensor and several factor matrices by O(dnr + rd) parameters. In general, CP decomposition provides a compact representation but with difficulties in finding the optimal solution, while Tucker decomposition is stable and flexible but its number of parameters scales exponentially to the tensor order.
Recently, tensor networks have emerged as a powerful tool for analyzing very high-order tensors (Cichocki et al., 2016). A powerful tensor network is tensor train / matrix product states (TT/MPS) representation (Oseledets, 2011), which requires O(dnr2) parameters and avoid the curse of dimensionality through a particular geometry of low-order contracted tensors. TT representation has been applied to model weight parameters in deep neural network and nonlinear kernel learning (Novikov et al., 2015; Stoudenmire & Schwab, 2016; Tsai et al., 2016), achieving a significant compression factor and scalability. It also has been successfully used for feature learning and classification (Bengua et al., 2015). To fully explore the advantages of tensor algebra, the key step is to efficiently represent the real-world dataset by tensor networks, which is not well studied. In addition, there are some limitations of TT including that i) the constraint on TT-ranks, i.e., r1 = rd+1 = 1, leads to the limited representation ability and flexibility; ii) TT-ranks are bounded by the rank of k-unfolding matricization, which might not be optimal; iii) the permutation of data tensor will yield an inconsistent solution, i.e., TT representations and TT-ranks are sensitive to the order of tensor dimensions. Hence, finding the optimal permutation remains a challenging problem.
In this paper, we introduce a new structure of tensor networks, which can be considered as a generalization of TT representations. First of all, we relax the condition over TT-ranks, i.e., r1 = rd+1 = 1, leading to an enhanced representation ability. Secondly, the strict ordering of multilinear products between cores should be alleviated. Third, the cores should be treated equivalently by
1

Under review as a conference paper at ICLR 2018

Figure 1: The effects of noise corrupted tensor cores. From left to right, each figure shows noise corruption by adding noise to one specific tensor core.

nd · · ·

n1 T

nk = n1

n2 · · ·

nd · · ·

rd

r1 Zd

··· rk+1

Z1 Zk nk

r2 Z2

rk ···

r3

n2 · · ·

Figure 2: A graphical representation of tensor ring decomposition.

making the model symmetric. To this end, we add a new connection between the first and the last core tensors, yielding a circular tensor products of a set of cores (see Fig. 2). More specifically, we consider that each tensor element is approximated by performing a trace operation over the sequential multilinear products of cores. Since the trace operation ensures a scalar output, r1 = rd+1 = 1 is not necessary. In addition, the cores can be circularly shifted and treated equivalently due to the properties of the trace operation. We call this model tensor ring (TR) decomposition and its cores tensor ring (TR) representations. To learn TR representations, we firstly develop a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm (Oseledets, 2011). To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented. Finally, we also propose a scalable algorithm by using stochastic gradient descend, which can be applied to handling large-scale datasets.
Another interesting contribution is that we show the intrinsic structure or high order correlations within a 2D image can be captured more efficiently than SVD by converting 2D matrix to a higher order tensor. For example, given an image of size I × J, we can apply an appropriate tensorization operation (see details in Sec. 5.2) to obtain a fourth order tensor, of which each mode controls one specific scale of resolution. To demonstrate this, Fig. 1 shows the effects caused by noise corruption of specific tensor cores. As we can see, the first mode corresponds to the small-scale patches, while the 4th-mode corresponds to the large-scale partitions. We have shown in Sec. 5.2 that TR model can represent the image more efficiently than the standard SVD.

2 TENSOR RING DECOMPOSITION

The TR decomposition aims to represent a high-order (or multi-dimensional) tensor by a sequence of 3rd-order tensors that are multiplied circularly. Specifically, let T be a dth-order tensor of size n1 ×n2 ×· · ·×nd, denoted by T  Rn1×···×nd , TR representation is to decompose it into a sequence of latent tensors Zk  Rrk×nk×rk+1 , k = 1, 2, . . . , d, which can be expressed in an element-wise
form given by

d

T (i1, i2, . . . , id) = Tr {Z1(i1)Z2(i2) · · · Zd(id)} = Tr

Zk(ik) .

k=1

(1)

T (i1, i2, . . . , id) denotes the (i1, i2, . . . , id)th element of the tensor. Zk(ik) denotes the ikth lateral slice matrix of the latent tensor Zk, which is of size rk × rk+1. Note that any two adjacent latent tensors, Zk and Zk+1, have a common dimension rk+1 on their corresponding modes. The last latent tensor Zd is of size rd × nd × r1, i.e., rd+1 = r1, which ensures the product of these matrices is
a square matrix. These prerequisites play key roles in TR decomposition, resulting in some important numerical properties. For simplicity, the latent tensor Zk can also be called the kth-core (or node). The size of cores, rk, k = 1, 2, . . . , d, collected and denoted by a vector r = [r1, r2, . . . , rd]T , are called TR-ranks. From (1), we can observe that T (i1, i2, . . . , id) is equivalent to the trace of a

2

Under review as a conference paper at ICLR 2018

sequential product of matrices {Zk(ik)}. Based on (1), we can also express TR decomposition in the tensor form, given by

r1 ,...,rd

T=

z1(1, 2)  z2(2, 3)  · · ·  zd(d, 1),

1 ,...,d =1

where the symbol `' denotes the outer product of vectors and zk(k, k+1)  Rnk denotes the (k, k+1)th mode-2 fiber of tensor Zk. The number of parameters in TR representation is O(dnr2), which is linear to the tensor order d as in TT representation.

The TR representation can also be illustrated graphically by a linear tensor network as shown in

Fig. 2. A node represents a tensor (including a matrix and a vector) whose order is denoted by the

number of edges. The number by an edge specifies the size of each mode (or dimension). The

connection between two nodes denotes a multilinear product operator between two tensors on a

specific mode. This is also called tensor contraction, which corresponds to the summation over the
indices of that mode. It should be noted that Zd is connected to Z1 by the summation over the index 1, which is equivalent to the trace operation. For simplicity, we denote TR decomposition by T = (Z1, Z2, . . . , Zd).

Theorem 1 (Circular dimensional permutation order tensor and its TR decomposition is given

invariance). Let T by T = (Z1, Z2,

.

 ..

Rn1×n2×...×nd be , Zd). If we define

Ta-kdth-

Rhanvke+1T-×·k··×=nd

×n1 ×···×nk
(Zk+1, . .

as the circularly shifted . , Zd, Z1, . . . Zk).

version

along

the

dimensions

of

T

by k, then we

A proof of Theorem 1 is provided in Appendix B.1.
It should be noted that circular dimensional permutation invariance is an essential feature that distinguishes TR decomposition from TT decomposition. For TT decomposition, the product of matrices must keep a strictly sequential order, yielding that the tensor with a circular dimension shifting does not correspond to the shifting of tensor cores.

3 LEARNING ALGORITHMS

3.1 SEQUENTIAL SVDS
We propose the first algorithm for computing the TR decomposition using d sequential SVDs. This algorithm will be called the TR-SVD algorithm. Theorem 2. Let us assume T can be represented by a TR decomposition. If the k-unfolding matrix T k has Rank(T k ) = Rk+1, then there exists a TR decomposition with TR-ranks r which satisfies that k, r1rk+1  Rk+1.

Proof. We can express TR decomposition in the form of k-unfolding matrix,



kd



T k (i1 · · · ik, ik+1 · · · id) = Tr

Zj (ij )

Zj(ij) =

j=1

j=k+1




k


k+1



vec Zj(ij), vec ZjT (ij) .

j=1

j=d

(2)

It can also be rewritten as

T k (i1 · · · ik, ik+1 · · · id) =

Zk i1 · · · ik, 1k+1 Z>k 1k+1, ik+1 · · · id ,

1 k+1

(3)

where we defined the subchain by merging multiple linked cores as Z<k(i1 · · · ik-1) =

k-1 j=1

Zj

(ij

)

and Z>k(ik+1 · · · id) =

d j=k+1

Zj

(ij

).

Hence,

we can obtain T k

= Z(2k) (Z>[2]k)T , where the

subchain Z(2k) is of size

k j=1

nj

×

r1rk+1,

and

Z[>2]k

is

of

size

d j=k+1

nj

×

r1rk+1.

Since

the

rank

of T k is Rk+1, we can obtain r1rk+1  Rk+1.

3

Under review as a conference paper at ICLR 2018

According to (2) and (3), TR decomposition can be written as

T 1 (i1, i2 · · · id) =

Z1(i1, 12)Z>1(12, i2 · · · id).

1 ,2

Since the low-rank approximation of T 1 can be obtained by the truncated SVD, which is T 1 =

UVT + E1, the first core Z1(i.e., Z1) of size r1 × n1 × r2 can be obtained by the proper

reshaping and permutation of U and the subchain Z>1 of size r2 ×

d j=2

nj

×

r1

is

obtained

by

the

proper reshaping and permutation of VT , which corresponds to the remaining d - 1 dimensions

of T . Note that this algorithm use the similar strategy with TT-SVD (Oseledets, 2011), but the

reshaping and permutations are totally different between them. Subsequently, we can further reshape

the subchain Z>1 as a matrix Z>1  Rr2n2×

d j=3

nj r1

which

thus

can

be

written

as

Z>1(2i2, i3 · · · id1) = Z2(2i2, 3)Z>2(3, i3 · · · id1).
3

By applying truncated SVD, i.e., Z>1 = UVT + E2, we can obtain the second core Z2 of size (r2 × n2 × r3) by appropriately reshaping U and the subchain Z>2 by proper reshaping of VT . This procedure can be performed sequentially to obtain all d cores Zk, k = 1, . . . , d.

As proved in (Oseledets, 2011), the approximation error by using such sequential SVDs is given by

T - (Z1, Z2, . . . , Zd) F 

d-1

Ek

2 F

.

k=1

Hence, given a prescribed relative error

p,

the

truncation

threshold



can

be

set

to

p d-1

T

F.

However, considering that E1 F corresponds to two ranks including both r1 and r2, while

Ek F , k > 1 correspond to only one rank rk+1. Therefore, we modify the truncation threshold as



k =

2 p T F / d k = 1,

p T F/ d

k > 1.

(4)

A pseudocode of the TR-SVD algorithm is summarized in Alg. 1. Note that the cores obtained by the TR-SVD algorithm are left-orthogonal, which is ZTk 2 Zk 2 = I for k = 2, . . . , d - 1.

3.2 BLOCK-WISE ALTERNATING LEAST-SQUARES (ALS)

The ALS algorithm has been widely applied to various tensor decomposition models such as CP

and Tucker decompositions (Kolda & Bader, 2009; Holtz et al., 2012). The main concept of ALS is

optimizing one core while the other cores are fixed, and this procedure will be repeated until some

convergence criterion is satisfied. Given a dth-order tensor T , our goal is optimize the error function

as

min T -
Z 1 ,...,Z d

(Z1, . . . , Zd) F .

(5)

According to the TR definition in (1), we have

T (i1, i2, . . . , id) =

Z1(1, i1, 2)Z2(2, i2, 3) · · · Zd(d, id, 1)

1 ,...,d

= Zk(k, ik, k+1)Z=k(k+1, ik+1 · · · idi1 · · · ik-1, k) ,
k ,k+1

where Z=k(ik+1 · · · idi1 . . . ik-1) =

d j=k+1

Zj

(ij

)

k-1 j=1

Zj

(ij

)

denotes

a

slice

matrix

of

subchain

tensor by merging all cores except kth core Zk. Hence, the mode-k unfolding matrix of T can be

expressed by

T[k](ik, ik+1 · · · idi1 · · · ik-1) =

Zk(ik, kk+1)Z=k(kk+1, ik+1 · · · idi1 · · · ik-1) .

k k+1

4

Under review as a conference paper at ICLR 2018

By applying different mode-k unfolding operations, we can obtain that T[k] = Zk(2)

Z=[2k]

T
, where

Z=k is a subchain obtained by merging d - 1 cores.

The objective function in (5) can be optimized by solving d subproblems alternatively. More

specifically, having fixed all but one core, the problem reduces to a linear least squares problem,

which is

min
Zk(2)

T[k] - Zk(2)

Z[=2k] T

F,

k = 1, . . . , d.

This algorithm is called TT-ALS.

Here, we propose a computationally efficient block-wise ALS (BALS) algorithm by utilizing truncated
SVD, which facilitates the self-adaptation of ranks. The main idea is to perform the blockwise
optimization followed by the separation of a block into individual cores. To achieve this, we consider merging two linked cores, e.g., Zk, Zk+1, into a block (or subchain) Z(k,k+1)  R .rk×nknk+1×rk+2 Thus, the subchain Z(k,k+1) can be optimized while leaving all cores except Zk, Zk+1 fixed. Subsequently, the subchain Z(k,k+1) can be reshaped into Z~ (k,k+1)  Rrknk×nk+1rk+2 and separated into a left-orthonormal core Zk and Zk+1 by a truncated SVD:

Z~ (k,k+1) = UVT = Zk 2 Zk+1 1 ,

(6)

where Zk 2  Rrknk×rk+1 is the 2-unfolding matrix of core Zk, which can be set to U, while Zk+1 1  Rrk+1×nk+1rk+2 is the 1-unfolding matrix of core Zk+1, which can be set to VT .
This procedure thus moves on to optimize the next block cores Z(k+1,k+2), . . . , Z(d-1,d), Z(d,1)
successively in the similar way. Note that since the TR model is circular, the dth core can also be merged with the first core yielding the block core Z(d,1).

The key advantage of our BALS algorithm is the rank adaptation ability which can be achieved simply

by separating the block core into two cores via truncated SVD, as shown in (6). The truncated rank

rk+1 can be chosen such that the approximation error is below a certain threshold. One possible choice is to use the same threshold as in the TR-SVD algorithm, i.e., k described in (4). However,
the empirical experience shows that this threshold often leads to overfitting and the truncated rank is

higher than the optimal rank. This is because the updated block Z(k,k+1) during ALS iterations is

not a closed form solution and many iterations are necessary for convergence. To relieve this problem,

we choose the truncation threshold based on both the current and the desired approximation errors,

which is

  = max T F / d, p T F / d .

A pseudo code of the BALS algorithm is described in Alg. 2.

3.3 STOCHASTIC GRADIENT DESCENT

For large-scale dataset, the ALS algorithm is not scalable due to the cubic time complexity in the target rank, while Stochastic Gradient Descent (SGD) shows high efficiency and scalability for matrix/tensor factorization (Gemulla et al., 2011; Maehara et al., 2016; Wang & Anandkumar, 2016). In this section, we present a scalable and efficient TR decomposition by using SGD, which is also suitable for online learning and tensor completion problems. To this end, we first provide the element-wise loss function, which is

L(Z1, Z2, . . . , Zd)

=

1 2

d

T (i1, i2, . . . , id) - Tr

Zk (ik )

i1 ,...,id

k=1

2

1 + 2 k

Zk (ik )

2,

(7)

where k is the regularization parameters. The core idea of SGD is to randomly select one sample T (i1, i2, . . . , id), then update the corresponding slice matrices Zk(ik), k = 1, . . . , d from each latent core tensor Zk based on the noisy gradient estimates by scaling up just one of local gradients, i.e. k = 1, . . . , d,

L  Zk (ik )

=

-

T (i1, i2, . . . , id) - Tr

d
Zk (ik )
k=1


d

T

 Zj(ij) + kZk(ik),

j=1,j=k

(8)

5

Under review as a conference paper at ICLR 2018

We employ Adaptive Moment Estimation (Adam) method to compute adaptive learning rates for each parameter. Thus, the update rule for each core tensor is given by

Zk (ik )t

=

Zkt-1 (ik )

-

 Vt

+

Mt - kZkt-1(ik),

k = 1, . . . , d,

(9)

where

Mt

=

1

Mt-1

+

(1

-

1

)



L Zkt (ik

)

denotes

an

exponentially

decaying

average

of

past

gradients

and

Vt

=

2Vt-1

+

(1

-

2

)(



L Ztk (ik

)

)2

denotes

exponentially

decaying

average

of

second

moment

of the gradients.

The SGD algorithm can be naturally applied to tensor completion problem, when the data points are

sampled only from a sparse tensor. Furthermore, this also naturally gives an online TR decomposition.

The batched versions, in which multiple local losses are averaged, are also feasible but often have

inferior performance in practice. For each element T (i1, i2, . . . , id), the computational complexity

of SGD is O(r3). If we define N =

d k=1

nk

consecutive

updates

as

one

iteration

of

SGD,

the

computational complexity per SGD iteration is thus only O(r3N ), which linearly scales to data size

and independent with the order of tensor. As compared to ALS, which needs O(N dr4 + dr6), it is

more efficient in terms of computational complexity for one iteration. The convergence condition

of SGD algorithm follows other stochastic tensor decompositions (Ge et al., 2015; Maehara et al.,

2016).

4 PROPERTIES OF TR REPRESENTATION

By assuming that tensor data have been already represented as TR decompositions, i.e., a sequence of third-order cores, we justify and demonstrate that the basic operations on tensors, such as the addition, multilinear product, Hadamard product, inner product and Frobenius norm, can be performed efficiently by the appropriate operations on each individual cores. We have the following theorems:
Theorem 3. Let T 1 and T 2 be dth-order tensors of size n1 × · · · × nd. If TR decompositions of these two tensors are T 1 = (Z1, . . . , Zd) where Zk  Rrk×nk×rk+1 and T 2 = (Y1, . . . , Yd) where Yk  R ,sk×nk×sk+1 then the addition of these two tensors, T 3 = T 1 + T 2, can also be represented in the TR format given by T 3 = (X 1, . . . , X d), where X k  Rqk×nk×qk+1 and qk = rk + sk. Each core X k can be computed by

Xk(ik) =

Zk(ik) 0 0 Yk(ik)

,

ik = 1, . . . , nk, k = 1, . . . , d.

(10)

A proof of Theorem 3 is provided in Appendix B.2. Note that the sizes of new cores are increased and not optimal in general. This problem can be solved by the rounding procedure (Oseledets, 2011).

Theorem 4. Let T  Rn1×···×nd be a dth-order tensor whose TR representation is T = (Z1, . . . , Zd) and uk  Rnk , k = 1, . . . , d be a set of vectors, then the multilinear products,
denoted by c = T ×1 uT1 ×2 · · · ×d uTd , can be computed by the multilinear product on each cores, which is

nk
c = (X1, . . . , Xd) where Xk = Zk(ik)uk(ik).
ik =1

(11)

A proof of Theorem 4 is provided in Appendix B.3. It should be noted that the computational complexity in the original tensor form is O(dnd), while it reduces to O(dnr2 + dr3) that is linear to tensor order d by using TR representation.
Theorem 5. Let T 1 and T 2 be dth-order tensors of size n1 × · · · × nd. If the TR decompositions of these two tensors are T 1 = (Z1, . . . , Zd) where Zk  Rrk×nk×rk+1 and T 2 = (Y1, . . . , Yd) where Yk  R ,sk×nk×sk+1 then the Hadamard product of these two tensors, T 3 = T 1 T 2, can also be represented in the TR format given by T 3 = (X 1, . . . , X d), where X k  Rqk×nk×qk+1 and qk = rksk. Each core X k can be computed by

Xk(ik) = Zk(ik)  Yk(ik), k = 1, . . . , d.

(12)

6

Under review as a conference paper at ICLR 2018

Figure 3: Highly oscillated functions. The left panel is f1(x) = (x + 1) sin(100(x + 1)2). The

middle

panel

is

Airy

function:

f2(x)

=

x-

1 4

sin(

2 3

x

3 2

).

The

right

panel

is

Chirp

function

f3(x)

=

sin

x 4

cos(x2).

A proof of Theorem 5 is provided in Appendix B.4. Furthermore, one can compute the inner product of two tensors in TR representations. For two tensors T 1 and T 2, it is defined as T 1, T 2 =
i1,...,id T3(i1, . . . , id), where T 3 = T 1 T 2. Thus, the inner product can be computed by applying the Hadamard product and then computing the multilinear product between T 3 and vectors of all ones, i.e., uk = 1, k = 1, . . . , d. In contrast to O(nd) in the original tensor form, the computational complexity is equal to O(dnq2 + dq3) that is linear to d by using TR representation.
Similarly, we can also compute the Frobenius norm T F = T , T in the TR representation.

5 EXPERIMENTAL RESULTS

5.1 NUMERICAL ILLUSTRATION

We consider highly oscillating functions that can be approximated perfectly by a low-rank TT
format (Khoromskij, 2015), as shown in Fig. 3. We firstly tensorize the functional vector resulting in a dth-order tensor of size n1 × n2 × · · · × nd, where isometric size is usually preferred, i.e., n1 = n2 = · · · = nd = n, with the total number of elements denoted by N = nd. The error bound (tolerance), denoted by p = 10-3, is given as the stopping criterion for all compared algorithms. As shown in Table 1, TR-SVD and TR-BALS can obtain comparable results with TT-SVD in terms
of compression ability. However, when noise is involved, TR model significantly outperforms TT
model, indicating its more robustness to noises.

Table 1: The functional data f1(x), f2(x), f3(x) is tensorized to 10th-order tensor (4×4×. . .×4). In the table, , r¯, Np denote relative error, average rank, and the total number of parameters, respectively.

f1(x) r¯ Np Time (s) TT-SVD 3e-4 4.4 1032 0.17 TR-SVD 3e-4 4.4 1032 0.17 TR-ALS 3e-4 4.4 1032 13.2 TR-BALS 9e-4 4.3 1052 4.6

f2(x) r¯ Np Time (s) 3e-4 5 1360 0.16 3e-4 5 1360 0.28 3e-4 5 1360 18.6 8e-4 4.9 1324 5.7

f3(x) r¯ Np Time (s) 3e-4 3.7 680 0.16 5e-4 3.6 668 0.15 8e-4 3.6 668 4.0 5e-4 3.7 728 3.4

f1(x) + N (0, ), SN R = 60dB r¯ Np Time (s)

1e-3 16.6 13064 0.5

1e-3 9.7 4644

0.4

1e-3 4.4 1032 11.8

1e-3 4.2 1000

6.1

We also tested TR-ALS and TR-SGD algorithms on datasets which are generated by a TR model, in which the core tensors are randomly drawn from N (0, 1). As shown in Table 2, TR-SGD can achieve similar performance as TR-ALS in all cases. In particular, when data is relatively large-scale (108), TR-SGD can achieve relative error = 0.01 by using 1% of data points only once.

Table 2: Results on synthetic data with fixed ranks r1 = r2 = · · · = 2.

Tensor size
n = 10, d = 4 n = 10, d = 6 n = 10, d = 8

TR-ALS
( = 0.01, Iteration = 19) ( = 0.01, Iteration = 10) ( = 0.05, Iteration = 9)

TR-SGD
( = 0.01, Iteration = 10 ) ( = 0.01, Iteration = 0.4 ) ( = 0.01, Iteration = 0.01 )

5.2 IMAGE REPRESENTATION BY HIGHER-ORDER TENSOR DECOMPOSITIONS
An image is naturally represented by a 2D matrix, on which SVD can provide the best low-rank approximation. However, the intrinsic structure and high-order correlations within the image is not well exploited by SVD. In this section, we show the tensorization of an image, yielding a higher-order
7

Under review as a conference paper at ICLR 2018

Iter: 50%, RSE=0.2058

Iter: 250%, RSE=0.1430

Iter: 500%, RSE=0.1270

Iter: 5000%, RSE=0.1116

Figure 4: TR-SGD decomposition with TR-ranks of 12 on the 8th-order tensorization of an image.
Iter: 50% indicates that only 50% elements are sampled for learning its TR representation. RSE indicates root relative square error Y^ - Y F / Y F .

tensor, and TR decomposition enable us to represent the image more efficiently than SVD. Given an image (e.g. `Peppers') denoted by Y of size I × J, we can reshape it as I1 × I2 × . . . × Id × J1 × J2 × . . . × Jd followed by an appropriate permutation to I1 × J1 × I2 × J2 . . . × Id × Jd and thus reshape it again to I1J1 × I2J2 × . . . × IdJd, which is a dth-order tensor. The first mode corresponds to small-scale patches of size I1 × J1, while the dth-mode corresponds to large-scale partition of whole image as Id × Jd. Based on this tensorization operations, TR decomposition is able to capture the intrinsic structure information and provides a more compact representation. As
shown in Table 3, for 2D matrix case, SVD, TT and TR give exactly same results. In contrast, for
4th-order tensorization cases, TT needs only half number of parameters (2 times compression rate) while TR achieves 3 times compression rate, given the same approximation error 0.1. It should be noted that TR representation provides significantly high compression ability as compared to TT. In
addition, Fig. 4 shows TR-SGD results on `Lena' image by sampling different fraction of data points.

Table 3: Image representation by using tensorization and TR decomposition. The number of parameters is compared for SVD, TT and TR given the same approximation errors.

Data n = 256, d = 2
Tensorization n = 16, d = 4 n = 4, d = 8 n = 2, d = 16

= 0.1 SVD TT/TR 9.7e3 9.7e3
= 0.1 TT TR 5.1e3 3.8e3 4.8e3 4.3e3 7.4e3 7.4e3

= 0.01 SVD TT/TR 7.2e4 7.2e4
= 0.01 TT TR 6.8e4 6.4e4 7.8e4 7.8e4 1.0e5 1.0e5

= 9e - 4 SVD TT/TR 1.2e5 1.2e5
= 2e - 3 TT TR 1.0e5 7.3e4 1.1e5 9.8e4 1.5e5 1.5e5

= 2e - 15 SVD TT/TR 1.3e5 1.3e5
= 1e - 14 TT TR 1.3e5 7.4e4 1.3e5 1.0e5 1.7e5 1.7e5

5.3 CIFAR-10
The CIFAR-10 dataset consists of 60000 32 × 32 colour images. We randomly pick up 1000 images for testing of TR decomposition algorithms. As shown in Table 4, TR-SVD outperforms TT-SVD in terms of compression rate given the same approximation error, which is caused by strict limitation that the mode-1 rank must be 1 for TT model. In addition, TR is a more generalized model, which contains TT as a special case, thus yielding better low-rank approximation. Moreover, all other TR algorithms can also achieve similar results. Note that TR-SGD can achieve the same performance as TR-ALS, which demonstrates its effectiveness on real-world dataset. Due to high computational efficiency of TR-SGD per iteration, it can be potentially applied to very large-scale dataset. For visualization, TR-SGD results after 10 and 100 iterations are shown in Fig. 5.
6 CONCLUSION
We have proposed a novel tensor decomposition model, which provides an efficient representation for a very high-order tensor by a sequence of low-dimensional cores. The number of parameters in our model scales only linearly to the tensor order. To optimize the latent cores, we have presented several different algorithms: TR-SVD is a non-recursive algorithm that is stable and efficient, while TR-BALS can learn a more compact representation with adaptive TR-ranks, TR-SGD is a scalable algorithm which can be also used for tensor completion and online learning. Furthermore, we have
8

Under review as a conference paper at ICLR 2018

Table 4: Results on CIFAR-10 images.

TT-SVD TR-SVD TR-BALS TR-ALS TR-SGD

0.092 0.095 0.094 0.1076 0.1041

Ranks (1 7 79 67) (5,3,49,58) (61,13,3,6) (5,3,49,58) (5,3,49,58)

Np 66099 42710 63278 42710 42710

Iterations NaN NaN 23 10 100

(a) RSE = 0.18, Iter = 10

(b) RSE=0.10, Iter =100

Figure 5: The reconstructed images by using TR-SGD after 10 and 100 iterations.

investigated the properties on how the basic multilinear algebra can be performed efficiently by operations over TR representations (i.e., cores), which provides a powerful framework for processing large-scale data. The experimental results verified the effectiveness of our proposed algorithms.
REFERENCES
Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1): 2773­2832, 2014.
J. A. Bengua, H. N. Phien, and H. D. Tuan. Optimal feature extraction and classification of tensors via matrix product state decomposition. In 2015 IEEE International Congress on Big Data, pp. 669­672, June 2015. doi: 10.1109/BigDataCongress.2015.105.
Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, Danilo P Mandic, et al. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. Foundations and Trends® in Machine Learning, 9(4-5):249­429, 2016.
L. De Lathauwer, B. De Moor, and J. Vandewalle. On the best rank-1 and rank-(R1,R2,. . .,RN) approximation of higher-order tensors. SIAM J. Matrix Anal. Appl., 21:1324­1342, 2000. ISSN 0895-4798.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points?online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Rainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis. Large-scale matrix factorization with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 69­77. ACM, 2011.
J. Goulart, M. Boizard, R. Boyer, G. Favier, and P. Comon. Tensor cp decomposition with structured factor matrices: Algorithms and performance. IEEE Journal of Selected Topics in Signal Processing, 2015.
S. Holtz, T. Rohwedder, and R. Schneider. The alternating linear scheme for tensor optimization in the tensor train format. SIAM J. Scientific Computing, 34(2), 2012.
9

Under review as a conference paper at ICLR 2018
Heishiro Kanagawa, Taiji Suzuki, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami. Gaussian process nonparametric tensor estimator and its minimax optimality. In International Conference on Machine Learning (ICML2016), pp. 1632­1641, 2016.
Boris N Khoromskij. Tensor numerical methods for multidimensional PDEs: theoretical analysis and initial applications. ESAIM: Proceedings and Surveys, 48:1­28, 2015.
T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455­500, 2009.
Ivan Laptev and Tony Lindeberg. Local descriptors for spatio-temporal recognition. In Spatial Coherence for Visual Motion Analysis, pp. 91­103. Springer, 2006.
Takanori Maehara, Kohei Hayashi, and Ken-ichi Kawarabayashi. Expected tensor decomposition with stochastic gradient descent. In AAAI, pp. 1919­1925, 2016.
S Nayar, S Nene, and Hiroshi Murase. Columbia object image library (coil 100). Department of Comp. Science, Columbia University, Tech. Rep. CUCS-006-96, 1996.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442­450, 2015.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5): 2295­2317, 2011.
Bernardino Romera-Paredes, Hane Aung, Nadia Bianchi-Berthouze, and Massimiliano Pontil. Multilinear multitask learning. In International Conference on Machine Learning, pp. 1444­1452, 2013.
Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 4799­4807. Curran Associates, Inc., 2016. URL http://papers. nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf.
Chuan-Yung Tsai, Andrew M Saxe, and David Cox. Tensor switching networks. In Advances in Neural Information Processing Systems, pp. 2038­2046, 2016.
Yining Wang and Anima Anandkumar. Online and differentially-private tensor decomposition. In Advances in Neural Information Processing Systems, pp. 3531­3539, 2016.
Qiang Wu, Liqing Zhang, and Andrzej Cichocki. Multifactor sparse feature extraction using convolutive nonnegative tucker decomposition. Neurocomputing, 129:17­24, 2014.
Zenglin Xu, Feng Yan, and Alan Qi. Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp. 1023­1030, 2012.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video classification. arXiv preprint arXiv:1707.01786, 2017.
Rose Yu and Yan Liu. Learning from multiway data: Simple and efficient tensor regression. In International Conference on Machine Learning, pp. 373­381, 2016.
Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Yuan Qi, and Zoubin Ghahramani. Distributed flexible nonlinear tensor factorization. In Advances in Neural Information Processing Systems, pp. 928­936, 2016.
G. Zhou, Q. Zhao, Y. Zhang, T. Adali, S. Xie, and A. Cichocki. Linked component analysis from matrices to high-order tensors: Applications to biomedical data. Proceedings of the IEEE, 104(2): 310­331, 2016.
10

Under review as a conference paper at ICLR 2018

A RELATION TO OTHER MODELS

In this section, we discuss the relations between TR model and the classical tensor decompositions including CPD, Tucker and TT models. All these tensor decompositions can be viewed as the transformed representation of a given tensor. The number of parameters in CPD is O(dnr) that is linear to tensor order, however, its optimization problem is difficult and convergence is slow. The Tucker model is stable and can approximate an arbitrary tensor as close as possible, however, its number of parameters is O(dnr + rd) that is exponential to tensor order. In contrast, TT and TR decompositions have similar representation power to Tucker model, while their number of paramters is O(dnr2) that is linear to tensor order.

A.1 CP DECOMPOSITION

The cannonical polyadic decomposition (CPD) aims to represent a dth-order tensor T by a sum of

rank-one tensors, given by

r

T=

u(1)  · · ·  u(d),

(13)

=1

where each rank-one tensor is represented by an outer product of d vectors. It can be also written in

the element-wise form given by

T (i1, . . . , id) = u(i11), . . . , u(idd) ,

(14)

where ·, . . . , · denotes an inner product of a set of vectors, i.e., u(ikk)  Rr, k = 1, . . . , d.

By defining Vk k = 1, . . . , d, ik

(ik ) = 1,

= ...

,dniakg,(wuei(kkc)a)nwrehwicrhiteis(1a4)daiasgonal

matrix

for

each

fixed

ik

and

k,

where

T (i1, . . . , id) = Tr(V1(i1)V2(i2) · · · Vd(id)).

(15)

Hence, CPD can be viewed as a special case of TR decomposition T = (V1, . . . , Vd) where the cores Vk, k = 1, . . . , d are of size r × nk × r and each lateral slice matrix Vk(ik) is a diagonal matrix of size r × r.

A.2 TUCKER DECOMPOSITION

The Tucker decomposition aims to represent a dth-order tensor T by a multilinear product between a

core tensor G  Rr1×···×rd and factor matrices U(k)  Rnk×rk , k = 1, . . . , d, which is expressed

by

T = G ×1 U(1) ×2 · · · ×d U(d) = [[G, U(1), . . . , U(d)]].

(16)

By assuming the core tensor G can be represented by a TR decomposition G = (V1, . . . , Vd), the Tucker decomposition (16) in the element-wise form can be rewritten as

T (i1, . . . , id)

= (V1, . . . , Vd) ×1 u(1)T (i1) ×2 · · · ×d u(d)T (id)

d
= Tr

rk
Vk(k)u(k)(ik, k)

k=1 k=1

d

= Tr

Vk ×2 u(k)T (ik) ,

k=1

(17)

where the second step is derived by applying Theorem 4. Hence, Tucker model can be represented as a TR decomposition T = (Z1, . . . , Zd) where the cores are computed by the multilinear products between TR cores representing G and the factor matrices, respectively, which is

Zk = Vk ×2 U(k), k = 1, . . . , d.

(18)

11

Under review as a conference paper at ICLR 2018

A.3 TT DECOMPOSITION

The tensor train decomposition aims to represent a dth-order tensor T by a sequence of cores Gk, k = 1, . . . , d, where the first core G1  Rn1×r2 and the last core Gd  Rrd×nd are matrices while the other cores Gk  Rrk×nk×rk+1 , k = 2, . . . , d - 1 are 3rd-order tensors. Specifically, TT decomposition in the element-wise form is expressed as

T (i1, . . . , id) = g1(i1)T G2(i2) · · · Gd-1(id-1)gd(id),

(19)

where g1(i1) is the i1th row vector of G1, gd(id) is the idth column vector of Gd, and Gk(ik), k = 2, . . . , d - 1 are the ikth lateral slice matrices of Gk.

According to the definition of TR decomposition in (1), it is obvious that TT decomposition is a special case of TR decomposition where the first and the last cores are matrices, i.e., r1 = rd+1 = 1. On the other hand, TR decomposition can be also rewritten as

T (i1, . . . , id) = Tr {Z1(i1)Z2(i2) · · · Zd(id)}

r1
= z1(1, i1, :)T Z2(i2) · · · Zd-1(id-1)zd(:, id, 1)

(20)

1 =1

where z1(1, i1, :)  Rr2 is the 1th row vector of the matrix Z1(i1) and zd(:, id, 1) is the 1th column vector of the matrix Zd(id). Therefore, TR decomposition can be interpreted as a sum of TT representations. The number of TT representations is r1 and these TT representations have the common cores Zk, for k = 2, . . . , d - 1. In general, TR outperforms TT in terms of representation
power due to the fact of linear combinations of a group of TT representations. Furthermore, given a

specific approximation level, TR representation requires smaller ranks than TT representation.

B PROOFS

B.1 PROOF OF THEOREM 1
Proof. It is obvious that (1) can be rewritten as
T (i1, i2, . . . , id) = Tr(Z2(i2), Z3(i3), . . . , Zd(id), Z1(i1)) = · · · = Tr(Zd(id), Z1(i1), . . . , Zd-1(id-1)).
Therefore, we have T-k = (Zk+1, . . . , Zd, Z1, . . . , Zk).

B.2 PROOF OF THEOREM 3

Proof. According to the definition of TR decomposition, and the cores shown in (10), the (i1, . . . , id)th element of tensor T 3 can be written as

T3(i1, . . . , id) =Tr

d k=1

Zk (ik )

0

0

d k=1

Yk

(ik

)

dd

= Tr

Zk(ik) + Tr

Yk(ik) .

k=1

k=1

Hence, the addition of tensors in the TR format can be performed by merging of their cores.

B.3 PROOF OF THEOREM 4

Proof. The multilinear product between a tensor and vectors can be expressed by

d

c = T (i1, . . . , id)u1(i1) · · · ud(id) = Tr Zk(ik) u1(i1) · · · ud(id)

i1 ,...,id

i1 ,...,id

k=1

d nk

=Tr Zk(ik)uk(ik) .

k=1 ik=1

Thus, it can be written as a TR decomposition shown in (11) where each core Xk  Rrk×rk+1 becomes a matrix. The computational complexity is equal to O(dnr2).

12

Under review as a conference paper at ICLR 2018

Algorithm 1 TR-SVD

Input: A dth-order tensor T of size (n1 × · · · × nd) and the prescribed relative error p.

Output: Cores Zk, k = 1, . . . , d of TR decomposition and the TR-ranks r.

1: Compute truncation threshold k for k = 1 and k > 1.

2: Choose one mode as the start point (e.g., the first mode) and obtain the 1-unfolding matrix T 1 .

3: Low-rank approximation by applying 1-truncated SVD: T 1 = UVT + E1.

4: Split ranks r1, r2 by minr1,r2 r1 - r2 , s. t. r1r2 = rank1 (T 1 ).

5: Z1  permute(reshape(U, [n1, r1, r2]), [2, 1, 3]).

6: Z>1  permute(reshape(VT , [r1, r2,

d j=2

nj

]),

[2,

3,

1]).

7: for k = 2 to d - 1 do

8: Z>k-1 = reshape(Z>k-1, [rknk, nk+1 · · · ndr1]). 9: Compute k-truncated SVD: Z>k-1 = UVT + Ek. 10: rk+1  rankk (Z>k-1). 11: Zk  reshape(U, [rk, nk, rk+1]).

12:

Z>k  reshape(VT , [rk+1,

d j=k+1

nj

,

r1

]).

13: end for

B.4 PROOF OF THEOREM 5

Proof. Each element in tensor T 3 can be written as

T3(i1, . . . , id) =Tr =Tr

dd

Zk(ik) Tr

Yk (ik )

k=1

k=1

d

Zk(ik)  Yk(ik) .

k=1

= Tr

dd

Zk(ik) 

Yk (ik )

k=1

k=1

Hence, T 3 can be also represented as TR format with its cores computed by (12), which costs O(dnq2).

C ALGORITHMS
Pseudo codes of TR-SVD and TR-BALS are provided in Alg. 1 and Alg. 2, respectively.

D ADDITIONAL EXPERIMENTAL RESULTS
D.1 COIL-100 DATASET
The Columbia Object Image Libraries (COIL)-100 dataset (Nayar et al., 1996) contains 7200 color images of 100 objects (72 images per object) with different reflectance and complex geometric characteristics. Each image can be represented by a 3rd-order tensor of size 128 × 128 × 3 and then is downsampled to 32 × 32 × 3. Hence, the dataset can be finally organized as a 4th-order tensor of size 32 × 32 × 3 × 7200. The number of features is determined by r4 × r1, while the flexibility of subspace bases is determined by r2, r3. Subsequently, we apply the K-nearest neighbor (KNN) classifier with K=1 for classification. For detailed comparisons, we randomly select a certain ratio  = 50% or  = 10% samples as the training set and the rest as the test set. The classification performance is averaged over 10 times of random splitting. In Table 5, rmax of TR decompositions is much smaller than that of TT-SVD. It should be noted that TR representation, as compared to TT, can obtain more compact and discriminant representations. Fig. 6 shows the reconstructed images under different approximation levels.
D.2 KTH VIDEO DATASET
We test the TR representation for KTH video database (Laptev & Lindeberg, 2006) containing six types of human actions (walking, jogging, running, boxing, hand waving and hand clapping)

13

Under review as a conference paper at ICLR 2018

Algorithm 2 TR-BALS

Input: A d-dimensional tensor T of size (n1 × · · · × nd) and the prescribed relative error p. Output: Cores Zk and TR-ranks rk, k = 1, . . . , d.

1: Initialize rk = 1 for k = 1, . . . , d. 2: Initialize Zk  Rrk×nk×rk+1 for k = 1, . . . , d. 3: repeat k  circular{1, 2, . . . , d};

4: Compute the subchain Z=(k,k+1).

5:

Obtain the mode-2 unfolding matrix Z[=2(]k,k+1) of size

d j=1

nj /(nknk+1)

×

rk rk+2 .

6: Z((k2),k+1)  arg min T[k] - Z((2k),k+1) Z=[2(]k,k+1) T .
F
7: Tensorization of mode-2 unfolding matrix

Z(k,k+1)  folding(Z((2k),k+1)).

8: Reshape the block core by

Z~ (k,k+1)  reshape(Z(k,k+1), [rknk × nk+1rk+2]).

9: Low-rank approximation by -truncated SVD Z~ (k,k+1) = UVT . 10: Zk  reshape(U, [rk, nk, rk+1]). 11: Zk+1  reshape(VT , [rk+1, nk+1, rk+2]). 12: rk+1  rank(Z~ (k,k+1)). 13: k  k + 1.
14: until The desired approximation accuracy is achieved, i.e.,  p.

Figure 6: The reconstruction of Coil-100 dataset by using TR-SVD. The top row shows the original images, while the reconstructed images are shown from the second to sixth rows corresponding to =0.1, 0.2, 0.3, 0.4, 0.5, respectively.
performed several times by 25 subjects in four different scenarios: outdoors, outdoors with scale variation, outdoors with different clothes and indoors as illustrated in Fig. 7. There are 600 video sequences for each combination of 25 subjects, 6 actions and 4 scenarios. Each video sequence was downsampled to 20×20×32. Finally, we can organize the dataset as a tensor of size 20×20×32×600. For extensive comparisons, we choose different error bound p  {0.2, 0.3, 0.4}. In Table 6, we can see that TR representations achieve better compression ratio reflected by smaller rmax, r¯ than that of TT-SVD, while TT-SVD achieves better compression ratio than CP-ALS. For instance, when
 0.2, CP-ALS requires rmax = 300, r¯ = 300; TT-SVD requires rmax = 139, r¯ = 78, while TR-SVD only requires rmax = 99, r¯ = 34.2. For classification performance, we observe that the best accuracy (5 × 5-fold cross validation) achieved by CP-ALS, TT-SVD, TR-SVD are 80.8%, 84.8%, 87.7%, respectively. Note that these classification performances might not be the state-of-the-art
14

Under review as a conference paper at ICLR 2018

Table 5: The comparisons of different algorithms on Coil-100 dataset. , rmax,r¯ denote relative error, the maximum rank and the average rank, respectively.

rmax

r¯

Acc. (%) Acc. (%) ( = 50%) ( = 10%)

0.19 67 47.3 99.05

89.11

0.28 23 16.3 98.99

88.45

TT-SVD 0.37 8 6.3 96.29

86.02

0.46 3 2.7 47.78

44.00

0.19 23 12.0 99.14

89.29

0.28 10 6.0 99.19

89.89

TR-SVD 0.36 5 3.5 98.51

88.10

0.43 3 2.3 83.43

73.20

Figure 7: Video dataset consists of six types of human actions performed by 25 subjects in four different scenarios. From the top to bottom, six video examples corresponding to each type of actions are shown.
on this dataset, we mainly focus on the comparisons of representation ability among CP, TT, and TR decomposition frameworks. To obtain the best performance, we may apply the powerful feature extraction methods to TT or TR representations of dataset. It should be noted that TR decompositions achieve the best classification accuracy when = 0.29, while TT-SVD and CP-ALS achieve their best classification accuracy when = 0.2. This indicates that TR decomposition can preserve more discriminant information even when the approximation error is relatively high. This experiment demonstrates that TR decompositions are effective for unsupervised feature representation due to their flexibility of TR-ranks and high compression ability.

Table 6: The comparisons of different algorithms on KTH dataset. denotes the obtained relative error; rmax denotes maximum rank; r¯ denotes the average rank; and Acc. is the classification accuracy.

CP-ALS
TT-SVD TR-SVD

rmax r¯ Acc. (5 × 5-fold)

0.20 300 300

80.8 %

0.30 40 40

79.3 %

0.40 10 10

66.8 %

0.20 139 78.0

84.8 %

0.29 38 27.3

83.5 %

0.38 14 9.3

67.8 %

0.20 99 34.2

78.8 %

0.29 27 12.0

87.7 %

0.37 10 5.8

72.4 %

15

