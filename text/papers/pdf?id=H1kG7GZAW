Under review as a conference paper at ICLR 2018
VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We evaluate the proposed approach using several quantitative metrics and empirically observe significant gains over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).
1 INTRODUCTION
Feature representations of the observed raw data play a crucial role in the success of machine learning algorithms. Effective representations should be able to capture the underlying (abstract or high-level) latent generative factors that are relevant for the end task while ignoring the inconsequential or nuisance factors. Disentangled feature representations have the property that the generative factors are revealed in disjoint subsets of the feature dimensions, such that a change in a single generative factor causes a highly sparse change in the representation. Disentangled representations offer several advantages ­ (i) Invariance: it is easier to derive representations that are invariant to nuisance factors by simply marginalizing over the corresponding dimensions, (ii) Transferability: they are arguably more suitable for transfer learning as most of the key underlying generative factors appear segregated along feature dimensions, (iii) Interpretability: a human expert may be able to assign meanings to the dimensions, (iv) Conditioning and intervention: they allow for interpretable conditioning and/or intervention over a subset of the latents and observe the effects on other nodes in the graph. Indeed, the importance of learning disentangled representations has been argued in several recent works (Bengio et al., 2013; Lake et al., 2016; Ridgeway, 2016).
Recognizing the significance of disentangled representations, several attempts have been made in this direction in the past (Ridgeway, 2016). Much of the earlier work assumes some sort of supervision in terms of: (i) partial or full access to the generative factors per instance (Reed et al., 2014; Yang et al., 2015; Kulkarni et al., 2015; Karaletsos et al., 2015), (ii) knowledge about the nature of generative factors (e.g, translation, rotation, etc.) (Hinton et al., 2011; Cohen & Welling, 2014), (iii) knowledge about the changes in the generative factors across observations (e.g., sparse changes in consecutive frames of a Video) (Goroshin et al., 2015; Whitney et al., 2016; Fraccaro et al., 2017; Denton & Birodkar, 2017; Hsu et al., 2017), (iv) knowledge of a complementary signal to infer representations that are conditionally independent of it1 Cheung et al. (2014); Mathieu et al. (2016); Siddharth et al. (2017), However, in most real scenarios, we only have access to raw observations without any supervision about the generative factors. It is a challenging problem and many of the earlier attempts have not been able to scale well for realistic settings (Cohen & Welling, 2015) (see also, Higgins et al. (2017)).
Recently, Chen et al. (2016) proposed an approach to learn a generative model with disentangled factors based on Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), however
1The representation itself can still be entangled in rest of the generative factors.
1

Under review as a conference paper at ICLR 2018
implicit generative models like GANs lack an effective inference mechanism2, which hinders its applicability to the problem of learning disentangled representations. More recently, Higgins et al. (2017) proposed an approach based on Variational AutoEncoder (VAE) Kingma & Welling (2013) for inferring disentangled factors. The inferred latents using their method (termed as -VAE ) are empirically shown to have better disentangling properties, however the method deviates from the basic principles of variational inference, creating increased tension between observed data likelihood and disentanglement. This in turn leads to poor quality of generated samples as observed in (Higgins et al., 2017).
In this work, we propose a principled approach for inference of disentangled latent factors based on the popular and scalable framework of amortized variational inference (Kingma & Welling, 2013; Stuhlmu¨ller et al., 2013; Gershman & Goodman, 2014; Rezende et al., 2014) powered by stochastic optimization (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014). Disentanglement is encouraged by introducing a regularizer over the induced inferred prior. Unlike -VAE (Higgins et al., 2017), our approach does not introduce any extra conflict between disentanglement of the latents and the observed data likelihood, which is reflected in the overall quality of the generated samples that matches the VAE and is much better than -VAE. This does not come at the cost of higher entanglement and our approach also outperforms -VAE in disentangling the latents as measured by various quantitative metrics.
2 FORMULATION
We start with a generative model of the observed data that first samples a latent variable z  p(z), and an observation is generated by sampling from p(x|z). The joint density of latents and observations is denoted as p(x, z) = p(z)p(x|z). The problem of inference is to compute the posterior of the latents conditioned on the observations, i.e., p(z|x). We assume that we are given a finite set of samples (observations) from the true data distribution p(x). In most practical scenarios involving high dimensional and complex data, this computation is intractable and calls for approximate inference. Variational inference takes an optimization based approach to this, positing a family D of approximate densities over the latents and reducing the approximate inference problem to finding a member density that minimizes the Kullback-Leibler divergence to the true posterior, i.e., qx = minqD KL(q(z) p(z|x)) (Blei et al., 2017). The idea of amortized inference (Kingma & Welling, 2013; Stuhlmu¨ller et al., 2013; Gershman & Goodman, 2014; Rezende et al., 2014) is to explicitly share information across inferences made for each observation. One successful way of achieving this for variational inference is to have a so-called recognition model, parameterized by , that encodes an inverse map from the observations to the approximate posteriors (also referred as variational autoencoder or VAE) (Kingma & Welling, 2013; Rezende et al., 2014). The recognition model parameters are learned by optimizing the problem min ExKL(q(z|x) p(z|x)). This can be shown as equivalent to maximizing what is termed as evidence lower bound (ELBO):
arg min ExKL(q(z|x) p(z|x)) = arg max Ex Ezq(z|x) [log p(x|z)] - KL(q(z|x) p(z)) (1)
, ,
The ELBO (the objective at the right side of Eq. 1) lower bounds the log-likelihood of observed data, and the gap vanishes at the global optimum. Often, the density forms of p(z) and q(z|x) are chosen such that their KL-divergence can be written analytically in a closed-form expression (e.g., p(z) is N (0, I) and q(z|x) is N (µ(x), (x))) (Kingma & Welling, 2013). In such cases, the ELBO can be efficiently optimized (to a stationary point) using stochastic first order methods where both expectations are estimated using mini-batches. Further, in cases when q(·) can be written as a continuous transformation of a fixed base distribution (e.g., the standard normal distribution), a low variance estimate of the gradient over  can be obtained by coordinate transformation (also referred as reparametrization) (Fu, 2006; Kingma & Welling, 2013; Rezende et al., 2014).
2.1 GENERATIVE STORY: DISENTANGLED PRIOR
Most VAE based generative models for real datasets (e.g., text, images, etc.) already work with a relatively simple and disentangled prior p(z) having no interaction among the latent dimensions (e.g.,
2There have been a few recent attempts in this direction for visual data (Dumoulin et al., 2016; Donahue et al., 2016; Kumar et al., 2017) but often the reconstructed samples are semantically quite far from the input samples, sometimes even changing in the object classes.
2

Under review as a conference paper at ICLR 2018

the standard Gaussian N (0, I)) (Bowman et al., 2015; Miao et al., 2016; Hou et al., 2017; Zhao et al., 2017). The complexity of the observed data is absorbed in the conditional distribution p(x|z) which encodes the interactions among the latents. Hence, as far as the generative modeling is concerned,
disentangled prior sets us in the right direction.

2.2 INFERRING DISENTANGLED LATENTS
Although the generative model starts with a disentangled prior, our main objective is to infer disentangled latents which are conducive for various goals mentioned in Sec. 1 (e.g., invariance, transferability, interpretability). To this end, we consider the density over the inferred latents induced by the approximate posterior inference mechanism,

q(z) = q(z|x)p(x)dx,

(2)

which we will subsequently refer to as the inferred prior. For inferring disentangled factors, this
should be factorizable along the dimensions, i.e., q(z) = i qi(zi), or equivalently qi|j(zi|zj) = qi(zi),  i, j. This can be achieved by minimizing a suitable distance between the inferred prior
q(z) and the disentangled generative prior p(z). If we take KL-divergence as our choice of distance, by relying on its pairwise convexity (i.e., KL(p1 + (1 - )p2 q1 + (1 - )q2)  KL(p1 q1) + (1 - )KL(p2 q2)) (Van Erven & Harremos, 2014), we can show that this distance is upper bounded
by the objective of the variational inference (the ELBO in Eq. (1)):

KL(q(z) p(z)) = KL(Exq(z|x) Exp(z|x))  Ex KL(q(z|x) p(z|x)).

(3)

Hence, variational posterior inference of latent variables with disentangled prior naturally encourages
inferring factors that are disentangled. We think this is the reason that the original VAE (Eq. (1) has
also been observed to exhibit some disentangling behavior on simple datasets such as MNIST (Kingma
& Welling, 2013). However, this behavior does not carry over to more complex datasets (Aubry et al.,
2014; Liu et al., 2015; Higgins et al., 2017), unless extra supervision on the generative factors is
provided (Kulkarni et al., 2015; Karaletsos et al., 2015). This can be due to non-convexity of the ELBO objective which prevents us from achieving the global minimum of ExKL(q(z|x) p(z|x)) = 0 (which would imply KL(q(z) p(z)) = 0). In other words, maximizing the ELBO (Eq. (1)) might also result in reducing the value of KL(q(z) p(z)), however, due to the non-convexity of the loss surface of the ELBO, the gap between KL(q(z) p(z)) and Ex KL(q(z|x) p(z|x)) could be large at the stationary point of convergence. Hence, minimizing KL(q(z) p(z)) explicitly will give us better control on the disentanglement. This motivates us to add KL(q(z) p(z)) as part of the objective to encourage disentanglement during inference, i.e.,

max Ex
,

Ezq(z|x) [log p(x|z)] - KL(q(z|x)

p(z))

-  KL(q(z)

p(z)),

(4)

where  controls its contribution to the overall objective.

Optimizing (4) directly is not tractable due to the presence of KL(q(z) p(z)) which does not have a closed-form expression. One possibility is use the variational formulation of the KL-divergence (Nguyen et al., 2010; Nowozin et al., 2016) that needs only samples from q(z) and p(z) to estimate a lower bound to KL(q(z) p(z)). However, this would involve optimizing for a third set of parameters  for the KL-divergence estimator, and would also change the optimization to a saddle-
point (min-max) problem which has its own challenges (e.g., gradient vanishing as encountered
in training generative adversarial networks with KL or JS divergences (Goodfellow et al., 2014; Arjovsky & Bottou, 2017)). Replacing KL(q(z) p(z)) with another suitable distance between q(z) and p(z) (e.g., integral probability metrics like Wasserstein distance (Sriperumbudur et al., 2009))
might alleviate some of these issues (Arjovsky et al., 2017) but will still involve complicating the optimization to a saddle point problem in three set of parameters3. It should also be noted that using
these variational forms of the distances will still leave us with an approximation to the actual distance.

We adopt a simpler alternative of matching the moments of the two distributions. In particular, we match the covariance of the two distributions which will amount to decorrelating the dimensions of

3Nonparametric distances like maximum mean discrepancy (MMD) with a characteristic kernel (Gretton et al., 2012) is also an option, however it has its own challenges when combined with stochastic optimization (Dziugaite et al., 2015; Li et al., 2015).

3

Under review as a conference paper at ICLR 2018

z  q(z) if p(z) is N (0, I). Let us denote Covq(z)(z) := Eq(z) (z - Eq(z)(z))(z - Eq(z)(z)) . By the law of total covariance, the covariance of z  q(z) is given by

Covq(z)(z) = Ep(x)Covq(z|x)(z) + Covp(x) Eq(z|x)(z) ,

(5)

where Eq(z|x)(z) and Covq(z|x)(z) are random variables that are functions of the random variable x (z is marginalized over). Most existing work on the VAE models uses q(z|x) having the form N (µ(x), (x)), where µ(x) and (x) are the outputs of a deep neural net. In this case Eq. (5) reduces to Covq(z)(z) = Ep(x)(x) + Covp(x)(µ(x)), which we want to be close to an identity matrix. For simplicity, we choose entry-wise squared 2-norm as the measure of proximity. However, as the entanglement is mainly reflected in the off-diagonal entries of this matrix, we opt
for two separate hyperparameters controlling the relative importance of the loss on the diagonal and
off-diagonal entries. This gives rise to the following optimization problem for inference:

max ELBO(, ) - od
,

Covp(x)(µ(x))

2 ij

-

d

i=j i

Covp(x)(µ(x)) ii - 1 2 .

(6)

The regularization terms involving Covp(x)(µ(x)) in the above objective (6) can be efficiently optimized using SGD. We maintain a running estimate of Covp(x)(µ(x)) which is updated with every minibatch of x  p(x). The gradient for the current minibatch can be computed by treating the previous estimate of Covp(x)(µ(x)) as constant.

2.3 COMPARISON WITH -VAE

Recently proposed -VAE (Higgins et al., 2017) proposes to modify the ELBO by upweighting the KL(q(z|x) p(z)) term in order to encourage the inference of disentangled factors:

max Ex
,

Ezq(z|x) [log p(x|z)] -  KL(q(z|x)

p(z))

,

(7)

where  is taken to be great than 1. Higher  is argued to encourage disentanglement at the cost of reconstruction error (the likelihood term in the ELBO). Authors report empirical results with  ranging from 4 to 250 depending on the dataset. As already mentioned, most VAE models proposed in the literature, including -VAE, work with N (0, I) as the prior p(z) and N (µ(x), (x)) with diagonal (x) as the approximate posterior q(z|x). This reduces the objective (7) to

max Ex
,

Ezq (z|x)

[log

p (x|z)]

-

 2

[(x)]ii - ln [(x)]ii

+

µ(x)

2 2

i

. (8)

For high values of , -VAE would try to pull µ(x) towards zero and (x) towards the identity matrix (as the minimum of x - ln x for x > 0 is at x = 1), thus making the approximate posterior q(z|x) insensitive to the observations. This is also reflected in the quality of the generated samples which is worse than VAE ( = 1), particularly for high values of . Our proposed method does not
have such increased tension between the likelihood term and the disentanglement objective, and the
sample quality with our method is on par with the VAE.

Finally, we note that both -VAE and our proposed method encourage disentanglement of inferred
factors by pulling Covq(z)(z) in Eq. (5) towards the identity matrix: -VAE attempts to do it by making Covq(z|x)(z) close to I and Eq(z|x)(z) close to 0 individually for all observations x, while the proposed method directly works on Covq(z)(z) (marginalizing over the observations x) which retains the sensitivity of q(z|x) to the conditioned-upon observation.

3 EXPERIMENTS
We evaluate our proposed method, referred as DIP-VAE subsequently (Disentangled Inferred Prior), on three datasets ­ (i) CelebA (Liu et al., 2015): It consists of 202, 599 RGB face images of celebrities. We use 64 × 64 × 3 cropped images as used in several earlier works, using 90% for training and 10% for test. (ii) 3D Chairs (Aubry et al., 2014): It consists of 1393 chair CAD models, with each model rendered from 31 azimuth angles and 2 elevation angles. Following earlier work (Yang et al., 2015; Dosovitskiy et al., 2015) that ignores near-duplicates, we use a subset of 809

4

Under review as a conference paper at ICLR 2018

Table 1: Disentanglement metric score Higgins et al. (2017) and reconstruction error (per pixel) on the test sets for 2D Shapes and CelebA (1 = 4, 2 = 20 for 2D Shapes, and 1 = 4, 2 = 8 for CelebA)

Method
VAE -VAE (=1) -VAE (=2)
DIP-VAE

2D Shapes Metric Reconst. error
81.3 0.0017 80.7 0.0031 88.0 0.0076 98.1 0.0018

CelebA Metric Reconst. error

7.5 8.1 7.1 11.31

0.0876 0.0937 0.1065 0.0911

Figure 1: Disentanglement metric score (Higgins et al., 2017) as a function of reconstruction error for -VAE and the proposed DIP-VAE (left: CelebA, right: 2D Shapes). The plots are generated by varying  for -VAE and od for DIP-VAE with d set to 10od.
chair models in our experiments. We use the binary masks of the chairs as the observed data in our experiments following (Higgins et al., 2017). First 80% of the models are used for training and the rest are used for test. (iii) 2D Shapes (Higgins et al., 2017): This is a synthetic dataset of binary 2D shapes generated from the Cartesian product of the shape (heart, oval and square), x-position (32 values), y-position (32 values), scale (6 values) and rotation (40 values). We consider two baselines for the task of unsupervised inference of disentangled factors: (i) VAE (Kingma & Welling, 2013; Rezende et al., 2014), and (ii) the recently proposed -VAE (Higgins et al., 2017). To be consistent with the evaluations in (Higgins et al., 2017), we use the same CNN network architectures (for our encoder and decoder), and same latent dimensions as used in (Higgins et al., 2017) for CelebA, 3D Chairs, 2D Shapes datasets.
Hyperparameters. For the proposed DIP-VAE, in all our experiments we vary d in the set {1, 2, 5, 10, 20, 50} while fixing d = 10od. For -VAE, we experiment with  = {1, 2, 4, 8, 16, 25, 32, 64, 100, 128, 200, 256} (where  = 1 corresponds to VAE). For both CelebA and 2D Shapes, we show the results for the best performing models in terms of the disentanglement metric score which was introduced in (Higgins et al., 2017). For 3D Chairs data, this metric is at 100% for almost all models and we pick the models based on our subjective evaluation of the reconstruction quality and disentanglement.
Disentanglement metric score and reconstruction error. Higgins et al. (2017) propose a metric to evaluate the disentanglement performance of the inference mechanism. It assumes that the ground truth generative factors are available. It works by first sampling a generative factor y, followed by sampling L pairs of examples such that for each pair, the sampled generative factor takes the same value. Given the inferred zx := µx for each example x, they compute the absolute difference of these vectors for each pair, followed by averaging these difference vectors. This average difference vector is assigned the label of y. By sampling n such minibatches of L pairs, we get n such averaged difference vectors for the factor y. This process is repeated for all generative factors. A low capacity multiclass classifier is then trained on these vectors to predict the identity of the corresponding generative factor. In all our experiments, we use a one-vs-rest linear SVM with weight on the hinge loss C set to 0.01 and weight on the regularizer set to 1. Higgins et al. (2017) argue that this
5

Under review as a conference paper at ICLR 2018

Table 2:

Attribute

classification

accuracy on

CelebA:

A

classifier

wk

=

1 |xi :yik =1|

xi:yik=1 µ(xi) -

1 |xi :yik =0|

xi:yik=0 µ(xi) is computed for every attribute k using the training set and a bias is learned

by minimizing the hinge loss. Accuracy on other attributes stays about same across all methods.

Method Arched Eyebrows Attractive Bangs Black hair Blond hair Heavy makeup Male Mouth slighly open No Beard Wavy hair Wearing hat Wearing lipstick

VAE 71.8 73.0 89.8 78.0 88.9 79.6 83.9 76.3 87.3 70.2 95.8 83.0  = 2 71.6 72.6 90.6 79.3 89.1 79.3 83.5 76.1 86.9 67.8 95.9 82.4  = 4 71.6 72.6 90.0 76.6 88.9 77.8 82.3 75.7 85.3 66.8 95.8 80.6  = 8 71.6 71.7 90.0 76.0 87.2 76.2 80.5 73.1 85.3 63.7 95.8 79.6 DIP-VAE 73.7 73.2 90.9 80.6 91.9 81.5 85.9 75.9 85.3 71.5 96.2 84.7

metric captures the disentangled property of the inferred latents reasonably well. Table 1 shows the disentanglement metric scores along with reconstruction error (which directly corresponds to the data likelihood) for the test sets for CelebA and 2D Shapes data. It is evident that the proposed DIP-VAE outperforms -VAE both in terms of the disentanglement metric score and reconstruction error. Further we also show the plot of how the disentanglement metric changes with the reconstruction error as we vary the hyperparameter for both methods ( and od, respectively). It is clear that the proposed method gives much higher disentanglement metric score at little to no cost on the reconstruction error when compared VAE ( = 1). The reconstruction error for -VAE gets much worse as  is increased.

Binary attribute classification for CelebA. We also experiment with predicting the binary attribute

values for each test example in CelebA from the inferred µ(x). For each attribute k, we compute the

attribute vector wk

=

1 |xi :yik =1|

xi:yik=1 µ(xi)

-

1 |xi :yik =0|

xi:yik=0 µ(xi) from the training

set, and project the µ(x) along these vectors. A bias is learned on these scalars (by minimizing

hinge loss) which is then used for classifying the test examples. Table 2 shows the results for the

attribute which show the highest change across various methods (most other attribute accuracies do

not change). The proposed DIP-VAE outperforms both VAE and -VAE for most attributes. The

performance of -VAE gets worse as  is increased further.

Correlations in the inferred latent space. We visualize the Pearson's correlations between dimensions of the inferred latents µ(x). We also visualize the correlations between inferred latent dimensions and the ground truth attributes which can be taken as a proxy for true generative factors. Tables 3 and 4 show these correlations for VAE, -VAE and the proposed DIP-VAE. We observe less correlations between inferred latents for DIP-VAE. Latent correlations for -VAE are even higher than those for VAE which seems to be going against the objective of disentanglement. This can be due to the fact that -VAE gives less weight to the ELBO which implies the KL(q(z) p(z)) does not get minimized as well (see Eq. (3)). Indeed Eq. (8) indicates that -VAE gives more importance to minimizing the 2-norm of individual posterior means µ(x) and does not minimize their correlations as DIP-VAE.

To further analyze the latent correlations with the ground truth attributes, we pick one attribute at a time, and pick the latent dimension that has highest correlation with that attribute. Then we plot the correlations of this latent dimension with rest of the attributes (picking top 4 correlations). This will indicate the entanglement of a latent dimension with the ground truth attributes. It should be noted that many attributes in CelebA are naturally entangled (e.g., lipstick, heavy-makeup and gender) which can be quite difficult to disentangle. These plots for two of the attributes are shown in Fig. 2.

Finally we also show the generated samples by varying one latent dimension at a time for CelebA and 3D Chairs data for all three methods in Tables 5 and Tables 6.

6

Under review as a conference paper at ICLR 2018

Table 3: Correlations between inferred latents (top), and correlations between ground truth attributes and inferred latents (bottom) for 2D Shapes data (Higgins et al., 2017).

VAE

-VAE ( = 4) -VAE ( = 20)

DIP-VAE

Latents autocorrelations

Latentsattributes correlations

Table 4: Correlations between inferred latents (top), and correlations between ground truth attributes and inferred latents (bottom) for CelebA dataset.

VAE

-VAE ( = 8) -VAE ( = 64)

DIP-VAE

Latents autocorrelations
Latentsattributes correlations

(a) Bangs

(b) Rosy Cheeks

Figure 2: Top attributes similar to Bangs and Rosy Cheeks attribute in CelebA dataset based on correlations with inferred latents for VAE, -VAE and the proposed DIP-VAE, respectively.

7

Under review as a conference paper at ICLR 2018

Table 5: Qualitative results for disentanglement in CelebA dataset.

VAE

-VAE ( = 4)

Proposed

Smile

Azimuth

Hair color

Table 6: Qualitative results for disentanglement in Chairs dataset.

VAE

-VAE ( = 4)

Proposed

Leg Shape

Rotation

Size 8

Under review as a conference paper at ICLR 2018
4 DISCUSSION
We proposed a principled variational framework with infer disentangled latents from unlabeled observations. Unlike -VAE our variational objective does not have any conflict between the data log-likelihood and the disentanglement of the inferred latents, which is reflected in our empirical results that outperform -VAE. Directions for future work include tackling the sampling bias in the generative process which makes the problem challenging (e.g., sampling the male gender makes it likely to sample beard), and effective use of disentangled representations in transfer learning.
REFERENCES
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C Russell, and Josef Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3762­3769, 2014.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 2017.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016.
Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden factors of variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups. In International Conference on Machine Learning, pp. 1755­1763, 2014.
Taco S Cohen and Max Welling. Transformation properties of learned visual representations. In International Conference on Learning Representations, 2015.
Emily Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from video. arXiv preprint arXiv:1705.10915, 2017.
Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.
Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox. Learning to generate chairs with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1538­1546, 2015.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pp. 258­267. AUAI Press, 2015.
9

Under review as a conference paper at ICLR 2018
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. arXiv preprint arXiv:1710.05741, 2017.
Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13: 575­616, 2006.
Samuel Gershman and Noah Goodman. Amortized inference in probabilistic reasoning. In Proceedings of the Cognitive Science Society, volume 36, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ross Goroshin, Michael F Mathieu, and Yann LeCun. Learning to linearize under uncertainty. In Advances in Neural Information Processing Systems, pp. 1234­1242, 2015.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723­773, 2012.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Conference on Artificial Neural Networks, pp. 44­51. Springer, 2011.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303­1347, 2013.
Xianxu Hou, Linlin Shen, Ke Sun, and Guoping Qiu. Deep feature consistent variational autoencoder. In Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on, pp. 1133­1141. IEEE, 2017.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled latent representations from sequential data. In Advances in neural information processing systems, 2017.
Theofanis Karaletsos, Serge Belongie, and Gunnar Ra¨tsch. Bayesian representation learning with oracle constraints. arXiv preprint arXiv:1506.05011, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems, pp. 2539­2547, 2015.
Abhishek Kumar, Prasanna Sattigeri, and P Thomas Fletcher. Improved semi-supervised learning with gans using manifold invariances. arXiv preprint arXiv:1705.08850, 2017.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, pp. 1­101, 2016.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718­1727, 2015.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730­3738, 2015.
Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. Disentangling factors of variation in deep representation using adversarial training. In Advances in Neural Information Processing Systems, pp. 5040­5048, 2016.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In International Conference on Machine Learning, pp. 1727­1736, 2016.
10

Under review as a conference paper at ICLR 2018
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847­5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In International Conference on Machine Learning, pp. 1431­ 1439, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Karl Ridgeway. A survey of inductive biases for factorial representation-learning. arXiv preprint arXiv:1612.05299, 2016.
N Siddharth, Brooks Paige, Van de Meent, Alban Desmaison, Frank Wood, Noah D Goodman, Pushmeet Kohli, Philip HS Torr, et al. Learning disentangled representations with semi-supervised deep generative models. arXiv preprint arXiv:1706.00400, 2017.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scho¨lkopf, and Gert RG Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.
Andreas Stuhlmu¨ller, Jacob Taylor, and Noah Goodman. Learning stochastic inverses. In Advances in neural information processing systems, pp. 3048­3056, 2013.
Tim Van Erven and Peter Harremos. Re´nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797­3820, 2014.
William F Whitney, Michael Chang, Tejas Kulkarni, and Joshua B Tenenbaum. Understanding visual concepts with continuation learning. arXiv preprint arXiv:1602.06822, 2016.
Jimei Yang, Scott E Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling with recurrent transformations for 3d view synthesis. In Advances in Neural Information Processing Systems, pp. 1099­1107, 2015.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational autoencoding models. arXiv preprint arXiv:1702.08658, 2017.
11

Under review as a conference paper at ICLR 2018
Appendix
A CORRELATIONS BETWEEN INFERRED LATENT FACTORS AND GROUND TRUTH ATTRIBUTES FOR CELEBA
12

Under review as a conference paper at ICLR 2018 13

Under review as a conference paper at ICLR 2018 14

Under review as a conference paper at ICLR 2018 15

Under review as a conference paper at ICLR 2018 16

Under review as a conference paper at ICLR 2018 17

Under review as a conference paper at ICLR 2018 18

Under review as a conference paper at ICLR 2018 19

Under review as a conference paper at ICLR 2018 20

Under review as a conference paper at ICLR 2018 21

Under review as a conference paper at ICLR 2018 22

Under review as a conference paper at ICLR 2018 23

Under review as a conference paper at ICLR 2018 24

Under review as a conference paper at ICLR 2018 25

Under review as a conference paper at ICLR 2018
B LABEL CORRELATIONS FOR CELEBA AND 2D SHAPES DATASETS
Figure 3: VAE latent correlations with attributes for CelebA dataset. 26

Under review as a conference paper at ICLR 2018
Figure 4: -VAE ( = 2) latent correlations with attributes for CelebA dataset. 27

Under review as a conference paper at ICLR 2018
Figure 5: -VAE ( = 4) latent correlations with attributes for CelebA dataset. 28

Under review as a conference paper at ICLR 2018
Figure 6: -VAE ( = 8) latent correlations with attributes for CelebA dataset. 29

Under review as a conference paper at ICLR 2018
Figure 7: -VAE ( = 64) latent correlations with attributes for CelebA dataset. 30

Under review as a conference paper at ICLR 2018
Figure 8: DIP-VAE latent correlations with attributes for CelebA dataset. 31

Under review as a conference paper at ICLR 2018
Figure 9: VAE latent correlations with attributes for 2D Shapes. 32

Under review as a conference paper at ICLR 2018
Figure 10: -VAE ( = 4) latent correlations with attributes for 2D Shapes. 33

Under review as a conference paper at ICLR 2018
Figure 11: -VAE ( = 20) latent correlations with attributes for 2D Shapes. 34

Under review as a conference paper at ICLR 2018
Figure 12: DIP-VAE latent correlations with attributes for 2D Shapes. 35

