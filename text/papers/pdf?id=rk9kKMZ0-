Under review as a conference paper at ICLR 2018
LEAP: LEARNING EMBEDDINGS FOR ADAPTIVE PACE
Anonymous authors Paper under double-blind review
ABSTRACT
The parameterization of mini-batches for training Deep Neural Networks (DNN) is a non-trivial problem. In this paper, we propose a Self-Paced Learning (SPL)fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. In LEAP, we train an embedding Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The student CNN classifier dynamically selects samples to form a mini-batch based on the easiness from cross-entropy losses and true diverseness of examples from the representation space sculpted by the embedding CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST and CIFAR10. We show that the LEAP framework can achieve a higher convergence w.r.t. the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. Our framework is implemented in PyTorch and will be released as open-source on GitHub following review.
1 INTRODUCTION
Stochastic non-convex optimization algorithms, which are driven by gradient information on minibatches of the dataset, are the most common way to train Deep Neural Networks (DNNs). Despite recent advances in DNNs making it possible to achieve high performance on several tasks across many application domains such as computer vision and natural language processing, their architectures have a large number of parameters, which raises the importance of data scheduling for training DNNs. The standard method to train DNNs is stochastic gradient descent (SGD) which employs backpropagation to compute gradients. It typically relies on fixed-size mini-batches of random samples drawn from a finite dataset. However, the contribution of each sample during model training varies across training iterations and configurations of the model's parameters (Lapedriza et al., 2013). Previous studies on Curriculum Learning (Bengio et al., 2009, CL) show that organizing training samples based on the ascending order of difficulty can favour model training. However, in CL, the curriculum remains fixed over the iterations and is determined without any knowledge or introspection of the model's learning. Self-Paced Learning (Kumar et al., 2010) presents a method for dynamically generating a curriculum by biasing samples based on their easiness under the current model parameters. This can lead to a highly imbalanced selection of samples, i.e. very few instances of some classes are chosen, which negatively affects the training process due to overfitting. Loshchilov & Hutter (2015) propose a simple batch selection strategy based on the loss values of training data for speeding up neural network training. However, their results are limited and the approach is time-consuming, as it achieves high performance on MNIST, but fails on CIFAR-10. Their work reveals that selecting the examples to present to a DNN is non-trivial yet the strategy of uniformly sampling the training data set is not necessarily the optimal choice.
Jiang et al. (2014b) show that partitioning the data into groups to use diversity and easiness in their Self-Paced Learning with Diversity (SPLD) framework, can have substantial effect on training. Rather than constraining the model to limited groups and areas, we can spread the sample selection as wide as possible to obtain diverse samples of similar easiness. However, their use of K-Means and Spectral Clustering to partition the data into groups can lead to sub-optimum clustering results when learning non-linear feature representations. Therefore, learning an appropriate metric by which to capture similarity among arbitrary groups of data is of great practical importance. Deep
1

Under review as a conference paper at ICLR 2018
Metric Learning (DML) approaches have recently attracted considerable attention and have been the focus of numerous studies (Bell & Bala (2015); Schroff et al. (2015)). The most common methods are supervised, in which a feature space in which distance corresponds to class similarity is obtained. The Magnet Loss (Rippel et al., 2015) presents state-of-the-art performance on fine-grained classification tasks. Song et al. (2016a) show that it achieves state-of-the-art on clustering and retrieval tasks.
To summarize, this paper makes a number of contributions toward improving mini-batch formation:
· We propose a general sample selection framework called Learning Embeddings for Adaptive Pace (LEAP) that is independent of model architecture or objective, and learns when to introduce certain samples to the DNN during training.
· We exploit a new type of knowledge --similar instance-level samples for knowledge transfer through DML from an embedding network for self-paced sample selection.
2 RELEVANT WORK
2.1 LEARNING SMALL AND EASY
The perspective of "starting small and easy" for structuring the learning regimen of neural networks dates back decades to Elman (1993). Recent studies show that selecting a subset of good samples for training a classifier can lead to better results than using all the samples (Lee & Grauman, 2011; Lapedriza et al., 2013). Pioneering work in this direction is Curriculum Learning (Bengio et al., 2009), which introduced a heuristic measure of easiness to determine the selection of samples from the training data. By comparison, SPL (Kumar et al., 2010) quantifies the easiness by the current sample loss. The training instances with loss values larger than a threshold, , are neglected during training and  dynamically increases in the training process to include more complex samples, until all training instances are considered. This theory has been widely applied to various problems, including dictionary learning for image classification (Tang et al., 2012), object detection (Sangineto et al., 2016), multimedia event detection (Jiang et al., 2014a), long-term tracking (III & Ramanan, 2013), visual tracking (Huang et al., 2017) and medical imaging analysis (Li et al., 2017). In SPLD Jiang et al. (2014b), training data are pre-clustered in order to balance the selection of the easiest samples with a sufficient inter-cluster diversity. However, the clusters and the feature space are fixed: they do not depend on the current self-paced training iteration. Adaptation of this method to a deep-learning scenario, where the feature space changes during learning, is nontrivial. Our self-paced sample selection framework aims at a similar goal but the diversity of samples is obtained with a DML approach to adaptively sculpt a representation space by autonomously identifying and respecting intra-class variation and inter-class similarity.
2.2 LEARNING REPRESENTATIONS BY METRIC EMBEDDING
Deep metric learning has gained much popularity in recent years, along with the success of deep learning. The objective of DML is to learn a distance metric consistent with a given set of constraints, usually minimizing the distances between pairs of data points from the same class and maximizing the distances between pairs of data points from different classes. DML approaches have shown promising results on various tasks, such as semantic segmentation (Fathi et al., 2017), visual product search (Kiapour et al., 2015), face recognition (Schroff et al., 2015), feature matching (Choy et al., 2016), fine-grained image classification (Zhang et al., 2015), zero-shot learning (Frome et al., 2013) and collaborative filtering (Hsieh et al., 2017). DML can also be used for challenging, extreme classification settings, where the number of classes is very large and the number of examples per class becomes scarce. Most of the current methods define the loss in terms of pairs (Song et al., 2016b; Ustinova & Lempitsky, 2016), triplets (Schroff et al., 2015; Wang et al., 2017) or n-pair tuples (Sohn, 2016) inside the training mini-batch. These methods require a separate data preparation stage which has very expensive time and space cost. Also, they do not take the global structure of the embedding space into consideration, which can result in reduced clustering. An alternative is the Magnet Loss (Rippel et al., 2015) and DML via Facility Location (Song et al., 2016a) which do not require the training data to be preprocessed in rigid paired format and are aware of the global structure of the embedding space. Our work employs the Magnet loss to learn a representation
2

Under review as a conference paper at ICLR 2018
space, where we compute centroids on the raw features and then update the learnt representation continuously. To our knowledge, the concept of employing DML for SPL-based DNN training has not been investigated. Effectively, an end-to-end DML can be constructed to be a feature extractor using a deep CNN which can learn to sculpt an expressive representation space by metric embedding. We can use this feature representation space which maintains intra-class variations and inter-class similarity to select samples based on the true diverseness and easiness for the CNN we want to learn. Our architecture combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol.
3 LEARNING EMBEDDINGS FOR ADAPTIVE PACE (LEAP)
The Learning Embeddings for Adaptive Pace (LEAP) framework consists of a dual DNN setup. An embedding DNN learns a salient representation space, then transfers its knowledge to the self-paced selection strategy to train the second DNN, called the student. Thus, an embedding CNN is trained alongside the student CNN we want to train (Figure 1). For now, but without loss of generality, we focus on training deep Convolutional Neural Networks (CNNs) for the task of supervised image classification. The embedding network is trained to learn a representation space using the Magnet Loss, because a more salient representation, in theory, enables improved classification performance. In this framework, we want to form mini-batches using the easiness and true diverseness as sample importance priors for the selection of training samples. Given that we are learning the representation space adaptively as training progresses, this has negligible computational cost compared to the actual training of the student CNN. We evaluate our framework on image classification datasets such as MNIST, FashionMNIST (Xiao et al., 2017) and CIFAR-10, allowing for a comparison of our results with other relevant sample selection strategies.
Figure 1: The LEAP framework, consisting of an embedding CNN that learns a representation for the student CNN to create a self-paced strategy using easiness and true diverseness as sample importance priors.
3

Under review as a conference paper at ICLR 2018

We adopt the Magnet Loss approach which is based on learning the distribution of distances for each

sample, from k clusters assigned for each class. It then uses an intermediate K-Means clustering,

to reposition the different assigned clusters. This proved to allow better accuracy than both margin-

based Triplet Loss and Softmax regression. The samples are fed into the embedding CNN in mini-

batches of random samples from the training data. The aim of LEAP can be formally described

as follows. Assuming that the training samples X = (x1, . . . , xn)  Rmxn are grouped into k clusters in the learned representation space through DML: X(1), . . . , X(k), where columns of X(j)

 Rmxnj correspond to the samples in the jth cluster, nj is the number of samples in each cluster

and

k j=1

nj

=

n.

The

weight

vector

is

denoted

accordingly

as

v

=

[v(1), . . . , v(k)],

where

v(j)

= (v1j , . . . , vn(jj))T  [0, 1]nj . Similar to the conventional SPL, we need to assign non-zero weights v to easy samples, and increase the diversity by dispersing non-zero elements across possibly more

groups' v(j). This leads to the optimization model similar to the one presented in SPLD, where ,

 are the two pacing parameters for sampling based on easiness and the true diverseness as sample

importance priors, respectively. The student CNN receives a diverse cluster of samples, the up-to-

date model parameters , ,  and the outputs of v of minvE(, v; , ) for extracting the global

optimum of this optimization problem.

4 EXPERIMENTS
All experiments were conducted using the PyTorch framework, while leveraging containerized multi-GPU training on NVIDIA P100 Pascal GPUs through Docker. We compared our LEAP framework against the original SPLD algorithm and Random sampling on MNIST, FashionMNIST and CIFAR-10. The  and  pace parameters are kept consistent between the SPL strategy in LEAP and the original SPLD algorithm to ensure a fair evaluation. In our experiments, we mainly compare convergence in terms of number of mini-batches required to achieve a comparable state-of-the-art test performance. We also visualize the original high-dimensional representations using t-SNE (van der Maaten & Hinton, 2008), where the different colours correspond to different classes and the values to density estimates. The following sections discuss the experimental setups and results in more detail.
4.1 LEAP WITH LENET + LENET ON MNIST
In the experiments for MNIST, we extract the feature embeddings from a LeNet (LeCun et al., 1998), as the embedding CNN, and learn a representation space using the Magnet Loss. The fullyconnected layer of the LeNet is replaced with an embedding layer for compacting the distribution of the learned features for feature similarity comparison using the Magnet Loss. The student CNN (classifier) was also a LeNet which we then trained with our LEAP framework. The embedding CNN was trained with randomly sampled mini-batches of size 64 and optimized using Adam with a learning rate of 0.0001. The results in Figure 2 show that the test performance is comparable to that of Random sampling and shows better convergence on the standard SPLD algorithm. The learned representation space of the MNIST dataset using LeNet is presented in Figure 3b and the convergence in Figure 3a. The MNIST experiments were primarily carried out to show that the LEAP framework can be deployed as an end-to-end DNN training protocol.

4

Under review as a conference paper at ICLR 2018

Test Accuracy (%) Test Loss

1.00 0.98 0.96 0.94 0.92 0.90 0

MNIST

0.14

0.12

0.10

LEAP SPLD Random

0.08 0.06

0.04

0.02

5000

10000

15000

20000

Mini-batches

0.00 0

MNIST

LEAP SPLD Random

5000

10000

15000

20000

25000

Mini-batches

Figure 2: Averaged results of 5 independent runs on MNIST. The solid line represents our LEAP framework.

Train Loss

18 16 14 12 10 8 6 4 2 00

2000

4000

MNIST
6000 8000 10000 Mini-batches
(a) Train Loss

12000

66

LeNet 14000 16000

0 0

000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006000000000000000000000000001011000101001000000000000000000000011100101040000100400101010011001000011001000000010000000000000100611001010100100011100000006001061011000010100101111001010100001000010111111110066000100001010000101000110010001001011101001100110604110010160010011100100160110110001141011601110100000100110001111110110011101101110001110801000001111111100100011011111111140110000111001111011101040116100001110016111111000101004010100011100110411100116101010111111100010116011110100000101011001610101111000100661110011011000106011000100101101111100110110011001611110100100411010101111101111110066111040000014011161104140010141001114414110111101010111110104011106101411111001141610101114611400401160100101014104146100114011010611606111116111460011111611411101101161401101111101110011014440441011101004146111104141010011114460144446160661104111110414100104114111146401111066101140016111440161016161101141140100116161041111116011101114044040110114116461110044101114140411116040040101411464441114001661110001060111414064116610041661114441106644100441016446046014401406101446110611016140614140011661014616601041600601161101404064440166614164144066446010141446644400144414106161441140444006044404441100611116614166646614161146646046064414141166064464101444146616116014404104464446410664066614614461166411406444610400444664044046666064461066661664644466444666060444116661664644444616140146606406414016164664441160420646046446660646441410441614164661416464411644696660461446441464461086466646046446641444164060466466610604664446624466661464444614464466664646464440414644446446464644164446410464444462060406166440644446446466646666446044466464641144646464406466144066444446660446044464446644464044666446666666464664646466666416604166644646161446640416666664616646484621641666044404641444604464620444666144646466466666644644460464664266444260244446644446646160460446646666004461464460446644646626406044444446464446464241466464664441646464406644462644664268446644662644666444644644666666166644414204044666424466484448661666442642664666640624446406444646664626444604446404666464404461466640641464421424646616266441616664646644664264464664046616648864664461662488444148664842666444644466662662246664646664641446424668164666462466244646642466660412421684446466626888114666924664444642624268616684162846466688626286664462208606461444698166622466626464266662822686846418444666248661162626244266268626264226486426668621824244648626862444226684676864246886862782446868828226426242824482444228464146664882284148644246244666484264668448228222868842282886626264642222242826286282666422622226240889466422228868662622208828848222280562422662228228244822882888682282612226282488482262686282268888246626622822888288826224824862862286812842228882682662682862282242282828828222208888882884888286882212622422626828282222828222269828882629626826828642242228228822222286887982278688882284682278826268882781822862222246828868826282869882288888267222222268188422288846826228226828824822222282468286868828868482288822222864882268842268282862829283428887829988222226248486628262626488226288228822922821822888882822225288292282288882228292867858828896288288226828228782242828862288828887882282822848268722628888887886228888228282882822226282292789688928258897288882288492822988878792282882922282288785882822282786868828284288822268888982768828888822278828289828288828628282222782282888888288228888486288222782822882299828228987282674822926282878878862828288778238822882827896622882888832927288827227688828222282262297882778288972288888828278888789225222722828872888982888277942822888982868828672269729282898872288826779288222989222222222982972878678227878288822298928798882997227928982998269822728228898292272782728769828852729288889778999887727876292787799897829929777889888825299279987277298289289842728897989798299879888827299282987229899288879989987728868998772279928227778267298728697288982922999872862972989959878898899999892968759777727987899578229759789977979822992979792775979722737897997599972979882799897479977983967972927277929939977739759258278979989797827777929259797999922857779799939789929977777992778297979279799979759778772599929779779998999977797979977725977799937997297978959927777797797997976929795977779779997959977279759599379799799777797779979999297792977987979997975979597579797277939779779777397729997999997279979979879777777599979995797977979997979977799799377975797579997599979739799979775729777975779977997959979797999797779779757995999997799579937993357799757997777779995999997779977775597797997997397995277973779995799777979777993977777777799995999739993993289995737799799539797975999757999777799777797999797795597799979979797537777997979739779999799557999999779777937775977975979997799973999575795977599777999797799979775377597977775775797957997977977779757977757957399577393799577527775399797977937597979577577995959755777797995773355399575393777795779555537977797937995995735979777795777599737797777557977775797579795995957797977777779955959997573975755737979775379959775397759997779599579779733777775977559775777995779957357575957757775579737979375777557355577573977577757737595573555393359575555757553575593577775535735737577777555777757757935993577775955595753593755357777777573753355737579557955579555557973555557375577353773555755553353575757559775979733575757535753553555555375557777575735359777555335535559359533577735355793557557535755555597555537535353537775555555535357335373553555553555355555375335555535335535535573555555553535555335357557335553535355335553555335333553533537355335555553553555753553555555555355755535353335553535355355355353553355553533355533555553535353533553553355553353335555335355355535535537555353553553555553555535555553333335533335553533333533355553355553335535333555555355555553337333353373533355353553535353553335333533553555553333333353553553535553333553333533535553535333335333355335533333533353533535535553553335335533333335533353355353333533333355335533533553333533553553533335333553353333533333333333335333335535333533333333353533333335333535335333333333355333333333353533533333333533333333533335333333535355333353533333333333533333333333333335533333353333333333353533333333333333333333333333333335333333333333333333333333333333333333333333333333333333533333333333335333333333333333333333333333333333333333333333333333333333333333333333333333333333333353333333

3 33 3 3

3 3

(b) MNIST feature representation space

Figure 3: (a) The train loss with the moving average and (b) the feature representation space learned by LeNet (embedding CNN) using the LEAP framework on MNIST.

4.2 LEAP WITH LENET + RESNET-18 ON FASHIONMNIST
The experiments for FashionMNIST had a similar setup to MNIST, however we use a ResNet-18 (He et al., 2016) for the classifier. The embedding CNN remains the same LeNet for feature extraction, and is trained using an identical setup to the MNIST experiments. The classifier is trained using SGD with momentum of 0.9, weight decay of 0.0005 and a learning rate of 0.001. The FashionMNIST dataset is considered a direct drop-in replacement for the original MNIST dataset, with a training set of 60000 examples and a test set of 10000 examples. Each example is a 28×28 grayscale image, associated with a label from 10 classes. We performed data augmentation on the training set with normalization, random horizontal flip, random vertical flip, random translation, random crop and random rotation. In comparison to SPLD and Random sampling, the results in Figure 4 reveal that LEAP converges to a higher test accuracy with a fewer number of mini-batch updates before saturating. Figure 5b depicts a learned representation space of the FashionMNIST dataset and the convergence of the embedding CNN in Figure 5a.

5

Under review as a conference paper at ICLR 2018

Test Accuracy (%) Test Loss

1.00 FashionMNIST

0.7 FashionMNIST

0.6 0.95
0.5

0.90

LEAP 0.4

LEAP

SPLD SPLD

0.85

Random

0.3

Random

0.2 0.80
0.1

0.75 0

5000

10000

15000

20000

Mini-batches

0.0 0

5000

10000

15000

20000

25000

Mini-batches

Figure 4: Averaged results of 5 independent runs on FashionMNIST. The solid line represents our LEAP framework.

Train Loss

18 16 14 12 10 8 6 4 2 00

2000

4000

FashionMNIST
6000 8000 10000 Mini-batches
(a) Train Loss

12000

LeNet 14000 16000

111

1 1111131111111111111111111111111111311111111311131131311133111113133111111131311113131111111111311111313111311111111131113111133311131111310131131311111111131113133111333111113131111111111111111131111111113331111111113113311331130113311313113131111113313133111111331113111113113331311131111313111111313111311131311111111311313313111131113131311131111113311311111131331111113111111513156313113111313311113311311133101311311113111133011131115111331113130133133111311103531315110133111331111133111113031313113131333113111333311111111533110111111313303131115111011333011113133111130311101331303113313313111333131330101336151306301113531313353131303313111531331133311511311131135031033131300113315531130331331313130131300513311331031111133113135533333111333153531531511131115150350130333105500313153133011301311111313013313113335333331033113511535163311301111300555330313000103100155503101131351533111355500330013310333003351103103100110113106303133511303310005355160151553330133111063515550315315030115531053015150133035113303053113655300133135333510111333533513031305350001033555331001100353155531135053513310513130335000011535315515155150003153000533313335135031503365503030005303501015031503530513135131010500313115305613033005533001053030153135555511305535530003155030500505506500013050500500013055130035311053155503003116137053011003050515301535555510300350153053500530005515030155015505556030530100553333006330016550133015316503650100003000155536350530553303005363355505535000551351055033305005650010053355355013305550035035504330005555533035550100550035050016353063030005053500053061105545350151013500550353355555030350535150535550333003030563150333055350600551057353003363350055565610515301015003000005350656153055303655300006565006650360533005030330150535005550000423050051050060300030555051055350105505033555653605500055760363305550550603550000013363555033510503565056555555333636750505065055055651365063500000500350550505553050055650015560355036505605504505550105131050550500335503553030505010660300573665506655050330553013160350560655506000030017153303157657755750550555635005056661350513556054075360055505605536555360505050036017570355016555503053550066106650060500555036306155305036000532536537653035635650430065000357430566530323501576053600506560316515375603565565070016705506365666075357760300503065571655030035353050300506065556000550050356060706005530460755650063056657005000006355555300060635565626065505776750007350037150707065556050056007550560053570335577056038705666650665006507655007360055057670766753555005005506000563033006527536655606530604670551365550300036003706603530074303603635606550553605060076530756600746646503036656736663026665060566306035777472522575065570665741577677576567370526557100066007606053665065750576665755536705650654007763777565406366135765570407753050600777550656575556375565647032657633655276353770666630007445757743557577653677667777004273556004578625007375646646457637007664556743776067436354370067777463376600603356567570306775766706057770647067777760733773146772736776455606377176067666434736672672570775774647766366056773267477476077175776467776537577557777666653207007744777547745777477077787306487706763077077677777420677576653877774477777777064774731766767708677777277342767777707777766077747677526287776537774777574757474778520075772557758363727467707777726872907377777777605745876700887746777677203077477469777477776672777732752708272617285277770447767377522277776077766857076772257856277777278847660262772777676674734777875777727675776777762726277876647374407876507777768272276727672774467727048787772247746237755726774874287678672767778642277287027782676567574776676272262445787767972287272727768872886426277466727767272626777728077762272672688277277262784668724787786682766876326847267027727277224422767476267767782772377622786648627882477267642777484727646226766767267482676268278768666462877272666827722676792227642878278876727682696627886666682876872226767772642686222767628767877282287886667627646624688678427762777627864268766776882684878667622776876274627878228762462964882422688662672666276668846784686268482287773882828426762827667266888867892826667682224287796682662278428827725686276476682662287782982668746282652264667222879887486629778766786266669748648668828967984622682672288788677626268627828428687284766626627498786689828888864688266867682786278762692884828866266678868642484882268668666726827276628884422284487867862846886286686827788889272874268866674966688887486666746286288728962268444866284286268889628227286768874282686282246268769628886948666688824762828878778680867684282446962886826687829682228842464872982828726272764628878686266662886697824722498297288288648482828427748868249929872827988872482864249646898882277228227922286926692244462762292882886786968626288222862674878227648822226962826982288478628868647566972647882826609284686288982869228982927426228784427262946228888882822928889499826964642768729948762894886927284828888829272262829889904222884898228296897867688827994874264929678269747974476842222279988242622484872229824248897882627982262828262227692228988828884728899864292882968499788276249224292872298744882464269266282289488484494227226728979284628442674822447697298294826689244242924272924498428268492899287298984828222692842282476942422464929299449868882822728989828248928462726978422267994294842998929244888842947944982968928242268764844692989889928928886478242892878426488446228824976998949292294928222298248942299489994228228482248424482422424228992224249424498999822948848992884468992494998298998248282499728999828928499648492992694964922994228842674999799988498824494496274299999699868989489884429994282998969924894942229894229999248444797929829882849989488228448989928299924428229482964962984894299888492429424242996848986289248848229244648984297942889479494228444948999782299924244992489944248899899428984428929994988424449484299294894287488492444992288424248889729899922944494429999489848699998788994249848698929948999288894999249228448948946999429244948299998984949944448494829284998979242499224444489499248494888224294699899944824964429798244429484494429982949994429449494244222944849929889999424224494894994444882929942889244944249489424299229969448829994929444492994999994848229999849984449989284999989949429249999448929848249994929989484994448249442989448994949929949929444944999948949924429948894949449998429448949494947449949899299492992948494499949449999942999984942899298492999989849949949444949892894442442994499998499449448499424944299944994989899924944442994444949494249929244492442449929449849924999929899429994999499299499929849999444424944992999992299949996494492749929492294949442924494289499924449499989929994422444424949949494224429442944294994444222444292999944994442292842999944949249989424494499224489449244444999949449299924949999944294424444494992999492444499494944

4

4 44

77

(b) FashionMNIST feature representation space

Figure 5: (a) The train loss with the moving average and (b) the feature representation space learned by LeNet (embedding CNN) using the LEAP framework on FashionMNIST.

4.3 LEAP WITH VGG-16 + RESNET-18 ON CIFAR-10
We ran two sets of experiments on CIFAR-10, one with a fixed learning rate and the other with a learning rate scheduler identical to that of the Wide ResNet (Zagoruyko & Komodakis, 2016) training scheme. The CIFAR-10 training set was augmented with normalization, random horizontal flip and random crop. We used VGG-16 as our embedding CNN and ResNet-18 as our classifier in the CIFAR-10 experiments. We chose ResNet-18 over other architectures because it is faster to train and achieves good performance on CIFAR-10. Our experiments revealed that VGG-16 (Simonyan & Zisserman, 2014) learned strong and rich feature representations which yielded the best convergence on the Magnet Loss. Therefore, we treat the VGG-16 model as a feature extraction engine and use it for feature extraction from CIFAR-10 images, without any fine-tuning. In the first experiment, the classifier is trained using SGD with a momentum of 0.9, weight decay of 0.0005, a fixed learning rate of 0.001 and batch size of 128. In the second experiment, the classifier is trained with batch sizes of 128 as well, using SGD with a momentum of 0.9, weight decay of 0.0005 and a starting learning rate of 0.1 which is dropped by a factor of 0.1 at 60, 120 and 160

6

Under review as a conference paper at ICLR 2018

epochs. In our experiments, this would translate to 23400, 46800 and 62400 mini-batch updates. In both experiments, VGG-16 is trained with randomly sampled mini-batches of size 64 and optimized using Adam at a learning rate of 0.0001.
At a fixed learning rate of 0.001, training ResNet-18 with the LEAP framework results in a faster convergence to achieve higher test accuracy than either Random sampling or SPLD (Figure 6). Interestingly, SPLD and Random sampling show comparable results on an average of 5 runs. This is because SPLD uses K-Means to partition its data into k clusters at the start of training, which would lead to sub-optimal clustering results containing samples that are not of similar-instance level. As a result, the SPLD would not be selecting diverse samples of similar-instance level when traversing through the different clusters. On the other hand, our LEAP framework ensures that optimal clustering is achieved using the Magnet Loss and the classifier is able to use the learned representation space adaptively as training progresses. This ensures that during early stages of training, the minibatches being fed into the student CNN are parameterized with truly diverse and easy samples. As the model matures, the mini-batches maintain diversity, as well as a mix of easy and hard samples.

Test Accuracy (%) Test Loss

1.0 CIFAR-10 with LR Fixed
0.9 0.8 0.7 LEAP
SPLD 0.6 Random 0.5 0.4 0.3 0 10000 20000 30000 40000 50000 60000 70000
Mini-batches

1.8 CIFAR-10 with LR Fixed
1.6 1.4 1.2 1.0 LEAP
SPLD 0.8 Random 0.6 0.4 0.2 0.0 0 10000 20000 30000 40000 50000 60000 70000 80000
Mini-batches

Figure 6: Averaged results of 5 independent runs on CIFAR-10 with a fixed learning rate of 0.001. The solid line represents our LEAP framework.

The second set of experiments we ran on CIFAR-10 with the learning rate scheduler shows that LEAP converges faster earlier on in training compared to SPLD and Random sampling (Figure 7). As the learning rate drops by a factor of 0.1 at 23400, 46800 and 62400 mini-batch updates, the classifier under the LEAP training protocol eventually achieves a higher test accuracy than SPLD and Random sampling. Also, the test loss of LEAP is lower than that of SPLD and Random. This is expected because LEAP is designed to sample for heterogeneity, thus maximizing the diversity relevant to the learning stage of the student CNN, which also helps prevent overfitting. The learned feature representation space of the augmented CIFAR-10 data is shown in Figure 8b and the convergence of the embedding CNN is presented in Figure 8a.

7

Under review as a conference paper at ICLR 2018

Test Accuracy (%) Test Loss

1.0 CIFAR-10 with LR Scheduler
0.9 0.8 0.7 LEAP
SPLD 0.6 Random 0.5 0.4 0.3 0 10000 20000 30000 40000 50000 60000 70000
Mini-batches

1.4 CIFAR-10 with LR Scheduler
1.2 1.0 0.8 LEAP
SPLD 0.6 Random 0.4 0.2 0.0 0 10000 20000 30000 40000 50000 60000 70000 80000
Mini-batches

Figure 7: Averaged results of 5 independent runs on CIFAR-10 with a learning rate scheduler identical to the Wide ResNet training scheme. The solid line represents our LEAP framework.

Train Loss

9 8 7 6 5 4 3 2 1 00

2000

CIFAR-10
4000 6000 8000 Mini-batches
(a) Train Loss

0

10000

VGG-16 12000

1

1 1
1 1

1 1 1 1

11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111119111111111111111111111111111111111119111111191111119111111111111111111111111111111111111911911111111111919119111111191111111111111111119911819111119191119911111111111119191111119111111111119911119119911118111191111919111111119111911111119191111119111191191111998911999911199999119191111111191911911111998919119111919911998119919111198911111911198999918199888119891191919111119199111199999119991998111199911111991991198191981911111119919919991991199189991199199991118111811919911191198998199981899111999898111199199199198899999911119889811991199991911919119899919191198811989911989187198888199111911199999989991991198889919189819111911891889981819119999998118981999119999191989188889191919998119889891898999999988111999981911889919181999819899989988919981889878819199198981999118981899919899819991988999998811999919988998889899199198989891119991999989899988919998991999898898999198918999988889988989889799919988899891989890997891999898111818999998998979918888889991898888981711881119991988999889999999988898188919811987989888199999989897811898991888989999998919798899899999989998989889989999789989719818889891899999988898898919988718999179987789788899889899888888877887888811989918999888989788998987898817819819889889778188889888798181997888888881898888897898989181918989888818989979888898999979187898899971888898999988799898878978877977899887918819789798818878888888898987999908777818919189879978988998799788998989798889797899887980899998788189778888878888778787997780799881979788877897798817877887917817888899188797798787879887987778888187797918817878899888798189978897898889989877070788879879888978787090878978888978781798888788789889888887998787887988797707788877897877777797978847787787777177197888978888797777970787997979007898898778070988797979787170788777999778870717788097788799787798009777777940870787898870718787890770078707779007778778878897980408887070879747877799777709878777717777897879708709708888887170787097778080807784778077818787700897880870878887777078747794777708000078777777974077789477700007889007707807748777794980047770774708080087797771018908707707878877848777777777747874748078877778707774977774087781904040847778707770888400887077877784077878479107807777440770074877747900708777777707784874770747440840791878888070087070777971794004770770774477700787100807577780007874740907077007178470747770074784048409407040804771077047478047749879740770440470774088007774779898007404400477000477844778897847300047047740778787870043780797490700777444007470800440070077984044877474700407747707000070877470777774874744704043040478770077707074507007447477744000170707477344844709477080707475474747000070807044077700070400444740008840274003047477440744980849377074408777444447444770447004407444747404007474777700740403077040470977704000474740444400440775477008808470497347044574008707470477007040777880400044070784040479774777045404478400880700004440700048370777734474444004400043047484050700430074747304980407047407407484440474400702543001233737054770044404470404400241404004344004304044047340434447303757005403445077004004400444000734847804744520445477402304404874044007437473743004054007500054400545044434540044074430040440444543404050004475347244030047475404420057074070440724444485005040474244380340504477400024452400000837445000740470450000484504040745030303204434330044404074405087034484440340554344043047007004340544040704034435745004800040524433053003437200500352040848454544440054044034005034004244004000300305087540430344704005704300203330344054450534400350040444054027434545440440004240250334400500035050535408000444400404704040444030335003537533424305000320438400554025053537032447050533354043403353523005204334305040403032030520444343330334535003000485400025454554325345052050404352235303333543433302003223022003572504344333204035054514250453234045233400304004270470045002035543350000234222332535033324052505553304300325433035400020033303573405253525272520435005450064505033224404555304004522333552535233240235452530502405503323265540045044003043465352535023025034243325054533330200332453323303024333552853253224503523543330023325355305334545573525530534505305335553320333505663525535235530552353232003353305560225252303353233033303353323225235532244530023233632430533305236335230234550025555332503355432032253333452230252532403363223253332533225354250333555033625323025235259334225553553322353363235062223030252033553435322533522233555535535552356523532363275563272533203554206222235332300335225555332623822625365553323320502232250323352525335832253533523322525332055520505066333357322550333325232050525324553353355352226353233665535222236235632225302356322533235523255030253255323222338556223352235232350223323350333230225335323255202553343523523253525523653345524222555243004503436133320635352325322525524333353532353220253326366653353322525205352302535622252626353256325532555523523333352232323552252252053232232223520332222553533634222325652632236553256532322252325532555033653263223522555222065553353223623246335523356532253633523225335220632232363555352306553535535262522622552225222653325323232222223222623534335555553355223555246625232365662352232325335525332560523252262653553325552225535365255263525256652552365226352532353235226656262555255265556252323262352232633553225322336255553025652625566233222352253522532362263652263253526532635232222602352353335522532352622255536263623353256232325230656222362622355236625532256225256653323252635235265626236326325222532625262222563566366225326525356322326352566552225236623656235555022255623626343563622265665262522656532522362662652365262622223636566352252326223565562256256226522552355552226352652666262252566556666235262256525662255626226565263665222225256662656255563662662662326656665262255262663536626636555266266525666633226662632535562266626256222553662662363326652626252566632626656225666662565625665266256626222656666236662266226626655626252662662666266622666626636556622565326262656666626652352525626266666626325624565222566666562632653666563666626226666525526663566636626626262226636666266626266626266666556666626666662666662265666656666662666626636665665265665622666266266265662666626666262666266666662663666626666626666666666666666626666656666666626666666666566666666266266626662666666666666666666666666666666566666666666666562656665666666666666626666666662666665666666666666666666666666626666665666666666666662566666666666626662666666666666666666666666626666666666666666666666666 66

7

(b) CIFAR-10 feature representation space
5

Figure 8: (a) The train loss with the moving average and (b) the feature representation space learned by VGG-16 (embedding CNN) using the LEAP framework on CIFAR-10.

5 DISCUSSION AND FUTURE WORK
An important finding is that fusing a salient non-linear representation space with a dynamic learning strategy can help a DNN converge towards an optimal solution. A random curriculum or a dynamic learning strategy without a good representation space were consistently found to converge more slowly. Biasing samples based on the easiness and true diverseness to select mini-batches shows improvement in convergence to achieve classification performance comparable or better than stateof-the-art. In cases where the classification dataset is balanced and the classes are clearly identifiable, we showed that our end-to-end LEAP training protocol is practical. However, an interesting line of work would be to apply LEAP on more complex classification datasets where there is room for improvement on classification results. Another interesting area of application would be learning representations using DML for different computer vision tasks (i.e. human pose estimation, human activity recognition, semantic segmentation, etc.) and fusing a representative SPL strategy to train the student CNN.

8

Under review as a conference paper at ICLR 2018
6 CONCLUSION
We introduced LEAP, an end-to-end representation learning SPL strategy for mini-batch formation. Our method uses an embedding CNN for learning an expressive representation space through a DML technique called the Magnet Loss. The student CNN is a classifier which can exploit this new knowledge from the representation space to place the true diverseness and easiness as sample importance priors during online mini-batch selection. LEAP training achieves faster convergence and higher test performance on MNIST, FashionMNIST and CIFAR-10 using a combination of deep CNN architectures. We hope this will help foster progress of end-to-end SPL fused DML strategies for DNN training, where a number of potentially interesting directions can be considered for further exploration. Code reproducing these results will be available on GitHub post-review.
ACKNOWLEDGMENTS
Acknowledgements were removed for double blind review.
REFERENCES
Sean Bell and Kavita Bala. Learning visual similarity for product design with convolutional neural networks. ACM Trans. on Graphics (SIGGRAPH), 34(4), 2015.
Yoshua Bengio, Je´ro^me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pp. 41­48, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1. doi: 10.1145/ 1553374.1553380. URL http://doi.acm.org/10.1145/1553374.1553380.
Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal correspondence network. In Advances in Neural Information Processing Systems 30. 2016.
Jeffrey L. Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71­99, 1993.
Alireza Fathi, Zbigniew Wojna, Vivek Rathod, Peng Wang, Hyun Oh Song, Sergio Guadarrama, and Kevin P. Murphy. Semantic instance segmentation via deep metric learning. CoRR, abs/1703.10277, 2017. URL http://arxiv.org/abs/1703.10277.
Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, MarcAurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Neural Information Processing Systems (NIPS), 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770­778, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin. Collaborative metric learning. In Proceedings of the 25th International Conference on World Wide Web (WWW), 2017.
Wenhui Huang, Jason Gu, Xin Ma, and Yibin Li. Self-paced model learning for robust visual tracking. Journal of Electronic Imaging, 26(1):013016­013016, 2017.
James Steven Supancic III and Deva Ramanan. Self-paced learning for long-term tracking. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, June 23-28, 2013, pp. 2379­2386, 2013. doi: 10.1109/CVPR.2013.308. URL https://doi.org/ 10.1109/CVPR.2013.308.
Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G. Hauptmann. Easy samples first: Selfpaced reranking for zero-example multimedia search. In Kien A. Hua, Yong Rui, Ralf Steinmetz, Alan Hanjalic, Apostol Natsev, and Wenwu Zhu (eds.), ACM Multimedia, pp. 547­556. ACM, 2014a. ISBN 978-1-4503-3063-3. URL http://dblp.uni-trier.de/db/conf/ mm/mm2014.html#JiangMMH14.
9

Under review as a conference paper at ICLR 2018
Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2078­2086. Curran Associates, Inc., 2014b. URL http://papers.nips.cc/paper/5568-selfpaced-learning-with-diversity.pdf.
M. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C. Berg, and Tamara L. Berg. Where to buy it: Matching street clothing photos in online shops. In ICCV, pp. 3343­3351. IEEE Computer Society, 2015. ISBN 978-1-4673-8391-2. URL http://dblp.uni-trier.de/db/ conf/iccv/iccv2015.html#KiapourHLBB15.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1, NIPS'10, pp. 1189­1197, USA, 2010. Curran Associates Inc. URL http: //dl.acm.org/citation.cfm?id=2997189.2997322.
Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, and Antonio Torralba. Are all training examples equally valuable? arXiv preprint arXiv:1311.6510, 2013.
Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, volume 86, pp. 2278­2324, 1998. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665.
Yong Jae Lee and K. Grauman. Learning the easy things first: Self-paced visual category discovery. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11, pp. 1721­1728, Washington, DC, USA, 2011. IEEE Computer Society. ISBN 9781-4577-0394-2. doi: 10.1109/CVPR.2011.5995523. URL http://dx.doi.org/10.1109/ CVPR.2011.5995523.
Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek, Jieping Ye, James Thrall, and Quanzheng Li. Self-paced convolutional neural network for computer aided detection in medical imaging analysis. In International Workshop on Machine Learning in Medical Imaging, pp. 212­219. Springer, 2017.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. CoRR, abs/1511.06343, 2015. URL http://arxiv.org/abs/1511.06343.
Oren Rippel, Manohar Paluri, Piotr Dolla´r, and Lubomir D. Bourdev. Metric learning with adaptive density discrimination. CoRR, abs/1511.05939, 2015. URL http://arxiv.org/abs/ 1511.05939.
Enver Sangineto, Moin Nabi, Dubravko Culibrk, and Nicu Sebe. Self paced deep learning for weakly supervised object detection, 2016.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 815­823, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 1857­1865. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6200-improved-deep-metriclearning-with-multi-class-n-pair-loss-objective.pdf.
Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning via facility location, 2016a.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 4004­4012, 2016b. doi: 10.1109/CVPR.2016.434. URL https://doi.org/10.1109/CVPR.2016.434.
10

Under review as a conference paper at ICLR 2018
Ye Tang, Yu-Bin Yang, and Yang Gao. Self-paced dictionary learning for image classification. In Proceedings of the 20th ACM international conference on Multimedia, pp. 833­836. ACM, 2012.
Evgeniya Ustinova and Victor S. Lempitsky. Learning deep embeddings with histogram loss. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 4170­4178, 2016. URL http://papers.nips.cc/paper/6464-learning-deepembeddings-with-histogram-loss.
L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008. Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep metric learning with angular
loss. CoRR, abs/1708.01682, 2017. URL http://arxiv.org/abs/1708.01682. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. Xiaofan Zhang, Feng Zhou, Yuanqing Lin, and Shaoting Zhang. Embedding label structures for
fine-grained feature representation, 2015.
11

