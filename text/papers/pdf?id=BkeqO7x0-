Under review as a conference paper at ICLR 2018
UNSUPERVISED CIPHER CRACKING USING DISCRETE GANS
Anonymous authors Paper under double-blind review
ABSTRACT
This work details CipherGAN, an architecture inspired by CycleGAN used for inferring the underlying cipher mapping given banks of unpaired ciphertext and plaintext. We demonstrate that CipherGAN is capable of cracking language data enciphered using shift and Vigene`re ciphers to a high degree of fidelity and for vocabularies much larger than previously achieved. We present how CycleGAN can be made compatible with discrete data and train in a stable way. We then prove that the technique used in CipherGAN avoids the common problem of uninformative discrimination associated with GANs applied to discrete data.
1 INTRODUCTION
Humans have been encoding messages for secrecy since before the ancient Greeks, and for the same amount of time, have been fascinated with trying to crack these codes using brute-force, frequency analysis, crib-dragging and even espionage. Simple ciphers have, in the past century, been rendered irrelevant in favor of the more secure encryption schemes enabled by modern computational resources. However, the question of cipher-cracking remains an interesting problem since it requires an intimate understanding of the structure in a language. Nearly all automated cipher cracking techniques have had to rely on a human-in-the-loop; grounding the automated techniques in a human's preexisting knowledge of language to clean up the errors made by simple algorithms such as frequency analysis. Such hand-crafted features have fallen out of favor (Goodfellow et al., 2016) as a result of their demonstrated inferiority to features learned directly from data in end-to-end learning frameworks such as neural networks. The question to be addressed is as follows:
Can a neural network be trained to deduce withheld ciphers from unaligned text, without the supplementation of preexisting human knowledge?
The implications for such a general framework would be far-reaching in the field of unsupervised translation, where each language can be treated as an enciphering of the other. The decoding of the Copiale cipher (Knight et al., 2011) stands as an excellent example of the potential for machine learning techniques to decode enciphered texts by treating the problem as language translation. The CycleGAN (Zhu et al., 2017) architecture is extremely general and we demonstrate our adaptation, CipherGAN, is capable of cracking ciphers to an extremely high degree of accuracy. CipherGAN requires little or no modification to be applied to plaintext and ciphertext banks generated by the user's cipher of choice. In addition to presenting a GAN that can crack ciphers, we contribute the following techniques:
· We show how to stabilize CycleGAN training: our CipherGAN achieves good performance in all training runs, compared to approximately 50% of runs for the original CycleGAN.
· We provide a theoretical description and analysis of the uninformative discrimination problem that impacts GANs applied to discrete data.
· We introduce a solution to the above problem by operating in the embedding space and show that it works in practice.
Code available at: withheld
1

Under review as a conference paper at ICLR 2018
1.1 SHIFT AND VIGENE` RE CIPHERS
The shift and Vigene`re ciphers are well known historical substitution ciphers. The earliest known record of a substitution cipher is believed to have dated back to 58 BCE, when Julius Caesar replaced each letter in a message with the letter that was three places further down the alphabet (Singh, 2000).
Plain alphabet a b c d e f g h i j k l m n o p q r s t u v w x y z Shifted alphabet D E F G H I J K L M N O P Q R S T U V W X Y Z A B C
Figure 1: An example of a right-shift-3 cipher.
Using Figure 1, the message "attackatdawn" can be encrypted to "DWWDFNDWGDZQ". This message can be easily deciphered by the intended recipient (who is aware of the particular shift number used) but looks meaningless to a third party. The shift cipher ensured secure communication between sender and receiver for centuries, until the ninth century polymath Al-Kindi introduced the concept of frequency analysis (Singh, 2000). He suggested that it would be possible to crack a cipher simply by analyzing the individual characters' frequencies. For instance, in English the most frequently occurring letters are `e' (12.7%), `t' (9.1%) and `a' (8.2%); whereas `q', `x' and `z' each have frequency of less than 1%. Moreover, the code-breaker can also focus on bigrams of repeated letters; `ss', `ee', and `oo' are the most common in English. This structure in language provides an exploit of efficiency to the code-breaker.
Polyalphabetic substitution ciphers, including the Vigene`re cipher, were introduced to inhibit the use of n-gram frequency analysis in determining the cipher mapping. Instead, the encrypter further scrambles the message by using a separate shift cipher for each element of a key that is tiled to match the length of the plaintext. Increasing the key length greatly increases the number of possible combinations and thus prevents against basic frequency analysis. In the mid nineteenth century, Charles Babbage recognized that the length of the used key could be determined by counting the repetitions and spacing of sequences of letters in the cipher (Singh, 2000). Using the determined length, we can then apply frequency analysis on the index of the cipher base. This method makes it possible to break the Vigene`re cipher, but is very time consuming and requires strong knowledge of the language itself.
There is a rich literature of automated shift-cipher cracking techniques (Ramesh et al., 1993; Forsyth & Safavi-Naini, 1993; Hasinoff, 2003; Knight et al., 2006; Verma et al., 2007; Raju et al., 2010; Knight et al., 2011) many of which achieve excellent results which is what one would expect from hand-crafted algorithms targeting specific ciphers and vocabularies. Work on automated cracking of polyalphabetic ciphers (Carroll & Martin, 1986; Toemeh & Arumugam, 2008; Omran et al., 2011) has seen similar success on small vocabularies. It is a difficult matter to compare the results of previous work with our own as their focus ranges from inferring cipher keys (Carroll & Martin, 1986; Ramesh et al., 1993; Omran et al., 2011), to inferring the mappings given limited quantities of ciphertext (determining unicity distance) (Carroll & Martin, 1986; Ramesh et al., 1993; Hasinoff, 2003; Verma et al., 2007), to analyzing the unicity distance required to solve small percentages of the cipher mappings (i.e. 20% in Carroll & Martin (1986)).
In comparison to these past works, we afford ourselves the advantage of an unconstrained corpus of ciphertext, however, we prescribe ourselves the following constraints: our model is not provided any prior knowledge of vocabulary element frequencies; and, no information about the cipher key is provided. Another complexity our work must overcome is our significantly larger vocabulary sizes; all previous work has addressed vocabularies of approximately 26 characters, while our model is capable of solving word-level ciphers with over 200 distinct vocabulary elements. As such, our methodology is notably `hands-off' in comparison to previous work and can be easily applied to different forms of cipher, different underlying data and unsupervised text alignment tasks.
1.2 GANS AND WASSERSTEIN GANS
Generative Adversarial Networks (GANs) are a class of neural network architectures introduced by Goodfellow et al. (2014) as an alternative to optimizing likelihood under a true data distribution. Instead, GANs balance the optimization of a generator network which attempts to produce convincing samples from the data distribution, and a discriminator which is trained to distinguish between
2

Under review as a conference paper at ICLR 2018

samples from the true data distribution and the generator's synthetic samples. GANs have been shown to produce compelling results in the domain of image generation, but comparatively weak performance in domains using discrete data (discussed in Section 2).

The original GAN objective as introduced in Goodfellow et al. (2014) is:

D°

"

arg

max
D

Ex,,X

rlog

Dpxqs

´

Ez,,Z rlogp1

´

DpF

pzqqqs

(1)

This loss is vulnerable to the problem of `mode collapse' where the generative distribution collapses to produce a generating distribution with low diversity. In order to more broadly distribute the mass, the WGAN objective (Arjovsky et al., 2017) considers the set of K-Lipschitz discriminator functions D : X Ñ R and minimizes the earth movers (1st Wasserstein) distance. The Lipschitz condition is enforced by clipping discriminators weights to fall within a predefined range.

D° " arg max Ex,,X rDpxqs ´ Ez,,Z rDpF pzqqs
}D}L K

(2)

An improved WGAN objective, introduced by Gulrajani et al. (2017), enforced the Lipschitz condition using a Jacobian regularization term instead of the originally proposed weight-clipping solution. This resulted in more stable training, avoiding capacity under-use and exploding gradients, and improved network performance over weight-clipping.

D° " arg maxEx,,X rDpxqs ´ Ez,,Z rDpF pzqqs`
D
 ¨ Ex^,,X^rp}x^DpF px^qq}2 ´ 1q2s

(3)

Here X^ are samples taken along a line between the true data distribution X and the generator's data distribution Xg " tF pzq|z ,, Zu.

1.3 CYCLEGAN

CycleGAN (Zhu et al., 2017) is a generative adversarial network designed to learn a mapping between two data distributions without supervision. Three separate works (Zhu et al., 2017; Yi et al., 2017; Liu et al., 2017) share many of the core features we describe below, however, for simplicity we will refer to CycleGAN as the basis for our work as it is the most similar to our model. It acts on distributions X and Y by using two mapping generators: F : X Ñ Y and G : Y Ñ X ; and two discriminators: DX : X Ñ r0, 1s and DY : Y Ñ r0, 1s.
CycleGAN optimizes the standard GAN loss LGAN:

LGANpF, DY , X , Yq " Ey,,Y rlog DY pyqs ` Ex,,X rlogp1 ´ DY pF pxqqqs While also considering a reconstruction loss, or `cycle' loss Lcyc:

(4)

LcycpF, G, X , Yq " Ex,,X r}GpF pxqq ´ x}1s ` Ey,,Y r}F pGpyqq ´ y}1s

(5)

Taken together the losses are balanced using a hyperparameter :

LpF, G, DX , DY , X , Yq " LGANpF, DY , X , Yq ` LGANpG, DX , Y, X q `  ¨ LcycpF, G, X , Yq

This leads to the training objectives:

F ° " arg min LcycpF, G, X , Yq ` LGANpF, DY , X , Yq
F

G° " arg min LcycpF, G, X , Yq ` LGANpG, DX , Y, X q
G

DX°

"

arg max LGANpG, DX , Y, X q
DX

DY°

"

arg max LGANpF, DY , X , Yq
DY

CycleGAN uses Lcyc to avoid mode collapse by preserving reconstruction of mapping inputs from outputs. It has demonstrated excellent results in unpaired image translation between two visually similar categories. Our architecture is the first example of this unsupervised learning framework being successfully applied to discrete data such as language.

3

Under review as a conference paper at ICLR 2018
Figure 2: Discriminators trained on the toy example of recognizing the bottom-right corner of a simplex as true data. From left to right the discriminators were regularized using: nothing; WGAN Jacobian norm regularization; and, the relaxed sampling technique.
2 DISCRETE GANS
Applying GANs to discrete data generation is still an open research problem that has seen great interest and development. The primary difficulty with training discrete data generators in a GAN setting is the lack of a gradient through a discrete node in the computation graph. The alternatives to producing discrete outputs - for instance, generators producing a categorical distribution over discrete elements - are prone to uninformative discrimination (described below), in that, the discriminator may use an optimal discrimination criterion that is unrelated to the correctness of the re-discretized generated data. In our example of a continuous distribution over discrete elements, the produced samples all lie within the standard simplex k with dimension k equal to the number of elements in the distribution. In this case, samples from the true data distribution always lie on a vertex vi of the simplex, while any sub-optimal generator will produce samples within the simplex's interior kztv1, ..., vku. In this example, a discriminator which performs uninformative discrimination might evaluate a sample's membership in the vertices of the simplex as an optimal discrimination criterion, which is entirely uninformative of the correctness of re-discretized samples from the generator.
A number of solutions to training generators with discrete outputs have been proposed: SeqGAN (Yu et al., 2017) uses the REINFORCE gradient estimate to train the generator; Boundary-seeking GANs (Hjelm et al., 2017) and maximum-likelihood augmented GANs (Che et al., 2017) proposed a gradient approximation with low bias and variance that resembles the REINFORCE (Williams, 1992) estimator. Gumbel-softmax GANs (Kusner & Herna´ndez-Lobato, 2016) replace discrete variables in the simplex with continuous relaxations called Concrete (Maddison et al., 2016) or Gumbel-softmax (Jang et al., 2016) variables; WGANs (Arjovsky et al., 2017) were suggested as a remedy to the uninformative discrimination problem by ensuring that the discriminator's rate of change with respect to its input is bound by some constant.
Our work utilizes both the Wasserstein GAN (Gulrajani et al., 2017; Arjovsky et al., 2017) and relaxation of discrete random variables (such as Concrete/Gumbel-softmax). It has been noted multiple times in implementations of CycleGAN as well as in the original paper itself that the architecture was sensitive to initialization and requires repeat attempts in order to converge to a satisfactory mapping (Bansal & Rathore, 2017; Sari, 2017). Our architecture suffered the same instability before the WGAN Jacobian norm regularization term was added to the discriminator's loss. In addition, we found that having the discriminator operate over embedding space instead of directly over softmax vectors produced by our generator has improved performance.
Our hypothesis, which is justified by Proposition 1 below, is that the embedding vectors may act as continuous relaxations of discrete random variables as small, noisy updates are applied throughout training; Proposition 1 asserts that by replacing discrete random variables with continuous ones, our discriminator is prevented from arbitrarily approximating a Dirac delta distribution. Figure 2 shows simple discriminators trained on the toy task of identify a single vertex of a simplex as true data; it is clear that a lack of regularization leads to the discriminator collapsing to the vertex of the simplex, leaving approximately zero gradient everywhere; while the Jacobian regularization of Wasserstein GANs leads to the space covered by lines leading from the true data vertex to the generated data having a lower rate of change (note that the gradient is still close to zero in the remaining area of the simplex); and finally, replacing the discrete random variables of the true data with continuous samples about the vertex results in a much more gradual transition, which is desirable since it provides a stronger gradient signal from which to learn.
4

Under review as a conference paper at ICLR 2018
Unique to CycleGAN is the auxiliary cycle loss described in Section 1.3. The effect of this additional objective is the parameters regularly being force away from the discriminator's minimum in favor of a mapping that better-satisfies reconstruction. For instance, it may be the case that the discriminator favors a particular cipher mapping that is not bijective, in this case the model will receive a strong signal from the cycle loss away from the discriminator's minimum. A relevant example we observed during the training of CipherGAN is the model needing to use a poor mapping (in `the eyes of' the discriminator) as an intermediary between a previous mapping and an improved mapping. In this case, the model moves against the gradient it receives from the discriminator into a region that has not seen the Jacobian norm regularization applied to it, and it may be the case - as is visually discernible from Figure 2 - that this region has near zero gradient norm. This motivates the benefits of having strong curvature globally, as opposed to linearly between the generators samples and the true data. Kodali et al. (2017) proposes regularizing in all directions about the generated samples, which would likely remedy the vanishing gradient in our case as well; for our experiments, the relaxed sampling technique proved sufficient.
Let us now introduce the definitions needed for the formal presentation of Proposition 1. Definitions.
· (Continuous relaxation of a discrete set). A continuous relaxation of a discrete set X is a proper, path-connected metric space X satisfying X  X .
· (Rediscretization function). A rediscretization function is an injective function R : X Ñ X from a continuous relaxation X of discrete space X satisfying @x P X , D  0 s.t. R " x on B rxs. Note that R defines an equivalence relation in X .
· (Uninformative Discrimination). A discriminator DX is said to perform uninformative discrimination under rediscretization function R if: Dx P X , x¯ P X s.t. pRpx¯q " xq ^ pDX pxq ff DX px¯qq.
· (Continuous relaxation of a function). A continuous relaxation of a function over discrete sets F : X Ñ Y is another function F : X Ñ Y (where X , Y are continuous relaxations of X , Y) such that F is continuous and F pxq " F pxq, @x P X .
The following proposition (proved in the Appendix) forms the theoretical basis of the technique. Proposition 1 (Reliable Fooling Via Relaxation). Given:
· discrete spaces X , Y and continuous relaxations X , Y
· generators F : X Ñ Y, G : Y Ñ X
· discrete discriminators DX , DY both optimal for fixed F, G
· rediscretization functions RX , RY
If, during training, we replace discrete random variables from X which lie in the continuous metric space X with samples from regions about them.
Then the optimal relaxed discriminators DX and DY have a continuous region about each x P X and y P Y where they are expected to assign values close to DX pxq and DY pyq.
Figure 3 compares a model trained with embedding vectors versus one with only the softmax outputs. It becomes clear on a harder task, such as Vigene`re, that the embeddings vastly outperforms softmax in terms of speed of convergence and final accuracy; however we found that the simpler task of a shift cipher showed little difference between embeddings and softmax, suggesting an increase in task complexity increases the benefits provided by the stronger gradient signal of embeddings.
3 METHOD
3.1 CIPHERGAN
GANs applied to text data have yet to produce truly convincing results (Kawthekar et al.). Previous attempts at discrete sequence generation with GANs have generally utilized a generator outputting
5

Under review as a conference paper at ICLR 2018

Work
Hasinoff (2003) Forsyth & Safavi-Naini (1993) Ramesh et al. (1993) Verma et al. (2007)

Ciphertext Length
500 5000 160 1000

Accuracy
,, 97% ,, 100% ,, 78.5% ,, 87%

Table 1: Previous results on automated shift cipher cracking with limited ciphertext length.

a probability distribution over the token space (Gulrajani et al., 2017; Yu et al., 2017; Hjelm et al., 2017). This leads to the discriminator receiving a sequence of discrete random variables from the data distribution, and a sequence of continuous random variables from the generator distribution; making the task of discrimination trivial and uninformative of the underlying data distribution. In order to avoid such a scenario, we perform all discrimination within the embedding space by allowing the generator's output distribution to define a convex combination of corresponding embeddings. This leads to the following losses:

LGANpF, DY , X , Yq " Ey,,Y rlog DY py ¨ WEJmbqs ` Ex,,X rlogp1 ´ DY pF px ¨ WEJmbq ¨ WEJmbqqs
LcycpF, G, X , Yq " Ex,,X r}GpF px ¨ WEJmbq ¨ WEJmbq ´ x}1s ` Ey,,Y r}F pGpy ¨ WEJmbq ¨ WEJmbq ´ y}1s

(6) (7)

We perform an inner product between the embeddings WEmb and the one-hot vectors in x as well as between the embeddings and the softmax vectors produced by generators F and G. The former is equivalent to a lookup operation over the table of embedding vectors, while the latter is a convex combination between all vectors in the vocabulary. The embeddings WEmb are trained at each step to minimize both Lcyc and LGAN, meaning the embeddings are easily mapped from and are easy to discriminate. As was discussed in Section 2, training with the above loss functions was unstable, with approximately three of every four experiments failing to produce compelling results. This is a problem we observed with the original CycleGAN horse-zebra experiment, and one that has been noted by multiple re-implementations online (Bansal & Rathore, 2017; Sari, 2017). We were able to significantly increase the stability by training the discriminator loss along with the Lipschitz conditioning term from the improved Wasserstein GAN (Gulrajani et al., 2017) (see Equation 3 and Fedus et al. (2017)), resulting in the following loss (DualGAN (Yi et al., 2017) opted to use weight-clipping to enforce the Lipschitz condition):

LGANpF, DY , X , Yq " Ey,,Y rDY py ¨ WEJmbqs ´ Ex,,X rDY pF px ¨ WEJmbq ¨ WEJmbqs `  ¨ Ey^,,Y^rp}y^DY py^q}2 ´ 1q2s

(8)

As a consequence of Proposition 1, discriminators trained on non-stationary embeddings will be unable to approximate Dirac delta distributions to arbitrary accuracy; implying there are dedicated `safe-zones' about members of X where the generator can reliably fool the discriminator and uninformative discrimination is prevented. In our experiments we make the assumption that the training of the embedding vectors approximates random sampling similar to that described in Proposition 1; in principle, this assumption is weak and would be strengthened by performing true random sampling according to some distribution such as Concrete (Maddison et al., 2016; Jang et al., 2016); in practice however, our experiments verify the assumption is sufficient for our particular use case.

4 EXPERIMENTS
4.1 DATA
Our experiments use plaintext natural language samples from the Brown English text dataset (Francis & Kucera, 1979). We generate 2 ° batch size plaintext samples, the first half are fed as the

6

Under review as a conference paper at ICLR 2018
Figure 3: Left: Comparison of different timing techniques for Brown-C Vigene`re. Right: Comparison of embedding vs. raw softmax on Brown-W with vocab size of 200.
CycleGAN's X distribution and the second half is passed through the cipher of choice and fed as the Y distribution. For our natural language plaintext data we used the Brown English-language corpus which consists of over one million words in 57340 sentences. We experiment with both word-level "Brown-W" and character-level "Brown-C" vocabularies. For word-level vocabularies, we control the size of the vocabulary by taking the top k most frequent words and introducing an `unknown' token which we use to replace all words that are not within the taken vocabulary. We demonstrate our method's ability to scale to large vocabularies using the word-level vocabularies; more modern enciphering techniques rely on large substitution-boxes (S-boxes) with many (often hundreds of) elements.
4.2 CIPHERS
For our experiments we employ two different substitution ciphers. Shift (also known as a Caesar cipher) and permutation are both examples of simple substitution ciphers, while Vigene`re is an example of a polyalphabetic substitution cipher. The first two, shift and permutation ciphers, are known as substitution ciphers as they both simply replace each element of a vocabulary with another. The shift cipher simply modulo-shifts the vocabulary right by some integral number of positions, while the permutation cipher permutes the elements of the vocabulary. Shift and permutation ciphers can be considered equivalent from the perspective of the discriminators and generators as the discriminator must learn to perform frequency analysis across all positions and the generators must learn the mapping that satisfies this analysis; namely, the target mapping. The third, Vigene`re, historically referred to as the indecipherable cipher, had resisted breaking attempts for three centuries (until 1863). It adds the complication of a secret key which is used to define a particular shift cipher at each position of the plaintext to be enciphered. The key is a list of integers which is repeated until the length of the key equals the length of the sequence to be enciphered, resulting in each element of the plaintext being associated with an element in the key. Each element of the plaintext si is then shifted to another vocabulary element according to the integer value of the key ki modulo the vocabulary size: psi ´ kiq mod vocab size.
4.3 TRAINING
As in Zhu et al. (2017) we replace the log-likelihood loss with a squared difference loss which was originally introduced by Mao et al. (2016):
LGANpF, DY , X , Yq " Ey,,Y rpDY py ¨ WEJmbqq2s ` Ex,,X rp1 ´ DY pF px ¨ WEJmbq ¨ WEJmbqq2s `  ¨ Ey^,,Y^rp1 ´ }y^DY py^q}2q2s
We adapted the convolutional architecture for the generator and discriminator directly from Zhu et al. (2017). We simply replace all two dimensional convolutions with the one dimension variant
7

Under review as a conference paper at ICLR 2018

Data Vocab size
Cipher
Acc. Cipher
Acc.

Brown-W Brown-W Brown-C 10 200 58

Shift/Permutation

100%

98.7% 99.8%

Vigene`re (Key: "CDE")

99.7% 75.7% 99.0%

Table 2: Average proportion of characters correctly mapped in a given sequence.

and reduce the filter sizes in our generators to 1 (pointwise convolutions). Convolutional neural networks have recently been shown to be highly effective on language tasks and can speed up training significantly (Zhang & LeCun, 2015; Kalchbrenner et al., 2016; Yu et al., 2017). Both our generators and discriminators receive a sequence of vectors in embedding space; our generators produce a softmax distribution over the vocabulary, while our discriminator produces a scalar output. For all our experiments we use a cycle loss  " 1. In order to be compatible with the WGAN we replace batch normalization (Ioffe & Szegedy, 2015) with layer normalization (Ba et al., 2016). We train using the Adam optmizer (Kingma & Ba, 2014) with batch size 64 and learning rate 2e ´ 4, 1 " 0 and 2 " 0.9. Our learning rate is exponentially warmed up to 2e ´ 4 over 2500 steps, and held constant thereafter. We use learned embedding vectors with 256 dimensions. The WGAN Lipschitz conditioning parameter was set to  " 10 as was prescribed in Gulrajani et al. (2017).
For the Vigene`re cipher, positional information is critical to the network being able to perform the mapping. In order to facilitate this we experimented with adding the timing signal described in Vaswani et al. (2017) ("Transformer Timing" in Figure 3) and found that performance increased relative to no explicit timing signal; we found that the best option was concatenating a learned positional embedding vector specific to each position onto the sequence ("Concat Timing" in Figure 3), this dramatically improved performance, however this means that the architecture can not generalize to sequences longer than those in the training set. A potential solution to the issue of generalizing to longer sequences would be making a 'soft' choice at each position for which positional embedding vector to concatenate using a softmax distribution over a set of embedding vectors larger than the expected key length, however, we leave this to future work.
4.4 DISCUSSION
Table 2 shows that CipherGAN was able to solve shift ciphers to near flawless accuracy, with all three vocabulary sizes being easily decoded by the model. CipherGAN performs extremely well on Vigene`re, achieving excellent results on the character-level cipher and strong results on the challenging word-level cipher with a vocabulary size of 200. The vocabulary size of 58 for our character level, containing punctuation and special characters, is more than double what has been previously explored. In comparison to the original CycleGAN architecture, we found CipherGAN to be extremely consistent in training and notably insensitive to the random initialization of weights; we attribute this stability to the Jacobian norm regularization term.
For both ciphers, the first mappings to be correctly determined were those of the most frequently occurring vocabulary elements, suggesting that the network does indeed perform some form of frequency analysis to distinguish outlier frequencies in the two banks of text. Another interesting observation is that of the mistakes made by the network: the network would frequently confuse punctuation marks with one another, perhaps suggesting that these vocabulary elements' skip-gram signatures were similar enough to lead to the repeated confusion observed across many training runs.
5 CONCLUSION
CipherGAN is a compelling demonstration of the potential generative adversarial networks hold to act on discrete data to solve difficult tasks that rely on an extremely sensitive and nuanced discrimination criterion. Our work serves to redouble the promise of the CycleGAN architecture for unsupervised alignment tasks for multiple classes of data. CipherGAN presents an algorithm that
8

Under review as a conference paper at ICLR 2018
is both stable and consistent in training, improving upon past implementations of the CycleGAN architecture. Our work theoretically motivates ­ and empirically confirms ­ the use of continuous relaxations of discrete variables, not only to facilitate the flow of gradients through discrete nodes, but also to prevent the oft-observed phenomena of uninformative discrimination. CipherGAN is highly general in its structure and can be directly applied to a variety of unsupervised text alignment tasks, without excess burden of adaptation. On the one hand, CipherGAN is an early step towards the goal of unsupervised translation between languages and has shown excellent performance on the simplified task of cipher map inference. On the other hand, the methods we introduce can be used more broadly in the field of text generation with adversarial networks.
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Hardik Bansal and Archit Rathore. Understanding and implementing cyclegan in tensorflow. https://hardikbansal.github.io/CycleGANBlog/, 2017.
John M Carroll and Steve Martin. The automated cryptanalysis of substitution ciphers. Cryptologia, 10(4):193­209, 1986.
Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint arXiv:1702.07983, 2017.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease a divergence at every step, 2017.
William S Forsyth and Reihaneh Safavi-Naini. Automated cryptanalysis of substitution ciphers. Cryptologia, 17(4):407­418, 1993.
W Nelson Francis and Henry Kucera. Brown corpus manual. Brown University, 2, 1979.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Sam Hasinoff. Solving substitution ciphers. Department of Computer Science, University of Toronto, Tech. Rep, 2003.
R Devon Hjelm, Athul Paul Jacob, Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundaryseeking generative adversarial networks. arXiv preprint arXiv:1702.08431, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.
9

Under review as a conference paper at ICLR 2018
Prasad Kawthekar, Raunaq Rewari, and Suvrat Bhooshan. Evaluating generative models for text generation.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. Unsupervised analysis for decipherment problems. In Proceedings of the COLING/ACL on Main conference poster sessions, pp. 499­506. Association for Computational Linguistics, 2006.
Kevin Knight, Bea´ta Megyesi, and Christiane Schaefer. The copiale cipher. In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, pp. 2­9. Association for Computational Linguistics, 2011.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. How to train your dragan. arXiv preprint arXiv:1705.07215, 2017.
Matt J Kusner and Jose´ Miguel Herna´ndez-Lobato. Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, and Zhen Wang. Multi-class generative adversarial networks with the l2 loss function. arXiv preprint arXiv:1611.04076, 2016.
SS Omran, AS Al-Khalid, and DM Al-Saady. A cryptanalytic attack on vigene`re cipher using genetic algorithm. In Open Systems (ICOS), 2011 IEEE Conference on, pp. 59­64. IEEE, 2011.
Bhadri Msvs Raju et al. Decipherment of substitution cipher using enhanced probability distribution. International Journal of Computer Applications, 5(8):34­40, 2010.
RS Ramesh, G Athithan, and K Thiruvengadam. An automated approach to solve simple substitution ciphers. Cryptologia, 17(2):202­218, 1993.
Eyyb Sari. tensorflow-cyclegan. https://github.com/Eyyub/tensorflow-cyclegan, 2017.
Simon Singh. The Code Book: The Science of Secrecy from Ancient Egypt to Quantum Cryptography. Anchor, 2000.
Ragheb Toemeh and Subbanagounder Arumugam. Applying genetic algorithms for searching keyspace of polyalphabetic substitution ciphers. International Arab Journal of Information Technology (IAJIT), 5(1), 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
AK Verma, Mayank Dave, and RC Joshi. Genetic algorithm and tabu search attack on the monoalphabetic substitution cipher i adhoc networks. In Journal of Computer science. Citeseer, 2007.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Zili Yi, Hao Zhang, Ping Tan Gong, et al. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint arXiv:1704.02510, 2017.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pp. 2852­2858, 2017.
10

Under review as a conference paper at ICLR 2018 Xiang Zhang and Yann LeCun. Text understanding from scratch. arXiv preprint arXiv:1502.01710,
2015. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
11

Under review as a conference paper at ICLR 2018

APPENDIX

Proposition 1 (Reliable Fooling Via Relaxation). Given:
· discrete spaces X , Y and continuous relaxations X , Y
· generators F : X Ñ Y, G : Y Ñ X bijections satisfying F pGpyqq " y and GpF pxqq " x
· discrete discriminators DX , DY both optimal for fixed F, G
· rediscretization functions RX , RY
If, during training, we replace discrete random variables from X which lie in the continuous metric space X with samples from regions about them. Then the optimal relaxed discriminators DX and DY have a continuous region about each x P X and y P Y where they are expected to assign values close to DX pxq and DY pyq.

Proof. We'll prove one side of the CipherGAN as the proof for both sides are similar.
Given bijective function between continuous relaxations of X and Y: F : pX , dX q Ñ pY, dY q, where X and Y contain countably infinite, finite sequences (length n) of vectors laying on the vertices of the simplex k, and are supports of data distributions pX , pY respectively.
Let:

· X " Y " k ^ ¨ ¨ ¨ ^ k with k equal to the number of elements in our vocabulary. looooooomooooooon
n

· the rediscretization function RX : X Ñ X

similarly for RY .

RX

px¯q

"

arg

min
xPX

dX

px¯,

xq

Now, for each x P X consider the infinite set Sx with cardinality of the continuum constructed according to:
x¯ P Sx ðñ x¯ P X , s.t. RX px¯q " x and RY pF px¯qq " F pxq

Equivalently,

Sx " RX´1 X F ´1pRY´1pF pxqqq

Note. Sx is never of cardinality less than the continuum since the following is implied by the definitions of RX , Sx and the fact that F is continuous: x P X ùñ x P Sx ^ D closed ball B rxs with radius
0   min dX px, zq
zPX RY pF pzqqRY pF pxqq

So, for each element x P X there exists a closed set of points in X which are rediscretized, under RX , to x. Since Sx is a Borel Set we can sample uniformly from it. Therefore, during training suppose we replace each element of x P X with a sample x¯ ,, Sx:
We begin with the discrete objective:
ÿ pX pxq logpDX pxqq ` pF pxq logp1 ´ DX pxqq
xPX

As was noted in Goodfellow et al. (2014), this objective is optimized in DX : X Ñ r0, 1s when:

DX

"

pX pX ` pG

12

Under review as a conference paper at ICLR 2018

which

is

undesirable

as

pX

is

a

sum

of

Dirac

delta

distributions

pX pxq

"


xi PX

xi pxq

and

lacks

a non-zero gradient to train the generator function with. Instead, let us consider a continuous relax-

ation DX : X Ñ r0, 1s of the discriminator DX and observe where it optimizes.

Let sample from Sx be uniformly distributed pSx " c; then,



pSx px¯qdx¯ " pX pxq
x¯,,Sx

ùñ

|Sx|

"


x¯,,Sx

dx¯

"

pX pxq c

Suppose for all y P Y: Ex¯,,SGpyq rpx¯qs  Ex¯,,GpSyqrpx¯qs for and  " DX and  " logpDX q

Observe the upper bound,

ÿ 1

ÿ 1

pX pxq
xPX

x¯,,Sx |Sx| logpDX px¯qqdx¯ ` yPY pY pyq

y¯,,Sy |Sy| logp1 ´ DX pGpy¯qqqdy¯

~¸

ÿ

^ 1

 ÿ

1

 pX pxq log
xPX

x¯,,Sx |Sx| DX px¯qdx¯

` pY pyq log
yPY

1 ´ y¯,,Sy |Sy| DX pGpy¯qqdy¯

~¸

ÿ

^ 1

 ÿ

" pX pxq log
xPX

x¯,,Sx |Sx| DX px¯qdx¯

` pY pyq log
yPY

1

´


x¯,,GpSy q

|JF px¯q| |Sy |

DX

px¯qdx¯

~¸

ÿ

^ 1

 ÿ

1

 pX pxq log
xPX

x¯,,Sx |Sx| DX px¯qdx¯

` pY pyq log
yPY

1 ´ x¯,,SGpyq |SGpyq| DX px¯qdx¯

~¸

ÿ ^ 1 

1

" pX pxq log
xPX

x¯,,Sx |Sx| DX px¯qdx¯ ` pY pF pxqq log 1 ´ x¯,,SGpF pxqq |SGpF pxqq| DX px¯qdx¯

ÿ ^ 1 

^

1



" pX pxq log
xPX

x¯,,Sx |Sx| DX px¯qdx¯ ` pGpxq log 1 ´ x¯,,Sx |Sx| DX px¯qdx¯

By Jensen's inequality, since logpaq, logp1´aq both concave and Sx Borel. Therefore DX is optimal

in the family of functions:

Ex¯,,Sx rDX px¯qs "

pX pX ` pG

" DX

In order to construct the lower bound:

ÿ 1

ÿ 1

pX pxq
xPX

x¯,,Sx |Sx| logpDX px¯qqdx¯ ` yPY pY pyq

y¯,,Sy |Sy| logp1 ´ DX pGpy¯qqqdy¯

ÿ "
xPX


pX pxq
x¯,,Sx

1 |Sx|

logpDX px¯qqdx¯

`

ÿ
yPY


pY pyq
x¯,,GpSy q

|JF px¯q| |Sy |

logp1

´

DX px¯qqdx¯

ÿ 1

 ÿ

1

 pX pxq
xPX

x¯,,Sx |Sx| logpDX px¯qqdx¯ ` yPY pY pyq

x¯,,SGpyq |SGpyq| logp1 ´ DX px¯qqdx¯

 ÿ

1

"
xPX

x¯,,Sx |Sx| ppX pxq logpDX px¯qq ` pGpxq logp1 ´ DX px¯qqq dx¯

Which is optimal in D at:

DX px¯

,,

Sxq

"

pX pX ` pG

"

DX

ùñ Ex¯,,Sx rDX px¯qs " DX

13

