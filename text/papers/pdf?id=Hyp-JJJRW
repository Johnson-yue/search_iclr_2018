Under review as a conference paper at ICLR 2018
STYLE MEMORY: MAKING A CLASSIFIER NETWORK GENERATIVE
Anonymous authors Paper under double-blind review
ABSTRACT
Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a "style memory" to the output layer of the network. We also show how to train such a neural network as stacked autoencoders, jointly minimizing both classification and reconstruction losses. The generative function of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs. We further investigate the nature of the style memory, and how it relates to composing digits from MNIST.
1 INTRODUCTION
Deep neural networks now rival human performance in many complex classification tasks, such as image recognition. However, these classification networks are different from human brains in some basic ways. In particular, the mammalian cortex has many feed-back connections that project in the direction opposite the sensory stream (Bullier et al., 1988). Moreover, these feed-back connections are implicated in the processing of sensory input, and seem to enable improved object/background contrast (Poort et al., 2012), and imagination (Reddy et al., 2011). One might ask if it is possible to add feed-back connections to a feed-forward classification network to get a network that performs classification in the feed-forward direction, and generates samples in the feed-back direction. Probably not. The nature of a classifier network is that it throws away most of the information, keeping only what is necessary to make accurate classifications. What if we combine the features of a classifier network and an autoencoder network by adding a "style memory" to the top layer of the network? The top layer would then consist of a classification component (eg. a softmax vector of neurons) as well as a collection of neurons that are not constrained by any target class. We hypothesized that adding a style memory to the top layer of a deep autoencoder would give us the best of both worlds, allowing the classification neurons to focus on the class of the input, while the style memory would record additional information about the encoded input ­ presumably information not encoded by the classification neurons. The objective of our network is to minimize both classification and reconstruction losses so that the network can perform both classification and reconstruction effectively. In this paper, we report on a number of experiments with MNIST that investigate the properties of this style memory.
2 RELATED WORK
Some others have developed neural architectures that encode both the class and style of digits to enable reconstruction. Luo et al. (2017) recently introduced a method called bidirectional backpropagation. Their network is generative because it has feed-back connections that project down from the top (soft-max) layer. A digit class can be chosen at the top layer, and the feed-back connections render a digit of the desired class in the bottom layer (as an image). However, the network always
1

Under review as a conference paper at ICLR 2018
renders the same, generic sample of the class, and does not reconstruct specific samples from the data. Stylistic image attributes can be transferred to another image. Gatys et al. (2015) proposed a way to transfer painting styles. Using pre-trained convolutional neural networks, they searched for (or evolved) an input image that minimized an objective function that incorporated both content loss and style loss. Content loss is usually measured in the lower layers of the network, and measures a form of reconstruction error. Style loss is tabulated using middle layers of the network, where the representations are separated from the specific image content, and instead guided by a sample piece of art. They show that the resulting images resemble the content of the target image, but exhibit the style of the painting. This work demonstrates the notion of style transfer. However, their network is not generative. Instead, the stylized images are created through an optimization process. Networks that have the capacity to generate images have been shown to learn meaningful features. Previous work (Hinton, 2007) showed that in order to recognize images, the network needs to first learn to generate images. Salakhutdinov & Hinton (2009) showed that a network consisting of stacked Restricted Boltzmann Machines (RBMs) learns good generative models, effective for pretraining a classifier network. RBMs are stochastic in nature, so while they can generate different inputs, they are not used to generate a specific sample of input. Bengio et al. (2006) also demonstrated that autoencoders pre-trained in a greedy manner also lead to better classifier networks. Both (Hinton et al., 2006) and (Bengio et al., 2006) use tied weights, where the feed-back weight matrices are simply the transpose of the feed-forward weights; this solution is not biologically feasible. These findings have inspired other successful models such as stacked denoising autoencoders (Vincent et al., 2010), which learn to reconstruct the original input image given a noise-corrupted input image.
3 METHOD
3.1 MODEL DESCRIPTION
Our bidirectional network consists of an input layer, several hidden layers, and an output layer. However, the output layer is augmented; in addition to classifier neurons, it also includes style memory neurons as shown in Fig. 1. A standard classifier network maps x  X  y  Y , where the dimension of Y is usually much smaller than the dimension of X. The feed-forward connections of our augmented network map x  X  (y, m)  Y × M . The output y is the classification vector (softmax). The output m is the style memory, meant to encode information about the particular form of an input. For the example of MNIST, the classification vector might represent that the digit is a `2', while the style memory records that the `2' was written on a slant, and with a loop in the bottom-left corner.
Figure 1: Our bidirectional network with a style memory in the output layer. Here, x denotes original inputs x  X, hi denotes hidden units at layer i, y denotes output label y  Y , and m denotes the style memory m  M .
A classifier network can be trained as an autoencoder network with an associative memory on the top layer. However, the decoder will only be able to generate a single, average element of a given
2

Under review as a conference paper at ICLR 2018

Figure 2: Learning consists of training the network as stacked autoencoders.

Figure 3: The "unrolled" network, where h^1 and h^2 denote the reconstruc-tion of the fi-rst and second hidden layers, respectively. The feed-forward weights are denoted by Wi, while Wi denotes the feed-back weights.

class. By adding a style memory in the output layer, the network will be able to learn to generate a variety of different renderings of that class.

3.2 TRAINING

To train the network, we follow the greedy layer-wise training of deep networks as proposed in Bengio et al. (2006), depicted in Fig. 2.

The objective for our network's top layer is to jointly minimize two loss functions. The first loss function is the classifier loss Ly, which is a categorical cross-entropy loss function,

Ly(yt, y) = - yt log(y) ,
x

(1)

where yt is the target label, and y is the predicted label. The second loss function is the reconstruction loss between the penultimate layer and top layer. This reconstruction loss, denoted Lr, is the Euclidean distance between the input to the top layer, and the reconstruction of that input,

Lr(h^2, h2) = h^2 - h2 2 ,

(2)

where h^2 is the reconstruction of the second hidden layer h2, as shown in Fig. 3. Our goal is to minimize the combination of both loss functions in the last layer such that

 = arg min Ly(yt, y) + (Lr(h^2, h2)) ,
 xX

(3)

where  represents the parameters (weights and biases) in the last autoencoder, and  adjusts the weight of the reconstruction loss.

4 EXPERIMENTS
This section describes the experimental setup and the results of each experiment.

3

Under review as a conference paper at ICLR 2018
4.1 EXPERIMENT SETUP We performed all experiments in this paper using the MNIST dataset, with an input dimensionality of 784. The network used for the experiment has two hidden layers, h1 and h2, which have 392 and 196 ReLU neurons, respectively (like in Glorot et al. (2011)). Furthermore, the network layers are fully connected. The style memory consists of 16 logistic neurons, and the classifier vector contains 10 softmax neurons. The reconstruction loss weight () was set to 0.1, and the optimization method used to train the network was Adam (Kingma & Ba, 2014) with a learning rate  of 0.00001. The first and second autoencoders were trained for 200 epochs, while the last autoencoder was trained for 2,500 epochs.
4.2 RECONSTRUCTION USING STYLE MEMORY The reconstructions produced by our network show that it has the capacity to reconstruct a specific sample, rather than just a generic example from a specific class. Figure 4 shows examples of digit reconstructions. Notice how the network has the ability to reconstruct different styles of a class, like the two different `4's and two different `9's. For each sample, the reconstruction mimics the style of the original digit.
Figure 4: Reconstruction of digits using the network's predictions and style memories. The top row shows the original images from the MNIST test set, and the bottom row shows the corresponding reconstructions produced by the network.
How do the softmax classification nodes and the style memory interact when a digit is misclassified? Figure 5b shows an example where the digit `2' was misclassified as a `7'. The resulting reconstruction looked more like a `7' (although there is a hint of `2'). Furthermore, by modifying the softmax neurons to the one-hot ground truth label for `2', the reconstruction changed to look more like a `2', as shown in Fig. 5c.
(a) (b) (c) Figure 5: (a) Original digit `2'. (b) Reconstruction of the digit using the prediction from the network, misclassified as a `7'. (c) Reconstruction of the digit using the same style memory, but with the corrected one-hot label for `2', rather than the prediction from the network.
4.3 STYLE-MEMORY SPACE We visualized the style memory of the MNIST digits using t-SNE (van der Maaten & Hinton, 2008) with the Barnes-Hut algorithm (Van Der Maaten, 2014). Figure 6 shows projections of MNIST training samples in (a) the image space, and in (b) the 16-dimensional style-memory space. Compared to image space, the classes in style-memory space exhibit a lot more overlap, suggesting that a given
4

Under review as a conference paper at ICLR 2018
(a) (b)
(c) Figure 6: (a) Visualization of 10,000 MNIST training digits using t-SNE. (b) Visualization of 10,000 style memories of MNIST training digits using t-SNE. (c) Color code used for t-SNE visualization.
style is not necessarily associated with a single digit class; many digits share similar styles. For example, the digits `4' (purple) and `9' (cyan) overlap in the style memory space.
4.4 STYLE MEMORY REPRESENTATION To better understand what was being encoded in the style memory, we generated digits that were close together in the style memory space (16-dimensional) and compared them with digits that are close together in the image space (784-dimensional). The distance, in either space, was calculated using the Euclidean norm. From Fig. 7, we can observe that proximity in the style-memory space has different semantic meaning than proximity in the image space. Figure 7a, showing the 97 images that are closest to the `5' image in the top-left corner, displays many digits that share common pixels. However, Fig. 7b, which shows the 97 digits with the closest style memories, displays digits that come from various different classes. Similarly, Fig. 7c shows many digits of class `4' and `9', while Fig. 7d shows digits from various classes. It is also interesting to note that the digits that are close together in the style-memory space share some similar style characteristics. In (a) and (c), the digits seem to share pixels, while in (b) and (d), the digits share stylistic components that are more difficult to articulate, such as general curviness in (b).
4.5 STYLE MEMORY TRANSFER We evaluated the possibility of performing style-memory transfer from one digit to another. Style transfer was done by first encoding an input image. Then, we overwrite its classification vector with a one-hot encoding of the digit we want to render with the style of the original digit. For example, Fig. 8 shows the style memory for a particular digit `4'. We tried generating a digit of each class using that style memory. Our results, shown in Fig. 9 for the style memory shown in Fig. 8, suggest that some style memories are transferable to another digit, and some are not. For example, the style memory for the `4' can only be transferred to `1', `7', and `9'. This is may be due to the fact that the digits `1', `4', `7', and `9' share a vertical-line "backbone", and the style memory encodes something about that backbone. The digits `0', `2', `3', `5', `6', and `8' do not have such a backbone.
4.6 STYLE MEMORY INTERPOLATION In this experiment, we attempted to reconstruct a digit with a continuum of styles by interpolating between two different style memories. For example, we encoded two different digits of `7', as shown in Fig. 10, and then generated a sequence of digit images that slowly evolve from one style
5

Under review as a conference paper at ICLR 2018

(a) Image Dist=8.6, Style Dist=0.76

(b) Image Dist=9.7, Style Dist=0.57

(c) Image Dist=6.2, Style Dist=0.56

(d) Image Dist=7.0, Style Dist=0.41

Figure 7: Nearest neighbours in image space and style-memory space. (a) and (c) show the 97 digit images closest to the image in the top-left, as well as their corresponding style-memories. (b) and (d) show the 97 style memories closest to the style memory in the top-left, as well as their corresponding digit images. The order of elements (across rows, then down) indicate increasing Euclidean distance. The subfigure captions give the average distance from the top-left element, both in image space, and style-memory space.

6

Under review as a conference paper at ICLR 2018

Figure 8: A sample digit `4' and its style memory.

Figure 9: Reconstructions of digits using the style memory shown in Fig. 8 and one-hot encoded ground truth label from `0' to `9' from left to right.

to the other. We performed the interpolation by simply taking convex combinations of the two style

memories, using

m^ () = m1 + (1 - )m2 ,

(4)

where m1 and m2 denote the style memories. The interpolated style memory is denoted by m^ (), where   [0, 1] denotes the interpolation coefficient.

Figure 11 shows the interpolated digits, illustrating that the generated digits transform smoothly when the style memory is interpolated. These results suggest that style memory interpolation has the potential to perform interpolation in the image space.

5 CONCLUSIONS AND FUTURE WORK
Classification networks do not typically maintain enough information to reconstruct the input; they do not have to. Their goal is to map high-dimensional inputs to a small number of classes, typically using a lower-dimensional vector representation (such as a softmax or one-hot). Thus, simply adding feed-back connections to a classification network, and training each layer as an autoencoder will not yield a network that performs classification in the feed-forward direction, and sample generation in the feed-back direction.
In this paper, we proposed the addition of "style memory" to the top layer of a classification network. The top layer is trained using a multi-objective optimization, trying to simultaneously minimize classification error and reconstruction loss.
Our experiments suggest that the style memory encodes information that is largely disjoint from the classification vector. For example, proximity in image space yields digits that employ an overlapping set of pixels. However, proximity in style-memory space yielded a very different set of digits.

Figure 10: Two different styles of `7' with their corresponding style memories. These two digits form the endpoints for the style interpolation experiment.
7

Under review as a conference paper at ICLR 2018
Figure 11: Image reconstruction with style memory interpolation between two digits shown in Fig. 10, where  was increasing from 0.1 to 1.0 with a step of 0.1 from left to right. The label that was given for the reconstruction was a one-hot encoded label of a digit `7'. We were able to demonstrate that the style of a digit can be transferred to some other digits classes. Further investigation into this phenomenon is needed. In fact, it would be interesting if we could articulate what the style memory is encoding. But we concede that words might fail us in this endeavour; the encoding of style could be very difficult to pinpoint with language. For the style interpolation experiment, we generated images from a straight line in style-memory space. However, each position on this line generates a sample in image space ­ an image; it would be interesting to see what shape that 1-dimensional manifold takes in image space, and how it differs from straight-line interpolation in image space. Finally, we used a greedy, layer-by-layer training method, training each pair of layers as an autoencoder, starting from the bottom. However, better training would result if we performed some additional passes using a method similar to the sleep-wake approach (Hinton et al., 1995). These experiments are ongoing.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Proceedings of the 19th International Conference on Neural Information Processing Systems, NIPS'06, pp. 153­160, Cambridge, MA, USA, 2006. MIT Press.
J Bullier, ME McCourt, and GH Henry. Physiological studies on the feedback connection to the striate cortex from cortical areas 18 and 19 of the cat. Experimental Brain Research, 70(1):90­ 98, 1988.
Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image Style Transfer Using Convolutional Neural Networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2414­ 2423, 2015.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Geoffrey Gordon, David Dunson, and Miroslav Dudk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 315­323, Fort Lauderdale, FL, USA, 11­13 Apr 2011. PMLR.
Geoffrey E. Hinton. To recognize shapes, first learn to generate images. In Paul Cisek, Trevor Drew, and John F. Kalaska (eds.), Computational Neuroscience: Theoretical Insights into Brain Function, volume 165 of Progress in Brain Research, pp. 535 ­ 547. Elsevier, 2007.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The "Wake-Sleep" Algorithm for Unsupervised Neural Networks. Science, 268(5214):1158­1161, 1995.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527­1554, 2006.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Hongyin Luo, Jie Fu, and James R. Glass. Bidirectional backpropagation: Towards biologically plausible error signal transmission in neural networks. CoRR, abs/1702.07097, 2017.
Jasper Poort, Florian Raudies, Aurel Wannig, Victor A F Lamme, Heiko Neumann, and Pieter R. Roelfsema. The role of attention in figure-ground segregation in areas V1 and V4 of the visual cortex. Neuron, 75(1):143­156, 2012.
Leila Reddy, Naotsugu Tsuchiya, and Thomas Serre. Reading the mind's eye: decoding category information during mental imagery. NeuroImage, 50(2):818­825, 2011.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann machines. In David van Dyk and Max Welling (eds.), Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, volume 5 of Proceedings of Machine Learning Research, pp. 448­455, Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA, 16­18 Apr 2009. PMLR.
Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. J. Mach. Learn. Res., 15 (1):3221­3245, January 2014.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579­2605, 2008.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371­3408, December 2010.
9

