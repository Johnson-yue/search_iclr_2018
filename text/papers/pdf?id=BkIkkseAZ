Under review as a conference paper at ICLR 2018
THEORETICAL PROPERTIES OF THE GLOBAL OPTIMIZER OF TWO-LAYER NEURAL NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves "almost" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given "good" class of differentiable functions and optimization is only with respect to the last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of O(1/number of iterations) to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.
1 INTRODUCTION
Neural networks architecture has recently emerged as a powerful tool for a wide variety of applications. In fact, they have led to breakthrough performance in many problems such as visual object classification (Krizhevsky et al., 2012), natural language processing (Collobert & Weston, 2008) and speech recognition (Mohamed et al., 2012). Despite the wide variety of applications using neural networks with empirical success, mathematical understanding behind these methods remains a puzzle. Even though there is good understanding of the representation power of neural networks (Barron, 1994), training these networks is hard. In fact, training neural networks was shown to be NP-complete for single hidden layer, two node and sgn(·) activation function (Blum & Rivest, 1988). The main bottleneck in the optimization problem comes from non-convexity of the problem. Hence it is not clear how to train them to global optimality with provable guarantees. Neural networks have been around for decades now. A sudden resurgence in the use of these methods is because of the following: Despite the worst case result by Blum & Rivest (1988), first-order methods such as gradient descent and stochastic gradient descent have been surprisingly successful in training these networks to global optimality. For example, Zhang et al. (2016) empirically showed that sufficiently over-parametrized networks can be trained to global optimality with stochastic gradient descent. Neural networks with zero hidden layers are relatively well understood in theory. In fact, several authors have shown that for such neural networks with monotone activations, gradient based methods will converge to the global optimum for different assumptions and settings (Mei et al., 2017; Hazan et al., 2015; Kakade et al., 2011; Kalai & Sastry, 2009). Despite the hardness of training the single hidden layer (or two-layer) problem, enough literature is available which tries to reduce the hardness by making different assumptions. E.g., Choromanska et al. (2014) made a few assumptions to show that every local minimum of the simplified objective is close to the global minimum. They also require some independent activations assumption which may not be satisfied in practice. For the same shallow networks with (leaky) ReLU activations, it was shown in Soudry & Carmon (2016) that gradient descent can attain global minimum of the modified loss function, instead of the original objective function. Under the same setting, Xie et al.
1

Under review as a conference paper at ICLR 2018
(2016) showed that critical points with large "diversity" are near global optimal. But ensuring such conditions algorithmically is difficult. All the theoretical studies have been largely focussed on ReLU activation but other activations have been mostly ignored. In our understanding, this is the first time a theoretical result will be presented which shows that for almost all nonlinear activation functions including softplus, a first-order optimal solution is also the global optimal provided certain "simple" properties of hidden layer. Moreover, we show that a stochastic algorithm will give us those required simple properties for free for all finite number of iterations. Our assumption on data distribution is very general and can be reasonable for practitioners. This comes at the cost that the hidden layer of our network can not be wider than the dimension of the input data, say d. Since we also look at this problem in overparametrized setting (where there is hope to achieve global optimality), this constraint on width puts a direct upper-bound of d2 on the number of data points that can be trained. Even though this is a strong upper bound, recent results from margin bounds (Neyshabur et al., 2017) show that if optimal network is closer to origin then we can get an upper bound on number of samples independent of dimension of the problem which will ensure closeness of population objective and training objective. We also show for the first time that even though the objective function for training neural networks is non-convex, it is Lipschitz smooth meaning that gradient of the objective function does not change a lot with small changes in underlying variable. This allows us to show convergence result for the gradient descent algorithm, enabling us to establish an upper bound on the number of iterations for finding an -approximate first-order optimal solution ( f ()  ). Therefore our algorithm will generate an -approximate first-order optimal solution which satisfies aforementioned properties of the hidden layer. Note that this does not mean that the algorithm will reach the global optimal point asymptotically. We discuss technical difficulties to prove such a conjecture in more detail in section 5 which details our convergence results. At this point we would also like to point that there is good amount of work happening on shallow neural networks. In this literature, we see variety of modelling assumptions, different objective functions and local convergence results. Li & Yuan (2017) focuses on a class of neural networks which have special structure called "Identity mapping". They show that if the input follows from Gaussian distribution then SGD will converge to global optimal for population objective of the "identity mapping" network. Brutzkus & Globerson (2017) show that for isotropic Gaussian inputs, with one hidden layer ReLU network and single non-overlapping convolutional filter, all local minimizers are global hence gradient descent will reach global optimal in polynomial time for the population objective. For the same problem, after relaxing the constraint of isotropic Gaussian inputs, they show that the problem is NP-complete via reduction from a variant of set splitting problem. In both of these studies, the objective function is a population objective which is significantly different from training objective in over parametrized domain. In over-parametrized regime, Soltanolkotabi et al. (2017) shows that for the training objective with data coming from isotropic Gaussian distribution, provided that we start close to the true solution and know maximum singular value of optimal hidden layer then corresponding gradient descent will converge to the optimal solution. This is one of its kind of result where local convergence properties of the neural network training objective function have studied in great detail. Our result differ from available current literature in variety of ways. First of all, we study training problem in the over-parametrized regime. In that regime, training objective can be significantly different from population objective. Moreover, we study the optimization problem for many general non-linear activation functions. Our result can be extended to deeper networks when considering the optimization problem with respect to outermost hidden layer. We also prove that stochastic noise helps in keeping the aforementioned properties of hidden layer. This result, in essence, provides justification for stochastic gradient descent. Another line of study looks at the effect of over-parametrization in the training of neural networks (Haeffele & Vidal, 2015; Nguyen & Hein, 2017). These result are not for the same problem as they require huge amount of over-parametrization. In essence, they require the width of the hidden layer to be greater than number of data points which is unreasonable in many settings. These result work for fairly general activations as do our results but we require a moderate over-parametrization, width × dimension  number of data population, much more reasonable in practice as pointed before from margin bound results. They also work for deeper neural network as do our results when optimization is with respect to outermost hidden layer (and aforementioned technical properties are satisfied for all hidden layers).
2

Under review as a conference paper at ICLR 2018

2 NOTATION AND PROBLEM OF INTEREST

We define set [q] := {1, . . . , q}. For any matrix A  Ra×b, we write vect(A)  Rab×1 as vector form of the matrix A. For any vector z  Rk, we denote h(z) := h(z[1]), . . . , h(z[k]) T , where z[i] is the i-th element in vector z. Bi(r) represents a li-ball of radius r, centred at origin. We define component-wise product of two vectors with operator .
We say that a collection of vectors, {vi}iN=1  Rd, is full rank if rank v1 . . . vN = min{d, N }. Similarly, we say that collection of matrices, {Mi}iN=1  Rn×d, is full rank if
rank [vect(M1) . . . vect(Mk)] = min{N, nd}.
A fully connected two-layer neural network has three parameters: hidden layer W , output layer  and activation function h. For a given activation function, h, we define neural network function as

W,(u) := T h(W u).
In the above equation, W  Rn×d is hidden layer matrix,   Rn is the output layer. Finally h : R  R is an activation function. The main problem of interest in this paper is the two-layer neural network problem given by

min f (W, ) :=
WRRnn×d

1 2N

N
(vi - W,(ui))2.
i=1

(2.1)

In this paper, we assume that (ui, vi)  Rd × R, i  [N ] are independently distributed data point and each ui is sampled from a d-dimensional Lebesgue measure.

3 THE BASIC IDEA AND THE ALGORITHM

First-order optimality condition for the problem defined in (2.1), with respect to W [j, k] (j-th row, k-th column element of matrix W )  j  [n],  k  [d] is

W f (W, )[j, k]

=

1 N

N
{vi - T h(W ui)}h (W [j, :]ui)[j]ui[k] = 0.

i=1

Equation (3.1) is equivalent to

(3.1)

N
{vi - T h(W ui)} h (W ui)
i=1
(3.1) can also be written in a matrix vector product form:

 uiT = 0.

(3.2)

Ds = 0,

(3.3)

where

h (W [1, :]u1)[1]u1 . . . h (W [1, :]uN )[1]uN 

 v1 - T h(W u1) 

D :=  

...

...

...

 and s :=  

...

. 

h (W [d, :]u1)[d]u1 . . . h (W [d, :]uN )[d]uN

vN - T h(W uN )

Notice that if matrix D  Rnd×N is of full column rank (which implies nd  N , i.e., number of samples is less than number of parameters) then it immediately gives us that s = 0 which means such a stationary point is global optimal. This motivates us to investigate properties of h under which we can provably keep matrix D full column rank and develop algorithmic methods to help maintain such properties of matrix D. For the rest of the discussion, we will assume that n = d (our results can be extended to case n  d easily) and hence W is a square matrix. In this setting, we develop the following algorithm whose output is a provable first-order approximate solution. Here we present the algorithm and in next sections we will discuss conditions that are required to satisfy full rank property of matrix D as well as convergence properties of the algorithm.

3

Under review as a conference paper at ICLR 2018

In the algorithm, we use techniques inspired from alternating minimization to minimize with respect to  and W . For minimization with respect to , we add gaussian noise to the gradient information. This will be useful to prove convergence of this algorithm. We use randomness in  to ensure some "nice" properties of W which help us in proving that matrix D generated along the trajectory of the algorithm is full column rank. More details will follow in next section. The algorithm has two loops. An outer loop implements a single gradient step with respect to hidden layer, W . For each outer loop iteration, there is an inner loop which optimizes objective function with respect to  using a stochastic gradient descent algorithm. In the stochastic gradient descent, we generate a noisy estimated of f (W, ) as explained below. Let   Rd be a vector whose elements are i.i.d. Gaussian random variable with zero mean. Then for a given value of W we define stochastic gradient w.r.t.  as follows:

GW (, ) = f (W, ) + .

(3.4)

Then we know that

E[GW (, )] = f (W, ).

We can choose a constant  > 0 such that following holds

E GW (, ) - f (W, ) 2  2.

(3.5)

Moreover, in the algorithm we consider a case where   R. Note that R can be kept equal to Rd but that will make parameter selection complicated. In our convergence analysis, we will use

R := B2(R/2),

(3.6)

for some constant R, to make parameter selection simpler. We use prox-mapping Px : Rd  R as

follows:

Px(y) = argmin
zR

y, z - x

1 +
2

z-x

2.

(3.7)

In case R is a ball centred at origin, solution of (3.7) is just projection of x - y on that ball. For case where R = Rd then the solution is quantity x - y itself.

Algorithm 1 SGD-GD Algorithm

procedure

W0  Random d × d matrix 0  Random d vector

Initialize No to predefined iteration count for outer ietaration

Initialize Ni to predefined iteration count for inner iteration

Begin outer iteration:

for k = 0, 1, 2, . . . , No do 1  k

Begin inner iteration:

for i = 1, 2, . . . , Ni do

i+1  Pi (iGWk (i, ik))

ia+v1 =

i  -1 i   +1

 =1

 =1

end for k+1  Navi+1 Wk+1  Wk - kW f (Wk, k+1)

end for

return {WNo+1; No+1} end procedure

Notice that the problem of minimization with respect to  is a convex minimization problem. So we can implement many procedures developed in the Stochastic optimization literature to get the convergence to optimal value (Nemirovski et al., 2009). We are implementing SGD which was developed by Lan (2012). In the analysis, we note that one does not even need to implement complete inner iteration as we
4

Under review as a conference paper at ICLR 2018

can skip the stochastic gradient descent suboptimally given that we improve the objective value with respect to where we started, i.e.,

f (Wk, k+1)  f (Wk, k).

(3.8)

In essence, if evaluation of f for every iteration is not costly then one might break out of inner iter-
ations before running Ni iterations. If it is costly to evaluate function values then we can implement the whole SGD for convex problem with respect to  as specified in inner iteration of the algorithm
above. In each outer iteration, we take one gradient decent step with respect to variable W . We have total of No outer iterations. So essentially we evaluate f (W, ·) a total of NoNi times and W f (·, ) total of No times. Overall, this algorithm is new form of alternate minimization, where one iteration can be potentially
left suboptimally and other one is only one gradient step.

4 FIRST ORDER OPTIMALITY IS ENOUGH

We say that h : R  R satisfy the condition "C1" if

·  interval (a, b),

{c1, c2, c3}  R3 s.t.
{h (x) = c1, x  (a, b)} or {(x + c2)h (x) + h(x) = c3, x  (a, b)}.

One can easily notice that most activation functions used in practice e.g.,

· (Softplus) h(x) := ln(1 + ex),

·

(Sigmoid) h(x) :=

1 1+e-x

,

·

(Sigmoid symmetric) h(x) :=

,1-e-x
1+e-x

· (Gaussian) h(x) := e-x2 ,

· (Gaussian Symmetric) h(x) := 2e-x2 - 1,

·

(Elliot) h(x) :=

x 2(1+|x|)

+ 0.5,

·

(Elliot Symmetric) h(x) :=

x 1+|x|

,

x

·

(Erf) h(x) :=

2 

e-t2/2dt,

0

· (Hyperbolic tangent) h(x) := tanh(x),

satisfy the condition C1. Note that h (x) also satisfy condition C1 for all of them. In fact, except for very small class of functions (which includes linear functions), none of the continuously differentiable functions satisfy condition C1. We first prove a lemma which establishes that columns of the matrix D (each column is a vector form of d×d matrix itself) are linearly independent when W = Id and h satisfies condition C1. We later generalise it to any full rank W using a simple corollary. The statement of following lemma is intuitive but its proof is technical.

Lemma 4.1 Suppose xi  Rd are independently chosen vectors from any d-dimensional Lebesgue measure and let h : R  R be any function that satisfies condition C1 then collection of matrices h(xi)xiT , i  [N ] are full rank with measure 1.
Now lemma 4.1 gives us a simple corollary:
Corollary 4.2 If W is a nonsingular square matrix and ui  Rd is independently sampled from a Lebesgue measure then the collection of matrices h(W ui)uiT N is full rank with measure 1.
i=1
5

Under review as a conference paper at ICLR 2018

This means that if ui in the Problem (2.1) are coming from a Lebesgue measure then by corollary 4.2

we have h(W ui)uiT will be a full rank collection given that we have maintained full rank property

of W . Now note that in the first-order condition, given in (3.3), row of matrix D are scaled by

constant factors [j]'s, j  [d]. Notice that we may assume [j] = 0 because otherwise there is

no contribution of corresponding j-th row of W to the Problem (2.1) and we might as well drop

it entirely from the optimization problem.

Hence we can rescale rows of matrix D by factor

1 [j]

without changing the rank. In essence, corollary 4.2 implies that matrix D is full rank when W is

full rank. So by our discussion in earlier section, we show that satisfying first-order optimality is

enough to show global optimality under condition C1.

Remark 4.3 Due to lemma 4.1 and corollary 4.2 then, rank of collection h(ui)uiT is invariant under any rotation.
Remark 4.4 As a result of corollary above one can see that the collection of vectors h(W xi) is full rank under the assumption that W is non-singular, xi  Rd are independently sampled from Lebesgue measure and h satisfies condition C1.
Remark 4.5 Since collection h(W ui) is also full rank, we can say that zi := h(W1ui) are independent and sampled from a Lebesgue measure for a non-singular matrix W1. Applying the lemma to zi, we have collection of matrices g(W2zi)ziT are full rank with measure 1 for non-singular W2 and g satisfying condition C1. So we see that for multiple hidden layers satisfying non-singularity, we can apply full rank property for collection of gradients with respect to outermost hidden layer.
Remark 4.6 If W  Rn×d is such that n  d and W is full row rank, then we can extend its basis to create W and apply corollary 4.2 to get that h(W ui)uiT is full rank with measure 1. So this implies that h(W ui)uiT must have been full rank with probability 1 otherwise we will have contradiction.
Remark 4.7 We can extend corollary 4.2 to a general result that h(W ui)uiT has rank min{rank(W )d, N } with measure 1 by removing dependent rows and using remark 4.6.

5 CONVERGENCE RESULTS

Even though we have proved that collection

h(W ui)uiT

N i=1

is

full

rank,

we

can

only

apply

it

to

an algorithm which is by design going to output a non-singular matrix as final answer. But deriving

such guarantees for just last iteration can be challenging. Hence we rather design an algorithm which

gives a non-singular W in every iteration. The SGD step we mentioned before is used precisely to

obtain such theoretical guarantees. In Lemma 5.1 below, we provide theoretical guarantee that for

any finite number of iterations the hidden layer matrix, W , is full rank. Later on, we will also

show that overall algorithm will converge to first order approximate solution to the problem (2.1).

It should be noted however that this can not guarantee convergence to a global optimal solution. To

prove such a result, one needs to analyze the smallest singular value of random matrix D, defined

in (3.3). More specifically, we have to show that min(D) decreases at the rate slower than the first-

order convergence rate of the algorithm so that the overall algorithm converges to the global optimal

solution. Even if it is very difficult to prove such a result in theory, we think that such an assumption

about min(D) is reasonable in practice. One more (probably simpler) approach would be to prove asymptotic convergence without any rate guarantees. In essence, we have to show that as No   we have W  W  then W  is non-singular. But here as well, we do not have guarantee over the rank(W ) since it is a limiting point of the open set of non-singular matrices which can be singular.

Analysis of both these approaches can be challenging.

Now we analyze the algorithm. For the sake of simplicity of notation, let us define

[k] := {[1Ni], . . . , [kNi]}

(5.1)

and

[jNi] = {1j . . . Nj i },

(5.2)

6

Under review as a conference paper at ICLR 2018

where Ni is the inner iteration count in Algorithm 1. Essentially [k] contains the record of all random samples used until the k-th outer iteration in Algorithm 1 and [jNi] contains record of all random samples used in the inner iterations of j-th outer iteration.
Lemma 5.1 Pr{ v such that Wkv = 0 [k-1]} = 0,  k  0, where Wk are matrices generated by Algorithm 1 and measure Pr{. [k-1]} is w.r.t. random variables [kNi].
Even though we have proved that Wk's generated by the algorithm are full rank, we can not necessarily apply lemma 4.1 directly because it takes an arbitrary W whereas Wk is dependent on data (ui, vi). We still prove that matrix D generated along the trajectory of the algorithm is full rank. We use techniques inspired from lemma 4.1 but this time we use Lebesgue measure over  rather than data. Over randomness of , we can show that our algorithm will not produce any W such that corresponding matrix D is rank deficient. Since  is essentially designed to be independent of data so we will not produce rank deficient D throughout the process of random iid data collection and randomized algorithm. Before we jump into proving that we give a supplementary lemma which shows a more general result about the rank of matrix D.
Lemma 5.2 Suppose W = W + DvZ where Dv := diag(v[i], i  [d]) and v is a random vector with Lebesgue measure in Rd. W , Z  Rd×d and Z = 0. Let h be a function which follows condition C1. Also assume that W is full rank with measure 1 over randomness of v. Then h(W ui)uiT is full rank with measure 1 over randomness of v.

Lemma 5.3 Collection of matrices h (Wk+1ui)uiT are full rank with measure 1, where the measure is over randomness of [kN+i1]

Proof. We know that
N
Wk+1 = Wk + kk+1 h (Wkuj )ujT (vi - T h(Wkuj )).
j=1
Now apply lemma 5.2 to obtain the required result.
Hence we showed that algorithm will generate full rank matrix D for any finite iteration. Now to prove convergence of the algorithm, we need to analyze the function f (defined in (2.1)) itself. We show that f is a Lipschitz smooth function for any given instance of data {ui, vi}iN=1. This will give us a handle to estimate convergence rates for the given algorithm.

Lemma 5.4 Assuming that h : R  R is such that its gradients, hessian as well as values are bounded and data {ui, vi}Ni=1 is given then there exists a constant L such that

W f (W1, ) - W f (W2, ) F  L W1 - W2 F . Moreover, a possible upper bound on L can be as follows:

(5.3)

L



1 N

max

Lh

N N

ui 22|vi| + 2dLhh  2

ui

2 2

i=1 i=1

Remark 5.5 Before staing the proof, we should stress that assumptions on h is satisfied by most activation functions e.g., sigmoid, sigmoid symmetric, gaussian, gaussian symmetric, elliot, elliot symmetric, tanh, Erf.

Remark 5.6 Note that one can easily calculate value of L given data and . Moreover, if we put constraints on  2 then L is constant in every iteration of the algorithm 1. As mentioned in section 3, this will provide an easier way to analyse the algorithm.

Lemma 5.7 Assuming that scalar function h is such that |h(·)|  u then there exists L s.t.

wf (W, 1) - wf (W, 2) 2  L 1 - 2 2

(5.4)

7

Under review as a conference paper at ICLR 2018

Notice that Lemma 5.7 gives us value of L irrespective of value of W or data. Also observe that f (W, ·) is convex function since hessian

2f (W, )

=

1 N

N

h(W ui)h(W ui)T ,

i=1

which is the sum of positive semidefinite matrices. By Lemma 5.7, we know that f (W, ·) is smooth as well. So we can use following convergence result provided by Lan (2012) for stochastic composite optimization. A simplified proof can be found in appendix.

Theorem 5.8 Assume that stepsizes i satisfy 0 < i  1/2L,  i  1. Let {ia+v1}i1 be the sequence computed according to Algorithm 1. Then we have,

E[f (Wk, ia+v1) - f (Wk, W k )]  K0(i),  i  1,  k  0,

(5.5)

where K0(i) :=

i -1


1 - W k

2 2

+

2

i

i2

where 1 is the starting point for inner

 =1

 =1

iteration and  is defined in (3.5).

Now we look at a possible strategy of selecting stepsize i. Suppose we adopt a constant stepsize policy then we have i = ,  i  [Ni]. Then we have

E[f (Wk, Navi+1) - f (Wk, W k )] 

1 - W k

2
+ 2.

Ni

Now if we choose

11

 = min

,

2L

Ni2 ,

(5.6)

we get

E[f (Wk, Navi+1) - f (Wk, W k )] 

1 - W k

2

2L +  Ni Ni

+  . Ni

By Lemma 5.4, the objective function for neural networks is Lipschitz-smooth with respect to the

hidden layer, i.e., it satisfies eq (5.3). Notice that it is equivalent to saying

f (W2, w) - f (W1, w) - W f (W1, w), W2 - W1

L 2

W1 - W2

2
,
F

 W1, W2  Rd×d.

(5.7)

Since we have a handle on the smoothness of objective function, we can provide a convergence

result for the overall algorithm.

Theorem 5.9

Suppose k

<

2 L

then we have

E

min
k=0,...,N

fW (Wk, k+1)

2 F

No
f (W0, 0) +
 k=0

Ni k -1
 =1

+R2 Ni k2 2
2  =1 2(1-L k)

No

(k - L/2k2)

,

k=0

(5.8)

where R/2 is the radius of origin centred ball, R in algorithm, defined as R := {r  Rd : r 2 

R 2

}.

In view of theorem 5.9, we can derive a possible way of choosing k,  and Ni to obtain a conver-

gence

result.

More

specifically,

if

Ni

=

No, 

=

1 Ni

,

k

=

1 L

and

k

is

chosen

according

to

(5.6)

then we have

2 2L f (W0, 0) + R2(L + 1/2) + 1

E min W f (Wk, k+1) 

k=0,...,N

F

No

Note that since we prove Lipschitz smoothness of objective function, f (·, ), we can apply whole host of the algorithms developed in literature for non-convex Lipschitz smooth objective minimization. More specifically, accelerated gradient method such as unified accelerated method proposed

8

Under review as a conference paper at ICLR 2018
by Ghadimi et al. (2015) or accelerated gradient method by Ghadimi & Lan (2016) can be applied in outer iteration. We can also use stochastic gradient descent method for outer iteration. For this, we need a stochastic algorithm that is designed for non-convex and Lipschitz smooth function optimization. Randomized stochastic gradient method, proposed by Ghadimi & Lan (2013), Stochastic variance reduction gradient method (SVRG) by Reddi et al. or Simplified SVRG by Allen-Zhu & Hazan can be employed in outer iteration. Convergence of these new algorithms will follow immediately from the convergence results of respective studies. Value of Lipschitz constant, L, puts a significant impact on the running time of the algorithm. Notice that if L increases then correspondingly No and Ni increase linearly with L. So we need methods by which we can reduce the value of the estimate of L. One possible idea would be to use l1-ball for feasible region of . More specifically, if R = B1(R/2) then we can possibly enforce sparsity on  which will allow us to put better bound on L.
REFERENCES
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 699­707.
Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, pp. 115­133, 1994.
Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. In Proceedings of the First Annual Workshop on Computational Learning Theory, COLT '88, pp. 9­18, 1988.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. CoRR, 2017.
Anna Choromanska, Mikael Henaff, Michae¨l Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surface of multilayer networks. 2014.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, pp. 160­167, 2008.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for non-convex stochastic programming. SIAM Journal on Optimization, pp. 2341­2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Math. Program., 156:59­99, 2016.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Generalized uniformly optimal methods for nonlinear programming. CoRR, 2015.
Benjamin D. Haeffele and Rene´ Vidal. Global optimality in tensor factorization, deep learning, and beyond. CoRR, 2015.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, pp. 1594­1602, 2015.
Sham Kakade, Adam Tauman Kalai, Varun Kanade, and Ohad Shamir. Efficient learning of generalized linear and single index models with isotonic regression. CoRR, 2011.
Adam Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS'12, pp. 1097­1105, 2012.
Guanghui Lan. An optimal method for stochastic composite optimization. Math. Program., 133 (1-2):365­397, 2012.
9

Under review as a conference paper at ICLR 2018

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. CoRR, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. 2017.
A. Mohamed, G. E. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. Trans. Audio, Speech and Lang. Proc., pp. 14­22, 2012.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. on Optimization, 19:1574­1609, 2009.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. CoRR, 2017.
Quynh N. Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. CoRR, 2017.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnaba´s Po´czo´s, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 314­323.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization landscape of over-parametrized shallow neural networks. CoRR, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. CoRR, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. CoRR, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. CoRR, 2016.

A PROOFS OF AUXILIARY RESULTS

In this appendix, we provide proofs for auxiliary results.

A.1 PROOF OF LEMMA 4.1

The result is trivially true for d =1, we will show this using induction on d.
Define vi := vect(h(xi)xiT ), i  [N ]. Note that it suffices to prove independence of vector vi, i  [N ] for N  d2. Now for sake of contradiction assume that vi, i  [N ], are linearly dependent with positive joint measure on xi, i  [N ] which is equivalent to positive measure on individual xi,  i  [N ] due to independence of vectors xi. Since xi's are sampled from Lebesgue measure so positive measure on xi, i  [N ], implies there exists a d-dimensional volume for each xi such that corresponding vi are linearly dependent. We can assume volume to be d-dimensional hyper-cuboid Zi := {x  Rd : ai < x < bi},  i  [N ] (otherwise we can inscribe a hyper-cuboid in that volume). Notice that since Zi is a d-dimensional hyper-cuboid so ai[k] < bi[k],  i  [N ],  k  [d]. Moreover, for any collection satisfying xi  Zi, corresponding collection of vector vi are linearly dependent, i.e.,

v1 = µ2v2 + · · · + µN vN , such that i  [N ], xi  Zi.

(A.1)

Noticing the definition of Z1, we can choose > 0 s.t. x1 := x1 + e1  Z1. Since we ensure that x1  Z1 then by (A.1) we have

v1 := vect(h(x1)x1T ) = µ2v2 + · · · + µN vN .

(A.2)

So using (A.1) and (A.2) we get

v1 - v1 = 2v2 + · · · + N vN .

(A.3)

10

Under review as a conference paper at ICLR 2018

Since h(xi)xiT [j, k] = h(xi[j])xi[k], we have h(x1)x1T [j, k] = h(x1)x1T [j, k], j  {2, . . . , d}, k  {2, . . . , d}. So we have (d - 1)2 components of v1 - v1 are zero. Let us de-
fine:

(x1[1] + )h(x1[1] + ) - x1[1]h(x1[1]) 









w1

=

 







h(x1[2])
... h(x1[d]) x1[2](h(x1[1] + ) - h(x1[1]))
... x1[d](h(x1[1] + ) - h(x1[1]))



















  

(2d - 1) ,













 





0 

z1 =  ... 

 (d - 1)2

,

0

and notice that v1 - v1 =

w1 z1

. Since

> 0, w1 = 0 with measure 1.

Let yi := xi[2 : d] then last (d - 1)2 equations in (A.3) gives us

2h(y2)y2T + · · · + N h(yN )yNT = z1 = 0

(A.4)

By definition we have yi  Rd-1 are independently sampled from (d - 1)-dimensional Lebesgue

measure. So by inductive hypothesis, rank of collection of matrices h(yi)yiT , i  {2, . . . , N } =

min{(d - 1)2, N - 1}. So if N - 1  (d - 1)2 then 2 = · · · = N = 0 with measure 1, then by (A.3) we have w1 = 0 with measure 1, which is contradiction to the fact that w1 = 0 with measure

1. This gives us

N > (d - 1)2 + 1

(A.5)

Notice that (A.4) in its matrix form can be written as linear system

 2 

vect(h(y2)y2T )

...

vect(h(yN )yNT )

 

...

=0 

N

(A.6)

By (A.6), we have that vector of 's lies in the null space of the matrix. Finally by inductive hypothesis and (A.5) we conclude that the dimension of that space is N - 1 - (d - 1)2{> 0}. Let
u1, . . . , uN-1-(d-1)2  RN-1 be the basis of that null space i.e.

vect(h(y2)y2T ) . . . vect(h(yN )yNT ) uj = 0,  j  {1, N - 1 - (d - 1)2}

Define ti  R2d-1 as: then we can rewrite (A.3) as

xi[1]h(xi[1])

 

...

 

ti

:=

xi[1]h(xi[d])  xi[2]h(xi[1])



 

...

 

xi[d]h(xi[1])



w1 z1

=

t2 vect(h(y2)y2T )

... ...

tN vect(h(yN )yNT )

u1

...

uN -1-(d-1)2  

2 ...


 

N -(d-1)2

(A.7)

which implies that

w1 = 2v2 + · · · + N-(d-1)2 vN-(d-1)2

(A.8)

where vi = t2 . . . tN ui-1, i = 2, . . . , N - (d - 1)2 and z1 part of the equation is already

satisfied due to selection of null space. Since N  d2  N - 1 - (d - 1)2  2d - 2 then 2d - 1 equations specified in (A.8) are

11

Under review as a conference paper at ICLR 2018

consistent in  (2d - 2) variables. Hence we get linearly dependent equations x11  (a11, b11) and small enough. Since x2, . . . , xN are kept constant, v2, . . . , vN are constant. So t2, . . . , tN are
constants and we can choose the same basis of null space u1, . . . , uN-1-(d-1)2 . Hence we have v2, . . . , v(N-(d-1)2) are constant. Let us define the set S to be the index set of linearly independent rows of matrix [v2 . . . vN-(d-1)2 ] and every other row is a linear combination of rows in S. Since (A.8) is consistent so the same combination must be valid for the rows of w1. Now if N  d2 - 1 then number of variables in (A.8) is  2d - 3 but number of equations is 2d - 1,
therefore at least two equations are linearly dependent on other equation. This implies last (2d - 2)
equations then function must be dependent on each other:

dd

jh(x(1)[j]) + h(x(1)[1] + ) - h(x(1)[1])

j x(1)[j] = 0

j=2

j=2

for some fixed combination j, j. If we divide above equation by and take the limit as then we see that h satisfies following differential equation on interval (a11, b11):
h (x) = c1

0

which is a contradiction to the condition C1! Clearly this leaves only one case i.e. N = d2 and (2d - 1) equations must satisfy dependency of
the following form for all x(11)  (a(11), b1(1)):

(x(1)[1] + )h(x(1)[1] + ) - x(1)[1]h(x(1)[1])

dd
= jh(x(1)[j]) + h(x(1)[1] + ) - h(x(1)[1]) jx(1)[j]

j=2

j=2

Again by similar arguments, the combination is fixed. Let H(x) = xh(x) then dividing above

equation by and taking the limit as  0, we can see that h satisfies following differential

equation:

H (x) = c1 + c2h (x)  (x - c2)h (x) + h(x) = c1

(A.9)

which is again a contradiction to the condition C1 So we conclude that for N  d2 there does not exist hyper-cuboids Zi such that vol(Zi) > 0 and for all xi  Zi, corresponding vi are linearly dependent. So we get rank of collection {vi}Ni=1 = min{N, d2} with measure 1.

A.2 PROOF OF COROLLARY 4.2

Let us define x := W u be another random variable. Since W is full rank and u has Lebesgue measure  x has Lebesgue measure. Now we claim that the collection h(W ui)uiT is full rank iff the collection h(xi)xiT is full rank.
This can observed as follows:

NN

ih(xi)xiT = 0 

ih(W ui)uiT W T = 0

i=1 i=1

N
 ih(W ui)uiT = 0

i=1

Here the second statement follows from the fact W is a non-singular matrix. Now by lemma 4.1 we have that collection h(xi)xiT is linearly independent with measure 1. So h(W ui)uiT is linearly independent with measure 1.
Since any rotation is U is a full rank matrix so we have the result.

A.3 PROOF OF LEMMA 5.1
This is true for k = 0 trivially since we are randomly sampling matrix W0. We now show this by induction on k.

12

Under review as a conference paper at ICLR 2018

N
Recall that gradient of f (W, ) with respect to W can be written as {vi-T h(W ui)} h (W ui)
i=1
 uiT . Notice that in effect, we are multiplying i-th row of the rank one matrix h (W ui)uiT by i-th
element of vector . So this can be rewritten as a matrix product

N
{vi - T h(W ui)}h (W ui)uiT ,
i=1
where  := diag{[i], i = 1, . . . , d}. So iterative update of the algorithm can be given as

Wk+1 = Wk - kk+1W f (Wk, k+1),  k  0.

Notice that given [k], vector k+1 and corresponding diagonal matrix k+1 are found by SGD in the inner loop so k+1 is a random vector. More specifically, since {ik+1}iN=i1 is sequence of independent d-dimensional isotropic Gaussian vectors. Hence the distribution of k+1 = {ik+1}iN=i1 induces a Lebesgue measure on random variable {k+1 [k]}
Given [k] then Wk is deterministic quantity. For the sake of contradiction, take any vector v that is supposed to be in the null space of Wk+1
with positive probability.

Wk+1 = Wk - kW f (Wk, k+1)

N
= Wk - k k+1(vi - kT+1h(Wkui))h (Wkui)uiT .
i=1

N
 Wk+1v = Wkv - k k+1(vi - kT+1h(W ui))h (Wkui)uiT v = 0.
i=1

N
 Wkv = k+1 (ivi - riT k+1)h (Wkui)
i=1

N

= k+1

ivih (Wkui) -

i=1

N
h (Wkui)riT k+1 .
i=1

setting i = k(vT ui), ri = ih(Wkui)

Now the last equation is of the form

b = k+1[w - M k+1],

(A.10)

where b = Wkv, w = N ivih (Wkui), M = N h (Wkui)riT .
i=1 i=1
Suppose we can find such  with positive probability. Then we can find hypercuboid Z := {x  Rd|a < x < b} such that any k+1 in given hypercuboid can solve equation (A.10). By induction we have b = 0. We may assume b[1] = 0. Then to get contradiction on existence of Z, we observe
that first equation in (A.10) is:

d
b[1] = k+1[1] w[1] - M [1, j]k+1[j] - M [1, 1]k+1[1]2,  k+1  (a, b).
j=2

(A.11)

Hence if we fix k+1[i]  (a[i], b[i]), i = 2, . . . , d then (A.11) holds for all k+1[1]  (a[1], b[1]). So
d
we conclude that b[1] = w[1] + M [1, j]k+1[j] = M [1, 1] = 0. But b[1] can not be 0. Hence we
j=2
arrive at a contradiction to the assumption that there existed a hypercuboid Z containing solutions
of (A.10). Since measure on k+1 was induced by {ik+1}Ni=i1 so we conclude that Pr{ v such that Wk+1v = 0 [k]} = 0,  k  0.

13

Under review as a conference paper at ICLR 2018

A.4 PROOF OF LEMMA 5.2

We use induction on d. For d = 1 this is trivially true. Now assume this is true for d - 1. We will

show this for d. Let zi := W ui = W ui + DvZui. For simplicity of notation define ti := Zui. Due to simple linear algebraic fact provided by full rank property of W we have rank of collection (h(W ui)uiT = rank

of collection h(zi)ziT . For the sake of contradiction, say the collection is rank deficient with positive

probability then there exists d-dimensional volume V such that for all v  V, we have h(W ui)uiT

is not full rank where W := W (v) = W + DvZ. Without loss of generality, we may assume d-dimensional volume to be a hypercuboid V := {x  Rd|a < x < b} (if not then we can inscribe
a hypercuboid in that volume). Let us take v  V and  small enough such that v := v + e1  V. Correspondingly we have zi and zi. Note that zi = zi + ti[1]. So in essence, a small  change in v[1] causes ti[1] change in vector zi[1].

Let vi = vect(h(zi)ziT ). Similarly, vi = vect(h(zi)ziT ). So we can divide vi =

ci gi

such

ci  R2d-1 and gi  R(d-1)2 . Here

h(zi[1])zi[1]

h(zi[2])zi[1]



 

...

 



ci := h(zi[d])zi[1] ,



h(zi[1])zi[2]



 

...

 

h(zi[1])zi[d]

gi := vect(h(yi)yiT ),

yi := zi[2 : d]

Similarly we also have vi =

ci gi

.

Now by the act that v, v corresponding to z, z are in V, and our

assumption of linear dependence for all v  V we get

v1 = µ2v2 + · · · + µN vN v1 = µ2v2 + · · · + µN vN

(A.12) (A.13)

Now notice that yi = yi,  i  [N ]. So gi = gi,  i  [N ]. Also by induction on d - 1, we have that

the rank of collection g2, . . . , gN  (d - 1)2. So we can rewrite matrix [g2 . . . gN ] := [G G] such

that G  R(d-1)2×(d-1)2 is an invertible matrix and rewrite one part of equation (A.12) as g1 =

[G

G]

µ µ

. Hence we can replace µ = G-1(g1 - Gµ) = G-1g1 - G-1Gµ. Essentially the vector

µ µ

is completely defined by parameter µ  RN-1-(d-1)2 . Similarly we have µ = G-1g1 - Gµ,

so vector

µ µ

is completely defined by µ  RN-1-(d-1)2 . So essentially we have satisfied one part

of equations (A.12) and (A.13). Notice that since we are moving only one coordinate of random

vector v i.e. v[1]  (a[1], b[1]) (by  incremental changes) keeping all other elements of v constant

so we will have yi as constants which implies gi, G, G are constant. So for the sake of simplicity of

notation we define l := G-1g1  R(d-1)2 and R := G-1G  R(d-1)2×(N-1-(d-1)2)

Now, we look at the remaining part of two equation (A.12),(A.13):

c1 = µ2c2 + · · · + µN cN , c1 = µ2c2 + · · · + µN cN ,

which can be rewritten as

c1 = [C

C]

l - Rµ µ

= Cl - CRµ + Cµ,

(A.14)

c1 = [C

C]

l - Rµ µ

= Cl - CRµ + Cµ.

(A.15)

14

Under review as a conference paper at ICLR 2018

After (A.15) - (A.14), we have

(C - C)l - (C - C)Rµ - CR(µ - µ) + (C - C)µ + C(µ - µ) = c1 - c1.

(A.16)

Now note that (A.16), characterizes incremental changes in C, C, µ due to . So taking the limit as   0, we have

c1 = C l - C Rµ - CRµ + C µ + Cµ .

c1

C

1 -l

= (-CR + C)µ + (-C R + C )µ.

 c1

C

1 -l

= (-CR + C)µ.

(A.17)

Here, last equation is due to product rule in calculus. In (A.17), we see that we have 2d-1 equations and N - 1 - (d - 1)2 unknowns at every point. If N  d2 then N - 1 - (d - 1)2  2d - 2. So
at least one equation should depend on others. But as we have shown earlier, h satisfying condition C1 does not have row dependence. So we arrive at the required contradiction for N  d2. That
completes the proof.

A.5 PROOF OF LEMMA 5.4

Assume that all the gradients in this proof are w.r.t. W then we know that

-f (W, )[j, k] = 1

N
{vi - T h(W ui)}h (W [j, :]ui)[j]ui[k]

N

i=1

Notice that W F = vect(W ) 2. Also notice that if W = abT then W F = a 2. b 2 Let us define vector ai s.t. ai[j] = [j]h (W [j, :]ui)(vi - T h(W ui)) so we have

-(f (W1)

-

f (W2))jk

=

1 N

N

uki (a1i [j] - a2i [j])

i=1

1  -(f (W1) - f (W2)) = N

N
(a1i - a2i )uiT

i=1



f (W1) - f (W2)

F



1 N

N

ui 2. a1i - a2i 2,

i=1

where the last inequality follows from Cauchy-Schwarz inequality. So if we can show Lipschitz constant Li on a1i - a2i 2,  i then we are done. Let max := max |j|, then
j

(A.18)

(ai1 - a2i )[j] = |j|. h (W1[j, :]ui)(vi - T h(W1ui)) - h (W2[j, :]ui)(vi - T h(W2ui))

 max h (W1[j, :]ui)(vi - T h(W1ui)) - h (W2[j, :]ui)(vi - T h(W2ui))

 a1i - a2i 2  max vi h (W1ui) - h (W2ui) - h(W1ui)h (W1ui)T - h(W2ui)h (W2ui)T 
2
 max vi h (W1ui) - h (W2ui)
2

+ (h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T ) .
2

Suppose the Lipschitz constants for the first and second term are Li,L and Li,R respectively. Then

N

Li

=

max(Li,L

+ Li,R)

and

possible

upper

bound

on

value

of

L

would

become

1 N

ui 2Li.

i=1

15

Under review as a conference paper at ICLR 2018

We now analyse existence of Li,L Since the Hessian of scalar function h(·) is bounded so we have h (x) is Lipschitz continuous with constant Lh . Let r1, r2 be two row vectors then we claim h (r1x) - h (r2x) 2  Lh x 2. r1 - r2 2,  r1, r2 because:
h (r1x) - h (r2x) 2  Lh r1x - r2x  Lh x 2 r1 - r2 2
From the relation above we have the following:

d

h (W1ui) - h (W2ui)

2 2

=

2
h (W1[j, :]ui) - h (W2[j, :]ui)

j=1

d

 Lh2

ui

2 2

W1[j, :] - W2[j, :]

2 2

=

Lh2

j=1

 Li,L = Lh ui 2|vi|.

Now we focus our attention to second term. Notice the simple fact that

ui

2 2

W1 - W2

2 F

(A.19)

W1 - W2 2  W1 - W2 F = vect(W1 - W2) 2. Define v := W1ui, u := W2ui, then we have

(A.20)

v - u 2 = (W1 - W2)ui  W1 - W2 . ui  ui . vect(W1 - W2) ,

2

22

2

2

and

(A.21)

h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T 
2

  . h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T
22

=  . h (v)h(v)T - h (u)h(u)T
22

  . h (v)h(v)T - h (u)h(u)T .
2F
The latter inequality implies that

2
(h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T )
2

d2





2 2

h (v[i])h(v[j]) - h (u[i])h(u[j]) .

i,j=1

Now let us define a 2-D function H(x1, x2) = h(x1)h (x2). Then H(x1, x2) =
so under given assumptions, H(·) 2 is bounded. Let that bound be Lhh . Now by mean value theorem, we have

h (x1)h (x2) h(x1)h (x2)

H(x1, x2) - H(y1, y2) = H()T {(x1, x2) - (y1, y2)}
22
 H(x1, x2) - H(y1, y2)  H() . (x1 - y1)2 + (x2 - y2)2
2
 Lh2h (x1 - y1)2 + (x2 - y2)2

2
So h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T 
2

d2





2 2

h (v[i])h(v[j]) - h (u[i])h(u[j])

i,j=1

d





2 2

L2hh (v[i] - u[i])2 + (v[j] - u[j])2

i,j=1

= 2dLh2h



2 2

v-u

2 2

(A.22)

16

Under review as a conference paper at ICLR 2018

It then follows from (A.20),(A.21) and (A.22) that

(h (W1ui)h(W1ui)T - h (W2ui)h(W2ui)T ) 2
 2dLhh  2. ui 2. W1 - W2 F
 So you get that Li,R = 2dLhh  2 ui 2 Finally, using (A.18), (A.19) and (A.22), we get a possible finite upper bound on the value of L:

L



1 N

max

Lh

N N

ui 22|vi| + 2dLhh  2

ui

2 2

i=1 i=1

Also note that this bound is valid even if W is not a square matrix.

A.6 PROOF OF LEMMA 5.7

Noting that

-f (W, )

=

1 N

N
{vi - T h(W ui)}h(W ui),

i=1

we have

f (W, 1) - wf (W, 2) 2

=

1N N

{vi - 1T h(W ui)}h(W ui) - {vi - 2T h(W ui)}h(W ui)

i=1

2

=

1N N

{-h(W ui)h(W ui)T 1 + h(W ui)h(W ui)T 2}

i=1

2

=

1 N

N
h(W ui)h(W ui)T (2 - 1)
i=1

2



1

N
h(W ui)h(W ui)T

N

i=1

. 1 - 2
2

2

1 =
N

N

h(W ui)h(W ui)T

. 1 - 2
2

2

i=1

1 = N max

N
h(W ui)h(W ui)T . 1 - 2 2

i=1

1 N

N
max h(W ui)h(W ui)T
i=1

. 1 - 2 2

 Weyl's Inequality

1 =
N

N

h(W ui)

2 2

. 1 - 2 2

i=1

 u2d 1 - 2 2

where u1 and u2 are upper bounds on scalar functions |h(·)| and |h (·)| respectively and d is rowdimension of W .

17

Under review as a conference paper at ICLR 2018

A.7 PROOF OF THEOREM 5.9

We know by lemma 5.4 that f (·, ) is a Lipschitz smooth function. So using (5.7) we have

L2 f (Wk+1, k+1)  f (Wk, k+1) + 2 vect(Wk+1 - Wk)

+ vect(W f (Wk, k+1)), vect(W1k+1 - W1k )

= f (Wk, k+1) -

k

-

L 2

k2

2
vect(W f (Wk, k+1))

 f (Wk, k) +

Ni
k -1

1 2

k - W k

Ni

2 2

+

k

k, W k - k

 =1

 =1

+

Ni  =1

k2 2(1 -

k 2 L k )

-

(k

-

L 2

k2

)

vect(W G(Wk, k+1))

2
,

(A.23)

where the last inequality follows from equation (A.28) and (A.29).

From (3.6), we have   R/2 so L is constant for each outer iteration. Summing (A.23) from

k

=

0 to No

and dividing both side by

No
(k -

L 2

k2),

we

get

k=0

min
k=0,...,N

2

W f (Wk, k+1)

2 F



No

(k

-

L 2

k2)

vect(W1 f (Wk, k+1))

No

k=0

(k

-

L 2

k2

)

k=0

No
f (W0, 0) +
k=0


Ni k -1

R2 2

+

Ni

k k, W k - k

 =1

 =1

No
(k - L/2k2)
k=0

+ k2 k 2
2(1-L k)

.

Now taking expectation with respect to [No] (which is defined in (5.1)), we have

E k, W k - k [k-1]  [k-1] = 0,

which implies E[No] k, W k - k = 0. We also have E[No] k 2  2, and hence

E min
k=0,...,N

2
W f (Wk, k+1)
F

No
f (W0, 0) +
 k=0

Ni k -1
 =1

+R2 Ni k2 2
2  =1 2(1-L k)

No

(k - L/2k2)

k=0

.

A.8 PROOF OF THEOREM 5.8

For sake of simplicity of notation, we define f (·) := f (Wk, ·), g(·) := f (·) = f (Wk, ·) and GWk ( , k) := G . Then from (3.4) and Algorithm 1 we get

G = g( ) + k.

(A.24)

Also define d := +1 -  . Notice that +1 is optimal solution to the problem

min 
uRd

G , u - 

1 +
2

u - 

2 2

,

(A.25)

by simply writing first order necessary condition for problem (A.25). Also we note that objective function in (A.25) is strongly convex with parameter 1. Then we have



G , d

1 +
2

d

2 2

+

1 2

u - +1

2 2





G , u - 

1 +
2

u - 

22.

(A.26)

18

Under review as a conference paper at ICLR 2018

We will use eq (A.26) along with smoothness and convexity of the function f to get the final convergence result. Notice that due to smoothness, we have

 f (+1)   [f ( ) +

g( ), d

+ L 2

d

2]

=  [f ( ) +

g( ), d

1 ]+
2

d

2 - (1 - L ) 2

d

2.

then due to (A.24) we have,

 f (+1)   [f ( ) + G , d ] - 

k, d

1

+ 2

d

2 - (1 - L ) 2

d

2

  [f ( ) +

G , d

1 ]+
2

d

2 - (1 - L ) 2

d

2 + 

k

.

d

  f ( ) +



G , d

1 +
2

d

2

+

2 k 2 2(1 - L

. )

By (A.26) we have

 f (+1)   f ( ) + 

G , u - 

1 +
2

u - 

2- 1 2

u - +1

2 + 2 k 2 2(1 - L )

=  f ( ) +  g( ), u -  +  k, u - 

1 +
2

u - 

2- 1 2

u - +1

2

+

2k 2 2(1 - L )

  f (u) + 

k, u - 

1 +
2

u - 

2- 1 2

u - +1

2

+

2 k 2(1 - L

2


)

.

(A.27)

Last equation is due to convexity of function f . So using (A.27) we have

i
 f (+1) - f (W k )
 =1

1 2

i
1 - W k 2 +
 =1



k, W k - 

+ 2 k 2 2(1 - L )

.

(A.28)

Note that from convexity of f , we get

f (ia+v1) - f (W k ) 

ii

 -1

 f (+1) - f (W k ) .

 =1

 =1

(A.29)

Moreover, noting the definition of [k] in (5.2) so we have,

E k, W k -  [k-1] = 0,

(A.30)

and from (3.5) we get E

k 2



2.

Hence

using

this

relation

and

noting

1 - L



1 2

,

(A.28),

(A.29) and (A.30) we prove the result.

19

