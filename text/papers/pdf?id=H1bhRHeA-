Under review as a conference paper at ICLR 2018

EXACT SCALABLE SOFTMAX OPTIMIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
Recent state-of-the-art neural network and language models have begun to rely on softmax distributions with an extremely large number of categories. In this context calculating the softmax normalizing constant is prohibitively expensive, which has spurred a growing literature of efficiently computable but biased estimates of the softmax. In this paper we present the first two unbiased algorithms for optimizing the softmax whose work per iteration is independent of the number of classes and datapoints (and does not require extra work at the end of each epoch). We compare their empirical performance to the state-of-the-art on seven real world datasets, with our Implicit SGD algorithm comprehensively outperforming all competitors.

1 INTRODUCTION

Under the softmax model1 the probability that a random variable y takes on the label  {1, ..., K},

is given by

p(y = |x; W ) =

ex w

K k=1

ex

,
wk

(1)

where x  RD is the covariate, wk  RD is the vector of parameters for the k-th class, and W = [w1, w2, ..., wK ]  RK×D is the parameter matrix. Given a dataset of N label-covariate pairs D = {(yi, xi)}Ni=1, the ridge regularized maximum log-likelihood problem is given by

L(W ) =

N

K
xi wyi - log(

exi wk ) - µ 2

W

22.

i=1 k=1

(2)

This paper focusses on how to numerically maximize (2) when both N and K are large. This is increasingly the case in modern applications such as natural language processing (Partalas et al., 2015). The softmax also has numerous applications in others fields such as economics and biomedicine (Rust & Zahorik, 1993; Gopal & Yang, 2013) and appears as a convex surrogate for the (hard) maximum loss in discrete optimization (Maddison et al., 2016) and network flows (Shahrokhi & Matula, 1990).

The difficulty in maximizing L(W ) for large values of K is that the normalizing sum

K k=1

exi

wk

becomes prohibitively expensive to calculate. Several approximations that avoid calculating the

normalizing sum have been proposed to address this difficulty. These include tree-structured meth-

ods (Bengio et al., 2003; Daume III et al., 2016; Grave et al., 2016), sampling methods (Bengio

& Sene´cal, 2008; Mnih & Teh, 2012; Joshi et al., 2017) and self-normalization (Andreas & Klein,

2015). Alternative models, such as the spherical family of losses (de Bre´bisson & Vincent, 2015;

Vincent et al., 2015), which do not require normalization have also been proposed to sidestep the

issue entirely (Martins & Astudillo, 2016). Krishnapuram et al. (2005) avoid calculating the sum us-

ing a maximization-majorization approach based on lower-bounding the eigenvalues of the Hessian

matrix.

All2 of these approximations are computationally tractable for large N and K, but do not converge to the true optimum W  = argmax L(W ). The goal of this paper is to develop methods that are both tractable and converge to the exact solution of (2) for large N and K.

1Also known as the multinomial logit model. 2The method of Krishnapuram et al. (2005) does converge to the true optimum, but has O(N D) runtime
per iteration which is not feasible for large N .

1

Under review as a conference paper at ICLR 2018

Following Raman et al. (2016), we recast (2) as a double-sum. The double-sum representation is amenable to Stochastic Gradient Descent (SGD) with only O(D) computation per iteration. The difficulty is that vanilla SGD is numerically unstable when applied to this formulation. To counter this instability we develop a new SGD method called U-max, which is guaranteed to have bounded gradients and converge to the true solution of (2) for all sufficiently small learning rates. We also discuss a second method that employs Implicit SGD, a stochastic gradient method that is known to be more stable than vanilla SGD and yet has similar convergence properties (Toulis et al., 2016). We show that the Implicit SGD updates for the double-sum formulation can be efficiently computed in a numerically stable manner.
We compare the performance of U-max and Implicit SGD to the state-of-the-art (biased) methods for optimizing the softmax which have runtime O(D) per iteration. Implicit SGD clearly outperforms all competitors, having a prediction error rate on average 40% lower than the next best algorithm. U-max has more variable performance due to its sensitivity to the learning rate.
In summary, our contributions in this paper are as follows:
1. Provide a simple derivation of the softmax double-sum formulation and identify why vanilla SGD is numerically unstable when applied to this formulation (Section 2).
2. Propose the U-max algorithm to stabilize the SGD updates and prove its convergence (Section 3.1).
3. Derive efficient Implicit SGD updates and analyze their runtime (Section 3.2).
4. Conduct experiments which show that Implicit SGD outperforms the previous state-of-theart (Section 4).

2 CONVEX DOUBLE-SUM FORMULATION

2.1 DERIVATION OF DOUBLE-SUM

In order to apply an SGD method that samples both datapoints and classes each iteration, we need to represent (2) as a double sum over datapoints and classes. We begin by rewriting (2) in a more convenient form,

N
L(W ) = - log(1 +

exi )(wk-wyi ) - µ 2

W

22.

i=1 k=yi

(3)

The key to converting (3) into its double-sum representation is to express the negative logarithm using its convex conjugate:

- log(a) = max{av - (- log(-v) - 1)}
v<0
= max{-u - exp(-u)a + 1}
u

(4)

where u = - log(-v) and the optimal value of u is u(a) = log(a). Applying (4) to each of the logarithmic terms in (3) yields

L(W )

=

N i=1

max{-ui
ui R

-

e-ui (1

+

k=yi

exi

)(wk-wyi )

+

1}

-

µ 2

W

2 2

= - min {f (u, W )} + N,
u0

where

N
f (u, W ) =

ui + e-ui + exi (wk-wyi )-ui + µ

K -1

2

W

2 2

i=1 k=yi

(5)

is our double-sum representation and the optimal solution for ui is ui(W ) = log(1 +
k=yi exi )(wk-wyi )  0. Clearly f is a jointly convex function in u and W . In Appendix A we prove that the optimal value of u and W is contained in a compact convex set and that f is strongly

2

Under review as a conference paper at ICLR 2018

convex within this set. Thus performing projected-SGD on f is guaranteed to converge to a unique optimum with a convergence rate of O(1/T ) where T is the number of iterations (Lacoste-Julien et al., 2012).
Raman et al. (2016) derived a similar expression for f , which is equivalent to applying the convex conjugate substitution to (2) instead of (3). Our double-sum formulation leads to more stable stochastic gradients and faster convergence, as will be discussed in Section 2.3. Their method for optimizing f also requires that the value of ui(W ) be calculated exactly at the end of each epoch for all i = 1, ..., N , at a cost of O(N KD). This is a significant bottleneck, even under their parallel implementation, and makes their method ill-suited for when N, K and D are all large. The goal in this paper is to avoid the O(N KD) bottleneck and to optimize f only using stochastic gradient descent methods that take O(D) per iteration.

2.2 NUMERICAL INSTABILITY OF VANILLA SGD

The challenge in optimizing f using SGD is that it is not numerically stable. Note that f = Eik[fik] where i  unif({1, ..., N }), k  unif({1, ..., K} - {yi}) and

fik(u, W ) = N

ui + e-ui + (K - 1)exi )(wk-wyi )-ui

µ + 2 (yi

wyi

2 2

+

k

wk

2 2

),

(6)

where j

=

N nj +(N -nj )(K-1)

is the inverse of the probability of class j

being sampled either through

i or k, and nj = |{i : yi = j, i = 1, ..., N }|. The corresponding stochastic gradient is:

wk fik(u, W ) = N (K - 1)exi x(wk-wyi )-ui i + µkwk
wyi fik(u, W ) = -N (K - 1)exi x(wk-wyi )-ui i + µyi wyi wj/{k,yi} fik(u, W ) = 0
ui fik(u, W ) = -N (K - 1)exi (wk-wyi )-ui + N (1 - e-ui )

(7)

If ui equals its optimal value ui(W ) = log(1 + k=yi exi )(wk-wyi ) then exi (wk-wyi )-ui  1 and the magnitude of the N (K - 1) terms in the stochastic gradient is bounded by N (K - 1) xi 2.
However if ui xi (wk - wyi ), then exi (wk-wyi )-ui 1 and the magnitude of the N (K - 1) terms can become extremely large. In practice we found that applying vanilla SGD lead to numerical
instability because of overflow. Increasing the number of bits or decreasing the step size helped with
stability, but led to much slower convergence.

The same problem of numerical instability arises if we approach optimizing (3) via stochastic composition optimization (Wang et al., 2016). As is shown in Appendix B, stochastic composition optimization yields near-identical expressions for the stochastic gradients in (7) and has the same numerical stability issues.

All of the (biased) sampled softmax optimizers in the literature (Bengio & Sene´cal, 2008; Mnih & Teh, 2012; Joshi et al., 2017) are numerically stable because their approximation to ui(W ) is always greater than or equal to xi (wk - wyi ). For example, in one-vs-each (Titsias, 2016), ui(W ) is approximated by log(1 + exi )(wk-wyi )  xi (wk - wyi ).
The U-max method that we will propose in Section 3.1 stabilizes SGD by increasing the value of ui whenever it is significantly below xi (wk - wyi ). This is similar in effect to clipping the gradients; however, unlike clipping, this method is guaranteed to converge to the true optimum. The other approach we pursue for stabilizing SGD is to use Implicit SGD. Implicit SGD is ideally suited to situations where the magnitude of the gradient sharply decreases along a gradient descent direction, as is the case when ui xi (wk - wyi ). The Implicit SGD method is discussed in Section 3.2.

2.3 CHOICE OF DOUBLE-SUM FORMULATION
In Section 2.1 we claimed that applying the convex conjugate substitution to (3) instead of (2) leads to more stable gradients and faster convergence. Here we provide some theoretical intuition to support the claim and later in Section 4 we will provide some numerical evidence.

3

Under review as a conference paper at ICLR 2018

Let us consider applying the convex conjugate substitution to (2) instead of (3). The corresponding

component function fik is

f~ik(u~, W ) = N

u~i - xi wyi + exi wyi -u~i + (K - 1)exi wk-u~i

µ + 2 (yi

wyi

2 2

+

k

wk

22)

(8)

and the optimal solution for u~i is u~i (W ) = log(

K k=1

exi

wk ).

The

functions

f

and

f~ are

identical

if we set ui = u~i - xi wyi . Typically xi wyi = argmaxk{xi wk}  0 and so the u~i, xi wyi and

exi wyi -u~i terms in (8) are of the greatest magnitude3. Even though at optimality these terms should

roughly cancel, this will not be the case during the early stages of optimization, leading to stochastic

gradients with large magnitude.

In contrast the function fik in (6) only has xi wyi appearing as a negative exponent. Thus if xi wyi is large then the magnitude of the stochastic gradients will be small. Since the converge of SGD is
inversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the
fik formulation to converge faster. In Section 4 we present numerical results confirming this fact.

3 STABLE SGD METHODS

3.1 U-MAX METHOD

As explained in Section 2.2, vanilla SGD becomes unstable when ui < xi (wk - wyi ). However we know4 that ui(W )  xi (wk - wyi ). Hence instability occurs only when ui is less than its optimum value for the current value of W . If ui < xi (wk - wyi ) then increasing ui will bring it closer to ui(W ) and decrease the objective f (u, W ). If we only know the value of xi (wk - wyi ), the most that we can increase ui without overshooting ui(W ) is to set ui = log(1 + exi (wk-wyi )). Since ui = log(1 + exi )(wk-wyi )  xi (wk - wyi ), this also restores the stability of the SGD gradients.

This is exactly the mechanism behind the U-max algorithm ­ see Algorithm 1 in Appendix C for its

pseudocode. U-max is the same as vanilla SGD except for two modifications: (a) ui is set equal to

log(1 + exi )(wk-wyi ) whenever ui  log(1 + exi )(wk-wyi ) -  for some threshold  > 0, (b) ui
is projected onto [0, Bu], and W onto {W : W 2  BW }, where Bu and BW are set so that the optimal ui  [0, Bu] and the optimal W  satisfies W  2  BW . See Appendix A for more details on how to set Bu and BW .

Theorem 1.

Let Bf

= max

W

2 2

BW2

,0uBu ,

maxik

fik(u, W ) 2. Suppose learning rate t 

2/(4Bf2), then U-max with threshold  converges to the optimum of (2), and the rate is at least as

fast as SGD with same learning rate, in expectation.

Proof. The proof is provided in Appendix D.

Since   xi (wk - wyi ) - ui (otherwise ui would be increased to log(1 + exi (wk-wyi ))), the magnitude of the U-max stochastic gradients is bounded above by N (K - 1)e. Thus, when  is small the magnitude of the gradients will be small, and they can be computed in a numerically stable manner. However, Theorem 1 implies that the learning rate needs to be set low to guarantee convergence. When  is large the learning rate can be large, but the gradient computation will be less numerically stable. Thus, in U-max, there is a trade-off between the learning rate  and numerical stability that is controlled by .

3.2 IMPLICIT SGD

Another method that solves the numerical instability issues is Implicit SGD5 (Bertsekas, 2011; Toulis et al., 2016). Implicit SGD uses the update equation

(t+1) = (t) - tf ((t+1), t),

(9)

3Recall that u~i = ui + xi wyi  xi wyi and so u~i will be large if xi wyi is. 4Since ui(W ) = log(1 + j=yi exi )(wk-wyi )  xi (wk - wyi ). 5Also known to as an "incremental proximal algorithm" (Bertsekas, 2011).

4

Under review as a conference paper at ICLR 2018

f( ) = 2 f( ) = 2

SGD iterates
4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0
2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

Implicit SGD iterates
4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0
2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

Figure 1: Illustration of the difference between SGD and Implicit SGD in optimizing f () = 2. The learning rate was set to be  = 1.75 in both cases.

where (t) is the value of the tth iterate, f is the function we seek to minimize and t is a random variable controlling the stochastic gradient such that f () = Et [f (, t)]. The update (9) differs from vanilla SGD in that (t+1) appears on both the left and right side of the equation, whereas in vanilla SGD it appears only on the left side. In our case  = (u, W ) and t = (it, kt) with f ((t+1), t) = fit,kt (u, W ).
It is easiest to explain the difference between SGD and Implicit SGD using an example. Consider the quadratic function f () = 2. The SGD vanilla update is (t+1) = (t) · (1 - t) whereas the Implicit SGD update is (t+1) = (t)/(1 + t) where t is the learning rate. An illustration of the iterates of vanilla SGD and Implicit SGD are plotted in Figure 1. We can observe that Implicit SGD converges faster to the optimum and takes smaller steps. Notably, Implicit SGD never overshoots the optimum, whereas SGD does. This property is particularly important in our application where the numerical instability comes from SGD gradients that grossly overshoot the optimum. Another desirable property of Implicit SGD is that it is more robust to the learning rate as compared to vanilla SGD (Toulis et al., 2016). Again this is important since a good value for the learning rate is never known a priori.
The difficulty in applying Implicit SGD is that in each iteration one has to compute a solution to (9). The tractability of this procedure is problem dependent. We show that computing a solution to (9) is indeed tractable for the problem considered in this paper. The details of these mechanisms are laid out in full in Appendix E. Below we comment on their runtime per iteration.
Proposition 1. Consider the Implicit SGD algorithm where in each iteration only one datapoint i and one class k = yi is sampled. Then the Implicit SGD iterate (t+1) can be computed to within accuracy in runtime O(D + log( -1)).
Proposition 2. Consider the Implicit SGD algorithm where in each iteration n datapoints and m classes are sampled. Then the Implicit SGD update (t+1) can be computed to within accuracy in runtime O(n(n + m)(D + n -1)).
Proof. The proofs are provided in Appendix E.
The reason for the -1 factor in the runtime of Proposition 2 is because the Implicit SGD update optimization problem contains a large number of variables and can only be solved with first-order methods. The optimization problem in Proposition 1 is much smaller and so second-order methods can be used, giving the O(log( -1)) runtime.
The runtime in Proposition 2 consists of two terms: the O(n(n + m)D) term for taking the inner products xi wk and the O(n2(n + m) -1) term for computing the resulting Implicit SGD update. When n = O(D ) the cost of computing the Implicit updated is approximately the same as taking the inner products, and so Implicit SGD will have a similar speed as vanilla SGD.
In the O(·) runtimes above we have ignored the time required to read or write the vectors wk. This cost may, indeed, be the bottleneck in the practical performance of our algorithms. The advantage

5

Under review as a conference paper at ICLR 2018

Table 1: Datasets with a summary of their properties. Where the number of classes, dimension or number of examples has been altered, the original value is displayed in brackets.

DATASET

CLASSES

DIMENSION

EXAMPLES

MNIST Bibtex Delicious Eurlex AmazonCat-13K Wiki10 Wiki-small

10 147 (159) 350 (983) 838 (3,993) 2,709 (2,919) 4,021 (30,938) 18,207 (28,955)

780 1,836 500 5,000 10,000 (203,882) 10,000 (101,938) 10,000 (2,085,164)

60,000 4,880 12,920 15,539 100,000 (1,186,239) 14,146 90,737 (342,664)

of sampling multiple points and classes each iteration can be seen by considering the ratio of inner product calculations to memory access operations. If we can maximize the number of inner products per memory access operation, then we can extract the maximum amount of information about our function per unit time (measured in memory access operations), leading to better performance. In Proposition 1 we have 2 inner products (one for the true class and one for the sampled class) and 2 memory access operations, which gives a ratio of 1. In Proposition 2 we have n(n + m) inner products (n datapoints with n true classes and m sampled classes) and n + m memory access operations, for a ratio of n. Thus sampling multiple points and classes each iteration leads to a higher inner product to memory access operation ratio, which should lead to faster runtime.
The benefit of sampling multiple points and classes must be balanced by the O( -1) vs O(log( -1)) runtime to solve the resulting Implicit SGD update equation and the requirement that n = O(D ) so that Implicit SGD has a similar runtime to vanilla SGD. Ultimately the choice of whether to sample a single or multiple datapoints and classes will depend on the properties of the hardware available and the value of D, which is problem dependent.
4 EXPERIMENTS
Here we report the results of experiments comparing the performance of U-max and Implicit SGD to the state-of-the-art on seven different datasets. We'll begin by specifying the experimental setup and then move onto the results.
Data. We ran our algorithms on the MNIST, Bibtex, Delicious, Eurlex, AmazonCat-13K, Wiki10, and Wiki-small datasets6, the properties of which are summarized in Table 1. Most of the datasets are multi-label and, as is standard practice (Titsias, 2016), we took the first label as being the true label and discarded the remaining labels. To make the computation more manageable, we truncated the number of features to be at most 10,000 and the training and test size to be at most 100,000. If, as a result of the dimension truncation, a datapoint had no non-zero features then it was discarded. The features of each dataset were normalized to have unit L2 norm. All of the datasets were preseparated into training and test sets. We only focus on the performance on the algorithms on the training set, as the goal in this paper is to investigate how best to optimize the softmax likelihood, which is given over the training set.
Algorithms. We compared our algorithms to the state-of-the-art methods for optimizing the softmax which have runtime O(D) per iteration7. The competitors include Noise Contrastive Estimation (NCE) (Mnih & Teh, 2012), Importance Sampling (IS) (Bengio & Sene´cal, 2008) and One-VsEach (OVE) (Titsias, 2016). Note that these methods are all biased and will not converge to the true softmax optimum, but something close to it. For these algorithms we set n = 100, m = 5,
6All of the datasets were downloaded from http://manikvarma.org/downloads/XC/ XMLRepository.html, except Wiki-small which was obtained from http://lshtc.iit. demokritos.gr/.
7Or O(nmD) per mini-batch where n are the number of datapoints in the mini-batch and m the number of classes.
6

Under review as a conference paper at ICLR 2018

0.65 0.60 0.55 0.50 0.45 0.40 0.35 0.30
0

Eurlex
20 40

0.35 0.30 0.25 0.20 0.15 0.10
0
0.90
0.85
0.80
0.75
0.70 0

MNIST
20 40
AmazonCat
20 40

Bibtex
0.7 0.6 0.5 0.4 0.3 0.2 0.1

0.84 0.82 0.80 0.78 0.76

0 10 20 30 40 50

0

Wiki10

0.9

0.8

0.9850 0.9825

0.7 0.9800

0.6 0.9775

0.5 0.9750

0.4 0.9725 0.3 0.9700

0 10 20 30 40 50

0

Delicious
20 40
Wiki-small
20 40

Figure 2: Prediction error. The x-axis is the number of epochs and the y-axis is the fraction of points for which yi = argmaxk{xi wk} for the current value of W .

MNIST Bibtex

Delicious

1.75 1.50 4
1.25 3 1.00 0.75 2

6.0 5.5 5.0 4.5 4.0

0.50 1

3.5

0.25 3.0

0 20 40

0 10 20 30 40 50

0 10 20 30 40 50

Eurlex

AmazonCat

Wiki10

Wiki-small

6

40 12

700

5

30 10

600 500

4 8 400

3

20 6

300

2 1

10 4 2

200 100
0

0 10 20 30 40 50

0 10 20 30 40 50 0 10 20 30 40 50

0 10 20 30 40 50

Figure 3: Log-loss. The x-axis is the number of epochs and the y-axis is the log-loss from (2) calculated at the current value of W .

which are standard settings8. For our Implicit SGD method we chose to implement the version in Proposition 1 which has n = 1, m = 1. Likewise for U-max we set n = 1, m = 1 and the threshold parameter  = 1. The ridge regularization parameter µ was set to zero for all algorithms.
Epochs and losses. Each algorithm is run for 50 epochs on each dataset. The learning rate is decreased by a factor of 0.9 each epoch. Both the prediction error and log-loss (2) are recorded at the end of 10 evenly spaced epochs over the 50 epochs.
Learning rate. The magnitude of the gradient differs in each algorithm, due to them either under or over-estimating the log-sum derivative from (2). To set a reasonable learning rate for each algorithm, we ran them on the Eurlex dataset with learning rates  = 100,±1,±2,±3,±4 and saw which learning
8We also experimented setting n = 1, m = 1 in these methods and there was virtually no difference except the runtime was slower. For example in Appendix F we plot the performance of NCE for different learning rates with n = 1, m = 1 and n = 100, m = 5 and there is very little difference between the two.
7

Under review as a conference paper at ICLR 2018

Table 2: Relative prediction error and log-loss. The values for each dataset are normalized by dividing by the corresponding Implicit SGD error/log-loss.

DATASET

PREDICTION ERROR

LOG-LOSS

OVE NCE IS U-M. IMP. OVE NCE IS U-M. IMP.

MNIST 1.16 2.53 1.20 1.12 1.00 6.60 6.82 6.38 1.15 1.00 Bibtex 3.36 2.33 2.83 5.52 1.00 15.58 15.04 15.14 5.18 1.00 Delicious 1.05 1.11 1.04 1.01 1.00 1.95 1.96 1.95 1.26 1.00 Eurlex 1.59 1.36 1.32 1.39 1.00 6.47 6.39 6.38 2.11 1.00 AmazonCat 1.10 1.24 1.08 0.98 1.00 1.96 1.98 1.95 3.23 1.00 Wiki10 1.86 1.81 1.37 1.42 1.00 6.72 6.71 6.60 7.69 1.00 Wiki-small 1.02 1.01 1.02 1.00 1.00 1.04 1.05 1.04 25.70 1.00

Average 1.59 1.63 1.41 1.78 1.00 5.76 5.71 5.63 6.62 1.00

rate gave the best performance for each algorithm (the results are presented in Appendix F). This learning rate is used in all subsequent experiments.
Results. Plots of the performance of the algorithms on each dataset are displayed in Figures 2 and 3 with the relative performance compared to Implicit SGD given in Table 2. The Implicit SGD method has the best performance on virtually all datasets for both the prediction error and the logloss metrics. Not only does it converge faster in the first few epochs, it also converges to the true optimum value (unlike the biased methods that prematurely plateau). On average after 50 epochs, Implicit SGD has a prediction error 41% lower than the next best algorithm and its log-loss is a factor of 5.63 lower than the next best. The U-max algorithm has more varied performance, it was the only algorithm to outperform Implicit SGD's prediction error on one dataset but other times had the worst performance. On AmazonCat and Wiki-Small the U-max log-loss becomes extremely large. The reason for this can be explained by Theorem 1, which does not guarantee convergence for U-max if the learning rate is too high. It is likely that the learning rate of U-max was too high when applied to those datasets.
The results of running each method on the Eurlex dataset with learning rates  = 100,±1,±2,±3,±4 is presented in Appendix F. The results are consistent with those in Figures 2 and 3, with Implicit having the best performance for most learning rate settings. In Appendix F we also present the results of U-max run on the alternative double-sum formulation discussed in Section 2.3, and its performance is significantly worse than using our double-sum formulation.
5 CONCLUSION
In this paper we have presented the U-max and Implicit SGD algorithms for optimizing the softmax distribution. These are the first algorithms that require only O(D) computation per iteration (without extra work at the end of each epoch) that converge to the true softmax optimum. Implicit SGD can be efficiently implemented and clearly out-performs the previous state-of-the-art on seven different datasets. The result is a new method that enables optimizing the softmax for extremely large number of samples and classes.
So far Implicit SGD has only been applied to the simple softmax, but could also be applied to any neural network where the final layer is the softmax. Applying Implicit SGD to word2vec type models, which can be viewed as softmaxes where both x and w are parameters to be fit, might be particularly fruitful.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In HLT-NAACL, pp. 244­249, 2015.
Yoshua Bengio and Jean-Se´bastien Sene´cal. Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4):713­722, 2008.
Yoshua Bengio, Jean-Se´bastien Sene´cal, et al. Quick training of probabilistic neural nets by importance sampling. In AISTATS, 2003.
Dimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical programming, 129(2):163, 2011.
Robert M Corless, Gaston H Gonnet, David EG Hare, David J Jeffrey, and Donald E Knuth. On the Lambert W function. Advances in Computational mathematics, 5(1):329­359, 1996.
Hal Daume III, Nikos Karampatziakis, John Langford, and Paul Mineiro. Logarithmic time one-against-some. arXiv:1606.04988, 2016.
Alexandre de Bre´bisson and Pascal Vincent. An exploration of softmax alternatives belonging to the spherical loss family. arXiv:1511.05042, 2015.
Siddharth Gopal and Yiming Yang. Distributed training of large-scale logistic models. In ICML (2), pp. 289­ 297, 2013.
Edouard Grave, Armand Joulin, Moustapha Cisse´, David Grangier, and Herve´ Je´gou. Efficient softmax approximation for GPUs. arXiv:1609.04309, 2016.
Bikash Joshi, Massih-Reza Amini, Ioannis Partalas, Franck Iutzeler, and Yury Maximov. Aggressive sampling for multi-class to binary reduction with applications to text classification. arXiv:1701.06511, 2017.
Balaji Krishnapuram, Lawrence Carin, Mario AT Figueiredo, and Alexander J Hartemink. Sparse multinomial logistic regression: Fast algorithms and generalization bounds. IEEE transactions on pattern analysis and machine intelligence, 27(6):957­968, 2005.
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. arXiv:1212.2002, 2012.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv:1611.00712, 2016.
Andre´ FT Martins and Ramo´n Fernandez Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. CoRR, abs/1602.02068, 2016.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv:1206.6426, 2012.
Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick Galinari. LSHTC: A benchmark for large-scale text classification. arXiv:1503.08581, 2015.
Parameswaran Raman, Shin Matsushima, Xinhua Zhang, Hyokun Yun, and SVN Vishwanathan. DS-MLR: Exploiting double separability for scaling up distributed multinomial logistic regression. arXiv:1604.04706, 2016.
Roland T Rust and Anthony J Zahorik. Customer satisfaction, customer retention, and market share. Journal of retailing, 69(2):193­215, 1993.
Farhad Shahrokhi and David W Matula. The maximum concurrent flow problem. Journal of the ACM (JACM), 37(2):318­334, 1990.
Michalis K Titsias. One-vs-each approximation to softmax for scalable estimation of probabilities. arXiv:1609.07410, 2016.
Panos Toulis, Dustin Tran, and Edo Airoldi. Towards stability and optimality in stochastic gradient descent. In Artificial Intelligence and Statistics, pp. 1290­1298, 2016.
Pascal Vincent, Alexandre de Bre´bisson, and Xavier Bouthillier. Efficient exact gradient update for training deep networks with very large sparse targets. In Advances in Neural Information Processing Systems, pp. 1108­1116, 2015.
Mengdi Wang, Ji Liu, and Ethan Fang. Accelerating stochastic composition optimization. In Advances in Neural Information Processing Systems, pp. 1714­1722, 2016.
9

Under review as a conference paper at ICLR 2018

A PROOF OF VARIABLE BOUNDS AND STRONG CONVEXITY

We first establish that the optimal values of u and W are bounded. Next, we show that within these

bounds the objective is strongly convex and its gradients are bounded.

Lemma 1 ((Raman et al., 2016)). The optimal value of W is bounded as

W

2 2



BW2

where

BW2

=

2 µ

N

log(K ).

Proof.

-N log(K) = L(0)  L(W )  - µ 2

W

2 2

Rearranging gives the desired result.

Lemma 2. The optimal value of ui is bounded as ui  Bu where Bu = log(1 + (K - 1)e2BxBw ) and Bx = maxi{ xi 2}

Proof.

ui = log(1 +

exi )(wk-wyi )

k=yi

 log(1 +

e )xi 2( wk 2+ wyi 2)

k=yi

 log(1 +

e2BxBw )

k=yi
= log(1 + (K - 1)e2BxBw )

Lemma 3. If

W

2 2



BW2

and

ui



Bu

then

f (u, W )

is

strongly

convex

with

convexity

constant

greater than or equal to min{exp(-Bu), µ}.

Proof. Let us rewrite f as

N
f (u, W ) = ui + e-ui +

exi (wk-wyi )-ui + µ 2

W

2 2

i=1 k=yi

N
= ai  + e-ui +

ebik + µ 2

W

22.

i=1 k=yi

where  = (u , w1 , ..., wk )  RN+KD with ai and bik being appropriately defined. The Hessian of f is

N

2f () = e-ui eiei +

ebikbikbik + µ · diag{0N , 1KD}

i=1 k=yi

where ei is the ith canonical basis vector, 0N is an N -dimensional vector of zeros and 1KD is a KD-dimensional vector of ones. It follows that

2f () I · min{ min {e-ui }, µ}
0uBu
= I · min{exp(-Bu), µ}

0.

Lemma 4. If

W

2 2



BW2

and ui



Bu then the 2-norm of both the gradient of f

and each

stochastic gradient fik are bounded by

Bf = N max{1, eBu - 1} + 2(N eBu Bx + µ max{k}BW ).
k

10

Under review as a conference paper at ICLR 2018

Proof. By Jensen's inequality

max

f (u, W ) 2 =

max

Eikfik(u, W ) 2

W

2 2

BW2

,0uBu

W

2 2

BW2

,0uBu

 max Eik fik(u, W ) 2

W

2 2

BW2

,0uBu

 max max fik(u, W ) 2.

W

2 2

BW2

,0uBu

ik

Using the results from Lemmas 1 and 2 and the definition of fik from (6),

ui fik(u, W ) 2 = N 1 - e-ui - (K - 1)exi )(wk-wyi )-ui 2

= N |1 - e-ui (1 + (K - 1)exi (wk-wyi ))|

 N max{1, (1 + (K - 1)e xi 2( wk 2+ wyi 2)) - 1}

 N max{1, eBu - 1}

and for j indexing either the sampled class k = yi or the true label yi,

wj fik(u, W ) 2 = ± N (K - 1)exi x(wk-wyi )-ui i + µj wj 2

 N (K - 1)e xi 2( wk 2+ wyi 2) xi 2 + µj wj 2

Letting

 N eBu Bx + µ max{k}BW .
k

we have

Bf = N max{1, eBu - 1} + 2(N eBu Bx + µ max{k}BW )
k

fik(u, W ) 2  ui fik(u, W ) 2 + wk fik(u, W ) 2 + wyi fik(u, W ) 2 = Bf . In conclusion:

max f (u, W ) 2  max max fik(u, W ) 2  Bf .

W

2 2

BW2

,0uBu

W

2 2

BW2

,ui

Bu

,

ik

B STOCHASTIC COMPOSITION OPTIMIZATION

We can write the equation for L(W ) from (3) as (where we have set µ = 0 for notational simplicity),

N
L(W ) = - log(1 +

exi (wk-wyi ))

i=1 k=yi

= Ei[hi(Ek[gk(W )])] where i  unif ({1, ..., N }), k  unif ({1, ..., K}), hi(v)  R, gk(W )  RN and

hi(v) = -N log(1 + ei v)

[gk(W )]i =

K exi (wk-wyi ) 0

if k = yi . otherwise

Here ei v = vi  R is a variable that is explicitly kept track of with vi  Ek[gk(W )]i =
k=yi exi )(wk-wyi (with exact equality in the limit as t  ). Clearly vi in stochastic composition optimization has a similar role as ui has in our formulation for f in (5).

If i, k are sampled with k = yi in stochastic composition optimization then the updates are of the form (Wang et al., 2016)

exi (zk-zyi ) wyi = wyi + tN K 1 + vi xi

exi (zk-zyi ) wk = wk - tN K 1 + vi xi,

where zk is a smoothed value of wk. These updates have the same numerical instability issues as

vanilla

SGD

on

f

in

(5):

it

is

possible

that

exi zk 1+vi

1 where ideally we should have 0 

exi zk 1+vi

 1.

11

Under review as a conference paper at ICLR 2018

C U-MAX PSEUDOCODE

Algorithm 1: U-max

Input : Data D = {(yi, xi) : yi  {1, . . . , K}, xi  Rd}iN=1, number of classes K, number of

datapoints

N,

learning

rate

t,

class

sampling

probability

k

=

nk

+(N

N -nk

)(K

-1)

,

threshold parameter  > 0, bound BW on W such that W 2  BW and bound Bu on u

such that ui  Bu for i = 1, ..., N

Output: W

1 Initialize
2 for k = 1 to K do 3 wk  0 4 end 5 for i = 1 to N do 6 ui  log(K) 7 end

8 Run SGD 9 for t = 1 to T do 10 Sample indices 11 i  unif ({1, ..., N }) 12 k  unif ({1, ..., K} - {yi})
13 Increase ui 14 if ui < log(1 + exi )(wk-wyi ) -  then 15 ui  log(1 + exi )(wk-wyi )

16 SGD step
17 wk  wk - t[N (K - 1)exi x(wk-wyi )-ui i + µkwk] 18 wyi  wyi - t[-N (K - 1)exi x(wk-wyi )-ui i + µyi wyi ] 19 ui  ui - t[N (1 - e-ui - (K - 1)exi (wk-wyi )-ui )]

20 Projection
21 wk  wk · min{1, BW / wk 2} 22 wyi  wyi · min{1, BW / wyi 2} 23 ui  max{0, min{Bu, ui}}
24 end

D PROOF OF CONVERGENCE OF U-MAX METHOD

In this section we will prove the claim made in Theorem 1, that U-max converges to the softmax optimum. Before proving the theorem, we will need a lemma.
Lemma 5. For any  > 0, if ui  log(1+exi (wk-wyi ))- then setting ui = log(1+exi )(wk-wyi ) decreases f (u, W ) by at least 2/2.

Proof. As in Lemma 3, let  = (u , w1 , ..., wk )  RN+KD. Then setting ui = log(1 + exi )(wk-wyi ) is equivalent to setting  =  + ei where ei is the ith canonical basis vector and
 = log(1 + exi )(wk-wyi ) - ui  . By a second order Taylor series expansion

f () - f ( + ei)  f ( + ei)

ei

+

2 2 ei

2f (

+

ei)ei

(10)

for some   [0, 1]. Since the optimal value of ui for a given value of W is ui(W ) = log(1 + k=yi exi )(wk-wyi )  log(1+exi (wk-wyi )), we must have f (+ei) ei  0. From Lemma 3

12

Under review as a conference paper at ICLR 2018

we also know that

ei 2f ( + ei)ei = exp(-(ui + )) +

exi (wk-wyi )-(ui+)

k=yi

= exp(-)e-ui (1 +

exi )(wk-wyi )

k=yi

= exp(-) exp(-(log(1 + exi )(wk-wyi ) - ))(1 +

exi )(wk-wyi )

 exp( - )

k=yi

 exp( - )

= 1.

Putting in bounds for the gradient and Hessian terms in (10),

2 2

f () - f ( + ei) 

2



. 2

Now we are in a position to prove Theorem 1.

Proof of Theorem 1. Let (t) = (u(t), W (t))   denote the value of the tth iterate. Here  = { :

W

2 2



BW2 ,

ui



Bu}

is

a

convex

set

containing

the

optimal

value

of

f ().

Let i()() denote the operation of setting ui = log(1 + exi )(wk-wyi ) if ui  log(1 + exi )(wk-wyi ) - . If indices i, k are sampled for the stochastic gradient and ui  log(1 +
exi )(wk-wyi ) - , then the value of f at the t + 1st iterate is bounded as

f ((t+1)) = f (i((t)) - tfik(i((t))))

 f (i((t))) + max tfik(i()) 2 max f () 2





 f (i((t))) + tBf2

 f ((t)) - 2/2 + tBf2

 f ((t) - tfik((t))) - 2/2 + 2tBf2

 f ((t) - tfik((t))),

since t  2/(4Bf2) by assumption. Alternatively if ui  log(1 + exi )(wk-wyi ) -  then

f ((t+1)) = f (i((t)) - tfik(i((t))))

= f ((t) - tfik((t))).

Either way f ((t+1))  f ((t) - tfik((t))). Taking expectations with respect to i, k,
Eik[f ((t+1))]  Eik[f ((t) - tfik((t)))].
Finally let P denote the projection of  onto . Since  is a convex set containing the optimum we have f (P ())  f () for any , and so

Eik[f (P ((t+1)))]  Eik[f ((t) - tfik((t)))],

which shows that the rate of convergence in expectation of U-max is at least as fast as that of standard SGD.

E UPDATE EQUATIONS FOR IMPLICIT SGD
In this section we will derive the updates for Implicit SGD. We will first consider the simplest case where only one datapoint (xi, yi) and a single class is sampled in each iteration with no regularizer. Then we will derive the more complicated update for when there are multiple datapoints and sampled classes with a regularizer.
13

Under review as a conference paper at ICLR 2018

E.1 SINGLE DATAPOINT, SINGLE CLASS, NO REGULARIZER

Equation (6) for the stochastic gradient for a single datapoint and single class with µ = 0 is

fik(u, W ) = N (ui + e-ui + (K - 1)exi (wk-wyi )-ui ). The Implicit SGD update corresponds to finding the variables optimizing

min
u,W

2fik(u, W ) +

u - u~

2 2

+

W - W~

2 2

,

where  is the learning rate and the tilde refers to the value of the old iterate (Toulis et al., 2016, Eq.

6). Since fik is only a function of ui, wk, wyi the optimization reduces to

min
ui ,wk ,wyi

2fik(ui, wk, wyi ) + (ui - u~i)2 +

wyi - w~yi

2 2

+

wk - w~k

2 2

= min 2N (ui + e-ui + (K - 1)exi )(wk-wyi )-ui
ui ,wk ,wyi

+ (ui - u~i)2 +

wyi - w~yi

2 2

+

wk - w~k

2 2

.

The optimal value of wk, wyi must deviate from the old value w~k, w~yi in the direction of xi. Furthermore we can observe that the deviation of wk must be exactly opposite that of wyi , that is:

wyi

=

w~yi

+

a 2

xi xi

2 2

wk

=

w~k

-

a 2

xi xi

2 2

(11)

for some a  0. The optimization problem reduces to

min
ui ,a0

2N (ui + e-ui + (K - 1)exi (w~k-w~yi )e-a-ui ) + (ui - u~i)2 + a2 2

1 xi

2 2

.

(12)

We'll approach this optimization problem by first solving for a as a function of ui and then optimize

over ui. Once the optimal value of ui has been found, we can calculate the corresponding optimal

value of a. Finally, substituting a into (11) will give us our updated value of W .

We solve for a by setting its derivative equal to zero in (12)

0 = a

2N (ui + e-ui + (K - 1)exi (w~k-w~yi )e-a-ui ) + (ui - u~i)2 + a2 2

1 xi

2 2

= -2N (K - 1)exi (w~k-w~yi )-ui e-a + a

1 xi

2 2

 aea = 2N (K - 1)

xi

2 2

exi

.(w~k-w~yi )-ui

(13)

The solution for a can be written in terms of the principle branch of the Lambert W function P ,

a(ui) = P (2N (K - 1)

xi

2 2

exi

)(w~k-w~yi )-ui

= P (exi

(w~k-w~yi )-ui+log(2N (K-1)

xi

2 2

)

).

Substituting the solution to a(ui) into (12), we now only need minimize over ui:

min
ui

2N ui + 2N e-ui + 2N (K - 1)exi e(w~k-w~yi ) -a(ui)-ui + (ui - u~i)2 + a(ui)2 2

1 xi

2 2

= min
ui

2N ui + 2N e-ui + a(ui)

xi

-2 2

+

(ui

-

u~i)2

+

a(ui)2

2

1 xi

2 2

(14)

where we used the fact that e-P (z) = P (z)/z. The derivative with respect to ui in (14) is

ui

2N ui + 2N e-ui + a(ui)

xi

-2 2

+

(ui

-

u~i)2

+

a(ui)2

2

1 xi

2 2

= 2N - 2N e-ui + ui a(ui)

xi

-2 2

+

2(ui

-

u~i)

+

2a(ui)ui

a(ui)

2

1 xi

2 2

= 2N - 2N e-ui - a(ui) 1 + a(ui)

xi

-2 2

+ 2(ui

- u~i)

-

a(ui)2 (1 + a(ui))

xi

2 2

14

Under review as a conference paper at ICLR 2018

where

to

calculate

ui a(ui)

we

used

the

fact

that

zP (z)

=

P (z) z(1+P (z))

and

so

ui a(ui)

=

- exi

a(ui)
(w~k-w~yi )-ui+log(2N (K-1)

xi

2 2

)

(1

+

a(ui))

exi

(w~k-w~yi )-ui+log(2N (K-1)

xi

2 2

)

= - a(ui) . 1 + a(ui)

The second derivative of ui in (14) maybe be calculated similarly as

u2i

2N ui + 2N e-ui + a(ui)

xi

-2 2

+

(ui

-

u~i)2

+

a(ui)2

2

1 xi

2 2

=

2N e-ui

+

(1

a(ui) + a(ui))3

xi

-2 2

+

2

+

a(ui)2(2 + a(ui))

(1 + a(ui))3

xi

2 2

.

Now that we have access to the first and second derivatives of ui, we can numerically solve for ui using techniques such as Newton's method, which can give -accurate solution in runtime O(log( -1)). Since (12) is a strongly convex function, we are guaranteed to find a unique optimum
for u.

Two challenges in performing the numerical optimization of ui is finding an appropriate initial iterate ui(0) and calculating P for large arguments. Setting the initial iterate to be

u(i0) = max{u~i, xi

(w~k - w~yi ) + log(2N (K - 1)

xi

2 2

)}

(15)

ensures that a(u(i0))  P (1)  0.57 and the objective in (14) is reasonably small. If the argument to P is large, then we can employ an asymptotic formula, such as (Corless et al., 1996)

P (z)  log(z) - log(log(z)) + log(log(z)) . log(z)

(16)

In practice we solved for ui using Newton's method starting with the initial iterate in (15) and setting P (z) to be approximated by (16) if z > 15. We found this to be numerically stable under nearly all
conditions (except if  or xi was extremely large).

E.2 MULTIPLE DATAPOINTS, MULTIPLE CLASSES
The Implicit SGD update when there are multiple datapoints, multiple classes, with a regularizer is similar to the singe datapoint, singe class, no regularizer case described above. However, there are a few significant differences. Firstly, we will require some pre-computation to find a low-dimensional representation of the x values in each mini-batch. Secondly, we will integrate out ui for each datapoint (not wk). And thirdly, since the dimensionality of the simplified optimization problem is large, we'll require first order or quasi-Newton methods to find the optimal solution.

E.2.1 DEFINING THE MINI-BATCH
The first step is to define our mini-batches of size n. We will do this by partitioning the datapoint indices into sets S1, ..., SJ with Sj = {j : = 1, ..., n} for j = 1, ..., N/n , SJ = {J : = 1, ..., N mod n}, Si  Sj =  and jJ=1Sj = {1, ..., N }.
Next we define the set of classes Cj which can be sampled for the jth mini-batch. The set Cj is defined to be all sets of m distinct classes that are not equal to any of the labels y for points in the mini-batch, that is, Cj = {(k1, ..., km) : ki  {1, ..., K}, ki = k   {1, ..., m} - {i}, ki = y   Sj}.
Now we can write down our objective from (5) in terms of an expectation of functions corresponding to our mini-batches:
f (u, W ) = E[fj,C (u, W )]

15

Under review as a conference paper at ICLR 2018

where j is sampled with probability pj = |Sj|/N and C is sampled uniformly from Cj and



fj,C (u, W ) = p-j 1

ui + e-ui +

exi (wk-wyi )-ui + K - n m

exi (wk-wyi )-ui

iSj

kSj -{i}

kC

µ +
2

k wk 22.

kC Sj

The value of the regularizing constant k is such that E[I[k  C  Sj]k] = 1, which requires that

k-1

=

1

-

1 J

J j=1

I [k

=

Sj ](1

-

K

m - |Sj| ).

E.2.2 SIMPLIFYING THE IMPLICIT SGD UPDATE EQUATION

The Implicit SGD update corresponds to solving

min
u,W

2fj,C (u, W ) +

u - u~

2 2

+

W - W~

2 2

,

where  is the learning rate and the tilde refers to the value of the old iterate (Toulis et al., 2016,
Eq. 6). Since fj,C is only a function of uSj = {ui : i  Sj} and Wj,C = {wk : k  Sj  C} the optimization reduces to

min
uSj ,Wj,C

2fj,C (uSj , Wj,C ) +

uSj - u~Sj

2 2

+

Wj,C - W~ j,C

2 2

.

The next step is to analytically minimize the uSj terms. The optimization problem in (19) decomposes into a sum of separate optimization problems in ui for i  Sj,

min
ui

2pj-1(ui + e-ui di) + (ui - u~i)2

where

di(Wj,C ) = 1 +

exi (wk-wyi ) + K - n m

exi .(wk-wyi )

kSj -{i}

kC

Setting the derivative of ui equal to zero yields the solution ui(Wj,C ) = u~i - p-j 1 + P (pj-1di(Wj,C ) exp(p-j 1 - u~i))
where P is the principle branch of the Lambert W function. Substituting this solution into our optimization problem and simplifying yields

min
Wj,C

(1 + P (pj-1di(Wj,C ) exp(pj-1 - u~i)))2 +
iSj

Wj,C - W~ j,C

2 2

+

µ 2

k

kC Sj

wk

2 2

,

(17)

where we have used the identity e-P (z) = P (z)/z. We can decompose (17) into two parts by splitting Wj,C = Wj,C + Wj,C , its components parallel and perpendicular to the span of {xi :
i  Sj} respectively. Since the leading term in (17) only depends on Wj,C , the two resulting sub-problems are

min
Wj,C
min
Wj,C

(1 + P (pj-1di(Wj,C ) exp(pj-1 - u~i)))2 +

Wj,C - W~ j,C

2 2

+

µ 2

k

wk

2 2

,

iSj

kC Sj

Wj,C - W~ j,C

2 2

+

µ 2

k

wk

2 2

kC Sj

(18)

16

Under review as a conference paper at ICLR 2018

Let us focus on the perpendicular component first. Simple calculus yields the optimal value wk =

1 1+µk

/2

w~k

for

k



Sj



C.

Moving onto the parallel component, let the span of {xi : i  Sj} have an orthonormal basis9 Vj = (vj1, ..., vjn)  RD×n with xi = Vjbi for some bi  Rn. With this basis we can write
wk = w~k + Vjak for ak  Rn which reduces the parallel component optimization problem to10

min
Aj,C

(1 + P (zijC (Aj,C )))2 +

(1 + µk ) 2

ak

2 2

+

µk w~k

Vj ak

,

iSj

kSj C

(19)

where Aj,C = {ak : k  Sj  C}  R(n+m)×n and

zijC (Aj,C ) = pj-1 exp(p-j 1)

exp(-u~i) +

e exi (w~k-w~yi )-u~i bi (ak-ayi )

kSj -{i}

K -n +

e exi (w~k-w~yi )-u~i bi (ak-ayi )

m

kC

.

The ebi (ak-ayi ) factors come from

xi wk = xi (w~k + ak Vj ) = xi w~k + (Vj bi) Vj ak = xi w~k + bi Vj Vj ak = xi w~k + bi ak,
since Vj is an orthonormal basis.

E.2.3 OPTIMIZING THE IMPLICIT SGD UPDATE EQUATION

To optimize (19) we need to be able to take the derivative:



a  (1 + P (zijC (Aj,C )))2 +

(1 + µk ) 2

ak

2 2

+

µk w~k

Vj ak

iSj

kSj C

= 2(1 + P (zijC (Aj,C )))zijC (Aj,C )P (zijC (Aj,C ))a zijC (Aj,C )
iSj

+ (2 + µ )a + µ w~ Vj

=

iSj

2(1

+

P

(zijC

(Aj,C

)))

zijC

P (zijC (Aj,C )) (Aj,C )(1 + P (zijC (Aj,C

)))

a

zijC (Aj,C )

+ (2 + µ )a + µ w~ Vj

=

iSj

2

P

(zijC (Aj,C )) zijC (Aj,C )

a

zijC (Aj,C ) + (2 + µ

)a

+ µ w~

Vj

= 2e-P (zijC (Aj,C ))a zijC (Aj,C ) + (2 + µ )a + µ w~ Vj
iSj

9We have assumed here that dim(span({xi : i  Sj})) = n, which will be most often the case. If the dimension of the span is lower than n then let Vj be of dimension D × dim(span({xi : i  Sj})).
10Note that we have used w~k instead of w~k in writing the parallel component optimization problem. This does not make a difference as w~k always appears as an inner product with a vector in the span of {xi : i  Sj}.

17

Under review as a conference paper at ICLR 2018

where

we

used

that

zP (z)

=

P (z) z(1+P (z))

and

e-P (z)

=

P (z)/z.

To

complete

the

calculation

of

the

derivate we need,

a zijC (Aj,C ) = a pj-1 exp(pj-1) exp(-u~i) +

exi (w~ e-w~yi )-u~i bi (a -ayi )

kSj -{i}

K -n +

exi (w~ e-w~yi )-u~i bi (a -ayi )

m

kC

= pj-1 exp(p-j 1)bi

· I [  Sj - {i}]exi (w~ e-w~yi )-u~i bi (a -ayi )

+ I[



K C]

-

n exi

(w~

e-w~yi )-u~i bi

(a

-ayi )

m

- I[ = yi]

exi (w~ e-w~yi )-u~i bi (a -ayi )

kSj -{i}

K -n +

exi (w~ e-w~yi )-u~i bi (a -ayi )

m

kC

.

In order to calculate the full derivate with respect to Aj,C we need to calculate bi ak for all i  Sj and k  Sj  C. This is a total of n(n + m) inner products of n-dimensional vectors, costing O(n2(n + m)). To find the optimum of (19) we can use any optimization procedure that only uses
gradients, such as L-BFGS. Since (19) is strongly convex, standard first order methods can solve to accuracy in O( -1) iterations. Thus once we can calculate all of the terms in (19), we can solve it
to accuracy in runtime O(n2(n + m) -1).

Once we have solved for Aj,C , we can reconstruct the optimal solution for the parallel component

of wk as wk = w~k + Vjak. Recall that the solution to the perpendicular component is wk =

1 1+µk

/2

w~k

.

Thus

our

optimal

solution

is

wk

=

w~k

+

Vj ak

+

1 1+µk

/2

w~k

.

If the features xi are sparse, then we'd prefer to do a sparse update to w, saving computation time. We can achieve this by letting

wk = k · rk

where

k

is

a

scalar

and

rk

a

vector.

Updating

wk

=

w~k

+

Vj ak

+

1 1+µk

/2

w~k

is

equivalent

to

k

=

~k

·

1

+

1 µk /2

rk = r~k + µk/2 · r~k + ~k-1(1 + µk/2) · Vjak.

Since we only update rk along the span of {xi : i  Sj}, its update is sparse.

E.2.4 RUNTIME
There are two major tasks in calculating the terms in (19). The first is to calculate xi w~k for i  Sj and k  Sj  C. There are a total of n(n + m) inner products of D-dimensional vectors, costing O(n(n + m)D). The other task is to find the orthonormal basis Vj of {xi : i  Sj}, which can be achieved using the Gram-Schmidt process in O(n2D). We'll assume that {Vj : j = 1, ..., J} is computed only once as a pre-processing step when defining the mini-batches. It is exactly because calculating {Vj : j = 1, ..., J} is expensive that we have fixed mini-batches that do not change during the optimization routine.
Adding the cost of calculating the xi w~k inner products to the costing of optimizing (19) leads to the claim that solve the Implicit SGD update formula to accuracy in runtime O(n(n + m)D + n2(n + m) -1) = O(n(n + m)(D + n -1)).
If n and m are small then instead of optimizing the Implicit SGD updates using a first order method, we can calculate the Hessian and solve for Aj,C using second order methods. These methods, e.g.

18

Under review as a conference paper at ICLR 2018

Newton's method, give O( ) accuracy in O(log( -1)) iterations. In the special case of Proposition 1 where n = m = 1 the runtime of solving the Implicit SGD update to accuracy becomes O(D + log( -1)), where O(D) comes from calculating the xi wk and xi wyi inner products.

E.2.5 INITIALIZING THE IMPLICIT SGD OPTIMIZER

As was the case in Section E.1, it is important to initialize the optimization procedure at a point
where the gradient is relatively small and can be computed without numerical issues. These numerical issues arise when an exponent xi (w~k - w~yi ) - u~i + bi (ak - ayi ) 0. To ensure that this does not occur for our initial point, we can solve the following linear problem,11

R = min
Aj,C
s.t.

ak 1
kC Sj
xi (w~k - w~yi ) - u~i + bi (ak - ayi )  0

i  Sj, k  C  Sj

(20)

Note that if k = yi then the constraint 0  xi (w~k -w~yi )-u~i +bi (ak -ayi ) = -u~i is automatically
fulfilled since u~i  0. Also observed that setting ak = -Vj w~k satisfies all of the constraints, and so

R
kC Sj

Vj

w~k

1  (n + m) max
kC Sj

Vj w~k 1.

We can use the solution to (20) to gives us an upper bound on (19). Consider the optimal value
Aj(R,C) of the linear program in (20) with the value of the minimum being R. Since Aj(R,C) satisfies
the constrain in (20) we have zijC (A(jR,C))  Kp-j 1 exp(pj-1). Since P (z) is a monotonically
increasing function that is non-negative for z  0 we also have (1 + P (zijC (Aj(R,C))))2  (1 + P (Kpj-1 exp(p-j 1)))2. Turning to the norms, we can use the fact that a 2  a 1 for any vector a to bound

(1 + µk ) 2

ak

2 2

+

µk w~k

Vj ak

kSj C



(1 + µk ) 2

ak

2 1

+

µk

w~k Vj

1

ak

1

kSj C



1 + µ · max {k}/2
kSj C

kSj C

ak

2 1

+

µ

max {k
kSj C

w~k Vj

1}
kSj C

ak 1



1 + µ · max {k}/2
kSj C

R2 + µ max {k} max {

kSj C

kSj C

w~k Vj

1}R

2

 1 + µ · max {k}/2
kSj C

(n + m) max
kC Sj

Vj

w~k

1

+ µ max {k} max {

kSj C

kSj C

w~k Vj

1}

 (1 + µ · max {k})(n + m)2 max

kSj C

kC Sj

 (1 + µ · max {k})(n + m)2 max

kSj C

kC Sj

(n + m) max
kC Sj

Vj

w~k

2 1

w~k 12.

Vj w~k 1

Putting the bounds together we have that the optimal value of (19) is upper bounded by its value at the solution to (20), which in turn is upper bounded by

n(1

+

P

(K pj-1

exp(p-j 1)))2

+

(1

+

µ

·

max {k})(n
kSj C

+

m)2

max
kC Sj

w~k

21.

This bound is guarantees that our initial iterate will be numerically stable.

11Instead bounding the constraints on the right with 0, we could also have used any small positive number, like 5.

19

Under review as a conference paper at ICLR 2018

F LEARNING RATE PREDICTION AND LOSS

Here we present the results of using different learning rates for each algorithm applied to the Eurlex dataset. In addition to the Implicit, NCE, IS, OVE and U-max algorithms whose configuration is described in Section 4, we also provide results for NCE with n = 1, m = 1, denoted as NCE (1,1) and U-max run on the alternative double-sum formulation discussed in Section 2.3, denoted as Umax (2). NCE and NCE (1,1) have near identical performance, whereas U-max (2) is significantly worse than U-max.

0.60 0.55 0.50 0.45 0.40 0.35 0.30
0
IS

0.8 0.65
0.7 0.60 0.6 0.55 0.5 0.50 0.4 0.45

0 10 20 30 40 50

0

Implicit
20 40
OVE
20 40

1.0 0.9 0.8 0.7 0.6 0.5 0.4
0 10
0.70 0.65 0.60 0.55 0.50 0.45
0

NCE
20 30
U-max
20

40 50 40

NCE (1,1)
1.0 0.9 0.8 0.7 0.6 0.5 0.4
0 10 20 30 40
U-max (2)
0.90 0.85 0.80 0.75 0.70 0.65
0 20 40

50

Figure 4: Prediction error on Eurlex for different learning rates.

Implicit

NCE

4.0 6.7

3.5

3.0 6.6

2.5 6.5

2.0 6.4

1.5 6.3

1.0 6.2

NCE (1,1)
6.7 6.6 6.5 6.4 6.3 6.2

0 10 20 30 40 50 0 10 20 30 40 50

0 10 20 30 40 50

IS
6.7 6.6 6.5 6.4 6.3

OVE
6.7 6.6 6.5 6.4

U-max
35000 30000 25000 20000 15000 10000

80000 U-max (2)
60000 40000 20000

6.2 6.3 5000 0

0

0 10 20 30 40 50

0 10 20 30 40 50

0

20 40

0 20 40

Figure 5: Prediction loss on Eurlex different learning rates.

20

Under review as a conference paper at ICLR 2018

Table 3: Optimal learning rate for each algorithm for the Eurlex log-loss.

ALGORITHM LEARNING RATE

Implicit NCE NCE (TF) IS OVE U-max U-max (2)

1.0 100 100 100 0.1 0.1 0.01

21

