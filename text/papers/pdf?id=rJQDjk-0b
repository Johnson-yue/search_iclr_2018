Under review as a conference paper at ICLR 2018
UNBIASED ONLINE RECURRENT OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as Truncated Backpropagation Through Time (truncated BPTT), a widespread algorithm for online learning of recurrent networks Jaeger (2002). UORO is a modification of NoBackTrack Ollivier et al. (2015) that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence. On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.
Current recurrent network learning algorithms are ill-suited to online learning via a single pass through long sequences of temporal data. Backpropagation Through Time (BPTT Jaeger (2002)), the current standard for training recurrent architectures, is well suited to many short training sequences. Treating long sequences with BPTT requires either storing all past inputs in memory and waiting for a long time between each learning step, or arbitrarily splitting the input sequence into smaller sequences, and applying BPTT to each of those short sequences, at the cost of losing long term dependencies.
This paper introduces Unbiased Online Recurrent Optimization (UORO), an online and memoryless learning algorithm for recurrent architectures: UORO processes and learns from data samples sequentially, one sample at a time. Contrary to BPTT, UORO does not maintain a history of previous inputs and activations. Moreover, UORO is scalable: processing data samples with UORO comes at a similar computational and memory cost as just running the recurrent model on those data.
Like most neural network training algorithms, UORO relies on stochastic gradient optimization. The theory of stochastic gradient crucially relies on the unbiasedness of gradient estimates to provide convergence to a local optimum. To this end, in the footsteps of NoBackTrack (NBT) Ollivier et al. (2015), UORO provides provably unbiased gradient estimates, in a scalable, streaming fashion.
Unlike NBT, though, UORO can be easily implemented in a black-box fashion on top of an existing recurrent model in current machine learning software, without delving into the structure and code of the model.
The framework for recurrent optimization and UORO is introduced in Section 2. The final algorithm is reasonably simple (Alg. 1), but its derivation (Section 3) is more complex. In Section 4, UORO is shown to provide convergence on a set of synthetic experiments where truncated BPTT fails to display reliable convergence. An implementation of UORO is provided as supplementary material.
1 RELATED WORK
A widespread approach to online learning of recurrent neural networks is Truncated Backpropagation Through Time (truncated BPTT) Jaeger (2002), which mimics Backpropagation Through Time,
1

Under review as a conference paper at ICLR 2018

but zeroes gradient flows after a fixed number of timesteps. This truncation makes gradient estimates biased; consequently, truncated BPTT does not provide any convergence guarantee. Learning is biased towards short-time dependencies. 1. Storage of some past inputs and states is required.
Online, exact gradient computation methods have long been known (Real Time Recurrent Learning (RTRL) Williams & Zipser (1989); Pearlmutter (1995)), but their computational cost discards them for reasonably-sized networks.
NoBackTrack (NBT) Ollivier et al. (2015) also provides unbiased gradient estimates for recurrent neural networks. However, contrary to UORO, NBT cannot be applied in a blackbox fashion, making it extremely tedious to implement for complex architectures.
Other previous attempts to introduce generic online learning algorithms with a reasonable computational cost all result in biased gradient estimates. Echo State Networks (ESNs) Jaeger (2002); Jaeger et al. (2007) simply set to 0 the gradients of recurrent parameters. Others, e.g., Maass et al. (2002); Steil (2004), introduce approaches resembling ESNs, but keep a partial estimate of the recurrent gradients. The original Long Short Term Memory algorithm Hochreiter & Schmidhuber (1997) (LSTM now refers to a particular architecture) cuts gradient flows going out of gating units to make gradient computation tractable. Decoupled Neural Interfaces Jaderberg et al. (2016) bootstrap truncated gradient estimates using synthetic gradients generated by feedforward neural networks. The algorithm in Movellan et al. (2002) provides zeroth-order estimates of recurrent gradients via diffusion networks; it could arguably be turned online by running randomized alternative trajectories. Generally these approaches lack a strong theoretical backing, except arguably ESNs.

2 BACKGROUND

UORO is a learning algorithm for recurrent computational graphs. Formally, the aim is to optimize , a parameter controlling the evolution of a dynamical system

st+1 = Fstate(xt+1, st, ) ot+1 = Fout(xt+1, st, )

(1) (2)

in order to minimize a total loss L :=

t(ot, ot ), where ot is a target output at time t.

0tT

For instance, a standard recurrent neural network, with hidden state st (preactivation values) and

output ot at time t, is described with the update equations Fstate(xt+1, st, ) := Wx xt+1 +

Ws tanh(st) + b and Fout(xt+1, st, ) := Wo tanh(Fstate(xt+1, st, )) + bo; here the parameter is  = (Wx, Ws, b, Wo, bo), and a typical loss might be s(os, os) := (os - os)2.

Optimization by gradient descent is standard for neural networks. In the spirit of stochastic gradient

descent, we can optimize the total loss L =

t(ot, ot) one term at a time and update the

0tT

parameter online at each time step via







-

t

t 

(3)

where t is a scalar learning rate at time t. (Other gradient-based optimizers can also be used, once

t 

is

known.)

The

focus

is

then

to

compute,

or

approximate,

 

t


.

BPTT

computes

t 

by

unfolding

the

network

through

time,

and

backpropagating

through

the

un-

folded network, each timestep corresponding to a layer. BPTT thus requires maintaining the full

unfolded network, or, equivalently, the history of past inputs and activations. 2 Truncated BPTT

only unfolds the network for a fixed number of timesteps, reducing computational cost in online

settings Jaeger (2002). This comes at the cost of biased gradients, and can prevent convergence of

the gradient descent even for large truncations, as clearly exemplified in Fig. 1a.

1 Arguably, truncated BPTT might still learn some dependencies beyond its truncation range, by a mechanism similar to Echo State Networks Jaeger (2002). However, truncated BPTT's gradient estimate has a marked bias towards short-term rather than long-term dependencies, as shown in the first experiment of Section 4.
2Storage of past activations can be reduced, e.g. Gruslys et al. (2016). However, storage of all past inputs is necessary.

2

Under review as a conference paper at ICLR 2018

3 UNBIASED ONLINE RECURRENT OPTIMIZATION

Unbiased Online Recurrent Optimization is built on top of a forward computation of the gradients, rather than backpropagation. Forward gradient computation for neural networks (RTRL) is described in Williams & Zipser (1989) and we review it in Section 3.1. The derivation of UORO follows in Section 3.2. Implementation details are given in Section 3.3. UORO's derivation is strongly connected to Ollivier et al. (2015) but differs in one critical aspect: the sparsity hypothesis made in the latter is relieved, resulting in reduced implementation complexity without any model restriction. The proof of UORO's convergence to a local optimum is soon to be published Masse & Ollivier (To appear).

3.1 FORWARD COMPUTATION OF THE GRADIENT

Forward computation of the gradient for a recurrent model (RTRL) is directly obtained by applying the chain rule to both the loss function and the state equation (1), as follows.

Direct differentiation and application of the chain rule to t+1 yields

 t+1 

=



t+1
o

(ot+1,

ot+1

)

·



Fout s

(xt+1,

st,

)

st 

+

Fout 

(xt+1,

st,

)

.

(4)

Here, the term st/ represents the effect on the state at time t of a change of parameter during the whole past trajectory. This term can be computed inductively from time t to t + 1. Intuitively, looking at the update equation (1), there are two contributions to st+1/:
· The direct effect of a change of  on the computation of st+1, given st. · The past effect of  on st via the whole past trajectory.

With this in mind, differentiating (1) with respect to  yields

st+1 

=



Fstate 

(xt+1

,

st

,

)

+



Fstate s

(xt+1,

st,

)

st 

.

(5)

This gives a way to compute the derivative of the instantaneous loss without storing past history: at each time step, update st/ from st-1/, then use this quantity to directly compute  t+1/. This is how RTRL Williams & Zipser (1989) proceeds.
A huge disadvantage of RTRL is that st/ is of size dim(state) × dim(params). For instance, with a fully connected standard recurrent network with n units, st/ scales as n3. This makes RTRL impractical for reasonably sized networks.
UORO modifies RTRL by only maintaining a scalable, rank-one, provably unbiased approximation of st/, to reduce the memory and computational cost. This approximation takes the form s~t ~t, where s~t is a column vector of the same dimension as st, ~t is a row vector of the same dimension as  , and  denotes the outer product. The resulting quantity is thus a matrix of the same size as st/. The memory cost of storing s~t and ~t scales as dim(state) + dim(params). Thus UORO is as memory costly as simply running the network itself (which indeed requires to store the current state and parameters). The following section details how s~t and ~t are built to provide unbiasedness.

3.2 RANK-ONE TRICK: FROM RTRL TO UORO

Given an unbiased estimation of st/, namely, a stochastic matrix G~t such that E G~t = st/, unbiased estimates of  t+1/ and st+1/ can be derived by plugging G~t in (4) and (5). Unbiasedness is preserved thanks to linearity of the mean, because both (4) and (5) are affine in st/.

Thus, assuming the existence of a rank-one unbiased approximation G~t = s~t  ~t at time t, we can plug it in (5) to obtain an unbiased approximation G^t+1 at time t + 1

G^ t+1

=



Fstate 

(xt+1,

st,

)

+

 Fstate s

(xt+1,

st,

)

s~t



~t.

(6)

3

Under review as a conference paper at ICLR 2018

However, in general this is no longer rank-one.

To transform G^t+1 into G~t+1, a rank-one unbiased approximation, the following rank-one trick, introduced in Ollivier et al. (2015) is used:

Proposition 1. Let A be a real matrix that decomposes as

k

A = vi  wi.

(7)

i=1
Let  be a vector of k independent random signs, and  a vector of k positive numbers. Consider

the rank-one matrix

A~ :=

k
iivi
i=1



k iwi i=1 i

Then A~ is an unbiased rank-one approximation of A: EA~ = A.

(8)

The rank-one trick can be applied for any . The choice of  influences the variance of the approxi-

mation; choosing

i = wi / vi

(9)

minimizes the variance of the approximation, E

A - A~

2 2

Ollivier et al. (2015).

The

UORO

update

is

obtained

by

applying

the

rank-one

trick

twice

to

(6).

First,



Fstate 

(xt+1

,

st

,



)

is reduced to a rank one matrix, without variance minimization. 3 Namely, let  be a vector of

independant random signs; then,

 Fstate 

(xt+1,

st,

)

=

E



 Fstate 

(xt+1

,

st

,

)

.

(10)

This results in a rank-two, unbiased estimate of st+1/ by substituting (10) into (6)

 Fstate s

(xt+1,

st,

)

s~t



~t

+









Fstate 

(xt+1,

st,

)

.

(11)

Applying Prop. 1 again to this rank-two estimate, with variance minimization, yields UORO's esti-

mate G~t+1

G~t+1 =

0

 Fstate s

(xt+1,

st,

)

s~t

+

1





~t 0

+

 1

 Fstate 

(xt+1,

st,

)

(12)

which satisfies that E G~t+1 is equal to (6). (By elementary algebra, some random signs that should appear in (12) cancel out.) Here

0 =

~t

 Fstate s

(xt+1

,

st

,

)

s~t

,

minimizes variance of the second reduction.

1 =





Fstate 

(xt+1

,

st

,

)



(13)

The unbiased estimation (12) is rank-one and can be rewritten as G~t+1 = s~t+1  ~t+1 with the update

s~t+1



0

 Fstate s

(xt+1,

st,

)

s~t

+

1



(14)

~t+1 

~t 0

+

 1



Fstate 

(xt+1

,

st,

).

(15)

Initially, s0/ = 0, thus s~0 = 0, ~0 = 0 yield an unbiased estimate at time 0. Using this initial estimate and the update rules (14)­(15), an estimate of st/ is obtained at all subsequent times, allowing for online estimation of  t/. Thanks to the construction above, by induction all these estimates are unbiased. 4

We are left to demonstrate that these update rules are scalably implementable.

3 Variance minimization is not used at this step, since computing

wi vi

for every i is not scalable.

4 In practice, since  changes during learning, unbiasedness only holds exactly in the limit of small learning

rates. This is not specific to UORO as it also affects RTRL.

4

Under review as a conference paper at ICLR 2018

3.3 IMPLEMENTATION

Implementing UORO requires maintaining the rank-one approximation and the corresponding gradient loss estimate. UORO's estimate of the loss gradient  t+1/ at time t + 1 is expressed by plugging into (4) the rank-one approximation st/  s~t  ~t, which results in

 t+1 o

(ot+1,

ot+1)

Fout s

(xt+1,

st,

)

·

s~t

~t

+



t+1
o

(ot+1

,

ot+1)

Fout 

(xt+1,

st,

).

(16)

Backpropagating  t+1/ot+1 once through Fout returns ( t+1/ot+1 · Fout/xt+1,  t+1/ot+1 · Fout/st,  t+1/ot+1 · Fout/), thus providing all necessary terms to compute (16).
Updating s~ and ~ requires applying (14)­(15) at each step. Backpropagating the vector of random signs  once through Fstate returns _, _,  Fstate(xt+1, st, )/ , providing for (15).
Updating s~ via (14) requires computing (Fstate/st) · s~t. This is computable numerically through

 Fstate s

(xt+1,

st,

)

·

s~t

=

lim
0

Fstate(xt+1,

st

+



s~t, ) 

-

Fstate(xt+1, st, )

(17)

computable through two applications of Fstate. This operation is referred to as tangent forward propagation Simard et al. (1991) and can also often be computed algebraically.
This allows for complete implementation of one step of UORO (Alg. 1). The cost of UORO (including running the model itself) is three applications of Fstate, one application of Fout, one backpropagation through Fout and Fstate, and a few elementwise operations on vectors and scalar products.
The resulting algorithm is detailed in Alg. 1. F.forward(v) denotes pointwise application of F at point v, F.backprop(v, o) backpropagation of row vector o through F at point v, and F.forwarddiff(v, v) tangent forward propagation of column vector v through F at point v. Notably, F.backprop(v, o) has the same dimension as v , e.g. Fout.backprop((xt+1, st, ), ot+1) has three components, of the same dimensions as xt+1, st and  .
The proposed update rule for stochastic gradient descent (3) can be directly adapted to other optimizers, e.g. Adaptative Momentum (Adam) Kingma & Ba (2014) or Adaptative Gradient Duchi et al. (2010). Vanilla stochastic gradient descent (SGD) and Adam are used hereafter. In Alg. 1, such optimizers are denoted by SGDOpt and the corresponding parameter update given current parameter , gradient estimate gt and learning rate t is denoted SGDOpt.update(gt, t, ).
3.4 MEMORY-T UORO AND RANK-k UORO

The unbiased gradient estimates of UORO injects noise via , thus requiring smaller learning rates. To reduce noise, UORO can be used on top of truncated BPTT so that recent gradients are computed exactly.

Formally, this just requires applying Algorithm 1 to a new transition function F T which is just T

consecutive steps of the original model F . Then the backpropagation operation in Algorithm 1

becomes a backpropagation over the last T steps, as in truncated BPTT. The loss of one step of F T

t+T

is the sum of the losses of the last T steps of F , namely

t+T t+1

:=

k. Likewise, the forward

k=t+1

tangent propagation is performed through F T . This way, we obtain an unbiased gradient estimate

in which the gradients from the last T steps are computed exactly and incur no noise. The resulting

algorithm is referred to as memory-T UORO. Its scaling in T is similar to T -truncated BPTT, both in

terms of memory and computation. In the experiments below, memory-T UORO reduced variance

early on, but did not significantly impact later performance.

The noise in UORO can also be reduced by using higher-rank gradient estimates (rank-r instead of rank-1), which amounts to maintaining r distinct values of s~ and ~ in Algorithm 1 and averaging the resulting values of g~. We did not exploit this possibility in the experiments below, although r = 2
visibly reduced variance in preliminary tests.

5

Under review as a conference paper at ICLR 2018

Algorithm 1 -- One step of UORO (from time t to t + 1)

Inputs:

­ xt+1, ot+1, st and : input, target, previous recurrent state, and parameters ­ s~t column vector of size state, ~t row vector of size params such that E s~t~t = st/ ­ SGDOpt and t+1: stochastic optimizer and its learning rate

Outputs:

­ t+1, st+1 and : loss, new recurrent state, and updated parameters ­ s~t+1 and ~t+1 such that E s~t+1  ~t+1 = st+1/ ­ g~t+1 such that E g~t+1 =  t+1/

/* compute next state and loss */ st+1  Fstate.forward(xt+1, st, ), t+1  (ot+1, ot+1)

ot+1  Fout.forward(xt+1, st, )

/* compute gradient estimate */

(_, s, )  Fout.backprop

(xt+1,

st,

),

 t+1 ot+1

g~t+1  (s · s~t) ~t + 

/* prepare for reduction */
Draw , column vector of random signs ±1 of size state s~t+1  Fstate.forwarddiff((xt+1, st, ), (0, s~t, 0)) (_, _, g)  Fstate.backprop((xt+1, st, ),  )

/* compute normalizers */

0 

~t s~t+1

+



+,

1 

g +



+



with



=

10-7

/* reduce */ s~t+1  0 s~t+1 + 1 , /* update  */

~t+1



~t 0

+

g 1

SGDOpt.update(g~t+1, t+1, )

4 EXPERIMENTS ILLUSTRATING TRUNCATION BIAS

The set of experiments below aims at displaying specific cases where the biases from truncated BPTT are likely to prevent convergence of learning. On this test set, UORO's unbiasedness provides steady convergence, highlighting the importance of unbiased estimates for general recurrent learning.

Influence balancing. The first test case exemplifies learning of a scalar parameter  which has a positive influence in the short term, but a negative one in the long run. Short-sightedness of truncated algorithms results in abrupt failure, with the parameter exploding in the wrong direction, even with truncation lengths exceeding the temporal dependency range by a factor of 10 or so.
Consider the linear dynamics

st+1 = A st + (, . . . , , -, . . . , -)

(18)

with A a square matrix of size n with Ai,i = 1/2, Ai,i+1 = 1/2, and 0 elsewhere;   R is a scalar parameter. The second term has p positive- entries and n - p negative- entries. Intuitively,
the effect of  on a unit diffuses to shallower units over time (Fig. 2a). Unit i only feels the effect
of  from unit i + n after n time steps, so the intrinsic time scale of the system is  n. The loss considered is a target on the shallowest unit s1,

t

=

1 2

(s1t

- 1)2.

(19)

6

Under review as a conference paper at ICLR 2018

Loss Average log-loss (bits per character)

1 × 106 1 × 104 1 × 102 1 × 100 1 × 10-2 1 × 10-4 1 × 10-6 1 × 10-8 1 × 10-10 1 × 10-12
0

UORO
1-truncated BPTT 10-truncated BPTT 100-truncated BPTT 200-truncated BPTT

20000

40000

60000

Epoch

(a)

80000

100000

2.6 2.5 2.4 2.3 2.2 2.1
2 1.9 1.8 1.7
10000

UORO, GRU 4-truncated BPTT, GRU 4-truncated BPTT, LSTM
UORO, LSTM
True entropy rate
Entropy rate no memory

100000

Epoch

(b)

1 × 106

1 × 107

Figure 1: (a)Results for influence balancing with 23 units and 13 minus; note the vertical log scale. (b)Learning curves on distant brackets (1, 5, 5, 10).

lt+1

 - - -

1111111 2222222

time

lt

 - - -
(a) Influence balancing, 4 units, 3 minus.

[a]eecbe[a] [j]fbfjd[j] [c]bgddc[c] [d]gjhai[d] [e]iaghb[e] [h]bigaj[h]
(b) Distant brackets (1, 5, 10).

Figure 2: Datasets.

aaaaaa bbbbbb aaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbb aaaaaaaa bbbbbbbb
(c) anbn(1, 32).

Learning is performed online with vanilla SGD, using gradient estimates either from UORO or T -

truncated BPTT with various T . .

Learning rates are of the form t

=

 1+ t

for suitable values of

As shown in Fig. 1a, UORO solves the problem while T -truncated BPTT fails to converge for any learning rate, even for truncations T largely above n. Failure is caused by ill balancing of time dependencies: the influence of  on the loss is estimated with the wrong sign due to truncation. For n = 23 units, with 13 minus signs, truncated BPTT requires a truncation T  200 to converge.

Next-character prediction. The next experiment is character-level synthetic text prediction: the goal is to train a recurrent model to predict the t + 1-th character of a text given the first t online, with a single pass on the data sequence.

A single layer of 64 units, either GRU or LSTM, is used to output a probability vector for the

next character. The cross entropy criterion is used to compute the loss.At each time t we plot the

cumulated

loss

per

character

on

the

first

t

characters,

1 t

t s=1

s. (Losses for individual characters

are quite noisy, as not all characters in the sequence are equally difficult to predict.) This would be

the compression rate in bits per character if the models were used as online compression algorithms

on the first t characters. In addition, in Table 1 we report a "recent" loss on the last 100, 000

characters, which is more representative of the model at the end of learning.

Optimization was performed using Adam with the default setting 1 = 0.9 and 2 = 0.999, and a

decreasing

learning rate

t

=

 1+ t

,

with

t

the number of

characters processed.

As

convergence

of

UORO requires smaller learning rates than truncated BPTT, this favors UORO. Indeed UORO can

fail to converge with non-decreasing learning rates, due to its stochastic nature.

DISTANT BRACKETS DATASET (s, k, a). The distant brackets dataset is generated by repeatedly outputting a left bracket, generating s random characters from an alphabet of size a, outputting a right bracket, generating k random characters from the same alphabet, repeating the same first s characters between brackets and finally outputting a line break. A sample is shown in Fig. 2b.
UORO is compared to 4-truncated BPTT. Truncation is deliberately shorter than the inherent time range of the data, to illustrate how bias can penalize learning if the inherent time range is unknown a priori. The results are given in Fig. 1b (with learning rates using  = 0.015 and  = 10-3). UORO

7

Under review as a conference paper at ICLR 2018

Average log-loss (bits per character) Average log-loss (bits per character)

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2
100

UORO, GRU UORO, LSTM 16-truncated BPTT, GRU 16-truncated BPTT, LSTM True entropy rate Entropy rate no memory

1 0.9 0.8 0.7

UORO, GRU Memory-2 UORO, GRU Memory-16 UORO, GRU 1-truncated BPTT, GRU 2-truncated BPTT, GRU 16-truncated BPTT, GRU
True entropy rate Entropy rate no memory

0.6

0.5

0.4

0.3

0.2

1000

10000

100000

Epoch

1 × 106

1 × 107

100

1000

10000

100000

1 × 106

1 × 107

Epoch

Figure 3: Learning curves on anbn(1,32)

beats 4-truncated BPTT in the long run, and succeeds in reaching near optimal behaviour both with GRUs and LSTMs. Truncated BPTT remains stuck near a memoryless optimum with LSTMs; with GRUs it keeps learning, but at a slow rate. Still, truncated BPTT displays faster early convergence.

anbn(k, l) DATASET. The anbn(k, l) dataset tests memory and counting Gers & Schmidhuber (2001); it is generated by repeatedly picking a random number n between k and l, outputting a string of n a's, a line break, n b's, and a line break (see Fig. 2c). The difficulty lies in matching the number of a's and b's.
Table 1: Averaged loss on the 105 last iterations on anbn(1, 32).

Truncation

LSTM GRU

No memory (default) 0.147 0.155

UORO

Memory-2

0.149 0.174

Memory-16

0.154 0.149

1 0.178 0.231

Truncated BPTT 2

0.149 0.285

16 0.144 0.207

Plots for a few setups are given in Fig. 3. The learning rates used  = 0.03 and  = 10-3. Numerical results at the end of training are given in Table 1. For reference, the true entropy rate is 0.14 bpc, while the entropy rate of a model that does not understand that the numbers of a's and b's coincide is double, 0.28 bpc.
Here, in every setup, UORO reliably converges and reaches near optimal performance. Increasing UORO's range does not significantly improve results: providing an unbiased estimate is enough to provide reliable convergence in this case. Meanwhile, truncated BPTT performs inconsistently. Notably, with GRUs, it either converges to a poor local optimum corresponding to no understanding of the temporal structure, or exhibits gradient reascent in the long run. Remarkably, with LSTMs rather than GRUs, 16-truncated BPTT reliably reaches optimal behavior on this problem even with biased gradient estimates.

CONCLUSION
We introduced UORO, an algorithm for training recurrent neural networks in a streaming, memoryless fashion. UORO is easy to implement, and requires as little computation time as truncated BPTT, at the cost of noise injection. Importantly, contrary to most other approaches, UORO scalably provides unbiasedness of gradient estimates. Unbiasedness is of paramount importance in the current theory of stochastic gradient descent. Furthermore, UORO is experimentally shown to benefit from its unbiasedness, converging even in cases where truncated BPTT fails to reliably achieve good results or diverges pathologically.

8

Under review as a conference paper at ICLR 2018
REFERENCES
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Technical Report UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010. URL http://www2.eecs.berkeley.edu/Pubs/ TechRpts/2010/EECS-2010-24.html.
Felix A Gers and Jürgen Schmidhuber. Long short-term memory learns context free and context sensitive languages. In Artificial Neural Nets and Genetic Algorithms, pp. 134­137. Springer, 2001.
Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/ abs/1606.03401.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735­ 1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx. doi.org/10.1162/neco.1997.9.8.1735.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. CoRR, abs/1608.05343, 2016. URL http://arxiv.org/abs/1608.05343.
Herbert Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the "echo state network" approach, 2002.
Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo Siewert. Optimization and Applications of Echo State Networks with Leaky-Integrator Neurons. Neural Networks, 20(3): 335­352, April 2007. ISSN 08936080. doi: 10.1016/j.neunet.2007.04.016. URL http: //www.sciencedirect.com/science/article/pii/S089360800700041X.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Wolfgang Maass, Thomas Natschläger, and Henry Markram. Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Comput., 14 (11):2531­2560, November 2002. ISSN 0899-7667. doi: 10.1162/089976602760407955. URL http://dx.doi.org/10.1162/089976602760407955.
Pierre-Yves Masse and Yann Ollivier. PhD thesis, To appear.
Javier R. Movellan, Paul Mineiro, and R. J. Williams. A Monte Carlo EM approach for partially observable diffusion processes: Theory and applications to neural networks. Neural Comput., 14 (7):1507­1544, July 2002. ISSN 0899-7667. doi: 10.1162/08997660260028593. URL http: //dx.doi.org/10.1162/08997660260028593.
Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without backtracking. CoRR, abs/1507.07680, 2015. URL http://arxiv.org/abs/1507. 07680.
Barak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey. IEEE Transactions on Neural networks, 6(5):1212­1228, 1995.
Patrice Y. Simard, Bernard Victorri, Yann LeCun, and John S. Denker. Tangent prop - a formalism for specifying selected invariances in an adaptive network. In John E. Moody, Stephen Jose Hanson, and Richard Lippmann (eds.), NIPS, pp. 895­903. Morgan Kaufmann, 1991. ISBN 1-55860-222-4. URL http://dblp.uni-trier.de/db/conf/nips/ nips1991.html#SimardVLD91.
Jochen J. Steil. Backpropagation-decorrelation: online recurrent learning with O(N) complexity. In Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, volume 2, pp. 843­848 vol.2. IEEE, July 2004. ISBN 0-7803-8359-1. doi: 10.1109/ijcnn.2004.1380039. URL http://dx.doi.org/10.1109/ijcnn.2004.1380039.
9

Under review as a conference paper at ICLR 2018 Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural Comput., 1(2):270­280, June 1989. ISSN 0899-7667. doi: 10.1162/ neco.1989.1.2.270. URL http://dx.doi.org/10.1162/neco.1989.1.2.270.
10

