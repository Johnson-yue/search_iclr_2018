Under review as a conference paper at ICLR 2018
INITIALIZATION MATTERS: ORTHOGONAL PREDICTIVE STATE RECURRENT NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al., 2017) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al., 2007) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs)(Yu et al., 2016) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation. Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs. In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.
1 INTRODUCTION
Learning to predict temporal sequences of observations is a fundamental challenge in a range of disciplines including machine learning, robotics, and natural language processing. There exist a wide variety of approaches to modeling time series data, however recurrent neural networks (RNNs) have emerged as the clear frontrunner, achieving state-of-the-art performance in applications such as speech recognition (Heigold et al., 2016), language modeling (Mikolov et al., 2010), translation (Cho et al., 2014b), and image captioning (Xu et al., 2015).
Predictive State Recurrent Neural Networks (PSRNNs) are a state-of-the-art RNN architecture recently introduced by Downey et al. (2017) that combine the strengths of probabilistic models and RNNs in a single model. Specifically PSRNNs offer strong statistical theory, globally consistent model initializations, and a rich functional form which is none-the-less amenable to refinement via backpropogation through time (BPTT). Despite consisting of a simple bi-linear operations, PSRNNs have been shown to significantly outperform more complex RNN architectures (Downey et al., 2017), such as the widely used LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014a).
PSRNNs leverage the concept of Hilbert Space embeddings of distributions (Smola et al., 2007) to embed predictive states into a Reproducing Kernel Hilbert Space (RKHS), then estimate, predict, and update these embedded states using Kernel Bayes Rule (KBR) (Smola et al., 2007). Because PSRNNs directly manipulate (kernel embeddings of) distributions over observations, they can be
1

Under review as a conference paper at ICLR 2018
initialized via a globally consistent method-of-moments algorithm which reduces to a series of linear ridge regressions.
Practical implementations of PSRNNs are made possible by the machinery of Random Features (RFs): input features are mapped into a new space where dot products approximate the kernel well (Rahimi & Recht, 2008). RFs are crucial to the success of PSRNNs, however PSRNNs often require a significant number of RFs in order to obtain good results. And, unfortunately, the number of required RFs grows with the dimensionality of the input, resulting in models which can be large, slow to execute, and slow to train.
One technique that has proven to be effective for reducing the required number of RFs for kernel machines is Orthogonal Random Features (ORFs) (Yu et al., 2016). When using ORFs, the matrix of RFs is replaced by a properly scaled random orthogonal matrix, resulting in significantly decreased kernel approximation error. A particularly nice feature of ORFs is that (Yu et al., 2016; Choromanski et al., 2017) prove that using ORFs results in a guaranteed improvement in pointwise kernel approximation error when compared with RFs.
Unfortunately the guarantees in Yu et al. (2016) are not directly applicable to the PSRNN setting. PSRNNs first obtain a set of model parameters via ridge regression, then use these model parameters to calculate inner products in RF space. This "downstream" application of RFs goes beyond the results proven in Yu et al. (2016) and Choromanski et al. (2017). Hence it is not clear whether or not ORF can be applied to obtain an improvement in the PSRNN setting.
In this work, we show that ORFs can be used to obtain OPSRNNs: PSRNNs initialized using ORFs which are smaller, faster to execute and train than PSRNNs initialized using conventional unstructured RFs. We theoretically analyze the orthogonal version of the KRR algorithm that is used to initialize OPSRNNs. We show that orthogonal RNNs lead to kernel algorithms with strictly better spectral properties and explain how this translates to strictly smaller upper bounds on failure probabilities regarding KRR empirical risk. We compare the performance of OPSRNNs with that of LSTMs as well as conventional PSRNNs on a number of robotics tasks, and show that OPSRRNs are consistently superior on all tasks. In particular, we show that OPSRNN models can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.
2 RELATED WORK
Orthogonal random features were introduced in Yu et al. (2016) as an alternative to the standard approach for constructing random feature maps to scale kernel methods (Rahimi & Recht, 2007). Several other structured constructions were known before (Ailon & Chazelle, 2006; Hinrichs & Vyb´iral, 2011; Vyb´iral, 2011; Zhang & Cheng, 2013; Choromanski & Sindhwani, 2016; Choromanska et al., 2016; Bojarski et al., 2017), however these were motivated by computational and space complexity gains and led to weaker accuracy guarantees. In contrast to this previous work, orthogonal random features were proposed to improve accuracy of the estimators relying on them. Such an improvement was theoretically and experimentally verified, but only for pointwise kernel approximation (Yu et al., 2016; Choromanski et al., 2017) and for specific kernels (such as Gaussian for dimensionality large enough, as well as dot-product and angular kernels). It was not clear whether these pointwise gains translate to downstream guarantees for algorithms relying on kernels (for instance kernel ridge regression), even though there was some partial empirical evidence that this might be the case (in Choromanski et al. (2017) orthogonal random features were experimentally tested to provide more accurate approximation of the groundtruth kernel matrix in terms of the Frobenius norm error). Even for the pointwise estimators and for the selected kernels, the guarantees were given only with the use of second moment methods (variance computation) and thus did not lead to strong concentration results with exponentially small probabilities of failure, which we obtain in this paper.
To the best of our knowledge, we are the first to apply orthogonal random features via kernel ridge regression for recurrent neural networks. There is however vast related literature on orthogonal recurrent neural networks, where the matrices are enforced to be orthogonal or initialized to be random orthogonal. Probably some of the most exploited recent directions are unitary evolution RNN architectures (Arjovsky et al., 2016), where orthogonal and unitary matrices are used to address a key problem of RNN training - vanishing or exploding gradients. Related results are presented in
2

Under review as a conference paper at ICLR 2018
Henaff et al. (2016), Saxe et al. (2013) (orthogonal initialization for training acceleration), Ganguli et al. (2008) and White et al. (2004). Most of these results do not provide any strict theoretical guarantees regarding the superiority of the orthogonal approach. Even though these approaches are only loosely related to our work, there is a common denominator: orthogonality, whether applied in our context or the aforementioned ones, seems to to be responsible for disentangling in (deep) representations (Achille & Soatto, 2017). Our rigorous theoretical analysis shows that this phenomenon occurs for the orthogonal KRR that is used as a subroutine of OPSRNNs, but the general mechanism is still not completely understood from the theoretical point of view.
3 PREDICTIVE STATE RECURRENT NEURAL NETWORKS
Below we describe the PSRNN architecture in more detail. We pay particular attention to how PSRNNS utilize RFs, which will be replaced with ORFs in OPSRNNs. We also describe the twostage regression algorithm, which uses kernel ridge regression to initialize PSRNNs. The explicit construction of ORFs is given in Section 4.

Figure 1: Single Layer PSRNN
Figure 2: PSRNN architecture: See equation 1 for details.
PSRNNs (Downey et al., 2017) are a GRU-like RNN architecture developed by combining a specific type of state from Predictive State Representations (Boots et al., 2013) with RFs.
PSRNNs use the notion of a predictive state (Littman et al., 2001): i.e. they provide a compact representation of a dynamical system by representing state as a set of predictions of features of future observations. Specifically, a predictive state is defined as qt = qt|t-1 = E[ft | ht], where ft = f (ot:t+k-1) is a vector of features of future observations and ht = h(o1:t-1) is a vector of features of historical observations. The features are selected such that qt determines the distribution of future observations P (ot:t+k-1 | o1:t-1).1 Filtering is the process of mapping a predictive state qt to qt+1 conditioned on ot, while prediction maps a predictive state qt = qt|t-1 to qt+k|t-1 = E[ft+k | o1:t-1] without intervening observations.
Let kf , kh, ko be translation invariant kernels (Rahimi & Recht, 2008) defined on ft, ht, and ot respectively. We use RFs (Rahimi & Recht, 2008) to define projections t = RFF (ft), t = RFF (ht), and t = RFF (ot) such that kf (fi, fj) = Ti j, kh(hi, hj) = iT j, ko(oi, oj) = iT j. Using this notation, the PSRNN predictive state is qt = E[t | t]. Formally a PSRNN consists of an initial state q1 and a 3-mode update tensor W (see: Figure 2). The PSRNN state update equation is:

qt+1 =

W ×3 qt ×2 ot W ×3 qt ×2 ot

(1)

3.1 TWO-STAGE REGRESSION FOR PSRNNS
Downey et al. (2017) show that PSRNNs can be initialized using the Two Stage Regression (2SR) approach of Hefny et al. (2015). This approach is fast, statistically consistent, and reduces to simple linear algebra operations. In 2SR the PSRNN model parameters q1 and W are learned by solving three Kernel Ridge Regression problems in two stages. Ridge regression is required in order
1For convenience we assume that the system is k-observable: that is, the distribution of all future observations is determined by the distribution of the next k observations. (Note: not by the next k observations themselves.) At the cost of additional notation, this restriction could easily be lifted.

3

Under review as a conference paper at ICLR 2018

to obtain a stable model, as it allows us to minimize the destabilizing effect of rare events while preserving statistical consistency.
In stage one we regress from past t to future t, and from past t to the outer product of shifted future  := t+1 with observations t. Let C be the matrix whose tth column is t, C the matrix whose tth column is t, and C be the matrix whose tth column is t  t:
C| = CCT CCT + I -1 , C| = CCT CCT + I -1 .
In stage two we regress from C| to C| to obtain the model weights W :

q1 = C|1,

-1

W = C|CT| C|CT| + I

,

where   R is the ridge parameter and I is the identity matrix and 1 is a column vector of ones. Once the state update parameters have been learned via 2SR we train a kernel ridge regression model to predict t from qt. 2
The 2SR algorithm is provably consistent in the realizable setting, meaning that in the limit we are guaranteed to recover the true model parameters. Unfortunately this result relies on exact kernel values, while scalable implementations work with approximate kernel values via the machinery of RFs. In practice we often require a large number of RFs in order to obtain a useful model. This can result in large models which are slow to execute and slow to train.
We now introduce ORFs and show that they can be used to obtain smaller, faster models. The key challenge with applying ORFs to PSRNNs is extending the theoretical guarantees of Yu et al. (2016) to the kernel ridge regression setting.

4 ORTHOGONAL RANDOM FEATURES

We explain here how to construct orthogonal random features to approximate values of kernels defined by the prominent family of radial basis functions and consequently, conduct kernel ridge regression for the OPSRNN model. A class of RBF kernels K (RBFs in shorthand) is a family of functions: Kn : Rn ×Rn  R for n = 1, 2, ... such that Kn(x, y) = (z), for z = x-y 2, where  : R  R is a fixed positive definite function (not parametrized by n). An important example is the class of Gaussian kernels.
Every RBF kernel K is shift-invariant, thus in particular its values can be described by an integral via Bochner's Theorem (Rahimi & Recht, 2007):

K(x, y) = Re exp(iw (x - y))µK (dw),
Rn

(2)

where µK  M(Rn) stands for some finite Borel measure. Some commonly used RBF kernels K together with the corresponding functions  and probability density functions for measures µK are given in Table 3. The above formula leads straightforwardly to the standard unbiased Monte-

Carlo estimator of K(x, y) given as: K(x, y) = m,n(x)m,n(y), where a random embedding m,n : Rn  R2m is given as:

m,n(x) =

1 m

cos(wi

x),

1 m

sin(wi

x)

m
,
i=1

(3)

vectors wi  Rn are sampled independently from µK and m stands for the number of random features used. In this scenario we will often use the notation wiiid, to emphasize that different wis are sampled independently from the same distribution.

2Note that we can train a regression model to predict any quantity from the state

4

Under review as a conference paper at ICLR 2018

Name Gaussian Laplacian

Positive-definite function 

2 exp

-

1 22

z

2

exp(-z)

Probability density function

2 (2 2 )n/2

exp

-

1 22

w

2 2

n1 i=1 (1+wi)

Figure 3: Common RBF kernels, the corresponding functions , and probability density functions (here: w = (w1, ..., wn) ).

For a datasets X , random features provide an equivalent description of the original kernel via the linear kernel in the new dataset (X ) = {(x) : x  X } obtained by the nonlinear transformation  and lead to scalable kernel algorithms if the number of random features needed to accurately approximate kernel values satisfies: m N = X .
Orthogonal random features are obtained by replacing a standard mechanism of constructing vectors wi and described above with the one where the sequence (w1, ..., wm) is sampled from a "related" joint distribution µKor,tm,n on Rn × ... × Rn satisfying the orthogonality condition, namely: with probability p = 1 different vectors wi are pairwise orthogonal. Since in practice we often need m > n, the sequence (wi)i=1,...,m is obtained by stacking some number of blocks, each of length l  n, where the blocks are sampled independently from µoKr,tm,n.
It remains to explain how µKor,tm,n is constructed. We consider two main architectures. For the first one the marginal distributions (distributions of individual vectors wi) are µK. A sample (w1ort, ..., wmort) from the joint distribution might be obtained for instance by constructing a random matrix G = [(w1iid) ; ...; (wmiid) ]  Rm×n, performing Gram-Schmidt orthogonalization and then renormalizing the rows such that the length of the renormalized row is sampled from the distribution from which wiiid s are sampled. Thus the Gram-Schmidt orthogonalization is used just to define the directions of the vectors. From now on, we will call such a joint distribution continuous-orthogonal. The fact that for RBF kernels the marginal distributions are exactly µK and thus, kernel estimator is still unbiased, is a direct consequence of the isotropicity of distributions fom which directions of vectors wiiid are sampled. For this class of orthogonal estimators we prove strong theoretical guarantees showing that they lead to kernel ridge regression models superior to state-of-the-art ones based on vectors wiiid.
Another class of orthogonal architectures considered by us is based on discrete matrices. We denote by D a random diagonal matrix with nonzero entries taken independently and uniformly at random from the two-element set {-1, +1}. Furthermore, we will denote by H a Hadamard matrix obtained via Kronecker-products (see: Choromanski et al. (2017)). An m-vector sample from the discreteorthogonal joint distribution is obtained by taking m first rows of matrix G defined as GHAD = HD1 · ... · HDk for: fixed k > 0, independent copies Di of D and then renormalizing each row in exactly the same way as we did it for continuous-orthogonal joint distributions. Note that GHAD is a product of orthogonal matrices, thus its rows are also orthogonal. The advantage of a discrete approach is that it leads to a more time and space efficient method for computing random feature maps (with the use of Fast Walsh-Hadamard Transform; notice that the Hadamard matrix does not even need to be stored explicitly). This is not our focus in this paper though. Accuracywise discrete-orthogonal distributions lead to slightly biased estimators (the bias is a decreasing function of the dimensionality n). However as we have observed, in practice they give as accurate PSRNN models as continuous-orthogonal distributions, consistently beating approaches based on unstructured random features. One intuitive explanation of that phenomenon is that even though in that setting kernel estimators are biased, they are still characterized by much lower variance than those based on unstructured features. We leave a throughout theoretical analysis of discreteorthogonal joint distributions in the RNN context to future work.
5 THE THEORY OF THE ORTHOGONAL KERNEL RIDGE REGRESSION
In this section we extend the theoretical guarantees of Yu et al. (2016) to give rigorous theoretical analysis of the initialization phase of OPSRNN. Specifically, we provide theoretical guarantees for kernel ridge regression with orthogonal random features, showing that they provide strictly better spectral approximation of the ground-truth kernel matrix than unstructured random features. As a
5

Under review as a conference paper at ICLR 2018

corollary, we prove that orthogonal random features lead to strictly smaller empirical risk of the model. Our results go beyond second moment guarantees and enable us to provide the first exponentially small bounds on the probability of a failure for random orthogonal transforms.

Before we state our main results, we will introduce some basic notation and summarize previous results. Assume that labeled datapoints (xi, yi), where xi  Rn, yi  R for i = 1, 2, ..., are generated as follows: yi = f (xi) + i, where f  : Rn  R is a function that the model aims
to learn, and i for i = 1, 2, ... are independent Gaussians with zero mean and standard deviation  > 0. The empirical risk of the estimator f : Rn  R is defined as follows:

1

R(f )



E{i }i=1,...,N

[ N

N
(f (xi) - f (xi))2],

j=1

(4)

where N stands for a dataset size.
By fvec  RN we denote a vector whose jth entry is f (xj). Denote by fKRR a kernel ridge regression estimator applying exact kernel method (no random feature map approximation). Assume that we analyze kernel K : Rn × Rn  R with the corresponding kernel matrix K. It is a well known result (Alaoui & Mahoney, 2015; Bach, 2013) that the empirical risk of fKRR is given by the formula:

R(fKRR) = N -12(fvec) (K + N IN )-2fvec + N -12Tr(K2(K + N IN )-2), (5)

where  > 0 stands for the regularization parameter and IN  RN×N is an identity matrix.

Denote by fKRR an estimator based on some random feature map mechanism and by K the corresponding approximate kernel matrix.

The expression that is used in several bounds on the empirical risk for kernel ridge regression (see

for instance Avron et al. (2017)) is the modified version of the above formula for R(fKRR), namely:

R¯NKI(Nfv)e-c)1

 N -12 ). It can be

(fvec) easily

(K + proven

N IN )-1fvec that R(fKRR)

+ 

N -12s(K), R¯ K(fvec).

where

s(K)



Tr(K(K +

To measure how similar to the exact kernel matrix (in terms of spectral properties) a kernel matrix obtained with random feature maps is, we use the notion of -spectral approximation (Avron et al., 2017).

Definition 1. For a given 0 <  < 1, matrix A  RN×N is a -spectral approximation of a matrix B  RN×N if (1 - )B A (1 + )B.
It turns out that one can upper-bound the risk R(fKRR) for the estimator fKRR in terms of the  parameter if matrix K + N IN is a -spectral approximation of the matrix K + N IN , as the next result (Avron et al., 2017) shows:

Theorem 1. Suppose that K 2  1 and that matrix K + N IN obtained with the use of random features is a -spectral approximation of matrix K + N IN . Then the empirical risk R(fKRR) of the estimator fKRR satisfies:

R(fKRR)



1

1 -



R¯ K(fvec

)

+

1

 +

rank(K) 2. N

(6)

5.1 SUPERIORITY OF THE ORTHOGONAL FEATURES FOR KERNEL RIDGE REGRESSION
Consider the following RBF kernels, that we call smooth RBFs. As we show next, Gaussian kernels are smooth.
Definition 2 (smooth RBFs). We say that the class of RBF kernels defined by a fixed  : R  R (different elements of the class corresponds to different input dimensionalities) and with associated sequence of probabilistic measures {µ1, µ2, ...} (µi  M(Ri)) is smooth if there exists a

6

Under review as a conference paper at ICLR 2018

nonincreasing function f : R  R such that f (x)  0 as x   and furthermore the kth moments of random variables Xn = w , where w  µn satisfy for every n, k  0: E[Xnk]  (n - 1)(n + 1) · ... · (n + 2k - 3)k!f k(k).

Many important classes of RBF kernels are smooth, in particular the class of Gaussian kernels. This

follows immediately from the well-known fact that for Gaussian kernels the above kth moments are

given

by

the

following

formula:

E[Xnk ]

=

2k

(

n 2

+k-1)!

(

n 2

-1)!

for

n

>

1.

Our main result is given below and shows that orthogonal random features lead to tighter bounds on  for the spectral approximation of K + N IN . Tighter bounds on , as Theorem 1 explains, lead to tighter upper bounds also on the empirical risk of the estimator. We will prove it for the setting where each structured block consists of a fixed number l > 1 of rows (note that many independent structured blocks are needed if m > n), however our experiments suggest that the results are valid also without this assumption.

Theorem 2 (spectral approximation). Consider a smooth RBF (in particular Gaussian kernel). Let

iid denote the smallest positive number such that Kiid + N IN is a -approximation of K +

N IN , where Kiid is an approximate kernel matrix obtained by using unstructured random features.

Then for any a > 0,

P[iid

>

a]



piNid,m

(

amin N

),

(7)

where: piNid,m is given as: pNiid,m(x) = N 2e-Cmx2 for some universal constant C > 0, m is the number of random features used, min is the smallest singular value of K + N IN and N is dataset size. If instead orthogonal random features are used then for the corresponding spectral parameter

ort the following holds:

P[ort

>

a]



pNor,tm

(

amin N

),

(8)

where function poNr,tm satisfies: poNr,tm < pNiid,m for n large enough.

We see that both constructions lead to exponentially small (in the number of random features m
used) probabilities of failure, however the bounds are tighter for the orthogonal case. An exact formula on pNor,tm can be derived from the proof that we present in the Appendix, however for clarity we do not give it here.

Theorem 2 combined with Theorem 1 lead to risk bounds for the kernel ridge regression model based on random unstructured and random orthogonal features. We use the notation introduced before and obtain the following:

Theorem 3. Under the assumptions of Theorem 1 and Theorem 2, the following holds for the kernel

ridge regression risk and any c > 0 if m-dimensional unstructured random feature maps are used to

approximate

a

kernel:

P[R(fKRR)

>

c]



piNid,m

(

ac

min N

),

where

ac

is

given

as:

ac

=

1

-

R¯ K(fvec)

c-

m2 2N

and the probability is taken with respect to the random choices of features. If instead random

orthogonal

features

are

used,

we

obtain

the

following

bound:

P[R(fKRR)

>

c]



poNr,tm

(

ac

min N

).

As before, since for large n function pNor,tm satisfies pNor,tm < pNiid,m, for orthogonal random features we obtain strictly smaller upper bounds on the failure probability regarding empirical risk than for
the state-of-the-art unstructured ones. In practice, as we will show in the experimental section, we
see gains also in the regimes of moderate dimensionalities n.

6 EXPERIMENTS
In section 5 we extended the theoretical guarantees for ORFs to the case of the initialization phase of OPSRNNs. In this section we confirm these results experimentally and show that they imply better performance of the entire model by comparing the performance of PSRNNs with that of OPSRNNs on a selection of robotics time-series datasets. Since OPSRNN models obtained via continuousorthogonal and discrete-orthogonal joint sampling (see: Section 4) gave almost the same results, presented OPSRNN-curves are for the continuous-orthogonal setting.

7

Under review as a conference paper at ICLR 2018
6.1 EXPERIMENTAL SETUP
We now describe the datasets and model hyperparameters used in our experiments. All models were implemented using the Tensorflow framework in Python. We use the following datasets in our experiments:
· Swimmer We consider the 3-link simulated swimmer robot from the open-source package OpenAI gym.3. The observation model returns the angular position of the nose as well as the (2D) angles of the two joints, giving in a total of 5 features. We collect 25 trajectories from a robot that is trained to swim forward (via the cross entropy with a linear policy), with a train/test split of 20/5.
· Mocap This is a Human Motion Capture dataset consisting of 48 skeletal tracks from three human subjects collected while they were walking. The tracks have 300 time steps each, and are from a Vicon motion capture system. We use a train/test split of 40/8. There are 22 total features consisting of the 3D positions of the skeletal parts (e.g., upper back, thorax, clavicle).
· Handwriting This is a digital database available on the UCI repository (Alpaydin & Alimoglu, 1998) created using a pressure sensitive tablet and a cordless stylus. Features are x and y tablet coordinates and pressure levels of the pen at a sampling rate of 100 milliseconds giving a total of 3 features. We use 25 trajectories with a train/test split of 20/5.
In two-stage regression we use history (similarly future) features consisting of the past (next) 2 observations concatenated together. We use a ridge-regression parameter of 10(-2) (this is consistent with the values suggested in Boots et al. (2013); Downey et al. (2017)). The kernel width is set to the median pairwise (Euclidean) distance between neighboring data points. We use a fixed learning rate of 0.1 for BPTT with a BPTT horizon of 20. We use a single layer PSRNN. We optimize and evaluate all models with respect to the Mean Squared Error (MSE) of one step predictions (this should not be confused with the MSE of the pointwise kernel approximation which does not give the downstream guarantees we are interested in here). This means that to evaluate the model we perform recursive filtering on the test set to produce states, then use these states to make predictions about observations one time step in the future.
6.2 RESULTS
6.2.1 ORTHOGONAL RF FOR 2SR
In our first experiment we examine the effectiveness of Orthogonal RF with respect to learning a good PSRNN via 2SR. In figure 4 we compare the MSE for a PSRNN learned via Orthogonal RF with that of one learned using Standard RF for varying numbers of random features. Note that these models were initialized using 2SR but were not refined using BPTT. We see that in all cases when the ratio of RF to input dimension is small Orthogonal RF significantly outperforms Standard RF. This difference decreases as the number of RF increases, with both approaches resulting in similar MSE for large RF to input ratios.
6.2.2 ORTHOGONAL RF FOR BPTT
In our second experiment we examine the effectiveness of Orthogonal RF with respect to learning a good PSRNN via 2SR initialization combined with refinement via BPTT. In figure 5 we compare the MSE for a PSRNN learned via Orthogonal RF with that of one learned using Standard RF over a number of epochs of BPTT. We see that on all datasets, for both Orthogonal RF and Standard RF, MSE decreases as the number of epochs increases. However it is interesting to note that in all datasets Orthogonal RF converges to a MSE than Standard RF.
3https://gym.openai.com/
8

Under review as a conference paper at ICLR 2018
Figure 4: MSE for Orthogonal RF vs Standard RF on all three data sets.
Figure 5: MSE for Orthogonal RF vs Standard RF on all three data sets
6.3 DISCUSSION These results demonstrate the effectiveness of Orthogonal RF as a technique for improving the performance of downstream applications. First we have shown that Orthogonal RF can offer significant performance improvements for kernel ridge regression, specifically in the context of the 2SR algorithm for PSRNNs. Furthermore we have shown that not only does the resulting model have lower error, it is also a better initialization for the BPTT gradient descent procedure. In other words, using a model initialization based on orthogonal RF results in BPTT converging to a superior final model. While the focus of these experiments was to compare the performance of PSRNNs and OPSRNNs, for the sake of completeness we also include error plots for LSTMs. We see that OPSRNNs significantly outperform LSTMs on all data sets.
7 CONCLUSIONS
We showed how structured orthogonal constructions can be effectively integrated with recurrent neural network based architectures to provide models that consistently achieve performance superior to the baselines. They also provide significant compression, achieving similar accuracy as PSRNNs with an order of magnitude smaller number of features needed. Furthermore, we gave the first theoretical guarantees showing that orthogonal random features lead to exponentially small bounds on the failure probability regarding empirical risk of the kernel ridge regression model. The latter one is an important component of the RNN based architectures for state prediction that we consider in this paper. Finally, we proved that these bounds are strictly better than for the standard nonorthogonal random feature map mechanism. Exhaustive experiments conducted on several robotics task confirm our theoretical findings.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep representations. CoRR, abs/1706.01350, 2017. URL http://arxiv.org/abs/1706. 01350.
N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In STOC, 2006.
A. El Alaoui and M. Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 775­783, 2015.
E Alpaydin and Fevzi Alimoglu. Pen-based recognition of handwritten digits data set. University of California, Irvine, Machine Learning Repository. Irvine: University of California, 1998.
Mart´in Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1120­1128, 2016. URL http://jmlr.org/ proceedings/papers/v48/arjovsky16.html.
H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, and A. Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and statistical guarantees. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 253­262, 2017. URL http://proceedings.mlr.press/v70/ avron17a.html.
F. Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pp. 185­209, 2013. URL http://jmlr.org/proceedings/papers/v30/Bach13.html.
M. Bojarski, A. Choromanska, K. Choromanski, F. Fagan, C. Gouy-Pailler, A. Morvan, N. Sakr, T. Sarlos, and J. Atif. Structured adaptive and random spinners for fast machine learning computations. In AISTATS, 2017.
Byron Boots, Geoffrey J. Gordon, and Arthur Gretton. Hilbert space embeddings of predictive state representations. CoRR, abs/1309.6819, 2013. URL http://arxiv.org/abs/1309.6819.
KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259, 2014a.
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014b.
A. Choromanska, K. Choromanski, M. Bojarski, T. Jebara, S. Kumar, and Y. LeCun. Binary embeddings with structured hashed projections. In ICML, 2016.
K. Choromanski and V. Sindhwani. Recycling randomness with structure for sublinear time kernel expansions. In ICML, 2016.
K. Choromanski, M. Rowland, and A. Weller. The unreasonable effectiveness of structured random orthogonal embeddings. In to appear in NIPS, volume arXiv abs/1703.00864, 2017.
Carlton Downey, Ahmed Hefny, Boyue Li, Byron Boots, and Geoffrey Gordon. Predictive state recurrent neural networks. arXiv preprint arXiv:1705.09353, 2017.
Surya Ganguli, Dongsung Huh, and Haim Sompolinsky. Memory traces in dynamical systems. 105(48):18970 18975, 2008. doi: 10.1073/pnas.0804451105., 2008.
Ahmed Hefny, Carlton Downey, and Geoffrey J. Gordon. Supervised learning for dynamical system learning. In Advances in Neural Information Processing Systems, pp. 1963­1971, 2015.
10

Under review as a conference paper at ICLR 2018
Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent speaker verification. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 5115­5119. IEEE, 2016.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory tasks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 2034­2042, 2016. URL http://jmlr.org/ proceedings/papers/v48/henaff16.html.
A. Hinrichs and J. Vyb´iral. Johnson-Lindenstrauss lemma for circulant matrices. Random Structures & Algorithms, 39(3):391­398, 2011.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735­ 1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.
Michael L. Littman, Richard S. Sutton, and Satinder Singh. Predictive representations of state. In In Advances In Neural Information Processing Systems 14, pp. 1555­1561. MIT Press, 2001.
Tomas Mikolov, Martin Karafia´t, Lukas Burget, Jan Cernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177­1184, 2008. Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlin-
ear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013. URL http://arxiv.org/abs/1312.6120. I. Schoenberg. Metric Spaces and Completely Monotone Functions. The Annals of Mathematics, 39 (4):811­841, 1938. Alex Smola, Arthur Gretton, Le Song, and Bernhard Scho¨lkopf. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory, pp. 13­31. Springer, 2007. J. Vyb´iral. A variant of the Johnson-Lindenstrauss lemma for circulant matrices. Journal of Functional Analysis, 260(4):1096­1105, 2011. O. White, D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural networks. Physical Review Letters, 92, 2004. ISSN 14., 2004. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pp. 2048­2057, 2015. F. Yu, A. Suresh, K. Choromanski, D. Holtmann-Rice, and S. Kumar. Orthogonal random features. In NIPS, pp. 1975­1983, 2016. H. Zhang and L. Cheng. New bounds for circulant Johnson-Lindenstrauss embeddings. CoRR, abs/1308.6339, 2013.
11

Under review as a conference paper at ICLR 2018

APPENDIX: INITIALIZATION MATTERS: ORTHOGONAL PREDICTIVE STATE RECURRENT NEURAL NETWORKS
We will use the notation from the main body of the paper.

8 PROOF OF THEOREM 2

We will assume that a dataset X = {x1, ..., xN } under consideration is taken from a ball B of a fixed radius r (that does not depend on data dimensionality n and dataset size N ) and center x0. We begin with the following lemma:

Lemma 1. Fix an RBF kernel K : Rn × Rn. Consider a randomized kernel estimator K with a corresponding random feature map: m,n : Rn  R2m and assume that for any fixed i, j 

{1, ..., N } the followig holds for any c > 0: P[|m,n(xi) m,n(xj) - K(xi, xj)| > c]  g(c)

for some fixed function g : R  R. Then with probability at least 1 - N 2g(c), matrix K + IN is

a

-spectral

approximation

of

matrix

K + IN

for



=

,N c
min

where

min

stands

for

the

minimal

singular value of K + IN .

Proof. Denote K + N IN = V 2V, where an orthonormal matrix V  RN×N and a diagonal matrix   RN×N define the eigendecomposition of K + N IN . Following Avron et al. (2017),

we notice that in order to prove that K + IN is a -spectral approximation of K + IN , it suffices

to show that:

-1VKV -1 - -1VKV -1 2  .

(9)

From basic properties of the spectral norm 2 and the Frobenius norm F we have:
P[ -1VKV -1 - -1VKV -1 2 > ]  P[ -1V 2 K - K F V -1 2 > ] (10)

The latter probability is equal to p = P[

K-K

2 F

>

].2

-1 V

2 2

·

V

-1

2 2

Furthermore, since V is an isometry matrix, we have:

-1V

2 2



1 min

and

V

-1

2 2



.1
min

Thus we have:

p  P[

K-K

2 F

> 2m2 in].

(11)

Now notice that from the union bound we get:

p



N 2P[|K(i, j)

-

K(i, j)|

>

min ] N

=

N 2P[|m,n(xi)

m,n(xj )

-

K(i, j)|

>

min ]. N

(12)

Therefore the probability that K + IN is a -spectral approximation of K + IN is at least

1 - N 2g(c)

for

c

=

min N

and

that

completes

the

proof.

Our goal right now is to compute function g from Lemma 1 for random feature maps constructed according to two procedures: the standard one based on independent sampling and the orthogonal one, where marginal distributions corresponding to the joint distribution (w1, ..., wm) are the same, but vectors wi are conditioned to be orthogonal.

We start with a standard random feature map mechanism. Note first that from basic properties of the trigonometric functions we conclude that for any two vectors x, y  Rn, the random feature map

approximation of the RBF kernel K(x, y) which is of the form K(x, y) = |m,n(x) m,n(y) can

be equivalently rewritten as:

K(x, y)

=

1 m

m i=1

cos(wi

z)

for

z

=

x - y.

This

is

true

for

any

joint distribution (w1, ..., wm).

Lemma 2. If mapping m,n is based on the standard mechanism of independent sampling then one can take as function g from Lemma 1 a function given by the following formula: g(x) = e-Cmx2

for some universal constant C > 0.

12

Under review as a conference paper at ICLR 2018

Proof. Notice first that by the remark above, we get: P[|m,n(xi) m,n(xj) - K(xi, xj)| > x] =

P[

m i=1

Zi

>

x],

where Zi

=

1 m

cos(wi

z) -

1 m

(z),

z

=

x - y 2 and  is a positive definite

function associated with an RBF kernel K. From the unbiasedness of the estimator we see that

E[Zi]

=

0.

Also,

notice that:

|Zi|



2 m

and

different

Zis are

independent.

Thus,

from

the standard

application of Chernoff inequality we get: P[

n i=1

Zi

>

x]



e-Cmx2

for

some

universal

constant

C > 0 (that does not depend on m). That completes the proof.

By combining Lemma 1 with the formula on g for the standard unstructured case from Lemma 2, we already obtain the formula for pNiid,m from the statement of the theorem. It remains to show that:
pNor,tm < piNid,m.

Note that in the previous proof the upper bound on g is derived as a monotonic function of

E[et

]m
i=1

Zi

for

a

parameter

t

>

0

(that

is

then

being

optimized),as

it

is

the

case

for

the

stan-

dard Chernoff's argument. Now, since variables Zi are independent, we obtained: E[et

] =m
i=1

Zi

m i=1

E[etZi

].

Thus, if we can prove that for the continuous-orthogonal distribution we have:

E[et

] <m
i=1

Zi

m i=1

E[etZi

],

then

we

complete

the

proof

of

Theorem

2

(note

that

the

marginal

dis-

tributions of Zi are the same for both: standard mechanism based on unstructured random features

and the one based on continuous-orthogonal sampling of the m-tuple of n-dimensional vectors).

This is what we prove below.
Lemma 3. Fix some z  Rn and t > 0. For a sample (w1ort, ..., wmort) from the continuousorthogonal distribution the following holds for n large enough:

t
E[e n

m

] <m
i=1

cos((wiort )

z)

E[e

t n

cos((wiort )

z)].

i=1

(13)

Proof. Since different blocks of vectors wi used to construct the orthogonal feature map are independent (the number of blocks is greater than one if m > n), it suffices to prove the inequality just
for one block. Thus from now on we will focus just on one block and thus without loss of generality we will assume that m  n.

Note first that

m

t
E[e n

cos((wiort )

z)]

=

t
E[e n

i=1

],m
i=1

cos((wiiid )

z)

(14)

where (w1iid, ..., wmiid) stands for the m-tuple sample constructed by the standard unstructured mechanism.

Thus we need to prove that

t
E[e n

m i=1

cos((wiort )

z)]

<

E[e

t n

]m
i=1

cos((wiiid )

z)

(15)

Using Taylor expansion for ex, we conclude that it sufficies to prove that:

i.e. that:

(

t n

)j1

+...+jm

E[

cos((w1ort

)

j1 ,j2 ,...,jm

z)j1 · ... · cos((wmort) j1! · ... · jk!

z)jm ]

<

(

t n

)j1

+...+jm

E[

cos((w1iid

)

j1 ,j2 ,...,jm

z)j1 · ... · cos((wmiid) j1! · ... · jk!

z)jm ],

( t )j1+...+jm n
j1 ,j2 ,...,jm

j1!

·

1 ...

·

jm! (j1,

...,

jm)

>

0,

13

(16) (17)

Under review as a conference paper at ICLR 2018

where:

(j1, ..., jk) = E[cos((w1iid) z)j1 · ... · cos((wmiid) z)jm - cos((w1ort) z)j1 · ... · cos((wmort) z)jm ] (18)

By applying the trigonometric formula:

cos() cos()

=

1 (cos(

+

)

+

cos(

-

)),

2

(19)

we get:

1

(j1, ..., jk) = 2j1+...+jm

E[

s1,...,sj1+...+jm {-1,+1}

cos ((w1iid s1 w1iid s2 ... sj1-1 w1iid) sj1 ...) z -

cos ((w1ort s1 w1ort s2 ... sj1-1 w1ort) sj1 ...) z ],

(20)

where 1 stands for vector-addition operator and -1 stands for vector-subtraction operator.
Note that without loss of generality we can assume that sj1 = sj1+j2 = ... = +1, since for other configurations we obtain a random variable of the same distribution. Consider a fixed configuration (s1, s2, ..., sj1+...+jm ) and the corresponding term of the sum above that can be rewritten in the compressed way as:
F = E[cos(n1w1iid+n2w2iid+...+nmwmiid) z]-E[cos(n1w1ort+n2w2ort+...+nmwmort) z], (21)
for some n1, ..., nm  Z. Without loss of generality, we can assume that n1, ..., nm  N, since the distribution of the random variables under consideration does not change if ni is replaced with -ni. Without loss of generality we can also assume that there exists i  {1, ..., m} such that ni > 0, since otherwise the corresponding term F is equal to 0.
Denote by R1, ..., Rm the set of independent random variables, where each is characterized by the distribution which is the distribution of vectors wiiid (and thus also of vectors wiort). Denote: R = n12R12 + ... + n2mRm2 . Note that n1w1ort + n2w2ort + ... + nmwmort  Rv, where v is a unit L2-norm vector taken uniformly at random from the sphere of radius 1 and furthermore: R and v are chosen independently. That is implied by the isotropicity of vectors wiort. Similarly, denote R = R2 + i,j{1,...,m} ninjRiRjvi vj, where v1, ..., vm stand for the independent copies of v. Note that, by the similar analysis as before, we conclude that n1w1iid + n2w2iid + ... + nmwmiid  Rv and furthermore, R and v are chosen independently.
Therefore, by expanding cos(x) using Taylor expansion, we get:


F=

z

2k (-1)k E[(v (2k)!

z)2k] E[R2k] -



z

2k (-1)k E[(v (2k)!

z)2k] E[R2k],

k=0

k=0

(22)

where: z =

z z

. Denote: A(k, n) = E[(v

z)k] (note that v, z  Rn). It is easy to see that for odd

k we have: A(n, k) = 0. We obtain:


F=

z

2k

(-1)kA(2k, (2k)!

n)

(E[R2k

]

-

E[R2k

]).

k=0

(23)

Using a well-known formula on A(k, n):

A(k, n) =

1

 0

sinn-2()d


cosk() sinn-2()d,
0

by deriving an explicit formula on

 0

cosk

()

sinn-2()d,

we

get:

(24)

14

Under review as a conference paper at ICLR 2018

A(2k, n)

=

(n - 2)(n - 4) · ... · (n (n - 3)(n - 5) · ... · (n

= =

2) 2)

·

(2k - 1)!! (n - 1)(n + 1)...(n +

2k

- 3) ·

(n + 2k - 3)(n + 2k - 5)... · (n = 2) (n + 2k - 2)(n + 2k - 4)... · (n = 2) ,

(25)

where: (n = 2) = 2 if n = 2 and (n = 2) = 1 otherwise and: (n = 2) = 1 if n = 2 and (n = 2) = 2 otherwise. In particular, the following is true:

|A(2k, n)|



(n

-

1)(n

(2k - 1)!! + 1) · ... · (n

+

2k

-

. 3)

(26)

We will use that later. Note that vi vj  v z. Therefore we obtain:

 z 2k(-1)kA(2k, n)

F=

(2k)!

k ,

k=0

(27)

where

k
k =

k i

E[(R2)k-ii],

i=1

(28)

and  = i,j{1,...,m} ninj RiRj vi vj .

Note that E[(R2)k-1] = 0 since E[(vi vj)] = 0 and furthermore, directions v1, ..., vm are chosen independently from lengths R1, ..., Rm. Therefore we have:

k =

k 2

E[(R2)k-22] + k,

(29)

where:

k
k =

k i

E[(R2)k-ii].

i=3

Now let us focus on a single term  = E[(R2)k-ll] for some fixed l  3.

Note that the following is true:

  E[(R2)k-l(

ninj

RiRj

)l]

·

max
i1 ,j1 ,...,il ,jl

E[|vi1

vj1

|

·

...

·

|vil

vjl

|],

i,j{1,...,m}

(30) (31)

where the maximum is taken over i1, j1, ..., il, jl  {1, ..., m} such that is = js for s = 1, ..., l.

Note first that:

E[(R2)k-l(

ninj RiRj )l] 

i,j{1,...,m}

E[(R2)k-l(

(niRi)2

+ 2

(nj

Rj

)2

)l]



E[(R2)k-l(m

-

1)l(R2)l]



(m

-

1)l E[R2k ].

i,j{1,...,m}

(32)

Let us focus now on the expression maxi1,j1,...,il,jl E[|vi1 vj1 | · ... · |vil vjl |]. Note that from

the isotropicity of Gaussian vectors

 g1

,

g12 +...+gn2

where

g1, ..., gn

stand

we for

can conclude that each single |vis vjs | is distributed as: n independent copies of a random variable taken from

N (0, 1). Note that g12 + ... + gn2 is taken from the n2 - distribution. Using the well-known bounds

for

the

tails

of

n2 -

distributions,

we

get:

P[g12

+

...

+

gn2

-

n



a]



e-

a2 4n

.

Note

also

that

P[|g1|

>

x]



2 e-x22

.

Thus,

by

taking:

a

=

 n log(n),

x

=

log(n)

and

applying

union

bound,

we

conclude

x 2

15

Under review as a conference paper at ICLR 2018

that

with

probability

at

least

1 - e-

log2 (n) 4

- e-

log2 (n) 2

a

fixed

random

variable

|vis vjs |

satisfies:

|vis vjs |





log(n) 

.

n- n log(n)

Thus,

by

the

union

bound

we

conclude

that

for

any

fixed

i1, j1, ..., il, jl

random

variable

|vi1 vj1 | · ... · |vil vjl |

satisfies:

|vi1 vj1 | · ... · |vil vjl |



(

log(n) 

)l

n- n log(n)

with

probability

at

least

1

-

l(e-

log2 (n) 4

+

e-

log2 (n) 2

).

Since

|vi1 vj1 | · ... · |vil vjl |

is

upper

bounded

by

one, we conclude that:

E[|vi1 vj1 | · ... · |vil vjl |]  (

log(n) 

)l

+

l(e-

log2 (n) 4

+

e-

log2 (n) 2

).

n - n log(n)

(33)

Thus we can conclude that:

  (m - 1)lE[R2k] · (

log(n) 

)l

+

l(e-

log2 (n) 4

+ e )-

log2 (n) 2

n - n log(n)

(34)

Therefore we have:

k
k 
i=3

k i

(m - 1)iE[R2k] ·

(

log(n) 

)i

+

i(e-

log2 (n) 4

+ e )-

log2 (n) 2

n - n log(n)

Thus we can conclude that for n large enough:

(35)

k



k(2m)k

(

2

log3 n3
2

(n)

+

2ke-

log2 (n) 4

)E[R2k

].

Thus we get:


F=

z 2k(-1)kA(2k, n) (2k)!

k 2

E[(R2)k-22] + ,

k=0

where:


|| 
k=0

z

2k A(2k, (2k)!

n)

k(2m)k

(

2

log3 n3
2

(n)

+

2ke-

log2 (n) 4

)E[R2k

]

=

2

log3(n) n3
2

A

+

2e-

log2 (n) 4

B,

B satisfies: B = Ak and A is given as:

(36) (37)
(38)


A=

z 2kE[R2k]A(2k, n) k(2m)k.

(2k)!

k=0

(39)

Now note that since data is taken from the ball of radius r, we have: z  2r. Furthermore, from the smoothness of the considered class of RBF kernels, we obtain:

E[R2k ]



max
i=1,...,m

ni2k mk (n

-

1)(n

+

1)

·

..

·

(n

+

2k

-

3)f

k(k)k!.

(40)

Denote h

=

argmaxi=1,...,mni.

Note

that

(2k

- 1)!!

=

(2k)! 2k k!

.

Thus,

by

applying the

above

upper

bound on A(2k, n), we obtain:



A, B  (m2(2r)2nh2 f (k))kk2  (4m2(2r)2nh2 f (k))k.

k=0

k=0

(41)

Now, notice that for a given nh, m and r, the above upper bound is of the form

 k=0

qkk ,

where

qk  0 as k   (since fk  0 as k   for smooth RBF kernels). Then we can conclude that

16

Under review as a conference paper at ICLR 2018

the

contribution

of

the

terms



to

the

expression



=

(j1 ,...,jm ) j1 !·...·jm !

from

the

LHS

of

17

is

of

the

order

O(

1
3

)

(notice

that

every

ni

satisfies:

ni

 ji). Now let us consider F

- .

We have:

n2


F -=

z 2k(-1)kA(2k, n) (2k)!

k 2

E[(R2)k-22]

k=0

(42)

By the same analysis as before we conclude that

F

-



=



+

1 on( n ),

(43)

where


=

z 2k(-1)kA(2k, n) (2k)!

k 2

E[(R2)k-22]

k=0

and

futhermore:



=

1 n

~

+

on(

1 n

),

where

(44)


~ = W

z 2k(-1)kA(2k, n) (2k)!

k 2

E[(R2k )]

k=0

(45)

and W is some constant (that depends on m). Thus to show Inequality 17 for n large enough, it suffices to show that ~ > 0 and that  does not depend on n (even though n is explicitly embedded in the formula on ~ via A(2k, n)). This follows from the straightforward extension (for different nis) of the fact that for n1 = ... = nm, ~ can be rewritten in terms of the positive definite function  describing an RBF kernel under consideration, namely:

~ =

1 ((
8n

n1z

2

d2(m(x)) dx2

)|x=

n1 z

- ( n1z

d(m(x)) dx )|x= n1z ).

(46)

That the above expression is positive is implied by the fact that every positive definite function  (not parametrized by n) can be rewritten as (r) = (r2), where  is completely monotone on [0, +] and from the basic properties of completely monotone functions (see: Schoenberg (1938)).
That completes the proof of the theorem.

9 PROOF OF THEOREM 3
The theorem follows straightforwardly from Theorem 1, Theorem 2 and the fact that rank(K)  m (since m-dimensional random feature maps are used for kernel approximation).

17

