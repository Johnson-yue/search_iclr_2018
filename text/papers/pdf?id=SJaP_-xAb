Under review as a conference paper at ICLR 2018
DEEP LEARNING WITH LOGGED BANDIT FEEDBACK
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a new output layer for deep networks that permits the use of logged contextual bandit feedback for training. Such contextual bandit feedback can be available in huge quantities (e.g., logs of search engines, recommender systems) at little cost, opening up a path for training deep networks on orders of magnitude more data. To this effect, we propose a counterfactual risk minimization approach for training deep networks using an equivariant empirical risk estimator with variance regularization, BanditNet, and show how the resulting objective can be decomposed in a way that allows stochastic gradient descent training. We empirically demonstrate the effectiveness of the method in two scenarios. First, we show how deep networks ­ ResNets in particular ­ can be trained for object recognition without conventionally labeled images. Second, we learn to place banner ads based on propensity-logged click logs, where BanditNet substantially improves on the state-of-the-art.
1 INTRODUCTION
Log data can be recorded from online systems such as search engines, recommender systems, or online stores at little cost and in huge quantities. For concreteness, consider the interaction logs of an ad-placement system for banner ads. Such logs typically contain a record of the input to the system (e.g., features describing the user, banner ad, and page), the action that was taken by the system (e.g., a specific banner ad that was placed) and the feedback furnished by the user (e.g., clicks on the ad, or monetary payoff). This feedback, however, provides only partial information ­ "contextual-bandit feedback" ­ limited to the actions taken by the system. We do not get to see how the user would have responded, if the system had chosen a different action (e.g., other ads or banner types). Thus, the feedback for all other actions the system could have taken is typically not known. This makes learning from log data fundamentally different from traditional supervised learning, where "correct" predictions and a loss function provide feedback for all actions.
In this paper, we propose a new output layer for deep networks that allows training on logged contextual bandit feedback. By circumventing the need for full-information feedback, our approach opens a new and intriguing pathway for acquiring knowledge at unprecedented scale, giving deep networks access to this abundant and ubiquitous type of data. Similarly, it enables the application of deep learning even in domains where manually labeling full-information feedback is not viable.
In contrast to online learning with contextual bandit feedback (e.g., (Williams, 1992; Agarwal et al., 2014)), we perform batch learning from bandit feedback (BLBF) (Beygelzimer & Langford, 2009; Swaminathan & Joachims, 2015a;b;c) and the algorithm does not require the ability to make interactive interventions. At the core of the new output layer for BLBF training of deep networks lies a counterfactual training objective that replaces the conventional cross-entropy objective. Our approach ­ called BanditNet ­ follows the view of a deep network as a stochastic policy. We propose a counterfactual risk minimization (CRM) objective that is based on an equivariant estimator of the true error that only requires propensity-logged contextual bandit feedback. This makes our training objective fundamentally different from the conventional cross-entropy objective for supervised classification, which requires full-information feedback. To enable large-scale training, we show how this training objective can be decomposed to allow stochastic gradient descent (SGD) optimization.
In addition to the theoretical derivation of BanditNet, we present an empirical evaluation in two application domains. It illustrates the generality of the approach, demonstrating how very different network architectures can be trained in the BLBF setting. First, we derive a BanditNet version of
1

Under review as a conference paper at ICLR 2018
ResNets (He et al., 2016) for visual object classification. Despite using potentially much cheaper data, we find that Bandit-ResNets can achieve the same classification performance given sufficient amounts of contextual bandit feedback as ResNets trained with cross-entropy on conventionally (full-information) annotated images. Second, we find that BanditNet achieves state-of-the-art performance for placing banner ads on the Criteo benchmark (Lefortier et al., 2016) when learning a policy from propensity-logged click logs. To easily enable experimentation on other applications, we share open source implementations of both BanditNet instances.1
2 RELATED WORK
Several recent works have studied weak supervision approaches for deep learning. Weak supervision has been used to pre-train good image features (Joulin et al., 2016) and for information retrieval (Dehghani et al., 2017). Closely related works have studied label corruption on CIFAR10 recently (Zhang et al., 2016). However, all these approaches use weak supervision/corruption to construct noisy proxies for labels, and proceed with traditional supervised training (using crossentropy or mean-squared-error loss) with these proxies. In contrast, we work in the BLBF setting, which is an orthogonal data-source, and modify the loss functions optimized by deep nets to directly implement risk minimization.
Virtually all previous methods that can learn from logged bandit feedback employ some form of risk minimization principle (Vapnik, 1998) over a model class. Most of the methods (Beygelzimer & Langford, 2009; Bottou et al., 2013; Swaminathan & Joachims, 2015a) employ an inverse propensity scoring (IPS) estimator (Rosenbaum & Rubin, 1983) as empirical risk and use stochastic gradient descent (SGD) to optimize the estimate over large datasets. Recently, the self-normalized estimator (Trotter & Tukey, 1956) was shown to be a more suitable estimator for BLBF (Swaminathan & Joachims, 2015c). The self-normalized estimator, however, is not amenable to stochastic optimization and scales poorly with dataset size. In our work, we demonstrate how we can efficiently optimize a reformulation of the self-normalized estimator using SGD.
Previous BLBF methods focus on simple model classes: log-linear and exponential models (Swaminathan & Joachims, 2015a) or tree-based reductions (Beygelzimer & Langford, 2009). In contrast, we demonstrate how current deep learning models can be trained effectively via batch learning from bandit feedback (BLBF), and compare these with existing approaches on a benchmark dataset (Lefortier et al., 2016).
Our work, together with independent concurrent work (Serban et al., 2017), demonstrates success with off-policy variants of the REINFORCE (Williams, 1992) algorithm. In particular, our algorithm employs a Lagrangian reformulation of the self-normalized estimator, and the objective and gradients of this reformulation are similar in spirit to the updates of the REINFORCE algorithm. This connection sheds new light on the role of the baseline hyper-parameters in REINFORCE: rather than simply reduce the variance of policy gradients, our work suggests that the baseline is instrumental in creating an equivariant counterfactual learning objective.
3 BANDITNET: COUNTERFACTUAL RISK MINIMIZATION FOR DEEP NETS
To formalize the problem of batch learning from bandit feedback for deep networks, consider the contextual bandit setting where a policy  takes as input x  X and outputs an action y  Y. In response, we observe the loss (or payoff) (x, y) of the selected action y, where (x, y) is arbitrary (unknown) function that maps actions and contexts to a bounded real number. For example, in display advertising, the context x could be a representation of the user and page, y denotes the displayed ad, and (x, y) could be the monetary payoff from placing the ad (zero if no click, or dollar amount if clicked). The contexts are drawn i.i.d. from a fixed but unknown distribution Pr(X).
In this paper, a (deep) neural network is viewed as implementing a stochastic policy . We can think of such a network policy as a conditional distribution w(Y | x) over actions y  Y , where w are the parameters of the network. The network makes a prediction by sampling an action y  w(Y | x), where deterministic w(Y | x) are a special case. As we will show as part of the empirical
1URL to open-source implementation suppressed until publication to conform to blind-reviewing rules.
2

Under review as a conference paper at ICLR 2018

evaluation, many existing network architectures are compatible with this stochastic-policy view. For

example, any network fw(x, y) with a softmax output layer

w(y | x) =

exp(fw(x, y)) y Y exp(fw(x, y ))

(1)

can be re-purposed as a conditional distribution from which one can sample actions, instead of

interpreting it as a conditional likelihood like in full-information supervised learning.

The goal of learning is to find a policy w that minimizes the risk (analogously: maximizes the payoff) defined as

R(w) = E

E [(x, y)].

xPr(X) yw(Y |x)

(2)

Any data collected from an interactive system depends on the policy 0 that was running on the system at the time, determining which actions y and losses (x, y) are observed. We call 0 the logging policy, and for simplicity assume that it is stationary. The logged data D are n tuples of

observed context xi  Pr(X), action yi  0(Y | xi) taken by the logging policy, the probability of this action pi  0(yi | xi) which we call the propensity, and the received loss i  (xi, yi):

D = [(x1, y1, p1, 1) , . . . , (xn, yn, pn, n)] .

(3)

We will now discuss how we can use this logged contextual bandit feedback to train a neural network

policy w(Y | x) that has low risk R(w).

3.1 COUNTERFACTUAL RISK MINIMIZATION

While conditional maximum likelihood is a standard approach for training deep networks, it requires that the loss (xi, y) is known for all y  Y. However, we only know (xi, yi) for the particular yi chosen by the logging policy 0. We therefore take a different approach following (Langford et al., 2008; Swaminathan & Joachims, 2015b), where we directly minimize an empirical risk that can be
estimated from the logged bandit data D. This approach is called counterfactual risk minimization
(CRM) (Swaminathan & Joachims, 2015b), since for any policy w it addresses the counterfactual question of how well that policy would have performed, if it had been used instead of 0.

While minimizing an empirical risk as an estimate of the true risk R(w) is a common principle in machine learning (Vapnik, 1998), getting a reliable estimate based on the training data D produced
by 0 is not straightforward. D is not only incomplete (i.e., we lack knowledge of (xi, y) for many y  Y that w would have chosen differently from 0), but it is also biased (i.e., the actions preferred by 0 are overrepresented). This is why existing work on training deep networks either requires full knowledge of the loss function, or requires the ability to interactively draw new samples yi  w(Y | xi) for any new policy w. In our setting we can do neither ­ we have a fixed dataset D that is limited to samples from 0.

To nevertheless get a useful estimate of the empirical risk, we explicitly address both the bias and

the variance of the risk estimate. To correct for sampling bias and handle missing data, we approach

the risk estimation problem using importance sampling and thus remove the distribution mismatch

between 0 and w (Langford et al., 2008; Owen, 2013; Swaminathan & Joachims, 2015b):

R(w) = E

E [(x, y)] = E

E

xPr(X) yw(Y |x)

xPr(X) y0(Y |x)

(x, y) w(y | x) 0(y | x)

,

(4)

The latter expectation can be estimated on a sample D of n bandit-feedback examples using the

following IPS estimator (Langford et al., 2008; Owen, 2013; Swaminathan & Joachims, 2015b):

R^IPS (w)

=

1 n

n i=1

i

w (yi 0(yi

| |

xi) xi)

.

(5)

This IPS estimator is unbiased and has bounded variance, if the logging policy has full support in

the sense that x, y : 0(y | x)  > 0. While at first glance it may seem natural to directly train the parameters w of a network to optimize this IPS estimate as an empirical risk, there are at

least three obstacles to overcome. First, we will argue in the following section that the naive IPS

estimator's lack of equivariance makes it sub-optimal for use as an empirical risk for high-capacity

models. Second, we have to find an efficient algorithm for minimizing the empirical risk, especially

making it accessible to stochastic gradient descent (SGD) optimization. And, finally, we are faced

with an unusual type of bias-variance trade-off since "distance" from the exploration policy impacts

the variance of the empirical risk estimate for different w.

3

Under review as a conference paper at ICLR 2018

3.2 EQUIVARIANT COUNTERFACTUAL RISK MINIMIZATION

While Eq. (5) provides an unbiased empirical risk estimate, it exhibits the ­ possibly severe ­ prob-
lem of "propensity overfitting" when directly optimized within a learning algorithm (Swaminathan
& Joachims, 2015c). It is a problem of overfitting to the choices yi of the logging policy, and it occurs on top of the normal overfitting to the i. Propensity overfitting is linked to the lack of equivariance of the IPS estimator: while the minimizer of true risk R(w) does not change when translating the loss by a constant (i.e., x, y : (x, y) + c) by linearity of expectation, the minimizer of the IPS-estimated empirical risk R^IPS (w) can change dramatically for finite training samples. Intuitively, when c shifts losses to be positive numbers, policies w that put as little probability mass as possible on the observed actions have low risk estimates. If c shifts the losses to the negative
range, the exact opposite is the case. For either choice of c, the choice of the policy eventually
selected by the learning algorithm can be dominated by where 0 happens to sample data, not by which actions have low loss.

The following self-normalized IPS estimator (SNIPS) adresses the propensity overfitting problem (Swaminathan & Joachims, 2015c) and is equivariant:

R^SNIPS (w) =

1 n
1

n w(yi|xi)
i=1 i 0(yi|xi) .n w(yi|xi)

n i=1 0(yi|xi)

(6)

In addition to being equivariant, this estimate can also have substantially lower variance than Eq. (5), since it exploits the knowledge that the denominator

S

:=

1 n w(yi | xi) n i=1 0(yi | xi)

(7)

always has expectation 1:

1n E[S] = n
i=1

w (yi 0(yi

| |

xi) xi)

0

(yi

|

xi) Pr(xi)dyidxi

=

1 n

n i=1

1 Pr(xi)dxi = 1.

(8)

The SNIPS estimator uses this knowledge as a multiplicative control variate (Swaminathan &

Joachims, 2015c). While the SNIPS estimator has some bias, this bias asymptotically vanishes

at

a

rate

of

O(

1 n

)

(Hesterberg,

1995).

Using

the

SNIPS

estimator

as

our

empirical

risk

implies

that

we need to solve the following optimization problem for training:

w^ = arg min R^SNIPS (w).
w N

(9)

Thus, we now turn to designing efficient optimization methods for this training objective.

3.3 TRAINING ALGORITHM

Unfortunately, the training objective in Eq. (9) does not permit stochastic gradient descent (SGD) optimization in the given form (see Appendix C), which presents an obstacle to efficient and effective training of the network. To remedy this problem, we will now develop a reformulation that retains both the desirable properties of the SNIPS estimator, as well as the ability to reuse established SGD training algorithms. Instead of optimizing a ratio as in Eq. (9), we will reformulate the problem into a series of constrained optimization problems. Let w^ be a solution of Eq. (9), and at that solution let S be the value of the control variate for w^ as defined in Eq. (7). For simplicity, assume that the minimizer w^ is unique. If we knew S, we could equivalently solve the following constrained optimization problem:

w^

=

arg min
w N

1 n

n i=1

i

w (yi 0(yi

| |

xi) xi)

subject to

1 n w(yi | xi) = S. n i=1 0(yi | xi)

(10)

Of course, we do not actually know S. However, we can do a grid search in {S1, . . . , Sk} for S and solve the above optimization problem for each value, giving us a set of solutions {w^1, . . . , w^k}.
Note that S is just a one-dimensional quantity, and that the sensible range we need to search for

4

Under review as a conference paper at ICLR 2018

S concentrates around 1 as n increases (see Appendix B). To find the overall (approximate) w^ that optimizes the SNIPS estimate, we then simply take the minimum:

1
w^ = arg min n
(w^j ,Sj )

n w^j (yi|xi)
 .i=1 i 0(yi|xi) Sj

(11)

This still leaves the question of how to solve each equality constrained risk minimization problem using SGD. Fortunately, we can perform an equivalent search for S without constrained optimization. To this effect, consider the Lagrangian of the constrained optimization problem in Eq. (10) with Sj in the constraint instead of S:

1 L(w, ) =

n

iw(yi | xi) -

1

n
i=1

0(yi | xi)

n

n w(yi | xi) i=1 0(yi | xi)

-Sj

=

1 n

n (i -)w(yi | i=1 0(yi | xi)

xi) +Sj.

The variable  is an unconstrained Lagrange multiplier. To find the minimum of Eq. (10) for a particular Sj, we need to minimize L(w, ) w.r.t. w and maximize w.r.t. .

w^j = arg min max L(w, )
w N 

(12)

However, we are not actually interested in the constrained solution of Eq. (10) for any specific Sj, we are merely interested in exploring a certain range S  [S1, Sk] in our search for S. So, we can reverse the roles of  and S, where we keep  fixed and determine the corresponding S in hindsight. In particular, for each {1, . . . , k} we solve

w^j = arg min L(w, j).
w N

(13)

Note that the solution w^j does not depend on Sj, so we can compute Sj after we have found the minimum w^j. In particular, we can determine the Sj that corresponds to the given j using the necessary optimality conditions,

L 1 =

n

w(yi | xi) (i - j)

=0

w n
i=1

w

0(yi | xi)

and

L j

=

1 n

n i=1

w(yi | xi) 0(yi | xi)

- Sj

= 0,

(14)

by solving the second equality of Eq. (14). In this way, the sequence of j produces solutions w^j corresponding to a sequence of {S1, . . . , Sk}.

To identify the sensible range of S to explore, we can make use of the fact that Eq. (7) concentrates around its expectation of 1 for each w as n increases. Theorem 2 in Appendix B provides a characterization of how large the range needs to be. Furthermore, we can steer the exploration of S via , since the resulting S changes monotonically with :

(a < b) and (w^a = w^b are not equivalent optima in Eq. (13))  (Sa < Sb).

(15)

A more formal statement and proof are given as Theorem 1 in Appendix A. In the simplest form one could therefore perform a grid search on , but more sophisticated search methods are possible too.

After this reformulation, the key computational problem is finding the solution of Eq. (13) for each

j. Note that in this unconstrained optimization problem, the Lagrange multiplier effectively translates the loss values in the conventional IPS estimate:

w^j

=

arg min
w

1 n

n i=1

(i

-

j

)

w (yi 0(yi

| |

xi) xi)

=

arg min R^IPj S (w).
w

(16)

We denote this -translated IPS estimate with R^IPS (w). Similar loss translations have previously been used as an ad-hoc heuristic in off-policy reinforcement learning (Williams, 1992), but our ar-

gument provides a rigorous justification and a constructive way of picking the value of  ­ namely

the value for which the corresponding Sj minimizes Eq. (11). We can further add variance regularization (Swaminathan & Joachims, 2015b) to improve the robustness of the risk estimate in Eq. (16)

(see Appendix D for details).

Note that each such optimization problem is now in the form required for SGD, where we merely weight the derivative of the stochastic policy network w(y | x) by a factor (i - j)/0(yi | xi). This opens the door for re-purposing existing fast methods for training deep networks, and we demonstrate experimentally that both Adam (Kingma & Ba, 2014) and SGD with momentum are
able to optimize our objectives scalably.

5

Under review as a conference paper at ICLR 2018

15 Bandit-ResNet
14 FullInfo ResNet with CrossE

13

Error Rate (test)

12

11

10

9

8 50000

100000

150000

200000

250000

Number of Bandit-Feedback Examples

Figure 1: Learning curve of BanditNet. The x-axis is the amount of bandit feedback, the y-axis is

the test error. Given enough bandit feedback, Bandit-ResNet converges to the skyline performance.

4 EMPIRICAL EVALUATION
We evaluate BanditNet on two tasks ­ visual object recognition and ad placement ­ serving two different purposes. For the visual object recognition task, we will generate synthetic contextual bandit feedback data from full-information labels, which allows us to compare how the same deep network architecture performs under different types of data and training objectives. The ad placement task is a real-world verification of our approach, since it uses real logged interaction data from Criteo and allows benchmarking against other learning approaches for ad placement.
4.1 EXPERIMENT 1: VISUAL OBJECT RECOGNITION
In order to demonstrate that deep networks can be trained using our BanditNet methodology, we adapted the ResNet20 architecture (He et al., 2016) by replacing the conventional cross-entropy objective with our counterfactual risk minimization objective. We evaluate the performance of this Bandit-ResNet on the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset, where we can compare training on full-information data with training on bandit feedback, and where there is a full-information test set for estimating prediction error.
To simulate logged bandit feedback, we perform the standard supervised to bandit conversion (Beygelzimer & Langford, 2009). We use a hand-coded logging policy that achieves about 49% error rate on the training data, which is substantially worse than what we hope to achieve after learning. This emulates a real world scenario where one would bootstrap an operational system with a mediocre policy (e.g., derived from a small hand-labeled dataset) and then deploys it to log bandit feedback. This logged bandit feedback data is then used to train the Bandit-ResNet.
We evaluate the trained model using error rate on the held out (full-information) test set. We compare this model against the skyline of training a conventional ResNet using the full-information feedback from the 50,000 training examples. Both the conventional full-information ResNet as well as the Bandit-ResNet use the same network architecture, the same hyperparameters, the same data augmentation scheme, and the same optimization method that were set in the CNTK implementation of Resnet. Since CIFAR10 does not come with a validation set for tuning the variance-regularization constant , we do not use variance regularization for Bandit-ResNet. The Lagrange multiplier   {0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05} is selected on the training set via Eq. (11). The only parameter we adjusted for Bandit-ResNet is lowering the learning rate to 0.1 and slowing down the learning rate schedule. The latter was done to avoid confounding the Bandit-ResNet results with potential effects from early stopping, and we report test performance after 1000 training epochs, which is well beyond the point of convergence in all runs.
Learning curve. Figure 1 shows the prediction error of the Bandit-ResNet as more and more bandit feedback is provided for training. First, even though the logging policy that generated the bandit feedback has an error rate of 49%, the prediction error of the policy learned by the Bandit-ResNet is substantially better. It is between 13% and 8.2%, depending on the amount of training data. Second, the horizontal line is the performance of a conventional ResNet trained on the full-information training set. It serves as a skyline of how good Bandit-ResNet could possibly get given that it is sampling bandit feedback from the same full-information training set. The learning curve in Figure 1 shows
6

Under review as a conference paper at ICLR 2018

Error Rate (test) SNIPS Error Estimate (train)
Value of Control Variate

30 30 1.4

Error Rate (test)

SNIPS Error Estimate (train)

Control Variate

1.3

25 25

1.2

20 20 1.1

15 15 1

0.9 10 10
0.8 55
0.7

0 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05

0 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05

Lagrange Multiplier (lambda)

Lagrange Multiplier (lambda)

Figure 2: The x-axis shows the value of the Lagrange multiplier  used for training. Left plot shows

the test error. Right plot shows the value of the SNIPS objective and the normalizer S. The size of

the training set is 50k bandit-feedback examples.

that Bandit-ResNet converges to the skyline performance given enough bandit feedback training data, providing strong evidence that our training objective and method can effectively extract the available information provided in the bandit feedback.
Effect of the choice of Lagrange multiplier. The left-hand plot in Figure 2 shows the test error of solutions w^j depending on the value of the Lagrange multiplier j used during training. It shows that  in the range 0.8 to 1.0 results in good prediction performance, but that performance degrades outside this area. The SNIPS estimates in the right-hand plot of Figure 2 roughly reflects this optimal range, given empirical support for both the SNIPS estimator and the use of Eq. (11). We also trained a network using the IPS estimator as the objective (i.e.,  = 0), which lead to prediction performance worse than that of the logging policy (not shown).
Also shown in the right-hand plot of Figure 2 is the value of the control variate in the denominator of the SNIPS estimate. As expected, it increases from below 1 to above 1 as  is increased. Note that large deviations of the control variate from 1 are a sign of propensity overfitting (Swaminathan & Joachims, 2015c). In particular, for all solutions w^j the estimated standard error of the control variate Sj was less than 0.013, meaning that the normal 95% confidence interval for each Sj is contained in [0.974, 1.026]. If we see a w^j with control variate Sj outside this range, we should be suspicious of propensity overfitting to the choices of the logging policy and discard this solution.
4.2 EXPERIMENT 2: CRITEO AD PLACEMENT
As a real-world validation of our approach, we also evaluate BanditNet on bandit feedback from Criteo's display advertising system (Lefortier et al., 2016). We consider the problem of filling a banner ad with a product the user may want to purchase. This part of the system takes place after the bidding agent has won the auction. In this context, each ad has a number of candidate products and the task is to choose the product to display in the ad in order to maximize the number of clicks. Note that BanditNet does not aim to predict click-through rate (CTR), but that BanditNet directly learns a policy that optimizes CTR.
We use the ad placement dataset consisting of 2.13e+07 bandit-feedback examples made available by Lefortier et al. (2016). The data was collected using a stochastic logging policy derived from the production system. The average inverse propensity is 11.96 and the largest is 5.36e+05. After binarizing the categorical features, there are 73989 binary features.
We evaluate BanditNet for two network architectures. The first is a linear model comparable to the one used in POEM. The second is a two-layer network with a tanh activation function and 50 hidden units. This architecture and the number of hidden units was chosen for simplicity, and other architectures may further improve performance. The hyperparameters , variance regularization , learning rate, l2 and l1 regularizations are selected to optimize IPS-estimated performance on the validation set. We report IPS-estimated performance on the test set, which is an unbiased estimate of generalization performance.
Comparison against baselines. Table 1 compares BanditNet against the following baselines as implemented in Vowpal Wabbit (VW) (Langford et al., 2007): 1. Random ­ Policy that picks product
7

Under review as a conference paper at ICLR 2018

Table 1: Test set clickthrough rate (104) on Criteo ad-placement benchmark. IPS estimate with one standard error for significance assessment.

Approach

Test performance

Random Logging Policy

44.676±0.819 53.540±0.087

Regression Regression (pairwise) IPS DRO POEM

48.353±1.261 51.043±1.573 54.125±0.976 57.356±5.430 58.040±1.321

BanditNet (linear) BanditNet (2-layer, w/o context) BanditNet (2-layer)

58.025±0.962 62.263±0.541 69.346±1.261

uniformly at random. 2. Regression ­ Learns a click predictor (i.e., predicts ) and chooses the product with highest estimated click probability. The hyperparameters are the number of training epochs, regularization for Lasso, and learning rate for SGD. 3. IPS ­ Optimizes the IPS estimator through a reduction to weighted one-against-all multi-class classification (Dud´ik et al., 2011). The hyperparameters are the same as in the Regression approach. 4. DRO (doubly robust optimizer) ­ Combines the Regression method with IPS using the doubly robust estimator. 5. POEM ­ A policy learning method (Swaminathan & Joachims, 2015c) which fits a linear model to optimize Eq. (6).
Table 1 shows that the non-linear BanditNet significantly and substantially outperforms the existing methods. Of the existing methods, the Regression approach (i.e., click-predictor) performs most poorly, and we conjecture that its model bias is responsible. Even when introducing pairwise features to increase the capacity of the model and thus reduce model bias, it still performs worse than the logging policy. Of the existing policy learning baselines, POEM outperforms IPS and DRO. As expected, the performance of the linear BanditNet is close to POEM, since they both use the same linear model and optimize similar objectives. However, the two-layer BanditNet substantially improves on this.
Using context-features. To understand where these improvements are coming from and whether the network can effectively learn non-linear models from bandit feedback, we also trained a 2-layer BanditNet where we withheld a subset of the features, called context features. These are features that are the same for all candidate actions and thus do not affect linear models. The difference between the last two rows of Table 1 shows that BanditNet can effectively model non-linear interactions involving those features, substantially improving performance. We conjecture that more sophisticated network architectures can further increase accuracy.
5 CONCLUSIONS AND FUTURE WORK
We proposed a new output layer for deep networks that enables the use of logged contextual bandit feedback for training. This type of feedback is abundant and ubiquitous in the form of interaction logs from autonomous systems, opening up the possibility of training deep networks on unprecedented amounts of data. In principle, this new output layer can replace the conventional cross-entropy layer for any network architecture. We provide a rigorous derivation of the training objective, linking it to an equivariant counterfactual risk estimator that enables counterfactual risk minimization. Most importantly, we show how the resulting training objective can be decomposed and reformulated to make it feasible for SGD training. Across two application domains ­ visual object recognition and ad placement ­ and very different network architectures, we find that the BanditNet approach achieves excellent predictive accuracy.
The paper opens up several directions for future work. First, it enables many new applications where contextual bandit feedback is readily available. Second, in settings where it is infeasible to log propensity-scored data, it would be interesting to combine BanditNet with propensity estimation techniques. Third, there may be improvements to BanditNet, like smarter search techniques for S, more efficient counterfactual estimators beyond SNIPS, and the ability to handle continuous outputs.
8

Under review as a conference paper at ICLR 2018
REFERENCES
A. Agarwal, D. Hsu, S. Kale, J. Langford, Lihong Li, and R. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In ICML, 2014.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In KDD, pp. 129­138, 2009.
L. Bottou, J. Peters, J. Quinonero-Candela, D. Charles, M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. JMLR, 14:3207­3260, 2013.
M. Dehghani, H. Zamani, A. Severyn, J. Kamps, and W. B. Croft. Neural Ranking Models with Weak Supervision. ArXiv e-prints, 2017. http://arxiv.org/abs/1704.08803.
M. Dud´ik, J. Langford, and Lihong Li. Doubly robust policy evaluation and learning. In ICML, pp. 1097­1104, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
T. Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37:185­194, 1995.
A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache. Learning visual features from large weakly supervised data. In ECCV, pp. 67­84, 2016.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Computer Science Department, University of Toronto, 2009.
J. Langford, Lihong Li, and A. Strehl. Vowpal Wabbit. Technical report, Yahoo!, 2007.
J. Langford, A. Strehl, and J. Wortman. Exploration scavenging. In ICML, pp. 528­535, 2008.
D. Lefortier, A. Swaminathan, Xiaotao Gu, T. Joachims, and M. de Rijke. Large-scale validation of counterfactual learning methods: A test-bed. CoRR, abs/1612.00367, 2016.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
P. Rosenbaum and D. Rubin. The central role of propensity score in observational studies for causal effects. Biometrica, 70:41­55, 1983.
I. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian, T. Kim, M. Pieper, S. Chandar, N. R. Ke, S. Mudumba, A. de Brebisson, J. M. R. Sotelo, D. Suhubdy, V. Michalski, A. Nguyen, J. Pineau, and Y. Bengio. A Deep Reinforcement Learning Chatbot. ArXiv e-prints, September 2017.
A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In ICML, pp. 814­823, 2015a.
A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. JMLR, 16:1731­1755, Sep 2015b.
A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In NIPS, 2015c.
H. F. Trotter and J. W. Tukey. Conditional monte carlo for normal samples. In Symposium on Monte Carlo Methods, pp. 64­79, 1956.
V. Vapnik. Statistical Learning Theory. Wiley, Chichester, GB, 1998.
R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4), May 1992.
Chiyuan Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016.
9

Under review as a conference paper at ICLR 2018

A APPENDIX: STEERING THE EXPLORATION OF S THROUGH .

Theorem 1. Let a < b and let

w^a = arg min R^IPaS (w)
w
w^b = arg min R^IPbS (w).
w

(17) (18)

If the optima w^a and w^b are not equivalent in the sense that R^IPaS (w^a ) = R^IPaS (w^b ) and R^IPbS (w^a ) = R^IPbS (w^b ), then

Sa < Sb.

(19)

Proof.

Abbreviate f (w) =

1 n

n w(yi|xi)
i=1 i 0(yi|xi)

and g(w)

=

1 n

n i=1

.w (yi |xi )
0 (yi |xi )

Then

R^IPS (w) = f (w) - g(w),

(20)

where g(w) corresponds to the value of the control variate S. Since w^a and w^b are not equivalent optima, we know that

f (w^a) - a g(w^a) < f (w^b) - a g(w^b) f (w^b) - b g(w^b) < f (w^a) - b g(w^a)

(21) (22)

Adding the two inequalities and solving implies that

 f (w^a) - a g(w^a) + f (w^b) - b g(w^b) < f (w^b) - a g(w^b) + f (w^a) - b g(w^a) (23)

 a g(w^a) + b g(w^b) > a g(w^b) + b g(w^a)

(24)

 (b - a) g(w^b) > (b - a) g(w^a)

(25)

 g(w^b) > g(w^a)

(26)

 Sb > Sa

(27)

B APPENDIX: CHARACTERIZING THE RANGE OF S TO EXPLORE.

Theorem 2. Let p  0(y | x) be a lower bound on the propensity for the logging policy, then constraining the solution of Eq. (9) to the w with control variate S  [1 - , 1 + ] for a training set
of size n will nevertheless recover the optimal w^ of Eq. (9) without this constraint with probability
at least

1 - 2 exp -2n 2p2 .

(28)

Proof. For the optimal w^, let

S = n w^(yi | xi) i=1 0(yi | xi)

(29)

be the control variate in the denominator of the SNIPS estimator. S is a random variable that is a sum of bounded random variables between 0 and

max

w^ (y

|

x)



1 .

x,y 0(y | x) p

(30)

We can bound the probability that the control variate S of the optimum w^ lies outside of [1 - , 1 + ] via Hoeffding's inequality:

P (|S - 1|  )



2 exp

-2n2 2 n(1/p)2

= 2 exp -2n 2p2 .

(31) (32)

10

Under review as a conference paper at ICLR 2018

C APPENDIX: WHY DIRECT STOCHASTIC OPTIMIZATION OF RATIO ESTIMATORS IS NOT POSSIBLE.

Suppose we have a dataset of n BLBF samples D = {(x1, y1, 1, p1) . . . (xn, yn, n, pn)} where each instance is an iid sample from the data generating distribution. In the sequel we will be considering two datasets of n + 1 samples, D = D  (x , y ,  , p ) and D = D  (x , y ,  , p ) where (x , y ,  , p ) = (x , y ,  , p ) and (x , y ,  , p ), (x , y ,  , p ) / D.

For

notational convenience,

let

fi

:=

i

w (yi|xi) 0 (yi |xi )

,

and

fi

:=

w fi ;

gi

:=

,w (yi |xi )
0 (yi |xi )

and gi

:=

w gi .

First consider the vanilla IPS risk estimate of Eqn (5).

R^IPS (w)

=

1 n

n i=1

i

w (yi 0(yi

| |

xi) xi)

=

1 n

n
fi.
i=1

To maximize this estimate using stochastic optimization, we must construct an unbiased gradient estimate. That is, we randomly select one sample from D and compute a gradient ((xi, yi, i, pi)) and we require that

wR^IPS (w)

=

1 n

n

fi = EiD [((xi, yi, i, pi))] .

i=1

Here the expectation is over our random choice of 1 out of n samples. Observe that ((xi, yi, i, pi)) = fi suffices (and indeed, this corresponds to vanilla SGD).

EiD [((xi, yi, i, pi))] =

n

11 n ((xi, yi, i, pi)) = n

n

fi = wR^IPS (w).

i=1 i=1

Other choices of (·) can also produce unbiased gradient estimates, and this leads to the study of stochastic variance-reduced gradient optimization.

Now let us attempt to construct an unbiased gradient estimate for Eqn (6).

R^SNIPS (w) =

n i=1 n i=1

fi gi

.

Suppose such a gradient estimate exists, ((xi, yi, i, pi)). Then,

wR^SNIPS (w) = w

n i=1

fi

n i=1

gi

=

EiD [((xi, yi, i, pi))]

=

1 n

n
((xi, yi, i, pi)).
i=1

This identity is true for any sample of BLBF instances ­ in particular, for D and D .

w

n i=1

fi

+

f

n i=1

gi

+

g

n1

((x , y ,  , p ))

= n + 1 ((xi, yi, i, pi)) +

n+1

,

i=1

w

n i=1

fi

+

f

n i=1

gi

+

g

n1

((x , y ,  , p ))

= n + 1 ((xi, yi, i, pi)) +

n+1

.

i=1

Subtracting these two equations,

w

n i=1

fi

+

f

n i=1

gi

+

g

-

n i=1

fi

+

f

n i=1

gi

+

g

((x , y ,  , p )) - ((x , y ,  , p )) =.
n+1

The LHS clearly depends on {(xi, yi, i, pi)}in=1 in general, while the RHS does not! This contradiction indicates that no construction of  that only looks at a sub-sample of the data can yield an
unbiased gradient estimate of R^SNIPS (w).

11

Under review as a conference paper at ICLR 2018

D APPENDIX: VARIANCE REGULARIZATION

Unlike in conventional supervised learning, a counterfactual empirical risk estimator like R^IPS (w) can have vastly different variances Var(R^IPS (w)) for different w in the hypothesis space (and R^SNIPS (w) as well) (Swaminathan & Joachims, 2015b). Intuitively, the "closer" the particular w is to the exploration policy 0, the larger the effective sample size (Owen, 2013) will be and the smaller the variance of the empirical risk estimate. For the optimization problems we solve in Eq. (16), this means that we should trust the -translated risk estimate R^IPjS (w) more for some w than for others, as we use R^IPj S (w) only as a proxy for finding the policy that minimizes its expected value (i.e., the true loss). To this effect, generalization error bounds that account for this
variance difference (Swaminathan & Joachims, 2015b) motivate a new type of overfitting control.
This leads to the following training objective (Swaminathan & Joachims, 2015b), which can be
thought of as a more reliable version of (16):



w^j = arg min R^IPj S (w) + 
w

Var(R^IPj S (w))  . n

(33)

Here, Var(R^IPj S (w)) is the estimated variance of R^IPj S (w) on the training data, and  is a regularization constant to be selected via cross validation. The intuition behind this objective is that we optimize the upper confidence interval, which depends on the variance of the risk estimate for each w. While this objective again does not permit SGD optimization in its given form, it has been shown that a Taylor-majorization can be used to successively upper bound the objective in Eq. (33), and that typically a small number of iterations suffices to converge to a local optimum (Swaminathan & Joachims, 2015b). Each such Taylor-majorization is again of a form

1 n A w(yi | xi) + B w(yi | xi) 2

n
i=1

0(yi | xi)

0(yi | xi)

(34)

for easily computable constants A and B (Swaminathan & Joachims, 2015b), which allows for SGD optimization.

12

