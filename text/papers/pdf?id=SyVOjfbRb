Under review as a conference paper at ICLR 2018

LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION
Anonymous authors Paper under double-blind review

ABSTRACT
Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which creates a chicken-and-egg loop making the fast convergence useless. In this paper, we break this barrier by providing the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.

1 MOTIVATION

Stochastic gradient descent or popularly known as SGD is the most popular choice of optimization in large-scale setting for its computational efficiency. A typical interest in machine learning is to minimize the average loss function f over the training data, with respect to the parameters , i.e., the objective function of interest is

 = arg min F () = arg min 1  N

N

f (xi, )

i=1

(1)

Throughout the paper, our training data D = {xi, yi}iN=1 will have N instances with xi  Rd d dimensional features and yi will be labels. The labels can be real valued for regression problems. For classification problem, they will take value in a discrete set, i.e., yi  {1, 2, · · · , K}. Typically, the function f is a convex function. The popular ones include least squares f (xi, ) = ( · xi - yi)2,
used in regression setting.

During the optimization, SGD (Bottou, 2010) samples an instance xj, uniformly at random from among the N instances, and performs the gradient descent update

t = t-1 - tf (xj , t-1)

(2)

where the gradient f (xj, t-1) is only evaluated on xj, using the current t-1. Here, t is the step size at the tth iteration.

It

should

be

noted

that

a

full

gradient

of

the

objective

is

given

by

the

average

1 N

N i=1

f

(xi,

t-1

).

Thus, a uniformly sampled gradient f (xj, t-1) is an unbiased estimator of the full gradient, i.e.,

1N

E(f (xj, t-1)) = N

f (xi, t-1).

i=1

(3)

This is the key reason why despite only using one sample, SGD still converges to the local minima, analogous to full gradient descent, provided t is chosen properly (Robbins & Monro, 1951; Bottou,
2010).

1

Under review as a conference paper at ICLR 2018
However, it is known that the convergence rate of SGD is slower than that of the full gradient descent (Shamir & Zhang, 2013). Nevertheless, the cost of computing the full gradient requires O(N ) evaluations of f compared to just constant evaluation in SGD. Thus, with the cost of one epoch of full gradient descent, SGD can perform O(N ) epochs, which overcompensates the slow convergence. Therefore, despite fast convergence rates, SGD is almost always the algorithm for choice in large-scale settings as the full calculation of gradient in every epoch is prohibitively slow. Further improving the speed of SGD is still an active area of exploration. Any improvement in the speed of SGD will directly speed up almost all the state-of-the-art in machine learning.
As expected, the slower convergence of SGD is due to the poor estimation of the gradient (the average) by only sampling a single instance uniformly. Clearly, the variance of the one sample estimator is high. As a result, there have been several efforts in finding better sampling strategies for better estimation of the gradients (Zhao & Zhang, 2014; Needell et al., 2014; Zhao & Zhang, 2015; Alain et al., 2015). The key idea behind these methods is to replace the uniform distribution with a weighted distribution with better variance. The weighted distribution has to change (adaptive) in every iteration, when the parameters and the gradients change. Unfortunately, as argued in (Gopal, 2016), all of these methods suffer from what we call the chicken-and-egg loop ­ maintaining any adaptive non-uniform distribution itself cost O(N ) per iteration. It is also the cost of computing the gradient exactly.
To the best of our knowledge, there does not exist any generic sampling scheme for adaptive gradient estimation, where the cost of maintaining and updating the distribution, per update, is constant O(1). It is also comparable in update time with stochastic gradient sampling. Our work provides first such sampling scheme utilizing the recent advances in sampling and unbiased estimation using Locality Sensitive Hashing (Spring & Shrivastava, 2017).
1.1 ADAPTIVE SAMPLING FOR SGD
For non-uniform sampling, we can sample each xi with an associated weight wi. These wi's can be tuned to minimize the variance. It was first shown in (Alain et al., 2015), that sampling xi in proportion to the L2 norm of the gradient, i.e. ||f (xi, t-1)||2, is the optimal distribution that minimizes the variance. However, sampling xi in proportion to wi = ||f (xi, t-1)||2, requires first computing all the N , wi's, which change with every update because the t-1 get updated. Thus, with every epoch just to maintain the values of wi's is costlier than computing the full gradient. (Gopal, 2016) proposed to mitigate this overhead partially by exploiting additional side information such as the cluster structure of the data. Prior to the realization of optimal variance distribution, (Zhao & Zhang, 2014) and (Needell et al., 2014) proposed to sample a training instance with a probability proportional to the Lipschitz constant of the function f (xi, t-1) and f (xi, t-1) respectively. Again, as argued, in (Gopal, 2016), the cost of maintaining the distribution is prohibitive.
It is worth mentioning that before these works, a very similar idea was used in designing importance sampling-based low-rank matrix approximation algorithms. The resulting sampling methods, popularly known as leverage score sampling, are again proportional to the squared Euclidean norms of rows and columns of the underlying matrix. See (Drineas et al., 2012).
The Chicken-and-Egg Loop: In summary, to speed up the convergence of stochastic gradient descent, we need non-uniform sampling for better estimates (low variance) of the full gradient. Any interesting non-uniform sampling is dependent on the data and the parameter t which changes in every iteration. Thus, maintaining the non-uniform distribution requires O(N ) computations to merely compute the weights wi, which is the same cost of computing the full gradient. It is not even clear if there is any sweet and adaptive distribution which breaks this computational chicken-andegg loop. We provide the first affirmative answer by giving an unusual distribution which is derived from probabilistic indexing based on locality sensitive hashing.
Our Contributions: In this work, we propose a novel LSH-based samplers, that breaks the aforementioned chicken-and-egg loop. Our algorithm, which we call LSD (LSH Sampled Stochastic gradient Descent), are generated via hash lookups which cost less than one dot product and the probability of selection xi is provably adaptive. As a result, the current gradient estimates have lower variance, compared to a single sample SGD, while the computational complexity of sampling is constant and of the order of SGD sampling.
2

Under review as a conference paper at ICLR 2018
As a direct consequence, we obtain a generic and faster gradient descent algorithm which converges significantly faster than SGD, both concerning epochs as well as running time. It should be noted that simply epoch wise rapid convergence does not imply computational efficiency. Newtons method converges faster, epoch wise, than any first-order gradient descent, but it is prohibitively slow in practice. The wall clock time of convergence or the amount of floating point operations to converge should be the metric of consideration for conclusive comparisons.
Accuracy Vs Running Time: It is rare to see any fair (same computational setting) empirical comparison of SGD with any adaptive SGD scheme, which compares the growth in accuracy with running time on the same computational platform. Almost all methods compare accuracy with the number of epochs, which is unfair to SGD which can complete O(N ) epochs at the computational cost (or running time) of 1 epoch for adaptive sampling schemes.
2 BACKGROUND
We first describe a recent advancement in the theory of sampling and estimation using locality sensitive hashing (LSH) (Indyk & Motwani, 1998) which will be heavily used in our proposal. Before we go in sampling, let us revise the two-decade-old theory of LSH.
2.1 LOCALITY SENSITIVE HASHING (LSH)
Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998) is a popular, sub-linear time algorithm for approximate nearest-neighbor search. The high-level idea is to place similar items into the same bucket of a hash table with high probability. An LSH hash function maps an input data vector to an integer key
h(x) : RD  [0, 1, 2, . . . , N ]. A collision occurs when the hash values for two data vectors are equal - h(x) = h(y). The collision probability of most LSH hash functions is generally a monotonic function of the similarity -
P r[h(x) = h(y)] = M(sim(x, y))
where M is a monotonically increasing function. Essentially, similar items are more likely to collide with each other under the same hash fingerprint.
The algorithm uses two parameters - (K, L). We construct L independent hash tables from the collection C. Each hash table has a meta-hash function H that is formed by concatenating K random independent hash functions from F. Given a query, we collect one bucket from each hash table and return the union of L buckets. Intuitively, the meta-hash function makes the buckets sparse (less crowded) and reduces the number of false positives because only valid nearest-neighbor items are likely to match all K hash values for a given query. The union of the L buckets decreases the number of false negatives by increasing the number of potential buckets that could hold valid nearest-neighbor items.
The candidate generation algorithm works in two phases [See (Spring & Shrivastava, 2017) for details]:
1. Pre-processing Phase: We construct L hash tables from the data by storing all elements x  C. We only store pointers to the vector in the hash tables because storing whole data vectors is very memory inefficient.
2. Query Phase: Given a query Q; we will search for its nearest-neighbors. We report the union from all of the buckets collected from the L hash tables. Note, we do not scan all the elements in C, we only probe L different buckets, one bucket for each hash table.
After generating the set of potential candidates, the nearest-neighbor is computed by comparing the distance between each item in the candidate set and the query.
2.2 LSH FOR ESTIMATIONS AND SAMPLING
An item returned as candidate from a (K, L)-parameterized LSH algorithm (section 3.2) is sampled with probability 1 - (1 - pK)L where p is the collision probability of LSH function. The LSH
3

Under review as a conference paper at ICLR 2018

family defines the precise form of p used to build the hash tables. This sampling view of LSH was first utilized to perform adaptive sparsification of deep networks in near-constant time, leading to efficient backpropagation algorithm (Spring & Shrivastava, 2016)
A year later, (Spring & Shrivastava, 2017) demonstrated the first theory of using these samples for unbiased estimation of partition functions in log-linear models. More specifically, the authors show that since we know the precise probability of sampled elements 1 - (1 - pK)L, we can design provably unbiased estimators using importance sampling type idea. This was the first demonstration that random sampling can be beaten with roughly the same computational cost as vanilla sampling. (Luo & Shrivastava, 2017) uses the same approaches for unbiased estimation of anomaly scoring function. (Charikar & Siminelakis) rigorously formalized these notions and show provable improvements in sample complexity of kernel density estimation problems. Recently (Chen et al., 2017) used the sampling in a very different context of connected component estimation for unique entity counts.
2.2.1 MIPS SAMPLING
Recent advances in maximum inner product search (MIPS) using asymmetric locality sensitive hashing has made it possible to sample large inner products.
For this paper, it is sufficient to assume that given a collection C of vectors and query vector Q, using (K, L)-parameterized LSH algorithm with MIPS hashing (Shrivastava & Li, 2014), we get a candidate set S, such that every element xi  C is sample with probability pi  1 where pi is a monotonically increasing function of Q·xi. Thus, we can pay a one-time linear cost of preprocessing C into hash tables, and any further adaptive sampling for query Q only requires few hash lookups and guarantees that returned candidates are likely to have higher Q · xi. We can also compute the exact probability of getting x.
We would like to touch upon some technical details about this sampling. It is not a distribution, i.e., sumxiC = 1. Also the probability of getting xi and xj is not independent. But the same can be used for unbiased estimation. Reader interested in details of such sampling can refer to (Spring & Shrivastava, 2017). In particular, the form of sampling probability is quite unusual, pi = (1 - (1 - g(q · xi))K )L, where g(q · xi) is the collision probability. Overall, it can shown that pi a monotonic function of q · xi.

3 THE LSD ALGORITHM

3.1 A GENERIC FRAMEWORK FOR EFFICIENT GRADIENT ESTIMATION

Our algorithm leverages the efficient estimations using locality sensitive hashing, which usually

beats random sampling estimators while keeping the sampling cost near-constant. We first provide

the intuition of our proposal, and the analysis will follow. Consider least square regression with loss

function

1 N

N i=1

(yi

-

t

·

xi)2,

where

t

is

the

parameter

in

the

tth

iteration.

The

gradient

is

just

like a partition function, and if we simply follow the procedures in (Spring & Shrivastava, 2017),

we can easily show a generic unbiased estimator via adaptive sampling. However, we can be little

smarter and get better sampling procedures.

Observe that, the gradient, with respect to t concerning xi is given by 2(yi - t · xi)xi, the L2 norm of the gradient, which is also the optimal sampling weight wi for xi according to (Alain et al., 2015), can be written as an absolute value of inner product.

||f (xi, t)||2 = 2(t · xi - yi)||xi||2 = 2 t, -1 · xi||xi||, yi||xi|| ,

(4)

where t, -1 is a vector concatenation of  with -1. If the data is normalized then we should sample xi in proportion to wi = t, -1 · xi, yi , i.e. large magnitude inner products should be sampled with higher probability.

As argued, wi changes with t and hence this sampling is costly. It turns out, that we can design a sampling process which does not exactly sample with probability wi but instead samples from a different weighted distribution which is a monotonic function of wi. In particular, we sample from wilsh = f (wi), where f is some monotonic function. Before we describe the efficient sampling process, we argue that a monotonic sampling is a good choice.

4

Under review as a conference paper at ICLR 2018

For any with t.

monotonic Also, due

tfounmcotinoontfon, itchietyw, eifigthheteodpdtiismtrailbusatimonplwinilgshp=reffer(swxii )oivsesrtixlljaid.ea.ptwivie

and changes  wj, then

monotonic sampling will also have same preference, i.e., wilsh  wjlsh.

The key insight is that there are two quantities in the inner product (equation 4), t, -1 and xi, yi . With successive iteration, t, -1 changes while xi, yi is fixed. Thus, it is possible to preprocess xi, yi into hash tables (one time cost) and query with t, -1 for efficient and adaptive sampling. With every iteration, only the query changes to t+1, -1 but the hash tables remains the same. Few hash lookups are sufficient to sample xi for gradient estimation adaptively. Therefore, we only
pay one-time preprocessing cost of building hash tables and for every iteration there are few hash
lookups, typically just one lookup (Section 2), to get a sample for estimation.

There are few more technical subtleties due to the absolute value of inner product t, -1 · xi, yi , rather than the inner product itself. However, the square of

t, -1 · xi, yi 2 = T ( t, -1 ) · T ( xi, yi )

can also be written as an inner product as it is a quadratic kernel, and T is the corresponding fea-
ture expansion transformation. Again square is monotonic function, and therefore, our sampling
is still monotonic as composition of monotonic functions is monotonic. Thus, technically we hash T ( xi, yi ) to create hash tables and the query at tth step is T ( t, -1 ).

Once an xi is sampled via LSH sampling (Algorithm 2), we can precisely compute the probability of its sampling, i.e., pi (See section 2). Therefore our estimation of full gradient is unbiased.

3.2 ALGORITHMIC DETAILS

We first describe the detailed step of our gradient estimator in Algorithm 1. We also give the sam-

pling algorithm 2 in detail. We assume that we have access to the right LSH function h, and its

collision probability expression cp(x, y) = P r(h(x) = h(y)). In case of linear regression, we can

use signed random projections or simhash (Charikar, 2002), instead of MIPS hashing, as with nor-

malized

data

simhash

collision

probability

is

cp(x, y)

=

1

-

cos-1 (

x·y ||x||2 ||y||2

)



which

is

monotonic

in the inner product.

Algorithm 1 LSH-Sampled Stochastic gradient Descent (LSD) Algorithm

1: Input: D = xi, yi, N , 0, 

2: Input: LSH Family H, parameters K, L 3: Output: 

4: HT = Preprocess all data vectors xi, yi into LSH Data structure (Section 2).

5: t = 0

6: while N otConverged do

7: x, p = Sample(H,HT, K, t, -1 ) (Algorithm 2)

8:

t+1

:=

t

-

t

(

f (x,t p×N

)

)

9: end while

10: return 

3.2.1 RUNNING TIME OF SAMPLING
The computational cost of SGD sampling is merely a single random number generator. The cost of gradient update (equation 2) is one inner product, which is d multiplications. If our adaptive sampling beats SGD, the sampling cost cannot be much larger than d multiplications.
The cost of LSD sampling (Algorithm 2) is K × l hash computations followed by l + 1 random number generator, (1 extra for sampling from the bucket). However, the scheme works for any K. Thus, we can always choose K small enough so that empty buckets are rare (see (Spring & Shrivastava, 2017)). In all our experiments, K = 5 for which l is almost always 1. Thus, we require K hash computations and only 2 random number generation. If we use very sparse random projections, then K hash computations only require a constant d multiplications. For example, in all our experiments we only need d/6 multiplication, in expectation, to get all the hashes using
5

Under review as a conference paper at ICLR 2018

Algorithm 2 Sample

1: Input: H (Hash functions), HT [][] (L Hash Tables), K, Query

2: cp(x, Q) is the collision probability Pr(h(x)= h(Q)), under given LSH (known)

3: Output: sampled data x, probability of sampling p

4: l, S = 0

5: while true do

6: ti = random(1, L)

7: bucket = H(Query, ti) (table specific hash)

8: if HT[ti][bucket] = empty then

9: l++

10: continue;

11: end if

12: S = |HT [ti][bucket]| (size of bucket)

13: x = randomly pick one element from HT [ti][bucket]

14: break;

15: end while

16:

p = (1 - (1 - cp(x, Query)K )l) ×

1 S

17: return x, p

sparse projections. Thus, our sampling cost is significantly less than d multiplication which is the cost of gradient update. Using fast hash computation is critical for our method to work in practice.
3.2.2 NEAR-NEIGHBOR IS COSTLIER THAN LSH-SAMPLING
It might be tempting to use approximate near-neighbor search with query t to find xi. However, near-neighbor queries are costly due to candidate generation and filtering. It is still sub-linear in N (and not constant). Moreover, the sampling probability of x cannot be calculated for near-neighbor search which will cause bias in the gradient estimate.
It is important to note that although LSH is heavily used for near-neighbor search, in this case, we just use them to sample. For efficient near neighbor search, K and L grows with N (Indyk & Motwani, 1998). In contrast, the sampling works for any K and L as small as 1. Efficient unbiased estimation is the key difference that makes sampling practical while near-neighbor query prohibitive.

3.3 VARIANCE ANALYSIS

In this section, we first prove that our estimator of gradient is unbiased with lower variance than SGD for most real datasets. Call S the bucket that provided the sample x from in Algorithm 2. For simplicity denote the query with t. Denote pi = 1 - (1 - cp(xi, t)K )l is the probability of finding xi in bucket S.
Theorem 1. The following expression is an unbiased estimator of the full gradient

Est =

1 N

1 1N f (xi, t) · |S|

xiS (xi=xm|xiS) i=1

pi

1 E[Est] = N

N

f (xi, t)

i=1

Theorem 2. The variance of our estimator is:

(5) (6)

V[Est]

=

1 N2

N i=1

f (xi, t)2 pi

·

|S|

-

1N N2(
i=1

f (xi, t))2

Theorem 3. Variance of LSD's estimator is smaller than variance of SGD's estimator if

(7)

1 N

N i=1

f (xi, t)2 · |S| pi

N
< ( f (xi, t))2
i=1

(8)

6

Under review as a conference paper at ICLR 2018

Recall that the collision probability pi = 1 - (1 - pK )l mentioned in Section 2.2. Noted that l here

according to Algorithm 2 is the number of tables that have being looked up. In most practical cases

and also in our experiment, K and L are relative small. LSD can achieve a much smaller variance

than SGD by setting small values of K and L. It is not difficult to see that if several terms in the

summation satisfy

|S| pi N



1, then the variance of our estimator is better than random sampling.

If

the data is clustered nicely, i.e. a random pair has low similarity, then we will have small |S| and

pi can be controlled by tuning K. See Spring & Shrivastava (2017) for more details on when LSH

sampling is better than random sampling.

Training Objective

LSD Train

LSD Test

104

SGD Train SGD Test

103

102

101 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 Time (ms)
(a) CIFAR100

Training Objective

LSD Train 103 LSD Test
SGD Train SGD Test 102

101

100

10 1 0

5000 10000 15000 20000 25000 30000 35000 40000 Time (ms)
(b) GHG

Training Objective

LSD Train 102 LSD Test
SGD Train SGD Test 101 100 10 1 10 2
0 20000 40000 60000 80000 100000 Time (ms)
(c) ElectricityLoad

Training Objective

LSD Train

102

LSD Test SGD Train

SGD Test

101

100

10 1

10 2 0

200 400 600 800 1000 1200 1400 Time (ms)

(d) DrivFace

Figure 1: Wall clock convergence comparisons of plain LSD and SGD.
4 EXPERIMENTS
We evaluate the effectiveness of our algorithm on four datasets covering a wide range of applications from image to time series data; CIFAR100: (Krizhevsky, 2009) An image database with 100 classes containing 600 images each. 50,000 of them were used for training. We used applied standard RGB channel preprocessing to the 32x32 images, leading to 3,072 features for each instance. ElectricityLoad: (Lichman, 2013) A set of 370 records of electricity load of different clients. The dimension is 140,256. There are 296 training examples and 74 testing examples. DrivFace: (Diaz-Chito et al., 2016)s A dataset containing 606 instances of images of 4 drivers with several facial features, each with 6,400 attributes. We used 303 instances for training and 303 for testing. GHG : (Lucas et al., 2015) A collection of 2921 time series of greenhouse gas concentrations in California. 1900 of them were used for training. There are 5232 features in total.
All datasets were preprocessed to have zero mean and unit variance for each feature column. Furthermore, each row was normalized using L2 norm. Noted that for all the experiments, the choice of the gradient decent algorithm is the same. For both SGD and LSD, the only difference in the gradient algorithm was the gradient estimator. For SGD a random sampling estimator was used, while for LSD the estimator used the adaptive estimator. We used fixed values K = 5 and L = 3

7

Under review as a conference paper at ICLR 2018

Training Objective

LSD+adaGrad Train

LSD+adaGrad Test

104

SGD+adaGrad Train SGD+adaGrad Test

103

102

101 0 50000 100000 150000 200000 250000 300000 Time (ms)
(a) CIFAR100

Training Objective

103
102
101
100
10 1 0

LSD+adaGrad Train LSD+adaGrad Test SGD+adaGrad Train SGD+adaGrad Test
1000 2000 3000 4000 5000 Time (ms)
(b) GHG

Training Objective

LSD+adaGrad Train

LSD+adaGrad Train

102

LSD+adaGrad Test SGD+adaGrad Train

102

LSD+adaGrad Test SGD+adaGrad Train

SGD+adaGrad Test

SGD+adaGrad Test

101

101

Training Objective

100 100
10 1

10 1 10 2

0 5000 10000 15000 20000 25000 30000 Time (ms)

10 2 0

500 1000 1500 2000 Time (ms)

(c) ElectricityLoad

(d) DrivFace

Figure 2: Wall clock convergence comparisons of adagrad with LSD and SGD gradient estimation.

for all the datasets. Our hash function is simhash (or signed random projections) and we use sparse random projection with sparsity 1/30.
To the best of our knowledge, there is no other adaptive estimation baseline, where the cost of sampling per iteration is less than linear O(N ). Since our primary focus will be on wall clock speed up, no O(N ) estimation method will be able to outperform O(1) SGD (and LSD) estimates on the same platform.
LSD vs. SGD In the first experiment, we compare vanilla SGD with LSD, i.e., we use simple SGD with fixed learning rate. This basic experiment aims to demonstrate the performance of pure LSD and SGD without involving other factors like L1/L2 regularizations on linear regression task. In such a way, we can quantify the superiority of LSD.
Figure 4 (in Appendix) shows the decrease in the squared loss error with epochs. Blue lines represent SGD and red lines represent LSD. It is obvious that LSD converges much faster than SGD in either training or testing loss comparisons. This is not surprising with the claims in Section 3.2.1 and theoretical proof in Section 3.3. Since LSD uses slightly more computations per epoch than SGD updates, it is hard to defend if LSD gains enough benefits simply from the epoch wise comparisons. We therefore also show the decrease in error with wall clock time. Wall clock time is the actual quantification of speedups. Again, on every single dataset, LSD shows faster convergence both in epochs as well as running time.
LSD+AdaGrad vs. SGD+AdaGrad As argued, our LSD algorithm is not an alternative but a complimentary to another gradient-based optimization algorithm. We repeated the first experiment one but using AdaGrad (Duchi et al., 2011) instead of plain SGD. Again, the only change in the competing algorithm is the gradient estimates per epoch. Figure 5 (in Appendix) shows epoch wise comparison, while Figure 2 shows running time comparisons. The trends as expected are similar. LSD with adagrad outperforms adagrad with SGD estimates of gradients.
LSD, SGD vs. True Gradient: In this section, as a sanity check, we compare the quality of estimates with SGD and LSD. We chose estimation DrivFace dataset. During an intermediate iteration, we compute the true gradient. We then estimate the gradient using m samples of SGD and LSD. We plot the estimation error with m in Figure 3 (in Appendix). It is not surprising to see LSD has better estimates of the true gradient than SGD most of the time confirming our theoretical results empirically.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.
Le´on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010, pp. 177­186. Springer, 2010.
Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimensions.
Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pp. 380­388. ACM, 2002.
Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with application to the syrian conflict. arXiv preprint arXiv:1710.02690, 2017.
Katerine Diaz-Chito, Aura Herna´ndez-Sabate´, and Antonio M. Lo´pez. A reduced feature set for driver head pose estimation. Appl. Soft Comput., 45(C):98­107, August 2016. ISSN 1568-4946. doi: 10.1016/j.asoc.2016.04.027. URL http://dx.doi.org/10.1016/j.asoc.2016. 04.027.
Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13 (Dec):3475­3506, 2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Siddharth Gopal. Adaptive sampling for sgd by exploiting side information. In International Conference on Machine Learning, pp. 364­372, 2016.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 604­613. ACM, 1998.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ ml.
D. D. Lucas, C. Yver Kwok, P. Cameron-Smith, H. Graven, D. Bergmann, T. P. Guilderson, R. Weiss, and R. Keeling. Designing optimal greenhouse gas observing networks that consider performance and cost. Geoscientific Instrumentation, Methods and Data Systems, 4(1):121­137, 2015. doi: 10.5194/gi-4-121-2015. URL https://www. geosci-instrum-method-data-syst.net/4/121/2015/.
Chen Luo and Anshumali Shrivastava. Arrays of (locality-sensitive) count estimators (ace): Highspeed anomaly detection via cache lookups. arXiv preprint arXiv:1706.06664, 2017.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems, pp. 1017­1025, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp. 400­407, 1951.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International Conference on Machine Learning, pp. 71­79, 2013.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Advances in Neural Information Processing Systems, pp. 2321­2329, 2014.
9

Under review as a conference paper at ICLR 2018 Ryan Spring and Anshumali Shrivastava. Scalable and sustainable deep learning via randomized
hashing. arXiv preprint arXiv:1602.08194, 2016. Ryan Spring and Anshumali Shrivastava. A new unbiased and efficient class of lsh-based sam-
plers and estimators for partition function computation in log-linear models. arXiv preprint arXiv:1703.05160, 2017. Peilin Zhao and Tong Zhang. Accelerating minibatch stochastic gradient descent using stratified sampling. arXiv preprint arXiv:1405.3080, 2014. Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1­9, 2015.
10

Under review as a conference paper at ICLR 2018
A EPOCH PLOTS AND PROOFS

Sqaured Error

0.00025 0.00020

LSD SGD

0.00015

0.00010

0.00005

0.00000 0

20 40 60 80 100 # Samples

Figure 3: Error in estimating full gradient for SGD and LSD with sample size m

Training Objective

LSD Train

LSD Test

104

SGD Train SGD Test

103

102

101 0

5

10 Epo1c5h

20

25

30

(a) CIFAR100

Training Objective

103 102 101 100 10 1
0

LSD Train LSD Test SGD Train SGD Test
20 40 Epoch 60 80 100
(b) GHG

Training Objective

LSD Train 102 LSD Test
SGD Train SGD Test 101 100 10 1 10 2
0 20 40 60 80 100 Epoch
(c) ElectricityLoad

Training Objective

102 101 100 10 1 10 2 0

LSD Train LSD Test SGD Train SGD Test

10 20 30 Epoch
(d) DrivFace

40

50

Figure 4: shows epoch wise the comparisons of training and testing loss of LSD and SGD for all four datasets

Theorem 4. Let S be the bucket that sample xm is chosen from in Algorithm 2. Let pm be the sampling probability associated with sample xm. Suppose we query a sample with t. Then we have
11

Under review as a conference paper at ICLR 2018

Training Objective

104 103 102 101
0

LSD+adaGrad Train LSD+adaGrad Test SGD+adaGrad Train SGD+adaGrad Test

10 20 Epoch 30
(a) CIFAR100

40

50

Training Objective

LSD+adaGrad Train 103 LSD+adaGrad Test
SGD+adaGrad Train SGD+adaGrad Test 102
101
100
10 1 0.0 2.5 5.0 7.5 E1p0o.c0h 12.5 15.0 17.5 20.0
(b) GHG

Training Objective

102 101 100 10 1 10 2
0

LSD+adaGrad Train LSD+adaGrad Test SGD+adaGrad Train SGD+adaGrad Test

10 20Epoch 30
(c) ElectricityLoad

40

Training Objective

102 101 100 10 1 10 2
0

LSD+adaGrad Train LSD+adaGrad Test SGD+adaGrad Train SGD+adaGrad Test

10 20 Epoch 30
(d) DrivFace

40

50

Figure 5: Epoch wise convergence comparisons of training and testing loss of LSD+adaGrad and SGD+adaGrad.

an unbiased estimator of the full gradient:

Est =

1 N

1 1N f (xi, t) · |S|

xiS (xi=xm|xiS) i=1

pi

1 E[Est] = N

N

f (xi, t)

i=1

Proof.

E[1xiS ] = pi

and

1E[ xi=xm|xiS ]

=

1 |S|

Also note that 1 1E[ xiS xi=xm|xiS ] = 1 1E[ xiS ]E[ xi=xm|xiS ]
12

Under review as a conference paper at ICLR 2018

E[Est]

=

1 N

E[

N i=1

1xiS

1xi =xm |xi S

f

(xi, t) pi

·

|S|

]

=

1 N

N i=1

E[1xiS

1xi =xm |xi S

]

·

E[

f

(xi, t) pi

·

|S|

]

=

1 N

N
pi
i=1

·

1 f (xi, t) · |S| |S| pi

1 =
N

N

f (xi, t)

i=1

Theorem 5. The variance of our estimator is:

V[Est]

=

1 N2

N i=1

f (xi, t)2 pi

·

|S|

-

1N N2(
i=1

f (xi, t))2

Proof.

V[Est] = E[Est2] - E[Est]2

Est2

=

1 N2

N i,j

1xiS

1xj

S

1xi =xm |xi S

1xj

=xm |xj

S

f

(xi,

t)

· f (xj pi · pj

,

t)

·

|S|2

1 = N2

N i

1xiS

·

1xi =xm |xi S

(f (xi, t))2 p2i

·

|S|2

E[Est2]

=

1 N2

N i

(f (xi, t))2 · |S| pi

V[Est]

=

1 N2

N i=1

f (xi, t)2 pi

· |S|

-

1N N2(
i=1

f (xi, t))2

Theorem 6. Variance of LSD's estimator is smaller than variance of SGD's estimator if

1 N

N i=1

f (xi, t)2 · |S| pi

<

N
f (xi, t)2
i=1

Proof. The variance of regular SGD is

1 V[Est'] = N

N

f (xi, t)2

-

1 N2(

N

f (xi, t))2

i i=1

By (11) and (17), one can easily see that V[Est'] < V[Est] when (16) satisfies.

13

