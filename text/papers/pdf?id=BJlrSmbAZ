Under review as a conference paper at ICLR 2018
BAYESIAN UNCERTAINTY ESTIMATION FOR BATCH NORMALIZED DEEP NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains. The techniques driving these advances, however, lack a formal method to account for model uncertainty. While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult. In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization. We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty. Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure. Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.
1 INTRODUCTION
Deep learning has dramatically advanced the state of the art in a number of domains, and now surpasses human-level performance for certain tasks such as recognizing the contents of an image (He et al., 2015) and playing Go (Silver et al., 2017). But, despite their unprecedented discriminative power, deep networks are prone to make mistakes. Sometimes, the consequences of mistakes are minor ­ misidentifying a food dish or a species of flower (Liu et al., 2016) may not be life threatening. But deep networks can already be found in settings where errors carry serious repercussions such as autonomous vehicles (Chen et al., 2016) and high frequency trading. In medicine, we can soon expect automated systems to screen for skin cancer (Esteva et al., 2017), breast cancer (Shen, 2017), and to diagnose biopsies (Djuric et al., 2017). As autonomous systems based on deep learning are increasingly deployed in settings with the potential to cause physical or economic harm, we need to develop a better understanding of when we can be confident in the estimates produced by deep networks, and when we should be less certain.
Standard deep learning techniques used for supervised learning lack methods to account for uncertainty in the model, although sometimes the classification network's output vector is mistakenly understood to represent the model's uncertainty. The lack of a confidence measure can be especially problematic when the network encounters conditions it was not exposed to during training. For example, if a network trained to recognize dog breeds is given an image of a cat, it may predict it to belong to a breed of small dog with high probability. When exposed to data outside of the distribution it was trained on, the network is forced to extrapolate, which can lead to unpredictable behavior. In such cases, if the network can provide information about its uncertainty in addition to its point estimate, disaster may be avoided. This work focuses on estimating such predictive uncertainties in deep networks (Figure 1).
The Bayesian approach provides a solid theoretical framework for modeling uncertainty (Ghahramani, 2015), which has prompted several attempts to extend neural networks (NN) into a Bayesian setting. Most notably, Bayesian neural networks (BNNs) have been studied since the 1990's (Neal, 2012). Although they are simple to formulate, BNNs require substantially more computational resources than their non-Bayesian counterparts, and inference is difficult. Importantly, BNNs do
1

Under review as a conference paper at ICLR 2018

MCBN

MCDO

CUBN

CUDO

Figure 1: We propose a method to estimate uncertainty in any network using batch normalization (MCBN). Here, we show results on a toy dataset from networks with three hidden layers (30 units per layer). The solid line is the predictive mean of 500 stochastic forward passes. The outer area depicts the model's uncertainty as the 95% CI of the predictive distribution for each x value (inner shaded area is 50% CI). On the right, we show a similar plot using dropout to estimate uncertainty (MCDO) (Gal & Ghahramani, 2015). The bottom row depicts a minimally useful baseline ­ the same networks but with a constant uncertainty (CUBN, CUDO).
not scale well and struggle to compete with modern deep learning architectures. Recently, Gal & Ghahramani (2015) developed a practical solution to obtain uncertainty estimates by casting dropout training in conventional deep networks as an approximate Bayesian model. They showed that any network trained with dropout is an approximate Bayesian model, and uncertainty estimates can be obtained by computing the variance on multiple predictions with different dropout masks.
This technique, called Monte Carlo Droput (MCDO), has a very attractive quality: it can be applied to existing NNs without any modification to the architecture or the way the network is trained. Uncertainty estimates come (nearly) for free. However, in recent years dropout has fallen out of favor, limiting MCDO's utility. Google's Inception network, which won ILSVRC in 2014, did not use dropout (Szegedy et al., 2015), nor did the ILSVRC 2015 winner, Microsoft's residual learning network (He et al., 2016). In place of traditional techniques like dropout, most modern networks such as Inception and ResNet have adopted other regularization techniques. In particular, batch normalization (BN) has become widespread thanks to its ability to stabilize learning with improved generalization (Ioffe & Szegedy, 2015).
An interesting aspect of BN is that the mini-batch statistics used for training each iteration depend on randomly selected batch members. We exploit this stochasticity and show that training using batch normalization, like dropout, is equivalent to approximate inference in Bayesian models1. We demonstrate how this finding allows us to make meaningful estimates of the model uncertainty in a technique we call Monte Carlo Batch Normalization (MCBN) (Figure 1). The method we propose makes no simplifying assumptions on the use of batch normalization, and applies to any network using BN as it appears in practical applications.
We validate our approach by empirical experiments on eight standard datasets used for uncertainty estimation. We measure uncertainty quality relative to a baseline of fixed uncertainty, and show that MCBN outperforms the baseline on nearly all datasets with strong statistical significance. We also show that the uncertainty quality of MCBN is on par with that of MCDO. As a practical demonstration of MCBN, we apply our method to estimate segmentation uncertainty using a conventional segmentation network (Badrinarayanan et al., 2015). Finally, as part of our evaluation, we make contributions to the methodology of measuring uncertainty quality by defining performance bounds on existing metrics and proposing a new visualization that provides an intuitive understanding of uncertainty quality.
1The possibility of using other stochastic regularization techniques is mentioned in Gal (2016).
2

Under review as a conference paper at ICLR 2018
2 RELATED WORK
Bayesian models provide a natural framework for modeling uncertainty, and several approaches have been developed to adapt NNs to Bayesian reasoning. A common approach is to place a prior distribution (often a Gaussian) over each weight. For infinite weights, the resulting model corresponds to a Gaussian process (Neal, 1995), and for a finite number of weights it corresponds to a Bayesian neural network (MacKay, 1992). Although simple to formulate, inference in BNNs is difficult (Gal, 2016). Therefore, focus has shifted to techniques to approximate the posterior distribution, leading to approximate BNNs. Methods based on variational inference (VI) typically rely on a fully factorized approximate distribution (Kingma & Welling, 2014; Hinton & Van Camp, 1993) but these methods do not scale easily. To alleviate these difficulties, Graves (2011) proposed a model using sampling methods to estimate a factorized posterior. Another approach, probabilistic backpropagation (PBP), also estimates a factorized posterior based on expectation propagation (Adams, 2015).
Deep Gaussian Processes (DGPs) formulate GPs as Bayesian models capable of working on large datasets with the aid of a number of strategies to address scaling and complexity requirements (Bui et al., 2016). The authors compare DGP with a number of state-of-the-art approximate BNNs, showing superior performance in terms of RMSE and uncertainty quality2. Another recent approach to Bayesian learning, Bayesian hypernetworks, use a neural network to learn a distribution of paramaters over another neural network (Krueger et al., 2017). Although these recent techniques address some of the difficulties with approximate BNNs, they all require modifications to the architecture or the way networks are trained, as well as specialized knowledge from practitioners.
Recently, Gal (2016) showed that a network trained with dropout implicitly performs the VI objective. Therefore any network trained with dropout can be treated as an approx. Bayesian model by making multiple predictions as forward passes through the network while sampling different dropout masks for each prediction. An estimate of the posterior can be obtained by computing the mean and variance of the predictions. This technique, referred to here as MCDO, has been empirically demonstrated to be competitive with other approx. BNN methods and DGPs in terms of RMSE and uncertainty quality (Li & Gal, 2017). However, as the name implies, MCDO depends on dropout. While once ubiquitous in training deep learning models, dropout has largely been replaced by batch normalization in modern networks, limiting its usefulness.
3 METHOD
The methodology of this work is to pose a deep network trained with batch normalization as a Bayesian model in order to obtain uncertainty estimates associated with its predictions. In the following, we briefly introduce Bayesian models and a variational approximation to it using KullbackLeibler (KL) divergence following Gal & Ghahramani (2015). We continue by showing a batch normalized deep network can be seen as an approximate Bayesian model. Then, by employing theoretical insights as well as empirical analysis, we study the induced prior on the parameters when using batch normalization. Finally, we describe the procedure we use for estimating uncertainty of batch normalized deep networks' output.
3.1 BAYESIAN MODELING
We assume a finite training set D = {(xi, yi)}i=1:N where each (xi, yi) is a sample-label pair. Using D, we are interested in learning an inference function f(x, y) with parameters . In deterministic models, the estimated label y^ is obtained as follows:
y^ = arg max f(x, y)
y
We assume f(x, y) = p(y|x, ) (e.g. in soft-max classifiers), and is normalized to a proper probability distribution. In Bayesian modeling, in contrast to finding a point estimate of the model parameters, the idea is to estimate an (approximate) posterior distribution of the model parameters p(|D) to be used for probabilistic prediction:
p(y|x, D) = f(x, y)p(|D)d
2By uncertainty quality, we refer to predictive probability distributions as measured by PLL and CRPS
3

Under review as a conference paper at ICLR 2018

The predicted label, y^, can then be accordingly obtained by sampling p(y|x, D) or takings its maxima.

Variational Approximation In approximate Bayesian modeling, it is a common approach to learn a parametrized approximating distribution q() that minimizes KL(q()||p(|D)); the Kullback-Leibler (KL) divergence of posterior w.r.t. its approximation, instead of the true posterior.
Minimizing this KL divergence is equivalent to the following minimization while being free of the data term p(D) 3:

N
LVA() := -
i=1

q() ln f(xi, yi)d + KL(q()||p())

Using Monte Carlo integration to approximate the integral using one realized ^i for each sample i 4, the approximated objective becomes:

N
L^VA() := - ln f^i (xi, yi) + KL(q()||p())
i=1

(1)

The first term is the data likelihood and the second term is divergence of the model prior w.r.t. the approximated distribution.
We now describe the optimization procedure of a deep network with batch normalization and draw the resemblance to the approximate Bayesian modeling in Eq (1).

3.2 BATCH NORMALIZED DEEP NETS AS BAYESIAN MODELING
The inference function of a feed-forward deep network with L layers can be described as:
f(x) = WLa(WL-1...a(W2a(W1x))
where a(.) is an element-wise nonlinearity function and Wl is the weight vector at layer l. Furthermore, we denote the input to layer l as xl with x1 = x and we then set hl = Wlxl. Parenthesized super-index for matrices (e.g. W(j)) and vectors (e.g. x(j)) indicates jth row and element respectively. Super-index u refers to a specific unit at layer l, (e.g. Wu = Wl,(j), hu = hl,(j)). 5
Batch Normalization Each layer of a deep network is constructed by several linear units whose parameters are the rows of the weight matrix W. Batch normalization is a unit-wise operation proposed in Ioffe & Szegedy (2015) to standardize the distribution of each unit's input. It essentially converts a unit's output hu in the following way:
h^u = hu - E[hu] Var[hu]
where the expectations are computed over the training set6. However, often in deep networks, the weight matrices are optimized using back-propagated errors calculated on mini-batches of data. Therefore, during training, the estimated mean and variance on the mini-batch B is used, which we denote by µB and B respectively. This makes the inference at training time for a sample x a stochastic process, varying based on other samples in the mini-batch.
3achieved by constructing the Evidence Lower Bound, called ELBO, and assuming i.i.d. observation noise; details can be found in the appendix sec 6.1.
4while a MC integration using a single sample is a weak approximation, in an iterative optimization for  several samples will be taken over time
5For a (softmax) classification network, f(x) is a vector with f(x, y) = f(x)(y), for regression networks with i.i.d. Gaussian noise we have f(x, y) = N (f(x), .).
6It further learns an affine transformation for each unit using parameters  and , which we omit in favor of brevity: x^a(fjfi)ne = (j)x^(j) + (j)

4

Under review as a conference paper at ICLR 2018

Loss Function and Optimization Training deep networks involves a (regularized) risk minimization with the following form:

LRR()

:=

-

1 N

N

l(y^i, yi) + ()

i=1

Where the first term is the empirical loss on the training data and the second term is a regularization

penalty acting as a prior on model parameters . If the loss l is cross-entropy for classification or

sum-of-squares for regression problems (assuming i.i.d. Gaussian noise on labels), the first term is

equivalent to minimizing the negative log-likelihood

LRR()

:=

-

1 N

N

ln f(xi, yi) + ().

i=1

In a batch normalized network the model parameters are  = {W1:L, 1:L, 1:L, µB1:L, B1:L}. If we decouple the learnable parameters  = {W1:L, 1:L, 1:L} from the stochastic parameters
z = {µB1:L, B1:L}, we get the following objective at each step of the mini-batch optimization of a batch normalized network

1N

LRR() := - N

ln f{,z^i}(xi, yi) + ()

i=1

(2)

where z^i is the mean and variances for sample i's mini-batch at a certain training step. Note that while z^i formally needs to be i.i.d. for each training example, a batch normalized network samples the stochastic parameters once per training step (mini-batch). For a large number of epochs, how-

ever, the distribution of sampled batch members for a given training example converges to the i.i.d.

case.

Comparing Eq. (1) and Eq. (2) reveals that minimizing the learnable parameters  for a batch nor-
malized network with a certain prior () is equivalent to an approximate variational inference for a
Bayesian model. With that we can use a pre-trained batch normalized network to estimate the uncertainty of its prediction using the inherent stochasticity of the normalization parameters µB1:L, B1:L. Before that, we briefly discuss what Bayesian prior is induced in a typical batch normalized network.

3.3 PRIOR p()
Arguably, the key ingredient of Bayesian methods is not the prior, but the idea of integrating over different possibilities of the random variables; in this case model parameters. In fact, the influence of the prior diminishes as the size of training data increases for Bayesian modeling.
It can, however, still be insightful to look into what type of prior the Bayesian interpretation of a batch normalized network induces. For that, we need to study the second term in eq (1), KL(q()||p()), and find out what p() makes the optimizations (1) and (2) equal given a certain ().
The purpose of () is to reduce variance in deep networks. L2-regularization, also referred to as weight decay (() =  l=1:L ||W l||2), is a popular technique in deep learning. For batch normalized networks however, Ioffe & Szegedy (2015) suggest that a significantly lower  should be used. As  is reduced and approaches zero, the equivalence of Eq. (2) and (1) implies that KL(q()||p())  0. Consequently, the induced prior distribution p() of the Bayesian modeling approaches to that of q(). This is an intuitive result for the case where we have no prior beliefs of , but rather let its distribution be fully determined by D.
For the case where L2 regularization is applied during training, the induced prior is studied further in Appendix 6.4.

3.4 PREDICTIVE UNCERTAINTY IN BATCH NORMALIZED DEEP NETS
In the absence of the true posterior we rely on the approximate posterior to express an approximate predictive distribution:
p(y|x, D) := f(x, y)q()d

5

Under review as a conference paper at ICLR 2018

Dataset name

N Q Target Feature

Boston Housing

506 13

Concrete Compressive Strength 1,030 8

Energy Efficiency

768 8 Heating Load

Kinematics 8nm

8,192 8

Power Plant

9,568 4

Protein Tertiary Structure

45,730 9

Wine Quality (Red)

1,599 11

Yacht Hydrodynamics

308 6

Table 1: Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size and Q is the n.o. input features. Only one target feature was used. In cases where the raw datasets contain more than one target feature, the feature used is specified by target feature.

Following Gal & Ghahramani (2015) we estimate the first and second moment of the predictive distribution empirically. This involves computing the sample mean and variance of T stochastic forward passes through the network. Let p denote p(y|x, D). As detailed in Appendix 6.3, we find unbiased estimates for the first two moments as follows (assuming i.i.d. Gaussian Noise  -1I)
Ep [y] = yp(y|x, D)dy
 1 T f t (x) T
t=1
Covp [y] = Ep [y y] - Ep [y] Ep [y]   -1I + Covp [y]
where the estimated predictive variance is the sum of constant variance from observation noise and the sample variance from the stochastic forward passes.
Each stochastic prediction is preceded by sampling network parameters t prior to prediction. In MCBN, this consists of sampling a batch from training data, and updating batch normalized layers with the resulting batch statistics. Taking the mean and variance of prediction following such updates yields estimates of the first two moments of the predictive distribution.

4 EXPERIMENTS AND RESULTS
We assess the uncertainty quality of MCBN quantitatively and qualitatively. Our quantitative analysis relies on eight standard regression datasets, listed in Table 1. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani, 1996), these datasets have been used to benchmark comparative models in recent related literature (see Adams (2015), Gal & Ghahramani (2015), Bui et al. (2016) and Li & Gal (2017)). We report results using standard metrics, and also propose useful upper and lower bounds to normalize these metrics for a more meaningful interpretation in Section 4.2.
Our qualitative results consist of three parts. First, in Figure 1 we demonstrate that MCBN produces reasonable uncertainty bounds on a toy dataset in the style of (Karpathy, 2015). Second, we develop a new visualization of uncertainty quality by plotting test errors sorted by predicted variance in Figure 2. Finally, we apply MCBN to SegNet (Kendall et al., 2015), demonstrating the benefits of MCBN in an existing batch normalized network.
4.1 METRICS
We evaluate uncertainty quality based on two metrics, described below: Predictive Log Likelihood (PLL) and Continuous Ranked Probability Score (CRPS). We also propose upper and lower bounds for these metrics which can be used to normalize them and provide a more meaningful interpretation.
Predictive Log Likelihood (PLL) Predictive Log Likelihood is a widely accepted metric for uncertainty quality, used as the main uncertainty quality metric for regression (e.g. (Adams, 2015),

6

Under review as a conference paper at ICLR 2018

(Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)). PLL is defined for a probabilistic model f and a single observation (yi, xi) as
PLL(f, (yi, xi)) = log p(yi|f (xi))
where p(yi|f (xi)) is the model's predicted PDF evaluated at yi, given the input xi. The metric is unbounded and maximized by a perfect prediction (mode at yi) with no variance. As the predictive mode moves away from yi, increasing the variance tends to increase PLL (by maximizing probability mass at yi). PLL is an elegant measure, but has been criticized for allowing outliers to have an overly negative effect on the score (Selten, 1998).

Continuous Ranked Probability Score (CRPS) Continuous Ranked Probability Score is a less sensitive measure that takes the full predicted PDF into account. A prediction with low variance that is slightly offset from the true observation will receive a higher score form CRPS than PLL. Letting F (y) be the predicted CDF from a probabilistic model f , CRPS is defined as



CRPS(f, (yi, xi)) =

F (y) - 1(y  yi) 2dy

-

where 1(y  yi) = 1 if y  yi and 0 otherwise (Gneiting & Raftery, 2007). The metric can be
interpreted as the sum of the squared area between the CDF and 0 where y < yi and between the CDF and 1 where y  yi. A perfect prediction with no variance yields a CRPS of 0; for all other
cases the value is larger. CRPS has no upper bound.

4.2 BENCHMARK MODELS AND NORMALIZED METRICS

In order to establish a lower bound on useful performance for uncertainty estimates, we define a baseline that predicts constant variance regardless of input. This benchmark model produces identical point estimates as MCBN, which yield the same predictive means. The variance is set to a fixed value that optimizes PLL or CRPS (corresponding to the metric being tested). We call this model Constant Uncertainty BN (CUBN). We similarly define a baseline for dropout, Constant Uncertainty Dropout (CUDO). The difference in variance modeling between MCBN, CUBN, MCDO and CUDO are visualized in plots of uncertainty bounds on toy data in Figure 1.

For a probabilistic model f , an upper bound on uncertainty performance can also be defined for

CRPS and PLL. For each observation (yi, xi), a value for the predictive variance Ti can be chosen that maximizes PLL or minimizes CRPS7. Using CUBN as a lower bound and the optimized

CRPS score as the upper bound, uncertainty estimates can be normalized between these bounds (1

indicating optimal performance, and 0 indicating performance on par with fixed uncertainty). We

call this normalized measure CRPS

=

CRPS(f,(yi,xi))-CRPS(fCU ,(yi,xi)) minT CRPS(f,(yi,xi))-CRPS(fCU ,(yi,xi))

×

100,

and

the

PLL

analogue

PLL

=

PLL(f,(yi,xi))-PLL(fCU ,(yi,xi)) maxT PLL(f,(yi,xi))-PLL(fCU ,(yi,xi))

× 100.

This

normalized

measure

gives

an

intu-

itive understanding of how close a Bayesian model is to estimating the perfect uncertainty for each

prediction.

4.3 TEST SETUP
The setup we use to test the quality of MCBN's uncertainty estimation largely follows that of Gal & Ghahramani (2015). The datasets, architecture, and metrics were chosen similarly. However, we use different training parameters to avoid overfitting. We also allow a larger range of dropout rates.
All models share a similar architecture: two hidden layers with 50 units each, using ReLU activations8. Input and output data were normalized during training. Results were averaged over five random splits of 20% test and 80% training and cross-validation (CV) data. For each split, 5-fold CV by grid search with a RMSE minimization objective was used to find training hyperparameters and optimal n.o. epochs. For BN-based models, the hyperparameter grid consisted of a weight decay factor ranging from 0.1 to 1-15 by a log 10 scale, and a batch size range from 32 to 1024 by a log 2 scale. For DO-based models, the hyperparameter grid consisted of the same weight decay range,
7Ti can be found analytically for PLL, but must be found numerically for CRPS 8For the Protein Tertiary Structure dataset we used 100 units per hidden layer and 2-fold CV.

7

Under review as a conference paper at ICLR 2018

Dataset
Boston Housing Concrete Energy Efficiency Kinematics 8nm Power Plant Protein Tertiary Structure Wine Quality (Red) Yacht Hydrodynamics

CRPS

MCBN

MCDO

8.50 **** 3.91 **** 5.75 **** 2.85 **** 0.24 *** 2.66 **** 0.26 ** -56.39 ***

3.06 **** 0.93 * 1.37 ns 1.82 **** -0.44 **** 0.99 **** 2.00 **** 21.42 ****

PLL MCBN MCDO

10.49 **** -36.36 ** 10.89 ****
1.68 *** 0.33 ** 2.56 **** 0.19 * 45.58 ****

5.51 **** 10.92 **** -14.28 * -0.26 ns 3.52 **** 6.23 ****
2.91 **** -41.54 ns

Table 2: Uncertainty quality measured on eight datasets. MCBN and MCDO are compared over 5 random 80-20 splits of the data with 5 different random seeds each split. Reported values are uncertainty metrics CRPS and PLL normalized to a lower bound of constant variance and upper bound that maximizes the metric. CRPS and PLL are expressed as a percentage, reflecting how close the model is to the upper bound. We check to see if CRPS and PLL significantly exceed the baseline using a one sample t-test (significance level indicated by *'s). See text for further details.

and dropout probabilities in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models used a batch size of 32 in all evaluations.
The model with optimal training hyperparameters was used to optimize  numerically. This optimization was made in terms of average CV CRPS for MCBN, CUBN, MCDO, and CUDO respectively, before evaluation on the test data.
All estimates for the predictive distribution were obtained by taking 500 stochastic forward passes through the network, throughout training and testing. The implementation was done with TensorFlow. The Adam optimizer was used to train all networks, with a learning rate of 0.001. The extensive part of the experiments (i.e. training and cross validation) was done on Amazon web services using 3000 machine/hours. All codes necessary for reproducing both the quantitative and qualitative results is released in an anonymous github repository (https://github.com/iclr-mcbn/mcbn).

4.4 TEST RESULTS
A summary of the results measuring uncertainty quality of MCBN and MCDO are provided in Table 2. Tests are run over eight datasets using 5 random 80-20 splits of the data with 5 different random seeds each split. We report CRPS and PLL, expressed as a percentage, which reflects how close the model is to the upper bound. The upper bounds and lower bounds for each metric are described in Section 4.2. We check to see if the reported values of CRPS and PLL significantly exceed the lower bound models (CUBN and CUDO) using a one sample t-test, where the significance level is indicated by *'s. Further details from the experiment are available in Appendix 6.5.
In Figure 2, we provide a novel visualization of uncertainty quality visualization in regression datasets. Errors in the model predictions are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray). This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for predictive errors.
Qualitative results for Bayesian SegNet using MCBN was produced by using the main CamVid model in Kendall et al. (2015). The pre-trained model was obtained from the online model zoo and was used without modification. 10 instances of mini-batches with size 6 were used to estimate the mean and variance of MCBN. Qualitative results can be found in Figure 3 depicting intuitive uncertainty at object boundaries. Quantitative measures on various segmentation datasets can be obtained and is beyond the scope of this work.
We provide additional experimental results in Appendix 6.5. In Tables 3 and 4, we show the mean CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par with MCDO across several datasets. In Table 5 we provide RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results indicate

8

Under review as a conference paper at ICLR 2018

Prediction error

Yacht

95% CI

Boston Housing

95% CI

Kinematics

95% CI

4.5

MCBN

50% CI

12

MCBN

50% CI

0.3

MCBN

50% CI

4 10
3.5

Prediction error

Prediction error

3 8 0.2

2.5 6
2

1.5 4 0.1

1 2
0.5

Yacht

95% CI

Boston Housing

95% CI

Kinematics

95% CI

4.5

MCDO

50% CI

12

MCDO

50% CI

0.3

MCDO

50% CI

4 10
3.5

Prediction error

Prediction error

3 8 0.2

2.5 6
2

1.5 4 0.1

1 2
0.5

Prediction error

Figure 2: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show MCBN's (blue) and MCDO's (red) model uncertainty (light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors. See Appendix for complete results.

that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the predictive accuracy of the network.

5 DISCUSSION
The results presented in Table 2 and Appendix 6.5 indicate that MCBN generates meaningful uncertainty estimates which correlate with actual errors in the model's prediction. We show statistically significant improvements over CUBN in the majority of the datasets, both in terms of CRPS and PLL. The visualizations in Figure 2 and in Appendix 6.5 show clear correlations between the estimated model uncertainty and actual errors produced by the network. We perform the same experiments using MCDO, and find that MCBN generally performs on par with MCDO. Looking closer, in terms of CRPS, MCBN performs better than MCDO in more cases than not. However, care must be used when comparing different models. The learned network parameters are different, leading to different predictive means which can confound direct comparison.
The results on the Yacht Hydrodynamics dataset seem contradictory. The CRPS score for MCBN is extremely negative, while the PLL score is extremely positive. The opposite trend is observed for MCDO. To add to the puzzle, the visualization in Figure 2 depicts an extremely promising uncertainty estimation that models the predictive errors with high fidelity. We hypothesize that this strange behavior is due to the small size of the data set, which only contains 60 test samples. There is also a large variability in the model's accuracy on this dataset, which further confounds the measurements for such limited data.
One might criticize the overall quality of the uncertainty estimates of MCBN and MCDO based on the magnitude of the CRPS and PLL scores in Table 2. The scores rarely exceed 10% improvement over the lower bound. However, we caution that these measures should be taken in context. The upper bound is very difficult to achieve in practice (it is optimized for each test sample individually), and the lower bound is a quite reasonable estimate for uncertainty. We can look to other studies to put these results in a broader context. Although we do not compare directly to other popular variational inference methods, such a study has been performed in (Gal & Ghahramani, 2015), where MCDO is compared to (Graves, 2011) and (Herna´ndez-Lobato & Adams, 2015). In this study, MCDO was found to outperform the VI methods in uncertainty quality. Since we have established that MCBN performs on par with MCDO, by proxy we might conclude that MCBN outperforms those VI methods as well.

9

Under review as a conference paper at ICLR 2018
Figure 3: Results applying MCBN to Bayesian SegNet (Kendall et al., 2015). In the upper left, a scene from the CamVid driving scenes dataset. In the upper right, the Bayesian estimated segmentation. In the lower left, estimated uncertainty using MCBN for the car class. In the lower right, the estimated uncertainty of MCBN for all 11 classes. In this work, we have shown that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure. We show evidence that the uncertainty estimates from MCBN correlate with actual errors in the model's prediction, and are useful for practical tasks such as regression or semantic image segmentation. Our experiments show that MCBN performs on par with MCDO, and by proxy, other Bayesian estimation methods. Finally, we make contributions to the evaluation of uncertainty quality by suggesting new evaluation metrics based on useful baselines and upper bounds, and proposing a new visualization tool which gives an intuitive visual explanation of uncertainty quality. Finally, it should be noted that, over the past few years, batch normalization has become an integral part of most-if-not-all cutting edge deep networks which signifies the relevance of our work for estimating model uncertainty.
REFERENCES
Ryan P Adams. Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks. Journal of Machine Learning Research, 37:1­6, 2015.
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. arXiv preprint arXiv:1511.00561, 2015.
Thang D. Bui, Daniel Herna´ndez-Lobato, Yingzhen Li, Jose´ Miguel Herna´ndez-Lobato, and Richard E. Turner. Deep Gaussian Processes for Regression using Approximate Expectation Propagation. In ICML, 2016.
Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3d object detection for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2147­2156, 2016.
Ugljesa Djuric, Gelareh Zadeh, Kenneth Aldape, and Phedias Diamandis. Precision histology: how deep learning is poised to revitalize histomorphology for personalized cancer care. npj Precision Oncology, 1(1):22, 2017.
10

Under review as a conference paper at ICLR 2018
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, Feb 2017.
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning. ICML, 48:1­10, 2015.
Zoubin Ghahramani. Delve Datasets. University of Toronto, 1996. URL http://www.cs. toronto.edu/{~}delve/data/kin/desc.html.
Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553): 452­459, May 2015.
Tilmann Gneiting and Adrian E Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American Statistical Association, 102(477):359­378, 2007.
Alex Graves. Practical Variational Inference for Neural Networks. NIPS, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Jose´ Miguel Herna´ndez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861­ 1869, 2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 5­13. ACM, 1993.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Arxiv, 2015. URL http://arxiv.org/abs/1502. 03167.
Andrej Karpathy. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford. edu/people/karpathy/convnetjs/demo/regression.html.
Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding. CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.02680.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In ICLR, 2014.
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron Courville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.
Yingzhen Li and Yarin Gal. Dropout Inference in Bayesian Neural Networks with Alphadivergences. arXiv, 2017.
Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, and Yuanqing Lin. Fully convolutional attention networks for fine-grained recognition. arXiv preprint arXiv:1603.06765, 2016.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448­472, 1992.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of Toronto, 1995.
11

Under review as a conference paper at ICLR 2018
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.
Reinhard Selten. Axiomatic characterization of the quadratic scoring rule. Experimental Economics, 1(1):43­62, 1998.
Li Shen. End-to-end training for whole image breast cancer diagnosis using an all convolutional design. arXiv preprint arXiv:1708.09427, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354­359, Oct 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1­9, 2015.
Irvine University of California. UC Irvine Machine Learning Repository, 2017. URL https: //archive.ics.uci.edu/ml/index.html.
12

Under review as a conference paper at ICLR 2018

6 APPENDIX

6.1 VARIATIONAL APPROXIMATION
Assume we were to come up with a faimly of distributions parametrised by  in order to approximate the posterior, q(). Our goal is to set  such that q() is as similar to p(|D) as possible.
One strategy is to minimizing KL(q()||p(|D)), the KL divergence of p(|D) wrt q(). Minimizing KL(q()||p(|D)) is equivalent to maximizing the ELBO:

q() ln p(Y|X, )d - KL(q()||p())

Assuming i.i.d. observation noise, this is equivalent to minimizing:

N
LVA() := -
n=1

q() ln p(yi|f(xi))d + KL(q()||p())

Instead of making the optimization on the full training set, we can use a subsampling (yielding an unbiased estimate of LVA()) for iterative optimization (as in mini-batch optimization):

L^VA()

:=

-

N M

iB

q() ln p(yi|f(xi))d + KL(q()||p())


We now make a reparametrisation: set  = g(, ) where is a RV. The function g and the distribution of must be such that p(g(, )) = q(). Assume q() can be written q(| )p( )d where q(| ) = ( - g(, )). Using this reparametrisation we get:

L^VA()

=

-

N M

p( ) ln p(yi|fg(, )(xi))d + KL(q()||p())

iB

6.2 DISTRIBUTION OF µBu , Bu
Here we approximate the distribution of mean and standard deviation of a mini-batch, separately to two Gaussians:

We know that

µB

=

Mm=1W(j)xm M

Here, xm are the examples in the sampled batch. We will assume these are sampled i.i.d.9. Samples of the random variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the following holds for sufficiently large M (often  30):

µB



N (µ,

2 )
M

As for standard deviation, during training we use:

Then

B =

mM=1(W(j)xm - µB)2 M

 M (B - ) = M

mM=1(W(j)xm

- µB)2

 - 2

M

9Although in practice with deep learning, mini-batches are sampled without replacement, stochastic gradient descent samples with replacement in its standard form.

13

Under review as a conference paper at ICLR 2018

We want to rewrite

.mM=1 (W(j ) xm -µB )2
M

Make

a

Taylor

expansion

of

f (x)

=

 x

around

a

=

2.

With x

=

:Mm=1 (W(j ) xm -µB )2
M

f (x) = f (a) + f (a)(x - a) + f

(a) (x

-

a)2

+

f (3)(a) (x

-

a)3

+

...

2! 3!

 x = 2 +

1

(x - 2) + O[(x - 2)2]

2 2

so

 M (B - ) = M

1 2 2

mM=1(W(j)xm - µB)2 - 2 + M

 M
= 2

O Mm=1(W(j)xm - µB)2 - 2 2 M

1 M

mM=1(W(j)xm

-

µB)2

-

2

+

 OM

Mm=1(W(j)xm - µB)2 - 2

M

2

= 1 2 M

mM=1(W(j)xm - µB)2 - M 2

+

 OM

mM=1(W(j)xm - µB)2 - 2 2

M

consider mM=1(W(j)xm - µB)2. We know that E[W(j)xm] = µ and write

mM=1(W(j)xm - µB)2 =Mm=1((W(j)xm - µ) - (µB - µ))2 =mM=1((W(j)xm - µ)2 + (µB - µ)2 - 2(W(j)xm - µ)(µB - µ)) =Mm=1(W(j)xm - µ)2 + M (µB - µ)2 - 2(µB - µ)mM=1(W(j)xm - µ) =Mm=1(W(j)xm - µ)2 - M (µB - µ)2 =mM=1((W(j)xm - µ)2 - (µB - µ)2)
14

Under review as a conference paper at ICLR 2018

then

 M (B

-

)

=

1 2 M

Mm=1((W(j)xm - µ)2 - (µB - µ)2) - M 2

+

 OM

mM=1(W(j)xm - µB)2 - 2

2

M

= 1 2 M

mM=1(W(j)xm - µ)2 - Mm=1(µB - µ)2 - M 2

+

 OM

Mm=1(W(j)xm - µB)2 - 2

2

M

= 1 2 M

Mm=1((W(j)xm - µ)2 - 2) - mM=1(µB - µ)2

+

 OM

mM=1(W(j)xm - µB)2 - 2

2

M

=

1 2 M

mM=1((W(j)xm

-

µ)2

-

2)

-

1 2 M

Mm=1(µB

-

µ)2

 +O M

Mm=1(W(j)xm - µB)2 - 2 2

M

=

1 2 M

mM=1((W(j)xm

-

µ)2

-

2)

 term A

-

M 2

(µB

-

µ)2

term B

 +O M

Mm=1(W(j)xm - µB)2 - 2 2

M

We go through each term in turn

term C

Term A

We have

Term

A

=

1 2 M

Mm=1((W(j

)xm

- µ)2

- 2)

where Mm=1(W(j)xm - µ)2 is the sum of M RVs (W(j)xm - µ)2. Note that since E[W(j)xm] = µ it holds that E[(W(j)xm - µ)2] = 2. Since (W(j)xm - µ)2 is sampled approximately iid (by assumptions above), for large enough M by CLT it holds approximately that

Mm=1(W(j)xm - µ)2  N (M 2, M Var((W(j)xm - µ)2))

where

Var((W(j)xm - µ)2) = E[(W(j)xm - µ)22] - E[(W(j)xm - µ)2]2 = E[(W(j)xm - µ)4] - 4

Then

Mm=1((W(j)xm - µ)2 - 2)  N (0, M  E[(W(j)xm - µ)4] - M 4)

so

Term

A



N

(0,

E[(W(j)xm - 42

µ)4]

-

4

)

15

Under review as a conference paper at ICLR 2018

Term B

We have

Term

B

=

 M 2

(µB

-

µ)2

=

1 2 M (µB

-

µ)(µB

-

µ)

Consider (µB - µ). As µB -p µ when M   we have µB - µ -p 0. We also have

 M (µB

-

µ)

=

mM=1W(j)xm

 - Mµ

M

which by CLT is approximately Gaussian for large M . We can then make use of the Cramer-Slutzky
Theorem, which states that if (Xn)n1 and (Yn)n1 are two sequences such that Xn -d X and Yn -p a as n   where a is a constant, then as n  , it holds that Xn  Yn -d X  a. Thus, Term B is approximately 0 for large M.

Term C We have

 Term C = O M

Mm=1(W(j)xm - µB)2 - 2 2

M

Since E[(W(j)xm - µ)2] = 2 we can make the same use of Cramer-Slutzky as for Term B, such that Term C is approximately 0 for large M.

Finalizing the distribution We have approximately

 M

(B

-

)



N

(0,

E[(W(j)xm - 42

µ)4]

-

4

)

so

B



N

(,

E[(W(j)xm - µ)4] 42M

-

4

)

6.3 PREDICTIVE MOMENTS
This section provides derivations of the approximate predictive mean and variance in section 3.4, following Gal (2016). In the following we let p denote p(y|x, D), the approximate predictive distribution.

Predictive mean We assume Gaussian iid noise defined by model precision  , i.e. f(x, y) = p(y|f(x)) = N (y; f(x),  -1I). Then:
Ep [y] = yp(y|x, D)dy

= y f(x, y)q()d dy
y
= y N (y; f(x),  -1I)q()d dy
y
= yN (y; f(x),  -1I)dy q()d
y

= f(x)q()d


1 T

T

f^i (x)

i=1

where we take the MC Integral with T samples of  for the approximation in the final step.

16

Under review as a conference paper at ICLR 2018

Predictive variance Our goal is to estimate:

Note that

Covp [y] = Ep [y y] - Ep [y] Ep [y]

Ep [y y] = y yp(y|x, D)dy
y

= y y f(x, y)q()d dy
y

= y yf(x, y)dy q()d
y

= Covf(x,y)(y) + Ef(x,y)[y] Ef(x,y)[y] q()d


=  -1I + f(x) f(x) q()d

=  -1I + Eq()[f(x) f(x)]

  -1I + 1 T

T

f^i (x) f^i (x)

i=1

where again we use MC integration for the final step. Denoting the estimated predictive mean by E~p [y], we get:

Covp [y]



 -1I

+

1 T

T

f^i (x) f^i (x) - E~p [y] E~p [y]

i=1

which is the sum of the variance from observation noise and the sample covariance of taking T stochastic forward passes though the network.

6.4 PRIOR

It is possible to investigate the prior by modelling the random variables in . With some weak
assumptions and approximations (details in Appendix 6.2), we can apply the central limit theorem to get the following distributions of the stochastic variables µBu , Bu

µB



N

(µ,

2 M

),

B



N (,

E[(h

- µ)4] 42M

-

4 )

where M is the mini-batch size, and µ and  are population-level moments. We assume the distri-
bution of each layer's unit weight, under q, is independent of the other units' weights as well as the transformation parameters  and .

We assume that µb and b are independent and Gaussian distributed. The empirical distributions have been numerically checked to be linearly independent and the joint distribution is close to
a bi-variate gaussian. If we assume q() factorizes over all individual parameters in  we get KL(q() p()) = KL(q(1) p(1)) + KL(q(2) p(2)) + ...

It is thus can be possible to derive the prior which with KL-divergence criterion corresponds to the weight decay terms.

6.5 EXTENDED EXPERIMENTAL RESULTS
Below, we provide extended results measuring uncertainty quality. In Tables 3 and 4, we provide tables showing the mean CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par or better than MCDO across several datasets. In Table 5 we provide RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO

17

Under review as a conference paper at ICLR 2018

number number

batch mean (unit-1, layer-1, batch size=32, epoch=10)
70

KS normality test 60 p = 4.963e-97

batch mean fitted Normal median

50

40

30

20

10

0 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08

batch stdev. (unit-1, layer-1, batch size=32, epoch=10)
70

KS normality test 60 p = 3.080e-124

batch stdev. fitted Normal median

50

40

30

20

10

0 0.08 0.09 0.1 0.11 0.12 0.13 0.14 0.15

Figure 4: Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov test checks that µB and B come from a standard normal distribution. More examples are available in Appendix 6.6.

networks. These results indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the accuracy of the network.
In Figure 5 and Figure 6, we provide a full set of our uncertainty quality visualization plots, where errors in predictions are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray). This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for predictive errors.

Dataset
Boston Housing Concrete Energy Efficiency Kinematics 8nm Power Plant Protein Wine Quality (Red) Yacht Hydrodynamics

MCBN
8.50 ±0.86 3.91 ±0.25 5.75 ±0.52 2.85 ±0.18 0.24 ±0.05 2.66 ±0.10 0.26 ±0.07 -56.39 ±14.27

CRPS

p-value

MCDO

6.39e-10 4.53e-14 6.71e-11 2.33e-14 2.32e-4 2.77-12 1.26e-3 5.94e-4

3.06 ±0.33 0.93 ±0.41 1.37 ±0.89 1.82 ±0.14 -0.44 ±0.05 0.99 ±0.08 2.00 ±0.21 21.42 ±2.99

p-value
1.64e-9 3.13e-2 1.38e-1 1.64e-12 2.17e-8 2.34e-12 1.83e-9 2.16e-7

Table 3: CRPS measured on eight datasets over 25 random 80-20 splits of the data. Mean values for MCBN and MCDO are reported along with standard error. A significance test was performed to check if CRPS significantly exceeds the baseline. The p-value from a one sample t-test is reported.

Dataset
Boston Housing Concrete Energy Efficiency Kinematics 8nm Power Plant Protein Wine Quality (Red) Yacht Hydrodynamics

MCBN
10.49 ±1.35 -36.36 ±12.12 10.89 ±1.16
1.68 ±0.37 0.33 ±0.14 2.56 ±0.23 0.19 ±0.09 45.58 ±5.18

p-value
5.41e-8 6.19e-3 1.79e-9 1.29e-4 2.72e-2 4.28e-11 3.72e-2 5.67e-9

PLL MCDO
5.51 ±1.05 10.92 ±1.78 -14.28 ±5.15 -0.26 ±0.18
3.52 ±0.23 6.23 ±0.19 2.91 ±0.35 -41.54 ±31.37

p-value
2.20e-5 2.34e-6 1.06e-2 1.53e-1 1.12e-13 2.57e-21 1.84e-8 1.97e-1

Table 4: PLL measured on eight datasets over 25 random 80-20 splits of the data. Mean values for MCBN and MCDO are reported along with standard error. A significance test was performed to check if PLL significantly exceeds the baseline. The p-value from a one sample t-test is reported.

18

Under review as a conference paper at ICLR 2018

Dataset
Boston Housing Concrete Energy Efficiency Kinematics 8nm Power Plant Protein Wine Quality (Red) Yacht Hydrodynamics

MCBN
2.75 ±0.05 4.78 ±0.09 0.59 ±0.02 0.07 ±0.00 3.74 ±0.01 3.66 ±0.01 0.62 ±0.00 1.23 ±0.05

RMSE BN MCDO

2.77 ±0.05 4.89 ±0.08 0.57 ±0.01 0.07 ±0.00 3.74 ±0.01 3.69 ±0.01 0.62 ±0.00 1.28 ±0.06

2.65 ±0.05 4.80 ±0.10 0.47 ±0.01 0.07 ±0.00 3.74 ±0.02 3.66 ±0.01 0.60 ±0.00 0.75 ±0.03

DO
2.69 ±0.05 4.99 ±0.10 0.49 ±0.01 0.07 ±0.00 3.72 ±0.02 3.68 ±0.01 0.61 ±0.00 0.72 ±0.04

Table 5: RMSE measured on eight datasets over 25 random 80-20 splits of the data. Mean values and standard errors are reported for MCBN and MCDO as well as conventional non-Bayesian models BN and DO.

6.6 BATCH NORMALIZATION STATISTICS
In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for training. The plots show the distribution of BN mean and BN variance over different mini-batches of an actual training of Yacht dataset for one unit in the first hidden layer and the second hidden layer. Data is provided for different epochs and for different batch sizes.

19

Under review as a conference paper at ICLR 2018

Prediction error

Boston Housing

95% CI

12

MCBN

50% CI

12

10 10

Prediction error

88

66

44

22

Concrete

95% CI

25

MCBN

50% CI

25

20 20

Prediction error

15 15

10 10

55

Prediction error

Prediction error

Energy

95% CI

3

MCBN

50% CI

3

2.5 2.5

Prediction error

22

1.5 1.5

11

0.5 0.5

Kinematics

95% CI

0.3

MCBN

50% CI

0.3

0.2 0.2

Prediction error

Prediction error

0.1 0.1

Boston Housing MCDO

95% CI 50% CI

Concrete MCDO

95% CI 50% CI

Energy MCDO

95% CI 50% CI

Kinematics MCDO

95% CI 50% CI

Figure 5: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show MCBN's (blue) and MCDO's (red) model uncertainty (light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.
20

Under review as a conference paper at ICLR 2018

Prediction error Prediction error

Power Plant

95% CI

16

MCBN

50% CI

16

14 14

12 12

10 10

88

66

44

22

Power Plant MCDO

95% CI 50% CI

Prediction error

Wine Quality

95% CI

Wine Quality

95% CI

2.5

MCBN

50% CI

2.5

MCDO

50% CI

22

Prediction error

1.5 1.5

11

0.5 0.5

Prediction error

Yacht

95% CI

Yacht

95% CI

4.5

MCBN

50% CI

4.5

MCDO

50% CI

44

3.5 3.5

Prediction error

33

2.5 2.5

22

1.5 1.5

11

0.5 0.5

Figure 6: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show MCBN's (blue) and MCDO's (red) model uncertainty (light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.
21

Under review as a conference paper at ICLR 2018

number

batch mean (unit-1, layer-1, batch size=32, epoch=10)
70

KS normality test 60 p = 4.963e-97

batch mean fitted Normal median

50

40

30

20

10

0 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08

number

batch mean (unit-1, layer-1, batch size=32, epoch=100)
80

KS normality test 70 p = 2.668e-106
60

batch mean fitted Normal median

50

40

30

20

10

0 -0.02 -0.015 -0.01 -0.005 0

0.005 0.01 0.015 0.02

number

number

batch mean (unit-1, layer-1, batch size=128, epoch=10)
80

KS normality test 70 p = 6.353e-101
60

batch mean fitted Normal median

50

40

30

20

10

0 -0.06

-0.04

-0.02

0

0.02 0.04 0.06

batch mean (unit-1, layer-2, batch size=32, epoch=10)
80

KS normality test 70 p = 3.107e-120
60

batch mean fitted Normal median

50

40

30

20

10

0 0.06 0.065 0.07 0.075 0.08

batch mean (unit-1, layer-2, batch size=128, epoch=10)
70

KS normality test 60 p = 8.417e-123

batch mean fitted Normal median

50

40

30

20

10

0 0.072 0.074 0.076 0.078 0.08 0.082 0.084 0.086 0.088 0.09 0.092

number

number

batch mean (unit-1, layer-1, batch size=128, epoch=100)
70

KS normality test 60 p = 9.099e-109

batch mean fitted Normal median

50

number

40

30

20

10

0 -6 -4 -2 0 2 4 6 8
10-3

batch mean (unit-1, layer-2, batch size=32, epoch=100)
70

KS normality test 60 p = 5.401e-111

batch mean fitted Normal median

50

40

30

20

10

0 -9.5 -9 -8.5 -8 -7.5 -7 -6.5 -6
10-3

batch mean (unit-1, layer-2, batch size=128, epoch=100)
60

KS normality test 50 p = 1.393e-113

batch mean fitted Normal median

40

30

20

10

0

0.021

0.0215

0.022

0.0225

0.023

number

Figure 7: The distribution of means of mini-batches during training of one of our datasets. The distribution closely follows our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for different batch sizes.

22

Under review as a conference paper at ICLR 2018

number

number

batch stdev. (unit-1, layer-1, batch size=32, epoch=10)
70

KS normality test 60 p = 3.080e-124

batch stdev. fitted Normal median

50

40

30

20

10

0 0.08 0.09 0.1 0.11 0.12 0.13 0.14 0.15

batch stdev. (unit-1, layer-1, batch size=128, epoch=10)
80

KS normality test 70 p = 1.342e-145
60

batch stdev. fitted Normal median

50

40

30

20

10

0 0.19 0.2 0.21 0.22 0.23 0.24 0.25 0.26

batch stdev. (unit-1, layer-2, batch size=32, epoch=10)
60

KS normality test 50 p = 2.996e-130

batch stdev. fitted Normal median

40

30

20

10

0 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19

number

number

batch stdev. (unit-1, layer-1, batch size=32, epoch=100)
70

KS normality test 60 p = 6.777e-115

batch stdev. fitted Normal median

50

40

30

20

10

0 0.028 0.03 0.032 0.034 0.036 0.038 0.04 0.042 0.044 0.046 0.048

number

batch stdev. (unit-1, layer-1, batch size=128, epoch=100)
70

KS normality test 60 p = 5.395e-114

batch stdev. fitted Normal median

50

40

30

20

10

0 0.023 0.024 0.025 0.026 0.027 0.028 0.029 0.03 0.031 0.032

batch stdev. (unit-1, layer-2, batch size=32, epoch=100)
70

KS normality test 60 p = 1.530e-113

batch stdev. fitted Normal median

50

40

30

20

10

0 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06

number

number

batch stdev. (unit-1, layer-2, batch size=128, epoch=10)
70

KS normality test 60 p = 1.725e-149

batch stdev. fitted Normal median

50

40

30

20

10

0 0.22 0.23 0.24 0.25 0.26 0.27

number

batch stdev. (unit-1, layer-2, batch size=128, epoch=100)
70

KS normality test 60 p = 4.207e-115

batch stdev. fitted Normal median

50

40

30

20

10

0 0.03

0.032

0.034

0.036

0.038

0.04

Figure 8: The distribution of standard deviation of mini-batches during training of one of our datasets. The distribution closely follows our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for different batch sizes.

23

