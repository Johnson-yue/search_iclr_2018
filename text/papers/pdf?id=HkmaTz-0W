Under review as a conference paper at ICLR 2018
VISUALIZING THE LOSS LANDSCAPE OF NEURAL NETS
Anonymous authors Paper under double-blind review
ABSTRACT
As the effectiveness of deep neural networks continues to improve, there remain significant questions about how choices in network architecture, batch size, and parameter initialization impact the network's trainability and effectiveness. Theoreticians have made significant discoveries, but it is often difficult to translate the assumptions required by theoretical results into meaningful statements about the differences between the neural nets used in practice. Another approach to understanding neural nets is to use visualizations to explore the empirical behavior of loss functions. However, without great care, these visualizations can produce distorted or misleading results. In this paper, we describe a simple approach to visualizing neural network loss functions that provides new insights into the trainability and generalization of neural nets. The technique is used to explore the effect of network architecture, choice of optimizer, and algorithm parameters on loss function minima.
1 INTRODUCTION
Training neural networks requires minimizing a high-dimensional non-convex loss function ­ a task that is hard in theory, but sometimes easy in practice. Despite the NP-hardness of training general neural loss functions (Blum & Rivest (1989)), simple gradient methods often find global minimizers (parameter configurations with zero training error), even when data and labels are randomized before training (Zhang et al. (2017)). However, this good behavior is not universal; the trainability of neural nets is highly dependent on network architecture design choices, the choice of optimizer, variable initialization, and a variety of other considerations. Unfortunately, the effect of each of these on training process is often unclear.

(a) without skip connections

(b) with skip connections

Figure 1: The loss surfaces of ResNet-56 with/without skip connections. The vertical axis is logarithmic to show dynamic range. The x/y axes depict the same distance scale in both plots.
To clarify these questions, a number of authors have used theoretical tools to peer into the structure of loss functions without resorting to expensive computations. Unfortunately, some of these theoretical

1

Under review as a conference paper at ICLR 2018
studies have arrived at conflicting conclusions, or require assumptions that are difficult to validate for practical neural network implementations. It is therefore desirable to visualize network loss function topography. However, this landscape is difficult to visualize because it lives in a high dimensional space. Furthermore, exact evaluation of a neural loss function requires a full pass through the training data, which makes visualization a computationally expensive task.
Our goal is to use high-resolution visualizations to provide an empirical characterization of neural loss functions, and to explore how different network architecture choices affect the loss landscape. Furthermore, we explore how the non-convex structure of neural loss functions relates to their trainability, and how the geometry of neural minimizers (i.e., their sharpness/flatness, and their surrounding landscape), affects their generalization properties. To do this in a meaningful way, we begin by studying previous work in visualization. We review the caveats of different visualization methods, and propose a normalization scheme that enables us to visualize minima more accurately. We then use our visualizations to explore sharpness and flatness, the effect of skip connections, and the effect of changing the number and width of network layers.
2 THEORETICAL BACKGROUND
Visualizations have the potential to help us answer several important questions about why neural networks work. In particular, why are we able to minimize highly non-convex neural loss functions? And why do the resulting minima generalize?
Because of the difficultly of visualizing loss functions, most studies of loss landscapes are largely theoretical in nature. A number of authors have studied our ability to minimize neural loss functions. Using random matrix theory and spin glass theory, several authors have shown that local minima are of low objective value (Dauphin et al. (2014); Choromanska et al. (2015)). It can also be shown that local minima are global minima, provided one assumes linear neurons (Hardt & Ma (2017)), very wide layers (Nguyen & Hein (2017)), or full rank weight matrices (Yun et al. (2017)). These assumptions have been relaxed by Kawaguchi (2016) and Lu & Kawaguchi (2017), although some assumptions (e.g., of the loss functions) are still required. Soudry & Hoffer (2017); Freeman & Bruna (2017); Xie et al. (2017) also analyzed shallow networks with one or two hidden layers under mild conditions.
Another approach is to show that we can expect good minimizers, not simply because of the endogenous properties of neural networks, but because of the optimizers. For restricted network classes such as those with one hidden layer, with some extra assumptions on the sample distribution, globally optimal or near-optimal solutions can be found by common optimization methods (Soltanolkotabi et al. (2017); Li & Yuan (2017); Tian (2017)). For networks with specific structures, Safran & Shamir (2016) and Haeffele & Vidal (2017) show there likely exists a monotonically decreasing path from an initialization to a global minimum. Swirszcz et al. (2017) show counterexamples that achieve "bad" local minima for toy problems. Finally, gradient descent methods with random initializations can asymptotically escape saddles (Lee et al. (2016)), though it may take a long time (Du et al. (2017)).
Also of interest is work on assessing the sharpness/flatness of local minima. Hochreiter & Schmidhuber (1997) defined "flatness" as the size of the connected region around the minimum where the training loss remains low. Keskar et al. (2017) propose -sharpness, which looks at the maximum loss in a bounded neighborhood of a minimum. Flatness can also be defined using the local curvature of the loss function at a critical point. Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian. However, Dinh et al. (2017) show that these quantitative measure of sharpness are problematic because they are not invariant to symmetries in the network, and are thus not sufficient to determine its generalization ability. This issue was addressed in Chaudhari et al. (2017), who used "local entropy" as a measure of sharpness. This measure is invariant to the simple transformation used by Dinh et al. (2017), but difficult to quantify for large networks.
Theoretical results make some restrictive assumptions such as the independence of the input samples, or restrictions on non-linearities and loss functions. For this reason, visualizations play a key role in verifying the validity of theoretical assumptions, and understanding loss function behavior in real-world systems. In the next section, we briefly review methods that have been used for this purpose.
2

Under review as a conference paper at ICLR 2018

3 VISUALIZATION METHODS AND EMPIRICAL STUDIES

Neural networks are trained on a corpus of feature vectors (e.g., images) {xi} and accompanying labels {yi} by minimizing a loss of the form

1m

L() = m

(xi, yi; )

i=1

where  denotes the parameters (weights) of the neural network, the loss function (xi, yi; ) measures how well the neural network with parameters  predicts the label of a data sample, and m is the
number of data samples.

Neural nets contain many parameters, and so their loss functions live in a very high-dimensional space. Unfortunately, visualizations are only possible using low-dimensional 1d (line) or 2d (surface) plots. Several methods exist for closing this dimensionality gap.

1-Dimensional Linear Interpolation One simple and lightweight way to plot loss functions is to choose two sets of parameters 1 and 2, and plot the values of the loss function along the line connecting these two points. This strategy was taken by Goodfellow et al. (2015), who studied the loss surface along the line between a (random) initial guess, and a nearby minimizer obtained by stochastic gradient descent. This method has been widely used to study the "sharpness" and "flatness" of different minima, and the dependence of sharpness on batch-size (Keskar et al. (2017); Dinh et al. (2017)).
Smith & Topin (2017) use the same 1D interpolation technique to show different minima and the "peaks" between them. Im et al. (2016) extend the 1D interpolation by plotting the line between minima obtained via different optimizers, and studying the divergence of different minimizers with the same initialization.
The 1D linear interpolation method suffers from several weaknesses. First, it is difficult to visualize non-convexities using 1D plots. Indeed, the authors of (Goodfellow et al. (2015)) found that loss functions appear to lack local minima along the minimization trajectory. We will see later, using 2D methods, that some loss functions have extreme non-convexities, and that these non-convexities correlate with the difference in performance between different network architectures. Second, this method does not consider batch normalization or invariance symmetries in the network. For this reason, the visual sharpness comparisons produced by 1D interpolation plots may be misleading; this issue will be explored in depth in the Section 5.

2D Contour Plots To use this approach, one chooses a center point, , in the graph, and chooses two direction vectors,  and . One then plots a function of the form f () = L( + ) in the 1d (line) case, or f (, ) = L( +  + ) in the 2D (surface) case. This approach was used in (Goodfellow et al. (2015)) to explore the trajectories of different minimization methods. It was also used in (Im et al. (2016)) to show that different optimization algorithms find different local minima within the 2d projected space.
Contour plots in 2 dimensions are advantageous over 1D plots because the shape of the contours make it easy to assess the level of convexity in the loss surface. However, as we will see below, 2D plots can cause extreme distortions of the loss function if an appropriate "normalization" scheme is not used.

4 PLOTTING APPROACH: RANDOM DIRECTIONS & FILTER-WISE
NORMALIZATION
We propose to produce plots using random direction vectors,  and , each sampled from a random Gaussian distribution with appropriate scaling (described below). Random projections of this type are advantageous for several reasons. Intuitively, random directions give us a sense of the "typical" behavior of the loss function. More rigorously, random projections of high-dimensional vectors are known to be approximately distance preserving (Johnson & Lindenstrauss (1984); Bingham & Mannila (2001); Ailon & Chazelle (2009)), and random Gaussian vectors in high dimensions are

3

Under review as a conference paper at ICLR 2018

nearly orthogonal with high probability (Goldstein & Studer (2016)). In principle, these properties allow us to use the parameters  and  to form the axes of a surface plot without distorting the "shape" of the loss function. While this approach seems simple, several complexities arise when applying it to the complex networks used today, and a naive application of the method can yield wildly distorted results, particularly for batch-normalized nets.

Because each filter of the neural network might live on a different size scale, a plot of the loss function will appear to change very quickly in the direction of a small filter, and very slowly in the direction of large filter, despite the fact that both of these filters may be equally important and their scalings are not meaningful. When batch normalization is used, this size scale is irrelevant to the behavior of the network, and should be discarded. We illustrate this distortion with an example in Section 5.

To remove this scaling effect, we plot loss functions using filter-wise normalized directions. To obtain such directions for a network with parameters , we begin by producing a random Gaussian direction vector d with dimensions compatible with . Then we normalize each filter in d to have the same norm of the corresponding filter in . In other words, we make the replacement

di 

di di

i ,

where df represents the ith filter of d (not the ith weight), and i denotes the Frobenius norm of the ith filter of . Note that the filter-wise normalization is different from that of (Im et al. (2016)),
which normalize the direction without considering the norm of individual filters.

The proposed scaling is an important factor when making meaningful plots of loss function geometry. We will explore the importance of proper scaling below as we explore the sharpness/flatness of different minimizers.

5 THE SHARP VS FLAT DILEMMA
It is widely thought that small-batch SGD produces "flat" minimizers that generalize better, while large batch sizes produce "sharp" minima with poor generalization (Chaudhari et al. (2017); Keskar et al. (2017); Hochreiter & Schmidhuber (1997)). This claim is disputed though, with Dinh et al. (2017); Kawaguchi et al. (2017) arguing that generalization is not directly related to the curvature of loss surfaces, and some authors proposing specialized training methods that achieve good performance with large batch sizes (Hoffer et al. (2017); Goyal et al. (2017); De et al. (2017)).
Here, we explore the difference between sharp and flat minimizers. We begin by discussing difficulties that arise when performing such a visualization, and how proper normalization can prevent such plots from producing distorted results. We train a CIFAR-10 classifier using a 9-layer VGG network (Simonyan & Zisserman (2015)) with Batch Normalization (Ioffe & Szegedy (2015)). We use two batch sizes: a large batch size of 8192 (16.4% of the training data of CIFAR-10), and a small batch size of 128. Let s and l indicate the solutions obtained by running SGD using small and large batch sizes, respectively1. Using the linear interpolation approach (Goodfellow et al. (2015)), we plot the loss values on both training and testing data sets of CIFAR-10, along a direction containing the two solutions, i.e., f (s + (l - s)). Similar to Keskar et al. (2017), we also superimpose the classification accuracy at the intermediate points. This plot is shown in Figure 2.
In Figure 2(a) and 2(b), we show such a plot with s at x-axis location 0, and l at location 1. As observed by Keskar et al. (2017), we can clearly see that the small-batch solution is quite wide, while the large-batch solution is sharp. However, this sharpness balance can be flipped simply by turning on weight decay (Krogh & Hertz (1992)). Figures 2(c) and 2(d) show results of the same experiment, except this time with a non-zero weight decay parameter. This time, the large batch minimizer is considerably flatter than the sharp small batch minimizer.
This apparent difference in sharpness can be explained by examining the weights of each solution. Histograms of the networks weights are shown for each experiment in Figure 3. We see that, when a
1In this section, we consider the "running mean" and "running variance" as trainable parameters and include them in . Note that the original study by Goodfellow et al. (2015) does not consider batch normalization. These parameters are not included in  in future sections, as they are only needed when interpolating between two minimizers.

4

Under review as a conference paper at ICLR 2018

Accuracy

Loss Loss

5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(a) SGD WD=0 5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(c) SGD, WD=5e-4

Accuracy Loss

Accuracy Loss

5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(b) Adam, WD=0 5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(d) Adam, WD=5e-4

Accuracy

Figure 2: 1D linear interpolation of solutions obtained by small-batch and large-batch methods for VGG9. The blue lines are loss values and the red lines are error. The solid lines are training curves and the dashed lines are for testing. Large batch is at abscissa 0, and small batch is at abscissa 1.

large batch is used with zero weight decay, the resulting weights tends to be smaller than in the small batch case. We reverse this effect by adding weight decay; in this case the large batch minimizer has much larger weights than the small batch minimizer. This occurs for a simple reason: because batch normalization was used, the relative largeness or smallness of the weights doesn't affect the network's behavior (the scale of the weights is removed by the batch normalization layer), and thus the weight decay (which is equivalent to an 2-norm penalty on the weights) is allowed to shrink the weights toward zero as training progresses. A smaller batch size results in more weight updates per epoch than a large batch size, and so this shrinking effect is more pronounced.
This affects our plot because smaller weights make the minimizer of the network appear sharper. When a filter has small weights, a small change in the weights can have a dramatic effect. For example, if a filter has entries of size 0.1, then an increase of size 1 would increase the weights by an order of magnitude, and dramatically alter the neural net performance. However, if the weights are large, then the network is insensitive to small changes. When a filter has entries of size 1000, for example, an increase of size 1 has a negligible effect (one tenth of one percent) on the output of the filter. When batch normalization layers are placed after a filter, the scale of weights becomes irrelevant; both small and large weights may produce the same output because the batch normalization layer re-scales the outputs to have unit variance. However this difference in sensitivity to changes in the weights persists, making the large-batch minimizer appear flatter when weight decay is used.
Weight scaling is still an issue even without batch normalization. When ReLU non-linearities are used, the weights in one layer can be multiplied by 10 (or any constant), and the weights of the next layer could be divided by 10 to compensate, thus resulting in an equivalent network with different weight scaling. This weight rescaling invariance was exploited by Dinh et al. (2017) to build pairs of equivalent networks that have different apparent sharpness.
Normalized sharpness comparison We repeat the experiment in Figure 5, but this time we plot the loss function along a random filter-wise normalized direction through each minimizer. This removes the apparent differences in geometry cause by the scaling depicted in Figure 3. The results,
5

Under review as a conference paper at ICLR 2018

Accuracy

Accuracy

5 1e5 4

bs=128 bs=8192

5 1e5 4

bs=128 bs=8192

33

22

11

0 0.010 0.005 0.000 0.005 0.010 0 0.04 0.02 0.00 0.02 0.04

(a) SGD, WD=5e-4

(b) Adam, WD=5e-4

5 1e5

bs=128

5 1e5

bs=128

4 bs=8192 4 bs=8192

33

22

11

0 0.2 0.1 0.0 0.1 0.2 0 0.2 0.1 0.0 0.1 0.2

(c) SGD WD=0

(d) Adam, WD=0

Figure 3: Histogram of weights. With non-zero weight decay, small-batch methods produce more weights with small scales. With zero weight decay, small-batch methods produces more weights with large scales.

presented in Figure 4, still show differences in sharpness between small batch and large batch minima, however these differences are much more subtle than it would appear in the un-normalized plots.

Loss Loss

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(a) SGD,WD=5e-4

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(c) SGD, WD=0

Accuracy Loss

Accuracy Loss

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(b) Adam, WD=5e-4

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(d) Adam,WD=0

Figure 4: The shape of minima obtained using different optimization algorithms, with varying batch size and weight decay. The leftmost plot in each subfigure is batch size 128, and the rightmost subfigure is batch size 8192.

We also visualize these results using two random directions and contour plots. As shown in Figure 5, the solutions obtained with small batch size and non-zero weight decay has wider contours than the sharper large batch solution.
Generalization and Flatness While our results show that the difference in sharpness between small and large batch minimizers is fairly subtle, our results still support the hypothesis that flat minima generalize better. Table 1 shows the test error for the different minima in Figure 4. For both the SGD and Adam (Kingma & Ba (2014)) optimizers, large batches produced visually sharper minima (although not dramatically so) with higher test error. Interestingly, the Adam optimizer attained larger test error than SGD, and, as predicted, the corresponding minima are visually sharper. Results of a similar experiment using ResNet-56 are presented in the Appendix.

6 WHY CAN WE TRAIN NEURAL NETS? INSIGHTS ON THE (NON) CONVEXITY
STRUCTURE OF LOSS SURFACES
Our ability to find global minimizers to neural loss functions is not universal; it seems that some neural architectures are easier to minimize than others. For example, using skip connections, He et al. (2016) were able to train extremely deep architectures, while comparable architectures without skip connections are not trainable. Furthermore, our ability to train seems to depend strongly on the initial parameters from which training starts.

6

Under review as a conference paper at ICLR 2018

4.100

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.001.00

3.600 0.100 3.100
8.100 7.9986...06160000000 1.100

3.100 0.75 0.50

0.600 0.25 0.00

3.100 3.600

4.100

1.00 0.75

8.96.01000 6.100

1.100 2.100

2.600 3.6404.10.60500.1050.6060.1000

0.25 0.50 0.75 1.00

0.50 0.25 0.00 0.25 0.50 0.75 1.001.00

4.100 3.600 2.600 0.100

1.600 4.600 5.100

2.100

0.600 3.100

8.600 7.100

0.75 0.50 0.25 0.00 0.25

(a) SGD, WD=5e-4

9.600 8.100 6.600 5.600

9.100 0.50 0.75

1.00

7.100
0.100 1.600
6.600

1.00 0.75

9.69880...16107000.6000760.1.060600.0100

0.50

0.25

0.00

8.100 4.100 3.600 6.600 7.600 8.69.01900.6000
0.600 2.600

1.00 0.75 0.50 0.25 0.00

7.100
5.100 43..660000 0.100

8.100 5.600 5.646..01100300.120.0100
7.680.01009.600

8.600 2.600
0.600

0.25

0.50 0.75

9.6980..160000087..16000 5.100

2.100 4.600

1.100 3.100

1.001.00

0.75

7.100 0.50 0.25 0.00

1.600 0.25 0.50

899.6..61000000 0.75 1.00

0.25 6.600 1.100 1.600 0.50 9.100
0.75
1.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00

(c) SGD, WD=0

1.00 0.75 0.50

88.677.106..16060.6000.150000.060050.100 4.600 4.100

0.25

0.00

3.100

2.100 1.600 0.100

5.100

5.100

6.660.0100 5.600

1.00 0.75

5.100

5.100

0.50

0.25

0.00

3.600

2.100

3.100 0.100

4.650.15006.606.10.76007.00.81608.001.0600000

2.600 1.600

0.600

1.100 6.100
7.100

0.25 0.50 0.75 1.001.00

4.600 3.600 0.75 0.50

0.600 2.600
0.25 0.00 0.25

65..66700.10000 0.50 0.75

8.600

1.00

4.600

0.25 1.100

6.100 9.9881...06610000000

0.50 3.600 4.100

0.75 1.001.00

0.75

7.600

7.100 0.50

0.25 0.00

5.600 7.100 0.25 0.50

0.75

6.600 1.00

(b) Adam, WD=5e-4

6.600 9.600 0.600 5.100 87..160000

8.600 6.600 0.600 2.100 5.600

9.600 4.600 3.600 2.600
9.100 7.100 1.100
3.600 6.600 8.600

1.00 0.75 7.600 4.600 0.50 3.100 0.25 0.100
0.00

1.00 0.75 0.50 0.25 0.00

9.100 6.100
0.100

5.100

0.25 0.50 0.75

1.600 2.100 5.660.0100

2.600 4.100 8.100

0.25 0.50 0.75

3.100 1.100 1.600 4.100
7.100 7.600

8.100

1.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00

(d) Adam, WD=0

Figure 5: 2D visualization of solutions obtained by SGD with small-batch and large-batch.

Table 1: Test accuracy for VGG-9 and ResNet-56 with different different optimization algorithms and hyper-parameters. Visualizations of ResNet-56 are in the appendix.

VGG-9 ResNet-56

WD=5e-4 WD=0
WD=5e-4 WD=0

SGD bs=128 bs=8192 6.00 10.19 7.37 11.07 bs=128 bs=4096 5.89 10.59 8.26 13.93

Adam bs=128 bs=8192 7.80 9.52 7.44 10.91 bs=128 bs=4096 7.67 12.36 9.55 14.30

Using visualization methods, we do an empirical study of neural architectures to explore why the non-convexity of loss functions seems to be problematic in some situations, but not in others. We aim to provide insight into the following questions: Do loss functions have significant non-convexity at all? If prominent non-convexities exist, why are they not problematic in all situations? Why are some architectures easy to train, and why are results so sensitive to the initialization?
6.1 EXPERIMENTAL SETUP
To understand the effects of network architecture on non-convexity, we trained a number of networks, and plotted the landscape around the obtained minimizers using the filter-normalized random direction method described in Section 4.
We consider three classes of neural networks:
· Residual networks that are optimized for performance on CIFAR (He et al. (2016)). We consider ResNet-20, ResNet-56, and ResNet-110, where each name is labeled with the number of convolutional layers it has.
· "VGG-like" networks that do not contain shortcut/skip connections. We produced these networks simply by removing the skip connections from the CIFAR-optimized ResNets. We call these networks ResNet-20-noshort, ResNet-56-noshort, and ResNet-110-noshort. Note that these networks do not all perform well on the CIFAR-10 task. We use them purely for experimental purposes to explore the effect of shortcut connections.
· "Wide" ResNets with filter numbers that have been optimized for ImageNet rather than CIFAR. These networks have more filters per layer than the CIFAR optimized networks, and also have different numbers of layers. These models include ResNet-18, ResNet-34, and ResNet-50.
7

Under review as a conference paper at ICLR 2018
All models are trained on the CIFAR-10 dataset using Nesterov momentum SGD with batch-size 128 and 0.0005 weight decay for 300 epochs. The learning rate was initialized at 0.1, and decreased by a factor of 10 at epochs 150, 225 and 275. Optimizing deeper experimental VGG-like networks (e.g., ResNet-56-noshort, as described below) is very difficult, and required a smaller initial learning rate.
High resolution 2D plots of the minimizers for different neural networks are shown in Figure 6. Results are shown as contour plots because we find this makes it extremely easy to see the (non)convexity structure of the loss surfaces. For surface plots of ResNet-56, see Figure 1. Note that the center of each plot corresponds to the minimizer found using SGD, and the two axes parameterize two random directions with filter-wise normalization. We make several observations below about how architecture effects the loss landscape. We also provide loss and error values for these networks in Table 2, and convergence curves in Figure 12.
6.2 THE EFFECT OF NETWORK DEPTH
From Figure 6, we see that network depth has a dramatic effect on the loss surfaces of neural networks when skip connections are not used. The network ResNet-20-noshort has a fairly benign landscape dominated by a region with convex contours in the center, and no dramatic non-convexity. This isn't too surprising: the original VGG networks for ImageNet had 19 layers and could be trained effectively (Simonyan & Zisserman (2015)).
However, as network depth increases, the loss surface of the VGG-like nets degrades. ResNet-56noshort has dramatic non-convexities and large regions where the gradient directions (which are normal to the contours depicted in the plots) do not point towards the minimizer at the center, and the loss function becomes extremely large as we move in some directions. ResNet-110-noshort displays even more dramatic non-convexities, and becomes extremely steep as we move in all directions shown in the plot. Furthermore, note that the minimizers at the center of the deep VGG-like nets seem to be fairly sharp. In the case of ResNet-56-noshort, the minimizer is also fairly ill-conditioned, as the contours near the minimizer have significant eccentricity.
6.3 SHORTCUT CONNECTIONS TO THE RESCUE
Shortcut connections have a dramatic effect of the geometry of the loss functions. In Figure 6, we see that the geometry of the ResNet architecture shows almost no dependence on network depth. In fact, the width and shape of the 0.1-level contour is almost identical for the 20- and 110-layer networks.
Interestingly, the effect of skip connections seems to be most important for deep networks. For the more shallow networks (ResNet-20 and ResNet-20-noshort), the effect of skip connections is fairly unnoticeable. However skip connections prevent the explosion of non-convexity that occurs when networks get deep.
6.4 "CONVEXITY PARTITIONING"
One of the most interesting observations seen in Figure 6 is that loss landscapes for all the networks considered seem to be partitioned into regions of apparent convexity, and regions of dramatic nonconvexity. All of the loss functions, including those without skip connections, have a well-defined region of low loss value and convex contours, surrounded by a well-defined region of high loss value and non-convex contours. The region of apparent convexity is fairly wide for all networks with residual connections (which are known to be easily trained), and fairly small for the deep VGG-like networks (which are known to be difficult to train for extreme depths).
This effect may explain the trainability of some neural networks, and also the dependence of trainability on initialization. Residual networks have wide, flat minimizers surrounded by large regions of apparent convexity that capture far-away initializers. In contrast, deep VGG-like landscapes seem to be populated by larger numbers of sharp minima with smaller regions of convexity, and thus have much stronger dependence on the initialization. In general is seems that large numbers of skip connections widen the region of convexity. This is also seen in Figure 11 of the Appendix, which shows an extremely wide region of apparent convexity in the contours of DenseNet (Huang et al. (2017)).
8

Under review as a conference paper at ICLR 2018

8.600 6.600
8.100 6.600 5.600 4.600
6.670.06009.9.160000 1.100 7.100
2.600 5.600

4.600 1.10305.1.10000

1.00 9.600 7.100

0.75 0.50

4.100 3.600 6.100 5.600

0.25

2.600 0.100

0.00

0.25
0.50 0.75 9.100

1.600 0.600 2.100 7.600 9.600

1.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
(a) ResNet-20

7.600 4.100 5.100
5.600

1.00

98..660000 9.100
1.600 0.600 3.100 1.100

0.75
0.50 0.25 9.100 0.00 7.100

3.600 2.600 0.100

6.100

0.25
9.600
0.50 0.75 8.100

4.600 2.100 6.600 9.1600

1.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75

(d) ResNet-20-noshort

1.00

1.00 0.75

5.6050.41.06400.1000

0.50 3.600

0.25

2.100 0.600

3.600

0.00

0.25 0.100

0.50 0.75 1.001.00

5.140.60040.1002.600

1.100 3.600

0.75 0.50 0.25

1.600
0.00

0.25

3.100
4.140.6000
0.50 0.75

(g) ResNet-18

1.00

5.1040.60430..160000
2.100 4.600
2.36.01300.6000

1.00 0.75

787..1.61600000

0.50

0.25

0.00

3.600 9.1090.600 6.761.087160..08660.0010000

6.600 5.100 5.6660.17.060.100000

3.100 2.100

4.600 5.100

0.100

5.100 4.600 5.600 77..610000

0.25 0.50 6.600 01..70501.088..09617.00.6617000.100000.75

6.100

3.600

0.600 2.600

1.100 1.600

4.100

78876.....161660000000000
0.50 0.25 0.00 0.25 0.50
(b) ResNet-56

5.150.6000
0.75 1.00

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.001.00

4.100 5.100 6.100

9.100

5.600 4.100

6.10550..6511.0.041600.0060000

4.600

66.7.16.600000

7.100 54.6.60040.600 6.16000

5.600

3.100 3.600

5.100 6.600

5.600 22.6.01000.160.01300.0100

5.100 3.600

4.100

4.600

8.600

1.600 5.600

6.100

5.1050.600

6.100

5.600

7.600 9.600

8.100 7.100 7.100

0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
(e) ResNet-56-noshort

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.001.00

2.100

3.100
2.600 1.600 1.100 0.600

0.100

2.1020.36.301.006400.1000

0.75 0.50 0.25 0.00 0.25 0.50 2.10020..760530.1030.6010.00
(h) ResNet-34

5.100 7.100 8.100

1.00 0.75 9.968.01.600000 0.50 8.100 0.25
0.00

6.100

4.100

3.100 0.100

2.100

6.100

5.100 56..66000650..160000 7.600

0.25 0.508.600
0.75 1.001.06.0100

0.600 5.600

0.756.100 0.50

1.600 4.600 5.100 3.600
6.600 987...16161000000
0.25 0.00 0.25 0.50

65..16500.010004.60505.176.60..1106000000
0.75 1.00

(c) ResNet-110

1.00

0.75 8.600

5.600 4.100
2.600 3.600 6.600
9.600

7.600 4.600 0.600

0.50 7.100 8.100

0.25 0.00

7.100 1.100

5.100 6.100

1.620.0100

0.25 0.50 0.75

44..6600500.66.060060.660.1000

4.100 5.100

3.100 6.100

5.600

9.600

5.600

5.600 6.100

8.100

1.001.00

0.75

0.50

9.100
0.25 0.00

0.25

0.50

0.75

(f) ResNet-110-noshort

1.00

1.00 0.75

3.6320..0160000

0.50

0.25

0.100

1.600

0.00

1.100 3.1002.600

0.25
0.50 10..70051.050.10044..610000033...76100500

0.600 34..61000045.6.10000
0.50 0.25 0.00 0.25 0.50 0.75 1.00
(i) ResNet-50

6.1050.600

Figure 6: 2D visualization of the solutions of different networks.

Table 2: Loss values and errors for different architectures.

Filters Training Loss Training Error Testing Error

ResNet-20 ResNet-20-noshort

16

0.017 0.025

0.286 0.560

7.37 8.18

ResNet-56 ResNet-56-noshort

16

0.004 0.024

0.052 0.704

5.89 10.83

ResNet-110 ResNet-110-noshort

16

0.002 0.258

0.042 8.732

5.79 16.44

ResNet-18

64 0.002

0.026

5.42

ResNet-34

64 0.001

0.014

4.73

ResNet-50

64 0.001

0.006

4.55

6.5 WIDE MODELS VS THIN MODELS
To see the effect of the number of convolutional filters per layer, we compare the narrow CIFARoptimized ResNets (ResNet-20/56/110) with wide ResNets (ResNet-18/34/50) that have more filters and were optimized for ImageNet. From Figure 6, we can see that the wider models have wider minima and wider regions of apparent convexity than the CIFAR-optimized nets (ResNet-20/56/110). Interestingly, these wider minima correspond to points of better generalization error as shown in Table 2. However, note that these two classes of ResNets have many architectural differences, and comparisons between them should be made with caution.
9

Under review as a conference paper at ICLR 2018

7 VISUALIZING OPTIMIZATION PATHS
Finally, we explore methods for visualizing the trajectories of different optimizers. We noted in Section 4 that random Gaussian directions in high dimensional spaces are nearly orthogonal with high probability2. This was helpful when plotting loss landscapes because random directions form orthogonal axes. However, it causes major problems when trying to plot the trajectories of different optimizers; a randomly chosen direction will be nearly orthogonal to the principle directions of the optimizer, and thus capture almost none of its variation.
This effect is shown in the (ineffective) visualizations in Figure 7. In Figure 7(a), we see the iterates of SGD projected onto the plane defined by two random directions. As predicted by theory, the projection plane is nearly orthogonal to the descent path, and almost none of the motion is captured (notice the super-zoomed-in axes and the seemingly random walk). This problem was noticed by Goodfellow et al. (2015), who then visualized trajectories using one direction that points from initialization to solution, and one random direction. This approach is shown in Figure 7(b). As seen in Figure 7(c), the random axis (which is orthogonal to the trajectory) captures almost no variation, leading to the (misleading) appearance of a straight line path.

0.0020 0.0015 0.0010 0.0005 0.0000 0.0005 0.0010 0.0015 0.00200.0010 0.0005 0.0000 0.0005 0.0010 0.0015 0.0020
(a) Two random directions

6.7.6608.009160.0561.001000

10.000 3.600

100.000 7.100

0.600 2.600 4.600

0.100

5.600 5.600

1.100 1.600 6.100
2.600 3.600 98..867616..00166.000010000

3.100 2.100 4.100

5.100 4.100

7.600

3.100 4.600
7.10880..160000

1.00 0.75 6.100
0.50
0.25
0.00
0.25
0.50 0.75 9.100
1.000.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50
(b) Random direction for y-axis

10.000 7.100 5.100

100.000

1.00 1e 3 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25
(c) Enlarged version (a)

1.50

Figure 7: Ineffective visualizations of optimizer trajectories. These visualizations suffer from the orthogonality of random directions in high dimensions.

Effective trajectory plotting using PCA directions To capture variation in trajectories, we need to use non-random (and carefully chosen directions). This was first noted by Liao & Poggio (2017), who used multidimensional scaling to project optimization trajectories. Here, we suggest an approach based on PCA that allows us to measure how much variation we've captured; we also provide plots of these trajectories along the contours of the loss surface.
Let i denote model parameters at epoch i and the final solution as n. Given m training epochs, we can apply PCA to the matrix M = [0 - n; · · · ; n-1 - n], and then select to two most explanatory directions. To remove the distorting effects of scaling, filter-wise normalization is then applied to the directions, and 2d plots are produced using these high-variation directions as the axes.
Projections on major PCA directions Optimizer trajectories (blue dots) and loss surfaces along filter-normalized PCA directions are shown in Figure 8. Epochs where the learning rate was decreased are shown as red dots. On each axis, we measure the amount of variation in the descent path captured by that PCA direction.
We make several observations about this plots. First, we note that the descent paths are highly curved. Also, we see that the PCA directions were highly effective at capturing the shape of the descent path: we were able to capture between 50% and 90% of the variation in the descent path using only 2 PCA directions.
We also see some interesting behavior in these plots. At early stages of training, the paths tend to move perpendicular to the contours of the loss surface, i.e., along the gradient directions as one would expect from non-stochastic gradient descent. The stochasticity becomes fairly pronounced in several plots during the later stages of training. This is particularly true of the plots that use weight decay and
2The cosine similarity between random vectors in n-dimensions is roughly 2/(n), which is nearly zero when plotting loss functions with millions of dimensions (Goldstein & Studer (2016), Lemma 5).

10

2nd PCA component: 15.04 %

2nd PCA component: 11.04 %

Under review as a conference paper at ICLR 2018

small batches (which leads to more gradient noise, and a more radical departure from deterministic gradient directions). When weight decay and small batches are used, we see the path turn nearly parallel to the contours and "orbit" the solution when the stepsize is large. When the stepsize is dropped (at the red dot), the effective noise in the system decreases, and we see a kink in the path as the trajectory falls into the nearest local minimizer.

2nd PCA component: 8.72 % 2nd PCA component: 9.56 %

2nd PCA component: 6.33 % 2.6003.100 0.100 3.100 4.600 10.006577990......1616160000008.0000008.6100006.600
2nd PCA component: 17.47 %

2.600 1.100

7.600 7.100
20 4.100

10
2.600

1.600

7.600 5.66.601.006000
2.100 3.100 3.600 1.100

87..110000 99..610000

100.000

8 6 2.100 4 2

5.100 4.100 3.600
2.100

0.600

10.000
0.600 0.100

00

10 8.600

4.600 5.100

20 100.000
5 01st PC5A com10pone15nt: 2420.57 %25

30

2
4
6 1.600
8 54.100 3.600 0 1st P5 CA c10omp1o5nent2:0 87.7258 %30

(a) SGD,WD=5e-4

35

40 20 0 20 40
20

0.100

10.0

3.600 4.100

4312.6...161000002000431....616100000000 0.600
0.100

5.100 5.600 6.100 6.600
7.100
7.600 8.100
8.600

4.600

0 1st 2P0CA c40omp6o0nent8:0 73.16005 %120 140

7.5 2.100 2.600 3.100 3.600 4.100 4.600 5.0 1.100
2.5 0.600

0.0

0.100

2.5

5.0

1st PCA component: 81.87 %170..50 58.71.0760.0165600...16600000005.44313...0616.1000000000

0

1.600
5

10

3.1030.600 4.100
15 20

(c) SGD, WD=0

25

2nd PCA component: 13.03 % 0.100 0.600

6.645450....0611630000.60000030.120.6000 1.100 32..16300.00604540...116000000

20 10 8.600

100.000

6.600 4.100

3.600 3.600

1.100

0
0.100

0.600

10 2.100 2.600 3.100
20 57..610000 9.10790..660000

4.600 6.100

100.000

1.600 5.100 10.000

8.100

100.000

15

10

5 2.100 1.600

7.600

6.100

7.8891...061190000.006000 10.000

0

5

10

5 01st PC5A com10pone15nt: 3220.80 %25 30

15 0 1st PC10A comp2o0nent: 7310.74 % 40

(b) Adam, WD=5e-4

2nd PCA component: 9.25 %

6.150.60034504...661.100000000 2.600
2.600 3.100 3.600 4.100 4.600 5.100 13..614002.0043211.....60661100000000000 0.600 0.100
0.100

100 15 9.600

10 2.100 3.100 500 5 0.100 0.600 1.100 1.600

6.6007878...6.11600000000 9.100

0

50 5

100
150 50 0 1st50PCA100com15p0 on2e00nt:27507.93700% 350 400

10

1st PCA component: 80.31 %15 103.6030.1002.6210.1.060000

0

10 20 30 40

(d) Adam,WD=0

50

Figure 8: Projected learning trajectories use normalized PCA directions for VGG-9. The left plot in each subfigure uses batch size 128, and the right one use batch size 8192.

8 CONCLUSION
In this paper, we presented a new, more accurate visualization technique that provided insights into the consequences of a variety of choices facing the neural network practitioner, including network architecture, optimizer selection, and batch sizing.
Neural networks have advanced dramatically in recent years, largely on the back of anecdotal knowledge and theoretical results with complex assumptions. For progress to continue to be made, a more general understanding of the structure of neural networks is needed. Our hope is that effective visualization, when coupled with continued advances in theory, can result in faster training, simpler models, and better generalization.

11

Under review as a conference paper at ICLR 2018
REFERENCES
Nir Ailon and Bernard Chazelle. The fast johnson­lindenstrauss transform and approximate nearest neighbors. SIAM Journal on computing, 39(1):302­322, 2009.
Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 245­250. ACM, 2001.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Advances in neural information processing systems, pp. 494­501, 1989.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing gradient descent into wide valleys. ICLR, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Automated inference with adaptive batches. In Artificial Intelligence and Statistics, pp. 1504­1513, 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. ICLR, 2017.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Barnabas Poczos, and Aarti Singh. Gradient descent can take exponential time to escape saddle points. arXiv preprint arXiv:1705.10412, 2017.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. ICLR, 2017.
Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. arXiv preprint arXiv:1610.07531, 2016.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. ICLR, 2015.
Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Benjamin D Haeffele and Rene´ Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331­7339, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. ICLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. NIPS, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. CVPR, 2017.
Daniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of deep network loss surfaces. arXiv preprint arXiv:1612.04010, 2016.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. 2015.
12

Under review as a conference paper at ICLR 2018
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.
Kenji Kawaguchi. Deep learning without poor local minima. NIPS, 2016.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In NIPS, 1992.
Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent converges to minimizers. Conference on Learning Theory, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv preprint arXiv:1705.09886, 2017.
Qianli Liao and Tomaso Poggio. Theory of deep learning ii: Landscape of the empirical risk in deep learning. arXiv preprint arXiv:1703.09833, 2017.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. International Conference on Machine Learning, 2017.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016.
Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2015.
Leslie N Smith and Nicholay Topin. Exploring loss function topology with cyclical learning rates. arXiv preprint arXiv:1702.04283, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of neural networks. stat, 1050:17, 2017.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. ICML, 2017.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Artificial Intelligence and Statistics, pp. 1216­1224, 2017.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks. arXiv preprint arXiv:1707.02444, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.
13

Under review as a conference paper at ICLR 2018

9 APPENDIX
9.1 LARGE-BATCH AND SMALL-BATCH RESULTS FOR RESNET-56

Accuracy

Loss Loss

5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(a) SGD, WD=0.0005 5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(c) SGD WD=0

Accuracy Loss

Accuracy Loss

5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(b) Adam, WD=0.0005 5 100 4 80 3 60 2 40 1 20 00.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.500
(d) Adam, WD=0

Accuracy

Figure 9: 1D linear interpolation of solutions obtained by small-batch and large-batch methods for ResNet56. The blue lines are loss values and the red lines are error. The solid lines are training curves and the dashed lines are for testing.

Loss Loss

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(a) SGD,WD=0.0005

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(c) SGD, WD=0

Accuracy Loss

Accuracy Loss

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(b) Adam, WD=0.0005

5 100 5 100

4 80 4 80

Accuracy Loss

3 60 3 60

2 40 2 40

1 20 1 20

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

01.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.000

(d) Adam,WD=0

Figure 10: The shape of minima obtained via different optimization algorithms for ResNet-56, with varying batch size and weight decay. The leftmost plot in each subfigure is batch size 128 and the rightmost is batch size 4096.

Generalization error for each plot is shown in Table 3.

10 TEST AND TRAINING DATA FOR VARIOUS NETWORKS

Accuracy

Accuracy

14

Under review as a conference paper at ICLR 2018

Table 3: Test accuracy for ResNet-56 with different optimization algorithms and batch-size/weight-

decay parameters.

SGD

Adam

bs=128 bs=4096 bs=128 bs=4096

WD=5e-4 5.89 10.59 7.67 12.36

WD=0 8.26 13.93 9.55 14.30

1.00

2.600 2.100 3.64.01040.506.501.006000 0.600
2.600 1.100

2.6002.100 0.100

0.75

0.50

0.25

0.00

0.25

0.50

0.75 4.1030.630.1000 1.001.00 0.75

0.50

1.600
0.25 0.00 0.25

0.50

3.314.06.01400.06000
0.75 1.00

Figure 11: The shape of solution for DenseNet-121 trained on CIFAR-10. The final training error is 0.002 and the testing error is 4.37

Training loss

2.00

ResNet-20

2.00

1.75

ResNet-56

1.75

1.50

ResNet-110

1.50

ResNet-18 ResNet-34 ResNet-50

2.00 1.75 1.50

ResNet-20_noshort ResNet-56_noshort ResNet-110_noshort

Training loss

Training loss

1.25 1.25 1.25

1.00 1.00 1.00

0.75 0.75 0.75

0.50 0.50 0.50

0.25 0.25 0.25

0.00 0 50 100 Ep1o5c0hs 200 250 300 0.00 0 50 100 Ep1o5c0hs 200 250 300 0.00 0 50 100 Ep1o5c0hs 200 250 300

(a) ResNet-CIFAR

(b) ResNet-ImageNet

(c) ResNet-CIFAR-noshort

40

ResNet-20

40

35

ResNet-56

35

ResNet-110

30 30

ResNet-18 ResNet-34 ResNet-50

40 35 30

ResNet-20_noshort ResNet-56_noshort ResNet-110_noshort

25 25 25

Error (%)

Error (%)

20 20 20

15 15 15

10 10 10

555

0 0 50 100 Ep1o5c0hs 200 250 300 0 0 50 100 Ep1o5c0hs 200 250 300 0 0 50 100 Ep1o5c0hs 200 250 300

(d) ResNet-CIFAR

(e) ResNet-ImageNet

(f) ResNet-CIFAR-noshort

Error (%)

Figure 12: Convergence curves for different architectures. The second row is for training loss and the first row is training and testing errors curves.

15

Under review as a conference paper at ICLR 2018

Loss Loss

2.00

CIFAR10, VGG-9, SGD bs=128, WD=0.0005

2.00

CIFAR10, VGG-9, Adam bs=128, WD=0.0005

1.75

bs=128, WD=0

1.75

bs=128, WD=0

bs=8192, WD=0.0005

bs=8192, WD=0.0005

1.50

bs=8192, WD=0

1.50

bs=8192, WD=0

1.25 1.25

1.00 1.00

0.75 0.75

0.50 0.50

0.25 0.25

0.00 0 50 100 Ep1o5c0hs 200 250 300 0.00 0 50 100 Ep1o5c0hs 200 250 300

(a) SGD Loss values

(b) Adam Loss values

Figure 13: Training and testing loss curves for VGG-9. Dashed lines are for testing, solid for training.

16

