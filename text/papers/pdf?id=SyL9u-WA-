Under review as a conference paper at ICLR 2018
STABILIZING GRADIENTS FOR DEEP NEURAL NETWORKS VIA EFFICIENT SVD PARAMETERIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large.
1 INTRODUCTION
Deep neural networks have achieved great success in various fields, including computer vision, speech recognition, natural language processing, etc. Despite their tremendous capacity to fit complex functions, optimizing deep neural networks remains a contemporary challenge. Two main obstacles are vanishing and exploding gradients, that become particularly problematic in Recurrent Neural Networks (RNNs) since the transition matrix is identical at each layer, and any slight change to it is amplified through recurrent layers (Bengio et al. (1994)).
Several methods have been proposed to solve the issue, for example, Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber (1997)) and residual networks (He et al. (2016)). Another recently proposed class of methods is designed to enforce orthogonality of the square transition matrices, such as unitary and orthogonal RNNs (oRNN) (Arjovsky et al. (2016); Mhammedi et al. (2017)). However, while these methods solve the exploding gradient problem, they limit the expressivity of the network.
In this paper, we present an efficient parametrization of weight matrices that arise in a deep neural network, thus allowing us to stabilize the gradients that arise in its training, while retaining the desired expressive power of the network. In more detail we make the following contributions:
· We propose a method to parameterize weight matrices through their singular value decomposition (SVD). Inspired by (Mhammedi et al. (2017)), we attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. The SVD parametrization allows us to retain the desired expressive power of the network, while enabling us to explicitly track and control singular values.
· We apply our SVD parameterization to recurrent neural networks to exert spectral constraints on the RNN transition matrix. Our proposed svdRNN method enjoys similar space and time complexity as the vanilla RNN. We empirically verify the superiority of svdRNN over RNN/oRNN, in some way even LSTM, over an exhaustive collection of time series classification tasks, especially when the network depth is large.
1

Under review as a conference paper at ICLR 2018
· Theoretically, we show how our proposed SVD parametrization can make the optimization process easier. Specifically, under a simple setting, we show that there are no spurious local minimum for the linear svdRNN in the population risk.
· Our parameterization is general enough to eliminate the gradient vanishing/exploding problem not only in RNN, but also in various deep networks. We illustrate this by applying SVD parametrization to problems with non-square weight matrices, specifically multi-layer perceptrons (MLPs) and residual networks.
We now present the outline of our paper. In Section 2, we discuss related work, while in Section 3 we introduce our SVD parametrization and demonstrate how it spans the whole parameter space and does not limit expressivity. In Section 4 we propose the svdRNN model that is able to efficiently control and track the singular values of the transition matrices, and we extend our parameterization to non-square weight matrices and apply it to MLPs in Section 5. Section 6 provides the optimization landscape of svdRNN by showing that linear svdRNN has no spurious local minimum. Experimental results on MNIST and a popular time series archive are present in Section 7. Finally, we present our conclusions and future work in Section 8.
2 RELATED WORK
Numerous approaches have been proposed to address the vanishing and exploding gradient problem. Long short-term memory (LSTM) (Hochreiter & Schmidhuber (1997)) attempts to address the vanishing gradient problem by adding additional memory gates. Residual networks (He et al. (2016)) pass the original input directly to the next layer in addition to the original layer output. Mikolov (2012) performs gradient clipping, while Pascanu et al. (2013) applies spectral regularization to the weight matrices. Other approaches include introducing L1 or L2 penalization on successive gradient norm pairs in back propagation (Pascanu et al. (2013)).
Recently the idea of restricting transition matrices to be orthogonal has drawn some attention. Le et al. (2015) proposed initializing recurrent transition matrices to be identity or orthogonal (IRNN). This strategy shows better performance when compared to vanilla RNN and LSTM. However, there is no guarantee that the transition matrix is close to orthogonal after a few iterations. The unitary RNN (uRNN) algorithm proposed in Arjovsky et al. (2016) parameterizes the transition matrix with reflection, diagonal and Fourier transform matrices. By construction, uRNN ensures that the transition matrix is unitary at all times. Although this algorithm performs well on several small tasks, Wisdom et al. (2016) showed that uRNN only covers a subset of possible unitary matrices and thus detracts from the expressive power of RNN. An improvement over uRNN, the orthogonal RNN (oRNN), was proposed by Mhammedi et al. (2017). oRNN uses products of Householder reflectors to represent an orthogonal transition matrix, which is rich enough to span the entire space of orthogonal matrices. Meanwhile, Vorontsov et al. (2017) empirically demonstrate that the strong constraint of orthogonality limits the model's expressivity, thereby hindering its performance. Therefore, they parameterize the transition matrix by its SVD, W = U V (factorized RNN) and restrict  to be in a range close to 1; however, the orthogonal matrices U and V are updated by geodesic gradient descent using the Cayley transform, thereby resulting in time complexity cubic in the number of hidden nodes which is prohibitive for large scale problems. Motivated by the shortcomings of the above methods, our work in this paper attempts to answer the following questions: Is there an efficient way to solve the gradient vanishing/exploding problem without hurting expressive power? Can we theoretically prove if a new principle actually makes the optimization process easier?
As brought to wide notice in He et al. (2016), deep neural networks should be able to preserve features that are already good. Hardt & Ma (2016) consolidate this point by showing that deep linear residual networks have no spurious local optima. In our work, we broaden this concept and bring it to the area of recurrent neural networks, showing that each layer is not necessarily near identity, but being close to orthogonality suffices to get a similar result.
Generalization is a major concern in training deep neural networks. Bartlett et al. (2017) provide a generalization bound for neural networks by a spectral Lipschitz constant, namely the product of spectral norm of each layer. Thus, our scheme of restricting the spectral norm of weight matrices reduces generalization error in the setting of Bartlett et al. (2017). As supported by the analysis in Cisse et al. (2017), since our SVD parametrization allows us to develop an efficient way to constrain the weight matrix to be a tight frame (Tropp et al. (2005)), we consequently are able to reduce the sensitivity of the network to adversarial examples.
2

Under review as a conference paper at ICLR 2018

3 SVD PARAMETERIZATION

The SVD of the transition matrix W  Rn×n of an RNN is given by W = U V T , where  is the diagonal matrix of singular values, and U, V  Rn×n are orthogonal matrices, i.e., U T U = U U T = I and V T V = V V T = I (Trefethen & Bau III (1997)). During the training of an RNN, our proposal
is to maintain the transition matrix in its SVD form. However, in order to do so efficiently, we need to maintain the orthogonal matrices U and V in compact form, so that they can be easily updated
by forward and backward propagation. In order to do so, as in Mhammedi et al. (2017), we use a
tool that is commonly used in numerical linear algebra, namely Householder reflectors (which, for example, are used in computing the QR decomposition of a matrix).

Given a vector u  Rk, k  n, the n × n Householder reflector Hkn(u) is defined as:


 Hkn(u) =

In-k

Ik -2

uu u

2

, u=0

 In

, otherwise.

(1)

The Householder reflector is clearly a symmetric matrix, and it can be shown that it is orthogonal,

i.e., H2 = I (Householder (1958)). Further, when u = 0, it has n - 1 eigenvalues that are 1, and one

eigenvalue which is -1 (hence the name that it is a reflector) . In practice, to store a Householder

reflector, we only need to store u  Rk rather than the full matrix.

Given a series of vectors {ui}in=k where uk  Rk, we define the map: Mk : Rk × ... × Rn  Rn×n

(uk, ..., un)  Hn(un)...Hk(uk),

(2)

where the right hand side is a product of Householder reflectors, yielding an orthogonal matrix (to

make the notation less cumbersome, we remove the superscript from Hkn for the rest of this section).

Theorem 1. The image of M1 is the set of all n × n orthogonal matrices.

The proof of Theorem 1 is an easy extension of the Householder QR factorization Theorem, and is presented in Appendix A. Although we cannot express all n × n matrices with Mk, any W  Rn×n can be expressed as the product of two orthogonal matrices U, V and a diagonal matrix , i.e. by its SVD: W = U V . Given   Rn and {ui}in=k1 , {vi}in=k2 with ui, vi  Ri, we finally define our proposed SVD parametrization:
Mk1,k2 :Rk1 × ... × Rn × Rk2 × ... × Rn × Rn  Rn×n
(uk1 , ..., un, vk2 , ..., vn, )  Hn(un)...Hk1 (uk1 )diag()Hk2 (vk2 )...Hn(vn). (3)
Theorem 2. The image of M1,1 is the set of n × n real matrices. i.e. Rn×n = M1,1 R1 × ... × Rn × R1 × ... × Rn × Rn

The proof of Theorem 2 is based on the singular value decomposition and Theorem 1, and is pre-

sented in Appendix A. The astute reader might note that M1,1 seemingly maps an input space of

n2 uk

+ 2n dimensions to a , the input space also

space of n2 has exactly

nd2imdeimnseinosniso;nhso. wAelvthero,usginhcTe hHeoknr(eumk s)

is 1

invariant to the norm of and 2 are simple exten-

sions of well known linear algebra results, they ensure that our parameterization has the ability to

represent any matrix and so the full expressive power of the RNN is retained.

Theorem 3. The image of Mk1,k2 includes the set of all orthogonal n×n matrices if k1+k2  n+2.

Theorem 3 indicates that if the total number of reflectors is greater than n: (n - k1 + 1) + (n - k2 + 1)  n, then the parameterization covers all orthogonal matrices. Note that when fixing  = 1, Mk1,k2 ({ui}in=k1 , {vi}in=k2 , 1)  O(n), where O(n) is the set of n × n orthogonal matrices. Thus when k1 + k2  n + 2, we have O(n) = Mk1,k2 Rk1 × ... × Rn × Rk2 × ... × Rn × 1 .
4 SVD-RNN

In this section, we apply our SVD parameterization to RNNs and describe the resulting svdRNN

algorithm in detail. Given a hidden state vector from the previous step h(t-1)  Rn and input

x(t-1)  Rni , RNN computes the next hidden state h(t) and output vector o(t)  Rno as:

h(t) = (W h(t-1) + M x(t-1) + b)

(4)

o(t) = Y h(t)

(5)

3

Under review as a conference paper at ICLR 2018

In svdRNN we parametrize the transition matrix W  Rn×n using m1 + m2 Householder reflectors

as:

W = Mn-m1+1,n-m2+1(un-m1+1, ..., un, vn-m2+1, ..., vn, )

(6)

= Hn(un)...Hn-m1+1(un-m1+1)diag()Hn-m2+1(vn-m2+1)...Hn(vn)

(7)

This parameterization gives us several advantages over the regular RNN. First, we can select the

number of reflectors m1 and m2 to balance expressive power versus time and space complexity. By Theorem 2, the choice m1 = m2 = n gives us the same expressive power as vanilla RNN. Notice oRNN could be considered a special case of our parametrization, since when we set m1 + m2  n and  = 1, we can represent all orthogonal matrices, as proven by Theorem 3. Most importantly,

we are able to explicitly control the singular values of the transition matrix. In most cases, we want

to constrain the singular values to be within a small interval near 1. The most intuitive method is to

clip the singular values that are out of range. Another approach would be to initialize all singular values to 1, and add a penalty term  -1 2 to the objective function. Here, we have applied another

parameterization of  proposed in Vorontsov et al. (2017): i = 2r(f (^i) - 0.5) + , i  [n]

(8)

where f is the sigmoid function and ^i is updated from ui, vi via stochastic gradient descent. The above allows us to constrain i to be within [ - r,  + r]. In practice,  is usually set to 1 and r 1. Note that we are not incurring more computation cost or memory for the parameterization.

For regular RNN, the number of parameters is (no + ni + n + 1)n, while for svdRNN it is (no +

ni

+ m1

+ m2

+ 2)n -

.m12 +m22 -m1 -m2
2

In

the

extreme

case

where

m1

=

m2

=

n,

it

becomes

(no + ni + n + 3)n. Later we will show that the computational cost of svdRNN is also of the same

order as RNN in the worst case.

4.1 FORWARD/BACKWARD PROPAGATION

In forward propagation, we need to iteratively evaluate h(t) from t = 0 to L using (4). The only

different aspect from a regular RNN in the forward propagation is the computation of W h(t-1).

Note that in svdRNN, W is expressed as product of m1 + m2 Householder matrices and a diago-

nal matrix.Thus W h(t-1) can be computed iteratively using (m1 + m2) inner products and vector

additions. Denoting u^k =

0n-k uk

, we have:

Hk(uk)h =

In

-

2u^k u^k u^k u^k

h

=

h

-

2

u^k h u^k u^k

u^k

(9)

Thus, the total cost of computing W h(t-1) is O((m1 + m2)n) floating point operations (flops).

Detailed analysis can be found in Section 4.2. Let L({ui}, {vi}, , M, Y, b) be the loss or objective

function,

C (t)

=

W

h(t),

^

=

diag(^).

Given



L C (t)

,

we

define:

L C(t)

:=

 uk(t)

 u(kt)

L L

 C (t)

; :=

C(t) vk(t)

 vk(t)

L ;
 C (t)

(10)

L C(t) (t) := (t)

L L

(t)

C(t) ; ^ (t) := ^ (t)

L (t) ;

(11)

L C(t) h(t-1) := h(t-1)

L  C (t)

(12)

Back

propagation for

svdRNN requires

, C (t)
 uk(t)

, C (t)
 vk(t)

 C (t) ^ (t)

and

. C (t)
 h(t-1)

These partial gradients

can

also be computed iteratively by computing the gradient of each Householder matrix at a time. We

drop

the

superscript

(t)

now

for

ease

of

exposition.

Given

h^

=

Hk (uk )h

and

g

=

L h^

,

we

have

L h^ =
h h

L h^ =

In

-

2u^k u^k u^k u^k

g

=

g

-

2 u^k g u^k u^k

u^k

(13)

L h^ =
u^k u^k

L h^

=

-2

u^k h u^k u^k

In

+

1 u^k u^k hu^k

+

u^k h (u^k u^k)2

u^k

u^k

=

-2 u^k h u^k u^k

g

-

2 u^k g u^k u^k

h

-

2

u^k h u^k u^k

u^k g u^k u^k

u^k

g

(14) (15)

4

Under review as a conference paper at ICLR 2018

Details of forward and backward propagation can be found in Appendix (B). One thing worth notic-
ing is that the oRNN method in Mhammedi et al. (2017) actually omitted the last term in (15) by assuming that uk are fixed. Although the scaling of uk in the Householder transform does not affect the transform itself, it does produce different gradient update for uk even if it is scaled to norm 1 afterwards.

4.2 COMPLEXITY ANALYSIS

Table 1 gives the time complexity of various algorithms. Hprod and Hgrad are defined in Algo-
rithm 2 3 (see Appendix (B)). Algorithm 2 needs 6k flops, while Algorithm 3 uses (3n + 10k) flops. Since uk 2 only needs to be computed once per iteration, we can further decrease the flops to 4k and (3n + 8k). Also, in back propagation we can reuse  in forward propagation to save 2k flops.

Hprod(h, uk) Hgrad(h, uk, g) svdRNN-Local FP(n, m1, m2) svdRNN-Local BP(n, m1, m2) oRNN-Local FP(n, m)
oRNN-Local BP(n, m)

flops
4k
3n + 6k 4n(m1 + m2) - 2m12 - 2m22 + O(n) 6n(m1 + m2) - 1.5m12 - 1.5m22 + O(n)
4nm - m2 + O(n) 7nm - 2m2 + O(n)

Table 1: Time complexity across algorithms

5 EXTENDING SVD PARAMETERIZATION TO GENERAL WEIGHT MATRICES

In this section, we extend the parameterization to non-square matrices and use Multi-Layer Percep-

trons(MLP) as an example to illustrate its application to general deep networks. For any weight matrix W  Rm×n(without loss of generality m  n), its reduced SVD can be written as:

W = U (|0)(VL|VR) = U VL ,

(16)

where U  Rm×m,   diag(Rm),VL  Rn×m. There exist un, ..., uk1 and vn, ..., vk2 s.t. U = Hmm(um)...Hkm1 (uk1 ), V = Hnn(vn)...Hkn2 (vk2 ), where k1  [m], k2  [n]. Thus we can extend the

SVD parameterization for any non-square matrix:

Mmk1,,nk2 :Rk1 × ... × Rm × Rk2 × ... × Rn × Rmin(m,n)  Rm×n
(uk1 , ..., um, vk2 , ..., vn, )  Hmm(um) · · · Hkm1 (uk1 )^ Hkn2 (vk2 ) · · · Hnn(vn). (17) where ^ = (diag()|0) if m < n and (diag()|0) otherwise. Next we show that we only need 2 min(m, n) reflectors (rather than m + n) to parametrize any m × n matrix. By the definition of Hkn, we have the following lemma:

Lemma 1. Given {vi}in=1, define V (k) = Hnn(vn)...Hkn(vk) for k  [n]. We have: V(,ki1) = V(,ki2), k1, k2  [n], i  min(n - k1, n - k2).

Here V,i indicates the ith column of matrix V . According to Lemma 1, we only need at most first m Householder vectors to express VL, which results in the following Theorem: Theorem 4. If m  n, the image of Mm1,n,n-m+1 is the set of all m × n matrices; else the image of Mmn-,nm+1,1 is the set of all m × n matrices.

Similarly if we constrain ui, vi to have unit length, the input space dimensions of Mm1,n,n-m+1 and Mmm,-nn+1,1 are both mn, which matches the output dimension. Thus we extend Theorem 2 to the non-square case, which enables us to apply SVD parameterization to not only the RNN transition

matrix, but also to general weight matrices in various deep learning models. For example, the

Multilayer perceptron (MLP) model is a class of feedforward neural network with fully connected

layers:

h(t) = f (W (t-1)h(t-1) + b(t-1))

(18)

Here h(t)  Rnt , h(t-1)  Rnt-1 and W (t)  Rnt×nt-1 . Applying SVD parameterization to W (t) say nt < nt-1, we have:
W (t) = Hnntt (unt )...H1nt (u1)Hnntt--11-nt+1(vnt-1-nt+1)...Hnntt--11 (vnt-1 ).
We can use the same forward/backward propagation algorithm as described in Algorithm 1. Besides

RNN and MLP, SVD parameterization method also applies to more advanced frameworks, such as

Residual networks and LSTM, which we will not describe in detail here.

5

Under review as a conference paper at ICLR 2018

6 THEORETICAL ANALYSIS

Since we can control and upper bound the singular values of the transition matrix in svdRNN, we can clearly eliminate the exploding gradient problem. In this section, we now analytically illustrate the advantages of svdRNN with lower-bounded singular values from the optimization perspective. For the theoretical analysis in this section, we will limit ourselves to a linear recurrent neural network, i.e., an RNN without any activation.

6.1 REPRESENTATIONS OF RNN WITHOUT ACTIVATION

Linear recurrent neural network. For simplicity, we follow a setting similar to Hardt &

Ma (2016). For compact presentation, we stack the input data as X  Rn×t, where X =

x(0)|x(1)| · · · |x(t-1) , and transition weights as W  Rn×nt where W = W |W 2| · · · |W t .

Then we can simplify the output as:

t

o(t)(X ) = Y (W th(0) + W i(M x(t) + b))

i=1

By absorbing M and b in each data x(t) and assuming h(0) = 0, we further simplify the output as:

t

o(t)(X ) = Y

W ix(t-1)

i=1

Suppose the input data X  D, and assume its underlying relation to the output is y = Avec(X )+,

where A  Rn×nt and residue   Rn satisfies EX D[|X ] = 0. We consider the individual loss:

f (W ; X , y) =

o(t)(X ) - y

2 2

=

Y Wvec(X ) - y 22.

Claim 1. With linear recurrent neural networks, the population risk

R[W ] = EX D[f (W ; X , y)] =

(Y W - A)1/2

2 F

+ C,

where  = EX D[vec(X )vec(X ) ], and C = E[  22]. Meanwhile

W R[W ] = (Y W - A) Id|2W |3W 2| · · · |tW t-1

6.2 ALL CRITICAL POINTS ARE GLOBAL MINIMUM

Theorem 5. With linear recurrent neural networks, if transition matrix W satisfies min(W )  e > 0, all critical points in the population risk are global minimum.

Proof. Write Y W - A as (E1|E2| · · · |Et) , where each Ei  Rd×d. By Claim 1,

W R[W ]

2 F

=

(Y W - A)

Id|2W

|3(W

)2| · · · |t(W

)t-1

2 F

 m2 in() (Y W - A) Id|2W |3(W )2| · · · |t(W

t

 m2 in()

i2e2(i-1)

Ei

2 F

i=1



m2 in()

min {i2e2(i-1)}
1it

YW -A

2 F

)t-1

2 F



m2 in()

min {i2e2(i-1)}(R(W
1it

)

-

R)

Therefore when W R[W ] = 0 suffices R(W ) = R, meaning W reaches the global minimum.

Theorem 5 potentially explains why our system is easier to optimize, since with our scheme of SVD parametrization, we have the following corollary.

Corollary 1. With the update rule in (8), linear svdRNNs have no spurious local minimum.

While the above analysis lends further credence to our observed experimental results, we leave it to future work to perform a similar analysis in the presence of non-linear activation functions.

7 EXPERIMENTAL RESULTS

In this section, we provide empirical evidence that shows the advantages of SVD parameterization in both RNNs and MLPs. For RNN models, we compare our svdRNN algorithm with (vanilla) RNN, IRNN(Le et al. (2015)), oRNN(Mhammedi et al. (2017)) and LSTM (Hochreiter & Schmidhuber (1997)). The transition matrix in IRNN is initialized to be orthogonal while other matrices are initialized by sampling from a Gaussian distribution. For MLP models, we implemented vanilla MLP, Residual Network (ResNet)(He et al. (2016)) and used SVD parameterization for both of them. We used a residual block of two layers in ResNet. In most cases leaky Relu is used as activation function, except for LSTM, where leaky Relu will drastically harm the performance.

6

Under review as a conference paper at ICLR 2018
To train these models, we applied Adam optimizer with stochastic gradient descent (Kingma & Ba (2014)). These models are implemented with Theano (Al-Rfou et al. (2016)).1
7.1 TIME SERIES CLASSIFICATION
In this experiment, we focus on the time series classification problem, where time series are fed into RNN sequentially, which then tries to predict the right class upon receiving the sequence end (Hu¨sken & Stagge (2003)). The dataset we choose is the largest public collection of classlabeled time-series with widely varying length, namely, the UCR time-series collection from Chen et al. (2015)2. We present the test accuracy on 20 datasets with RNN, LSTM, oRNN and svdRNN in Table 3(Appendix C) and Figure 1. In all experiments, we used hidden dimension nh = 32, and chose total number of reflectors for oRNN and svdRNN to be m = 16 (for svdRNN m1 = m2 = 8). We choose proper depth t as well as input size ni. Given sequence length L, since tni = L, we choose ni to be the maximum divisor of L that satisfies depth  L. To have a fair comparison

(a) (b) (c) Figure 1: Performance comparisons of the RNN based models on three UCR datasets.

of how the proposed principle itself influences the training procedure, we did not use dropout in any of these models. As illustrated in the optimization process in Figure 1, this resulted in some overfitting (see (a) CBF), but on the other hand it shows that svdRNN is able to prevent overfitting. This supports our claim that since generalization is bounded by the spectral norm of the weights Bartlett et al. (2017), svdRNN will potentially generalize better than other schemes. This phenomenon is more drastic when the depth is large (e.g. ArrowHead(251 layers) and FaceAll(131 layers)), since regular RNN, and even LSTM, have no control over the spectral norms. Also note that there are substantially fewer parameters in oRNN and svdRNN as compared to LSTM.

7.2 MNIST

In this experiment, we compare different models on the MNIST image dataset. The dataset was

split into a training set of 60000 instances and a test set of 10000 instances. The 28 × 28 MNIST

pixels are flattened into a vector and then traversed by the RNN models. Table 2 shows accuracy

scores across multiple We tested different models with different network depth as well as width.

Figure 2(a)(b) shows the test accuracy on networks with 28 and 112 layers (20 and 128 hidden

dimensions) respectively. It can be seen that the svdRNN algorithms have the best performance

and the choice of r (the amount that singular values are allowed to deviate from 1) does not have

much influence on the final precision. Also we explored the effect of different spectral constraints

and explicitly tracked the spectral margin (maxi |i - 1|) of the transition matrix. Intuitively, the

influence of large spectral margin should increase as the network becomes deeper. Figure 2(d)

shows the spectral margin of different RNN models. Although IRNN has small spectral margin at

first few iterations, it quickly deviates from orthogonal and cannot match the performance of oRNN

and svdRNN. Figure 2(e) shows the magnitude of first layer gradient

L  h(0)

2. RNN suffers from

vanishing gradient at first 50k iterations while oRNN and svdRNN are much more stable. Note that

LSTM can perform relatively well even though it has exploding gradient in the first layer.

We also tested RNN and svdRNN with different amount of non-linearity, as shown in Figure 2(c).

This is achieved by adjusting the leak parameter in leaky Relu: f (x) = max(leak · x, x). With

leak = 1.0, it reduces to the identity map and when leak = 0 we are at the original Relu function.

From the figures, we show that svdRNN is resistant to different amount of non-linearity, namely

converge faster and achieve higher accuracy invariant to the amount of the leak factor. To explore

the reason underneath, we illustrate the gradient in Figure 2(f), and find out svdRNN could eliminate

the gradient vanishing problem on all circumstances, while RNN suffers from gradient vanishing

when non-linearity is higher.

1we thank Mhammedi for providing their code for oRNN(Mhammedi et al. (2017)) 2Details of the data sets, including how to split into train/vaildiation/test sets, are given in Appendix C

7

Under review as a conference paper at ICLR 2018

(a) (b) (c)

(d) (e) Figure 2: RNN models on MNIST

(f)

Models svdRNN oRNN(Mhammedi et al. (2017)) RNN(Vorontsov et al. (2017)) uRNN(Arjovsky et al. (2016)) RC uRNN(Wisdom et al. (2016)) FC uRNN(Wisdom et al. (2016)) factorized RNN(Vorontsov et al. (2017)) LSTM (Vorontsov et al. (2017))

Hidden dimension 256(m1, m2 = 16)
256(m = 32) 128 512 512 116 128 128

Number of parameters  13k  11k  35k  16k  16k  16k  32k  64k

Test accuracy 97.6 97.2 94.1 95.1 97.5 92.8 94.6 97.3

Table 2: Results for the pixel MNIST dataset across multiple algorithms.

For the MLP models, each instance is flattened to a vector of length 784 and fed to the input layer. After the input layer there are 40 layers with hidden dimension 32 (Figure 3(a)) or 30 to 100 layers with hidden dimension 128 (Figure 3(b)). On a 40-layer network, svdMLP and svdResNet achieve similar performance as ResNet while MLP's convergence is slower. However, when the network is deeper, both MLP and ResNet start to fail. With nh = 128, MLP is not able to function with L > 35 and ResNet with L > 70. On the other hand, the SVD based methods are resilient to increasing depth and thus achieve higher precision.

(a) (b)
Figure 3: MLP models on MNIST with L layers nh hidden dimension
8 CONCLUSIONS
In this paper, we have proposed an efficient SVD parametrization of various weight matrices in deep neural networks, which allows us to explicitly track and control their singular values. This parameterization does not restrict the network's expressive power, while simultaneously allowing fast forward as well as backward propagation. The method is easy to implement and has the same time and space complexity as compared to original methods like RNN and MLP. The ability to control singular values helps in avoiding the gradient vanishing and exploding problems, and as we have empirically shown, gives good performance. Although we only showed examples in the RNN and MLP framework, our method is applicable to many more deep networks, such as Convolutional Networks etc. However, further experimentation is required to fully understand the influence of using different number of reflectors in our SVD parameterization. Also, the underlying structures of the image of Mk1,k2 when k1, k2 = 1 is a subject worth investigating.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fre´de´ric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano: A python framework for fast computation of mathematical expressions. arXiv preprint, 2016.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120­1128, 2016.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1706.08498, 2017.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157­166, 1994.
Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, and Gustavo Batista. The UCR time series classification archive, July 2015. www.cs.ucr. edu/~eamonn/time_series_data/.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854­863, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Alston S Householder. Unitary triangularization of a nonsymmetric matrix. Journal of the ACM (JACM), 5(4):339­342, 1958.
Michael Hu¨sken and Peter Stagge. Recurrent neural networks for time series classification. Neurocomputing, 50:223­235, 2003.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal parametrisation of recurrent neural networks using Householder reflections. In International Conference on Machine Learning, pp. 2401­2409, 2017.
Toma´s Mikolov. Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April, 2012.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pp. 1310­1318, 2013.
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. SIAM, 1997. Joel A Tropp, Inderjit S Dhillon, Robert W Heath, and Thomas Strohmer. Designing structured
tight frames via an alternating projection method. IEEE Transactions on information theory, 51 (1):188­209, 2005. Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. In International Conference on Machine Learning, pp. 3570­3578, 2017. Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 4880­4888, 2016.
9

Under review as a conference paper at ICLR 2018

APPENDIX A PROOFS
A.1 PROOF OF PROPOSITION 1
Proposition 1. (Householder QR factorization) Let B  Rn×n. Then there exists an upper triangular matrix R with positive diagonal elements, and vectors {ui}in=1 with ui  Ri, such that B = Hnn(un)...H1n(u1)R.

Proof of Proposition 1. For n = 1, note that H11(u1) = ±1. By setting u1 = 0 if B1,1 > 0 and u1 = 0 otherwise, we have the factorization desired.

Assume that the result holds for n = k, then for n = k + 1 set uk+1 = B1 - B1 e1. Here B1 is

the first column of B and e1 = (1, 0, ..., 0) . Thus we have

Hkk++11(uk+1)B =

B1 B^1,2:k+1 0 B^

,

where B^  Rk×k. Note that Hkk++11(uk+1) = Ik+1 when uk+1 = 0 and the above still holds. By

assumption we have B^ = Hkk(uk)...H1k(u1)R^. Notice that Hik+1(ui) =

1 Hik (ui )

, so we have

that

H1k+1(u1)...Hkk+1(uk)Hkk++11(uk+1)B =

B1 B~1,2:k+1 0 R^

=R

is an upper triangular matrix with positive diagonal elements. Thus the result holds for any n by the theory of mathematical induction.

A.2 PROOF OF THEOREM 1
Proof. Observe that the image of M1 is a subset of O(n), and we now show that the converse is also true. Given A  O(n), by Proposition 1, there exists an upper triangular matrix R with positive diagonal elements, and an orthogonal matrix Q expressed as Q = Hnn(un)...H1n(u1) for some set of Householder vectors {ui}in=1, such that A = QR. Since A is orthogonal, we have A A = AA = In, thus:
A A = R Q QR = R R = In; Q AA Q = Q QRR Q Q = RR = In Thus R is orthogonal and upper triangular matrix with positive diagonal elements. So R = In and A = Q = Hnn(un)...H1n(u1).
A.3 PROOF OF THEOREM 2
Proof. It is easy to see that the image of M1,1 is a subset of Rn×n. For any W  Rn×n, we have its SVD, W = U V , where  = diag(). By Theorem 1, for any orthogonal matrix U, V  Rn×n, there exists {ui}in=1{vi}in=1 such that U = M1(u1, ..., un) and V = M1(v1, ..., vn), then we have:
W = Hnn(un)...H1n(u1)H1n(v1)...Hnn(vn) = M1,1(u1, ..., un, v1, ..., vn, )

A.4 PROOF OF THEOREM 3

Proof. Let A  Rn×n be an orthogonal matrix. By Theorem 1, there exist {ai}in=1, such that A = M1(a1, ..., an). Since A is also orthogonal, for the same reason, there exist {bi}in=1, such that A = M1(b1, ..., bn). Thus we have:
A = Hn(an)...H1(a1) = H1(b1)...Hn(bn) Observe that one of k2  k1 - 1 and k1  k2 - 1 must be true. If k2  k1 - 1, set
uk = ak, k = n, n - 1, ..., k1,

vk2+k1-k-1 = ak, k = k1 - 1, ..., 1,

(19)

vt = 0, t = k2 + k1 - 2, ..., n,

and then we have:

Mk1,k2 (uk1 , ..., un, vk2 , ..., vn, 1) = Hn(un)...Hk1 (uk1 )InHk2 (vk2 )...Hn(vn)

= Hn(an)...Hk1 (ak1 )InHk1-1(ak1-1)...H1(a1)

= A (20)

10

Under review as a conference paper at ICLR 2018

Else, assign: vk = bk, k = n, n - 1, ..., k2,
uk2+k1-k-1 = bk, k = k2 - 1, ..., 1, ut = 0, t = k2 + k1 - 2, ..., n,
and then we have: Mk1,k2 (uk1 , ..., un, vk2 , ..., vn, 1) = H1(b1)...Hk2-1(bk2-1)InHk2 (bk2 )...Hn(bn)
=A

(21) (22)

A.5 PROOF OF THEOREM 4
Proof. It is easy to see that the image of Mm,,n is a subset of Rm×n. For any W  Rm×n, we have its SVD, W = U V , where  is an m × n diagonal matrix. By Theorem 1, for any orthogonal matrix U  Rm×m, V  Rn×n, there exists {ui}mi=1{vi}ni=1 such that U = Hmm(um)...H1m(u1) and V = Hnn(vn)...H1n(v1). By Lemma 1, if m < n we have:
W = Hnm(un)...H1m(u1)H1n(v1)...Hnn(vn) = Hnm(un)...H1m(u1)Hnn-m+1(vn-m+1)...Hnn(vn).
Similarly, for n < m, we have: W = Hnm(un)...H1m(u1)H1n(v1)...Hnn(vn) = Hnm(un)...Hmm-n+1(um-n+1)H1n(v1)...Hnn(vn).

A.6 PROOF OF CLAIM 1

Proof. R[W ] = EX D[f (W ; X , y)]
= EX D (Y W - A)vec(X ) -  2
= E tr (Y W - A)vec(X )vec(X ) (Y W - A)

+ E[  2] - 2E [ (Y W - A)vec(X ),  ]

= tr (Y W - A)E vec(X )vec(X ) (Y W - A) + C

=

(Y W - A)1/2

2 F

+C

For the derivative,

R[W + W ] =

(Y

W + W |(W + W )2| · · · |(W + W )t

- A)1/2

2 F

+C

= R[W ] +

W |2W W | · · · |tW t-1W

1/2, (Y W - A)1/2

+ o(

W

2 F

)

= R[W ] + W, (Y W - A) Id|2W |3W 2| · · · |tW t-1

Therefore R[W ] = (Y W - A) Id|2W |3W 2| · · · |tW t-1

Remark: here when W and W are not commutative, each W iW should instead be written as

i j=0

W

j W

W

i-j .

Since

the

change

of

order

doesn't

impact

the

analysis,

we

informally

simplify

the expressions here.

11

Under review as a conference paper at ICLR 2018

APPENDIX B DETAILS OF FORWARD AND BACKWARD PROPAGATION ALGORITHMS

Algorithm 1 Local forward/backward propagation

Input:

h(t-1),



L C (t)

,

U

=

(un|...|un-m1+1),

, V = (vn|...|vn-m2+1)

Algorithm 2

Output:

C (t)

=

W h(t-1),

L U

,

L V

,

L ^

,

L  h(t-1)

// Begin forward propagation

hn(v+)1  h(t-1) for k = n, n - 1, ..., n - m2 + 1 do

h(kv)  Hprod(h(kv+)1, vk) end for hfok(ru1)-k1=n-hm(kv2)1 + 1, ..., n do

// Compute V^ h // Compute V^ h

h^ = Hprod(h, uk)

Input: h, uk Output: h^ = Hk(uk)h

//

Compute

h^

=

(I

-

2uk uk uk uk

)h



2 uk

2 uk h

h^  h - uk

h(ku)  Hprod(h(ku-)1, uk) // Compute U^ V^ h

end for

C(t)  h(nu) //Begin backward propagation

Algorithm 3 h¯, u¯k = Hgrad(h, uk, g)

g for k

L  C (t)
= n,

n

-

1,

...,

n

-

m1

+

1

do

g, G(u,n)-k+1  Hgrad(h(ku), uk, g)

end for

¯  diag(g  hk(v2)), g  g

g(^)



diag() ^

 diag(¯ )

for k = n - m2 + 1, ..., n do

g, G(v,n) -k+1  Hgrad(h(ku+)1, vk, g)

//

Compute

L uk

//

Compute

L 

//

Compute

L ^

//

Compute

L vk

Input:

h, uk, g

=

L C

where

C = Hk(uk)h

Output:

h¯

=

L h

,

u¯k

=

L uk

=

2 uk

2 uk h

=

2 uk

2 uk g

h¯  g - uk

u¯k



-g

-

h

-

 2

uk

end for

L U



G(u),

L V

 G(v),

L ^

 g(^),

L  h(t-1)

g

APPENDIX C MORE EXPERIMENTAL DETAILS
We use the training and testing sets directly from the UCR time series archive http://www.cs. ucr.edu/~eamonn/time_series_data/, and randomly choose 20% of the training set as validation data. We provide the statistical descriptions of the data and experimental results in Table 3.

Models
50words Adiac ArrowHead Beef BeetleFly CBF Coffee Cricket X DistalPhalanxOutlineCorrect DistalPhalanxTW ECG200 ECG5000 ECGFiveDays FaceAll FaceFour FacesUCR Gun Point InsectWingbeatSound ItalyPowerDemand Lighting2 MiddlePhalanxOutlineCorrect

[training size]/[test size]/[length]/[depth]
450/455/270/27 390/391/176/16 36/175/251/251
30/30/470/47 20/20/512/32 30/900/128/16 28/28/286/22 390/390/300/20 276/600/80/10 154/399/80/10 100/100/96/12 500/4500/140/14 23/861/136/17 560/1690/131/131 24/88/350/25 200/2050/131/131 50/150/150/15 220/1980/256/16 67/1029/24/6 60/61/637/49 291/600/80/10

RNN accuracy (nparam)
0.492 (3058) 0.552 (2694) 0.509 (1219) 0.600 (1606) 0.950 (1699) 0.702 (1476) 1.000 (1570) 0.310 (1997) 0.790 (1410) 0.815 (1641) 0.640 (1410) 0.941 (1606) 0.947 (1443) 0.549 (1615) 0.625 (1701) 0.449 (1615) 0.947 (1507) 0.534 (1996) 0.970 (1315) 0.541 (1570) 0.793 (1410)

LSTM accuracy (nparam)
0.598 (7218) 0.706 (6950) 0.537 (4515) 0.700 (5766) 0.850 (6435) 0.967 (5444) 1.000 (6018) 0.456 (6637) 0.798 (5378) 0.795 (5609) 0.640 (5378) 0.936 (5766) 0.790 (5411) 0.455 (4911) 0.477 (6245) 0.629 (4911) 0.920 (5667) 0.515 (6732) 0.969 (4899) 0.541 (6018) 0.783 (5378)

oRNN accuracy (nparam)
0.642 (2426) 0.668 (2062) 0.669 (587) 0.733 (974) 0.900 (1067) 0.881 (844) 1.000 (938) 0.495 (1365) 0.830 (778) 0.807 (1009) 0.640 (778) 0.940 (974) 0.976 (811) 0.714 (983) 0.511 (1069) 0.710 (983) 0.953 (875) 0.598 (1364) 0.972 (683) 0.541 (938) 0.712 (778)

svdRNN accuracy (nparam)
0.651 (2850) 0.726 (2486) 0.800 (1011) 0.733 (1398) 0.950 (1491) 0.948 (1268) 1.000 (1362) 0.500 (1789) 0.840 (1202) 0.815 (1433) 0.640 (1202) 0.945 (1398) 0.948 (1235) 0.714 (1407) 0.716 (1493) 0.727 (1407) 0.960 (1299) 0.586 (1788) 0.973 (1107) 0.541 (1362) 0.820 (1202)

Table 3: Test accuracy (number of parameters) on UCR datasets. For each dataset, we present the testing accuracy when reaching the smallest validation error. The highest precision is in bold, and lowest two are colored gray.

12

