Under review as a conference paper at ICLR 2018
TRAINING AUTOENCODERS BY ALTERNATING MINI-
MIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.
1 INTRODUCTION
For much of the recent march of deep learning, gradient-based backpropagation methods, e.g. Stochastic Gradient Descent (SGD) and its variants, have been the mainstay of practitioners. The use of these methods, especially on vast amounts of data, has led to unprecedented progress in several areas of artificial intelligence. On one hand, the intense focus on these techniques has led to an intimate understanding of hardware requirements and code optimizations needed to execute these routines on large datasets in a scalable manner. Today, myriad off-the-shelf and highly optimized packages exist that can churn reasonably large datasets on GPU architectures with relatively mild human involvement and little bootstrap effort.
However, this surge of success of backpropagation-based methods in recent years has somewhat overshadowed the need to continue to look for options beyond backprogagation to train deep networks. Despite several advancements in deep learning with respect to novel architectures such as encoderdecoder networks and generative adversarial models, the reliance on backpropagation methods remains. While reinforcement learning methods are becoming increasingly popular, their scope is limited to a particular family of settings such as agent-based systems or reward-based learning. Recent efforts have studied the limitations of SGD-based backpropagation, including parallelization of SGD-based techniques that are inherently serial (Taylor et al. (2016)), vanishing gradients, especially for certain activation functions (Hochreiter & Schmidhuber (1997)), convergence of stochastic techniques to local optima (Anandkumar & Ge (2016)), and many more. For a well-referenced critique of gradient-based methods, we point the reader to Taylor et al. (2016).
From another perspective, there has been marked progress in recent years in the area of non-convex optimization (beyond deep learning), which has resulted in scalable methods such as iterated hard thresholding (Blumensath & Davies (2009)) and alternating minimization (Jain et al. (2013)) as methods of choice for solving large-scale sparse recovery, matrix completion, and tensor factorization tasks. Several of these methods not only scale well to large problems, but also offer provably accurate solutions. In this work, we investigate a non-backpropagation strategy to train neural networks, leveraging recent advances in quasi-convex optimization. Our method is called DANTE (Deep AlterNations for Training autoEncoders), and it offers an alternating minimization-based technique for training neural networks - in particular, autoencoders.
1

Under review as a conference paper at ICLR 2018
DANTE is based on a simple but useful observation that the problem of training a single hidden-layer autoencoder can be cast as a bi-quasiconvex optimization problem (described in Section 3.1). This observation allows us to use an alternating optimization strategy to train the autoencoder, where each step involves relatively simple quasi-convex problems. DANTE then uses efficient solvers for quasiconvex problems including normalized gradient descent (Nesterov (1984)) and stochastic normalized gradient descent (Hazan et al. (2015)) to train autoencoder networks. The key contributions of this work are summarized below:
· We show that viewing each layer of a neural network as applying an ensemble of generalized linear transformations, allows the problem of training the network to be cast as a bi-quasiconvex optimization problem (exact statement later).
· We exploit this intuition by employing an alternating minimization strategy, DANTE, that reduces the problem of training the layers to quasi-convex optimization problems.
· We utilize the state-of-the-art Stochastic Normalized Gradient Descent (SNGD) technique (Hazan et al. (2015)) for quasi-convex optimization to provide an efficient implementation of DANTE for networks with sigmoidal activation functions. However, a limitation of SNGD is its inability to handle non-differentiable link functions such as the ReLU.
· To overcome this limitation, we introduce the generalized ReLU, a variant of the popular ReLU activation function and show how SNGD may be applied with the generalized ReLU function. This presents an augmentation in the state-of-the-art in quasi-convex optimization and may be of independent interest. This allows DANTE to train AEs with both differentiable and non-differentiable activation functions, including ReLUs and sigmoid.
· We show that SNGD offers provably more rapid convergence with the generalized ReLU function than it does even for the sigmoidal activation. This is corroborated in experiments as well. A key advantage of our approach is that these theoretical results can be used to set learning rates and batch sizes without finetuning/cross-validation.
· We also show DANTE can be easily extended to train deep AEs with multiple hidden layers. · We empirically validate DANTE with both the generalized ReLU and sigmoid activations
and establish that DANTE provides comparable or better test errors, reconstructions and classification performance (with the learned representations), when compared to an identical network trained using standard mini-batch SGD-based backpropagation.
2 RELATED WORK
Backpropagation-based techniques date back to the early days of neural network research (Rumelhart et al. (1986); Chauvin & Rumelhart (1995)) but remain to this day, the most commonly used methods for training a variety of neural networks including multi-layer perceptrons, convolutional neural networks, autoencoders, recurrent networks and the like. Recent years have seen the development of other methods, predominantly based on least-squares approaches, used to train neural networks. Carreira-Perpinan and Wang (Carreira-Perpinan & Wang (2014)) proposed a least-squares based method to train a neural network. In particular, they introduced the Method of Auxiliary Constraints (MAC), and used quadratic penalties to enforce equality constraints. Patel et al. (Patel et al. (2015)) proposed an Expectation-Maximization (EM) approach derived from a hierarchical generative model called the Deep Rendering Model (DRM), and also used least-squared parameter updates in each of the EM steps. They showed that forward propagation in a convolutional neural network was equivalent to the inference on their DRM. Unfortunately, neither of these methods has publicly available implementations or even published training results to compare against.
More recently, Taylor et al. proposed a method to train neural networks using the Alternating Direction Method of Multipliers (ADMM) and Bregman iterations (Taylor et al. (2016)). The focus of this method, however, was on scaling the training of neural networks to a distributed setting on multiple cores across a computing cluster. Jaderberg also proposed the idea of 'synthetic gradients' in Jaderberg et al. (2016). While this approach is interesting, this work is more focused towards a more efficient way to carry out gradient-based parameter updates in a neural network.
However, in our work, we focus on an entirely new approach to training neural networks ­ in particular, autoencoders ­ using alternating optimization, quasi-convexity and SNGD, and show that this approach shows promising results on the a range of datasets. Although alternating minimization has found much appeal in areas such as matrix factorization Jain et al. (2013), to the best of our
2

Under review as a conference paper at ICLR 2018

knowledge, this is the first such effort in using alternating principles to train neural networks with related performance guarantees.

3 DANTE: DEEP ALTERNATIONS FOR TRAINING AUTOENCODERS

In this section, we will first set notation and establish the problem setting, then present details of the DANTE method, including the SNGD algorithm. For sake of simplicity, we consider networks with just a single hidden layer. We then offer some theoretical insight intro DANTE's inner workings, which also allow us to arrive at the generalized ReLU activation function, and finally describe how DANTE can be extended to deep networks with multiple hidden layers.

3.1 PROBLEM FORMULATION

Consider a neural network with L layers. Each layer l  {1, 2, . . . , L} has nl nodes and is characterized by a linear operator Wl  Rnl-1×nl and a non-linear activation function l : Rnl  Rnl . The activations generated by the layer l are denoted by al  Rnl . We denote by a0, the input activations and n0 to be the number of input activations i.e.a0  Rn0 . Each layer uses activations being fed into it to compute its own activations as al = l Wl, al-1  Rnl , where  ., . denotes ( ., . ) for simplicity of notation. A multi-layer neural network is formed by nesting such layers to form a composite function f given as follows:

f (W; x) = L WL, L-1 WL-1, · · · , 1 W1, x

(1)

where W = {Wl} is the collection of all the weights through the network, and x = a0 contains the input activations for each training sample.

Given m data samples {(xi, yi)}mi=1 from some distribution D, the network is trained by tuning the weights W to minimize a given loss function, J:

min E(x,y)D[J (f (W; x), y)]
W

(2)

Note that a multi-layer autoencoder is trained similarly, but with the loss function modified as below:

min ExD[J (f (W; x), x)]
W

(3)

For purpose of simplicity and convenience, we first consider the case of a single-layer autoencoder, represented as f (W; x) = 2 W2, 1 W1, x to describe our methodology. We describe in a later section on how this idea can be extended to deep multi-layer autoencoders. (Note that our definition of a single-layer autoencoder is equivalent to a two-layer neural network in a classification setting, by nature of the autoencoder.)

A common loss function used to train autoencoders is the squared loss function which, in our simplified setting, yields the following objective.

min
W

ExD

f (W; x) - x

2 2

(4)

Now denote

f (W; x) - x

2 2

=

2 W2, 1 W1, x

-x

2 2

(5)

An important observation here is that if we fix W1, then Eqn (5) turns into a Generalized Linear Model problem with 2 as the activation function, i.e.

min ExD
W2

2 W2, z

-x

2 2

where z = 1 W1, x . Similarly, fixing W2 turns the problem into yet another Generalized Linear Model problem, this time with W1 as the parameter (note that W2 · = 2 W2, 1 · ).

min ExD
W1

W2

W1, x

-x

22.

Thus, a single-layer autoencoder is a combination of two Generalized Linear Model (GLM) problems, and we exploit this key observation in this work. In particular, we leverage a recent result by Hazan

3

Under review as a conference paper at ICLR 2018

Algorithm 1: Stochastic Normalized Gradient Descent (SNGD)

Input :Number of iterations T , training data S = {(xi, yi)}im=1  Rd × R, learning rate , minibatch size b, Initialization parameters w0

1 for t = 1 to T do

2 Sample {(xi, yi)}bi=1  Unif(S)

3 4

Let Let

ft(w)

=

1 b

b i=1

(yi

-



gt = ft(wt), and g^(t)

w, =

xi
gt gt

)2

5 wt+1 = wt -  · g^t

//Select a random mini-batch of training points

6 end

Output :Model given by wT

et al. (2015) that shows that GLMs with nice, differentiable link functions such as sigmoid (or even a combination of sigmoids such as W2 (·)), satisfy a property the authors name Strict Locally Quasi-Convexity (SLQC), which allows techniques such as SNGD to solve the GLM problems effectively.
This is quite advantageous for us since it allows us to solve each sub-problem of the alternating setup efficiently. In a subsequent section, we will show that GLMs with non-differentiable activation ­ in particular, a generalized Rectified Linear Unit (ReLU) ­ can also satisfy the SLQC property, thus allowing us to extend the proposed alternating strategy, DANTE, to ReLU-based autoencoders too. We note that while we have developed this idea to train autoencoders in this work (since our approach relates closely to the greedy layer-wise training in autoencoders), DANTE can be used to train standard multi-layer neural networks too (discussed in Section 5).

3.2 METHODOLOGY
We begin our presentation of the proposed method by briefly reviewing the Stochastic Normalized Gradient Descent (SNGD) method, which is used to execute the inner steps of DANTE. We explain in the next subsection, the rationale behind the choice of SNGD as the optimizer. We stress that although DANTE does use stochastic gradient-style methods internally (such as the SNGD algorithm), the overall strategy adopted by DANTE is not a descent-based strategy, rather an alternating-minimization strategy.

Stochastic Normalized Gradient Descent (SNGD): Normalized Gradient Descent (NGD) is an adaptation of traditional Gradient Descent where the updates in each iteration are purely based on the direction of the gradients, while ignoring their magnitudes. This is achieved by normalizing the gradients. SNGD is the stochastic version of NGD, where weight updates are performed using individual (randomly chosen) training samples, instead of the complete set of samples. Mini-batch SNGD generalizes this by applying updates to the parameters at the end of every mini-batch of samples, as does mini-batch Stochastic Gradient Descent (SGD). In the remainder of this paper, we refer to mini-batch SNGD as SNGD itself, as is common for SGD. Algorithm 1 describes the SNGD methodology for a generic GLM problem.

DANTE: Given this background, Algorithm 2 outlines the proposed method, DANTE. Consider

the autoencoder problem below for a single hidden layer network:

min f (W1, W2) = ExD 2 W2, 1 W1, x
W

-x

2 2

Upon fixing the parameters of the lower layer i.e. W1, it is easy to see that we are left with a GLM

problem:

min ExD
W

2

W2, z

-x

22,

where z = 1 W1, x . DANTE solves this intermediate problem using SNGD steps by sampling several mini-batches of data points and performing updates as dictated by Algorithm 1. Similarly,

fixing the parameters of the upper layer, i.e. W2, we are left with another GLM problem:

min ExD
W

W2

W1, x

-x

22,

where W2 · = 2 W2, 1 · . This is once again solved by mini-batch SNGD, as before.

4

Under review as a conference paper at ICLR 2018

Algorithm 2: DANTE: Deep AlterNations for Training autoEncoders

Input :Stopping threshold , Number of iterations of alternating minimization TAM , Number of iterations for SNGD TSNGD, initial values w10, w02, learning rate , minibatch size b
1 t := 1

2 while |f (W1t, W2t) - f (W1t-1, W2t-1)|  or t < TAM do

3

W2t  arg min ExD 2 W, 1 W1t-1, x

-x

2 2

W

4

W1t  arg min ExD 2 W2t, 1 W, x

-x

2 2

W

5 t := t + 1

//use SNGD //use SNGD

6 end

Output :w1, w2

3.3 RATIONALE

To describe the motivation for our alternating strategy in DANTE, we first define key terms and results that are essential to our work. We present the notion of a locally quasi-convex function (as introduced in Hazan et al. (2015)) and show that under certain realizability conditions, empirical objective functions induced by Generalized Linear Models (GLMs) are locally quasi-convex. To this end, we introduce a new activation function, the generalized ReLU, and show that the GLM with the generalized ReLU also satisfies this property. We then cite a result that shows that SNGD converges to the optimum solution provably for locally quasi-convex functions, and extend this result to the newly introduced activation function.
Definition 3.1 (Local-Quasi-Convexity). Let x, z  Rd, , > 0 and let f : Rd  R be a differentiable function. Then f is said to be ( , , z)-Strictly-Locally-Quasi-Convex (SLQC) in x, if at least one of the following applies:

1. f (x) - f (z)  2. f (x) > 0, and y  B (z, /), f (x), y - x  0

where B (z, /) refers to a ball centered at z with radius /.

Definition 3.2 (Idealized and Noisy Generalized Linear Model (GLM)). Given an (unknown) distribution D and an activation function  : R  R, an idealized GLM is defined by the existence of a w  Rd such that yi =  w, xi i  [m] where w is the global minimizer of the error function:

1 e^rr(w) =
m

m

(yi - ( w, xi ))2

i=1

Similarly, a noisy GLM is defined by the existence of a w  Rd, which is the global minimizer of

the error function:

e^rr(w) = E(x,y)D (yi - ( w, xi ))2

(Hazan et al., 2015, Lemma 3.2) show that if we draw m  

exp(2 w
2

)

log

1 

samples

{(xi, yi)}mi=1 from a GLM with the sigmoid activation function, then with probability at least 1 - , the empirical error function

1 f (w) =
m

m
(yi -  w, xi )2

i=1

is ( , e w 2 , w)-SLQC in w. However, this is a bit restrictive, since the proof of this result critically

uses properties of the sigmoid function, which are not satisfied by other popular activation functions

such as the ReLU.

In order to go beyond the sigmoid activation function, we introduce a new generalized ReLU activation function.
Definition 3.3. (Generalized ReLU) The generalized ReLU function f : R  R, 0 < a < b, a, b  R is defined as:

5

Under review as a conference paper at ICLR 2018

f (x) =

ax bx

x0 x>0

This function is differentiable at every point except 0. Note that this definition also subsumes variants of ReLU such as the leaky ReLU Xu et al. (2015). We define the function g that gives a valid subgradient at all x for the generalized ReLU to be

g(x) =

a b

x<0 x0

While SLQC is primarily defined for differentiable functions, we now show that with the above definition of the subgradient, the GLM with the generalized ReLU is also SLQC.

Theorem 3.4. In the idealized GLM with generalized ReLU activation, assuming ||w||  W ,

err^(w) is

,

2b2 W a

,

w

- SLQC in w for all w  Bd(0, W ).

Proof.

Consider ||w||



W

such that

e^rrm(w)

=

1 m

mi=1(yi -  w, xi )2  , where m is the

total number of samples. Also let v be a point

/-close

to

minima

w

with



=

2b2 W a

.

Let

g

be

the

subgradient of the generalized ReLU activation and G be the subgradient of e^rrm(w). (Note that as

before, g ., . denotes g( ., . )). Then:

G(w), w - v

2 =
m

m

g w, xi

( w, xi

- yi)

xi, (w - v)

i=1

2 =
m

m

g w, xi

( w, xi

-  w,xi ) [ xi, w - w

+

xi, w - v ]

i=1

(Step 1)

2 m

m

g w, xi

b-1 ( w, xi -  w,xi )2 +( w, xi -  w,xi ) xi, w - v

i=1

(Step 2)

2 m

m

g w, xi (b-1 ( w, xi

-  w,xi )2 - | w, xi

-  w,xi |||xi||||w - v||

i=1

2 m

m

ab-1 ( w, xi

-  w, xi )2 - | w, xi

-  w,xi |||xi||||w - v||

i=1

(Step 3)

2 m

m

ab-1 ( w, xi

-  w, xi )2 - b|| w, xi

-

w,xi ||  ||xi||

i=1

 2ab-1

-

a bW

||

w, xi

-

w,xi ||||xi||

(Step 4) (Step 5)



ab-1

(2 -

1 W

||w

-

w

||||xi||2

)

0

In the above proof, we first use the fact (in Step 1) that in the GLM, there is some w such that  w, xi = yi. Then, we use the fact (in Steps 2 and 4) that the generalized ReLU function is
b-Lipschitz, and the fact that the minimum value of the quasigradient of g is a (Step 3). Subsequently, in Step 5, we simply use the given bounds on the variables xi, w, w due to the setup of the problem (w  Bd(0, W ), and xi  Bd, the unit d-dimensional ball, as in Defn 2.6).

We also prove a similar result for the Noisy GLM below.

Theorem 3.5. In the noisy GLM with generalized ReLU activation, assuming ||w||  W , given

w



B(0, W ), then with probability 

1 -  after m



4W b2 a2

log(1/

)/

2 samples, err^(w) is

,

2b2 W a

,

w

- SLQC in w.

6

Under review as a conference paper at ICLR 2018

(a) Phase - 1

(b) Phase - 2

Figure 1: An illustration of the proposed multi-layer DANTE (best viewed in color). In each training

phase, the outer pairs of weights (shaded in gold) are treated as a single-layer autoencoder to be

trained using single-layer DANTE, followed by the inner single-layer auroencoder (shaded in black).

These two phases are followed by a finetuning process that may be empirically determined, similar to

standard deep autoencoder training.

The proof for Theorem 3.5 is included in Appendix A (Supplementary Section).

Now, we review a result from Hazan et al. (2015) which shows that SNGD provably converges to the optimum for SLQC functions, and hence, with very high probability, for empirical objective functions induced by noisy GLM instances too.

Theorem 3.6 (Hazan et al. (2015)). Let , , G, M,  > 0, let f : Rd  R and w =

arg minw f (w). Assume that for b  b0( , , T ), with probability  1 - , ft defined in Algorithm

1

is

(

, , w)-SLQC

w,

and

|ft|



M t



{1, · · ·

,T}

.

If

we

run

SNGD

with

T



2 ||w1 -w ||2
2

and  =  , and b  max

M

2log(
22

4T 

)

,

b0(

, , T )

, with probability 1 - 2, f (w) - f (w)  3 .

Note that by performing each DANTE alternation with a freshly sampled batch of data points, we can easily establish SLQC properties of the problem, and as a consequence, harness the convergence guarantees provided by Theorem 3.6. Thus, given a single-layer autoencoder with either a sigmoid activation function or a generalized ReLU activation function, DANTE uses SNGD to solve a SLQC problem in each alternating step, which converges to the optima solution in each step with very high probability, as shown above in Theorem 3.6.

Note that our contributions allow us to cover a large family of activation functions, since the tanh

activation is simply a rescaled sigmoid and the generalized ReLU includes many variants of ReLUs.

More importantly, note that the convergence rate of SNGD depends crucially on the  parameter.

Whereas the GLM error function with sigmoid activation has  = eW Hazan et al. (2015), we obtain



=

2b2 W a

(i.e.

linear in W ) for the generalized ReLU setting, an exponential improvement!

This

is significant as in Theorem 3.6, the number of iterations T depends on 2. This shows that SNGD

offers accelerated convergence with generalized ReLU GLMs than sigmoid GLMs.

3.4 EXTENDING TO A MULTI-LAYER AUTOENCODER
In the previous sections, we illustrated how a single hidden-layer autoencoder can be cast as a set of SLQC problems and proposed an alternating minimization method, DANTE. This approach can be generalized to deep autoencoders by considering the greedy layer-wise approach to training a neural network (Bengio et al. (2007)). In this approach, each pair of layers of a deep stacked autoencoder is successively trained in order to obtain the final representation. Each pair of layers considered in this paradigm is a single hidden-layer autoencoder, which can be cast as a pair of SLQC problems that can be trained using DANTE. Therefore, training a deep autoencoder using greedy layer-wise approach can be modeled as a series of SLQC problem pairs. Algorithm 3 summarizes the proposed approach to use DANTE for a deep autoencoder, and Figure 1 illustrates the approach.
Note that it may be possible to use other schemes to use DANTE for multi-layer autoencoders such as a round-robin scheme, where each layer is trained separately one after the other in the sequence in which the layers appear in the network.

7

Under review as a conference paper at ICLR 2018

Algorithm 3: DANTE for a multi-layer autoencoder
Input :Encoder e with weights U, Decoder d with weights V, Number of hidden layers 2n - 1, Learning rates , Stopping threshold , Number of iterations of alternating minimization TAM , initial values U0, V0, minibatch size b
1 t := 1 2 for l = 1 to n do 3 while |f (Ut, Vt) - f (Ut-1, Vt-1)|  or t < TAM do 4 //Use SNGD for minimizations
utl  arg min ExD d(e(x, Ut[1:l-1] · u · U[tl-+11:n-1]), Vt[1:l-1] · V[tl-:n1-1]) - x 2
u

5

vnt -l  arg min ExD d(e(x, U[t1:l] · U[tl-+11:n-1]), V[t1:n-l-1] · v · V[tn--1l+1:n-1]) - x 2
v

t := t + 1 6 end
7 end Output :U, V

(a) Single-layer autoencoder with Sigmoid activation

(b) Single-layer autoencoder with Generalized ReLU activation

Figure 2: Plots of training and test errors vs training iterations for single hidden-layer autoencoder with Sigmoid (left) and Generalized ReLU (right) activations for both DANTE and SGD.

4 EXPERIMENTS AND RESULTS
We validated DANTE by training autoencoders on an expanded 32×32 variant of the standard MNIST dataset LeCun et al. (1998) as well as other datasets from the UCI repository. We also conducted experiments with multi-layer autoencoders, as well as studies with varying number of hidden neurons on single-layer autoencoders. Our experiments on MNIST used the standard benchmarking setup of the dataset1, with 60, 000 data samples used for training and 10, 000 samples for testing. Experiments were conducted using Torch 7 Collobert et al. (2011).
Autoencoder with Sigmoid Activation: A single-layer autoencoder (equivalent to a neural network with one hidden layer) was trained using DANTE and SGD with 600 hidden units, a sigmoid activation, a learning rate of 0.001, standard Mean-Squared Error loss function and a minibatch size of 500 (same setup was maintained for SGD and the SNGD used inside DANTE for fair comparison; one could optimize both SGD and SNGD to improve the absolute result values). The results are shown in Figure 2a. While DANTE takes slightly (negligibly) longer to reach a local minimum, it obtains a better solution than SGD.
Autoencoder with ReLU Activation: A single-layer autoencoder was trained using DANTE and SGD with 600 hidden units, a Leaky ReLU activation (with leakiness parameter 0.01), a learning rate of 0.001, Mean-Squared Error loss function and a minibatch size of 500. The results are shown in Figure 2b. The results for ReLU showed an improvement, and DANTE was consistently superior to SGD across the iterations.
1http://yann.lecun.com/exdb/mnist/
8

Under review as a conference paper at ICLR 2018

MNIST ionosphere svmguide4
USPS

DANTE
93.6% 92.45% 87.65% 90.43%

SGD
92.44% 96.22% 70.37% 89.49%

Figure 3: Reconstructions using the autoencoder Table 1: Classification accuracies using models with ReLU activation. Top Row: Orig- ReLU autoencoder features on different inal Images; Middle Row:Model trained using datasets DANTE; Bottom: Model trained using SGD.

(a) ionosphere dataset

(b) svmguide4 dataset

(c) USPS dataset

Figure 4: Comparison of proposed DANTE vs SGD on other datasets from the UCI repository. The x-axis on all sub-figures is number of mini-batch iterations and y-axis denotes test error, which shows the generalization performance. (Best viewed in color; DANTE = purple, SGD = green)

In Figure 3, we also show the reconstructions obtained by both trained models (DANTE and SGD) for the autoencoder with the Generalized ReLU activation. The model trained using DANTE shows comparable performance as a model trained by SGD under the same settings, in this case. We also conducted experiments to study the effectiveness of the feature representations learned using the models trained using DANTE and SGD in the same setting. After training, we passed the dataset through the autoencoder, extracted the hidden layer representations, and then trained a linear SVM. The classification accuracy results using the hidden representations are given in Table 1. The table clearly shows the superior performance of DANTE on this task.
Experiments on other datasets: We also studied the performance of proposed method on other standard datasets 2, viz. Ionosphere (34 dimensions, 351 datapoints), SVMGuide4 (10 dimensions, 300 datapoints), and USPS (256 dimensions, 7291 datapoints). Figure 4 and Table 1 show the performance of the proposed method vs SGD on the abovementioned datasets. It can be seen that DANTE demonstrates superior generalization performance on most datasets.
Varying Number of Hidden Neurons: Given the decomposable nature of the proposed solution to learning autoencoders, we also studied the effect of varying hyperparameters across the layers, in particular, the number of hidden neurons in a single-layer autoencoder. The results of these experiments are shown in Figure 5. The plots show that when the number of hidden neurons is low, DANTE reaches its minumum value much sooner (considering this is a subgradient method, one can always choose the best iterate over training) than SGD, although SGD finds a slightly better solution. However, when the number of hidden neurons increases, DANTE starts getting consistently better. This can be attributed to the fact that the subproblem is relatively more challenging for an alternating optimization setting when the number of hidden neurons is lesser.
Multi-Layer Autoencoder: We also studied the performance of the proposed multi-layer DANTE method (Algorithm 3) for the MNIST dataset. Figure 6 shows the results obtained by stacking two single-layer autoencoders, each with the generalized (leaky) ReLU activation (note that a two singlelayer autoencoder corresponds to 4 layers in the overall network, as mentioned in the architecture on the figure). Evidently, the figure shows promising performance for DANTE in this experiment. Note
2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
9

Under review as a conference paper at ICLR 2018

(a) 200 hidden neurons

(b) 300 hidden neurons

(c) 400 hidden neurons

(d) 600 hidden neurons

Figure 5: Plots of training and test error vs training iterations on a single-layer autoencoder with generalized ReLU activation, with a varying number of nodes in the hidden layer.

(a) Architecture: 1024->500->500->1024

(b) Architecture: 1024->750->500->750->1024

Figure 6: Plots of (a) training error and (b) test error vs training iterations for multi-layer autoencoders with generalized (leaky) ReLU activations for both DANTE and SGD.

that Figure 6b shows two spikes: one when the training for the next pair of layers in the autoencoder begins, and another when the end-to-end finetuning process is done. This is not present in Figure 6a, since the 500  500 layer in between is only randomly initialized, and is not trained using DANTE or SGD.

5 CONCLUSIONS AND FUTURE WORK

In this work, we presented a novel methodology, Deep AlterNations for Training autoEncoders

(DANTE), to efficiently train autoencoders using alternating minimization, thus providing an effective

alternative to backpropagation. We formulated the task of training each layer of an autoencoder as a

Generalized Linear Model (GLM) problem with an activation function, and leveraged recent results

to use Stochastic Normalized Gradient Descent (SNGD) to train each step of the autoencoder. While

recent work was restricted to using sigmoidal activation functions, we introduced a new generalized

ReLU activation function, and showed that a GLM with this activation function also satisfies the

SLQC property, thus allowing us to expand the applicability of the proposed method to autoencoders

with both sigmoid and ReLU family of activation functions. In particular, we extended the definitions

of local quasi-convexity to use subgradients in order to prove that the GLM with generalized ReLU

activation is

,

b2 W 2a

,

w

- SLQC, which significantly improves the convergence bound for the

corresponding GLM with the generalized ReLU (as compared to a GLM with sigmoid). We also

showed how DANTE can be extended to train multi-layer autoencoders. We empirically validated

DANTE with both sigmoidal and ReLU activations on standard datasets as well as in a multi-layer

setting, and observed that it performs comparably or better than the standard SGD approach to train

autoencoders.

10

Under review as a conference paper at ICLR 2018
DANTE can not only be used to train autoencoders, but can be extended to train standard multi-layer neural networks too. One could use DANTE to train a neural network layer-wise in a round robin fashion, and then finetune end-to-end using SGD. In case of autoencoders with tied weights, one would ideally use DANTE to learn the weights of the required layers, and then finetune end-to-end using a method such as SGD. Our future work will involve a more careful study of the proposed method for deeper autoencoders, as well as in theoretically analyzing the end-to-end convergence of the method for deep multi-layer autoencoders.
REFERENCES
Animashree Anandkumar and Rong Ge. Efficient Approaches for Escaping Higher Order Saddle Points in Non-Convex Optimization. In 29th Conference on Learning Theory (COLT), 2016.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pp. 153­160, 2007.
Thomas Blumensath and Mike E. Davies. Iterative Hard Thresholding for Compressed Sensing. Applied and Computational Harmonic Analysis, 27(3):265­274, 2009.
Miguel Carreira-Perpinan and Weiran Wang. Distributed Optimization of Deeply Nested Systems. In 17th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 10­19, 2014.
Yves Chauvin and David E Rumelhart. Backpropagation: Theory, Architectures, and Applications. Psychology Press, 1995.
R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A Matlab-like Environment for Machine Learning. In NIPS 2011 Workshop on Algorithms, Systems, and Tools for Learning at Scale (Big Learning), 2011.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond Convexity: Stochastic Quasi-Convex Optimization. In 29th Annual Conference on Neural Information Processing Systems (NIPS), pp. 1594­1602, 2015.
Sepp Hochreiter and Jurgen Schmidhuber. Long Short-term Memory. Neural Computation, 8(9): 1735­1780, 1997.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343, 2016.
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank Matrix Completion using Alternating Minimization. In 45th Annual ACM Symposium on Theory of Computing (STOC), 2013.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Yurii E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29:519­531, 1984.
Ankit B. Patel, Tan Nguyen, and Richard G. Baraniuk. A Probabilistic Theory of Deep Learning. arXiv:1504.00641 [stat.ML], 2015.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Internal Representation by Back-propagating Errors. Nature, 323(9):533­536, 1986.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training Neural Networks Without Gradients: A Scalable ADMM Approach. In 33rd International Conference on Machine Learning (ICML), 2016.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical Evaluation of Rectified Activations in Convolutional Network. arXiv:1505.00853v2 [cs.LG], 2015.
11

Under review as a conference paper at ICLR 2018

A NOISY GLM WITH GENERALIZED RELU ACTIVATION FUNCTION

The theorem below is a continuation of our discussion from Section ??.

Theorem A.1. In the noisy GLM with generalized ReLU activation, assuming ||w||  W , given

w



B(0, W ), then with probability 

1 -  after m



4W b2 a2

log(1/

)/

2 samples, err^(w) is

,

2b2 W a

,

w

- SLQC in w.

Proof. Here, i, yi  [0, 1], the following holds:

yi =  w, x + i

(6)

where {i}im=1 are zero mean, independent and bounded random variables, i.e. i  [m], ||i||  1. Then, e^rrm(w) may be written as follows (expanding yi as in Eqn 6):

1 e^rrm(w) = m

m
(yi -  w, xi) )2

i=1

1 =
m

m
( w, xi

-  w, xi )2

i=1

mm

+ 2i( w, xi -  w, xi ) + i2

i=1 i=1

Therefore, we also have (by definition of noisy GLM in Defn 3.2):

e^rrm(w) -

e^rrm(w)

=

1 m

m
( w, xi

-  w, xi )2

i=1

1 +
m

m

2i( w, xi

-  w, xi )

i=1

Consider ||w||  W such that e^rrm(w) - e^rrm(w)  . Also, let v be a point /-close to minima

w,



=

2b2 W a

.

Let

g

be

the

subgradient

of

the

generalized

ReLU

activation

and

G

be

the

subgradient

of e^rrm(w). Then:

G(w), w - v

2 =
m

m

g w, xi

( w, xi

- yi)

xi, (w - v)

i=1

2 =
m

m

g w, xi

( w, xi

-  w, xi

- i)

i=1

[ xi, w - w + xi, w - v ]

2b-1 
m

m

g w, xi ( w, xi

-  w, xi )2

i=1

-2 m

m

g w, xi i( w, xi

-

w, xi )

i=1

2 +
m

m

g w, xi ( w, xi

-  w, xi

- i) w - v, xi )

i=1

12

(Step 1) (Step 2)

Under review as a conference paper at ICLR 2018

 2b-1 m

m

g w, xi ( w, xi

-  w, xi )2

i=1

-2 m

m

g w, xi i( w, xi

-

w, xi

) - 2 b |w - w*| 

i=1

 2b-1 m

m

a

( w, xi

-  w, xi )2

i=1

-2i( w, xi -  w, xi ))

2 +
m

m

ai (( w, xi

-

w, xi )

i=1

+2b-1( w, xi

-  w, xi )

- 2 b |w - w*| 

 2ab-1

- 2 b |w - w*| + 1 m

m

i(w)

i=1

 2ab-1

- ab-1W -1 |w - w*| + 1 m

m

ii(w)

i=1

 2ab-1

- ab-1

1 +
m

m

ii(w)

i=1

 ab-1

1 +
m

m

ii(w)

i=1

(Step 3)
(Step 4)
(Step 5) (Step 6) (Step 7) (Step 8)
(7)

Here, i(w) = 2a( w, xi - w, xi ) + 2b-1( w, xi -  w, xi ), and |ii(w)|  2a(| w, xi - w, xi | + 2b-1| w, xi -  w, xi |)  2a(2| w, xi - w, xi |)  2a(4W ) = 8aW

The above proof uses arguments similar to the proof for the idealized GLM (please see the lines after the proof of Theorem 3.4, viz. the b-Lipschitzness of the generalized ReLU, and the problem setup).

Now, when

1 m

m

i(w)  -ab-1

i=1

our model is SLQC. By simply using the Hoeffding's bound, we get that the theorem statement holds

for

m



4W a

b2
2

log(1/)/

2.

13

