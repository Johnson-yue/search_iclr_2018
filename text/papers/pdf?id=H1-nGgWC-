Under review as a conference paper at ICLR 2018
GAUSSIAN PROCESS BEHAVIOUR IN WIDE DEEP NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented. 1
1 INTRODUCTION
Deep feedforward neural networks have emerged as an essential component of modern machine learning. As such there has been significant research effort in trying to understand the theoretical properties of such models. One important branch of such research is the study of random networks. By assuming a probability distribution on the network parameters, a distribution is induced on the input to output function that such networks encode. This has proved important in the study of initialisation and learning dynamics (Schoenholz et al., 2017) and expressivity (Poole et al., 2016). It is, of course, essential in the study of Bayesian priors on networks (Neal, 1996). The Bayesian approach makes little sense if prior assumptions are not understood, and distributional knowledge can be essential in finding good posterior approximations. Since we typically want our networks to have high modelling capacity, it is natural to consider limit distributions of networks as they become large. Whilst distributions on deep networks are generally challenging to work with exactly, the limiting behaviour can lead to more insight. Further, as we shall see, networks used in the literature may be very close to this behaviour. The seminal work in this area is that of Neal (1996), which showed that under certain conditions random neural networks with one hidden layer converge to a Gaussian process. The question of the type of convergence is non-trivial and part of our discussion. Historically this result was a significant one because it provided a connection between flexible Bayesian neural networks and Gaussian processes (Williams, 1998; Rasmussen & Williams, 2006) 1.1 OUR CONTRIBUTIONS We extend the theoretical understanding of random fully connected networks and their relationship to Gaussian processes. In particular, we prove a rigorous result (Theorem 1) on the convergence of certain finite networks with more than one hidden layer to Gaussian processes. Further, we empirically study the distance between finite networks and their Gaussian process analogues by using maximum mean discrepancy (Gretton et al., 2012) as a distance measure. We find that Bayesian deep networks from the literature can exhibit predictions that are close to Gaussian
1Code for the experiments in the paper can be found at https://github.com/ widedeepnetworks/widedeepnetworks
1

Under review as a conference paper at ICLR 2018

processes. To demonstrate this, we systematically compare exact Gaussian process inference with `gold standard' MCMC inference for Bayesian neural networks. Our work is of relevance to the theoretical understanding of neural network initialisation and dynamics. It is also important in the area of Bayesian deep networks because it demonstrates that Gaussian process behaviour can arise in more situations of practical interest than previously thought. If this behaviour is desired then Gaussian process inference (exact and approximate) should also be considered. In some scenarios, the behaviour may not be desired because it implies a lack of a hierarchical representation. We therefore highlight promising ideas from the literature to prevent such behaviour.

1.2 RELATED WORK The case of random neural networks with one hidden layer was studied by Neal (1996). Cho & Saul (2009) provided analytic expressions for single layer kernels including those corresponding to a rectified linear unit (ReLU). They also studied recursive kernels designed to `mimic computation in large, multilayer neural nets'. As discussed in Section 3 they arrived at the correct kernel recursion through an erroneous argument. Such recursive kernels were later used with empirical success in the Gaussian process literature (Krauth et al., 2017), with a similar justification to that of Cho and Saul. The first case we are aware of using a Gaussian process construction with more than one hidden layer is the work of Hazan & Jaakkola (2015). Their contribution is similar in content to Lemma 1 discussed here, and the work has had increasing interest from the kernel community (Mitrovic et al., 2017). Recent work from Daniely et al. (2016) uses the concept of `computational skeletons' to give concentration bounds on the difference in the second order moments of large finite networks and their kernel analogue, with strong assumptions on the inputs. The Gaussian process view given here, without strong input assumptions, is related but concerns not just the first two moments of a random network but the full distribution. As such the theorems we obtain are distinct. A less obvious connection is to the recent series of papers studying deep networks using a mean field approximation (Poole et al., 2016; Schoenholz et al., 2017). In those papers a second order approximation gives equivalent behaviour to the kernel recursion. By contrast, in this paper the claim is that the behaviour emerges as a consequence of increasing width and is therefore something that needs to be proved. Another surprising connection is to the analysis of self-normalizing neural networks (Klambauer et al., 2017). In their analysis the authors assume that the hidden layers are wide in order to invoke the central limit theorem. The premise of the central limit theorem will only hold approximately in layers after the first one and this theoretical barrier is something we discuss here. An area that is less related than might be expected is that of `Deep Gaussian Processes' (DGPs) (Damianou & Lawrence, 2013). As will be discussed in Section 6, narrow intermediate representations mean that the marginal behaviour is not close to that of a Gaussian process. Duvenaud et al. (2014) offer an analysis that largely applies to DGPs though they also study the Cho and Saul recursion with the motivating argument from the original paper.

Inputs

Activations Activities 11

Activations Activities 22

Output

Figure 1: In this paper we consider fully connected feedforward networks with more than one hidden layer. We call the pre-nonlinearity an activation and post-nonlinearity an activity. As the network becomes increasingly wide the distribution of the marginal distributions of the activations at each layer and of the output will become close to a Gaussian process in a sense described in the text.
2

Under review as a conference paper at ICLR 2018

2 THE DEEP WIDE LIMIT
We consider a fully connected network as shown in Figure 1. The inputs and outputs will be real valued vectors of dimension M and L respectively. The network is fully connected. The initial step and recursion are standard. The initial step is:

M

fi(1)(x) =

wi(,1j)xj + bi(1) .

j=1

(1)

We make the functional dependence on x explicit in our notation as it will help clarify what follows. For a network with D hidden layers the recursion is, for each µ = 1, . . . , D,

gi(µ)(x) = (fi(µ)(x))

(2)

Hµ

fi(µ+1)(x) =

wi(,µj+1)gj(µ)(x) + bi(µ+1) ,

(3)

j=1

so that f (D+1)(x) is the output of the network given input x.  denotes the non-linearity. In all cases the equations hold for each value of i; i ranges between 1 and Hµ in Equation (2), and between 1 and Hµ+1 in Equation (3) except in the case of the final activation where the top value is L. The network could of course be modified to be probability simplex-valued by adding a softmax at the end.

A distribution on the parameters of the network will be assumed. Conditional on the inputs, this induces a distribution on the activations and activities. In particular we will assume independent normal distributions on the weights and biases

wi(,µj)  N (0, Cw(µ)) indep

(4)

b(iµ)  N (0, Cb(µ)) indep.

(5)

We will be interested in the behaviour of this network as the widths Hµ becomes large. The weight variances for µ  2 will be scaled according to the width of the network to avoid a divergence in the variance of the activities in this limit. As will become apparent, the appropriate scaling is

Cw(µ)

=

C^w(µ) Hµ

µ  2.

(6)

The assumption is that C^w(µ) will remain fixed as we take the limit. Neal (1996) analysed this problem

for D = converge

1, to

showing that as H1  , the values of a certain multi-output Gaussian process

iffi(t2h)e(xa)c,titvhietieosuthpauvteobfotuhnedneedtwvaorrikanincet.his

case,

Since our approach relies on the multivariate central limit theorem we will arrange the relevant terms into (column) vectors to make the linear algebra clearer. Consider any two inputs x and x and all output functions ranging over the index i. We define the vector f (2)(x) of length L whose elements
are the numbers fi(1)(x). We define f (2)(x ) similarly. For the weight matrices defined by wi(,µj) for fixed µ we use a `placeholder' index · to return column and row vectors from the weight matrices. In particular wj(,1·) denotes row j of the weight matrix at depth 1. Similarly, w·(,2j) denotes column j at depth 2. The biases are given as column vectors b(1) and b(2). Finally we concatenate the two vectors f (2)(x) and f (2)(x ) into a single column vector F 2 of size 2L. The vector in question takes the form

F2 =

f (2)(x) f (2)(x )

=

b(2) b(2)

+

j

w·(,2j)(wj(,1·)x + bj(1)) w·(,2j)(wj(,1·)x + bj(1))

(7)

3

Under review as a conference paper at ICLR 2018

The benefit of writing the relation in this form is that the applicability of the multivariate central limit theorem is immediately apparent. Each of the vector terms on this right hand side is independent and identically distributed conditional on the inputs x and x . By assumption, the activities have bounded variance. The scaling we have chosen on the variances is precisely that required to ensure the applicability of the theorem. Therefore as H becomes large F 2 converges in distribution to a multivariate normal distribution. The limiting normal distribution is fully specified by its first two moments. The moments in question are:

E fi(2)(x) = 0 E fi(2)(x)fj(2)(x ) = i,j Cw(2)E , ( T x + )( T x + ) + Cb(2)

(8) (9)

Note that we could have taken a larger set of input points to give a larger vector F and again we would conclude that this vector converged in distribution to a multivariate normal distribution. More formally, we can consider the set of possible inputs as an index set. A set of consistent finite dimensional Gaussian distributions on an index set corresponds to a Gaussian process by the Kolmogorov extension theorem. The Gaussian process in question is a distribution over functions defined on the product -algebra, which has the relevant finite dimensional distributions as its marginals.
In the case of a multivariate normal distribution a set of variables having a covariance of zero implies that the variables are mutually independent. Looking at Equation (8), we see that the limiting distribution has independence between different components i, j of the output. Combining this with the recursion (2), we might intuitively suggest that the next layer also converges to a multivariate normal distribution in the limit of large Hµ. Indeed we state the following lemma, which we attribute to Hazan & Jaakkola (2015): Lemma 1 (Normal recursion). If the activations of a previous layer are normally distributed with moments:

E fi(µ-1)(x) = 0 E fi(µ-1)(x)fj(µ-1)(x ) = i,j K(x, x ),

(10) (11)

Then under the recursion (2) and as H   the activations of the next layer converge in distribution to a normal distribution with moments

E fi(µ)(x) = 0 E fi(µ)(x)fj(µ)(x ) = i,j C^w(µ)E( 1, 2)N (0,K)[( 1)( 2)] + Cb(µ)

(12) (13)

where K is a 2 × 2 matrix containing the input covariances.
Unfortunately the lemma is not sufficient to show that the joint distribution of the activations of higher layers converge in distribution to a multivariate normals. This is because for finite H the input activations do not have a multivariate normal distribution - this is only attained (weakly or in distribution) in the limit. We are able to offer the following theorem rigorously: Theorem 1. Consider a Bayesian deep neural network of the form in Equations (1) and (2) using ReLU activation functions. Then there exist width functions hµ : N  N such that H1 = h1(n), . . . , HD = hD(n), and for a countable input set (x(i))i=1, the distribution of the output of the network converges in distribution to a Gaussian process as n  .
A proof is included in the appendix. We conjecture that a more general theorem will hold. In particular we expect that the width functions hµ can be taken to be the identity and that the nonlinearity can be extended to monotone functions with well behaved tails. Our conjecture is based on the intuition from Lemma 1 and from our experiments, in which we always take the width function to be the identity.

4

Under review as a conference paper at ICLR 2018

MMD SQUARED(GP, NN)

3 SPECIFIC KERNELS UNDER RECURSION
Cho & Saul (2009) suggest a family of kernels based on a recurrence designed to `mimic computation in large, multilayer neural nets'. It is therefore of interest to see how this relates to deep wide Gaussian processes. A kernel may be associated with a feature mapping (x) such that K(x, x ) = (x) · (x ). Cho and Saul define a recursive kernel through a new feature mapping by compositions such as ((x)). However this cannot be a legitimate way to create a kernel because such a composition represents a type error. There is no reason to think the output dimension of the function  matches the input dimension and indeed the output dimension may well be infinite. The paper does find elegant ways to do the kernel expectation in Equation (8) for the special case where:

(u) = (u)ur for r = 0, 1, 2, 3

(14)

where  is the Heaviside step function. They also apply their recursion method to this nonlinearity. This in fact turns out to be equivalent to applying the correct recursion formula from Lemma 1 (Hazan & Jaakkola, 2015). Since r = 1 corresponds to rectified linear units we apply this analytic kernel recursion in all of our experiments.

0.0200 0.0175 0.0150

1 hidden layer 2 hidden layers 3 hidden layers

0.0125

0.0100

0.0075

0.0050

0.0025

0.0000 0 10 20 Number of hidd3e0n units per layer 40 50 60

Figure 2: A comparison of finite random neural networks to their corresponding Gaussian process

analogue using an (RBF) kernel estimator of the squared maximum mean discrepancy (MMD). The

results are consistent with the emergence of Gaussian process behaviour as the networks become

wide. The red processes with

dashed line is for calibration and isotropic RBF kernels and length

denotes scales l

tahneds2qluwarheedreMlM=Dbe8twisetehne

two Gaussian characteristic

length scale of the input space (see text).

4 MEASURING CONVERGENCE USING MAXIMUM MEAN DISCREPANCY
In this section we use the kernel based two sample tests of Gretton et al. (2012) to empirically measure the similarity of finite random neural networks to their Gaussian process analogues. The maximum mean discrepancy (MMD) between two distributions P and Q is defined as:

MMD(P, Q, H) := sup EP [h] - EQ[h]
||h||H 1

(15)

where H denotes a reproducing kernel Hilbert space and || · ||H denotes the corresponding norm. It gives the biggest possible difference between expectations of a function under the two distributions under the constraint that the function has Hilbert space norm less than or equal to one. We used the unbiased estimator of squared MMD given in Equation (3) of Gretton et al. (2012).

5

Under review as a conference paper at ICLR 2018

In this experiment and all those that follow we take weight variance parameters C^w(µ) = 0.8 and bias

variance Cb = 0.2. We took 10 standard normal input points in 4 dimensions and pass them through

2000 independent random neural networks drawn from the distribution discussed in this paper. This

was then compared to 2000 samples drawn from the corresponding Gaussian process distribution.

The experiment was performed with different numbers of hidden layers and numbers of units per

hidden layer. We repeated each experiment 20 times which allows us to reduce variance in our results

and give a simple estimate of measurement error. The experiments use an RBF kernel for the MMD

estimate with lengthscale 1/2. In order to help give an intuitive sense of the distances involved

we also include a comparison between two Gaussian processes with isotropic RBF kernels using

the same MMD distance measure. processes are taken to be l and 2l,

The kernel where the

length scales characteristic

for this pair length scale

of l

='calib8raistiocnh'osGeanutsosibane

sensible for the standard Normal input distribution on the four dimensional space.

The results of the experiment are shown in Figure 2. We see that for each fixed depth the network converges towards the corresponding Gaussian process as the width increases. For the same number of hidden units per layer, the MMD distance between the networks and their Gaussian process analogue becomes higher as depth increases. The rate of convergence to the Gaussian process is slower as the number of hidden layers is increased.

5 COMPARING BAYESIAN DEEP NETWORKS TO GAUSSIAN PROCESSES
In this section we compare the behaviour of finite Bayesian deep networks of the form considered in this paper with their Gaussian process analogues. If we make the networks wide enough the agreement will be very close. It is also of interest, however, to consider the behaviour of networks actually used in the literature, so we use 3 hidden layers and 50 hidden units which is typical of the networks used by Herna´ndez-Lobato & Adams (2015). Fully connected Bayesian deep networks with finite variance priors on the weights have also been considered in other works (Graves, 2011; Hernandez-Lobato et al., 2016; Blundell et al., 2015), though the specific details vary. We use rectified linear units and correct the variances to avoid a loss of prior variance as depth is increased as discussed in Section 3. Our general strategy was to compare exact Gaussian process inference against expensive `gold standard' Markov Chain Monte Carlo (MCMC) methods. We choose the latter because used correctly it works well enough to largely remove questions of posterior approximation quality from the calculus of comparison. It does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive. We therefore sound a note of caution about extrapolating our empirical finite network conclusions too confidently to this domain. On the other hand, prior dominated problems are generally regarded as an area of strength for Bayesian approaches and in this context our results are directly relevant. We computed the posterior moments by the two different methods on some example datasets. For the MCMC we used Hamiltonian Monte Carlo (HMC) (Neal, 2010) updates interleaved with elliptical slice sampling (Murray et al., 2010). We considered a simple one dimensional problem and a two dimensional real valued embedding of the four data point XOR problem. We see in Figures 3 and 4 (left) that the agreement in the posterior moments between the Gaussian process and the Bayesian deep network is very close. A key quantity of interest in Bayesian machine learning is the marginal likelihood. It is the normalising constant of the posterior distribution and gives a measure of the model fit to the data. For a Bayesian neural network, it is generally very difficult to compute, but with care and computational time it can be approximated using Hamiltonian annealed importance sampling (Sohl-Dickstein & Culpepper, 2012). The log-importance weights attained in this way constitute a stochastic lower bound on the marginal likelihood (Grosse et al., 2015). Figure 4 (right) shows the result of such an experiment compared against the (extremely cheap) Gaussian process marginal likelihood computation on the XOR problem. The value of the log-marginal likelihood computed in the two different ways agree to within a single nat which is negligible from a model selection perspective (Grosse et al., 2015). Predictive log-likelihood is a measure of the quality of probabilistic predictions given by a Bayesian regression method on a test point. To compare the two models we sampled 10 standard normal train and test points in 4 dimensions and passed them through a random network of the type under study to get regression targets. We then discarded the true network parameters and compared the predic-

6

Under review as a conference paper at ICLR 2018

Output Output

tions of posterior inference between the two methods. We also compared the marginal predictive distributions of a latent function value. Figure 5 shows the results. We see that the correspondence in predictive log-likelihood is close but not exact. Similarly the marginal function values are close to those of a Gaussian process but are slightly more concentrated.

Gaussian process
2 1 0 1 2
2.0 1.5 1.0 0.5 In0p.0ut 0.5 1.0 1.5 2.0

Bayesian deep network
2 1 0 1 2
2.0 1.5 1.0 0.5 In0p.0ut 0.5 1.0 1.5 2.0

Figure 3: A comparison between Bayesian posterior inference in a Bayesian deep neural network and posterior inference in the analogous Gaussian process. The neural network has 3 hidden layers and 50 units per layer. The lines show the posterior mean and two  credible intervals.

2 Gaussian process

1

0

1

22 1 0

1

2 Bayesian deep network
1

1.0 0.5

0 0.0

1 2 22 1 0

0.5 1.0 12

12.5 L1o2g.0margin1a1l.5likeliho1o1d.0

10.5

Figure 4: A comparison between posterior inference for a Gaussian process and a Bayesian deep network for a real value embedding of the XOR function. Left and centre: The two posterior means. The mean absolute different between the two posterior estimate grids is 0.027. Right: Kernel density estimate of the log weights from annealed importance sampling on a Bayesian deep network compared to the analogous Gaussian process marginal likelihood shown by the vertical line. The neural network has 3 hidden layers and 50 units per layer.

6 AVOIDING GAUSSIAN PROCESS BEHAVIOUR
When using deep Bayesian neural networks as priors, the emergence of Gaussian priors raises important questions in the cases where it is applicable, even if one sets aside questions of computational tractability. It has been argued in the literature that there are important cases where kernel machines with local kernels will perform badly (Bengio et al., 2005). The analysis applies to the posterior mean of a Gaussian process. The emergent kernels in our case are hyperparameter free. Although they do not meet the strict definition of what could be considered `local' the fact remains that any Gaussian process with a fixed kernel does not use a learnt hierarchical representation. Such representations are widely regarded to be essential to the success of deep learning. The question therefore arises as to what can be done to avoid Gaussian process behaviour if it is not desired. Speaking loosely, to stop the onset of the central limit theorem and the approximate analogues discussed in this paper one needs to make sure that one or more of its conditions is far from being met. Since the chief conditions on the summands are independence, bounded variance and many terms, violating these assumptions will remove Gaussian process behaviour. Deep Gaussian processes (Damianou & Lawrence, 2013) are not close to standard Gaussian processes marginally because they are typically used with narrow intermediate layers. It can be challenging to choose the precise nature of these narrow layers a priori. Neal (1996) suggests using networks with infinite
7

Under review as a conference paper at ICLR 2018

Neural network log density Density

0 1 2 3 4 5 6 7 8 8 7 Ga6ussian5proce4ss log3densit2y 1 0

1.2 1.0 0.8 0.6 0.4 0.2 0.0 3

2 Fun1ction va0lue 1

2

Figure 5: A comparison of the predictive distributions of a Bayesian deep network and a Gaussian process on a randomly generated test case. Left: the per point log-densities of the two models. Right: a randomly selected predictive marginal distribution for the latent function on a randomly selected test point.

variance in the activities. With a single hidden layer and correctly scaled, these networks become alpha stable processes in the wide limit. Neal also discusses variants that destroy independence by coupling weights. Our results about the emergence of Gaussian processes even with more than one hidden layer mean these ideas are of considerable interest going forward.
7 CONCLUSIONS
Studying the limiting behaviour of distributions on feedforward networks has been a fruitful avenue for understanding these models historically. In this paper we have extended the state of knowledge about the wide limit, including for networks with more than one hidden layer. In particular, we have exhibited limit sequences of networks that converge in distribution to Gaussian processes with a certain recursively defined kernel. Our empirical study using MMD suggests that this behaviour is exhibited in a variety of models of size comparable to networks used in the literature. This led us to juxtapose finite Bayesian neural networks with their Gaussian process analogues, finding that the agreement in terms of key predictors is close empirically. If this Gaussian process behaviour is desired then exact and approximate inference using the analytic properties of Gaussian processes should be considered as an alternative to neural network inference. Since Gaussian processes have an equivalent flat representation then in the context of deep learning the behaviour may well not be desired and steps should be taken to avoid it. We view these results as a new opportunity to further the understanding of neural networks in the work that follows. Initialisation and learning dynamics are crucial topics of study in modern deep learning which require that we understand random networks. Bayesian neural networks should offer a principled approach to generalisation but this relies on successfully approximating a clearly understood prior. In illustrating the continued importance of Gaussian processes as limit distributions, we hope that our results will further research in these broader areas.
REFERENCES
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The Curse of Dimensionality for Local Kernel Machines. Technical Report 1258, De´partement d'informatique et recherche ope´rationnelle, Universite´ de Montre´al, 2005.
8

Under review as a conference paper at ICLR 2018
V. Bentkus. On the Dependence of the Berry-Esseen bound on Dimension. Journal of Statistical Planning and Inference, 2003.
Patrick Billingsley. Convergence of Probability Measures. John Wiley & Sons Inc., Second edition, 1999.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight Uncertainty in Neural Networks. International Conference on Machine Learning (ICML), 2015.
Youngmin Cho and Lawrence K. Saul. Kernel Methods for Deep Learning. Advances in Neural Information Processing Systems (NIPS), 2009.
Andreas C. Damianou and Neil D. Lawrence. Deep Gaussian Processes. International Conference on Artificial Intelligence and Statistics (AISTATS), 2013.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. Advances in Neural Information Processing Systems (NIPS), 2016.
David Duvenaud, Oren Rippel, Ryan P. Adams, and Zoubin Ghahramani. Avoiding Pathologies in very Deep Networks. International Conference on Artificial Intelligence and Statistics (AISTATS), 2014.
Alex Graves. Practical Variational Inference for Neural Networks. Advances in Neural Information Processing Systems (NIPS), 2011.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A Kernel Two-sample test. Journal of Machine Learning Research (JMLR), 2012.
R. B. Grosse, Z. Ghahramani, and R. P. Adams. Sandwiching the marginal likelihood using bidirectional Monte Carlo. ArXiv e-prints, November 2015.
T. Hazan and T. Jaakkola. Steps Toward Deep Kernel Methods from Infinite Neural Networks. ArXiv e-prints, August 2015.
Jose Hernandez-Lobato, Yingzhen Li, Mark Rowland, Thang Bui, Daniel Hernandez-Lobato, and Richard Turner. Black-box alpha divergence minimization. International Conference on Machine Learning (ICML), 2016.
Jose´ Miguel Herna´ndez-Lobato and Ryan P. Adams. Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks. International Conference on Machine Learning (ICML), 2015.
Gu¨nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-Normalizing Neural Networks. CoRR, abs/1706.02515, 2017.
Karl Krauth, Edwin V Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring the capabilities and limitations of Gaussian Process models. Conference on Uncertainty in Artificial Intelligence (UAI), 2017.
J. Mitrovic, D. Sejdinovic, and Y. W. Teh. Deep Kernel Machines via the Kernel Reparametrization Trick. In International Conference on Learning Representations (ICLR) Workshop Track, 2017.
Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical Slice Sampling. International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer, 1996. Radford M. Neal. MCMC using Hamiltonian Dynamics. Handbook of Markov Chain Monte Carlo,
2010. Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Ex-
ponential expressivity in Deep Neural Networks through Transient Chaos. Advances in Neural Information Processing Systems (NIPS), 2016.
9

Under review as a conference paper at ICLR 2018
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. International Conference on Learning Representations (ICLR), 2017.
Jascha Sohl-Dickstein and Benjamin J. Culpepper. Hamiltonian Annealed Importance Sampling for partition function estimation. CoRR, abs/1205.1925, 2012.
Christopher K. I. Williams. Computing with Infinite Networks. Advances in Neural Information Processing Systems (NIPS), 1998.
A PROOF OF MAIN THEOREM
A.1 STATEMENT OF THEOREM AND NOTATION In this section, we provide a proof of the main theorem of the paper, which we begin by recalling. Theorem 1. Consider a Bayesian deep neural network of the form in Equations (1) and (2) using ReLU activation functions. Then there exist width functions hµ : N  N such that H1 = h1(n), . . . , HD = hD(n), and for a countable input set (x(i))i=1, the distribution of the output of the network converges in distribution to a Gaussian process as n  . The theorem is proven via use of the propositions that follow below. The broad structure of the proof is to use a particular variant of the Berry-Esseen inequality to upper bound how far each layer is from a multivariate normal distribution, and then to inductively propagate these inequalities through the network, leading to a bound on the distance between the output of the network for a collection of input points, and a multivariate Gaussian distribution. These notions will be made precise below. We begin in Section A.2 by stating the propositions that will be used in the proof of Theorem 1, but first establish notation that will be used in the remainder of the appendix. Given a finite set of inputs x(1), . . . , x(n)  RM , we will write:
· f (µ)(x) for the random variables (f (µ)(x(i)))ni=1 collectively taking values in RnHµ ; · fj(µ)(x) for the random variables (fj(µ)(x(i)))ni=1 collectively taking values in Rn; · g(µ)(x) for the random variables (g(µ)(x(i)))ni=1 collectively taking values in RnHµ ; · gj(µ)(x) for the random variables (gj(µ)(x(i)))ni=1 collectively taking values in Rn; Throughout, if U1, U2 are random variables taking in values in some Euclidean space Rd, we will define
d(U1, U2) = sup |P(U1  A) - P(U2  A)| .
ARd A convex
wWiethwciollvaalrsiaonccoenmsiadterricmesulotfivbalroiactkednioargmonaal ldfiostrrmib,ustuiocnhsth(Zatj(µ)(x(i))|j = 1, . . . , Hµ , i = 1, . . . , n) Cov(Zk(µ)(x(a)), Zl(µ)(x(b))) = 0 for distinct k, l  {1, . . . , Hµ} , for all x(a), x(b) .
To avoid writing this in full every time it is required, we will refer to this condition as blockwise independence with respect to the index j. We will avoid specification of all covariance values, deferring to the expression (13) given in the main paper. Finally, to simplify notation, we will assume that the network output is one dimensional. Our proof trivially extends to arbitrary finite output dimension where the limiting distribution is a coordinate-wise independent multivariate GP.
10

Under review as a conference paper at ICLR 2018

A.2 SUPPORTING RESULTS

Proposition 1. Let  > 0, and x(1), . . . x(n)  RM . Then for Hµ-1 sufficiently large, suppose the condition
d(f (µ-1)(x), Z(µ-1)(x))  2-nHµ-1  ,
holds, where Z(µ-1)(x) = (Zj(µ-1)(x(i))|j = 1, . . . , Hµ-1 , i = 1, . . . , n) is mean-zero multivariate normal, with blockwise independence with respect to the index j. Then we have

d(f (µ)(x), Z(µ)(x))   ,

where Z(µ)(x) = (Zj(µ)(x(i))|j = 1, . . . , Hµ , i = 1, . . . , n) is mean-zero multivariate normal, with blockwise independence with respect to the index j.
Proposition 2. Let  > 0, and x(1), . . . , x(n)  RM . Then there exist layer widths HD, Hµ = O(2Hµ+1 ) for µ = D - 1, . . . , 1, such that

d(f (D+1)(x), Z(x))   ,

where Z(x) is a mean-zero multivariate normal random variable.

In establishing the two propositions above, the following three lemmas will be useful.
Lemma 2. Let  > 0, and let Z(µ-1)(x) = (Zj(µ-1)(x(i))|j = 1, . . . , Hµ-1 , i = 1, . . . , n) be mean-zero multivariate normal, with blockwise independence with respect to the index j. Let g(µ-1)(x) = (Z(µ-1)(x)), and let f (µ)(x) be given by

Hµ-1

f (µ)(x(i)) =

w·(,µj)gj(µ-1)(x(i)) + b(µ) ,

j=1

for i = 1, . . . , n. Then given  > 0, for all sufficiently large Hµ-1 we have:

d(f (µ)(x), Z(µ)(x))   ,

where Z(µ)(x) = (Zj(µ)(x(i))|j = 1, . . . , Hµ , i = 1, . . . , n) is mean-zero multivariate normal, with blockwise independence with respect to the index j. Lemma 3. Let Z(µ-1)(x) = (Zj(µ-1)(x(i))|j = 1, . . . , Hµ-1 , 1, . . . , n) be mean-zero multivariate normal, with blockwise independence with respect to the index j, such that for some  > 0,
d(Z(µ-1)(x), f (µ-1)(x)) <  .

Then, defining f (µ)(x) by

Hµ

f (µ)(x(i)) =

w·,j (Zj(µ-1)(x(i))) + b(µ) ,

j=1

in the particular case where  is the elementwise ReLU function, we have

d(f (µ)(x), f (µ)(x))  2nHµ-1  .

Lemma 4. Let X1, . . . , Xµ-1 be iid random variables of the form Xj = gj(µ-1)(x)  w·(,µj), where w·(,µj) is a multivariate normal variable taking values in RHµ with mean vector 0, and covariance C^w(µ)I. We denote the variance of Xj by  and its Schur decomposition as  = QQT. Then  = E Q-1/2QT Xj 3  CHµ,n, where CHµ,n  R depends on Hµ and n, but is
independent of Hµ-1.

11

Under review as a conference paper at ICLR 2018

A.3 PROOFS

Proof of Lemma 2. We use a straightforward variant of a particular Berry-Esseen inequality described in Bentkus (2003). We first state this result from the literature, and then derive a straightforward variation that we will use in the sequel.

Theorem 2 (From Bentkus with mean vector 0, identity

(2003)). Let X1, . . covariance matrix,

. , Xn and 

be =

iid E

random Xi 3

variables < . Let

taking Sn =

values in

1 n

n i=1

Rd, Xi,

and let Y be a standard d-dimensional multivariate normal random vector. Then we have

sup
ARd

|P(Sn

 A) - P(Y

 A)| 

400d1/4 n

A convex

We need a mildly modified version of this theorem to deal with iid random vectors X1, . . . , Xn with non-identity covariance matrices. To this end, suppose that  is the (full-rank) covariance matrix of each Xi, with decomposition  = RR , for some invertible matrix R; R can be obtained, for example, by using Cholesky or Schur decomposition. The random variables R-1X1, . . . , R-1Xn are then iid, mean zero and with identity covariance matrices, so we may apply Theorem 2 to obtain:

where  = E

sup
ARd

|P(R-1Sn

 A) - P(Y

 A)| 

400d1/4 n

,

A convex

R-1Xi 3 . Now note that this is equivalent to

sup
ARd

|P(Sn  RA) - P(RY

 RA)| 

400d1/4 n

,

A convex

noting that LY  N (0, ).

Since R is invertible, and recalling the definition of the distance d above, this is exactly equivalent to:

d(Sn, RY

)



400d1/4 n

,

which is the variant of Bentkus' result we will require in the sequel.

(16)

We apply this bound to the sum

Hµ-1
gj(µ-1)(x)  w·(,µj)
j=1

where  denotes the Kronecker product. Noting that the summands indexed by j are iid by assump-

tion, with the expected third moment norm featuring in the Berry-Esseen inequality upper-bounded

by  ness

 of

CHµ,n CHµ,n

,fofollroswosmferocmonLsteamntmCaH4µ).,n

depending

on

Hµ

and

n,

but

independent

of

Hµ-1

(finite-

As a consequence, we have the following bound:

Hµ-1



d  gj(µ-1)(x)  w·(,µj), Z (x)  400CHµ,n(nHµ)1/4/

j=1

nHµ-1 ,

wblhoecrkewZise(xin)de=pe(nZdje(nxc(ei)w)|ijth=res1p,e.c.t.

, Hµ , to the

i = 1, . index j.

.

.,

n)

is

mean-zero

multivariate

normal,

with

We now select Hµ-1 large enough so that the right-hand side of the inequality is bounded above by , which yields

Hµ-1



d  gj(µ-1)(x)  w·(,µj), Z (x)   .

j=1

12

Under review as a conference paper at ICLR 2018

Adding the independent bias vector b(µ) immediately yields

 Hµ-1



d 1n  b(µ) +

gj(µ-1)(x)  w·(,µj), Z(x)   ,

j=1

where Z(x) is mean-zero multivariate normal, with the same block-diagonal covariance structure as described for Z (x) above, and 1n  Rn is a vector of 1's.

Proof of Lemma 3. Let A  RnHµ be an arbitrary convex set. First, observe that we have



Hµ



P 1n  b(µ) + (fj(µ-1)(x))  w·(,µj)  A

j=1

 

Hµ

 

=E P 1n  b(µ) + (fj(µ-1)(x))  w·(,µj)  A w(µ), b(µ)

j=1

  Hµ





=E P  (fj(µ-1)(x))  w·(,µj)  A - 1n  b(µ) w(µ), b(µ)

j=1

Now, note that for fixed w(µ) and b(µ), the event

 Hµ





 

(fj(µ-1)(x))



w·(,µj ) 



A

- 1n



b(µ)

 w(µ), b(µ)

 j=1



(17)

is exactly that the vector (f (µ-1)(x)) lies in the preimage of the convex set A - 1n  b(µ) under the linear map w(µ), which is again a convex set. Secondly, observe that for the specific ReLU nonlinearity , if C is an arbitrary convex set, then {(f (µ-1)(x)|(f (µ-1)(x))  C} may be written as the disjoint union of at most 2n×Hµ-1 convex sets:
{f (µ-1)(x)|(f (µ-1)(x))  C}
= (-1(C)  {t  RnHµ-1 |ti  0 i})
{t  RnHµ-1 |tI < 0, y  C s.t. yIc = tIc , yI = 0} .
I {1,...,nHµ-1 } I =

Applying the assumed bound in the statement of the lemma to each of these sets, we obtain |P((f (µ-1)(x))  C) - P((f (µ-1)(x))  C)|  2nHµ-1  .
Substituting this bound into the conditional probability (17) yields |P(f (µ)(x)  A) - P(f (µ)(x)  A)|  2nHµ-1  .
Since A was an arbitrary convex set, the proof is complete.

Proof of Lemma 4. Note that by independence of gj(µ-1)(x) from w·(,µj) we have that each Xj has mean zero and covariance  =   C^w(µ)I where  is the covariance matrix of gj(µ-1)(x). By standard properties of the Kronecker product, the Schur decomposition of  is (QQT )(C^w(µ)I) where QQT is the Schur decomposition of . Simple algebraic manipulation yields:

E Q-1/2QT (gj(µ-1)(x)  w·(,µj)) 3 = E (Q-1/2QT gj(µ-1)(x))  ((C^w(µ))-1/2w·(,µj)) 3 = E Q-1/2QT gj(µ-1)(x) 3 E (C^w(µ))-1/2w·(,µj) 3 .
13

Under review as a conference paper at ICLR 2018

Ntriobtuictieotnh,aatntdhethruasnditosmsqvuaarrieadbnleo(rmC^w(fµo)l)lo-w1/s2twhe·(,µjc)hfio-sllqouwarsetdhediRstHriµb-udtiiomnewnsiitohnHalµsdtaengdreaerds

normal disof freedom,

which is also known as the Gamma(Hµ/2, 1/2) distribution. Exponentiating to the power of 3/2

and taking the expectation, we obtain:

E

(C^w(µ))-1/2w·(,µj) 3

= 23/2 ((Hµ + 3)/2) . (Hµ/2)

Finally, Q-1/2QT gj(µ-1)(x) 3  gj(µ-1)(x) 3/m3/in2 where min is the smallest value on the diagonal of . If the activation  does not increase the norm of the input vector (as is the case for trhecetkifineodwlninmeaurl)t,ivwareiahteavneormgj(aµl-d1i)s(trxib)u3tion wfitj(hµm-1e)a(nx)ze3roaalmndosctosvuarreialyn,cewmhearterifxj(wµ-ho1)s(exS)cfhoulrlodwescomposition will be denoted as U U T . Using standard Gaussian identities, we can write

E

fj(µ-1)(x) 3 = E

U 1/2 3 = E

1/2 3



23/2m3/a2x

((n + 3)/2) (n/2)

,

where   N (0, In) and max is the highest entry on the diagonal of . Putting it all together, we arrive at the desired upper bound CHµ,n

E

Q-1/2QT Xj 3 

4 max 3/2 ((Hµ + 3)/2) ((n + 3)/2) .

min

(Hµ/2)

(n/2)

Because max and min are derived from distributions of the limiting variables gj(µ-1)(x), fj(µ-1)(x) and w·(,µj), the bound only depends on Hµ and n as desired.

Proof of Proposition 1. First, we apply Lemma 2 with /2 to obtain Hµ-1 sufficiently large such that
d(f (µ)(x), Z(µ)(x))  /2 . We next apply Lemma 3 with the assumption of the proposition to obtain
d(f (µ)(x), f (µ)(x))  /2 .
Using the triangle inequality to combine these two inequalities yields the statement of the proposition.

Proof of Proposition 2. The idea of the proof is to chain Proposition 1 together across the layers of the network. We first apply the result to the output layer. For a given  > 0, this yields a final hidden layer size HD such that if
d(f (D)(x), Z(D)(x)) < 2-nHD  ,
then d(f (D+1)(x), Z(x)) <  .

Applying this inductively iteratively produces hidden layer sizes HD-1, HD-2, . . . , H1 such that if

d(f (µ)(x), Z(µ)(x))  2-n

 ,D
i=µ

Hj

then d(f (D+1)(x), Z(x))   .

Finally, note that from the definition of the network, the distribution of f (1)(x) is exactly multivariate normal with the required covariance structure, completing the proof.

14

Under review as a conference paper at ICLR 2018

Proof of Theorem 1. respect to the metric

To

prove

that

(f (D+1)(x(i)))i=1

converges

weakly

to

a

Gaussian

process

with



(x, x ) = 2-i x(i) - x (i) ,

i=1
it is sufficient (by e.g. Billingsley (1999)) to prove weak convergence of the finite-dimensional marginals of the process to multivariate Gaussian random variables, with covariance matrix matching that specified by the kernel of the proposed Gaussian process.

To this end, let I be a finite subset of N, and consider the inputs (x(i))iI . We may now apply Proposition 2 to obtain weak convergence of the joint distribution of the output variables of the network, (f (D+1)(x(i)))iI to a multivariate Gaussian with the correct covariance matrix. As the finite subset of inputs was arbitrary, we are done.

15

