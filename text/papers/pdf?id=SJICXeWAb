Under review as a conference paper at ICLR 2018
DEPTH SEPARATION AND WEIGHT-WIDTH TRADE-OFFS
FOR SIGMOIDAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Some recent work has shown separation between the expressive power of depth-2 and depth-3 neural networks. These separation results are shown by constructing functions and input distributions, so that the function is well-approximable by a depth-3 neural network of polynomial size but it cannot be well-approximated under the chosen input distribution by any depth-2 neural network of polynomial size. These results are not robust and require carefully chosen functions as well as input distributions. We show a similar separation between the expressive power of depth-2 and depth3 sigmoidal neural networks over a large class of input distributions, as long as the weights are polynomially bounded. While doing so, we also show that depth2 sigmoidal neural networks with small width and small weights can be wellapproximated by low-degree multivariate polynomials.
1 INTRODUCTION
Understanding the remarkable success of deep neural networks in many domains is an important problem at present (e.g., LeCun et al. (2015)). This problem has many facets such as understanding generalization, expressive power, optimization algorithms in deep learning. In this paper, we focus on the question of understanding the expressive power of neural networks. In other words, we study what functions can and cannot be represented and approximated by neural networks of bounded size, depth, width and weights.
The early results on the expressive power of neural networks showed that the depth-2 neural networks are universal approximators; that is to say, with only mild restrictions on the activation functions or neurons, the depth-2 neural networks are powerful enough to uniformly approximate arbitrary continuous functions on bounded domains in Rd, e.g., Cybenko (1989); Hornik et al. (1989); Barron (1994). However, the bounds that they provide on the size or width of these neural networks are quite general, and therefore, weak. Understanding what functions can be represented or wellapproximated by neural networks with bounded parameters is a general direction in the study of expressive power of neural networks. Here the parameters could mean the number of neurons, the width of hidden layers, the depth, and the magnitude of its weights etc.
Natural signals (images, speech etc.) tend to be representable as compositional hierarchies LeCun et al. (2015), and deeper networks can be thought of as representing deeper hierarchies. The power of depth has been a subject of investigation in deep learning, e.g., He et al. (2016). We are interested in understanding the effect of depth on the expressive power. In particular, one may ask whether having more depth allows representation of more functions if the size bound remains the same.
Eldan & Shamir (2016) show a separation between depth-2 and depth-3 neural networks. More precisely, they exhibit a function g : Rd  R anda probability distribution µ on Rd such that g is bounded and supported on a ball of radius O( d) and expressible by a depth-3 network of size polynomially bounded in d. But any depth-2 network approximating g in L2-norm (or squared error) within a small constant under the distribution µ must be of size exponentially large in d. Their separation works for all reasonable activation functions including ReLUs (Rectified Linear Units) and sigmoids. The function and the input distribution in Eldan & Shamir (2016) are carefully constructed and their proof techniques seem to crucially rely on the specifics of these constructions. Building upon this result, Safran & Shamir (2017) show that while the indicator function of the
1

Under review as a conference paper at ICLR 2018
L2-ball can be well-approximated by depth-3 networks of polynomial size, any good approximation to it by depth-2 networks must require exponential size. Here, the notion of approximation in the lower bound is the same as in Eldan & Shamir (2016) and a carefully constructed distribution that is arguably not quite natural.
Daniely (2017) (see also Martens et al. (2013)) also gave a separation between depth-2 and depth-3 networks by exhibiting a function g : Sd-1 × Sd-1  R which can be well-approximated by a depth-3 ReLU neural network of polynomially bounded size and weights but cannot be approximated by any depth-2 (sigmoid, ReLU or more general) neural network of polynomial size with (exponentially) bounded weights. This separation holds under uniform distribution on Sd-1 × Sd-1, which is more natural than the previous distributions. However, the proof technique crucially uses harmonic analysis on the unit sphere, and does not seems robust or applicable to other distributions.
Telgarsky (2016) shows a separation between depth-2k3 + 8 and depth-k ReLU neural networks, for any positive integer k, when the input is uniformly distributed over [-1, 1]d. Liang & Srikant (2017) (see also Safran & Shamir (2017); Yarotsky (2016)) show that there are univariate functions on a bounded interval such that neural networks of constant depth require size at least  (poly(1/ )) for a uniform -approximation over the interval, whereas deep networks (the depth can depend on ) can have size O (polylog(1/ )).
The above separation results all fit the following template: certain carefully constructed functions can be well approximated by deep networks, but are hard to approximate by shallow networks using a notion of error that uses a carefully defined distribution. (Only Liang & Srikant (2017) is distribution-independent as it deals with uniform approximation everywhere in the domain). Thus these results do not tell us the extent to which deeper networks are more expressive than the shallow ones. We would like to understand whether there are large classes of functions and distributions that witness the separation between deep and shallow networks. An answer to this question is also more likely to shed light on practical applications of neural networks. Shamir (2016); Shalev-Shwartz et al. (2017); Song et al. (2017) show that even functions computed by a depth-2 neural network of polynomial size can be hard to learn using gradient descent type of algorithms for a wide class of distributions. These results address questions about learnability rather than the expressive power of deep neural networks.
Hanin (2017) shows that piecewise affine functions on [0, 1]d with N pieces can be exactly represented by a width (d + 3) network of depth at most N . Lower bound of ((N + d - 1)/(d + 1)) on the depth is proven for functions of the above type when the network has width at most (d + 1) and very closely approximates the function.
Our depth separation results apply to neural networks with bounds on the magnitudes of the weights. While we would prefer to prove our results without any weight restrictions, we now argue that small weights are natural. In training neural networks, often weights are not allowed to be too large to avoid overfitting. Weight decay is a commonly used regularization heuristic in deep learning to control the weights. Early stopping can also achieve this effect. Another motivation to keep the weights low is to keep the Lipschitz constant of the function computed by the network (w.r.t. changes in the input, while keeping the network parameters fixed) small. Goodfellow et al. (2016) contains many of these references. One of the surprising discoveries about neural networks has been the existence of adversarial examples (Szegedy et al. (2013)). These are examples obtained by adding a tiny perturbation to input from class so that the resulting input is misclassified by the network. The perturbations are imperceptible to humans. Existence of such examples for a network suggests that the Lipschitz constant of the network is high as noted in Szegedy et al. (2013). This lead them to suggest regularizing training of neural nets by penalizing high Lipschitz constant to improve the generalization error and, in particular, eliminate adversarial examples. This is carried out in Cisse´ et al. (2017), who find a way to control the Lipschitz constant by enforcing an orthonormality constraint on the weight matrices along with other tricks. They report better resilience to adversarial examples. On the other hand, Neyshabur et al. (2017) suggest that Lipschitz constant cannot tell the full story about generalization.
2

Under review as a conference paper at ICLR 2018

2 OUR RESULTS
We exhibit a simple function (derived from Daniely (2017)) over the unit ball Bd in d-dimensions can be well-approximated by a depth-3 sigmoidal neural network with size and weights polynomially bounded in d. However, its any reasonable approximation using a depth-2 sigmoidal neural network with polynomially bounded weights must have size exponentially large in d.
Our separation is robust and works for a general class of input distributions, as long as their density is at least 1/poly(d) on some small ball of radius 1/poly(d) in Bd. The function we use can also be replaced by many other functions that are polynomially-Lipschitz but not close to any low-degree polynomial.
As a by-product of our argument, we also show that constant-depth sigmoidal neural networks are well-approximated by low-degree multivariate polynomials (with a degree bound that allows the depth separation mentioned above).

3 POLYNOMIAL APPROXIMATIONS TO SIGMOIDAL NEURAL NETWORKS

In this section, we show that a sigmoid neuron can be well-approximated by a low-degree polynomial. As a corollary, we show that depth-2 (and in genenral, small-depth) sigmoidal neural networks can be well-approximated by low-degree multivariate polynomials. The main idea is to use Chebyshev polynomial approximation as in Shalev-Shwartz et al. (2011), which closely approximates the minimax polynomial (or the polynomial that has the smallest maximum deviation) to a given function. For the simplicity of presentation and arguments, we drop the bias term b in the activation function ( w, x + b). This is without loss of generality, as explained at the end of the last section.

3.1 POLYNOMIAL APPROXIMATION TO A SIGMOID NEURON The activation function of a sigmoid neuron  : R  R is defined as
1 (t) = 1 + exp(-t) .

Chebyshev polynomials of the first kind {Tj(t)}j0 are defined recursively as T0(t) = 1, T1(t) = t,

and Tj+1(t) = 2t · Tj(t) - Tj-1(t). They form an orthonormal basis of polynomials over [-1, 1]

with respect to the density

 j=0

cj

Tj

(t)

over

[-1,

1]

1/ 1 - t2. is given by

The

coefficient

cj

in

the

Chebyshev

expansion

of

(wt)

=

1 + 1(j > 0)

cj =



1 (wt) Tj(t) dt. -1 1 - t2

Proposition 1 (see Lemma B.1 in Shalev-Shwartz et al. (2011)) bounds the magnitude of coefficients

cj in the Chebyshev expansion of (wt) =

 j=0

cj

Tj

(t).

Proposition 1. For any j > 1, the coefficient cj in the Chebyshev expansion of a sigmoid neuron

(wt) is bounded by

|cj| 

42 |w| + 

 -j 1+ .
|w|

Proposition 1 implies low-degree polynomial approximation to sigmoid neurons as follows. This observation appeared in Shalev-Shwartz et al. (2011) (see equation (B.7) in their paper). For completeness, we give the proof in Appendix A.
Proposition 2. Given any w  R with |w|  B, there exists a polynomial p of degree O (B log (B/ )) such that |(wt) - p(t)|  , for all t  [-1, 1].

We use this O (log(1/ )) dependence in the above bound crucially in some of our results, e.g., a weaker version of Daniely's separation result for depth-2 and depth-3 neural networks. Notice that this logarithmic dependence does not hold for a ReLU neuron; it is O(1/ ) instead.

3

Under review as a conference paper at ICLR 2018

3.2 POLYNOMIALS APPROXIMATIONS TO SMALL-DEPTH NEURAL NETWORKS
A depth-2 sigmoidal neural network on input t  [-1, 1] computes a linear combination of sigmoidal neurons (w1t), (w2t), . . . , (wnt), for w1, w2, . . . , wn  R, and computes a function f : [-1, 1]  R given by
n
f (t) = ai(wit)
i=1
Now we show that a depth-2 sigmoidal neural network of bounded weights and width is close to a low-degree polynomial.
Proposition 3. Let f : [-1, 1]  R be a function computed by a depth-2 sigmoidal neural network of width n and weights bounded by B. Then f is -approximated (in L-norm) over [-1, 1] by a polynomial of degree O B log nB2/ .

Proof. Let f be computed by a depth-2 sigmoidal neural network given by f (t) =

n i=1

ai

(wit).

Define a parameter = /nB. Proposition 2 guarantees polynomial p1, p2, . . . , pn of degree

O (B log(B/ )) such that |(wit) - pi(t)|  , for all t  [-1, 1]. Thus, the polynomial

p(t) =

n i=1

aipi(t)

has

degree

O (B

log(B/

))

=

O

B log(nB2/) , and for any t  [-1, 1],

n nn
ai(wit) - p(t) = ai(wit) - aipi(t)
i=1 i=1 i=1 n
 |ai| |(wit) - pi(t)|
i=1
 nB
=

using |ai|  B using = /nB.

Now consider a depth-2 sigmoidal neural network on input x  Bd, where Bd = {x  Rd : x  1}. It is given by a linear combination of sigmoidal activations applied to linear functions w1, x , w2, x , . . . , wn, x (or affine functions when we have biases), for w1, w2, . . . , wn  Rd and it computes a function F : Bd  R given by
n
F (x) = ai( wi, x )
i=1
Now we show Proposition 4, which is a multivariate version of Proposition 3.
Proposition 4. Let F : Bd  R be a function computed by a depth-2 sigmoidal neural network with width n and bounded weights, that is, |ai|  B and wi  B, for 1  i  n. Then F is approximated (in L-norm) over Bd by a polynomial of degree O B log nB2/ in d variables given by the coordinates x = (x1, x2, . . . , xd).

Proof. where

Let |ai|

F 

be B

computed and wi

by 

a depth-2 B, for 1

neural network given by  i  n. Thus, F (x)

F =

(x) =
n
i=1

n i=1

ai

ai( wi

( wi, x ), ti), where

ti = wi/ wi , x  [-1, 1] because wi/ wi  Bd, for 1  i  n, and x  Bd.

Define a parameter = /nB. Proposition 2 guarantees polynomial p1, p2, . . . , pn of degree

O (B log(B/ )) such that |( wi t) - pi(t)|  , for all t  [-1, 1]. Consider the following

polynomial P (x) = P (x1, x2, . . . , xd) =

n i=1

aipi(

wi/

wi

, x ).

P (x) is a d-variate poly-

nomial of degree O (B log(B/ )) = O B log(nB2/) in each variable x1, x2, . . . , xd. For any

4

Under review as a conference paper at ICLR 2018

x  Bd,

nn

|F (x) - P (x)| = ai( wi, x ) - aipi( wi/ wi , x )

i=1 i=1

n

 |ai| |( wi, x ) - pi( wi/ wi , x )|

i=1

n

 B |( wi ti) - pi(ti)|

using ti = wi/ wi , x and |ai|  B

i=1

 nB

using |( wi t) - pi(t)|  , for all t  [-1, 1]

= .

Note that the above proof crucially uses the fact that Proposition 2 guarantees a low-degree polynomial that approximates a sigmoid neuron everywhere in [-1, 1].
A depth-k sigmoidal neural network can be thought of as a composition ­ a depth-2 sigmoidal neural network on top, whose each input variable is a sigmoid applied to a depth-(k - 2) sigmoidal neural network. In other words, it computes a function F : Bd  R given by
n
F (x) = ai ( wi, y ) ,
i=1
where y = (y1, y2, . . . , ym) has each coordinate yj = (Fj(x)), for 1  j  m, such that each Fi : Bd  R is a function computed by a depth-(k - 2) sigmoidal neural network.
Now we show an interesting consequence of our proof technique, namely, any constant-depth sigmoidal neural network with polynomial width and polynomially bounded weights can be wellapproximated by a low-degree multivariate polynomial. The bounds presented in Proposition 5 are not optimal but the qualitative statement is interesting in contrast with the depth separation result. The growth of the degree of polynomial approximation is dependent on the widths of hidden layers and it is also the subtle reason why a depth separation result is still possible (when the weights are bounded). Proposition 5. Let F : Bd  R be a function computed by a depth-k sigmoidal neural network of width at most n in each layer and weights bounded by B, then F (x) can be -approximated (in L-norm) over Bd by a d-variate polynomial of degree O (nB)k logk (nB/) in each coordinate variable of x = (x1, x2, . . . , xd).
Note that when n and B are polynomial in d and the depth k is constant, then this low-degree polynomial approximation also has degree polynomial in d.
Proof. We prove this by induction on the depth k. By induction hypothesis each Fj(x) can be 1-approximated (in L-norm) by a d-variate polynomial Qj(x) of degree O (nB)k-2 log(k-2)(nB/ 1) in each variable. Thus, |Fj(x) - Qj(x)| = 1, for any x  Bd and 1  j  m. Because a sigmoid neuron is Lipschitz,
|yj - (Qj(x))| = |(Fj(x)) - (Qj(x))|  |Fj(x) - Qj(x)|  1,
for any x  Bd and 1  j  m.
Since Fj(x) is the output of a depth-(k-2) sigmoidal neural network of width at most n and weights at most B, we must have |Fj(x)|  nB, for all x  Bd. Thus, |Qj(x)|  nB + 1  2nB. By Proposition 2, there exists a polynomial q(t) of degree at most O (nB log(nB/ 2)) such that
|(Qj(x)) - q(Qj(x))|  2,
for all x  Bd and 1  j  m.
5

Under review as a conference paper at ICLR 2018

Consider q  Rm as q = (q(Q1(x)), q(Q2(x)), . . . , q(Qm(x))). Then, for any x  Bd, we have

| wi, y - wi, q | = | wi, y - q |

 wi y - q


m

1/2

 B  (yj - q(Qj(x)))2

j=1





B m( 

1

+

2)

 B n ( 1 + 2) .

Again by Proposition 2, there is a polynomial p of degree at most O (nB log(nB/ )) such that |( wi, q ) - p( wi, q )|  , for all x  Bd and 1  i  n. This is because | wi, q | = O(nB).

Let's define P (x) =

n i=1

aip(

wi, q

).

Therefore,

for

any

x



Bd,

nn
|F (x) - P (x)| = ai( wi, y ) - aip( wi, q )
i=1 i=1 n
 |ai| |( wi, y ) - p( wi, q )|
i=1 n
 |ai| (|( wi, y ) - ( wi, q )| + |( wi, q ) - p( wi, q )|)
i=1 n
 |ai| (| wi, y - wi, q | + ) i=1 
 nB B n( 1 + 2) +
 ,

if we use 1 = 2 = /3n3/2B2 and = /3nB. P (x) is a d-variate polynomial of degree

deg(P )  deg(p)deg(q) · deg(Qj) = O (nB)k logk (nB/) ,

in each variable.

4 DEPTH-2 VS. DEPTH-3 SEPARATION FOR GENERAL INPUT DISTRIBUTIONS
Daniely shows that if g : [-1, 1]  R cannot be approximated by a polynomial of degree O(d2), then G : Sd-1 × Sd-1  R defined as G(x, y) = g( x, y ) cannot be approximated by any depth2 neural network of polynomial size and (exponentially) bounded weights. Daniely shows this lower bound for a general neuron or activation function that includes sigmoids and ReLUs. Daniely then uses G(x, y) = g( x, y ) = sin(d3 x, y ) which, on the other hand, is approximable by a depth-3 ReLU neural network with polynomial size and polynomially bounded weights. This gives a separation between depth-2 and depth-3 ReLU neural networks w.r.t. uniform distribution over Sd-1 × Sd-1. Daniely's proof uses harmonic analysis on the unit sphere, and requires the uniform distribution on Sd-1 × Sd-1 in a crucial way.
We show a simple proof of separation between depth-2 and depth-3 sigmoidal neural networks that compute functions F : Bd  R. Our proof works for a large class of distributions on Bd but requires the weights to be polynomially bounded.
The following lemma appears in Debao (1993). Assumption 1 in Eldan & Shamir (2016) and their version of this lemma for ReLU networks was used by Daniely (2017) in the proof of separation between the expressive power of depth-2 and depth-3 ReLU networks.

6

Under review as a conference paper at ICLR 2018
Lemma 6. Let f : [-1, 1]  R be any L-Lipschitz function. Then there exists a function g : [-1, 1]  R computed by a depth-2 sigmoidal neural network such that
n
g(t) = f (0) + ai(wit + bi),
i=1
the width n as well as the weights are bounded by poly(L, 1/ ), and |f (t) - g(t)|  , for all t  [-1, 1].
Now we are ready to show the separation between depth-2 and depth-3 sigmoidal neural networks. The main idea, similar to Daniely (2017), is to exhibit a function that is Lipschitz but far from any low-degree polynomial. The Lipschitz property helps in showing that our function can be wellapproximated by a depth-3 neural network of small size and small weights. However, being far from any low-degree polynomial, it cannot be approximated by any depth-2 neural network. Theorem 7. Consider the function G : Bd  R given by G(x) = sin(d5 x 2). Then G can be -approximated (in L-norm) by a depth-3 sigmoidal neural network of width and weights polynomially bounded in d. However, any function F : Bd  R computed by a depth-2 sigmoidal neural network with weights O(d2) cannot -approximate G even when its width n is 2O(d).
By modifying the function to G(x) = sin(N x 2), this lower bound with L-norm holds for any distribution over Bd whose support contains a radial line segment of length at least 1/poly(d), by making N = poly(d), for a large enough polynomial.
Remark: Given any distribution µ over Bd whose probability density is at least 1/poly(d) on some small ball of radius 1/poly(d), the lower bound or inapproximability by any depth-2 sigmoidal neural network can be made to work with L2-norm (squared error), for a large enough N = poly(d).
Proof. First, we will show that G(x) can be well-approximated by a depth-3 sigmoidal neural network of polynomial size and weights. The idea is similar to Daniely's construction for ReLU networks in Daniely (2017). By Lemma 6, there exists a function f : [-1, 1]  R computed by a depth-2 sigmoidal neural network of size and weights bounded by poly(d, 1/ ) such that t2 - f (t)  /10d6, for all t  [-1, 1]. Thus, we can compute xi2 for each coordinate of x and add them up to get an -approximation to x 2 over Bd. That is, there exists a function S : Bd  R computed by a depth-2 sigmoidal neural network of size and weights bounded by poly(d, 1/ ) such that S(x) - x 2  /10d5, for all x  Bd. Again, by Lemma 6, we can approximate sin(d3t) over [0, 1] using f : [-1, 1]  R computed by another depth-2 sigmoidal neural network with size and weights bounded by poly(d, 1/ ) such that sin(d3t) - f (t)  /2, for all t  [0, 1]. Note that the composition of these two depth-2 neural networks f (N (x)) gives a depth-3 neural network as the output of the hidden layer of the bottom network can be fed into the top network as inputs.
|G(x) - f (S(x))| = sin(d5 x 2) - f (S(x))
 sin(d5 x 2) - f ( x 2) + f ( x 2) - f (S(x))
 /2 + 4d5 x 2 - S(x)
using that f that approximates sin(d5t) closely must also be 4d5-Lipschitz  /2 + 4d5 · /10d5  .
Now we will show the lower bound. Consider any function F : Bd  R computed by a depth-2 sigmoidal neural network whose weights are bounded by B = O(d2) and width is n. Proposition 4 shows that there exists a d-variate polynomial P (x) of degree O B log(nB2/) = O d2 log(n/) + d2 log d in each variable such that |F (x) - P (x)|  , for all x  Bd. Let µ be any measure on Bd whose support contains some radial line segment of length at least 1/poly(d) in Bd. In other words, there exists a unit vector u such that the support of µ intersects the radial set {x  Bd : x = tu, for some t  [-1, 1]} in some line segment of length at least 1/poly(d). Then P (tu) is a univariate polynomial of degree O(d3 log(n/) + d3 log d) that approximates F (tu), for all t  [-1, 1]. By Lemma 8, using D = O(d3 log(n/) + d3 log d),
7

Under review as a conference paper at ICLR 2018
l = 1/poly(d) and N = d5/l, we get that if n = 2O(d), then there exists a t0  [-1, 1] such that sin(N t02) - P (tu)  1. Therefore, by triangle inequality, sin(N t0u 2) - F (t0u)  sin(N t02) - P (t0u) - |P (t0u) - F (t0u)|  1 -  > , for  < 1/2. This means that G(x) cannot be well-approximated by any F (x) computed by a depth-2 neural network with polynomially bounded weights even when it has width 2O(d).
Now we show that the candidate function proposed by Daniely g(t) = sin(N t), for large enough N , is far from any low-degree polynomial w.r.t. any measure µ on [-1, 1] with a reasonable support. Lemma 8. Let p be any polynomial of degree D and µ be any measure on [-1, 1] whose support contains an interval of length at least l. Then, for N large enough to satisfy N l > D + 3, there exists t0  [-1, 1] such that µ(t0) > 0 and |sin(N t0) - p(t0)| > 1. In other words, sin(N t) is 1-far (in L-norm) from any polynomial of degree D over interval [-1, 1] with measure µ.
Proof. Let µ(t) > 0 for some interval [a, a + l]  [-1, 1]. Consider S = {t  [a, a + l] : t = -1 + (i + 1/2)/N, for some integer i}. Then S contains at least N l - 2 points where sin(N t) alternates as ±1. Any polynomial p of degree D cannot match the sign of sin(N t) on all the points in S. Otherwise, by intermediate value theorem, p must have at least N l - 3 roots between the points of S, which means D  N l - 3, a contradiction. Thus, there exists t0  S such that p(t0) and sin(N t0) have opposite signs. Since sin(N t) = ±1, for any t  S, the sign mismatch implies |sin(N t0) - p(t0)| > 1.
An important remark on biases: Even though we handled the case of sigmoid neurons without biases, the proof technique carries over the sigmoid neurons with biases ( w, x + b). The idea is to consider a new (d+1)-dimensional input xnew = (x, xd+1) = (x1, x2, . . . , xd+1) with xd+1 = 1, and consider the new weight vector wnew = (w, b). Thus, wnew, xnew = w, x + b. The new input lies on a d-dimensional hyperplane slice of Bd+1, so we need to look at the restriction of the input distribution µ to this slice. Most of the ideas in our proofs generalize without any technical modifications. We defer the details to the full version.
REFERENCES
Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115­133, Jan 1994. ISSN 1573-0565. doi: 10.1007/BF00993164. URL https: //doi.org/10.1007/BF00993164.
Moustapha Cisse´, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 854­863, 2017. URL http://proceedings.mlr.press/v70/cisse17a.html.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303­314, Dec 1989. ISSN 1435-568X. doi: 10.1007/BF02551274. URL https://doi.org/10.1007/BF02551274.
Amit Daniely. Depth separation for neural networks. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pp. 690­696, 2017. URL http://proceedings.mlr.press/v65/daniely17a.html.
Chen Debao. Degree of approximation by superpositions of a sigmoidal function. Approximation Theory and its Applications, 9(3):17­28, Sep 1993. ISSN 1573-8175. doi: 10.1007/BF02836480. URL https://doi.org/10.1007/BF02836480.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pp. 907­940, 2016. URL http://jmlr.org/proceedings/papers/v49/eldan16. html.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
8

Under review as a conference paper at ICLR 2018
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. CoRR, abs/1708.02691, 2017. URL http://arxiv.org/abs/1708.02691.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770­778, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359 ­ 366, 1989. ISSN 0893-6080. doi: https://doi.org/10.1016/0893-6080(89)90020-8. URL http://www.sciencedirect. com/science/article/pii/0893608089900208.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436­444, 2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.
Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? In ICLR, 2017.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted boltzmann machines. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 2877­2885. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/ 5020-on-the-representational-efficiency-of-restricted-boltzmann-machines. pdf.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. CoRR, abs/1706.08947, 2017. URL http://arxiv.org/abs/ 1706.08947.
Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2979­2987, 2017. URL http://proceedings.mlr.press/v70/safran17a.html.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM J. Comput., 40(6):1623­1646, December 2011. ISSN 0097-5397. doi: 10. 1137/100806126. URL http://dx.doi.org/10.1137/100806126.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3067­3075, 2017. URL http://proceedings. mlr.press/v70/shalev-shwartz17a.html.
Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037, 2016. URL http://arxiv.org/abs/1609.01037.
Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks. CoRR, abs/1707.04615, 2017. URL http://arxiv.org/abs/1707.04615.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http://arxiv.org/abs/1312.6199.
Matus Telgarsky. benefits of depth in neural networks. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pp. 1517­1539, 2016. URL http://jmlr.org/proceedings/papers/v49/telgarsky16.html.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. CoRR, abs/1610.01145, 2016. URL http://arxiv.org/abs/1610.01145.
9

Under review as a conference paper at ICLR 2018

A PROOF OF PROPOSITION 2
Proof. Consider the degree-D approximation to (wt) given by the first D terms in its Chebyshev expansion. The error of this approximation for any t  [-1, 1] is bounded by

|(wt) - p(t)| =

cj Tj (t)

j>D

 |cj|
j>D



42 |w| + 



42 |w| + 

42 = |w| +  ,

 -j 1+
|w|
j>D

 -(D+1) 

 -j

1 + |w|

1 + |w|

j=0

 1 + |w|

-(D+1) · |w| 

 1 + |w|

using Proposition 1, |w|  B, and D = O (B log (B/ )).

10

