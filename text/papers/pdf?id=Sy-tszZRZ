Under review as a conference paper at ICLR 2018
BOUNDING AND COUNTING LINEAR REGIONS OF DEEP NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Montu´far et al. (2014) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.
1 INTRODUCTION
We have witnessed an unprecedented success of deep learning algorithms in computer vision, speech, and other domains (Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). While the popular deep learning architectures such as AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2015), and residual networks (He et al., 2016) have shown record beating performance on various image recognition tasks, empirical results still govern the design of network architecture in terms of depth and activation functions. Two important practical considerations that are part of most successful architectures are greater depth and the use of PWL activation functions such as rectified linear units (ReLUs). Due to the large gap between theory and practice, many researchers have been looking at the theoretical modeling of the representational power of DNNs (Cybenko, 1989; Anthony & Bartlett, 1999; Pascanu et al., 2013; Montu´far et al., 2014; Bianchini & Scarselli, 2014; Eldan & Shamir, 2015; Telgarsky, 2015; Mhaskar et al., 2016; Raghu et al., 2017).
Any continuous function can be approximated to arbitrary accuracy using a single hidden layer of sigmoid activation functions (Cybenko, 1989). This does not imply that shallow networks are sufficient to model all problems in practice. Typically, shallow networks require exponentially more number of neurons to model functions that can be modeled using much fewer activation functions in deeper ones (Delalleau & Bengio, 2011). There have been a wide variety of activation functions such as threshold (f (z) = (z > 0)), logistic (f (z) = 1/(1 + exp(-e))), hyperbolic tangent (f (z) = tanh(z)), rectified linear units (ReLUs f (z) = max(0, z)), and maxouts (f (z1, z2, . . . , zk) = max(z1, z2, . . . , zk)). The activation functions offer different modeling capabilities. For example, sigmoid networks are shown to be more expressive than similar-sized threshold networks (Maass et al., 1994). It was recently shown that ReLUs are more expressive than similar-sized threshold networks by deriving transformations from one network to another (Pan & Srikumar, 2016).
The complexity of neural networks belonging to the family of PWL functions can be analyzed by looking at how the network can partition the input space to an exponential number of linear response regions (Pascanu et al., 2013; Montu´far et al., 2014). The basic idea of a PWL function is simple: we can divide the input space into several regions and we have individual linear functions for each of these regions. Functions partitioning the input space to a larger number of linear regions are considered to be more complex ones, or in other words, possess superior representational power. In the
1

Under review as a conference paper at ICLR 2018
case of ReLUs, it was shown that deep networks separate their input space into exponentially more linear response regions than their shallow counterparts despite using the same number of activation functions (Pascanu et al., 2013). The results were later extended and improved (Montu´far et al., 2014; Raghu et al., 2017). In particular, Montu´far et al. (2014) shows both upper and lower bounds on the maximal number of linear regions for a ReLU DNN and a single layer maxout network, and a lower bound for a maxout DNN. Furthermore, Raghu et al. (2017) improves the upper bound for a ReLU DNN. This upper bound asymptotically matches the lower bound from Montu´far et al. (2014) when the number of layers and input dimension are constant and all layers have the same width.
In this work, we directly improve on the results of Montu´far et al. (Pascanu et al., 2013; Montu´far et al., 2014) and Raghu et al. (Raghu et al., 2017) in better understanding the representational power of DNNs employing PWL activation functions.
2 NOTATIONS AND BACKGROUND
We will only consider feedforward neural networks in this paper. Let us assume that the network has n0 input variables given by x = {x1, x2, . . . , xn}, and m output variables given by y = {y1, y2, . . . , ym}. Each of the hidden layer l = {1, 2, . . . , L} has nl hidden neurons whose activations are given by hl = {hl1, hl2, . . . , hlnl }. Let W l be the nl × nl-1 matrix where each row corresponds to the weights of a neuron of layer l. Let bl be the bias vector used to obtain the activation functions of neurons in layer l. We will assume that the network is already trained and all the weights W l's and biases bl's are known. Based on the ReLU(x) = max(0, x) activation function, the activations of the hidden neurons and the outputs are given below:
h1 = max(0, W 1x + b1) hl = max(0, W lhl-1 + bl) y = W L+1hL As considered in Pascanu et al. (2013), the output layer is a linear layer that computes the linear combination of the activations from the previous layer without any ReLUs.
We can treat the DNN as a PWL function F : Rn0  Rm that maps the input x in Rn0 to y in Rm. This paper primarily deals with investigating the bounds on the linear regions of this PWL function. There are two subtly different definitions for linear regions in the literature and we will formally define them. Definition 1. Given a PWL function F : Rn0  Rm, a linear region is defined as a maximal connected subset of the input space Rn0 , on which F is linear (Pascanu et al., 2013; Montu´far et al., 2014).
Activation Pattern: Let us consider an input vector x = {x1, x2, . . . , xn}. For every layer l we define a set Sl  {1, 2, . . . , nl} such that e  Sl if and only if the ReLU e is active, that is, hel > 0. We aggregate these Sl into S = (S1, . . . , Sl ), which we call an activation pattern. Note that we may consider activation patterns up to a layer l  L. Activation patterns were previously defined in terms of strings (Raghu et al., 2017).
We say that an input x corresponds to an activation pattern S in a DNN if feeding x to the DNN results in the activations in S. Definition 2. Given a PWL function F : Rn0  Rm represented by a DNN, a linear region is the set of input vectors x that corresponds to an activation pattern S in the DNN.
We prefer to look at linear regions as activation patterns. Definitions 1 and 2 are essentially the same, except in a few degenerate cases. There could be scenarios where two different activation patterns may correspond to two adjacent regions with the same linear function. In this case, Definition 1 will produce only one linear region whereas Definition 2 will yield two linear regions. This has no effect on the bounds that we derive in this paper.
In Fig. 1(a) we show a simple ReLU DNN with two inputs {x1, x2} and 3 hidden layers.
The activation units {a, b, c, d, e, f } in the hidden layers can be thought of as hyperplanes that each divide the space in two. On one side of the hyperplane, the unit outputs a positive value. For all points on the other side of the hyperplane including itself, the unit outputs 0.
2

Under review as a conference paper at ICLR 2018

Figure 1: (a) Simple DNN with two inputs and three hidden layers with 2 activation units each. (b), (c), and (d) Visualization of the hyperplanes from the first, second, and third hidden layers respectively partitioning the input space into several linear regions. The arrows indicate the directions in which the corresponding neurons are activated. (e), (f), and (g) Visualization of the hyperplanes from the first, second, and third hidden layers in the space given by the outputs of their respective previous layers.

One may wonder: into how many regions do n hyperplanes split a space? Zaslavsky (1975) shows

that an arrangement of n hyperplanes divides a d-dimensional space into at most

d s=0

n s

regions,

a bound that is attained when they are in general position. This corresponds to the exact maximal

number of regions of a single layer DNN with n ReLUs and input dimension d.

In Figs. 1(b)­(g), we provide a visualization of how ReLUs partition the input space. Figs. 1(e),

(f), and (g) show the hyperplanes corresponding to the ReLUs at layers l = 1, 2, and 3 respectively.

Figs. 1(b), (c), and (d) consider these same hyperplanes in the input space x. In Fig. 1(b), as per

Zaslavsky (1975), the 2D input space is partitioned into 4 regions (

2 0

+

2 1

+

2 2

= 4). In Figs. 1(c)

and (d), we add the hyperplanes from the second and third layers respectively, which are affected

by the transformations applied in the earlier hidden layers. The regions are further partitioned as we

consider additional layers.

Fig. 1 also highlights that activation boundaries behave like hyperplanes when inside a region and may bend whenever they intersect with a boundary from a previous layer. This has also been pointed out by Raghu et al. (2017). In particular, they cannot appear twice in the same region as they are defined by a single hyperplane if we fix the region. Moreover, these boundaries do not need to be connected, as illustrated in Fig. 2.

Main Contributions

We summarize the main contributions of this paper below:

· We achieve tighter upper and lower bounds on the number of linear regions of the PWL function corresponding to a DNN that employs ReLUs. We additionally provide an upper bound on the number of linear regions for maxout DNNS (See Sections 3 and 4).
· We show a mixed-integer linear formulation that maps the input space to output. The formulation is shown for both ReLU and maxout networks (See Section 5).
· Using the mixed-integer linear formulation we show that exact counting of the linear regions is indeed possible. For the first time, we show the exact counting of the number of linear regions for several small-sized DNNs during the training process. This new capa-

3

Under review as a conference paper at ICLR 2018

Figure 2: (a) A network with one input x1 and three activation units a, b, and c. (b) We show the hyperplanes x1 = 0 and -x1 + 1 = 0 corresponding to the two activation units in the first hidden layer. In other words, the activation units are given by ha = max(0, x1) and hb = max(0, -x1 +1). (c) The activation unit in the third layer is given by hc = max(0, 4ha + 2hb - 3). (d) The activation
boundary for neuron c is disconnected.

bility allows us to visualize the correlation between validation accuracy and the number of linear regions. It also provides new insights as to how the linear regions vary during the training process (See Section 6).

3 TIGHTER BOUNDS FOR RECTIFIER NETWORKS

Montu´far et al. (2014) derives an upper bound of 2N for N hidden units, which can be obtained

by mapping linear regions to activation patterns. Raghu et al. (2017) improves this result by

deriving an asymptotic upper bound of O(nLn0 ) to the maximal number of regions, assuming

nl = n for all layers l and n0 = O(1). Moreover, Montu´far et al. (2014) prove a lower bound

of

L-1 l=1

nl/n0

n0

n0 nL j=0 j

when n  n0, or asymptotically ((n/n0)(L-1)n0 nn0 ). We

derive both upper and lower bounds that improve upon these previous results.

3.1 AN UPPER BOUND ON THE NUMBER OF LINEAR REGIONS

In this section, we prove the following upper bound on the number of regions.
Theorem 1. Consider a deep rectifier network with L layers, nl rectified linear units at each layer l, and an input of dimension n0. The maximal number of regions of this neural network is at most

L dl l=1 j=0

nl j

where dl = min{n0, n1, . . . , nl}.

Although this bound is asymptotically equivalent to the one given by Raghu et al. (2017), O(nLn0 ), this expression allows for tighter bounds when layers have different widths. More importantly, it reveals a bottleneck-like behavior on the maximal number of linear regions: reducing the width of a layer at the beginning of the network impacts the maximal number of regions substantially more than doing so at the end.
For a given activation set Sl and a matrix W with nl rows, let Sl (W ) be the operation that zeroes out the rows of W that are inactive according to Sl. This represents the effect of the ReLUs. Finally, for an activation pattern S up to layer l - 1, define W¯ Sl := W l Sl-1 (W l-1) · · · S1 (W 1).
Each region S viewed in the input space may contain a set of hyperplanes defined by the neurons of layer l. These hyperplanes are the rows of W¯ Sl x + b = 0 for some b. To verify this, observe

4

Under review as a conference paper at ICLR 2018

that if we recursively substitute out the hidden variables hl-1, . . . , h1 from the original hyperplane W lhl-1 + bl = 0 following S, the resulting weight matrix applied to x is W¯ Sl .
The proof of Theorem 1 focuses on the rank of W¯ Sl at each layer l. A key observation is that once it falls to a certain value, it cannot recover in subsequent layers.

Zaslavsky (1975) showed that the maximal number of regions in Rd induced by an arrangement of m

hyperplanes is at most

d j=0

m j

.

Moreover, this value is attained if and only if the hyperplanes are

in general position. The lemma below tightens this bound for a special case where the hyperplanes

may not be in general position.

Lemma 2. Consider m hyperplanes in Rd defined by the rows of W x + b = 0. Then the number of

regions induced by the hyperplanes is at most

rank(W ) j=0

m j

.

The proof is given in Appendix A. The next lemma simply brings Lemma 2 into our context.

Lemma 3. The number of regions induced by the nl neurons at layer l within a certain region S is

at most

rl j=0

nl j

, where rl

= rank(W¯ Sl ).

Appendix B gives the proof. A final ingredient to Theorem 1 is a bound on the ranks of the weight matrices, which allows us to express it in terms of the sizes of the input and the neural network.
Lemma 4.
rank(W¯ Sl )  min{n0, n1, . . . , nl}

The proof can be found in Appendix C. Using the three previous lemmas, we prove the upper bound stated in Theorem 1 in Appendix D.
Lemma 4 relaxes the bound substantially in order to express it in terms of the input dimension n0 and network sizes nl. There are insights we can extract from it if we do not relax the bound as much. For instance, we may also define dl in terms of the ranks of the weight matrices: dl = min{n0, rank(W 1), . . . , rank(W l)}. This gives us a tighter bound if the minimum rank is sufficiently low.
It is convenient from now on to define the dimension of a region S as dim(S) := rank(Sl-1 (W l-1) · · · S1 (W 1)). This allows us to express a variant of Lemma 4 as rank(W¯ Sl )  min{rank(W l), dim(S)}. A key insight from these results is that the dimensions of the regions are non-increasing as we move through the layers partitioning it. In other words, if at any layer the dimension of a region becomes small, then that region will not be able to be further partitioned into a large number of regions. For instance, if the dimension of a region falls to zero, then that region will never be further partitioned. This suggests that if we want to have many regions, we need to keep dimensions high. We use this idea in the next section to construct a DNN with many regions.

3.2 THE CASE OF DIMENSION ONE
If the input dimension n0 is equal to 1 and nl = n for all layers l, the upper bound presented in the previous section reduces to (n + 1)L. On the other hand, the lower bound given by Montu´far et al. (2014) becomes nL-1(n + 1). It is then natural to ask: are either of these bounds tight? The answer is that the upper bound is tight in the case of n0 = 1, assuming there are sufficiently many neurons.
Theorem 5. Consider a deep rectifier network with L layers, nl  3 rectified linear units at each layer l, and an input of dimension 1. The maximal number of regions of this neural network is exactly lL=1(nl + 1).
The expression above is a simplified form of the upper bound from Theorem 1 in the case n0 = 1.
The proof of this theorem, given in Appendix E, provides a construction with n + 1 regions that replicate themselves as we add layers, instead of n as in Montu´far et al. (2014). This construction is motivated by an insight from the previous section: in order to obtain many regions, we want every region to have dimension as high as possible. In the case of n0 = 1, we want all regions to have dimension one. This intuition leads to a new construction with one additional region that can be replicated along with the other ones.

5

Under review as a conference paper at ICLR 2018

3.3 A LOWER BOUND ON THE MAXIMAL NUMBER OF LINEAR REGIONS

The lower bound from Montu´far et al. (2014) can be slightly improved, since their approach is based on extending a 1-dimensional construction similar to the one in Section 3.2.

Theorem 6. The maximal number of linear regions induced by a rectifier network with n0 input units and L hidden layers with nl  3n0 for all l is lower bounded by

L-1 l=1

nl

n0
+1

n0

nL .

n0

j
j=0

The proof of this theorem is in Appendix F. For comparison, the differences between the lower bound theorem (Theorem 5) from Montu´far et al. (2014) and the above theorem is the replacement of the condition nl  n0 by the more restrictive nl  3n0, and of nl/n0 by nl/n0 + 1.

4 AN UPPER BOUND ON THE NUMBER OF LINEAR REGIONS FOR MAXOUT
NETWORKS

We now consider a deep neural network composed of maxout units. Given weights Wjl for j = 1, . . . , k, the output of a rank-k maxout layer l is given by
hl = max(W1lhl-1 + bl1, . . . , Wkl hl-1 + bkl )

Using techniques similar to the ones from Section 3.1, the following theorem can be shown (see Appendix G for the proof).

Theorem 7. Consider a deep neural network with L layers, nl rank-k maxout units at each layer l, and an input of dimension n0. The maximal number of regions of this neural network is at most

L dl l=1 j=0

k(k-1) 2

nl

j

where dl = min{n0, n1, . . . , nl}.

Asymptotically, if nl = n for all l = 1, . . . , L, n  n0, and n0 = O(1), then the maximal number of regions is at most O((k2n)Ln0 ).

5 EXACT COUNTING OF LINEAR REGIONS
If the input space x  Rn0 is bounded by minimum and maximum values along each dimension, or else if x corresponds to a polytope more generally, then we can define a mixed-integer linear formulation mapping polyhedral regions of x to the output space y  Rm. The assumption that x is bounded and polyhedral is natural in most applications, where each value xi has known lower and upper bounds (e.g., the value can vary from 0 to 1 for image pixels). Among other things, we can use this formulation to count the number of linear regions.
In the formulation that follows, we use continuous variables to represent the input x, which we can also denote as h0, the output of each neuron i in layer l as hli, and the output y as hL+1. To simplify the representation, we lift this formulation to a space that also contains the output of a complementary set of neurons, each of which is active when the corresponding neuron is not. Namely, for each neuron i in layer l we also have a variable hil := max{0, -Wilhl-1 - bli}. We use binary variables of the form zil to denote if each neuron i in layer l is active or else if the complement of such neuron is. Finally, we assume M to be a sufficiently large constant.
For a given neuron i in layer l, the following set of constraints maps the input to the output:
Wilhlj-1 + bil = hil + hil, hli  M zil, hli  M (1 - zil), hil  0, hil  0, zil  {0, 1} (1)
Theorem 8. Provided that |wilhlj-1 + bil|  M for any possible value of hl-1, a formulation with the set of constraints (1) for each neuron of a rectifier network is such that a feasible solution with a fixed value for x yields the output y of the neural network.

6

Under review as a conference paper at ICLR 2018
The proof is given in Appendix H. These results have important consequences. First, they allow us to tap into the literature of mixedinteger representability (Jeroslow, 1987) and disjunctive programming (Balas, 1979) to understand what can be modeled on rectifier networks with a finite number of neurons and layers. Second, they imply that we can potentially use mixed-integer optimization solvers to analyze the (x, y) mapping of a trained neural network. That is technically feasible due to the linear proportion between the size of the neural network and that of the mixed-integer formulation. Apart from the bounds on x, we have a linear correspondence between the number of neurons and the size of the formulation: each neuron maps to 6 constraints, 2 continuous variables, and 1 binary variable. Hence, counting linear regions of a network that admits mixed-integer representability relates to enumerating the solutions of the projection on the binary variables z defining activation patterns. More details on the procedure for exact counting are in Appendix I. In addition, we show the theory for unrestricted inputs and a mixed-integer formulation for maxout networks in Appendices J and K, respectively.
6 EXPERIMENTS
We perform two different experiments for region counting using small-sized networks with ReLU activation units on the MNIST benchmark dataset (LeCun et al., 1998). In the first experiment, we generate rectifier networks with 1, 2, 3, and 4 hidden layers having 8 neurons each (plus the 10 neurons for output), each having validation accuracy above 90% after training. While these networks have only a few neurons in each layer, the classification accuracy with 1,2,3, and 4 hidden layers are respectively 90.8%, 91.0%, 89.4%, and 90.2%. The training was carried out for 20 epochs or training steps, and we count the number of linear regions during each training step. For those networks, we count the number of linear regions within 0  x  1 in which a single neuron is active in the output layer, hence partitioning these regions in terms of the digits that they classify. In Fig. 3, we show how the number of regions classifying each digit progresses during training. Some digits have zero linear regions in the beginning, which explains why they begin later in the plot. The total number of such regions per training step is presented in Fig. 4(a). Overall, we observe that the number of linear regions jumps one order of magnitude for each added layer. Furthermore, there is an initial jump in the number of linear regions classifying each digit that seems proportional to the number of layers. We also observe a general increase in the number of linear regions as the validation accuracy increases, which varies more widely as the networks have more hidden layers, thereby reinforcing the idea that linear regions relate to representational power of DNNs.
Figure 3: Total number of regions classifying each digit (different colors for 0-9) of MNIST alone as training progresses, each plot corresponding to a different number of hidden layers.
In the second experiment, we train rectifier networks with two hidden layers summing up to 16 neurons. We train a network for each width configuration under the same conditions as above. In this case, we count all linear regions within 0  x  1, hence not restricting by activation in output layer as before. The number of linear regions of these networks are plotted in Fig. 4(b), along with the upper bound from Theorem 1 and the upper bound from Montu´far et al. (2014).
7 DISCUSSION
The representational power of DNN can be studied by observing the number of linear regions of the PWL function that the DNN represents. In this work, we improve on the upper and lower bounds
7

Under review as a conference paper at ICLR 2018
Figure 4: (a) Total number of linear regions classifying a single digit of MNIST as training progresses, each plot corresponding to a different number of hidden layers. (b) Comparison of upper bounds from Montu´far et al. (2014) and from Theorem 1 with the total number of linear regions of a network with two hidden layers totaling 16 neurons.
on the linear regions derived in prior work (Montu´far et al., 2014; Raghu et al., 2017). We obtain several valuable insights from our extensions. Our upper bound indicates that small widths in early layers cause a bottleneck effect on the number of regions. If we reduce the width of an early layer, the dimensions of the linear regions become irrecoverably smaller throughout the network and the regions will not be able to be partitioned as much. Moreover, the dimensions of the linear regions are not only driven by width, but also the number of activated ReLUs corresponding to the region. This intuition also allows us to create a 1-dimensional construction with the maximal number of regions by eliminating a zero-dimensional bottleneck. In addition to achieving tighter bounds, we show a mixed-integer linear formulation that maps the input space to the output. Using the mixed-integer linear formulation, we show the exact counting of the number of linear regions for several small-sized DNNs during the training process. In the first experiment, we observed that the number of linear regions correctly classifying each digit of the MNIST benchmark increases and vary in proportion to the depth of the network during the first training epochs. In the second experiment, we count the total number of linear regions as we vary the width of two layers with a fixed number of neurons, and we experimentally validate the bottleneck effect by observing that the results follow a similar pattern to the upper bound that we show.
REFERENCES
M. Anthony and P. Bartlett. Neural network learning: Theoretical foundations. 1999. E. Balas. Disjunctive programming. Annals of Discrete Mathematics, (5):3­51, 1979. E. Balas, S. Ceria, and G. Cornue´jols. A lift-and-project cutting plane algorithm for mixed 0­1
programs. Math. Program., 58:295­324, 1993. M. Bianchini and F. Scarselli. On the complexity of neural network classifiers: A comparison
between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning Systems, 2014. Jeffrey D. Camm, Amitabh S. Raturi, and Shigeru Tsubakitani. Cutting big m down to size. Interfaces, 20(5):61­66, 1990. D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. Multi column deep neural network for traffic sign classification. 2012. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 1989.
8

Under review as a conference paper at ICLR 2018
E. Danna, M. Fenelon, Z. Gu, and R. Wunderling. Generating multiple solutions for mixed integer programming problems. In M. Fischetti and D. P. Williamson (eds.), Proceedings of IPCO, pp. 280­294. Springer, 2007.
O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In NIPS, 2011. R. Eldan and O. Shamir. The power of depth for feedforward neural networks. CoRR,
abs/1512.03965, 2015. J.B.J. Fourier. Solution dune question particulie´re du calcul des ine´galite´s. Nouveau Bulletin des
Sciences par la Socie´te´ Philomatique de Paris, pp. 317­319, 1826. I.J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In
ICML, 2013. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. G. Hinton, L. Deng, G.E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 2012. R.G. Jeroslow. Representability in mixed integer programmiing, I: Characterization results. Discrete Applied Mathematics, 17(3):223 ­ 243, 1987. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. W. Maass, G. Schnitger, and E.D. Sontag. A comparison of the computational power of sigmoid and boolean threshold circuits. In Theoretical Advances in Neural Computation and Learning, 1994. H. Mhaskar, Q. Liao, and T. A. Poggio. Learning real and boolean functions: When is deep better than shallow. CoRR, abs/1603.00988, 2016. G. Montu´far, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014. X. Pan and V. Srikumar. Expressivenss of rectifier networks. In ICML, 2016. R. Pascanu, G. Montu´far, and Y. Bengio. On the number of inference regions of deep feed forward networks with piece-wise linear activations. In arXiv:1312.6098, 2013. M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein. On the expressive power of deep neural networks. In ICML, 2017. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015. M. Telgarsky. Representation benefits of deep feedforward networks. CoRR, abs/1509.08101, 2015. T. Zaslavsky. Facing up to arrangements: face-count formulas for partitions of space by hyperplanes. In American Mathematical Society, 1975.
9

Under review as a conference paper at ICLR 2018

Appendices

All the proofs for theorems and lemmas associated with the upper and lower bounds on the linear regions are provided below. The theory for mixed-integer formulation for exact counting in the case of maxouts and unrestricted inputs are also provided below.

A PROOF OF LEMMA 2

Proof. Consider the row space R(W ) of W , which is a subspace of Rd of dimension rank(W ). We show that the number of regions NRd in Rd is equal to the number of regions NR(W ) in R(W )
induced by W x + b = 0 restricted to R(W ). This suffices to prove the lemma since R(W ) has at

most

rank(W ) j=0

m j

regions according to Zavlavsky's theorem.

Since R(W ) is a subspace of Rd, it directly follows that NR(W )  NRd . To show the converse, we
apply the orthogonal decomposition theorem from linear algebra: any point x¯  Rd can be expressed uniquely as x¯ = x^ + y, where x^  R(W ) and y  R(W ). Here, R(W ) = Ker(W ) := {y  Rd : W y = 0}, and thus W x¯ = W x^ + W y = W x^. This means x¯ and x^ lie on the same side of each hyperplane of W x = b and thus belong to the same region. In other words, given any x¯  Rd, its
region is the same one that x^  R(W ) lies in. Therefore, NRd  NR(W ). Hence, NRd = NR(W ) and the result follows.

B PROOF OF LEMMA 3
Proof. The hyperplanes in a region S of the input space are given by the rows of W¯ Sl x + b = 0 for some b. Applying Lemma 2 directly yields the result.

C PROOF OF LEMMA 4

Proof.

rank(W¯ Sl ) = rank(W l Sl-1 (W l-1) . . . S1 (W 1))  min{rank(W l), rank(Sl-1 (W l-1)), . . . , rank(S1 (W 1))}  min{n0, n1, . . . , nl}.

D PROOF OF THEOREM 1

Proof. At each layer, we partition each region independently from the other. Applying Lemma 3

and then Lemma 4 to bound the rank, the maximal number of regions is at most

L l=1

dl j=0

nl j

,

where dl = min{n0, n1, . . . , nl}.

E PROOF OF THEOREM 5
Section D provides us with a helpful insight to construct an example with a large number of regions. It tells us that we want regions to have large dimension in general. In particular, regions of dimension zero cannot be further partitioned. This suggests that the one-dimensional construction from Montu´far et al. (2014) can be improved, as it contains n regions of dimension one and 1 region of dimension zero. This is because all ReLUs point to the same direction as depicted in Fig. 5, leaving one region with an empty activation pattern.
Our construction essentially increases the dimension of this region from zero to one. This is done by shifting the neurons forward and flipping the direction of the third neuron, as illustrated in Fig. 5. We assume n  3.

10

Under review as a conference paper at ICLR 2018
Figure 5: (a) The 1D construction from Montu´far et al. (2014). All units point to the right, leaving a region with dimension zero before the origin. (b) The 1D construction described in this section. Within the interval [0, 1] there are five regions instead of the four in (a).
We review the intuition behind the construction strategy from Montu´far et al. (2014). They construct a linear function h~ : R  R with a zigzag pattern from [0, 1] to [0, 1] that is composed of n ReLUs. More precisely, h~(x) = (1, -1, 1, . . . , ±1) (h1(x), h2(x), . . . , hn(x)), where hi(x) for i = 1, . . . , n are ReLUs. This linear function can be absorbed in the preactivation function of the next layer. The zigzag pattern allows it to replicate in each slope a scaled copy of the function in the domain [0, 1]. Fig. 6 shows an example of this effect. Essentially, when we compose h~ with itself, each linear piece in [t1, t2] such that h~(t1) = 0 and h~(t2) = 1 maps the entire function h~ to the interval [t1, t2], and each piece such that h~(t1) = 1 and h~(t2) = 2 does the same in a backward manner.
Figure 6: A function with a zigzag pattern composed with itself. Note that the entire function is replicated within each linear region, up to a scaling factor. In our construction, we want to use n ReLUs to create n + 1 regions instead of n. In other words, we want the construct this zigzag pattern with n + 1 slopes. In order to do that, we take two steps to give ourselves more freedom. First, observe that we only need each linear piece to go from zero to one or one to zero; that is, the construction works independently of the length of each piece. Therefore, we turn the breakpoints into parameters t1, t2, . . . , tn, where 0 < t1 < t2 < . . . < tn < 1. Second, we add sign and bias parameters to the function h~. That is, h~(x) = (s1, s2, . . . , sn) (h1(x), h2(x), . . . , hn(x)) + d, where si  {-1, +1} and d are parameters to be set. Here, hi(x) = max{0, w~ix + ~bi} since it is a ReLU. We define wi = siw~i and bi = si~bi, which are the weights and biases we seek in each interval to form the zigzag pattern. The parameters si are needed because the signs of w~i cannot be arbitrary: it must match the directions the ReLUs point towards. In particular, we need a positive slope (w~i > 0) if we want i to point right, and a negative slope (w~i < 0) if we want i to point left. Hence, without loss of generality, we do not need to consider the si's any further since they will be directly defined from the signs of the wi's and the directions. More precisely, si = 1 if wi  0 and si = -1 otherwise for i = 1, 2, 4, . . . , n, and s3 = -1 if w3  0 and s3 = 1 otherwise. To summarize, our parameters are the weights wi and biases bi for each ReLU, a global bias d, and the breakpoints 0 < t1 < . . . < tn < 1. Our goal is to find values for these parameters such that each piece in the function h~ with domain in [0, 1] is linear from zero to one or one to zero.
11

Under review as a conference paper at ICLR 2018

More

precisely,

if

the

domain

is

[s,

t],

we

want

each

linear

piece

to

be

either

1 t-s

x

-

s t-s

or

-

1 t-s

x

+

t t-s

,

which

define

linear

functions

from

zero

to

one

and

from

one

to

zero

respectively.

Since we

want a zigzag pattern, the former should happen for the interval [ti, ti-1] when i is odd and the latter

should happen when i is even.

There is one more set of parameters that we will fix. Each ReLU corresponds to a hyperplane, or
a point in dimension one. In fact, these points are the breakpoints t1, . . . , tn. They have directions that define for which inputs the neuron is activated. For instance, if a neuron hi points to the right, then the neuron hi(x) outputs zero if x  ti and the linear function wix + bi if x > ti.

As previously discussed, in our construction all neurons point right except for the third neuron h3, which points left. This is to ensure that the region before t1 has one activated neuron instead of zero, which would happen if all neurons pointed left. However, although ensuring every region
has dimension one is necessary to reach the bound, not every set of directions yields valid weights.
These directions are chosen so that they admit valid weights.

The directions of the neurons tells us which neurons are activated in each region. From left to right,
we start with h3 activated, then we activate h1 and h2 as we move forward, we deactivate h3, and finally we activate h4, . . . , hn in sequence. This yields the following system of equations, where tn+1 is defined as 1 for simplicity:

1

w3x

+ (b3

+ d)

=

x t1

(R1)

(w1

+

w3)

x

+

(b1

+

b3

+

d)

=

- t2

1 -

t1

x

+

t2

t2 -

t1

(R2)

(w1

+

w2

+

w3) x

+ (b1

+ b2

+ b3

+ d)

=

t3

1 - t2 x -

t3

t2 - t2

(R3)

(w1

+

w2)

x

+

(b1

+

b2

+

d)

=

- t4

1 -

t3

x

+

t4

t4 -

t3

(R4)

 



i-1 i-1

w1 + w2 + wj x + b1 + b2 + bj + d =

j=4

j=4

x -1
ti -ti-1

ti-1 ti -ti-1

if i is odd

- x +1
ti -ti-1

ti ti -ti-1

if i is even

(Ri)

for all i = 5, . . . , n + 1

It is left to show that there exists a solution to this system of linear equations such that 0 < t1 < . . . < tn < 1.

First, note that all of the biases b1, . . . , bn, d can be written in terms of t1, . . . , tn. Note that if we subtract (R4) from (R3), we can express b3 in terms of the ti variables. The remaining equations become triangular, and therefore given any values for ti's we can back-substitute the remaining bias variables.

The same subtraction yields w3 in terms of ti's. However, both (R1) and (R3) - (R4) define w3 in terms of the ti variables, so they must be the same:

11

1

t1 = t3 - t2 + t4 - t3 .

If we find values for ti's satisfying this equation and 0 < t1 < . . . < tn < 1, all other weights can be obtained by back-substitution since eliminating w3 yields a triangular set of equations.

In

particular,

the

following

values are

valid:

t1

=

1 2n+1

and

ti

=

2i-1 2n+1

for

all

i

=

2, . . . , n.

The

remaining weights and biases can be obtained as described above, which completes the desired

construction.

As an example, a construction with four units is depicted in Fig. 5.

Its breakpoints are t1

=

1 9

,

t2 =

3 9

,

t3

=

5 9

,

and

t4

=

7 9

.

Its ReLUs are h1(x)

=

max(0,

-

27 2

x

+

3 2

),

h2(x)

=

max(0, 9x - 3), h3(x) = max(0, 9x - 5), and h4(x) = max(0, 9x). Finally, h~(x) =

(-1, 1, -1, 1) (h1(x), h2(x), h3(x), h4(x)) + 5.

12

Under review as a conference paper at ICLR 2018

F PROOF OF THEOREM 6

We follow the proof of Theorem 5 from (Montu´far et al., 2014) except that we use a different

1-dimensional construction. The main idea of the proof is to organize the network into n0 indepen-

dent networks with input size 1 each and apply the 1-dimensional construction to each individual

network. In particular, for each layer l we assign nl/n0 ReLUs to each network, ignoring any re-

mainder units. In (Montu´far et al., 2014), each of these networks have at least

L l=1

nl/n0

regions.

We instead use Theorem 5 to attain Ll=1( nl/n0 + 1) regions in each network.

Since the networks are independent from each other, the number of activation patterns of the com-

pound network is the product of the number of activation patterns of each of the n0 networks. Hence,

the same holds for the number of regions. Therefore, the number of regions of this network is at

least (

L l=1

(

nl/n0

+ 1))n0 .

In addition, we can replace the last layer by a function representing an arrangement of nL hyper-

planes in general position that partitions (0, 1)n0 into

n0 j=0

nL j

regions. This yields the lower

bound of

Ll=-11( nl/n0 + 1)n0

n0 j=0

nL j

.

G PROOF OF THEOREM 7

We denote by Wjl the nl × nl-1 matrix where the rows are given by the j-th weight vectors of each
rank-k maxout unit at layer l, for j = 1, . . . , k. Similarly, blj is the vector composed of the j-th biases at layer l.

In the case of maxout, an activation pattern S = (S1, . . . , Sl) is such that Sl is a vector that maps
from layer-l neurons to {1, . . . , k}. We say that the activation of a neuron is j if wjx + bj attains the maximum among all of its functions; that is, wjx + bj  wj x + bj for all j = 1, . . . , j. In the
case of ties, we assume the function with lowest index is considered as its activation.

Similarly to the ReLU case, denote by Sl : Rnl×nl-1×k  Rnl×nl-1 the operator that selects the rows of W1l, . . . , Wkl that correspond to the activations in Sl. More precisely, Sl (W1l, . . . , Wkl ) is a matrix W such that its i-th row is the i-th row of Wjl, where j is the neuron i's activation in Sl. This essentially applies the maxout effect on the weight matrices given an activation pattern.

Montu´far et al. (2014) provides an upper bound of

n0 j=0

k2n j

for the number of regions for a

single rank-k maxout layer with n neurons. The reasoning is as follows. For a single maxout unit,

there is one region per linear function. The boundaries between the regions are composed by pieces

that are each contained in a hyperplane. Each piece is part of the boundary of at least two regions

and conversely each pair of regions corresponds to at most one piece. Extending these pieces into

hyperplanes cannot decrease the number of regions. Therefore, if we now consider n maxout units

in a single layer, we can have at most the number of regions of an arrangement of k2n hyperplanes.

In the results below we replace k2 by

k 2

, as only pairs of distinct functions need to be considered.

We need to define more precisely these

k 2

n

hyperplanes

in

order

to

apply

the

strategy

from

the

Section 3.1. In a single layer setting, they are given by wjx + bj = wj + bj for each distinct pair

j, j

within a neuron. In order to extend this to multiple layers, consider a

k 2

nl

× nl-1

matrix

W^ l

where its rows are given by wj - wj for every distinct pair j, j within a neuron i and for every

neuron i = 1, . . . , nl. Given a region S, we can now write the weight matrix corresponding to

the hyperplanes described above: W^ Sl := W^ l Sl-1 (W1l-1, . . . , Wkl-1) · · · S1 (W11, . . . , Wk1). In other words, the hyperplanes that extend the boundary pieces within region S are given by the rows

of W^ Sl x + b = 0 for some bias b.

We can now use Lemma 2 to tighten the bound from Zaslavsky (1975) in terms of the rank of the matrix W^ Sl , providing us with the analogous of Lemma 3 for the maxout case.

Lemma 9. The number of regions induced by the nl neurons at layer l within a certain region S is

at most

rl j=0

k(k-1) 2

nl

j

, where rl = rank(W^ Sl ).

13

Under review as a conference paper at ICLR 2018

Proof. For a fixed region S, an upper bound is given by the number of regions of the hyperplane arrangement corresponding to W^ Sl x + b = 0 for some bias b. Applying Lemma 2 yields the result.

We then bound the rank of W^ Sl in terms of the size of the network. Lemma 10.
rank(W^ Sl )  min{n0, n1, . . . , nl}
Proof. Analogous to the proof of Lemma 4.

Finally, the remainder of the proof is analogous to the proof of Theorem 1, except that we

use Lemmas 9 and 10. This yields the upper bound of

L l=1

dl

k(k-1) 2

nl

j=0

j

where dl =

min{n0, n1, . . . , nl}.

H PROOF OF THEOREM 8

Proof. For ease of explanation, we expand the set of constraints (1) as follows:
Wilhjl-1 + bli = hil + hil hli  M zil
hli  M (1 - zil) hil  0 hli  0
zil  {0, 1}

(2) (3) (4) (5) (6) (7)

It suffices to prove that the constraints for each neuron map the input to the output in the same way
that the neural network would. If Wilhl-1 + bil > 0, it follows that hli - hli > 0 according to (2). Since both variables are non-negative due to (5) and (6) whereas one is non-positive due to (3), (4), and (7), then zil = 1 and hli = max 0, Wilhl-1 + bli . If Wilhl-1 +bli < 0, then it similarly follows that hil - hil < 0, zil = 0, and thus hil = min 0, Wilhjl-1 + bli . If j Wilhjl-1 + bil = 0, then
either hli = 0 or hil = 0 due to constraints (5) to (7) whereas (2) implies that hli = 0 or hli = 0, respectively. In this case, the value of zil is arbitrary but irrelevant.

I EXACT COUNTING FOR RECTIFIER NETWORKS USING A MIXED-INTEGER
FORMULATION

A systematic method to count these solutions is the one-tree approach (Danna et al., 2007), which
resumes the search after an optimal solution has been found using the same branch-and-bound tree.
That method can also be applied to near-optimal solutions by revisiting nodes pruned when solving for an optimal solution. Note that in constraints (1), the variables zil can be either 0 or 1 when they lie on the activation boundary, whereas we want to consider a neuron active only when its output is
strictly positive. This discrepancy may cause double-counting when activation boundaries overlap.
We can address that by defining an objective function that maximizes the minimum output f of an
active neuron, which is positive in non-degenerate cases. The formulation is as follows:

max f
s.t. (1) f  hli + (1 - zil)M xX

for each neuron i in layer l for each neuron i in layer l

(8)

14

Under review as a conference paper at ICLR 2018

Corollary 11. The number of z assignments of (8) yielding a positive objective function value corresponds to the number of linear regions of the neural network.

Proof. Implicit in the discussion above. Corollary 12. If the input X is a polytope, then (x, y) is mixed-integer representable.

Proof. Immediate from the existence of a mixed-integer formulation mapping x to y, which is correct as long as the input is bounded and therefore a sufficiently large M exists.

In practice, the value of constant M should be chosen to be as small as possible, which also implies
choosing different values on different places to make the formulation tighter and more stable numer-
ically (Camm et al., 1990). For the constraints set (1), it suffices to choose M to be as large as either hli or h¯li can be given the bounds on the input. For the constraint involving f in formulation (8), we should choose a larger value if some neurons are never active within the input bounds.

J COUNTING LINEAR REGIONS OF RELUS WITH UNRESTRICTED INPUTS

More generally, we can represent linear regions as a disjunctive program (Balas, 1979), which consist of a union of polyhedra. Disjunctive programs are used in the integer programming literature to generate cutting planes by lift-and-project (Balas et al., 1993). In what follows, we assume that a neuron can be either active or inactive when the output lies on the activation hyperplane.

For each active neuron, we can use the following constraints to map input to output:

wilhl-1 + bil = hli hil  0

(9) (10)

For each inactive neuron, we use the following constraint:

wilhl-1 + bil  0 hli = 0

(11) (12)

Theorem 13. The set of linear regions of a rectifier network is a union of polyhedra.

Proof. First, the activation set Sl for each level l defines the following mapping:

(h0, h1, . . . , hL+1) | (9) - (10) if i  Sl; (11) - (12) otherwise
S l {1,...,nl },l{1,...,L+1}

(13)

Consequently, we can project the variables sets h1, . . . , hL+1 out of each of those terms by FourierMotzkin elimination (Fourier, 1826), thereby yielding a polyhedron for each combination of active sets across the layers.

Note that the result above is similar in essence to Theorem 2 of Raghu et al. (2017).
Corollary 14. If X is unrestricted, then the number of linear regions can be counted using (8) if M is large enough.

Proof. To count regions, we only need one point x from each linear region. Since the number of
linear regions is finite, then it suffices if M is large enough to correctly map a single point in each
region. Conversely, each infeasible linear region either corresponds to empty sets of (13) or else to a polyhedron P such that {(h1, . . . , hL+1)  P | hil > 0 l  {1, . . . , L + 1}, i  Sl is empty, and neither case would yield a solution for the z-projection of (8).

15

Under review as a conference paper at ICLR 2018

K MIXED-INTEGER REPRESENTABILITY OF MAXOUT UNITS
In what follows, we assume that we are given a neuron i in level l with output hli. For that neuron, we denote the vector of weights as w1li, . . . , wkli. Thus, the neuron output corresponds to
hil := max w1lihl-1 + b1, . . . , wklihl-1 + bk
Hence, we can connect inputs to outputs for that given neuron as follows:

wjlihjl-1 + bjli = gjli, hli  gjli,
hli  gjli + M (1 - zjli) zjli  {0, 1},
k
zjli = 1
j=1

j = 1, . . . , k j = 1, . . . , k j = 1, . . . , k j = 1, . . . , k

(14) (15) (16) (17)
(18)

The formulation above generalizes that for ReLUs with some small modifications. First, we are
computing the output of each term with constraint (14). The output of the neuron is lower bounded by that of each term with constraint (15). Finally, we have a binary variable zmli per term of each neuron, which denotes which neuron is active. Constraint (18) enforces that only one variable is
at one per neuron, whereas constraint (16) equates the output of the neuron with the active term.
Each constant M should be chosen in a way that the other terms can vary freely, hence effectively
disabling the constraint when the corresponding binary variable is at zero.

16

