Under review as a conference paper at ICLR 2018
ON THE DIFFERENCE BETWEEN BUILDING AND EXTRACTING PATTERNS: A CAUSAL ANALYSIS OF DEEP GENERATIVE MODELS.
Anonymous authors Paper under double-blind review
ABSTRACT
Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but reverse, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggests that enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures.
1 INTRODUCTION
Deep generative models have proven powerful in learning to design realistic images in a variety of complex domains (handwritten digits, human faces, interior scenes). In particular, two approaches have recently emerged: Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which train an image generator by having it fool a discriminator that should tell apart real from artificially generated images; and Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) which learn both a mapping from latent variables to the data (the decoder) and the converse mapping of the data to latent variables (the encoder), such that correspondences between latent variables and data features can be easily investigated. Although these architecture have been lately the subject of extensive investigations, understanding why and how they work, and how they can be improved, remains elusive.
An interesting feature of GANs and VAEs is that they both involve the learning of two deep subnetworks. These sub-networks have a "mirrored" architecture, as they both consists in a hierarchy of convolutional layers, but information flows in opposite ways: generators and decoders mapping to the data space, while discriminators and encoders extract information from the same space. Interestingly, this difference could be framed in a causal perspective, with information flowing in the causal direction from the putative causes of variations in the observed data, while extracting high level properties from observations would operate in the anti-causal direction.
Generative models in machine learning are usually not required to be causal, as modeling the data distribution is considered to be the goal to achieve. However, the idea that a generative model able to capture the causal structure of the data by disentangling the contribution of independent factors should perform better has been suggested in the literature (Bengio et al., 2013a; Mathieu et al., 2016) and evidence supports that this can help the learning procedure (Bengio et al., 2013b). Although many approaches have been implemented on specific examples, principles and automated ways of learning disentangled representation from data remains largely an open problem both for learning representations (targeting supervised learning applications) and for fitting good generative models. GANs have for example recently been a subject of intensive work in this direction, leading
1

Under review as a conference paper at ICLR 2018
to algorithms disentangling high level properties of the data such as InfoGans (Chen et al., 2016) or conditional GANs (Mirza & Osindero, 2014). However such models require supervision (e.g. feeding digit labels as additional inputs) to disentangle factors of interest.
We propose that the coupling between high dimensional parameters can be quantified and exploited in a causal framework to infer whether the model correctly disentangles independent aspects of the deep generative model. This hypothesis relies on recent work exploiting the postulate of Independence of Cause and Mechanism stating that Nature chooses independently the properties of a cause and those of the mechanism that generate effects from the cause (Janzing & Scho¨lkopf, 2010; Lemeire & Janzing, 2012). Several methods relying on this principle have been proposed in the literature in association to different model classes (Janzing et al., 2010; Zscheischler et al., 2011; Daniusis et al., 2010; Janzing et al., 2012; Shajarisales et al., 2015; Sgouritsa et al., 2015; Scho¨lkopf et al., 2012). Among these methods, the Spectral Independence Criterion (SIC) (Shajarisales et al., 2015) can be used in the context of linear dynamical systems, which involve a convolution mechanism.
In this paper, we show how SIC can be adapted to investigate the coupling between the parameters of successive convolutional layers.Empirical investigation shows that SIC is approximately valid between successive layers of generative models, and suggest SIC violations indicate deficiencies in the learning algorithm or the architecture of the network. Interestingly, and in line with theoretical predictions (Shajarisales et al., 2015), SIC tends to be more satisfied for generative sub-networks (mapping latent variables to data), than for the part of the system that map the anti-causal direction (data to latent variables). Overall, our study suggests that enforcing Independence of Mechanisms can help design better generative models.
2 BACKGROUND
2.1 WHY A CAUSAL PERSPECTIVE?
An insightful description of causal model is based on Structural Equations (SEs) of the form Y := f (X1, X2, · · · , XN , )
The right hand side variable in this equation may or may not be random, and the additional independent noise term representing exogenous effects (originating from outside the system under consideration) may be absent. The := sign indicates the asymmetry of this expression and signifies that the left-hand-side variable is computed from the right-hand-side expression but not the opposite. This expression stays valid if something selectively changes on the right hand side variables, for example if the value of X1 is externally forced to stay constant (hard intervention) or if the shape of f or of an input distribution changes (soft intervention). These properties account for the robustness or invariance that is expected from causal models with respect to purely probalistic ones. Assume now X1 itself is determined by other variables according to
X1 := g(U1, U2) then the resulting structural causal model summarized by this system of equations also implies a modularity assumption: one can intervene on the second equation while the first one stays valid, baring the changes in the distribution of X1 entailed by the intervention.
This structural equation framework describes well what would be expected from a robust generative model. Assume the model generates faces, one would like to be able to intervene on e.g. the pose without changing the rest of the top-level parameters (hair color,...), and still be able to observe a realistic output. One could also expect intervening on specific operations performed at intermediate levels by keeping the output "qualitatively" of the same nature. For example, we can imagine that by slightly modifying the mapping that positions the eyes with respect to the nose on a face, one generates different heads that do not match exactly human standards, but are human-like. What we would not want is instead to see artifacts emerging all over the generated image, or edges being blurred.
2.2 THE PRINCIPLE OF INDEPENDENCE OF MECHANISMS
Assume we have two variables X and Y , possibly multidimensional and neither necessarily from a vector space, nor necessary random. Assume the data generating mechanism obeys the following
2

Under review as a conference paper at ICLR 2018

structural equation:

Y := m(X)

with m the mechanism, X the cause and Y the effect. We rely on the postulate that properties of X and m are "independent" in a sense that can be formalized in various ways (Peters et al., 2017). The present analysis will be based on a formalization explained below. Classical Independence of Cause and Mechanism(ICM)-based methods address the problem of cause-effect pair inference, where both directions of causation are plausible. They then take advantage of specific settings for which it is possible to prove that if the assumption is valid for the true generative model, the converse independence assumption is very likely violated (with high probability) for the anticausal or backward model
X := m-1(Y )
(independence is then computed between m-1 and Y ). As a consequence, the true causal direction can be identified by evaluating the independence assumptions of both models and pick the direction for which independence is the most likely.

2.3 SPECTRAL INDEPENDENCE
We introduce a specific formalization of independence of mechanisms that is well suited to study convolutional layers of neural network. This relies on representing signals or images in the Fourier domain.

2.3.1 BACKGROUND ON DISCRETE SIGNALS AND IMAGES

The Discrete-time Fourier Transform (DTFT) of a sequence a = {a[k], k  Z} is defined as

a() = kZ a[k] exp(-i2k),   R

Note that the DTFT of such sequence is a continuous 1-periodic function of the normalized fre-

quency . By Parseval's theorem, the energy can be expressed in the Fourier domain by

a

2 2

=

1/2 -1/2

|a()|2d.

To

simplify

notations,

we

will

denote

by

.

the integral (or average) of a function

over the unit interval I, that is,

a

2 2

=

|a|2 .

The Fourier transform can be easily generalized to 2D signals, leading to a 2D function 1-periodic with respect to both arguments

b(u, v) = kZ,lZ a[k, l] exp(-i2(uk + vl), (u, v)  R2 .

2.3.2 SIC POSTULATE

Assume now that our cause-effect pairs X and Y are weakly stationary time series. This means that the power of these signals can be decomposed in the frequency domain using their Power Spectral Densities (PSD) Sx() and Sy(). We assume that Y is the output of a Linear Time (translation) Invariant Filter with convolution kernel h receiving input X such that

Y = { Z h Xt- } = h  X,

(1)

In that case input and output PSDs are related by the formula Sy() = |h()|2Sx() for all frequencies . The Spectral Independence Postulate consists then in assuming that the power amplification of the filter at each frequency does not adapt to the input power spectrum, i.e. the filter will not tend to selectively amplify or attenuate the frequencies with particularly large or low power. This can be translated into the fact that output power factorizes in the product of input power times the energy of the filter, leads to the postulate (Shajarisales et al., 2015):

Postulate 1 (Spectral Independence Criterion (SIC)). Let Sx be the Power Spectral Density (PSD) of a cause X and h the system impulse response of the causal system of (1), then

1/2 -1/2

Sx()|h()|2d

=

1/2 -1/2

Sx()d

·

1/2 -1/2

|h()|2d

,

(2)

holds approximately.

3

Under review as a conference paper at ICLR 2018

It can be shown that (2) is violated in the backward direction under mild assumptions when the filter is invertible. We can define a scale invariant quantity SIC measuring the departure from the SIC assumption, i.e. the dependence between input power spectrum and frequency response of the filter: the Spectral Dependency Ratio (SDR) from X to Y is defined as

SIC :=

Sx ·|h|2 Sx |h|2

,

(3)

where . denotes the integral over the unit frequency interval. The values of this ratio can be inter-

preted as follows: SIC  1 reflects spectral independence, while SIC > 1 reflects a correlation between the input power and frequency response of the filter (the filter selectively amplifies input

frequency peaks of large power, leading to anomalously large output power), conversely SIC < 1 reflect anticorrelation between these quantities. We will use these terms to analyze our experimen-

tal results. In addition (Shajarisales et al., 2015) also derived theoretical results suggesting that if

SIC  1 for an invertible causal system, then SIC < 1 in the anticausal direction. These interpretations of SDR values are summarized in Fig. 3b, which can be use to understand experimental

results.

3 INDEPENDENCE OF MECHANISMS IN DEEP NETWORKS
We now introduce our causal reasoning in the context of deep convolutional networks, where the output of successive layers are often interpreted as different levels of representations of an image, from detailed low level features to abstract concepts. We thus investigate whether a form of modularity between successive layer can be identified using the above framework.
3.1 STRIDED CONVOLUTION UNITS AND INDEPENDENCE BETWEEN SCALES
DCGANs have successfully exploited the idea of convolutional networks to generate realistic images (Radford et al., 2015). While strided convolutional units are used as pattern detectors in deep convolutional classifiers, they obviously play a different role in generative models: they are pattern producers. We provide in Fig. 1 (left) a toy example to explain their potential ability to generate independent features at multiple scales. On the picture the activation of one channel at the top layer (top left) may encode the (coarse) position of the pair of eyes in the image. After convolution by a first kernel, activations in a second layer indicate the location of each eye. Finally, a kernel encoding the shape of the eye convolves this input to generate an image that combines the three types of information, distributed over three different spatial scales.
What we mean by assuming independence of the features encoded at different scales can be phrased as follows: there should be typically no strong relationship between the shape of patterns encoding a given object at successive scales. Although counter examples may be found, we postulate that a form of independence may hold approximately, for naturalistic images that possess a large number of different multiscale features. A case where this assumption may be violated for a deep network is given in Fig. 1, where a long edge in the image cannot be captured by a single convolution kernel (because of the kernel size limitation to 3 by 3 in this case). Hence, an identical kernel needs to be learned at an upper layer in order to control the precise alignment of activations in the bottom layer. Any misalignment between the orientation of the kernels would lead to a very different pattern, and witness that information in the two successive layers is entangled.
3.2 SIC BETWEEN SUCCESSIVE (DE)CONVOLUTIONAL UNITS
To connect dependency between convolutional units to SIC, that is intuitively to this context, we must take into account two things: the striding adds spacing between input pixels in order to progressively increase the dimension and resolution of the image from one layer to the next, and there is a non-linearity between successive layers. Striding can be easily modeled as it amounts to upsampling the input image before convolution (see illustration Fig. 1). We denote .s the upsampling operation with integer factor1 s that turns the 2D tensor x into
xs[k, l] = x[k/s, l/s], k and l multiple of s 0 otherwise.
1s is the inverse of the stride parameter, which is fractional in that case

4

Under review as a conference paper at ICLR 2018

Figure 1: Left: schematic composition of coarse and finer scale features using two convolution kernels in successive layers to form the eyes of a human face. Right: Example of violation of independence of mechanisms between two successive layers. Crosses indicate center of patch where each active pixel of the previous layer maps to.

z FC

Generator Discriminator Image
256 512

128

64

3

Decision FC

Figure 2: Architeture of the pretrained DCGAN generator used in our experiments

Interestingly, the definition implies that striding amounts to a compression of the normalized frequency axis in the Fourier domain with xs(u, v) = x^(su, sv).

Next, the non-linear activation between successive layers is more challenging to study. We will thus make the simplifying assumption that rectified linear Units are used (ReLU), such that a pixel is either linearly dependent on small variations of its synaptic inputs, or the pixel is not active at all. We then follow the idea that ReLU activations may be used as a switch that controls the flow of relevant information in the network (Tsai et al., 2016; Choi & Kim, 2017). . Hence, for convolution kernels in successive layers that encode different aspects of the same object, we make the assumption that their corresponding outputs will be frequently coactivated, such that the linearity assumption may hold. If pairs of kernels are not frequently coactivated, this suggests that they do not encode the same object and it is thus unlikely that their weights would be dependent.

We thus write down the mathematical expression for the application of two successive deconvolutional units to a given input tensor using linearity assumptions leading to y = g  xs = g  (h  z)s. In the Fourier domain, this implies
x^(u, v) = g^(u, v)y^(su, sv) = g^(u, v)h^(su, sv)z^(s2u, s2.v)

In order to have a criterion independent from the incoming data, we assume the input z has no spatial structure (e.g. z is sampled from a white noise), and thus its power is uniformly distributed across spatial frequencies (the PSD is flat). Then the spatial properties of x are entirely determined by h and the SIC criterion of equation 2 writes

|g^(u, v).h^(su, sv)|2 = |g(u, v)|2 |h(su, sv)|2 ,

(4)

by denoting the double integral by angular brackets. We can write down an updated version of the SDR in equation 3 corresponding to testing SIC between a cascade of two filter (each taken from one of two successive layers) as

SIC =

|g^(u,v).h^ (su,sv)|2 |g(u,v)|2 · |h(u,v)|2

,

which we will evaluate in the experiments.

5

Under review as a conference paper at ICLR 2018

hg
F

g^

Generative (causal) model Inverse (anti-causal) model
Negative correlation Independence

h2

h(2u, 2v)

Positive correlation

F

0 0 0.5 1 1.5 2

SDR

(b) SDR interpretation.

h2  g

h(2u, 2v)  g
F

g layer n layer n+1
h

(c) Convolution pathways between layers.
(a) Illustration of successive convolutions.
Figure 3: 3a Kernels and corresponding Fourier transforms (zero frequencies at the center of each picture). 3b Illustration of the meaning of SDR values. 3c Illustration of the multiple compositions of convolution kernels belonging to successive layers. The pathway depends on the input (blue), output (red) and intermediate or interlayer (green) channels.

4 EXPERIMENTS
We used a version of DCGANs pretrained on the CelebFaces Attributes Dataset (CelebA)2. The structures of the generator and discriminator are summarized in Fig. 2 and contains 4 convolutional layers. We also experimented with a plain VAEs3 with a similar convolutional architecture. Each layer uses 4 pixels wide convolution kernels for the GAN, and 5 pixels wide for the VAE. In all cases, layers are enumerated following the direction of information flow. We will talk about coarse scales for layers that are towards the latent variables, and fine scales for the ones that are close to the image.
4.1 SIC BETWEEN SUCCESSIVE DECONVOLUTIONAL UNITS We first illustrate the theory by showing in Fig. 3a an example of two convolution kernels between successive layers (1 and 2) of the GAN generator. It shows a slight anti-correlation between the absolute values of the Fourier transforms of both kernels, resulting I a SDR of .88. One can notice the effect of kernels g (upper layer) and h (lower layer) on the resulting convolved kernel in the Fourier domain: the kernel from the upper layer tends to modulate the fast variations of g  h in the Fourier domain, while h affects the `slow' variations. This is a consequence of the design of the strided fractional convolution. We use this approach to characterize the amount of dependency between successive layers by plotting the histogram of the SDR that we get for all possible combination of kernels belonging to each layer, i.e. all possible pathways between all input and output channels, as described in Fig. 3c. The result is shown in Fig. 4, witnessing a rather good concentration of the SDR around 1, which suggests independence of the convolution kernels between successive layers. It is interesting to compare these histograms to the values computed for the discriminator of the GAN, which implements convolutional layers of the same dimensions in reverse order. The result
2http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html 3https://github.com/yzwxx/vae-celebA

6

Under review as a conference paper at ICLR 2018

Finer scale

Intermediate scale

Coarser scale

8

generator discriminator

6

6

8 6

444

222

00 1 2 3 00 1 2 3 00 1 2 3

Figure 4: Superimposed histograms of SIC generic ratios for layers at the same level of resolution (from lower to higher).

16 14 12 10
8 6 4 2 00

Lower level
generator discriminator
123

6 5 4 3 2 1 00

Intermediate level
12

3.5 3.0 2.5 2.0 1.5 1.0 0.5 3 0.0 0

Higher level)
12

3

Figure 5: Comparison of SDR statistics between decoder (causal, blue) and encoder (anticausal, orange) of a trained VAE.

also shown in Fig. 4 exhibits a broader distribution of the SDR, especially for layers encoding lower level image features. This is in line with the ICM principle, as the discriminator is operating in the anticausal direction. However, the difference between the generator and discriminator is not strong, which may be due to the fact that the discriminator does not implement an inversion of the putative causal model, but only extract relevant latent information for discrimination. In order to check our method on a generative model including a network mapping back the input to their cause latent variables, we applied it to a trained VAE. The results presented in Fig. 5 show much sharper differences between generator (decoder) and encoder. The shape of the histograms are matching prediction from (Shajarisales et al., 2015) shown in Fig. 3b. The difference with GANs can be explained by the fact that VAEs are indeed performing an invertion of the generative model, leading to very small SDR values in the anticausal direction. We also note overall a much broder distribution of VAE SDRs in the causal direction (decoder). Interestingly, the training of the VAE did not lead to values as satisfactory are what we obtained with the GAN. Examples generated from the VAE are shown in appendix (Fig. 8). This suggests that dependence between cause and mechanism may reflect a suboptimal performance of the generator.
4.2 REMOVING CORRELATED UNITS
We saw in the above histograms that while many successive convolutional unit had a SDR close to one, there are tails composed of units exhibiting either a ratio inferior to one (reflecting negative correlation between the Fourier transform of the corresponding kernels) or a ratio larger than one (reflecting positive correlation). Interestingly, if we superimpose the histograms (see Fig. 4) of the lower level layer of the generator and discriminator networks, we see that these tails are quite different between networks, showing more negative correlations for the discriminative networks, while the positive correlation tail of the generative networks remains rather strong. This suggests that negative and positive correlation are qualitatively different phenomena, in line with our analysis of the matrix factorization algorithms. In order to investigate the nature of filters exhibiting different signs of correlation, we selectively removed filters of the third layer of the generator (the last but one), based on the magnitude (above or below one) of the average SDR that each filter achieved when combined with any of the filters in the last layer. In order to check that our results were not influenced by filters with very small weights (that still can exhibit correlations), we zeroed the filters having the kernels with the smallest energy, while maintaining an acceptable quality of the output

7

Under review as a conference paper at ICLR 2018

original

reduced

<0 correlation removed >0 correlation removed

Figure 6: Random face generated by a pretrained DCGAN (left column). Second column: output of the same network when removing low energy filters from the third layer (reduction from 8,192 to 4,260). Third column: output of the same network when removing the filters leading to the lowest average generic ratio (SIC < .9, leading to 3,684 filters). Fourth column: same when removing the filters leading to the largest average generic ratio ( > 1.45, leading to 3,660 filters). More examples are shown in appendix Fig. 7.
of the network (see Fig. 6 second column). This removed around half of the filters of the third layer. Then we remove additional filters exhibiting either large or small (anti-correlated) generic ratio, such that the same proportion of filters is removed (see Fig. 6 third and fourth column). It appears clearly from this result that filters exhibiting large positive or negative correlation do not play the same role in the network. From the quality of the generated images, filters from the third layer negatively correlated to filters from the fourth seem to allow correction of checkerboard artifacts, potentially generated as side effect of the fractional strided convolution mechanisms4. Despite a decrease in texture quality, the removal of such weight does not distinctively affect an informative aspect of the generated images. Conversely, removing positively correlated filters lead to a disappearance of the color information in a majority of the regions of the image. This suggests that such filters do encode the color structure of images, and the introduced positive correlation between the filters from the third and fourth layer may result from the fact that uniform color patches corresponding to specific parts of the image (hair, skin) need a tight coordination of the filters at several scales of the image in order to produce large uniform areas with sharp border. As we observed more dependency betwen GAN layers at fine scales, we considered using dropout (Srivastava et al., 2014) in order to reduce this dependency. Indeed, dropout has been introduced with the idea that it can prevent neurons from over-adapting to each other and thus regularize the network. The results shown in Fig. 9 witness on the contrary and increase of the dependencies (especially positive correlation) between these layers and exhibit a strongly deteriorated performance as shown in examples Fig. 10. We suggest that dropout limits the expressivity of the network by enforcing more redundancy in the convolutional filters, leading also to more dependency between them.
5 DISCUSSION
In this work, we derived a measure of independence between the weights learned by convolutional layers of deep networks. The results suggest that generative models that map latent variables to data tend to have more independence between successive layers than discriminative or encoding networks. This is in line with theoretical predictions about independence of mechanisms for causal and anticausal systems. In addition, our results suggest the dependency between successive layers relates to the bad performance of the trained generative models. Enforcing independence during training may thus help build better generative models. Moreover, the SDR analysis also indicates which layers should be modified to improve the performance. Finally, we speculate that independence between successive layers, by favoring modularity of the network, may help build architecture that can be easily adapted to new purposed. In particular, separation of spatial scales in such models may help build architectures where one can intervene on one scale without affecting others, with applications such as style transfer Gatys et al. (2015). One specific feature of our approach is that this quantitative measure of the networks in not statistical and as such does not require test samples to be computed. Only the parameters of the model are used, which makes the approach easy to apply to any neural network equipped with convolutional layers.
4 https://distill.pub/2016/deconv-checkerboard/
8

Under review as a conference paper at ICLR 2018
REFERENCES
Y Bengio, A Courville, and P Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013a.
Y Bengio, G Mesnil, Y Dauphin, and S Rifai. Better mixing via deep representations. In ICML 2013, 2013b.
X Chen, Y Duan, R Houthooft, J Schulman, I Sutskever, and P Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. jun 2016.
Jae-Seok Choi and Munchurl Kim. A deep convolutional neural network with selection units for superresolution. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on, pp. 1150­1156. IEEE, 2017.
P. Daniusis, D. Janzing, K. Mooij, J. Zscheischler, B. Steudel, K. Zhang, and B. Scho¨lkopf. Inferring deterministic causal relations. In UAI 2010, 2010.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
D. Janzing and B. Scho¨lkopf. Causal inference using the algorithmic Markov condition. Information Theory, IEEE Transactions on, 56(10):5168­5194, 2010.
D. Janzing, P.O. Hoyer, and B. Scho¨lkopf. Telling cause from effect based on high-dimensional observations. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.
D. Janzing, J. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniusis, B. Steudel, and B. Scho¨lkopf. Information-geometric approach to inferring causal directions. Artificial Intelligence, 182­183:1­31, 2012.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
J. Lemeire and D. Janzing. Replacing causal faithfulness with algorithmic independence of conditionals. Minds and Machines, pp. 1­23, 7 2012. doi: 10.1007/s11023-012-9283-1.
M F Mathieu, J J Zhao, A Ramesh, P Sprechmann, and Y LeCun. Disentangling factors of variation in deep representation using adversarial training. In Advances in Neural Information Processing Systems, pp. 5041­ 5049, 2016.
M Mirza and S Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
J. Peters, D. Janzing, and B. Scho¨lkopf. Elements of Causal Inference ­ Foundations and Learning Algorithms. MIT Press, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
B. Scho¨lkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. On causal and anticausal learning. In ICML 2012, 2012.
E. Sgouritsa, D. Janzing, P. Hennig, and B. Scho¨lkopf. Inference of cause and effect with unsupervised inverse regression. In ICML 2015, 2015.
N. Shajarisales, D. Janzing, B. Scho¨lkopf, and M. Besserve. Telling cause from effect in deterministic linear dynamical systems. In ICML 2015, 2015.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1): 1929­1958, 2014.
Chuan-Yung Tsai, Andrew M Saxe, and David Cox. Tensor switching networks. In Advances in Neural Information Processing Systems, pp. 2038­2046, 2016.
J. Zscheischler, D. Janzing, and K. Zhang. Testing whether linear equations are causal: A free probability theory approach. In UAI 2011, 2011.
9

Under review as a conference paper at ICLR 2018
APPENDIX

original

reduced

<0 correlation removed >0 correlation removed

Figure 7: Example generated figures using a pretrained DCGAN (left column). Second column: the output of the same network when removing low energy filters from the third layer (reduction of the number of filters from 8,192 to 4,260). Third column: the output of the same network when removing the filters leading to the lowest average generic ratio ( < .9, leading to 3,684 filters). Fourth column: same when removing the filters leading to the largest average generic ratio ( > 1.45, leading to 3,660 filters).
10

Under review as a conference paper at ICLR 2018
Figure 8: Random examples generated by the trained VAE. 11

Under review as a conference paper at ICLR 2018

Layer 3->4 (lower level) Layer 2->3 Layer 1->2 (higher level)

0 20 20 20 20 2

0 20 20 20 20 2

02 600 ite.

02 1500 ite.

02 3000 ite.

02 6000 ite.

02 7200 ite.

Figure 9: Evolution of the SIC generic ratios between successive layers as a function of training iteration when dropout is used between the two finer scale layers of the generator.

Figure 10: Evolution of generated examples (for a fixed latent input) as function of training iteration (same as Fig. 9) when dropout is used.
12

