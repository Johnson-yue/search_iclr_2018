Under review as a conference paper at ICLR 2018
VARIANCE REGULARIZED COUNTERFACTUAL RISK MINIMIZATION VIA VARIATIONAL DIVERGENCE MIN-
IMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Off-policy learning, the task of evaluating and improving policies using historic data collected from a logging policy, is important because on-policy evaluation is usually expensive and has adverse impacts. One of the major challenge of offpolicy learning is to derive counterfactual estimators that also has low variance and thus low generalization error. In this work, inspired by learning bounds for importance sampling problems, we present a new counterfactual learning principle for off-policy learning with bandit feedbacks. Our method regularizes the generalization error by minimizing the distribution divergence between the logging policy and the new policy, and removes the need for iterating through all training samples to compute sample variance regularization in prior work. With neural network policies, our end-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms and is also consistent with our theoretical results.
1 INTRODUCTION
Off-policy learning refers to evaluating and improving a deterministic policy using historic data collected from a stationary policy, which is important because in real-world scenarios on-policy evaluation is oftentimes expensive and has adverse impacts. For instance, evaluating a new treatment option, a clinical policy, by administering it to patients requires rigorous human clinical trials, in which patients are exposed to risks of serious side effects. As another example, an online advertising A/B testing can incur high cost for advertisers and bring them few gains. Therefore, we need to utilize historic data to perform off-policy evaluation and learning that can enable safe exploration of the hypothesis space of policies before deploying them.
There has been extensive studies on off-policy learning in the context of reinforcement learning and contextual bandits, including various methods such as Q learning Sutton & Barto (1998), doubly robust estimator Dud´ik et al. (2014), self-normalized Swaminathan & Joachims (2015b), etc. A recently emerging direction of off-policy learning involves the use of logged interaction data with bandit feedback. However, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another action, the best action we should have take, and the relationship between the change in policy and the change in reward. For example, after an item is suggested to a user by an online recommendation system, although we can observe the user's subsequent interactions with this particular item, we cannot anticipate the user's reaction to other items that could have been the better options.
Using historic data to perform off-policy learning in bandit feedback case faces a common challenge in counterfactual inference: How do we handle the distribution mismatch between the logging policy and a new policy and the induced generalization error? To answer this question, Swaminathan & Joachims (2015a) derived the new counterfactual risk minimization framework, that added the sample variance as a regularization term into conventional empirical risk minimization objective. However, the parametrization of policies in their work as linear stochastic models has limited representation power, and the computation of sample variance regularization requires iterating through
1

Under review as a conference paper at ICLR 2018
all training samples. Although a first-order approximation technique was proposed in the paper, deriving accurate and efficient end-to-end training algorithms under this framework still remains a challenging task.
Our contribution in this paper is three-fold:
1. By drawing a connection to the generalization error bound of importance sampling Cortes et al. (2010), we propose a new learning principle for off-policy learning with bandit feedback. We explicitly regularize the generalization error of the new policy by minimizing the distribution divergence between it and the logging policy.
2. To enable end-to-end training, we propose to parametrize the policy as a neural network, and solves the divergence minimization problem using recent work on variational divergence minimization Nowozin et al. (2016) and Gumbel soft-max Jang et al. (2016) sampling.
3. Our experiment evaluation on benchmark datasets shows significant improvement in performance over conventional baselines, and case studies also corroborates the soundness of our theoretical proofs.
2 BACKGROUND
2.1 PROBLEM FRAMEWORK
We first review the framework of off-policy learning with logged bandit feedback introduced in Swaminathan & Joachims (2015a). A policy maps an input x  X to a structured (discrete) output y  Y. For example, the input x can be profiles of users, and we recommend movies of relevance to the users as the output y; or in the reinforcement learning setting, the input is the trajectory of the agent, and the output is the action the agent should take in the next time point. We use a family of stochastic policies, where each policy defines a posterior distribution over the output space given the input x, parametrized by some , i.e., h(Y|x). Note that here a distribution which has all its probability density mass on one action corresponds to a deterministic policy. With the distribution h(Y|x), we take actions by sampling from it, and each action y has a probability of h(y|x) being selected. In the discussion later, we will use h and h(y|x) interchangeably when there will not create any confusion. In online systems, we observe feedbacks (x, y; y) for the action y sampled from h(Y|x) by comparing it to some underlying `best' y that was not revealed to the system. For example, in recommendation system, we can use a scalar loss function (x, y; y)  R, with smaller values indicating higher satisfaction with recommended items.
The expected risk of a policy h(Y|x) is defined as
R(h) = ExP(X ),yh(Y|x)[(x, y)]
, and the goal of off-policy learning is to find a policy with minimum expected risk on test data.
In the off-line logged learning setting, we only have data collected from a logging policy h0(Y|x), and we aim to find an improved policy h(Y|x) that has lower expected risks R(h) < R(h0). Specifically, the data we will use will be
D = {xi, yi, i = i(xi, yi), pi = h0(yi|xi)}
, where i and pi are the observed loss feedback and the logging probability (also called propensity score).
Two main challenges are associated with this task: 1) If the distribution of a logging policy is skewed towards a specific region of the whole space, and doesn't have support everywhere, feedbacks of certain actions cannot be obtained and improvement for these actions is not possible as a result. 2) since we cannot compute the expectation exactly, we need to resort to empirical estimation using finite samples, which creates generalization error and needs additional regularization.
A vanilla approach to solve the problem is propensity scoring approach using importance sampling Rosenbaum & Rubin (1983), by accounting for the distribution mismatch between h and h0. Specif-
2

Under review as a conference paper at ICLR 2018

ically, we can rewrite the expected risk w.r.t h as the risk w.r.t h0 using an importance reweighting:

R(h) = ExP(X ),yh(y|x)[(x, y)] h(y|x)
= ExP(X ),yh0(y|x)[ h0(y|x) (x, y)]

(1) (2)

With the collected historic dataset D, we can estimate the empirical risk R^D(h), short as R^(h)

R^(h) =

1 N

N i=1

h(yi|xi) h0(yi|xi)

i(xi,

yi

)

(3)

2.2 COUNTERFACTUAL RISK MINIMIZATION

Swaminathan & Joachims (2015a) pointed out several flaws with the vanilla approach, namely, not being invariant to loss scaling, large and potentially unbounded variance. To regularize the variance, the authors proposed a regularization term for sample variance derived from empirical Bernstein bounds.

The modified objective function to minimize is now:

R^(h) = 1 N

N

ui + 

V ar(u¯) N

i=1

(4)

,

where

ui

=

h(yi |xi ) h0 (yi |xi )

i

,

and

u¯

=

1 N

N
ui
i=1

is

the

average

of

{ui}

obtained

from

training

data.

As the variance term is dependent on the whole dataset, stochastic training is difficult, the authors approximated the regularization term via first-order Taylor expansion and obtained a stochastic optimization algorithm. Despite its simplicity, such first-order approximation neglects the non-linear terms from second-order and above, and introduces approximation errors while trying to reduce the sample variance.

3 METHOD

3.1 NEW VARIANCE REGULARIZATION OBJECTIVE

Instead of estimating variance empirically from the samples, which prohibits direct stochastic training, the fact that we have a parametrized version of the policy h(Y|x) motivates us to think: can we
derive a variance bound directly from the parametrized distribution?

We first note that the empirical risk term R^(h) is the average loss reweigthed by importance sampling

function

h(y|x) h0 (y |x)

,

and

we

have

a

general

learning

bounds

for

importance

sampling

weights.

Let

w(z)

=

p(z) p0 (z )

,

where

p

and

p0

are

two

probability

density

functions,

the

following

identity

holds

Lemma 1. Cortes et al. (2010)

E[w] = 1, E[w2] = d2(p||p0) = 2D2(p||p0)

V ar(w) = E[w2] - E[w] = d2(p||p0) - 1 Ezp0(z)[w2(z)L2(z)]  d2(p||p0)
, where D2 is the Re´nyi divergence D Re´nyi et al. (1961) with  = 2, i.e. squared Chi-2 divergence.

Based on this lemma, we can derive an upper bound for the second moment of the weighted loss

Theorem 1.

ExP(X ),yh0(y|x)[w2(y|x)2(x, y)]  d2(h(y|x)||h0(y|x); P(x))

(5)

3

Under review as a conference paper at ICLR 2018

Proof. Let z = (x, y), p(z) = P(x)h(y|x), p0(z) = P(x)h0(y|x), and L(z) = (x, y). We apply Lemma 1 to z, importance sampling weight function w(z) = p(z)/p0(z) = h(y|x)/h0(y|x), and loss L(z), we have
ExP(X ),yh0(y|x)[w2(y|x)2(x, y)] = Ezp0(z)[w2(z)L2(z)]  d2(p(z)||p0(z)) = p(z) p(z)dz z p0(z) = h(y|x) h(y|x)P(x)dxdy x,y h0(y|x)
= d2(h(y|x)||h0(y|x))P(x)dx
x
= d2(h(y|x)||h0(y|x); P(x))

From the above theorem, we are able to derive a generalization bound between the expected risk R(h) and empirical risk R^(h) using the distribution divergence function as
Theorem 2. Assume d2(h||h0; P(X)) is upper bounded by M , and with N training samples and a finite set of hypothesis H, we have, for any > 0, with probability at least 1 - ,

R(h)  R^(h) + 2M (log (1/ ) + log |H|) + 2d2(h||h0; P(x))(log (1/ ) + log |H|) 3N N

(6)

The proof of the theorem is similar to that of Cortes et al. (2010) by utilizing Bernstein's inequality and the second order moment upper bound in Theorem 1, except that we are working with the joint distribution on z = (x, y). We refer readers to the original paper for more details.
Although the theorem above only works for finite hypothesis space, analogous results hold for infinite hypothesis space by using covering number theory Maurer (2009) instead of sample size N . Similarly, we can have a bound in the format of

R(h)  R^(h) + C1(M, , H, N ) + C2(M, , H, N ) d2(h||h0; P(x))

(7)

This motivates us that in bandit learning setting, instead of directly optimizing the reweighed loss, we try to minimize the variance regularized objectives as

min R^(h) +  d2(h||h0; P(x))
h=h(Y |x)

(8)

, where  is a model hyper-parameter controlling the trade-off between empirical risk and model
variance. At first glance, the new objective function removes the needs to compute the sample variance in existing bounds (4), but when we have a parametrized distribution of h(y|x), and finite samples {xi, yi}Ni=1 from h0(yi|xi), estimating the divergence function is not an easy task. In the next subsection, we will present how recent f-gan networks for variational divergence minimization
Nowozin et al. (2016) and Gumbel soft-max sampling Jang et al. (2016) can help solve the task.

Discussion: Possibility of Counterfactual Learning: One interesting aspect of our bounds also
stresses the need for the stochasticity of the logging policy Langford et al. (2008). For a deterministic
logging policy, if the corresponding probability distribution can only have some peaked masses, and
zeros elsewhere in its domain, our intution suggests that learning will be difficult, as those regions
are never explored. Our theory well reflects this intuition in the calculation of the divergence term, the integral of form y h2(y|x)/h0(y|x)dy. A deterministic policy has a non-zero measure region of h0(Y|x) with probability density of h0(y|x) = 0, while the corresponding h(y|x) can have finite values in the region. The resulting integral results is thus unbounded, and in turn induces an
unbounded generalization bound, making counterfactual learning in this case not possible.

4

Under review as a conference paper at ICLR 2018

3.2 ADVERSARIAL LEARNING OF THE DIVERGENCE

The derived variance regularized objective (8) requires us to minimize the square root of the condi-

tional divergence, d2(h||h0; P(X)) = Ex

y

h2 h0

dy.

For simplicity, we can examine the term inside the expectation operation first. With simple calculation, we have

h(y|x)2 y h0(y|x) dy =

y

h0(y|x)[(

h(y|x) h0(y|x)

)2

-

1

+

1]

=

Df

(h(Y |x)||h0 (Y |x))

-

1

, where f (t) = t2 - 1 is a convex function in the domain {t : t  0} with f (1) = 0. Combining with the expectation operator gives a minimization objective of Df (h||h0; P(X)) (-1 omitted as constant).
The above calculation draws connection between our divergence and the f-divergence measure Nguyen et al. (2010). Follow the f-GAN for variational divergence minimization method proposed in Nowozin et al. (2016), we can reach a lower bound of the above objective as

Df (h(Y|x)||h0(Y|x); P(X)) = Ex[ f (h(y|x)/h0(y|x))h0(y|x)dy]

y

 Ex[sup{Eyh(y|x)[T (y)] - Eyh0(y|x)[f (T (y))])}
T

= sup{ExEyh(y|x)[T (x, y)] - ExEyh0(y|x)[f (T (x, y))]}
T

= sup{Ex,yh(y|x)T (x, y) - Ex,yh0(y|x)f (T (x, y))}
T

(9)

F (Tw, h)

(10)

The lower bound objective (9) is a saddle point of a function T (x, y) : X × Y  R that maps input pairs to a scalar value, and the policy we want to learn h(Y|x). Again, a generative-adversarial approach Goodfellow et al. (2014) can be applied.
Toward this end, we represent the T function as a discriminator network parametrized as Tw(x, y). We then parametrize the distribution of our policy h(y|x) as another generator neural network h(y|x) mapping x to the probability of sampling y. For structured output problems with discrete values of y, to allow the gradients of samples obtained from sampling backpropagated to all other parameters, we use the Gumbel soft-max sampling Jang et al. (2016) methods for differential sampling from the distribution h(y|x). We list the complete training procedure Alg. 1 for completeness.
Data: D = {xi, yi}Ni=1 sampled from logging policy h0; a predefined threshold D0; an initial generator distribution h0 (y|x); an initial discriminator function Tw0 (x, y)
Result: An optimized generator h (y|x) distribution that has minimum divergence to h0 initialization; while Df (h||h0; P(X)) > D0 or iter < max iter do
Sample a mini-batch `real' samples (xi, yi) from D ; Sample a mini-batch x from D, and construct `fake' samples (xi, y^i) by sampling y^ from
ht (y|x) with Gubmel soft-max ; Update wt+1 = wt + wF (Tw, h)(9) ; Update t+1 = t - F (Tw, h)(9) ; end
Algorithm 1: Varitional Minimizing Df (h||h0; P(X))
For our purpose of minimizing the variance regularization term, we can similarly derive a training algorithm, as the gradient of t  t - 1 can also be backpropagated.

5

Under review as a conference paper at ICLR 2018

3.3 TRAINING ALGORITHM

With the above two components, we are now ready to present the full treatment of our end-to-end learning for counterfactual risk minimization from logged data.

Data: D = Result: An

{xi, yi}Ni=1 sampled optimized generator

from logging policy h0; regularization hyper-parameter h(y|x) that is an approximate minimizer of R(w)



initilization;

while Not Converged do

/* Update discriminator Sample a mini-batch of `fake' samples (xi, y^i) with xi from D and y^i  ht (y|xi); Sample a mini-batch of `real' samples (xi, yi) from D ;
Update wt+1 = wt + wF (Tw, h)(9) ;
/* Update generator Sample a mini-batch of m samples from D ;

*/ */

Estimate

the

reweighted

loss

as

R^t

=

1 m

m i=1

ht (yi|xi)
pi

i

and

get

the

gradient

as

g1

=

 Rt

;

Sample a mini-batch of m1 `fake' samples ;

Estimate the generator gradient as g2 = F (Tw, h)(9) ;

Update t+1 = t - (g1 + g2) ;

end

Algorithm 2: Minimizing Variance Regularized Risk - Co-Training Version

In practice, we find the above training is difficult because of the blending of two types of gradients. Instead of training them together, we proposed a slightly different training scheme, by converting a regularized optimization to a (potentially) easier constrained optimization. 1
The algorithm now tries to solve the optimization problem as

min
h

1 m

m i=1

ht

(yi|xi pi

)

i

s.t.

D2(h||h0; P(X))



 N

(11)

, where  is a pre-selected constant as the regularization hyper-parameter. The new formulation can be solved in an easier fashion:

Data: D = {xi, yi}Ni=0 sampled from h0; regularization hyper-parameter , and maximum
iteration of divergence minimization steps T Result: An optimized generator h(y|x) that is an approximate minimizer of R(w) initialization;

while Not Converged do

/* Update  to minimize the reweighted loss Sample a mini-batch of m samples from D ;

*/

Estimate

the

reweighted

loss

as

R^t

=

1 m

m i=1

ht (yi|xi)
pi

i

and

get

the

gradient

as

g1

=

 Rt

;

Update t+1 = t - g1 ;

/* Update discriminator and generator for divergence

minimization

*/

Call Algorithm 1 to minimize the divergence D2(h||h0; P(X)) with threshold = , and max

iter set to I ;

end

Algorithm 3: Minimizing Variance Regularized Risk - Separate Training

1Duchi & Namkoong (2016) established guarantees for the two solution are close in a statistical sense for simple discrete distributions, but we weren't able to derive similar bounds for much more complex distributions parametrized by neural networks.
6

Under review as a conference paper at ICLR 2018

4 RELATED WORK
Exploiting historic data is an important problem in multi-armed bandit and its variants such as contextual bandit and has wide applications Strehl et al. (2010); Shivaswamy & Joachims (2012); Beygelzimer & Langford (2009). Approaches such as doubly robust estimators Dud´ik et al. (2014) have been proposed, and recent theoretical study explored the finite-time minimax risk lower bound of the problem Li et al. (2015), and an adaptive learning algorithm Wang et al. (2017) using the theoretical analysis.
Bandits problems can be interpreted as a single-state reinforcement learning (RL) problems, and techniques including doubly robust estimators Jiang & Li (2015); Thomas & Brunskill (2016); Munos et al. (2016) have also been extended to RL domains. Conventional techniques such as Q function learning, and temporal difference learning Sutton & Barto (1998) are alternatives for off-policy learning in RL by accounting for the Markov property of the decision process. Recent works in deep RL studies have also addressed off-policy updates by methods such as multi-step bootstrapping Mahmood et al. (2017), off-policy training of Q functions Gu et al. (2017).
Learning from logs traces backs to Horvitz & Thompson (1952); Rosenbaum & Rubin (1983) , where propensity scores are applied to evaluate candidate policies. In statistics, the problem is also described as treatment effect estimation Imbens (2004), where the focus is to estimate the effect of an intervention from observational studies that are collected by a different intervention. Bottou et al. (2013) derived unbiased counterfactual estimators to study an example of computational advertising; another set of techniques reduce the bandit learning to a weighted supervised learning problems Zadrozny et al. (2003), but is shown to have poor generalization performance Beygelzimer & Langford (2009).
Although our variance regularization aims at off-policy learning with bandit feedback, part of the proof comes from the study of generalization bounds in importance sampling problems Cortes et al. (2010), where the original purpose was to account for the distribution mismatch between training data and testing distribution, also called covariate shift, in supervised learning. Duchi & Namkoong (2016) also discussed variance regularized empirical risk minimization for supervised learning with a convex objective function, which has connections to distributionally robust optimization problem Bertsimas et al. (2011). It will be of further interest to study how our divergence minimization technique can be applied to supervised learning and domain adaptation Sugiyama et al. (2007); Gretton et al. (2009) problems as an alternative to address the distribution match issue.

5 EXPERIMENTS

5.1 EXPERIMENT SETUPS

For empirical evaluation of our proposed algorithms, we follow the conversion from supervised learning to bandit feedback method Agarwal et al. (2014). For a given supervised dataset D = wa{(epxriae,ldsyioic)tui}osNien=1tyh,iewceofihnr0ds(ittyic|oxonnia)sl,trrauancndtdaocomlollgefigcetilndtgh(epCofRelieFcd)ybphaoc0lk(icYays|xtr)a(,iyanine,ddyitoh)n.en5F%oforrothefaeDcphusraapsmotspheleeolfxoibg,egwninceghsmpamoalrpikclsye, h0 , and use hamming loss, the number of incorrectly misclassified labels between yi and yi, as the loss function  Swaminathan & Joachims (2015a). To create bandit feedback datasets D =
{xi, yi, i, pi}, each of the samples xi were passed four times to the logging policy h0 and sampled actions yi were recorded along with the loss value i and the propensity score pi = h0(yi|xi).

In evaluation, we use two type of evaluation metrics for the probabilistic policy h(Y|x). The first is

the

expected

loss

(referred

to

as

`EXP'

later)

R(h)

=

1 Ntest

i Eyh(y|xi)(yi, y), a direct measure

of the generalization performance of the learned policy. The second is the average hamming loss

of maximum a posteriori probability (MAP) prediction yMAP = arg max h(y|x) derived from the

learned policy, as MAP is a faster way to generate predictions without the need for sampling in

practice. However, since MAP predictions only depend on the regions with highest probability, and

doesn't take into account the diverse of predictions, two policies with same MAP performance could

have very different generalization performance. Thus, a model with high MAP performance but low

EXP performance might be over-fitting, as it may be centering most of its probability masses in the

regions where h0 policy obtained good performance.

7

Under review as a conference paper at ICLR 2018

Table 1: Benchmark Comparison Results.

Dataset
Evaluation Metrics
Logging Policy h0
NN-NoReg NN-Hard NN-Soft
IPS POEM IPS(Stochastic) POEM(Stochastic)
Supervised Learning

Scene

MAP EXP

1.069 1.887

1.465 1.981 1.303 1.463 1.347 1.457

1.350 1.169 1.291 1.322

1.350 1.169 1.291 1.323

1.110 1.423

Yeast

MAP EXP

3.255 5.485

3.223 4.705 3.047 3.788 3.097 3.789

4.256 4.238 4.090 4.140

4.521 4.508 4.605 4.570

2.807 4.047

TMC

MAP EXP

4.995 5.053

1.706 1.724 1.694 1.720 1.683 1.707

4.601 4.611 2.812 3.601

4.416 4.505 2.737 3.561

1.344 1.241

LYRL

MAP EXP

1.047 1.949

0.247 0.263 0.248 0.255 0.247 0.255

1.240 1.169 1.149 1.237

1.240 1.306 1.479 1.237

0.240 0.437

5.2 BENCHMARK COMPARISON
Baselines Vanilla importance sampling algorithms using inverse propensity score (IPS), and the counterfactual risk minimization algorithm are compared, with both L-BFGS optimization and stochastic optimization solvers. The hyper-parameters are selected by performance on validation set and we more details of their methods can be found in the original paper Swaminathan & Joachims (2015a).
Neural network policies without divergence regularization (short as "NN-NoReg" in later discussions) is also compared as baselines, to verify the effectiveness of variance regularization.
Dataset We use four multi-label classification dataset collected in the UCI machine learning repo Asuncion & Newman (2007), and perform the supervised to bandit conversion. We report the statistics in Table 2 in the Appendix.
For these datasets, we choose a three-layer feed-forward neural network for our policy distribution, and a two or three layer feed-forward neural network as the discriminator for divergence minimization. Detailed configurations can be found in the Appendix.
For benchmark comparison, we use the separate training version 3 as it has faster convergence and better performance (See Sec. 5.5 for an empirical comparison). The networks are trained with Adam Kingma & Ba (2014) of learning rate 0.001 and 0.01 respectively for the reweighted loss and the divergence minimization part. We used PyTorch to implement the pipelines and trained networks with Nvidia K80 GPU cards. Codes for reproducing the results as well as preprocessed data can be downloaded with the link 2. We will re-factor the codes and make them publicly available on GitHub after the anonymous review periods.
Results by an average of 10 experiment runs are obtained and we report the two evaluation metrics in Table 1. We report the regularized neural network policies with two Gumbel-softmax sampling schemes, soft Gumbel soft-max (NN-Soft), and straight-through Gumbel soft-max (NN-Hard).
As we can see from the result, by introducing a neural network parametrization of the polices, we are able to improve the test performance by a large margin compared to the baseline CRF policies, as the representation power of networks are often reported to be stronger than other models. The introduction of additional variance regularization term (comparing NN-Hard/Soft to NN-NoReg), we can observe an additional improvement in both testing loss and MAP prediction loss. We observe no significant difference between the two Gumbel soft-max sampling schemes.
2https://www.dropbox.com/sh/etuc8dnxyope1xh/AAAjFJ06cFyJeYr8YN8z996Ta? dl=0
8

Under review as a conference paper at ICLR 2018

5.3 EFFECT OF VARIANCE REGULARIZATION
To study the effectiveness of variance regularization quantitatively, we vary the maximum number of iterations (I in Alg. 3) we take in each divergence minimization sub loop. For example, `NNHard-10' indicates that we use ST Gubmel soft-max and set the maximum number of iterations to 10. Here we didn't use thresholds for divergence as the stopping criterion so that results more comparable. We plot the expected loss in test sets against the epochs average over 10 runs with error bars using the dataset yeast.

Hamming Loss Hamming Loss

7

NN-No-Reg

8

NN-No-Reg

NN-Soft-1

NN-Soft-1

NN-Hard-1

NN-Hard-1

6

NN-Soft-5

7

NN-Soft-5

NN-Hard-5

NN-Hard-5

NN-Soft-10 NN-Hard-10

6

NN-Soft-10 NN-Hard-10

5

5

44

3
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Test Epochs
(a) Test Hamming Loss with Expected Loss

3
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Test Epochs
(b) Test Hamming Loss with MAP Predictions

Figure 1: Stronger regularization can help obtain faster convergence and better test performance.

As we can see from the figure, models with no regularization (gray lines in the figure) have higher loss, and slower convergence rate. As the number of maximum iterations for divergence minimization increases, the test loss decreased faster and the final test loss is also better. This behavior suggests that by adding the regularization term, our learned policies are able to generalize better to test sets, and the stronger the regularization we impose by taking more divergence minimization steps, the better the test performance is.

5.4 GENERALIZATION PERFORMANCE
Our theoretical bounds implies that the generalization performance of our algorithm improves as the number of training samples increases. We vary the number of passes of training data x was passed to the logging policy to sample an action y, and vary it in the range 2[1,2,...,8] with log scales.

Test Hamming Loss (EXP) Test Hamming Loss (MAP)

5.5

NN-No-Reg

3.3

NN-No-Reg

NN-Soft-3

NN-Soft-3

5.0

NN-Soft-5

3.2

NN-Soft-5

NN-Soft-10

NN-Soft-10

4.5 3.1

4.0 3.0

3.5 2.9

3.0 2.8

20 21 22 23 24 25 26 27 28 Replay Count
(a) Test Hamming Loss with Expected Loss

20 21 22 23 24 25 26 27 28 Replay Count
(b) Test Hamming Loss with MAP Predictions

Figure 2: Neural network policies both have increasing performance with increasing number of training data, while models with regularization have faster convergence rate and better performance.

When the number of training samples in the bandit dataset increases, both models with and without regularization have an increasing test performance in the expected loss and reaches a somewhat

9

Under review as a conference paper at ICLR 2018

stable level in the end. Moreover, regularized policies have a better generalization performance compared to the model without regularization constantly. This matches our theoretical intuitions that explicitly regularizing the variance can help improve the generalization ability, and that stronger regularization induces better generalization performance. But as indicated by the MAP performance, after the replay of training samples are more than 24, MAP prediction performance starts to decrease, which suggests the models may be starting over-fitting already.
5.5 TRAINING SCHEMES
In this section, we use some experiments to present the difference in two training schemes: cotraining in Alg. 2 and the easier version Alg. 3. For the second algorithm, we also compare the two Gumbel-softmax sampling schemes in addition, denoted as Gumbel-softmax, and Straight-Through (ST) Gumbel-softmax respectively.

Hamming Losses Hamming Losses

7

No-Reg Gumbel-Softmax

8

No-Reg Gumbel-Softmax

ST-Gumbel-Softmax

ST-Gumbel-Softmax

6

Co-Training =0.01

7

Co-Training =0.01

Co-Training =1

Co-Training =1

Co-Training =10

6

Co-Training =10

5 Co-Training =1000

Co-Training =1000

5

44

3
0 5 10 15 20 25 30 35 40 Test Epochs
(a) Test Hamming Loss with Expected Loss

3
0 5 10 15 20 25 30 35 40 Test Epochs
(b) Test Hamming Loss with MAP Predictions

Figure 3: Results from different training schemes suggest that separately minimizing reweighted loss and divergence is better compared to training the two losses together.

The figures suggest that blending the weighted loss and distribution divergence performs slightly better than the model without regularization, however, the training is much more difficult compared to the separate training scheme, as it's hard to balance the gradient of the two parts of the objective function. We also observe no significant performance difference between the two sampling schemes of the Gumbel-softmax.

5.6 EFFECT OF LOGGING POLICY VS RESULTS
In this section, we discuss how the effect of logging policies, in terms of stochasticity and quality, will affect the learning performance and additional visualizations of other metrics can be found in the Appendix 6.
As discussed before, the ability of our algorithm to learn an improved policy relies on the stochasticity of the logging policy. To test how this stochasticity affects our learning, we modify the parameter of h0 by introducing a temperature multiplier . For CRF logging policies, the prediction is made by normalizing values of wT (x, y), where w is the model parameter and can be modified by  with w  w. As  becomes higher, h0 will have a more peaked distribution, and ultimately become a deterministic policy with   .
We varied  in the range of 2[-1,1,...,8], and report the average ratio of expected test loss to the logging policy loss of our algorithms (Y-axis in Fig 4a, where smaller values indicate a larger improvement). We can see that NN polices are performing better than logging policy when the stochasticity of h0 is sufficient, while after the temperature parameter increases greater than 23, such deterministic logging policies cannot be used for policy improvement purposes. Comparing within NN policies, policies with stronger regularization have slight better performance against models with weaker ones, which in some extent shows the robustness of our learning principle.

10

Under review as a conference paper at ICLR 2018

Ratio Test Hamming Loss (EXP) to h0 Ratio of Test Hamming Loss (EXP) to h0

NN-No-Reg 1.0 NN-Soft-3
NN-Soft-10
0.9

0.8

0.7

0.6

0.5

21

20

21 22 Deterministicity

of

h0

23

24

25

NN-No-Reg

0.75

NN-Soft-3 NN-Soft-10

0.70

0.65

0.60

0.55

0.50 24

23

Quality

2 of

2
h0

21

20

(a) The effect of stochasticity of h0 vs ratio of expected (b) The effect of quality of h0 vs ratio of expected test

test loss

loss

Figure 4: a) The decreasing stochasticity of h0 decreases the model performance, while our regularization can help the model be more robust and achieve better generalization performance. b) As h0 improves, the models constantly outperform the baselines, however, the difficulty is increasing with
the quality of h0. Note: more visualizations of other metrics can be found in the appendix 6.

Finally, we further discusses the impact of logging policies to the our learned improved policies. Intuitively, a better policy that has lower hamming loss can produce bandit datasets with more correct predictions, however, it's also possible that the sampling biases introduced by the logging policy is larger, and such that some predictions might not be available for feedbacks. To study the tradeoff between better policy accuracy and the sampling biases, we vary the proportion of training data points used to train the logging policy from 0.05 to 1, and compare the performance of our improved policies obtained by in Fig. 4b. We can see that as the logging policy improves gradually, both NN and NN-Reg policies are outperforming the logging policy, indicating that they are able to address the sampling biases. The increasing ratios of test expected loss to h0 performance, as a proxy for relative policy improvement, also matches our intuition that h0 with better quality is harder to beat.
6 CONCLUSION
In this paper, we started from an intuition that explicitly regularizing variance can help improve the generalization performance of off-policy learning for logged bandit datasets, and proposed a new training principle inspired by learning bounds for importance sampling problems. The theoretical discussion guided us to a training objective as the combination of importance reweighted loss and a regularization term of distribution divergence measuring the distribution match between the logging policy and the policy we are learning. By applying variational divergence minimization and Gumbel soft-max sampling techniques, we are able to train neural network policies end-to-end to minimize the variance regularized objective. Evaluations on benchmark datasets proved the effectiveness of our learning principle and training algorithm, and further case studies also verified our theoretical discussion.
Limitations of the work mainly lies in the need for the propensity scores (the probability an action is taken by the logging policy), which may not always be available. How to estimate propensity scores and plug the estimation into our training framework will increase the applicability of our algorithms. For example, as suggested by Cortes et al. (2010), directly learning importance weights (the ratio between new policy probability to the logging policy probability) has comparable theoretical guarantees, which might be a good extension for the proposed algorithm.
Although the work focuses on off-policy from logged data, the techniques and theorems may be extended to general supervised learning and reinforcement learning. It will be interesting to study how this training algorithm can work for empirical risk minimization and what generalization bounds it may have as the future direction of research.
11

Under review as a conference paper at ICLR 2018
ACKNOWLEDGEMENTS
We thank Dr. Adith Swaminathan for publishing their codes and answering the authors' questions. We'd also like to thank XXX, YYY, ZZZ for their precious comments and TTT for their funding support.
REFERENCES
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pp. 1638­1646, 2014.
Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust optimization. SIAM review, 53(3):464­501, 2011.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 129­138. ACM, 2009.
Le´on Bottou, Jonas Peters, Joaquin Quin~onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. The Journal of Machine Learning Research, 14(1):3207­3260, 2013.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in neural information processing systems, pp. 442­450, 2010.
John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. arXiv preprint arXiv:1610.02581, 2016.
Miroslav Dud´ik, Dumitru Erhan, John Langford, Lihong Li, et al. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485­511, 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and Bernhard Scho¨lkopf. Covariate shift by kernel mean matching. 2009.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 3389­3396. IEEE, 2017.
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American statistical Association, 47(260):663­685, 1952.
Guido W Imbens. Nonparametric estimation of average treatment effects under exogeneity: A review. The review of Economics and Statistics, 86(1):4­29, 2004.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. arXiv preprint arXiv:1511.03722, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
John Langford, Alexander Strehl, and Jennifer Wortman. Exploration scavenging. In Proceedings of the 25th international conference on Machine learning, pp. 528­535. ACM, 2008.
12

Under review as a conference paper at ICLR 2018
Lihong Li, Re´mi Munos, and Csaba Szepesva´ri. Toward minimax off-policy value estimation. 2015.
Ashique Rupam Mahmood, Huizhen Yu, and Richard S Sutton. Multi-step off-policy learning without importance sampling ratios. arXiv preprint arXiv:1702.03006, 2017.
Andreas Maurer. Empirical bernstein bounds and sample variance penalization. In In COLT. Citeseer, 2009.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054­1062, 2016.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847­5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Alfre´d Re´nyi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics. The Regents of the University of California, 1961.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41­55, 1983.
Pannagadatta Shivaswamy and Thorsten Joachims. Multi-armed bandit problems with history. In Artificial Intelligence and Statistics, pp. 1046­1054, 2012.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit exploration data. In Advances in Neural Information Processing Systems, pp. 2217­2225, 2010.
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert MA~ zller. Covariate shift adaptation by importance weighted cross validation. Journal of Machine Learning Research, 8(May):985­1005, 2007.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pp. 814­823, 2015a.
Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. In Advances in Neural Information Processing Systems, pp. 3231­3239, 2015b.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pp. 2139­2148, 2016.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation in contextual bandits. In International Conference on Machine Learning, pp. 3589­3597, 2017.
Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on, pp. 435­442. IEEE, 2003.
A. STATISTICS OF BENCHMARK DATASETS
We report the statistics of the datasets as in the following table. For the latter two datasets TMC, LYRL, as they have sparse features with high dimension of features, we first reduced their feature dimensions to 1000 via truncated singular value decomposition (latent semantic analysis).
13

Under review as a conference paper at ICLR 2018

Table 2: Dataset Statistics

Name Yeast Scene TMC LYRL

# Features 103 294
30438 47236

# Labels 14 6 22 4

# Train 1500 1211 21519 23149

# Test 917 1196 7077 781265

B. NETWORK STRUCTURES
For policy networks, we use the following simple network structure Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm -> ReLU -> Linear -> Sigmoid , mainly because of features used in the datasets don't invole imaging or original text features. For the discriminator, a three-layer design is Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm -> ReLU -> Linear
C. SUPPLEMENT FIGURES TO SECTION 5.6

Test Hamming Loss (Exp) Test Hamming Loss (Exp) Ratio Test Hamming Loss (MAP) to h0

6.0 NN-No-Reg NN-Soft-3

5.5 5.0

NN-Soft-5 hh00 map

4.5

4.0

3.5

3.0

21

20

21 22 Deterministicity

of

h0

23

24

25

6.0 NN-No-Reg NN-Soft-3

5.5 5.0

NN-Soft-5 hh00 map

4.5

4.0

3.5

3.0

21

20

21 22 Deterministicity

of

h0

23

24

25

1.00 NN-No-Reg

0.98

NN-Soft-3 NN-Soft-10

0.96

0.94

0.92

0.90

0.88

0.86

21

20

21 22 Deterministicity

of

h0

23

24

25

(a) The effect of stochasticity of h0 (b) The effect of stochasticity of h0 (c) The effect of stochasticity of h0

vs expected test loss

vs test loss with MAP predictions vs ratio of test loss with MAP

Figure 5: As the logging policy becomes more deterministic, NN policies are still able to find improvement over h0 in a) expected loss and b) loss with MAP predictions. c) We cannot observe a clear trend in terms of the performance of MAP predictions. We hypothesize it results from that h0 policy already has good MAP prediction performance by centering some of the masses. While NN policies can easily pick up the patterns, it will be difficult to beat the baselines. We believe this
phenomenon worth further investigation.

Test Hamming Loss (EXP) Test Hamming Loss (MAP) Ratio of Test Hamming Loss (MAP) to h0

5.5 5.0 4.5 4.0 3.5 3.0
24

23

Quality

2 of

2
h0

NN-No-Reg NN-Soft-3 NN-Soft-10 h0
2 1 20

3.3 3.2 3.1 3.0 2.9 2.8
24

23

Quality

2 of

2
h0

NN-No-Reg NN-Soft-3 NN-Soft-10 h0 map
2 1 20

NN-No-Reg 1.05 NN-Soft-3
NN-Soft-10
1.00

0.95

0.90

0.85 24

23

Quality

2 of

2
h0

21

20

(a) The quality of h0 vs ratio of ex- (b) The quality of h0 vs ratio of ex- (c) The quality of h0 vs ratio of ex-

pected test loss

pected test loss

pected test loss with MAP

Figure 6: a) As the quality of the logging policy increases, NN policies are still able to find improvement over h0 in expected loss. and b) c) For MAP predictions, however, it will be really difficult for NN policies to beat if the logging policy was already exposed to full training data and trained in a supervised fashion.

14

