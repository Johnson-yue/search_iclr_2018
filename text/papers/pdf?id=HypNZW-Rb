Under review as a conference paper at ICLR 2018
TARGET-AWARE MEMORY NETWORKS FOR ASPECT SENTIMENT CLASSIFICATION
Anonymous authors Paper under double-blind review
ABSTRACT
Memory networks with external memories and the attention mechanism have been used in numerous applications and achieved superior results. Many recent studies have also made great efforts to capture more accurate attentions. In this paper, we focus on solving an important sentiment analysis problem, called aspect sentiment classification (ASC), which requires us to extend the traditional memory networks because in ASC, the classification can be dependent on the target aspect. For example, given two target aspects (which are product features) resolution and price, "higher resolution" and "lower price" express positive opinions while "higher price" and "lower resolution" express negative opinions. We observe that the same word higher (or lower) can indicate different sentiments for different target aspects. Traditional memory networks have difficulty in capturing such target-aware (dependent) sentiment signals. In this paper, we propose the target-aware memory networks to tackle the problem, which is able to model sentiment interaction between target aspects and the context words jointly with the attention mechanism. Our experimental results demonstrate its effectiveness.
1 INTRODUCTION
Memory networks with external memories and the attention mechanism have been applied to many natural language processing (NLP) tasks such as question answering and machine translation. In this work, we focus on solving a particular sentiment analysis problem, called aspect sentiment classification (ASC) (Liu, 2012), using memory networks. Although memory networks have been applied to solve this problem and have given promising results (Tang et al., 2016). However, we found that using traditional memory networks to solve the ASC problem has a major shortcoming. That is, they are not able to model the interaction between aspects and contextual sentiment signals/indicators, which is important in many cases.
Before discussing the above issue, let us first give the task definition of ASC: given a target aspect and a sentence containing the aspect, we want to classify the sentiment/opinion polarity (positive, negative, or neutral) expressed in the sentence about the aspect. The aspects are also referred to as opinion targets (or targets), which are usually product features/attributes in product reviews. Note that in practice, the aspects can be specified by the user or extracted automatically using an existing aspect extraction technique (Liu, 2012). In this work, we assume that the aspects are given.
We use some sample opinion/sentiment sentences to illustrate the ASC task and also the need to model sentiment and aspect interactions. For example, we have four sentences: (1)"the screen resolution is excellent," (2) "the screen resolution is excellent but the price is ridiculous," and (3)"the price is very high," and (4) "the screen resolution is very high." In sentence (1), the sentiment expressed on the aspect picture is positive. In sentence (2), the sentiment expressed on the aspect screen resolution (or resolution for short) is positive but the sentiment expressed on the aspect price is negative. We note that before classifying the sentiment polarity, we first need to identify the sentiment context about the given aspect. Memory networks (MNs) are capable of performing this task because when a specified target is given, its important sentiment context can be captured by the attention mechanism, e.g., "excellent" will be identified as an important sentiment context for resolution and "ridiculous"' will be pointed out for price. Prior studies (Tang et al., 2016) have shown the effectiveness of MNs in tackling this problem. Howevre, if we look at sentences (3) and (4), we realize that the above mechanism is not sufficient. In these two sentences, the sentiment
1

Under review as a conference paper at ICLR 2018

contexts are both "high", but sentence (3) is negative and (4) is positive simply because the target aspects are different. Note that the aspects themselves indicate no sentiment. Thus, in ASC, we need target-aware (or -dependent) sentiment, meaning that the sentiment of a context word is conditioned on the assigned target.
In spite of their superior performance, existing MNs have difficulties to deal with the target-aware sentiment issue. Their capability of attention alignment is definitely useful, because it can point out the important context words for a given aspect, and in many cases the sentiment can be directly inferred from the attended context words. From the above sentence (1), we can see that if the main attention is placed correctly on "excellent", which is an aspect-independent sentiment word, classifying the sentiment expressed as positive is fairly easy. Attentions can help classifying sentence (2) as well. However, for the sentences (3) and (4), placing the correct attention on "high" is necessary but not sufficient, as its sentiment is conditioned on the aspect. We will gain more theoretical insight of this problem with MNs in Section 3. To tackle this problem, we propose target-aware memory networks, which can model target-context sentiment interactions jointly with the attention mechanism in their memories. Our experimental results will demonstrate the effectiveness of our proposed techniques.

2 MEMORY NETWORK FOR ASC

In this section we describe our basic memory network for ASC. It does not include the proposed aspect-context sentiment interaction techniques, which will be introduced in Section 4. The design is similar to previous works (Sukhbaatar et al., 2015; Tang et al., 2016) except we use a different attention alignment function (shown in Eq. 1). Their models will be compared in our experiments as well.
Input Representation: Given a target aspect t  RV , an embedding matrix A  Rd×V is used to convert it to a vector representation, vt (vt = At). Similarly, each context word (each of the other non-aspect words in the sentence) xi  {x1, x2, ...xn} is also projected to the continuous space stored in memory, denoted as mi(mi = Axi)  {m1, m2, ...mn}. Here n is the number of words in a sentence and i indicates the word in position i.
Attention: Attention can be obtained based on the above input representations. Specifically, an attention score i for the context word xi is computed based on the alignment function in Eq. 1, where M  Rd×d is a general learning matrix suggested by Luong et al. (2015).

i = sof tmax(vtT M mi)

(1)

In this manner, attention  = {1, 2, ..n} is represented as a vector of probabilities, indicating the weight/importance of different context words towards a given target aspect.

Output Representation: Another embedding matrix C is used for each context word xi for generating its individual continuous vector ci(ci = Cxi). A response/output vector o is produced by summing over the transformed vectors weighted by the probability vectors .

o = ici
i

(2)

Sentiment Scores: The aspect sentiment scores for classes (positive, neutral, or negative) are then calculated. A sentiment-specific weight matrix W  RK×d is used. The sentiment scores s are represented in a vector s  RK, where K is the number of (sentiment) classes, which is 3 in ASC.

s = W (o + u)

(3)

Final Prediction: The final sentiment probability y is produced with a softmax operation. The class with the highest probability is used as the categorical prediction y^ = max(yk) where k is the index of a sentiment class.

y = sof tmax(s) 2

(4)

Under review as a conference paper at ICLR 2018

3 PROBLEM WITH THE ABOVE MODEL FOR ASC

As indicated in the introduction, in this section we analyze the target-aware sentiment issue in the above model in detail. We first expand the sentiment score calculation from Eq. 3 to its individual terms (Eq. 5). We further transform it to a linear combination of W ci and W vt as shown in Eq. 6.

s = W (o + vt) = W ( ici + vt)

(5)

i
s = W ( ici + vt) = 1W ci + 2W ci + ...nW ci + W vt

(6)

i

A problem with Eq. 6 for ASC is that the sentiment effect of a context word iW ci does not explicitly depend on the target word t. When the final sentiment scores from the context and the target

are summed together, they can be viewed as contributing separately to achieve the final score, which

does not reflect their dependency relationship that we identified earlier. As discussed in introduc-

tion, a context word can express a totally different sentiment on a different target (e.g., "high").

Ideally, the sentiment score for each context word should not be independently calculated without

considering the target. Therefore, Eq.6 has a shortcoming in dealing with the target-aware sentiment.

We use our previous examples to gain further insight. Let us say we have two (target) aspect words
price and resolution (denoted as p and r). We also have two possible context words "high" and
"low" (denoted as h and l). As these two sentiment words can modify both two aspects, we can
construct four snippets "high price", "low price", "high resolution" and "low resolution", which we
can regard as four short review sentences. Their sentiments are negative, positive, positive, negative respectively. Let us set W to be R1×d so s becomes a 1-dimensional sentiment score indicator. s > 0 indicates a positive sentiment and s < 0 indicates a negative sentiment. Based on the above
examples we have four corresponding inequalities: (a) W (hvh + vp) < 0, (b) W (lvl + vp) > 0, (c) W (hvh + vr) > 0 and (d) W (lvl + vr) < 0. We can drop all  terms here as they all equal to 1, i.e., they are the only context word in the snippets to attend to (the target words are not contexts). From (a) and (b) we have (e) W vh < -W vp < W vl). From (c) and (d) we have (f) W vl < -W vr < W vh. From (e) and (f) we have (g) W vh < W vl < W vh, which is a contradiction.

Clearly, real-world review sentences are usually longer than these short expressions, but since the attention mechanism will focus on sentiment signals (e.g., "high" and "low"), we face the same problem. Furthermore, non-sentiment context words should carry no sentiment information. For example, in a sentence like "the price is high", context words "the" and "is" will not help address the above problem.

One may then ask whether the attention mechanism can help address the problem, i.e., the  terms cannot be dropped in a long sentence so they can affect the final results too. However, while the adjustment of attention can affect the final sentiment score, it can only assign higher or lower weight to increase or reduce the sentiment score of a context word. It cannot change the intrinsic sentiment polarity, which is determined by W ci only. We will show examples in our experiments.

4 THE PROPOSED APPROACHES
This section presents our proposed approaches, target-aware memory networks, which include five alternative techniques. This section also discusses related implementation details.
Non-Linear (NL): This is the first approach which utilizes a non-linear projection to couple context and aspect. Instead of directly using the linear combination to calculate the sentiment score as shown in Eq.5, we use a non-linear projection (tanh) as replacement, shown in Eq.7.

s = W · tanh(o + vt) = W · tanh( ici + vt)
i

W · tanh( ici + vt) = W · tanh( i(ci + vt)) = W · tanh( i(ci ))

ii

i

(7) (8)

In this technique, the target vector vt will affect the context by shifting their representation to ci (shown in Eq.8). In this manner, the vector of the same context word will be changed differently

3

Under review as a conference paper at ICLR 2018

depending on different targets. By applying a non-linear projection over ci, the context and aspect are coupled in a way that the final sentiment score cannot be obtained by simply summing their individual contributions (compared with Eq.6). However, by using the non-linear projection for replacement, we lose some interpretability, e.g, we may have difficulty to track how each individual context affects the final sentiment score. To avoid it, we can use the following four techniques.
Interaction Term (IT): The second proposed approach is to add explicit aspect-context iteration items. Specifically, Eq.5 is replaced by Eq. 9. Four new factors are involved, namely, di, dt, Ws, wI (di, dt  Rd×1, WS  Rk×d and wI  Rk×1). Ws models the sentiment effect for ci. di and dt are new vector representations for context i and aspect t produced from a new embedding matrix D, i.e., di = Dxi, dt = Dt. Different from the aforementioned embeddings, D is specifically designed for capturing the sentiment interaction effect between context and aspect words, as they will affect the final sentiment score directly via wI < di, dt >. wI is a sentiment-specific vector and < di, dt > denotes the dot product of the two vectors di and dt. Compared to the basic memory network, this model can capture aspect-aware sentiment signals because the interactions between a context word w and different aspect words (say, p and r) can be different, i.e., < dw, dp >=< dw, dr >.

s = Wso + wI ai < di, dt > = ai(Wsci + wI < di, dt >)
ii

(9)

Coupled Interaction (CI): This proposed technique couples aspect-context interaction with a con-
text output representation, defined by Eq. 10. Specifically, ei is another output representation for xi, which is coupled with aspect-context interaction factor < di, dt >. For each context word xi, ei is generated as ei = Exi where E is an embedding matrix. < di, dt > and ei function together as a target-aware contextual vector and are used to produce sentiment scores with WI (WI  RK×d).

s = Wso + WI ai < di, dt > ei = ai(Wsci + WI < di, dt > ei)
ii

(10)

Joint Coupled Interaction (JCI): A variant of the above model is to replace ei with ci, which means to learn a joint output representation (Eq. 11). This can reduce learning parameters and simplify the
CI model.

s = Wso + WI ai < di, dt > ci = ai(Wsci + WI < di, dt > ci)
ii

(11)

Joint Projected Interaction (JPI): This model also employs a unified output representation like JCI, but a context output vector ci will be projected to two different continuous spaces before sentiment score calculation. To achieve the goal, two projection matrices W1, W2 and the non-linear projection function tanh are used. The intuition is that, when we want to reduce the (embedding) parameters and learn a joint representation, two different sentiment effects may need to be considered in different vector spaces. The two sentiment effects are modeled as two terms shown in Eq.13: the first term can be viewed as a target-independent sentiment effect of a context word while the second term captures the target-aware sentiment interaction. A joint sentiment-specific weight matrix Wj(Wj  RK×d) is used to control and balance the interplay between these two effects.

s = Wj[ i tanh(W1ci) + i < di, dt > tanh(W2ci)]
ii

(12)

= iWj tanh(W1ci) + iWj < di, dt > tanh(W2ci)
ii

(13)

Notes. (1) In the above models their first terms are still needed, because not in all cases a sentiment prediction needs target-aware interaction. In a simple sentence like "the screen resolution is good", the context word "good" can be used as a strong and aspect independent sentiment indicator. (2) The aspect-context interaction can be calculated by other functions. We have tried several methods and found that using the dot product < di, dt > or diW dt (with a projection matrix W ) can produce good results. In fact, diW dt generates slightly better results and we adopt it in our experiments. However, for explanation purposes we still use < di, dt > consistently in this paper.

4

Under review as a conference paper at ICLR 2018

Loss function: The proposed models are all trained in an end-to-end manner by minimizing the
cross entropy loss. Let us denote a sentence and a target aspect as x and t respectively. They appear together in a pair format as input and all such pairs construct the dataset D. gxk,t denotes a gold
sentiment label of a (x, t) pair for sentiment class k. yx,t is a model-predicted sentiment distribution based on the (x, t) pair, and yxk,t denotes its probability for class k. Based on them, the training loss can be constructed as Eq. 14.

loss = -

g(kx,t) log y(kx,t)

(x,t)D kK

(14)

5 RELATED WORK

Studies about memory networks, attention mechanisms and aspect sentiment classification (ASC) are related to our work. Memory networks (Weston et al., 2014; Sukhbaatar et al., 2015), equipped with external storage and the attention mechanism, have become state-of-the-art neural models. The large explicit storage is usually endowed with continuous representation (Graves et al., 2014), which can be effectively used for attention calculation. The attention mechanism has also been widely used in various tasks such as image recognition (Mnih et al., 2014), machine translation (Bahdanau et al., 2014), reading comprehension (Hermann et al., 2015) and sentence summarization (Rush et al., 2015). (Cho et al., 2015) provided a comprehensive review and (Luong et al., 2015) introduced different attention alignment functions. Meanwhile, another track of research lies in learning better attentions, by considering more advanced attention architectures. For example, Kumar et al. (2016) designed a dynamic and recurrent attention method and Kim et al. (2017) proposed to learn structured attention by incorporating graphical models. Different from them, the focus of this work is not on improving attention learning (it does not solve our problem, discussed in section 3), but to make further use of the memory storage for jointly learning target-aware sentiment effects.
Sentiment analysis (Liu, 2012) is an important NLP problem and ASC is a fundamental task of sentiment analysis. Different from document level or sentence level sentiment classification (Pang et al., 2002; Socher et al., 2011; Kim, 2014; Yang et al., 2016), ASC is a fine-grained analysis that identifies sentiment polarities based on given target aspects. The above studies cannot be directly applied to the ASC task as they are not capable of mining finer-grained aspect dependent sentiments. The closest related work to ours comes from Tang et al. (2016), where the authors applied a traditional memory network variant to the ASC task. However, the target-aware sentiment effect is not considered, i.e,. the target-aware sentiment issue remains in their model. To the best of our knowledge, our work is the first to exploit the external memory to jointly capture target-context sentiment interactions, in addition to attention learning.

6 EXPERIMENTS

We perform experiments on two popular review datasets of domains Laptop and Restaurant from SemEval 2014 (Pontiki et al., 2014). In these datasets, aspect-based sentiment polarities are manually labeled as negative, neutral and positive. The training and test sets have also been provided. Each record in each dataset is a {sentence, aspect, sentiment} triple, i.e., a review sentence, an aspect/target and its corresponding aspect sentiment polarity. Our task is to classify the sentiment polarity on an specified (target) aspect in a review sentence. Full statistics of the datasets are shown in Table 1.

Dataset
Restaurant Laptop

Negative 807 870

Train Neutral
637 464

Positive 2164 994

Negative 196 128

Test Neural
196 169

Positve 728 341

Table 1: Statistics of the datasets.

6.1 CANDIDATE MODELS FOR COMPARISON

Baseline 1 (BL1): The classic end-to-end memory network (Sukhbaatar et al., 2015). Baseline 2 (BL2): A state-of-the-art and representative memory network used for aspect sentiment

5

Under review as a conference paper at ICLR 2018

1-hop
BL1 BL2 BL3 NL IT CI JCI JPI

Macro
58.91 63.82 64.34 64.62 65.37 66.78 66.21 66.58

Neg.
57.07 61.76 61.96 64.89 65.22 65.49 65.74 65.44

Neu.
36.81 43.56 45.86 43.21 44.44 48.32 46.23 47.60

Restaurant Pos. 3-hops 82.86 BL1 86.15 BL2 85.19 BL3 85.78 NL 86.46 IT 86.51 CI 86.65 JCI 86.71 JPI

Macro
62.68 66.46 65.71 65.98 68.64 68.49 68.84 67.86

Neg.
60.35 65.57 63.83 64.18 67.11 64.83 66.28 66.72

Neu.
44.57 46.64 46.91 47.86 51.47 53.03 52.06 49.63

Pos.
83.11 87.16 86.39 85.90 87.33 87.60 88.19 87.24

Table 2: Results on dataset Restaurant. Top three F-Macro scores are marked in bold.

1-hop
BL1 BL2 BL3 NL IT CI JCI JPI

Macro
56.16 60.01 62.89 62.63 63.07 63.65 63.09 64.53

Neg.
47.06 52.67 57.16 56.43 57.01 57.33 57.80 58.62

Neu.
45.81 47.89 49.51 49.62 50.62 52.60 50.71 51.71

Laptop Pos. 3-hops 75.63 BL1 79.48 BL2 81.99 BL3 81.83 NL 81.58 IT 81.02 CI 80.76 JCI 83.25 JPI

Macro
60.61 65.16 67.11 67.79 66.23 66.79 67.23 65.16

Neg.
55.59 60.00 63.10 63.17 61.43 61.80 61.08 59.01

Neu.
45.94 52.56 54.53 56.27 53.69 55.30 57.49 54.25

Pos.
80.29 82.91 83.69 83.92 83.57 83.26 83.11 82.20

Table 3: Results on dataset Laptop. Top three F-Macro scores are marked in bold.

classification Tang et al. (2016). The main difference from BL1 is its attention alignment function, which concatenates the distributed representations of the context and aspect, and uses an additional weight matrix for attention calculation, following the method introduced in (Bahdanau et al., 2014). Baseline 3 (BL3): This is our proposed basic memory network, which does not have the targetdependent interaction component. This baseline also allows us to show the effect of modeling target-aware sentiment signal by comparing it with the target-aware models below. Target-Aware models (TA): The 5 proposed approaches Non-Linear (NL), Interaction Terms (IT), Coupled Interaction (CI), Joint Coupled Interaction (JCI) and Joint Projected Interaction (JPI) are used to set 5 different target-aware memory networks.
6.2 TRAINING AND SETTINGS
We use the open-domain word embeddings1 trained on a large Google News corpus for the initialization of all word vectors. We randomize other model parameters from a uniform distribution U (-0.05, 0.05). The dimension of the word embedding and the size of the hidden layers are 300. The learning rate is set to 0.01 and the dropout rate is set to 0.9 for all models. Stochastic gradient descent is used as our optimizer to minimize the cross-entropy error of sentiment classification. We follow other training settings from the study of end-to-end memory network (Sukhbaatar et al., 2015). The position encoding is used, following Sukhbaatar et al. (2015) and Tang et al. (2016). We also compared the candidate models in their multiple-hops version. The number of hops is set to 3 as used in the above previous works.
6.3 EVALUATION MEASURE
Since we have a three-class classification task and the classes are imbalanced as shown in Table 1 (i.e., the positive samples are much more populous than the negative and neutral samples), we use F1-score as our evaluation measure. We report both F1-Macro (averaged F1-score) over all classes and all class-based F1 scores so as to gain a comprehensive insight.
6.3.1 RESULT ANALYSIS
The classification results of Restaurant and Laptop are shown in Table 4 and Table 3. The left part and the right part in each table show the F1 scores for the 1-hop version and 3-hops version of all
1https://github.com/mmihaltz/word2vec-GoogleNews-vectors

6

Under review as a conference paper at ICLR 2018

Data Goal TA model NTA model Analysis
Data Goal TA model NTA model Analysis

Record 1

Sentence

Price was higher when purchased on MAC..

Target

Price

Sentiment

Negative

The interaction effect between target price and context "higher" is important,

in which the sentiment tendency should be negative.

Scores on "higher"

Negative 0.2663 (Correct)

Neutral -0.2604

Positive -0.0282

Scores on "higher"

Negative 0.3641 (Correct)

Neutral -0.3275

Positive -0.0750

Both models correctly assign high negative sentiment scores for "higher".

Record 2

Sentence

(MacBook) Air has higher resolution ...

Target

Resolution Sentiment

Positive

The interaction effect between target resolution and context "higher" is important,

in which the sentiment tendency should be positive.

Scores on "higher"

Negative -0.4729

Neutral -0.3949

Positive 0.9041 (Correct)

Scores on "higher"

Negative 0.2562 (Wrong)

Neutral -0.2305

Positive - 0.0528

While NTA model wrongly assigns the highest score to negative class for "higher", TA correctly gives highest sentiment scores to positive class.

Table 4: Sample Records and Model Comparison between TA and NTA.

models respectively. The top three F-Macro scores are marked in bold in each setting. Based on them, we have the following observations:

1. In the 1-hop setting, we can see significant performance gains achieved by the target-aware models on both Restaurant and Laptop, which are shown by the overall Macro-F1 scores. We should note that the negative (Neg.) and neural (Neu.) classes as minority classes are usually harder to predict due to the lack of training information. However, we can see the target-aware models have better performance consistently due to the target-dependent information modeling . For example, we can see that JCI improves BL3 by 3.78% in the F1 of negative class (Neg.) on Restaurant. Overall, JPI performs the best in this setting.
2. In the 3-hops setting, the target-aware models achieve much better results on Restaurant. JCI, IT and CI achieve the best scores, outperforming the best baseline model BL2 by 2.38%, 2.18% and 2.03% respectively. On Laptop, BL3 and other target-aware models except JPI perform similarly. However, BL3 performs much poorer on Restaurant, only better than BL1.
3. In both the 1-hop and 3-hops settings, the highest scores are always achieved by targetaware models, which implies the usefulness of capturing target-dependent signals. While comparing the target-aware models, we found that JCI is most stable, as it always obtains the top-three scores in both 2 datasets and 2 settings. It may indicate that the joint and coupled output embeddings can help better parameter fitting between different layers/hops.
4. It is important to note that these improvements are quite large considering that in many cases target-aware interactions are not necessary because the sentiment words themselves can already tell the sentiment polarity without knowing the target, e.g., "great battery life", and "good resolution".

6.4 EFFECT OF TARGET-CONTEXT SENTIMENT INTERACTION: CASE STUDIES
Here we present some real sample result records to demonstrate the effectiveness of modeling targetcontext interaction. First, we present two sentences that require target-context interaction for sentiment classification. We then focus on comparing the performance of a target-aware (TA) model and a non-target-aware (NTA) model. Specifically, the JPI and BL3 are used. Other TA/NTA models have similar performance qualitatively to JPI/BL3 in terms of the effect of interaction modeling so

7

Under review as a conference paper at ICLR 2018
we do not list their comparisons. These two representative models are selected because they generally achieve the best classification results in TA and NTA groups in the 1-hop setting. We show the prediction results of their 1-hop version as in this setting the target-context interaction is less complicated and more linearly trackable/separable, meaning that the sentiment contribution (or sentiment score) of a single context word (indexed by i) can be calculated by iWsci (from Eq. 6) and iWjtanh(W1ci) + iWj < di · dt > tanh(W2ci) (from 12) respectively, each of which is a 3-dimensional vector denoting the sentiment scores for negative, neutral, and positive.
Table 3 shows two real records in Laptop. In record 1, the row of Data means that when price is targeted in the sentence "Price was higher when purchased on MAC", the expressed sentiment is negative. The row of Goal indicates that to correctly identify its sentiment polarity, the targetaware interaction between context word "higher" and price in this sentence/snippet is the key. We then report the specific sentiment scores of the word "higher" towards negative, neutral and positive classes in both TA and NTA models in the rows of TA Model and NTA Model. We can see that both of them correctly assign highest sentiment scores to the negative class, as presented in the row of Analysis. We can also observe that in the NTA model the first value (3.3641) of the 3-dimension vector {3.3641, -0.3275, -0.0750} calculated by iWsci is greater than the second (-0.3275) and the third values (-0.0750). Notice that i is always positive (as attention scores range in (0, 1] with softmax), so it can be inferred that the first value in vector Wsci is greater than the other two values. We will refer to this inference again below.
Now we move forward to record 2 in Table 3. In this record the target is Resolution and its sentiment is positive in the sentence "(MacBook) Air has higher resolution". Similarly, in this case we need the correct interaction between aspect resolution and context word "higher". Notice that although we have the same context word "higher", different from record 1, this interaction needs to be positive (see Goal). In rows of TA Model and NTA Model, we find that while the TA model assigns the highest sentiment score of word "higher" to positive class correctly, the NTA model gives it to the negative class wrongly. This error is understandable if we refer to the above inference, i.e., we already know that in this NTA model the first value in vector Wsci is greater than the other two values when i is the word "higher". The reason of this unavoidable mistake is that Wsci is not coupled with the target term. In contrast, Wj < di, ·dt > tanh(W2ci) can adjust the sentiment polarity with the aspect vector dt encoded. Other TA models like JCI can also realize it by WI < di, dt > ci.
One may say that the aspect information (vt) is actually also considered in the form of iWsci + Wsvt in NTA models which should make a difference, i.e., Wsvt may help address the problem given different vt. Yes, it may help, which means in the above example a NTA model can make Wsvresolution favor the positive class and Wsvprice favor the negative class. But then we will have trouble when the context word is "lower", where it requires Wsvresolution to favor negative and Wsvprice to favor positive in order to help. We can see the contradictions. They reflect the theoretical problem discussed in the above section 3.
In addition to this case study, we also found many other interesting target-aware sentiment like "large bill" and "large portion", "small tip" and "small portion" in Restaurant. Furthermore, it is worth noting that the TA model improves the identification of neutral sentiments as well. For instance, it generates the sentiment score vector for the context word "over" for target aspect price as {0.1373, 0.0066, -0.1433} (negative) and for target aspect dinner as {0.0496, 0.0591, -0.1128} (neutral), but the NTA model produces both negative scores {0.0069, 0.0025, -0.0090} and {0.0078, 0.0028, -0.0102} for the two different target aspects.
7 CONCLUSION AND FUTURE WORK
In this paper, we first introduced the aspect-dependent sentiment issue in ASC and indicated the difficulty of modeling it using traditional memory networks. After that, we discussed the basic memory network for ASC and analyzed the reason why it is not capable of capturing aspect-dependent sentiment from a theoretical perspective. We then presented our five alternative approaches for constructing target-aware memory networks. Finally, we reported the experimental results quantitatively and presented some case studies, which demonstrate the effectiveness of our proposed approaches. One direction of future work is to improve the attention learning and target-aware interaction at the same time. We can also try to integrate with other more advanced attention architectures and dependency relationships.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Kyunghyun Cho, Aaron Courville, and Yoshua Bengio. Describing multimedia content using attention-based encoder-decoder networks. IEEE Transactions on Multimedia, 17(11):1875­ 1886, 2015.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pp. 1693­1701, 2015.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.
Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks. arXiv preprint arXiv:1702.00887, 2017.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378­1387, 2016.
Bing Liu. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies, 5(1):1­167, 2012.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In Advances in neural information processing systems, pp. 2204­2212, 2014.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pp. 79­86. Association for Computational Linguistics, 2002.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task4: Aspect based sentiment analysis. In ProWorkshop on Semantic Evaluation (SemEval-2014). Association for Computational Linguistics, 2014.
Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685, 2015.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the conference on empirical methods in natural language processing, pp. 151­161. Association for Computational Linguistics, 2011.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440­2448, 2015.
Duyu Tang, Bing Qin, and Ting Liu. Aspect level sentiment classification with deep memory network. arXiv preprint arXiv:1605.08900, 2016.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.
Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. Hierarchical attention networks for document classification. In HLT-NAACL, pp. 1480­1489, 2016.
9

