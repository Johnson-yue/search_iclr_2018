Under review as a conference paper at ICLR 2018
TOWARDS A GENERALIZATION THEORY AND TEST FOR
GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper we consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalize beyond memorizing the training data. We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalization is. This find that this procedure is, like Wasserstein generative adversarial networks too, sensitive to the choice of ground metric. This is illustrated by using this procedure to assess the performance of several modern generative adversarial network architectures. An improved ground-metricparameterized assessment procedure is proposed and demonstrated to produce sensible assessments. We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently attracted significant interest as a means for performing unsupervised learning. However, recently concerns have been raised about their ability to generalise from training data and their capacity to overfit (Arora & Zhang, 2017; Arora et al., 2017). Moreover, techniques for evaluating the quality of GAN output are either ad hoc, lack theoretical rigor, or are not suitably objective ­ often times "visual inspection" of samples is the main tool of choice for practitioner. More fundamentally, it is sometimes unclear exactly what we want a GAN to do: what is the unsupervised learning task that we are trying to achieve? In this paper, we provide a simple formulation of the GAN training framework, which consists of using a finite dataset to estimate an underlying true distribution. The quality of GAN output is measured quantitatively in terms of a statistical distance D between the estimated and true distribution. Within this context, we propose an intuitive notion of what it means for a GAN to generalise. We also show how our notion of performance can be measured empirically for any given GAN, when D is chosen to be a Wasserstein distance. Unlike other methods (such as the inception score (Salimans et al., 2016)), we show that this choice of D requires us to make no density assumptions about the underlying true distribution. We investigate this choice of D empirically, showing that its performance is heavily dependent on the choice of ground metric underlying the Wasserstein distribution. We suggest a novel choice of ground metric that we show achieves good performance, and discuss how we might use this observation to improve the design of Wasserstein GANs (WGANs).Arjovsky et al. (2017)
2 THE OBJECTIVE OF UNSUPERVISED LEARNING
In order to assess the effectiveness of GANs, it is essential that we define what we mean for a GAN to work well. We do so in this section by providing a simple but general formulation of the goal of unsupervised learning in this context. Our formulation is compatible with most of what is considered in the literature and what is done in practice, but we hope it will allow us to clarify certain details about the task.
1

Under review as a conference paper at ICLR 2018

To begin with, we assume some distribution  on a set X . For instance, X may be the set of 32x32 colour images, and  the distribution from which the CIFAR-10 dataset was sampled. Informally, the goal will be to learn  in a particular sense.

We assume that  is almost completely intractable: we do not know its density (or even if it has

one), and we do not have a procedure to draw new samples from it. However, we do suppose that

we have a fixed dataset X := {x1, · · · , xn}, where xi iid . Equivalently, we have the empirical

distribution

X^ := 1 n

n

xi ,

i=1

where  denotes the Dirac mass.

Denote by P(X ) the set of distributions on X . We wish to use X to produce a distribution (X)  P(X ) that is as "close" as possible to . We measure closeness measured in terms of a function D : P(X ) × P(X )  R. Usually D is chosen to be a statistical divergence, which means that D(P, Q)  0 for all P and Q, with equality iff P = Q. Stated compactly, then, our goal as
designers of unsupervised learning algorithms in this context is to choose  such that D((X), )
is usually small for the sorts of X and  we see in practice.

Now, we can immediately see that one possibility is simply to choose

(X) := X^ .

Moreover, in the case that D is a metric for the weak topology on P(X ), we have that

D(X^ , )  0

almost surely as n  , so that, assuming X is large enough, we can already expect D(X^ , ) to be small. This suggests a natural notion of generalisation in this context: we can say that a choice of  generalises for a given X if

D((X), ) < D(X^ , ).

In other words, using  here actually achieved something: in a sense, it has injected additional information about  into X^ (perhaps through some sort of smoothing or regularisation), and brought
us closer to  than we already were a priori.

3 GENERALISATION IN GANS
How do GANs fit into this framework? Most GAN algorithms in widespread use adhere to the following template. They take as input a distribution P , from which we assume we can sample, and compute (or approximate)
(P ) := arg min D(P, Q)
QQ
for some choices Q  P(X ) and D : P(X ) × P(X )  R. In other words, a GAN takes a distribution P from which only sampling is possible, and (in the ideal case) outputs the Dprojection of P onto Q.
In practice, Q denotes corresponds to the set of pushforward measures G() obtained from a fixed noise distribution  on a noise space Z and some set G of functions G : Z  X . That is,
Q = {G() : G  G} .
Usually G corresponds to the set of functions realisable by some neural network architecture, and  is some multivariate uniform or Gaussian distribution.
For example, the original GAN formulation (Goodfellow et al., 2014) took D to be the JensonShannon divergence, whereas the f -GAN (Nowozin et al., 2016) generalised this to arbitrary f divergences, and the Wasserstein GAN (Arjovsky et al., 2017) advocated the Wasserstein distance. These works also made various recommendations about the choice G through various neural network architectural considerations. Many of the results proved in these papers involve showing that

2

Under review as a conference paper at ICLR 2018

(usually under some assumptions, such as sufficient discriminator capacity) a proposed objective for G is in fact equivalent to D(P, G  ). For instance, the original GAN paper showed that
In terms of our framework in the previous section, then, using a GAN  amounts to choosing

(X) := (X^ ) = arg min D(P, Q).
QQ

It is a crucial point that, in most applications,  receives X^ as input rather than  itself: we only have access to finitely many CIFAR-10 samples, for example. Moreover, training GANs usually involves making many passes over the same dataset, so that, in effect, sampling from P returns many duplicate points over the run. We would not expect this to occur with nonzero probability if P =  for most  of interest.
The observation that P is X^ rather than  itself was recently made by Arora et al. (2017). The authors argue that this introduces a problem for the ability of GANs to generalise, since, if D is a divergence (which is almost always the case), and if Q is too big (in particular, if it is big enough that X^  Q), then we have trivially that (X^ ) = X^ . In other words, the GAN objective appears to actively encourage (X^ ) to memorise the dataset X, and never to produce novel samples from outside it.

The authors' proposed remedy involves trying to find a better choice of D. The problem, they argue, is that popular choices of D do not allow the approximation D(, Q)  D(X^ , Q). They
claim a good D is one that satisfies the condition (at least with high probability):

D(, Q) is small whenever D(X^ , Q) is small.

(1)

They point out that this is certainly violated when D is the Jensen-Shannon divergence, since JS(P Q) = log 2

when one of P and Q is discrete and the other continuous. They give a similar result for the Wasserstein distance in the case that  is Gaussian. As a solution, they introduce the neural network distance DNN, defined by

DNN(P,

Q)

:=

max
f F

ExP

[f (x)]

-

ExQ

[f (x)]

,

for some choice of a class of functions F. They show that, assuming some smoothness conditions on the members of F , the choice D = DNN satisfies (1) with high probability given a fairly modest sample size n.
However, (1) does not seem to capture an appealing notion of generalisation for GANs. Ultimately what matters is not that D generalises, but that (X^ ) does. Indeed, fixating solely on (1) invites choosing D so as to bring Q artificially close to  when Q is close to ^ (with "closeness" measured in terms of d). This does not seem intuitively desirable; we see, for instance, that the degenerate choice D0(P, Q) := 0 trivially satisfies (1). Moreover, we also note that, just like DNN, D0 is a pseudometric, and it is therefore unclear what mathematical properties of DNN render it more suitable for estimating  than the obviously undesirable D0. The authors do somewhat acknowledge this shortcoming of DNN, pointing out that, for instance, DNN(P, Q) can be small even if P and Q are "quite different" (in some sense).
The issues raised in (Arora et al., 2017) apply only in the case that Q is too large. In practice, however, Q is heavily restricted (since G is restricted via a choice of neural network architecture), so that a priori we do not know whether (X^ ) = X^ is even possible. As such, we do not see the choice of (X) = (X^ ) necessarily as a bad idea. In fact, this choice falls perfectly within the framework of minimum distance estimation (MDE) (Wolfowitz, 1957; Basu et al., 2011), which involves estimating an underlying distribution by minimising a distance measure to a given empirical distribution.
We believe therefore that it is an open question as to how well GANs actually work in the sense described in the previous section. In other words, what is the value of D((X^ ), )? We turn to considering this question in the next section.

3

Under review as a conference paper at ICLR 2018

4 TESTING GANS

Our goal in this section is to compute D((X), ) for (X) = (X^ ). This raises some difficulties, given that  is intractable. Our approach is to take D to be the first Wasserstein distance Wd defined by

Wd(P, Q) = inf

d(x, y) d(x, y),

(P,Q) X ×X

where d is a metric on X referred to as the ground metric, and (P, Q) denotes the set of joint

distributions on the product space X × X with marginals P and Q.

Before proceeding, we point out that it still makes sense to choose D = Wd even if D = Wd. In our setup, D defines our end goal, whereas D simply defines a means to (possibly) achieve that goal, namely by allowing us to project onto Q. If our goal is to minimise Wd((X), ), it is still conceivable that the original GAN formulation might be useful, even though in this case D is the Jenson-Shannon divergence ­ perhaps the choice of Q regularises X^ in such a way that brings us closer in Wd sense also.
Our choice of the Wasserstein distance means that D is sensitive to the topology of the underlying set X , which we control by our choice of d. Moreover, Wd metricizes weak convergence for the Wasserstein space Pd(X ) defined by

Pd(X ) := P  P(X ) : d(x0, y) dP (y) <  for some x0  X
X

(Villani, 2008). Consequently, if we denote by A a set of samples a1, · · · , am iid (X^ ), and by Y a set of samples (separate from X) y1, · · · , yk iid , with A^ and Y^ the corresponding empirical distributions, then, provided

(X^ ),   Pd(X ),

(2)

we have that D(A^, Y^ )  D((X^ ), ) almost surely as min {m, k}  . Note that condition (2)

holds automatically in the case that (X , d) is compact, since then Pd(X ) = P(X ).

As such, to estimate D((X), ), for D = Wd, we propose the following. Before training, we divide all the samples from  that we have into a training set X and a testing set Y . We train

our GAN on X, obtaining (X). We then take samples A from (X), and obtain an estimate

of D((X), ) by computing D(A^, Y^ ), which can be done exactly by solving a linear program

(Villani, 2003). We can also use the same methodology to estimate D(X^ , ) by D(X^ , Y^ ), which

allows us to check whether

D((X^ ), ) < D(X^ , )

(3)

by checking whether

D(A^, Y^ ) < D(X^ , Y^ ).

(4)

4.1 RESULTS
We used our methodology to test two popular GAN varieties: the DCGAN (Radford et al., 2015), and the Improved Wasserstein GAN (I-WGAN) (Gulrajani et al., 2017).
4.1.1 L2 AS GROUND METRIC
Initially, we took our ground metric d to be the L2 distance, which has the appealing property of making (X , d) compact when X is a space of RGB or greyscale images, therefore ensuring that (2) holds. We found that this choice produced nice results for the I-WGAN. Figure 1 shows the value of WL2 (G^, Y^ ) over multiple training runs for MNIST and CIFAR-10. In both cases, this curve decays towards an asymptote that nicely corresponds to the visual quality of the samples produced, examples of which are shown in Figures 6 and 7 in the appendix. Moreover, we see that WL2 (G^, Y^ ) is much closer to WL2 (X^ , Y^ ) towards the tail end of training for MNIST than for CIFAR-10. This seems to reflect the fact that, visually, the eventual MNIST samples produced by the I-WGAN do seem to match more closely the true MNIST distribution than the I-WGAN CIFAR-10 samples, which are easily identified as "fake".

4

Under review as a conference paper at ICLR 2018

32 12 30

WL2(G, Y)

WL2(G, Y)

10 28

8 6
0 10000 20000 30000 40000 50000 60000 70000 Batch

26
24
22 0

10000 20000 30000 40000 50000 60000 70000 Batch

Figure 1: WL2 (G^, Y^ ) over training run for I-WGAN trained on MNIST (left) and CIFAR-10 (right). WL2 (X^ , Y^ ) is shown dashed in both cases.

WL2(G, Y)

57.5 55.0 52.5 50.0 47.5 45.0 42.5 40.0
0

500 1000 1500 2000 2500 3000 3500 4000 Batch

Figure 2: WL2 (G^, Y^ ) over training run for DCGAN trained on MNIST. WL2 (X^ , Y^ ) is shown dashed.

However, we did not obtain such a nice result when we ran the same experiment on CIFAR-10 samples produced by a DCGAN. In fact, the WL2 (G^, Y^ ) curves produced, shown in Figure 2 apparently show that G^ is closer to Y^ than X^ very early on in training, at around batch 500. As observed in Figure 8 in the appendix, the visual quality of samples at this stage is very poor. This is both intriguing and concerning: our metric considers 10000 samples whose appearance resembles the heavily blurred Figure 8b to be closer to Y^ than 10000 actual  samples sourced from X.
Motivated by the visual blurriness of the samples in Figure 8b, we explored the effect on WLp (X^ , Y^ ) of blurring the CIFAR-10 training set X. In particular, we let X and Y each consist of 10000 distinct CIFAR-10 samples in X. We then independently convolved each channel of each image with a Gaussian kernel having standard deviation , obtaining a blurred dataset (X) and corresponding empirical distribution (X^ ). The visual effect of this procedure is shown in Figure 9 in the appendix. We then computed WLp ((X^ ), Y^ ) with  ranging between 0 and 10 for a variety of values of p. The results of this experiment in the case p = 2 are shown in Figure 3, and similar results were observed for other values of p: in all cases, we found that
WLp (X^ , Y^ ) < WLp ((X^ ), Y^ )
whenever  > 0. That is, blurring X by any amount brings X^ closer to Y^ in WLp than not. This occurs even though X is distributed identically to Y (both being drawn from ), while (X) (presumably) is not when  > 0.

5

Under review as a conference paper at ICLR 2018

WL2( (X), Y) WL2 ( (X), Y)

22.0 21.5 21.0 20.5 20.0 19.5 19.0 18.5 18.0
0 2 4 6 8 10

395 390 385 380 375
02468

Figure 3: Effect of  on Wd((X^ ), Y^ ) with X and Y obtained from CIFAR-10, for d = L2 (left) and d = L2   (right)

4.1.2 EMBEDDED L2 AS GROUND METRIC
To remedy these issues, we sought to replace L2 with a choice of d that is more naturally suited to the space of images in question. Our idea involves mapping X through a fixed pre-trained neural network  into a feature space Y, and then computing distances using some metric dY on Y, rather than in X directly. It is easily seen that, provided  is injective, dY   : X × X  R defined by
dY  (x, x ) := dY ((x), (x ))
is a metric. It also holds that, when  is (d, dY )-continuous, (X , dY  ) is compact when (X , d) is: given a sequence xi  X , there exists a subsequence xi that converges in d to some x (by compactness), so that dY  (xi , (x)) = dY ((xi ), (x))  0 by continuity of . Neither of these conditions on  ­ injectivity and (d, dY )-continuity ­ are unreasonable to expect from a typical neural network trained using stochastic gradient descent (at least, when dY is a typical metric such as an Lp distance). Consequently, WdY constitutes a valid metric on PdY(X ), and (2) is automatically satisfied.
To test the performance of WdY, we repeated the blurring experiment described above. We took (x) to be the result of scaling x  X to size 224x224, mapping the result through a DenseNet-121 (Huang et al., 2016) pre-trained on ImageNet (Deng et al., 2009), and extracting features immediately before the linear output layer. Under the same experimental setup as above otherwise, we obtained the plot of WL2((X^ ), Y^ ) shown in Figure 3. Happily, we now see that this curve increases monotonically as  grows in accordance with the declining visual quality of (X) shown in Figure 9.
Next, we reran our earlier experiments, this time computing WL2(A^, Y^ ) over the course of training. In all cases, we observed that the value of this quantity reflected very well the visual quality of the training samples. For the I-WGAN, we used the same  as above, and obtained the results on MNIST and CIFAR-10 shown in Figure 4. (Note that on MNIST we modified first duplicated each input across three channels for shape compatibility with the DenseNet.) For the DCGAN on CIFAR-10, we obtained the curve shown in Figure 5.
In all cases we observe that WL2(A^, Y^ ) decreases monotonically towards an asymptote in a way that accurately summarises the visual quality of the samples throughout the training run. Moreover, in all cases there is a large gap between the eventual value of WL2(A^, Y^ ) and WL2(X^ , Y^ ), which reflects the fact that the GAN samples produced are ultimately distinguishable from real samples from . In particular, we see an improvement in this respect for the I-WGAN on MNIST: previously, the final value of WL2 (A^, Y^ ) was essentially the same as WL2 (X^ , Y^ ), despite the fact that it is still quite easy to distinguish between real and generated samples (see e.g. the various mistakes that are discernible throughout Figure 6)
5 DISCUSSION AND FUTURE WORK
We believe our work reveals two promising avenues of future inquiry. First, we suggest that WLp is an appealing choice of D, both due to its nice theoretical properties ­ it metricizing weak con-

6

Under review as a conference paper at ICLR 2018

400 395 380 390 360 385

WL2 (G, Y) WL2 (G, Y)

340 380

320 0

10000 20000 30000 40000 50000 60000 70000 Batch

375 0

20000

40000 60000 Batch

80000

Figure 4: WL2(G^, Y^ ) over training run for I-WGAN trained on MNIST (left) and CIFAR-10 (right). WL2(X^ , Y^ ) is shown dashed in both cases.

WL2 (G, Y)

402 400 398 396 394 392 390 388 386
0

500 1000 1500 2000 2500 3000 3500 4000 Batch

Figure 5: WL2(G^, Y^ ) over training run for DCGAN trained on MNIST. WL2(X^ , Y^ ) is shown dashed.

7

Under review as a conference paper at ICLR 2018
vergence, and does not require us to make any density assumptions about  ­ and due to its sound empirical performance demonstrated above. It would be very interesting to use this D to produce a systematic and objective comparison of the performance of all current major GAN implementations, and indeed to use this as a metric for assessing future GAN designs. We also view the test (4) as potentially useful for determining whether our algorithms are overfitting. This would be particularly so if applied via a cross-validation procedure: if we consistently observe that (4) holds when training a GAN according to many different X and Y partitions of our total  samples, then it seems reasonable to infer that (X) has indeed learnt something useful about .
We also believe that the empirical inadequacy of WL2 that we observed suggests a path towards a better WGAN architecture. At present, WGAN implementations implicitly use WL2 for their choice of D. We suspect that altering this to our suggested WL2 may yield better quality samples. We briefly give here one possible way to do so that is largely compatible with existing WGAN setups. In particular, following Arjovsky et al. (2017), we take
D(P, Q) = max ExP [f (x)] - ExQ [f (x)]
f F
for a class F of functions f : X  R that are all (L2  , dR)-Lipschitz for some fixed Lipschitz constant K. Here dR denotes the usual distance on R. To construct such an F in practice, we can simply consider a space F of (dY , dR)-Lipschitz functions with fixed Lipschitz constant L, which may be done for instance via weight-clipping like Arjovsky et al. (2017), or via a gradient penalty like Gulrajani et al. (2017). Provided  is itself (d, dY )-Lipschitz with some Lipschitz constant M (which, for a neural network with fixed weights, can easily be checked), we then obtain a KM Lipschitz function f : X  R by taking
f (x) := h((x)).
Moreover, for our suggested , this still enables us to compute the gradient of f via the chain rule, since  is a neural network and therefore assumed to be differentiable. We have begun some experimentation in this area but leave a more detailed empirical inquiry to future work.
It is also clearly important to establish better theoretical guarantees for our method. At present, we have no guarantee that the number of samples in A and Y are enough to ensure that
D(A^, Y^ )  D((X), Y^ )
(perhaps with some fixed bias that is fairly independent of , so that it is valid to use the value of D(A^, Y^ ) to compare different choices of ), or that (4) entails (3) with high probability. We do however note that some recent theoretical work on the convergence rate of empirical Wasserstein estimations (Weed & Bach, 2017) does suggest that it is plausible to hope for fast convergence of D(A^, Y^ ) to D((X), Y^ ). We also believe that the convincing empirical behaviour of WL2 does suggest that it is possible to say something more substantial about our approach, which we leave to future work.
6 RELATED WORK
The maximum mean discrepancy (MMD) is another well-known notion of distance on probability distributions, which has been used for testing whether two distributions are the same or not Gretton et al. (2012) and alo for learning generative models in the style of GAN Li et al. (2015); Dziugaite et al. (2015); Sutherland et al. (2016); Li et al. (2017). It is parameterized by a characteristic kernel k, and defines the distance between probability distributions by means of the distance of k's reproducing kernel Hilbert space (RKHS). The MMD induces the same weak topology on distributions as the one of the Wasserstein distance. Under a mild condition, the MMD between distributions P and Q under a kernel k can be understood as the outcome of the following two-step calculation. First, we push-forward P and Q from their original space X to a Hilbert space H (isomorphic to k's RKHS) using a feature function  : X  H induced by the kernel k. Typically, H is an infinite-dimensional space, such as the set 2 of sqaure-summable sequences in R as in Mercer's theorem. Let P and Q be the resulting distributions on the feature space. Second, we compute the supremum of EP (X)[f (X)] - EQ (X)[f (X)] over all linear 1-Lipschitz functions f : H  R. The result of this calculation is the MMD between P and Q.
8

Under review as a conference paper at ICLR 2018

This two-step calculation shows the key difference between MMD and our use of Wasserstein distance and neural embedding . While the co-domain of the feature function  is an infinitedimensional space (e.g. 2) in most cases, its counterpart  in our setting uses a finite-dimensional space as co-domain. This means that the MMD possibly uses a richer feature space than our approach. On the other hand, the MMD takes the supremum over only linear functions f among all 1-Lipschitz functions, whereas the Wasserstein distance considers all these 1-Lipschitz functions. These difference balancing acts between the expressiveness of features and that of functions taken in the supremum affect the learning and testing of various GAN approaches as observed experimentally in the literature. One interesting future direction is to carry out systematic study on the theoretical and practical implications of these differences.

REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. arXiv:1701.07875, 2017.

Wasserstein gan.

arXiv preprint

Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and Equilibrium in Generative Adversarial Nets (GANs). arXiv preprint arXiv:1703.00573, 2017.

Ayanendranath Basu, Hiroyuki Shioya, and Chanseok Park. Statistical inference: the minimum distance approach. CRC Press, 2011.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.

Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The Netherlands, pp. 258­267, 2015.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scho¨lkopf, and Alexander J. Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723­773, 2012.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.

Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.

Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnaba´s Po´czos. MMD GAN: towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584, 2017.

Yujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 1718­1727, 2015.

Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

9

Under review as a conference paper at ICLR 2018 Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016. Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. arXiv preprint arXiv:1611.04488, 2016. Ce´dric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003. Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017. Jacob Wolfowitz. The minimum distance method. The Annals of Mathematical Statistics, pp. 75­88, 1957.
10

Under review as a conference paper at ICLR 2018
A EXAMPLE I-WGAN SAMPLES

(a) Batch 0

(b) Batch 600

(c) Batch 5000

(d) Batch 10000

(e) Batch 50000

(f) Batch 69800

Figure 6: Samples from I-WGAN trained on MNIST

11

Under review as a conference paper at ICLR 2018

(a) Batch 0

(b) Batch 500

(c) Batch 5000

(d) Batch 10000

(e) Batch 50000

(f) Batch 69750

Figure 7: Samples from I-WGAN trained on CIFAR-10

12

Under review as a conference paper at ICLR 2018
B EXAMPLE DCGAN SAMPLES

(a) Batch 0

(b) Batch 500

(c) Batch 1000

(d) Batch 2000

(e) Batch 3000

(f) Batch 3920

Figure 8: Samples from I-WGAN trained on CIFAR-10 test set

13

Under review as a conference paper at ICLR 2018
C EFFECT OF BLURRING CIFAR-10

(a) Unblurred

(b)  = 1

(c)  = 2

(d)  = 3

(e)  = 4

(f)  = 6

(g)  = 8

(h)  = 10

Figure 9: Effect of different  on (X), for X the CIFAR-10 training set

14

