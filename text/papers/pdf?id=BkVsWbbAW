Under review as a conference paper at ICLR 2018
DEEP GENERATIVE DUAL MEMORY NETWORK FOR CONTINUAL LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time ­ this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.
1 INTRODUCTION
Many machine learning models, when trained sequentially on tasks, forget how to perform the previously learnt tasks. This phenomenon is referred to as catastrophic forgetting and is prominent in neural networks (McCloskey & Cohen, 1989). Without a way to avert catastrophic forgetting, a learning system needs to be presented with all the training data at once, and cannot be retrained on new data thereafter, unless all the previous data is also stored and presented again at the time of retraining. Hence, it is an important and challenging problem which needs to be solved in order to enable systems to learn continuously.
McCloskey & Cohen (1989) first suggested that the underlying cause of forgetting was the distributed shared representation of tasks via network weights ­ the very property that gives neural networks their remarkable abilities to generalize well. Following this, subsequent works attempted to remedy the issue by reducing representational overlap between input representations via activation sharpening algorithms (Kortge, 1990), orthogonal recoding of inputs (Lewandowsky, 1991) or orthogonal activations at all hidden layers (McRae & Hetherington, 1993; French, 1994). More recent works have explored activations like dropout (Goodfellow et al., 2015) and local winner-takes-all (Srivastava et al., 2013) to create sparse, less correlated feature representations. But such sparse encodings can be task specific at times and in general act as heuristics to mildly pacify the underlying problem.
Further, it is known that natural cognitive systems are also connectionist in nature and yet they only forget gradually and not `catastrophically'. For instance, humans demonstrate catastrophic forgetting only in cases of direct brain damage, but otherwise forget very gradually and systematically. Frequently and recently encountered tasks tend to survive much longer in the human memory, while those rarely encountered are slowly forgotten. Even though some of the earlier tasks may be seen again, this is not necessary for them to be retained in memory (French, 1999). Hence we believe that sparsifying representations does not address the problem. Instead, neuroscientific evidence suggests that human brain has evolved mechanisms to separately learn on new incoming data and
1

Under review as a conference paper at ICLR 2018

consolidate the learning with previous knowledge to avert catastrophic forgetting (McClelland et al., 1995; O'Neill et al., 2010; French, 1999).
Complementary learning systems: McClelland et al. (1995) suggested that this separation has been achieved in the human brain via evolution of two separate areas of the brain, the hippocampus and the neocortex. The neocortex is a long term memory specializing in consolidating incoming knowledge with what has been previously learnt, to gradually learn the joint structure of all tasks and experiences, whereas the hippocampus acts as a temporary memory to rapidly learn new tasks and then slowly transfer the knowledge to neocortex after acquisition.
Experience replay: Another factor deemed essential for sequential learning is experience replay. McClelland et al. (1995); O'Neill et al. (2010) have emphasized the importance of replayed data patterns in the human brain during sleep and waking rest. Robins (1995; 2004) proposed several replay techniques (a.k.a. pseudopattern rehearsal) to achieve replay, but they involved generating replay data without storing input representations and our experiments show that they lack the accuracy required for consolidation.
Weight consolidation or freezing: Recent evidence from neuroscience also suggests that mammalian brain protects knowledge in the neocortex via task-specific consolidation of neural synapses over long periods of time (Yang et al., 2014; Benna & Fusi, 2016). This technique has recently been emulated in progressive neural networks (Rusu et al., 2016) and Pathnets (Fernando et al., 2017) both of which freeze neural network weights after learning tasks. Kirkpatrick et al. (2017) have recently used the fisher information matrix (FIM) to consolidate network weights, which store previously learnt knowledge, by slowing down learning on them.
In this paper, we address the catastrophic forgetting problem by drawing inspiration from the above neuroscientific insights, and present a method to overcome catastrophic forgetting. More specifically, we propose a dual-memory model for sequential multitask learning while averting catastrophic forgetting. Our model comprises of two generative models: a short-term memory (STM) to emulate the human hippocampal system and a long term memory (LTM) to emulate the neocortical learning system. The STM learns new tasks without interfering with previously learnt tasks in the LTM. The LTM stores all previously learnt tasks and aids the STM in learning tasks similar to previous tasks. During sleep/down-time, the STM generates and transfers samples of learnt tasks to the LTM which are gradually consolidated with the LTM's knowledge base of previous tasks via generative replay.
Our approach is inspired from the strengths of deep generative models, experience replay and the complementary learning systems literature. We demonstrate our method's effectiveness in averting catastrophic forgetting by sequentially learning multiple tasks. Moreover, our experiments shed light on some characteristics of human memory as observed in the psychology and neuroscience literature.

2 SEQUENTIAL MULTITASK LEARNING

Formally, our continual learning problem setting can be called sequential multitask learning. It is
characterized by a set of tasks T, which are to be learnt by a model parameterized by weights  (e.g. a
neural network). From here on, we will use the the phrase model and neural network interchangeably.
In this work we mainly consider supervised learning tasks i.e. task t  T has training examples: {xti, yit}i=1:Nt for xti  X and yit  Y, but our model easily generalizes to unsupervised learning settings. The training examples for each task come sequentially i.e. the model sees examples from
one task at a time, and the total number of tasks |T| is not known a priori.

Finite memory assumption: To impose a finite memory size, we further assume that any training

algorithm employed in this setting can store some examples from each task if needed, but the

storage size (Nmax) is limited and might be smaller than the total number of examples from all

tasks combined

|T| t=1

Nt

. Hence, algorithms are not allowed to store all training examples and

re-learn on them when new tasks arrive. In case of algorithms employing generative models, the same

restriction still applies i.e. the generative models are not allowed to keep more than Nmax examples

in storage (generated + stored combined).

At test time, the model can be asked to predict the label yt  Y for any previously seen/unseen example xt  X from any task t  T. Our goal is to devise an algorithm which learns these tasks

2

Under review as a conference paper at ICLR 2018
sequentially while avoiding catastrophic forgetting and can achieve a test loss close to that of a model which learnt all the tasks jointly.
3 DEEP GENERATIVE DUAL MEMORY NETWORK
The idea of replaying experience to a neural network dates back to the 1990s and has been used for various reinforcement learning tasks (Lin, 1993; Mnih et al., 2015). A study by O'Neill et al. (2010) suggests that experience replays occurs in the human brain during sleep and waking rest and aids in consolidation of learnt experiences.
3.1 GENERATIVE EXPERIENCE REPLAY
We propose that any experience replay system must be generative in nature, to accurately generate samples of previously learnt tasks for experience replay. This is a much better option than storing all samples in replay memory buffers as is common in reinforcement learning (Mnih et al., 2015), since sampling from a generative model is synonymous to sampling from the density of the seen inputs and automatically provides the most frequently encountered samples. Moreover, this can be done even with a fixed budget on total memory whereas storing all (or a fraction of) samples from previous tasks requires knowledge of the total number of tasks |T|, the number of examples per task Nt and the frequency of occurrence of samples a priori, which are often not available when tasks arrive sequentially. Previously proposed non-generative approaches to experience replay e.g. pseudopattern rehearsal (Robins, 1995; French, 1997; Robins, 2004) propose to preserve neural networks' learnt mappings by arbitrarily sampling random inputs and their corresponding outputs from the neural networks and using them along with new task samples while training. These approaches have only been tested in small binary input spaces in previous works, and our experiments in section 4 show that sampling random inputs in high-dimensional spaces (e.g. images) does not preserve the mapping learnt by neural networks.
Figure 1: Deep Generative Memory
To implement generative experience replay, we first introduce a sub-model called the Deep Generative Memory (DGM) as shown in Figure 1. It consists of three elements: (i) a generative model (called the generator), (ii) a feedforward network (called the learner), and (iii) a dictionary (Ddgm) with task IDs of tasks learnt so far along with number of times the tasks were encountered. We call this sub-model a memory because of its weights and learning capacity (not due to any recurrent connections). Our model assumes the availability of unique task IDs for tasks, which are required to distinguish tasks from each other and to identify repetitive tasks (which many previous works do not address). In this work, we assume the availability of a task oracle which provides these task IDs. The design of this oracle is not central to our learning algorithm and in practice a hidden markov model based inference scheme like that used in (Kirkpatrick et al., 2017) would suffice. We use variational autoencoders (VAEs) (Kingma & Welling, 2014) as our choice of generative model for the generator, since as shown in section 3.2, our generative model also requires reconstruction capabilities. Deep Generative Replay: To train a DGM, we employ the following replay scheme ­ Given the task dictionary of the DGM Ddgm, the new incoming samples (X, Y ) and the task dictionary for the new tasks Dtasks (since the new samples could be from multiple tasks), the DGM first computes
3

Under review as a conference paper at ICLR 2018
the fraction of total samples that should come from the incoming samples and the fraction to come from generated previous task samples, in proportion to the number of tasks (counting repetitions) in Dtasks and Ddgm respectively. Next, this fraction is tweaked in favor of the incoming task samples (if needed) to ensure that each new task gets at least a minimum fraction  of the DGM's maximum capacity Nmax. The minimum ratio  ensures that as the DGM saturates with tasks over time, it still allows learning new tasks at the cost of losing performance on the least frequent previous task samples (since they would appear least frequently in the self-generated DGM samples). Lastly, the actual number of samples to be generated from the previous tasks and the number of samples to be retained from the currently incoming tasks are computed, while obeying the earlier computed fractions and the maximum memory capacity (Nmax). Keeping the total number of samples  Nmax sometimes requires subsampling X and Y , if the DGM is being trained on many new tasks. Let the subsampled (or otherwise) external task data be X~ and Y~ . The DGM then generates the computed number of samples of its previously learnt tasks (Xgen) using the generator and their corresponding labels (Ygen) using the learner. The reconstruction of the total data {X~ , Xgen} from the generator (hence we use a VAE) and {Y~ , Ygen} is then used to train the generator and the learner of the DGM. Doing this final reconstruction helps makes the learner's input robust to noisy and distorted data as shown in section 5.2. We point out that a similar idea has also been proposed recently by Shin et al. (2017) independently, where they use a generative model to sample previously seen inputs and a feedforward network (solver) to learn task labels. But (i) they use Wasserstein GAN (Arjovsky et al., 2017) as their choice of generative model, (ii) they always train the solver on generated and obtained samples without reconstructing the obtained data, which makes their input representations less robust to noise and distortions, (iii) they do not use task IDs for replay and hence gain no advantage when learning an already learnt task, and (iv) they do not describe any rule for balancing the ratio between the new samples and the generated samples.
3.2 DUAL MEMORY NETWORKS
We posit that a good continual learning system needs to be fast at acquiring new tasks, but at the same time should also protect previously learnt tasks without losing performance on them. It is easy to see that these two requirements are conflicting in nature and hard to satisfy simultaneously. Hence, inspired by nature's solution to this problem, we propose a dual memory network to combat catastrophic forgetting.
Figure 2: Deep Generative Dual Memory Network (DGDMN)
Our model (DGDMN) shown in Figure 2 comprises of a large deep generative memory (DGM) called the long-term memory (LTM) which maintains information about all previously learnt tasks, thereby emulating the human neocortical learning system, and a short-term memory (STM) which behaves similar to the human hippocampal system and learns new incoming tasks quickly without interfering with the existing performance on the previous tasks. The STM is a collection of small, dedicated, task-specific deep generative memories (called short-term task memory ­ STTM), which can each learn one unique task. Whenever a new task comes in, if it is already in an STTM, the same STTM is used to retrain on it, otherwise a fresh STTM in the STM is allocated for the task. Additionally, if the task has been consolidated into the LTM previously, then the LTM reconstructs the incoming samples
4

Under review as a conference paper at ICLR 2018
for that task using the generator (hence we use a VAE), predicts labels for the reconstructions using its learner and sends these newly generated samples to the STTM allocated to this task. This provides extra samples on tasks which have been learnt previously and helps to learn them better, while also preserving the previous performance on that task to some extent.
Once all STTMs in the STM are exhausted (say after learning nST M tasks), the architecture sleeps (like humans do) in order to consolidate all tasks in the STM into the LTM and free up the STTMs for new incoming tasks. While asleep, the STM generates samples of learnt tasks from all STTMs and sends them to the LTM, where these tasks are consolidated while preserving all the LTM's previous knowledge via deep generative replay (see Figure 2).
Testing with our architecture can be done after training on all tasks or even during intermediate steps. While predicting task labels for task t, if any STTM currently contains task t, it is used to predict the labels, else the prediction is deferred to the LTM. This allows us to take advantage of all the tasks seen uptil now (including the most recent ones).
Note that after learning many tasks, the LTM would need a larger total number of samples to consolidate all previously learnt tasks. But due to the maximum limit (Nmax), this cannot be done indefinitely. Hence, our generative replay strategy gradually reduces the fraction of samples per previously learnt task and causes slow forgetting of previous tasks as opposed to catastrophic forgetting. This also causes an eventual slowdown in learning new tasks, although they continue to be learnt because of the minimum fraction of LTM () granted to them for every consolidation. This is synonymous to how learning slows down in humans as they age, but forgetting is only gradual and not catastrophic (French, 1999).
4 EXPERIMENTS
We perform experiments to demonstrate forgetting on sequential image classification tasks. We briefly describe our datasets here (see section 8 (appendix B) for a detailed description).
Permnist: An existing benchmark dataset in continual learning literature (Goodfellow et al., 2015; Kirkpatrick et al., 2017), where each new task consists of classifying some fixed permutation of pixels on images from the MNIST dataset LeCun et al. (1998). This way each task is as hard as MNIST and the tasks share some common underlying structure. Digits: We introduce this smaller dataset which contains 10 tasks with the tth task being classification of digit t from the MNIST dataset. TDigits: This is another transformed variant of MNIST with all the ten digits, and their reflections as classification tasks making a total of 40 tasks. This dataset poses similar difficulty as the Digits dataset and we use it for experiments involving longer sequence of tasks.
4.1 BASELINES
Along with our model (DGDMN), we test several baselines for catastrophic forgetting, which are briefly presented below (more details in section 8 (appendix B):
Feedforward neural networks (NN): We use these to characterize the forgetting in the absence of any prevention mechanism, and as a datum for other approaches. Neural nets with dropout (DropNN): Goodfellow et al. (2015) suggested using dropout as a means to prevent representational overlaps and pacify catastrophic forgetting. Pseudopattern Rehearsal (PPR): A non-generative approach to experience replay (Robins, 2004). Elastic Weight Consolidation (EWC): Kirkpatrick et al. (2017) proposed using the Fisher Information Matrix for task-specific consolidation of weights in a neural network. Deep Generative Replay (DGR): We strip away the STM from DGDMN to separate the effects of experience replay and dual memory architecture. This leaves only the LTM, and each new task is consolidated directly into the LTM via deep generative replay.
4.2 TESTS
Unlike previous works, we have refrained from using large neural network classifiers in our experiments. We observed that large networks have many excessive parameters and can more easily adapt to
5

Under review as a conference paper at ICLR 2018
the sequentially incoming tasks, thereby masking the severity of the catastrophic forgetting problem. Hence, we chose smaller networks with fewer parameters, so the networks have to share all their parameters appropriately amongst the various tasks to achieve a reasonable accuracy on any dataset. Consequently, we achieve less absolute accuracy on tasks as compared to the known state-of-the-art with aptly regularized large neural networks, but we emphasize that obtaining state-of-the-art accuracies on these tasks is not central to our goal. Instead, we focus on preserving the achieved accuracies on sequentially incoming tasks.
4.2.1 ACCURACY AND FORGETTING CURVES

(a) NN

(b) DropNN

(c) PPR

(d) EWC

(e) DGR

(f) DGDMN

Figure 3: Accuracy curves for Permnist; x-axis shows tasks encountered and y-axis shows classification accuracy on each task.

We trained DGDMN and all above baselines sequentially on the 10 image classification tasks of Permnist and Digits datasets (separately). The classification accuracy on a held out test set for each task, after training on the tth task has been shown in Figures 3 and 4. We used the same network architecture for each of NN, PPR, EWC, learner in DGR, and learner in the LTM of DGDMN (for a given dataset). DropNN also had the same architecture, but with two intermediate dropout layers after each hidden layer (see section 8 (appendixB) for more details).
We observe from Figures 3a and 3b, that NN and DropNN can forget catastrophically and their accuracy on initially learnt tasks can decrease abruptly when they learn new tasks (see Task 6). Further, we see that DropNN can actually perform worse than NN and at times displays higher forgetting (see Task 6). This confirms that though sparse representations may be helpful, they rely on the neural network being of high enough capacity to be able to sparsify the representation Goodfellow et al. (2015) and may not perform well if the network does not have redundant weights available for sparsification.
EWC forgets less than NN and DropNN, but it rapidly slows down learning on many of its network weights and its learning approximately stagnates after Task 3. Though it is able to prevent forgetting in the learnt tasks, it does not really learn new tasks effectively after Task 3 (e.g. see Tasks 5 and 6 in Figure 3d). This happens because of excessive slowdown of learning on many weights, which hinders EWC from utilizing those weights later on to jointly discover common structures in its learnt tasks and the new incoming ones. Note that the networks do have the capacity to learn all tasks and we observe that our generative replay-based algorithms DGR and DGDMN outperform all other baselines, by learning all tasks sequentially with the same learner network (Figures 3e, 3f).
We observed heavy forgetting on Digits (Figure 4) for most baselines, which is expected because all samples in the tth task have a single label (t) and so the tth task can be learnt on its own by setting the tth bias of the softmax layer to be high and the other biases low. But doing this, causes catastrophic forgetting. We observed that NN, DropNN, PPR and EWC learnt only the task being

6

Under review as a conference paper at ICLR 2018

(a) NN

(b) DropNN

(c) PPR

(d) EWC

(e) DGR

(f) DGDMN

Figure 4: Accuracy curves for Digits; x-axis shows tasks encountered and y-axis shows classification accuracy on each task.

(a) Permnist

(b) Digits

Figure 5: Average forgetting curves; x-axis shows tasks encountered and y-axis shows average classification accuracy averaged over all tasks encountered so far.

trained on, and forgot all previous knowledge immediately. Sometimes, we also observed saturation due to the softmax bias being set very high and then being unable to recover from it. PPR showed severe saturation since its replay prevented it from coming out of the saturation.

Our methods DGR and DGDMN still retain performance on all tasks when learning sequentially

on Digits (but forget more than on Permnist), and our replay strategy prevents saturation by nicely

balancing the ratios of new incoming samples and generated samples from previous tasks. The

average forgetting on all tasks  {1, . . . , t}, after training on the tth task (for both Digits and

Permnist) is shown in Figure 5. For absolute reference, the accuracy of NN by training it jointly on

all tasks uptil the tth task has also been shown for each t. We again observe that DGR and DGDMN

outperform baselines in terms of retained average accuracy. In figure 5b, NN, DropNN, PPR and

EWC

are

only

able

to

learn

one

task

at

a

time

and

follow

nearly

overlapping

curves

(acc



1 t

).

We also point out that though PPR involves experience replay, it does not compare against the performance of DGR and DGDMN (see Figures 3c, 4c). Although, it does preserve its learnt mapping around the points randomly sampled from its input domain, these random samples are not close to real images and hence it fails to preserve performance on real images. These observations substantiate our claim that any good experience replay mechanism must model the input domain accurately and hence needs to be generative in nature.

7

Under review as a conference paper at ICLR 2018
We believe that datasets like Digits, which contain tasks with highly correlated input (and/or output) samples are very important benchmarks for continual learning algorithms for two main reasons: (i) Such high correlation amongst task samples promotes quick overfitting to the new incoming task and therefore causes heavy catastrophic forgetting. Being able to retain performance on such tasks is a strong indicator of the efficacy of a continual learning algorithm. (ii) Humans also learn tasks by seeing many correlated samples together in a short duration of time. For instance, kids are taught to read and write a single alphabet per day in kindergarten by first showing them many examples of that alphabet and then asking them to write the same, many times on that day. Since NN, DropNN and PPR cannot compete against other approaches, we carry forward our experiments only on EWC, DGR and DGDMN from this point onwards.
4.3 REPEATED TASKS AND REVISION
(a) (b)
Figure 6: Accuracy curves when tasks are revised: (a) EWC, (b) DGDMN.
It is well known in psychology literature that human learning improves via revision i.e. by repeating learnt tasks after some time (Kahana & Howard, 2005; Cepeda et al., 2006). Most existing works do not investigate revision but we emphasize that it is crucial for learning continuously and a continual learning algorithm should improve its performance on tasks under revision. We show the performance of EWC and DGDMN (DGR performs very similar to DGDMN) on Permnist, when some tasks are repeated in the end (Figure 6). It is easily seen that DGDMN learns all tasks uptil Task 6, then benefits by revising Task 1 again (accuracy goes up), and somewhat for Task 6 (since it did not forget Task 6 substantially). However, EWC faces stagnation again and its learning halts quickly. Interestingly, its performance also does not improve much on Task 1 after revising it i.e. once learning has been slowed down on the weights important for Task 1, the weights can never be used again (not even for improving on Task 1 itself). Further, it did not learn Task 6 well the first time, and revision does not help much either. We reiterate that DGDMN, by its design, benefits significantly from revision. If an STTM learns a repeated task, which is also present in the LTM, the LTM provides extra reconstructed samples for that task to the STTM. If the task is already in the same STTM (in case of very quick revision before the task ever got consolidated into the LTM), the STTM being a deep generative memory preserves its previous contents via deep generative replay and hence reinforces the learning of the task with extra samples. Lastly, if a task already known to the LTM is being re-consolidated, then there would be more samples of that task in the LTM as compared to a freshly learnt task (some from the STTM and the rest generated by the LTM). The ability to learn from correlated task samples and revision makes our memory architecture somewhat similar to that of humans functionally.
4.4 LONG SEQUENCES OF TASKS
So far we have noticed similar performance of DGR and DGDMN on all tasks. To explore the role of the dual memory architecture, and differentiate between DGDMN and DGR, we trained both these algorithms on a long sequence of 40 tasks from the TDigits dataset. We limited Nmax to 120, 000 samples for this task to explore the case where the LTM in DGDMN (DGM in DGR) cannot regenerate as many samples as in the full dataset, and has to forget some tasks. At least  = 0.05
8

Under review as a conference paper at ICLR 2018
fraction of memory was ensured to each new task while consolidating and consolidation for DGDMN happened after every nST M = 5 tasks.
(a) (b)
Figure 7: Accuracy curves for TDigits on: (a) tasks encountered so far, (b) last 10 encountered tasks.
The average forgetting curves vs. tasks encountered are plotted in Figure 7a. We immediately note that DGDMN and DGR start at an average accuracy of 1.0, but start dropping after about 10 tasks since the LTM (DGM for DGR) begins to saturate. While DGDMN drops slowly and ends up retaining about 40% accuracy on all tasks in the end, DGR drops down below 20% accuracy. This is because DGR consolidates its DGM too often with very few new samples and mostly samples from its own DGM. Since, any generator would have some error, a DGM's self-generated samples are always slightly different from the real-world samples and hence the error compounds quite fast for DGR. DGDMN, on the other hand, uses small STTMs to hold many tasks and transfers them simultaneously to the LTM. A consequence of this is that DGDMN consolidates its LTM less often and with more accurate real-looking samples, hence its error accumulates much slower. Note that the STTM representations are also erroneous (since they are also being generated), and we explore the effect of this error in section 5.1. Since we plotted figure 7a under severe constraints of memory, forgetting was bound to happen. But note that forgetting in our model is very gradual and not catastrophic as seen for NN, DropNN, PPR etc. on Digits dataset. However, a better idea would be to measure average accuracy on the last few tasks encountered (we choose 10), to see if the method can retain performance on the most recent tasks. We show this in Figure 7b and we see that DGDMN indeed oscillates around 80% average accuracy on the last 10 encountered tasks. However, DGR's frequent memory consolidation propagates errors too fast and its accuracy drops even with this metric. However, an even bigger benefit of having a dual memory system reveals itself when we look at the training time for the above 40 tasks. This is shown in Figure 8a and we observe an order of magnitude of difference between DGDMN and DGR in training time. This is clearly because STTMs are smaller and much faster to train than the LTM. LTM with its bigger architecture requires many more samples to consolidate. Consolidation is a costly process and should not be done for every task. Hence, learning multiple tasks quickly in the STM and holding them till sleep is clearly a better alternative than sleeping after every task. This provides a speed advantage to DGDMN and allows us to learn tasks quickly while only consolidating them periodically. Note that the dual memory architecture is a critical design choice for scalability and has also emerged naturally in humans, in the form of the complementary learning systems and the need to sleep periodically. Even though sleeping is a dangerous behavior for any organism (since it can be harmed or attacked by a predator while sleeping), it has still survived through eons of evolution and never been lost Joiner (2016). In fact, most organisms with even a slightly developed nervous system (centralized or diffuse) display either sleep or light-resting behavior Nath et al. (2017). Our experiment demonstrates the importance of sleep in organisms, since without the dual memory architecture intertwined with periodic sleep, learning would be very short lived and highly time consuming (as in DGR).
9

Under review as a conference paper at ICLR 2018
(a) (b) Figure 8: (a) Training time for DGDMN and DGR on TDigits, (b) Accuracy curves for DGDMNalternative without STM on Digits.
5 ANALYSIS AND DISCUSSION
In this section we present more analysis of DGDMN. Some visualizations of the learnt latent structures in the LTM when trained jointly vs. sequentially have been deferred to section 7 (appendix A).
5.1 WHY DO WE NEED A SHORT-TERM MEMORY? We showed in section 4.4, that the STM learns multiple tasks and holds them till sleep, after which they are transferred to the LTM. Such periodic consolidation obviates the need to learn new tasks directly in the LTM and hence provides a speed advantage. However, the LTM always learns from representations of the STTMs, and never from real data. Since the STTMs also have < 100% accuracy (both for generation and classification), these errors propagate into the LTM when it learns representations from STTMs. An alternative to get rid of the STM could be to directly keep the data from new incoming tasks around, consolidate it into the LTM after periodic intervals, and then discard the data. We show the accuracy curves on the Digits dataset for this approach in Figure 8b and note that this results in higher retention than when using an STM (compare with DGDMN in Figure 4), because the LTM now learns from the real data rather than from imprecise representations of STTMs. However, this approach is not truly online, since the recently learnt tasks cannot be used immediately after learning, unlike in DGDMN where any learnt task can be evaluated upon (since the tasks in the STM also count as learnt). In this case, we need to wait till the next consolidation (sleeping) period before the recently learnt tasks can be put to use. Knowing that the STM's error can be made smaller by using higher capacity generators and classifiers, we recommend using a STM over its alternative to have true online learning.
5.2 THE CHOICE OF THE UNDERLYING GENERATIVE MODEL Our consolidation ability and retention performance relies heavily on the generation and reconstruction ability of the underlying generative model. We chose a VAE to have the ability to reconstruct samples, which is required whenever our DGMs (both LTM and STTMs) learn and also when LTM aids the STM by generating extra samples of previously learnt tasks to assist in its learning. Further, using a VAE provides us resilience to noisy and corrupted samples, since before learning or predicting on any task sample, we always reconstruct the sample (both in STTMs and LTM). The reconstructed images are less noisy and can also recover from partial occlusion, which gives our model human-like abilities to recognize objects in noisy, distorted or occluded images. We tested our LTM model and a non-generative NN model, both jointly trained on Digits data, by making them classify noisy and occluded images. Figure 9a shows the LTM's reconstruction of noisy and occluded samples. Figure 9b shows the plot of classification accuracy (y-axis) with increasing standard deviation of the added gaussian noise (x-axis) for both the LTM and the NN models. We
10

Under review as a conference paper at ICLR 2018

(a) (b)

(c)

Figure 9: Resilience of LTM: (a) LTM reconstruction from noisy and occluded digits, (b) Plot of classification accuracy with increasing gaussian noise, and (c) Plot of classification accuracy with increasing occlusion.

see that the LTM is more robust to noisy images and has a smoother degradation in classification accuracy because of its denoising reconstructive properties. A similar effect is seen in Figure 9c, where the x-axis shows the increasing occlusion factor (width of the central occluding path as fraction of total image width) and we observe robustness of the LTM's classification performance as opposed to the faster degradation in an NN model.
However, we point out that VAE is not the only generative model that can be used with our architecture. In fact, our dual memory network is agnostic to the choice of the underlying generative model as long as it can generate reliable samples and reconstruct incoming samples accurately. Hence, our model can also work with variants of generative adversarial networks (GANs) (Goodfellow et al., 2014) such as the conditional GAN (Mirza & Osindero, 2014). Another recent work by Mescheder et al. (2017) which improves the generative performance of VAEs by learning the posterior distribution implicitly (like in GANs), can also replace the VAE as the choice of generative model. We leave the evaluation over different generative models as future work.

5.3 CHOOSING HYPERPARAMETERS OF DGDMN

Our model introduces two new hyperparameters: (i) nST M : number of tasks learnable in STM before consolidation, and (ii) : minimum fraction of maximum DGM samples (Nmax) which an incoming task should get while consolidating the DGM via generative replay. Both of these have straightforward interpretations in our model and hence can be set directly to control the forgetting rate and speed of learning, almost independent of the dataset we are learning on. We discuss these and present other hyperparameters in section 8 (appendix B).

 ensures continual incorporation of new tasks by guaranteeing them at least a minimum fraction of

LTM samples during the consolidation process. Without a lower limit (), the LTM would always

generate samples obeying the ratio of the number of previously learnt tasks to that of the newly

incoming ones. If the LTM is saturated with lots of previous tasks, the newly incoming tasks would

receive decreasingly fewer samples and wouldn't be learnt effectively. Having the lower limit ()

lets the newer tasks be incorporated continuously, at the expense of slowly forgetting older and less

frequently observed tasks, similar to how humans forget gradually and naturally. We provide an

approximate way to set  for the LTM to perform well on the last K tasks seen after learning a

long sequence of tasks. We observed that it is safe to assume that about 50% of the LTM would be

crowded by tasks seen before the last K, and the remaining 0.5 fraction should be distributed to the

last

K

tasks.

Hence

choosing



=

0.5 K

works

well

in

practice

(or

as

a

good

starting

point

for

further

tuning). We made this same choice in section 4.4 with K = 10 and  = 0.05, and hence plotted the

average accuracy over the last 10 tasks as a metric.

nST M controls the consolidation cycle frequency. Increasing nST M means more tasks can be stored in the STM before a consolidation is required, thereby resulting in a learning speed advantage. But
this also implies that fewer samples of previous tasks would participate in consolidation (due to

11

Under review as a conference paper at ICLR 2018

maximum sample capacity Nmax of LTM), and hence more forgetting would occur. This parameter

does not affect learning much during the early stages of learning, till the LTM remains unsaturated

(and Nmax capacity is not violated by generated + new samples combined), but becomes effective

after that. For long sequences of tasks, we found it best to keep at least 75% of the total samples from

previously

learnt

tasks

to

have

appropriate

retention.

Hence,

nST M

can

be

set

as

approximately

0.25 

in practice, or as a starting point for further tuning (as we did in section 4.4).

6 CONCLUSION
In this work, we have developed a model capable of learning continuously on sequentially incoming tasks, while averting catastrophic forgetting. Our model employs a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We have shown that generative replay performs the best for long-term performance retention even for neural networks with small capacity, while demonstrating the benefits of using generative replay and a dual memory architecture via our experiments. Our model hyperparameters have simple interpretations and can be set independently of datasets without much tuning. Moreover, our architecture displays many characteristics of the human memory and provides useful insights about the connection between sleep and learning in humans.
Though our architecture draws inspiration from complementary learning systems and experience replay in the human brain, there is also considerable neuroscientific evidence for synaptic consolidation in the human brain (like in EWC). It would be interesting to explore how synaptic consolidation can be incorporated in our dual memory architecture without causing stagnation, and we leave this to future work. We also plan to extend our architecture to learn optimal policies over time via reinforcement learning without maintaining explicit replay memories.

12

Under review as a conference paper at ICLR 2018
REFERENCES
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. In International Conference on Machine Learning (ICML), 2017.
Marcus K Benna and Stefano Fusi. Computational principles of synaptic memory consolidation. Nature neuroscience, 2016.
Nicholas J Cepeda, Harold Pashler, Edward Vul, John T Wixted, and Doug Rohrer. Distributed practice in verbal recall tasks: A review and quantitative synthesis. Psychological bulletin, 132(3): 354, 2006.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.
Robert M French. Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference. network, 1111:00001, 1994.
Robert M French. Pseudo-recurrent connectionist networks: An approach to the'sensitivitystability'dilemma. Connection Science, 9(4):353­380, 1997.
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3 (4):128­135, 1999.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems (NIPS), pp. 2672­2680, 2014.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2015.
Geoffrey Hinton. Neural networks for machine learning - lecture 6a - overview of mini-batch gradient descent, 2012.
William J Joiner. Unraveling the evolutionary determinants of sleep. Current Biology, 26(20): R1073­R1087, 2016.
Michael J Kahana and Marc W Howard. Spacing and lag effects in free recall of pure lists. Psychonomic Bulletin & Review, 12(1):159­164, 2005.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2014.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, pp. 201611835, 2017.
Chris A Kortge. Episodic memory in connectionist networks. In Proceedings of the 12th Annual Conference of the Cognitive Science Society, volume 764, pp. 771. Erlbaum, 1990.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Stephan Lewandowsky. Gradual unlearning and catastrophic interference: A comparison of distributed architectures. Relating theory and data: Essays on human memory in honor of Bennet B. Murdock, pp. 445­476, 1991.
Long-Ji Lin. Reinforcement learning for robots using neural networks. PhD thesis, Fujitsu Laboratories Ltd, 1993.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579­2605, 2008.
13

Under review as a conference paper at ICLR 2018
James L McClelland, Bruce L McNaughton, and Randall C O'reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109­165, 1989.
Ken McRae and Phil A Hetherington. Catastrophic interference is eliminated in pretrained networks. In Proceedings of the 15h Annual Conference of the Cognitive Science Society, pp. 723­728, 1993.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Ravi D Nath, Claire N Bedbrook, Michael J Abrams, Ty Basinger, Justin S Bois, David A Prober, Paul W Sternberg, Viviana Gradinaru, and Lea Goentoro. The jellyfish cassiopea exhibits a sleep-like state. Current Biology, 27(19):2984­2990, 2017.
Joseph O'Neill, Barty Pleydell-Bouverie, David Dupret, and Jozsef Csicsvari. Play it again: reactivation of waking experience and memory. Trends in neurosciences, 33(5):220­229, 2010.
Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2): 123­146, 1995.
Anthony Robins. Sequential learning in neural networks: A review and a discussion of pseudorehearsal based methods. Intelligent Data Analysis, 8(3):301­322, 2004.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Neural Information Processing Systems (NIPS), 2017.
Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen Schmidhuber. Compete to compute. In Neural Information Processing Systems (NIPS), pp. 2310­2318, 2013.
Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma, Wei Li, and Wen-Biao Gan. Sleep promotes branch-specific formation of dendritic spines after learning. Science, 344(6188):1173­1178, 2014.
14

Under review as a conference paper at ICLR 2018
7 APPENDIX A
7.1 JOINTLY VS. SEQUENTIALLY LEARNT STRUCTURE To explore whether learning tasks sequentially results in a similar structure as learning them jointly, we visualized t-SNE (Maaten & Hinton, 2008) embeddings of the latent vectors of the LTM generator (VAE) in DGDMN, after training it (a) jointly over all tasks (Figure 10a), and (b) sequentially over tasks seen one at a time (Figure 10b) on the Digits dataset. To maintain consistency, we used the same random seed in t-SNE for both joint and sequential embeddings.
(a) (b) Figure 10: t-SNE embedding for latent vectors of the VAE generator on Digits dataset when: (a) tasks are learnt jointly, and (b) tasks are learnt sequentially. We observe that the LTM's latent space effectively segregates the 10 digits in both cases (joint and sequential). Though the absolute locations of the digit clusters differ in the two plots, the relative locations of digits share some similarity between both plots i.e. the neighboring digit clusters for each cluster are roughly similar. This may not be sufficient to conclude that the LTM discovers the same latent representation for the underlying shared structure of tasks in these cases, and we leave a more thorough investigation to future work. 7.2 VISUALIZATIONS FOR THE JOINTLY AND SEQUENTIALLY LEARNT LTM We also show visualizations of digits from the LTM when trained jointly on Digits tasks (Figure 11a) and when trained sequentially (Figure 11b). Though the digits generated from the jointly trained LTM are quite sharp, the same is not true for the sequentially trained LTM. We observe that the sequentially trained LTM produces sharp samples of the recently learnt tasks (digits 6, 7, 8 and 9), but blurred samples of previously learnt tasks, which is due to partial forgetting on these previous tasks.
(a) (b) Figure 11: Visualization of digits from LTM when trained: (a) jointly, (b) sequentially
15

Under review as a conference paper at ICLR 2018
8 APPENDIX B
8.1 DATASET PREPROCESSING
All our datasets have images of size (28, 28) pixels with intensities normalized in the range [0.0, 1.0].
Permnist: Our version involved six tasks, each containing a fixed permutation on images sampled from the original MNIST dataset. We sampled 30, 000 images from the training set and all the 10, 000 test set images for each task. The tasks were as follows: (i) Original MNIST, (ii) 8x8 central patch of each image blackened, (iii) 8x8 central patch of each image whitened, (iv) 8x8 central patch of each image permuted with a fixed random permutation, (v) 12x12 central patch of each image permuted with a fixed random permutation, and (vi) mirror images of MNIST. Digits: This dataset contains 10 tasks with the tth task being classification of digit t from MNIST. TDigits: Transformed variant of MNIST containing all ten digits, their mirror images, their upside down images, and their images when reflected about the main diagonal making a total of 40 tasks.
8.2 TRAINING ALGORITHM AND ITS PARAMETERS
All models were trained with RMSProp (Hinton, 2012) using learning rate = 0.001,  = 0.9, = 10-8 and no decay. We used a batch size of 128 and all classifiers were provided 20 epochs
of training when trained jointly, and 6 epochs when trained sequentially over tasks. For generative models (VAEs), we used gradient clipping in RMSProp with clipnorm= 1.0 and clipvalue= 0.5, and they were always trained for 25 epochs regardless of the task or dataset involved.
8.3 NEURAL NETWORK ARCHITECTURES
We chose all our models by first training them jointly on all tasks in adataset to ensure that our models had enough capacity to perform reasonably well on all tasks. But we gave preference to simpler models over very higher capacity models.
Classifier Models: Our implementation of NN, DropNN, PPR, EWC, learner for DGR and the learner for LTM in DGDMN used a neural network with three fully-connected layers with the number of units tuned differently according to the dataset (24, 24 units for Digits, 48, 48 for Permnist and 36, 36 for TDigits). DropNN also added two dropout layers, one after each hidden layer with droput rate = 0.2 each. The two hidden layers used ReLU activations, the last layer had a softmax activation, and the model was trained to minimize the cross-entropy objective function. The learners for STTMs in DGDMN were kept smaller for speed and efficiency concerns.
Generative models: The generators (VAE) for DGR and LTM of DGDMN employed an encoder and decoder with two fully connected hidden layers each with ReLU activation. The sizes and number of units/kernels in the layers were tuned independently for each dataset with an approximate coarse grid-search. The size of the latent variable z was set to 32 for Digits, 64 for Permnist and 96 for TDigits. The STTM generators for DGDMN were kept smaller for speed and efficiency concerns.
8.4 ALGORITHM SPECIFIC HYPERPARAMETERS
PPR: We used a maximum memory capacity of about 6 times the number of samples in a task for the dataset being learnt on (i.e. 18, 000 for Digits and 60, 000 for Permnist). While replaying, apart from the task samples, the whole remaining memory capacity was filled with previous samples.
EWC: Most values of the coefficient of the Fisher Information Matrix based regularizer between 1 to 500 worked reasonably well for our datasets. We chose 100 for our experiments.
DGR and DGDMN: Nmax for the DGM in DGR and for the LTM in DGDMN for Digits and Permnist was set as the total number of samples in the datasets (summed over all tasks) to ensure that there was enough capacity to regenerate the datasets well. For TDigits, we deliberately restricted memory capacity to see the effects of learning tasks over a long time and we kept Nmax as half the total number of samples in TDigits. nST M was kept at 2 for both Digits and Permnist, and 5 for TDigits.  was set to be small, so that it does not come into play for Digits and Permnist since we already provided memories with full capacity for all samples. For TDigits, we used  = 0.05 which would let us incorporate roughly 10 out of the 40 tasks well.
16

