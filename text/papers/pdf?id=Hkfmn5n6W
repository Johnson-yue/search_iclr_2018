Under review as a conference paper at ICLR 2018

EXPONENTIALLY VANISHING SUB-OPTIMAL LOCAL
MINIMA IN MULTILAYER NEURAL NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT

Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., "near" linear separability), or an unrealistically wide hidden layer with  (N ) units.

Results: We examine a MNN with one hidden layer of piecewise linear units, a

single output, and a quadratic loss. We prove that, with high probability in the

limit of N   datapoints, the volume of differentiable regions of the empiric

loss containing sub-optimal differentiable local minima is exponentially vanishing

in comparison with the input of dimension d0 =

s~ame Nvolu,manedoaf

global minima, given standard more realistic number of d1 = ~

normal (N/d0)

hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d0  16 hidden neurons.

1 INTRODUCTION
Motivation. Multilayer Neural Networks (MNNs), trained with simple variants of stochastic gradient descent (SGD), have achieved state-of-the-art performances in many areas of machine learning (LeCun et al., 2015). However, theoretical explanations seem to lag far behind this empirical success (though many hardness results exist, e.g., (Síma, 2002; Shamir, 2016)). For example, as a common rule-of-the-thumb, a MNN should have at least as many parameters as training samples. However, it is unclear why such over-parameterized MNNs often exhibit remarkably small generalization error (i.e., difference between "training error" and "test error"), even without explicit regularization (Zhang et al., 2017a).
Moreover, it has long been a mystery why MNNs often achieve low training error (Dauphin et al., 2014). SGD is only guaranteed to converge to critical points in which the gradient of the expected loss is zero (Bottou, 1998), and, specifically, to local minima (Pemantle, 1990) (this is true also for regular gradient descent (Lee et al., 2016)). Since loss functions parameterized by MNN weights are non-convex, it is unclear why does SGD often work well ­ rather than converging to sub-optimal local minima with high training error, which are known to exist (Fukumizu & Amari, 2000; Swirszcz et al., 2016). Understanding this behavior is especially relevant in important cases where SGD does get stuck (He et al., 2016) ­ where training error may be a bottleneck in further improving performance.
Ideally, we would like to quantify the probability to converge to a local minimum as a function of the error at this minimum, where the probability is taken with the respect to the randomness of the initialization of the weights, the data and SGD. Specifically, we would like to know, under which conditions this probability is very small if the error is high, as was observed empirically (e.g., (Dauphin et al., 2014; Goodfellow et al., 2015)). However, this seems to be a daunting task for realistic MNNs, since it requires a characterization of the sizes and distributions of the basins of attraction for all local minima.
Previous works (Dauphin et al., 2014; Choromanska et al., 2015), based on statistical physics analogies, suggested a simpler property of MNNs: that with high probability, local minima with high error diminish exponentially with the number of parameters. Though proving such a geometric property with realistic assumptions would not guarantee convergence to global minima, it appears to

1

Under review as a conference paper at ICLR 2018

be a necessary first step in this direction (see discussion on section 6). It was therefore pointed out as an open problem at the Conference of Learning Theory (COLT) 2015. However, one has to be careful and use realistic MMN architectures, or this problem becomes "too easy".
For example, one can easily achieve zero training error (Nilsson, 1965; Baum, 1988) ­ if the MNN's last hidden layer has more neurons than training samples. Such extremely wide MNNs are easy to optimize (Yu, 1992; Huang et al., 2006; Livni et al., 2014; Shen, 2016; Nguyen & Hein, 2017). In this case, the hidden layer becomes linearly separable in classification tasks, with high probability over the random initialization of the weights. Thus, by training the last layer we get to a global minimum (zero training error). However, such extremely wide layers are not very useful, since they result in a huge number of weights, and serious overfitting issues. Also, training only the last layer seems to take little advantage of the inherently non-linear nature of MNNs.
Therefore, in this paper we are interested to understand the properties of local and global minima, but at a more practical number of parameters ­ and when at least two weight layers are trained. For example, Alexnet (Krizhevsky, 2014) is trained using about 1.2 million ImageNet examples, and has about 60 million parameters ­ 16 million of these in the two last weight layers. Suppose we now train the last two weight layers in such an over-parameterized MNN. When do the sub-optimal local minima become exponentially rare in comparison to the global minima?

Main contributions. We focus on MNNs with a single hidden layer and piecewise linear units,

optimized using the Mean Square Error (MSE) in a supervised binary classification task (Section

2). We define N as the number of training samples, dl as the width of the l-th activation layer, and

g

(x) < h (x)

as

an

asymptotic

inequality

in

the

leading

order

(formally:

limx

log g(x) log h(x)

<

1).

We

examine Differentiable Local Minima (DLMs) of the MSE: sub-optimal DLMs where at least a

fraction of > 0 of the training samples are classified incorrectly, and global minima where all

samples are classified correctly.

Our main result, Theorem 10, states that, with high probability, the total volume of the differentiable regions of the MSE containing sub-optimal DLMs is exponentially vanishing in comparison to the same volume of global minima, given that:

Assumption 1. The datapoints (MNN inputs) are sampled from a standard normal distribution.

Assumption 2.

N



,

d0

(N

)

and

d1

(N

)

increase 

with

N

,

while

 (0, 1) is a constant1.

Assumption 3. The input dimension scales as N < d0 N .

Assumption 4. The hidden layer width scales as

N

log4 d0

N

< d1< N

.

(1.1)

Importantly, we use a standard, unmodified, MNN model, and make no assumptions on the target
function. Moreover, as the number of parameters in the MNN is approximately d0d1, we require only "asymptotically mild" over-parameterization: d0d1> N log4 N from eq. (1.1). For example, if d0  N , we only require d1> log4 N neurons. This improves over previously known results (Yu, 1992; Huang et al., 2006; Livni et al., 2014; Shen, 2016; Nguyen & Hein, 2017) ­ which require an extremely wide hidden layer with d1  N neurons (and thus N d0 parameters) to remove sub-optimal local minima with high probability.

In section 5 we validate our results numerically. We show that indeed the training error becomes low when the number of parameters is close to N . For example, with binary classification on CIFAR and ImageNet, with only 16 and 105 hidden neurons (about N/d0), respectively, we obtain less then 0.1% training error. Additionally, we find that convergence to non-differentiable critical points does not appear to be very common.

Lastly, in section 6 we discuss our results might be extended, such as how to apply them to "mildly" non-differentiable critical points.

Plausibility of assumptions. Assumption 1 is common in this type of analysis (Andoni et al., 2014; Choromanska et al., 2015; Xie et al., 2016; Tian, 2017; Brutzkus & Globerson, 2017). At first it may
1For brevity we will usually keep implicit the N dependencies of d0 and d1.

2

Under review as a conference paper at ICLR 2018

appear rather unrealistic, especially since the inputs are correlated in typical datasets. However, this no-correlation part of the assumption may seem more justified if we recall that datasets are many times whitened before being used as inputs. Alternatively, if, as in our motivating question, we consider the input to the our simple MNN to be the output of the previous layers of a deep MNN with fixed random weights, this also tends to de-correlate inputs (Poole et al., 2016, Figure 3). The remaining part of assumption 1, that the distribution is normal, is indeed strong, but might be relaxed in the future, e.g. using central limit theorem type arguments.
In assumption 2 we use this asymptotic limit to simplify our proofs and final results. Multiplicative constants and finite (yet large) N results can be found by inspection of the proofs. We assume a constant error since typically the limit  0 is avoided to prevent overfitting.
In assumption 3, for simplicity we have d0 N , since in the case d0  N the input is generically linearly separable, and sub-optimal local minima are not a problem (Gori & Tesi, 1992; Safran & Shamir, 2016). Additionally, we have N < d0, which seems very reasonable, since for example, d0/N  0.016, 0.061 and 0.055 MNIST, CIFAR and ImageNet, respectively.
In assumption 4, for simplicity we have d1< N , since, as mentioned earlier, if d1  N the hidden layer is linearly separable with high probability, which removes sub-optimal local minima. The other bound N log4 N < d0d1 is our main innovation ­ a large over-parameterization which is nevertheless asymptotically mild and improves previous results.
Previous work. So far, general low (training or test) error guarantees for MNNs could not be found ­ unless the underlying model (MNN) or learning method (SGD or its variants) have been significantly modified. For example, (Dauphin et al., 2014) made an analogy with high-dimensional random Gaussian functions, local minima with high error are exponentially rare in high dimensions; (Choromanska et al., 2015; Kawaguchi, 2016) replaced the units (activation functions) with independent random variables; (Pennington & Bahri, 2017) replaces the weights and error residuals with independent random variables; (Baldi, 1989; Saxe et al., 2014; Hardt & Ma, 2017; Lu & Kawaguchi, 2017; Zhou & Feng, 2017) used linear units; (Zhang et al., 2017b) used unconventional units (e.g., polynomials) and very large hidden layers (d1 = poly (d0), typically N ); (Brutzkus & Globerson, 2017; Du et al., 2017; Shalev-Shwartz et al., 2017) used a modified convnet model with less then d0 parameters (therefore, not a universal approximator (Cybenko, 1989; Hornik, 1991)); (Tian, 2017; Soltanolkotabi et al., 2017; Li & Yuan, 2017) assume the weights are initialized very close to those of the teacher generating the labels; and (Janzamin et al., 2015; Zhong et al., 2017) use a non-standard tensor method during training. Such approaches fall short of explaining the widespread success of standard MNN models and training practices.
Other works placed strong assumptions on the target functions. For example, to prove convergence of the training error near the global minimum, (Gori & Tesi, 1992) assumed linearly separable datasets, while (Safran & Shamir, 2016) assumed strong clustering of the targets ("near" linear-separability). Also, (Andoni et al., 2014) showed a p-degree polynomial is learnable by a MNN, if the hidden layer is very large (d1 =  d60p , typically N ) so learning the last weight layer is sufficient. However, these are not the typical regimes in which MNNs are required or used. In contrast, we make no assumption on the target function. Other closely related results (Soudry & Carmon, 2016; Xie et al., 2016) also used unrealistic assumptions, are discussed in section 6, in regards to the details of our main results.
Therefore, in contrast to previous works, the assumptions in this paper are applicable in some situations (e.g., Gaussian input) where a MNN trained using SGD might be used and be useful (e.g., have a lower test error then a linear classier).

2 PRELIMINARIES AND NOTATION

Model. We examine a Multilayer Neural Network (MNN) with a single hidden layer and a

scalar output. The MNN is trained on a finite training set of N datapoints (features) X

x(1), . . . , x(N)  Rd0×N with their target labels y

y(1), . . . , y(N)  {0, 1} N ­ each

datapoint-label pair x(n), y(n) is independently sampled from some joint distribution PX,Y . We

3

Under review as a conference paper at ICLR 2018

define W = [w1, . . . , wd1 ]  Rd1×d0 and z  Rd1 as the first and second weight layers (bias terms are ignored for simplicity), respectively, and f (·) as the common leaky rectifier linear unit
(LReLU (Maas et al., 2013))

f (u) ua (u) with a (u)

1 , if , u > 0 ,
 , if u < 0

(2.1)

for some  = 1 (so the MNN is non-linear) , where both functions f and a operate component-wise

(e.g., for any matrix M: (f (M))ij = f (Mij)). Thus, the output of the MNN on the entire dataset

can be written as

f (WX) z  RN .

(2.2)

We use the mean square error (MSE) loss for optimization

MSE 1 e 2 with e y - f (WX) z , N

(2.3)

where · is the standard euclidean norm. Also, we measure the empiric performance as the fraction
of samples that are classified correctly using a decision threshold at y = 0.5, and denote this as the mean classification error, or MCE2. Note that the variables e, MSE, MCE and other related variables
(e.g., their derivatives) all depend on W, z, X, y and , but we keep this dependency implicit, to
avoid cumbersome notation.

Additional Notation.

We

define

g

(x) < h (x)

if

and

only

if

limx

log g(x) log h(x)

<

1

(and

similarly



and = ). We denote "M  N " when M is a matrix with entries drawn independently from a standard

normal distribution (i.e., i, j: Mij  N (0, 1)). The Khatari-rao product (cf. (Allman et al., 2009))

of two matrices, A = a(1), . . . , a(N)  Rd1×N and X = x(1), . . . , x(N)  Rd0×N is defined as

A  X a(1)  x(1), . . . , a(N)  x(N)  Rd0d1×N ,

(2.4)

where a  x = a1x , . . . , ad1 x is the Kronecker product.

3 BASIC PROPERTIES OF DIFFERENTIABLE LOCAL MINIMA

MNNs are typically trained by minimizing the loss over the training set, using Stochastic Gradient Descent (SGD), or one of its variants (e.g., Adam (Kingma & Ba, 2015)). Under rather mild conditions (Pemantle, 1990; Bottou, 1998), SGD asymptotically converges to local minima of the loss. For simplicity, we focus on differentiable local minima (DLMs) of the MSE (eq. (2.3)). In section 4 we will show that sub-optimal DLMs are exponentially rare in comparison to global minima. Non-differentiable critical points, in which some neural input (pre-activation) is exactly zero, are shown to be numerically rare in section 5, and are left for future work, as discussed in section 6.
Before we can provide our results, in this section we formalize a few necessary notions. For example, one has to define how to measure the amount of DLMs in the over-parameterized regime: there is an infinite number of such points, but they typically occupy only a measure zero volume in the weight space. Fortunately, using the differentiable regions of the MSE (definition 1), the DLMs can partitioned to a finite number of equivalence groups, so all DLMs in each region have the same error (Lemma 2). Therefore, we use the volume of these regions (definition 3) as the relevant measure in our theorems.

Differentiable regions of the MSE. The MSE is a piecewise differentiable function of W, with at most 2d1N differentiable regions, defined as follows.
Definition 1. For any A  {, 1}d1×N we define the corresponding differentiable region

DA (X) {W|a (WX) = A}  Rd1×d0 . Also, any DLM (W, z), for which W  DA (X) is denoted as "in DA (X)".

(3.1)

2Formally (this expression is not needed later): MCE

1 2N

N n=1

1+

1 - 2y(n)

sign

e(n)

-

1 2

.

4

Under review as a conference paper at ICLR 2018

Note that DA (X) is an open set, since a (0) is undefined (from eq. 2.1). Clearly, for all W  DA (X) the MSE is differentiable, so any local minimum can be non-differentiable only if it is not in any differentiable region. Also, all DLMs in a differentiable region are equivalent, as we prove on appendix section 7:

Lemma 2. At all DLMs in DA (X) the residual error e is identical, and furthermore

(A  X) e = 0 .

(3.2)

The proof is directly derived from the first order necessary condition of DLMs (MSE = 0) and their stability. Note that Lemma 2 constrains the residual error e in the over-parameterized regime: d0d1  N . In this case eq. (3.2) implies e = 0, if rank (A  X) = N . Therefore, we must have rank (A  X) < N for sub-optimal DLMs to exist. Later, we use similar rank-based constraints to
bound the volume of differentiable regions which contain DLMs with high error. Next, we define this
volume formally.

Angular Volume. From its definition (eq. (3.1)) each region DA (X) has an infinite volume in Rd1×d0 : if we multiply a row of W by a positive scalar, we remain in the same region. Only by rotating the rows of W can we move between regions. We measure this "angular volume" of a region in a probabilistic way: we randomly sample the rows of W from an isotropic distribution, e.g., standard Gaussian: W  N , and measure the probability to fall in DA (X), arriving to the following
Definition 3. For any region R  Rd1×d0 . The angular volume of R is

V (R) PWN (W  R) .

(3.3)

4 MAIN RESULTS

Some of the DLMs are global minima, in which e = 0 and so, MCE = MSE = 0, while other DLMs are sub-optimal local minima in which MCE > > 0. We would like to compare the angular volume (definition 3) corresponding to both types of DLMs. Thus, we make the following definitions.

Definition 4. We define3 L  Rd1×d0 as the union of differentiable regions containing sub-optimal DLMs with MCE > , and G  Rd1×d0 as the union of differentiable regions containing global minima with MCE = 0.

Definition 5. We define the constant  as  {0, 1}, and  0.23 3/4 if  = 0.

0.23 max [limN (d0 (N ) /N ) , ]3/4 if  =

In this section, we use assumptions 1-4 (stated in section 1) to bound the angular volume of the region L encapsulating all sub-optimal DLMs, the region G, encapsulating all global minima, and the ratio
between the two.

Angular volume of sub-optimal DLMs. First, in appendix section 8 we prove the following upper bound in expectation
Theorem 6. Given assumptions 1-4, the expected angular volume of sub-optimal DLMs, with MCE > > 0, is exponentially vanishing in N as
EXN V (L (X, y))  exp - N 3/4 [d1d0]1/4 .

and, using Markov inequality, its immediate probabilistic corollary

Corollary 7. Given assumptions 1-4, for any  > 0 (possibly a vanishing function of N ), we have, with probability 1 - , that the angular volume of sub-optimal DLMs, with MCE > > 0, is
exponentially vanishing in N as

V (L (X, y))  1 exp 

- N 3/4 [d1d0]1/4

3More formally: if A (X, y, ) is the set of A  {, 1}d1×N for which DA(X) contains a DLM with

MCE = , then  > 0, L (X, y)

 AA(X,y, ) DA(X) and G (X, y)

AA(X,y,0) DA(X).

5

Under review as a conference paper at ICLR 2018

Proof idea of Theorem 6: we first show that in differentiable regions with MCE > > 0, the condition in Lemma 2, (A  X) e = 0, implies that A = a (WX) must have a low rank. Then, we show that, when X  N and W  N , the matrix A = a (WX) has a low rank with exponentially
low probability. Combining both facts, we obtain the bound.

Existence of global minima. Next, to compare the volume of sub-optimal DLMs with that of global minima, in appendix section 9 we show first that, generically, global minima do exist (using a variant of the proof of (Baum, 1988, Theorem 1)): Theorem 8. For any y  {0, 1} N and X  Rd0×N almost everywhere4 we find matrices W  Rd1×d0 and z  Rd1 , such that y = f (WX) z , where d1 4 N/ (2d0 - 2) and i, n : wi x(n) = 0. Therefore, every MNN with d1  d1 has a DLM which achieves zero error e = 0.
Recently (Zhang et al., 2017a, Theorem 1) similarly proved that a 2-layer MNN with approximately 2N parameters can achieve zero error. However, that proof required N neurons (similarly to (Nilsson, 1965; Baum, 1988; Yu, 1992; Huang et al., 2006; Livni et al., 2014; Shen, 2016)), while Theorem 8 here requires much less: approximately d1  2N/d0. Also, (Hardt & Ma, 2017, Theorem 3.2) showed a deep residual network with N log N parameters can achieve zero error. In contrast, here we require just one hidden layer with 2N parameters.
Note the construction in Theorem 8 here achieves zero training error by overfitting to the data realization, so it is not expected to be a "good" solution in terms of generalization. To get good generalization, one needs to add additional assumptions on the data (X and y). Such a possible (common yet insufficient for MNNs) assumption is that the problem is "realizable", i.e., there exist a small "solution MNN", which achieves low error. For example, in the zero error case:
Assumption 5. (Optional) The labels are generated by some teacher y = f (WX) z with weight matrices W  Rd1×d0 and z  Rd1 independent of X, for some d1< N/d0.
This assumption is not required for our main result (Theorem 10) ­ it is merely helpful in improving the following lower bound on V (G).

Angular volume of global minima. We prove in appendix section 10:

Theorem 9. Given assumptions 1-3, we set =

8 

d-0 1/2

+

2d10/2log

d0/N

and

d1

=

2N/d0

,

or

if assumption 5 holds, we set d1 as in this assumption. Then, with probability 1 - , the angular

volume of global minima is lower bounded as,

V (G (X, y)) > exp (-d1d0 log N )  exp (-2N log N ) .

Proof idea: First, we lower bound V (G) with the angular volume of a single differentiable region of one global minimum (W, z) ­ either from Theorem 8, or from assumption 5. Then we show that this angular volume is lower bounded when W  N , given a certain angular margin between the datapoints in X and the rows of W. We then calculate the probability of obtaining this margin when X  N . Combining both results, we obtain the final bound.

Main result: angular volume ratio. Finally, combining Theorems 6 and 9 it is straightforward to

prove our main result in this paper, as we do in appendix section 11:

Theorem 10. Given assumptions 1-3, we set  =.

8 

d0-1/2

+

2d10/2log d0/N .

Then, with

probability 1 - , the angular volume of sub-optimal DLMs, with MCE > > 0, is exponentially

vanishing in N, in comparison to the angular volume of global minima with MCE = 0

V (L V (G

(X, y)) (X, y))



exp

- N 3/4 [d1d0]1/4

 exp (- N log N ) .

5 NUMERICAL EXPERIMENTS

Theorem 10 implies that, with "asymptotically mild" over-parameterization (i.e. in which #parameters =~ (N )), differentiable regions in weight space containing sub-optimal DLMs (with high MCE) are
4i.e., the set of entries of X, for which the following statement does not hold, has zero measure (Lebesgue).

6

Under review as a conference paper at ICLR 2018

Figure 5.1: Gaussian data: final training error (mean±std, 30 repetitions) in the over-
parameterized regime is low (right of the dashed black line). We trained MNNs with one and two hiddens layer (with widths equal to d = d0) on a synthetic random dataset in which n = 1, . . . , N , x(n) was drawn from a normal distribution N (0, 1), and y(n) = ±1 with probability 0.5.

MCE d0 d1

N #parameters/N

MNIST

0% 784 89 7 · 104

0.999

CIFAR

0% 3072 16 5 · 104

0.983

ImageNet (downsampled to 64 × 64) 0.1% 12288 105 128 · 104

1.008

Table 1: Binary classification of MNIST, CIFAR and ImageNet: 1-hidden layer achieves very low training error (MCE) with a few hidden neurons, so that #parameters  d0d1  N . In ImageNet we downsampled the images to allow input whitening.

exponentially small in comparison with the same regions for global minima. Since these results are asymptotic in N  , in this section we examine it numerically for a finite number of samples and parameters. We perform experiments on random data, MNIST, CIFAR10 and ImageNetILSVRC2012. In each experiment, we used ReLU activations ( = 0), a binary classification target (we divided the original classes to two groups), MSE loss for optimization (eq. (2.3)), and MCE to determine classification error. Additional implementation details are given in appendix part III.
First, on the small synthetic Gaussian random data (matching our assumptions) we perform a scan on various networks and dataset sizes. With either one or two hidden layers (Figure 5.1) , the error goes to zero when the number of non-redundant parameters (approximately d0d1) is greater than the number of samples, as suggested by our asymptotic results. Second, on the non-syntehtic datasets, MNIST, CIFAR and ImageNet (In ImageNet we downsampled the images to size 64 × 64, to allow input whitening) we only perform a simulation with a single 1-hidden layer MNN for which #parameters  N , and again find (Table 1) that the final error is zero (for MNIST and CIFAR) or very low (ImageNet).
Lastly, in Figure 5.2 we find that, on the Gaussian dataset, the inputs to the hidden neurons converge to a distinctly non-zero value. This indicates we converged to DLMs ­ since non-differentiable critical points must have zero neural inputs. Note that occasionally, during optimization, we could find some neural inputs with very low values near numerical precision level, so convergence to non-differentiable minima may be possible. However, as explained in the next section, as long as the number of neural inputs equal to zero are not too large, our bounds also hold for these minima.
6 DISCUSSION
In this paper we examine Differentiable Local Minima (DLMs) of the empiric loss of Multilayer Neural Networks (MNNs) with one hidden layer, scalar output, and LReLU nonlinearities (section 2). We prove (Theorem 10) that with high probability the angular volume (definition 3) of sub-optimal DLMs is exponentially vanishing in comparison to the angular volume of global minima (definition 4), under assumptions 1-4. This results from an upper bound on sub-optimal DLMs (Theorem 6) and a lower bound on global minima (Theorem 9).

7

Under review as a conference paper at ICLR 2018

Convergence of SGD to DLMs. These re-

sults suggest a mechanism through which low

training error is obtained in such MNNs. Ho-

wever, they do not guarantee it. One issue is

that sub-optimal DLMs may have exponentially

large basins of attraction. We see two possi-

ble paths that might address this issue in future

work, using additional assumptions on y. One

Figure 5.2: Gaussian data: convergence of the approach is to show that, with high probability,

MSE to differentiable local minima, as indica- no sub optimal DLM falls within the vanishingly

ted by the convergence of the neural inputs to small differentiable regions we bounded in The-

distinctly non-zero values. We trained MNNs orem 6. Another approach would be to bound

with one hidden layer on the Gaussian dataset from the size of these basins of attraction, by showing

Figure 5.1, with various widths d = d0 = d1 and that sufficiently large of number of differentiaN = d2/5 for 1000 epochs, then decreased the ble regions near the DLM are also vanishingly

learning rate exponentially for another 1000 epo- small (other methods might also help here (Free-

chs. This was repeated 30 times. For all d and man & Bruna, 2016)). Another issue is that

repeats, we see that (left) the final absolute value SGD might get stuck near differentiable saddle

of the minimal neural input (i.e., mini,n wi x(n) ) in the range of 10-3 - 100, which is much larger then (right) the final MSE error for all d and all repeats ­ in the range 10-31 - 10-7.

points, if their Hessian does not have strictly negative eigenvalues (i.e., the strict saddle property (Sun et al., 2015)). It should be straightforward to show that such points also have exponentially vanishing angular volume, similar

to sub-optimal DLMs. Lastly, SGD might also

converge to non-differentiable critical points, which we discuss next.

Non-differentiable critical points. The proof of Theorem 6 stems from a first order necessary condition (Lemma 2): (A  X) e = 0, which is true for any DLM. However, non-differentiable
critical points, in which some neural inputs are exactly zero, may also exist (though, numerically, they
don't seem very common ­ see Figure 5.2). In this case, to derive a similar bound, we can replace the condition with P (A  X) e = 0, where P is a projection matrix to the subspace orthogonal to the
non-differentiable directions. As long as there are not too many zero neural inputs, we should be able to obtain similar results. For example, if only a constant ratio r of the neural inputs are zero, we can simply choose P to remove all rows of (A  X) corresponding to those neurons, and proceed with exactly the same proof as before, with d1 replaced with (1 - r) d1. It remains a theoretical challenge to find reasonable assumptions under which the number of non-differentiable directions (i.e., zero
neural inputs) does not become too large.

Related results. Two works have also derived related results using the (A  X) e = 0 condition from Lemma 2. In (Soudry & Carmon, 2016), it was noticed that an infinitesimal perturbation of A makes the matrix A  X full rank with probability 1 (Allman et al., 2009, Lemma 13) ­ which entails that e = 0 at all DLMs. Though a simple and intuitive approach, such an infinitesimal perturbation is problematic: from continuity, it cannot change the original MSE at sub-optimal DLMs ­ unless the weights go to infinity, or the DLM becomes non-differentiable ­ which are both undesirable results. An extension of this analysis was also done to constrain e using the singular values of AX (Xie et al., 2016), deriving bounds that are easier to combine with generalization bounds. Though a promising approach, the size of the sub-optimal regions (where the error is high) does not vanish exponentially in the derived bounds. More importantly, these bounds require assumptions on the activation kernel spectrum m, which do not appear to hold in practice (e.g., (Xie et al., 2016, Theorems 1,3) require mm 1 to hold with high probability, while mm < 10-2 in (Xie et al., 2016, Figure 1)).

Modifications and extensions. There are many relatively simple extensions of these results: the Gaussian assumption could be relaxed to other near-isotropic distributions (e.g., sparse-land model, (Elad, 2010, Section 9.2)) and other convex loss functions are possible instead of the quadratic loss. More challenging directions are extending our results to MNNs with multi-output and multiple hidden layers, or combining our training error results with novel generalization bounds which might be better suited for MNNs (e.g., (Feng et al., 2016; Sokolic et al., 2016; Dziugaite & Roy, 2017)) than previous approaches (Zhang et al., 2017a).

8

Under review as a conference paper at ICLR 2018
REFERENCES
Elizabeth S. Allman, Catherine Matias, and John A. Rhodes. Identifiability of parameters in latent structure models with many observed variables. Annals of Statistics, 37(6 A):3099­3132, 2009. ISSN 00905364. doi: 10.1214/09-AOS689.
A Andoni, R Panigrahy, G Valiant, and L Zhang. Learning Polynomials with Neural Networks. In ICML, 2014. Pierre Baldi. Linear Learning: Landscapes and Algorithms. Advances in Neural Information Processing Systems
1, (1):65­72, 1989. Eric B. Baum. On the capabilities of multilayer perceptrons. Journal of Complexity, 4(3):193­215, 1988. ISSN
10902708. doi: 10.1016/0885-064X(88)90020-9. L Bottou. Online learning and stochastic approximations. In On-line learning in neural networks, pp. 9­42.
1998. ISBN 978-0521117913. Alon Brutzkus and Amir Globerson. Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs.
arXiv, 2017. Ronald W. Butler. Saddlepoint Approximations with Applications. 2007. ISBN 9780511619083. doi: 10.1017/
CBO9780511619083. Yingtong Chen and Jigen Peng. Influences of preconditioning on the mutual coherence and the restricted
isometry property of Gaussian/Bernoulli measurement matrices. Linear and Multilinear Algebra, 64(9): 1750­1759, 2016. ISSN 0308-1087. doi: 10.1080/03081087.2015.1116495. Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Y LeCun. The Loss Surfaces of Multilayer Networks. AISTATS15, 38, 2015. T M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. Electronic Computers, IEEE Transactions on, (3):326­334, 1965. G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2:303­314, 1989. YN Dauphin, Razvan Pascanu, and Caglar Gulcehre. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, pp. 1­9, 2014. Simon S. Du, Jason D. Lee, and Yuandong Tian. When is a Convolutional Filter Easy To Learn? arXiv, sep 2017. Gintare Karolina Dziugaite and Daniel M. Roy. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. ArXiv, 2017. Michael Elad. Sparse and redundant representations: from theory to applications in signal and image processing. Springer New York, New York, NY, 2010. Jiashi Feng, Tom Zahavy, Bingyi Kang, Huan Xu, and Shie Mannor. Ensemble Robustness of Deep Learning Algorithms. ArXiv, feb 2016. C. Daniel Freeman and Joan Bruna. Topology and Geometry of Deep Rectified Network Optimization Landscapes. ArXiv: 1611.01540, 2016. K. Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer perceptrons. Neural Networks, 13:317­327, 2000. ISSN 08936080. doi: 10.1016/S0893-6080(00)00009-5. Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network optimization problems. In ICLR, 2015. Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76­86, 1992. ISSN 01628828. doi: 10.1109/34.107014. Moritz Hardt and Tengyu Ma. Identity Matters in Deep Learning. ICLR, pp. 1­19, 2017. K He, X Zhang, S Ren, and J. Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing HumanLevel Performance on ImageNet Classification. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1026­1034, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.123. K Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(1989):251­257, 1991. Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: Theory and applications. Neurocomputing, 70(1-3):489­501, 2006. ISSN 09252312. doi: 10.1016/j.neucom.2005.12.126. M Janzamin, H Sedghi, and A Anandkumar. Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods. ArXiv:1506.08473, pp. 1­25, 2015. Kenji Kawaguchi. Deep Learning without Poor Local Minima. In NIPS, 2016. Diederik P Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. In ICLR, pp. 1­13, 2015. Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv:1404.5997, 2014. Y LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015. ISSN 0028-0836. doi: 10.1038/nature14539. Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient Descent Converges to Minimizers. Conference on Learning Theory, 2016.
9

Under review as a conference paper at ICLR 2018
Yuanzhi Li and Yang Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation. arXiv, may 2017.
Roi Livni, S Shalev-Shwartz, and Ohad Shamir. On the Computational Efficiency of Training Neural Networks. NIPS, 2014.
Haihao Lu and Kenji Kawaguchi. Depth Creates No Bad Local Minima. ArXiv, (2014):1­9, 2017. Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier Nonlinearities Improve Neural Network
Acoustic Models. In Proceedings of the 30 th International Conference on Machine Learning, pp. 6, 2013. Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. Arxiv, 2017. Nils J. Nilsson. Learning machines. McGraw-Hill New York, 1965. R Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of
Probability, 18(2):698­712, 1990. Jeffrey Pennington and Yasaman Bahri. Geometry of Neural Network Loss Surfaces via Random Matrix
Theory. Proceedings of the 34th International Conference on Machine Learning, 70:2798­2806, 2017. ISSN 1938-7228. Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In NIPS, 2016. Mark Rudelson and Roman Vershynin. Non-asymptotic Theory of Random Matrices: Extreme Singular Values. Proceedings of the International Congress of Mathematicians, pp. 1576­1602, 2010. doi: 10.1142/ 9789814324359_0111. Itay Safran and Ohad Shamir. On the Quality of the Initial Basin in Overspecified Neural Networks. In ICML, 2016. A M Saxe, J L. McClelland, and S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. ICLR, 2014. Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Weight Sharing is Crucial to Succesful Optimization. jun 2017. Ohad Shamir. Distribution Specific Hardness of Learning Neural Networks. arXiv preprint arXiv:1609.01037, pp. 1­26, 2016. Hao Shen. Designing and Training Feedforward Neural Networks: A Smooth Optimisation Perspective. ArXiv, (i):1­19, 2016. Jirí Síma. Training a single sigmoidal neuron is hard. Neural computation, 14(11):2709­28, 2002. ISSN 0899-7667. doi: 10.1162/089976602760408035. D Slepian. The One Sided Problem for Gaussian Noise. Bell System Technical Journal, 1962. Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural Networks, 2016. Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv, jul 2017. D. Soudry and Y Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. In arXiv:1605.08361, 2016. Ju Sun, Qing Qu, and John Wright. When Are Nonconvex Problems Not Scary? arXiv:1510.06096 [cs, math, stat], pp. 1­6, 2015. Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of deep networks. arXiv:1611.06310, pp. 1­13, 2016. Yuandong Tian. Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity. Submitted to ICLR, 2017. L. Welch. Lower bounds on the maximum cross correlation of signals. IEEE Transactions on Information Theory, 20(3):397­399, may 1974. ISSN 0018-9448. doi: 10.1109/TIT.1974.1055219. Bo Xie, Yingyu Liang, and Le Song. Diversity Leads to Generalization in Neural Networks. pp. 1­23, 2016. Xiao Hu Yu. Can Backpropagation Error Surface Not Have Local Minima. IEEE Transactions on Neural Networks, 3(6):1019­1021, 1992. ISSN 19410093. doi: 10.1109/72.165604. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017a. Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi. Electron-Proton Dynamics in Deep Learning. arXiv:1702.00458, pp. 1­31, 2017b. Kai Zhong, Ut-Austin Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery Guarantees for One-hidden-layer Neural Networks. ICML, jun 2017. Pan Zhou and Jiashi Feng. The Landscape of Deep Learning Algorithms. may 2017.
10

Under review as a conference paper at ICLR 2018

Supplementary information - Appendix

The appendix is divided into three parts. In part I we prove all the main theorems mentioned in the paper. Some of these rely on other technical results, which we prove later in part II. Lastly, in part III we give additional numerical details and results. First, however, we define additional notation (some already defined in the main paper) and mention some known results, which we will use in our proofs.

EXTENDED PRELIMINARIES

· The indicator function I (A)

1 0

, ,

if A else

,

for

any

event

A.

· Kronecker's delta ij I (i = j).
· The Matrix Id as the identity matrix in Rd×d, and Id×k is the relevant Rd×k upper left sub-matrix of the identity matrix.

· [L] {1, 2, . . . , L}

· The vector mn as the n'th column of a matrix M, unless defined otherwise (then mn will be a row of M).

· M > 0 implies that i, j : Mij > 0.
· MS is the matrix composed of the columns of M that are in the index set S.
· A property holds "M-almost everywhere" (a.e. for short), if the set of entries of M for which the property does not hold has zero measure (Lebesgue).

· v 0=

d i=1

I

(vi

>

0)

is

the

L0

"norm"

that

counts

the

number

of

non-zero

values

in

v  Rd.

· If x  N (µ, ) the x is random Gaussian vector.

·  (x)

1 exp
2

-

1 2

x2

as the univariate Gaussian probability density function.

·  (x)

x -



(u)

du

as

the

Gaussian

cumulative

distribution

function.

· B (x, y) as the beta function.

Lastly, we recall the well known Markov Inequality: Fact 11. (Markov Inequality) For any random variable X  0, we have  > 0

P (X



)



EX 

.

Part I
Proofs of the main results

7 FIRST ORDER CONDITION: PROOF OF LEMMA 2

Lemma 12. (Lemma 2 restated) At all DLMs in DA (X) the residual error e is identical, and

furthermore

(A  X) e = 0 .

(7.1)

Proof. Let W = [w1, . . . , wd1 ]  DA (X), G A  X  Rd0d1×N , W~ = diag (z) W = [w~ 1, . . . , w~ d1 ] and w~ vec W~  Rd0d1 , where diag (v) is the diagonal matrix with v in its

11

Under review as a conference paper at ICLR 2018

diagonal, and vec (M) is vector obtained by stacking the columns of the matrix M on top of one another. Then, we can re-write the MSE (eq. (2.3)) as

1 MSE =

y - G w~ 2 = 1

e 2,

NN

(7.2)

where G w~ is the output of the MNN. Now, if (W, z) is a DLM of the MSE in eq. (2.3), then there is no infinitesimal perturbation of (W, z) which reduces this MSE.

Next, for each row i, we will show that MSE/w~ i = 0, since otherwise we can find an infinitesimal perturbation of (W, z) which decreases the MSE, contradicting the assumption that (W, z) is a local
minimum. For each row i, we divide into two cases:

First, we consider the case zi = 0. In this case, any infinitesimal perturbation qi in w~ i can be produced by an infinitesimal perturbation in wi: w~ i + qi = (wi + qi/zi)zi. Therefore, unless the gradient MSE/w~ i is equal to zero, we can choose an infinitesimal perturbation qi in the opposite direction to this gradient, which will decrease the MSE.

Second, we consider the case zi = 0. In this case, the MSE is not affected by changes made exclusively to wi. Therefore, all wi derivatives of the MSE are equal to zero (kMSE/kwi, to any order k) . Also, since we are at a differentiable local minimum, MSE/zi = 0. Thus, using a Taylor expansion, if we perturb (wi, zi) by (w^ i,z^i ) then the MSE is perturbed by

z^iw^ i

 w~ i

 zi

MSE

+

O(z^i2)

Therefore, unless 2MSE/ (wizi) = 0 we can choose w^ i and a sufficiently small z^i such that the MSE is decreased. Lastly, using the chain rule







MSE =

zi wi

zi

zi w~ i MSE

= MSE . w~ i

Thus, MSE/w~ i = 0. This implies that w~ is also a DLM5 of eq. (7.2), which entails

N

0=-

MSE = G y - G w~ .

2 w~ i

(7.3)

Since G = A  X and e = y - G w~ this proves eq. (7.1). Now, for any two solutions w~ 1 and w~ 2 of eq. (7.3), we have

0 = G y - G w~ 1 - G y - G w~ 1 = GG (w~ 2 - w~ 1) .

Multiplying by (w~ 2 - w~ 1) from the left we obtain G (w~ 2 - w~ 1) 2 = 0  G (w~ 2 - w~ 1) = 0 .
Therefore, the MNN output and the residual error e are equal for all DLMs in DA (X).

8 SUB-OPTIMAL DIFFERENTIABLE LOCAL MINIMA: PROOF OF THEOREM 6
AND ITS COROLLARY
Theorem 13. (Theorem 6 restated) Given assumptions 1-4, the expected angular volume of suboptimal DLMs, with MCE > > 0, is exponentially vanishing in N as
EXN V (L (X, y))  exp - N 3/4 [d1d0]1/4 ,
where  0.23 max [limN (d0 (N ) /N ) , ]3/4 if  = {0, 1}, and  0.23 3/4 if  = 0.
To prove this theorem we upper bound the angular volume of L (definition 4), i.e., differentiable regions in which there exist DLMs with MCE > > 0. Our proof uses the first order necessary condition for DLMs from Lemma 2, (A  X) e = 0, to find which configurations of A allow for
5Note that the converse argument is not true ­ a DLM in w~ might not be a DLM in (W, z).

12

Under review as a conference paper at ICLR 2018

a high residual error e with MCE > > 0. In these configurations A  X cannot have full rank, and therefore, as we show (Lemma 14 below), A = a (WX) must have a low rank. However, A = a (WX) has a low rank with exponentially low probability when X  N and W  N (Lemmas 15 and 16 below). Thus, we derive an upper bound on EXN V (L (X, y)).
Before we begin, let us recall some notation: [L] {1, 2, . . . , L},M > 0 implies that i, j : Mij > 0, MS is the matrix composed of the columns of M that are in the index set S, v 0 as the L0 "norm" that counts the number of non-zero values in v. First we consider the case  = 0. Also, we denote Kr max [N , rd0] .
First we consider the case  = 0.
From definition 3 of the angular volume

EXN V (L (X, y)) =P(X,y)PX,Y ,WN (W  L (X, y))

(1)
 P(X,y)PX,Y ,WN

A  {, 1}d1×N , W  DA (X) , v  RN : (A  X) v = 0 , N



v0

(2)
= PXN ,WN

A  {, 1}d1×N , W  DA (X) , v  RN : (A  X) v = 0 , N



v0

(3)
 PXN ,WN (S  [N ] : |S|  max [N , rank (a (WXS)) d0 + 1])

=EXN [PWN (S  [N ] : |S|  max [N , rank (a (WXS)) d0 + 1] |X)]

(4) N/d0



 EXN  PWN (S  [N ] : |S| = Kr, rank (a (WXS)) = r|X)

r=1

(5) N/d0  EXN 

 PWN (rank (a (WXS)) = r|X) ,

r=1 S:|S|=Kr

(8.1)

where
1. If we are at DLM a in DA (X), then Lemma 2 implies (A  X) e = 0. Also, if e(n) = 0 on some sample, we necessarily classify it correctly, and therefore MCE  e 0 /N . Since MCE > in L this implies that N < e 0 . Thus, this inequality holds for v = e.
2. We apply assumption 1, that X  N .
3. Assumption 4 implies d0d1> N log4 N  N . Thus, we can apply the following Lemma, proven in appendix section 12.1:
Lemma 14. Let X  Rd0×N , A  {, 1}d1×N , S  [N ] and d0d1  N . Then, simultaneously for every possible A and S such that

|S|  rank (AS) d0 ,
we have that, X-a.e., v  RN such that vn = 0 n  S and (A  X) v = 0 .
4. Recall that Kr max [N , rd0]. We use the union bound over all possible ranks r  1: we ignore the r = 0 case since for  = 0 (see eq. (2.1)) there is zero probability that rank (a (WXS)) = 0 for some non-empty S. For each rank r  1, it is required that |S| > Kr = max [N , rd0], so |S| = Kr is a relaxation of the original condition, and thus its probability is not lower.
5. We again use the union bound over all possible subsets S of size Kr.

13

Under review as a conference paper at ICLR 2018

Thus, from eq. (8.1), we have

EXN V (L (X, y))

N/d0
 EXN [PWN (rank (a (WXS)) = r|X)]
r=1 S:|S|=Kr

N/d0
(=1)

N Kr

PXN ,WN rank a WX[Kr] = r

r=1

(2) N/d0


N Kr

2Kr+rd0(log d1+log Kr)+r2 PXN ,WN WX[Kr/2] > 0

r=1

(3) N/d0

r=1

N Kr

2Kr+rd0(log d1+log Kr)+r2 exp

-0.2Kr

2 d0d1 Kr

1/4

(4) N/d0
 2N log N exp -0.23N 3/4 [d1d0]1/4 max [ , rd0/N ]3/4
r=1

(5)
 exp - N 3/4 [d1d0]1/4 .

(8.2)

1. Since we take the expectation over X, the location of S does not affect the probability. Therefore, we can set without loss of generality S = [Kr].
2. Note that r  N/d0< min [d0, d1] from assumptions 3 and 4. Thus, with k = Kr  d0, we apply the following Lemma, proven in appendix section 12.2: Lemma 15. Let X  Rd0×k be a random matrix with independent and identically distributed columns, and W  Rd1×d0 an independent standard random Gaussian matrix. Then, in the limit min [k, d0, d1] > r,
P (rank (a (WX)) = r)  2k+rd0(log d1+log k)+r2 P WX[ k/2 ] > 0 .
3. Note that Kr  N = N > 2d1, and min [Kr, d0, d1] > d0d1/Kr> 1 from assumptions 2 and 4. Thus, we apply the following Lemma (with C = X ,B = W , M = d0, L = d1 and N = Kr/2), proven in appendix section 12.3: Lemma 16. Let C  RN×M and B  RM×L be two independent standard random Gaussian matrices. Without loss of generality, assume N  L, and denote  M L/N . Then, in the regime M  N and in the limit min [N, M, L] > > 1, we have
P (CB > 0)  exp -0.4N 1/4 .

4. We use rd0  N ,

N Kr

 2N ,Kr  N , and d1< N (from assumption 4) and r2 

N 2/d02< N (from assumption (3)) to simplify the combintaorial expressions.

5. First, note that r = 1 is the maximal term in the sum, so we can neglect the other, exponentially smaller, terms. Second, from assumption 3 we have d0 N , so

lim 0.23 max [
N 

, d0 (N ) /N ]3/4

=

0.23 max

3/4

,

lim
N 

d0

(N

)

/N

= .

Third, from assumption 4 we have N log4 N < d0d1, so the 2N log N term is negligible.

Thus,

EXN V (L (X, y))  exp - N 3/4 [d1d0]1/4 .

(8.3)

which proves the Theorem for the case  = 0.

Next, we consider the case  = 0. In this case, we need to change transition (4) in eq. (8.1), so the sum starts from r = 0, since now we can have rank (a (WXS)) = 0. Following exactly the same

14

Under review as a conference paper at ICLR 2018

logic (except the modification to the sum), we only need to modify transition (5)in eq. (8.2) ­ since now the maximal term in the sum is at r = 0. This entails  = 0.23 3/4.

Corollary 17. (Corollary 7 restated) Given assumptions 1-4, for any  > 0 (possibly a vanishing function of N ), we have, with probability 1 - , that the angular volume of sub-optimal DLMs, with
MCE > > 0, is exponentially vanishing in N as

V (L (X, y))  1 exp 

- N 3/4 [d1d0]1/4

Proof. Since V (L (X, y))  0 we can use Markov's Theorem (Fact 11)  > 0:

PXN (V (L

(X, y)) < ) > 1 - EXN V (L 

(X, y))

denoting  =

1 

EXN

V

(L

(X, y)), and using Theorem (6) we prove the corollary.

1 -  < PXN

V (L

(X, y)) <

1 

EXN

V

(L

(X, y))

< PXN

V (L (X, y))  1 exp 

- N 3/4 [d1d0]1/4

where we note that replacing a regular inequality< with inequality in the leading order only removes constraints, and therefore increases the probability.

9 CONSTRUCTION OF GLOBAL MINIMA: PROOF OF THEOREM 8:

Recall the LReLU non-linearity

f (x)

x , if x < 0 x , if x  0

in eq. (2.1), where  = 1.
Theorem 18. (Theorem 8 restated) For any y  {0, 1} N and X  Rd0×N almost everywhere we can find matrices W  Rd1×d0 and z  Rd1 , such that y = f (WX) z , where d1 4 N/ (2d0 - 2) and i, n : wi x(n) = 0.. Therefore, every MNN with d1  d1 has a DLM which achieves zero error (MSE = MCE = 0).

We prove the existence of a solution (W,z), by explicitly constructing it. This construction is a variant of (Baum, 1988, Theorem 1), except we use LReLU without bias and MSE ­ instead of threshold units with bias and MCE. First, we note that for any 1 > 2 > 0, the following trapezoid function can be written as a scaled sum of four LReLU:

 0 
 (x) 1

, if |x| > 1 , if |x|  2

 

1 -|x|

1- 2

, if 2 < |x|  1

=

1 1-

1 2 1 -  [f (x +

1) - f (x +

2) - f (x -

2) + f (x -

1)] .

(9.1)

Next, we examine the set of data points which are classified to 1: S+

Without

loss

of

generality,

assume

|S + |



N 2

.

We

partition

S+

to

n  [N ] |y(n) = 1 .

|S + | K= 

N

d0 - 1

2 (d0 - 1)

subsets Si+ iK=1, each with no more than d0 - 1 samples. For almost any dataset we can find K hyperplanes passing through the origin, with normals {w~ i}iK=1 such that each hyperplane contains all d0 - 1 points in subset Si+, i.e.,

w~ i XSi+ = 0 ,

(9.2)

15

Under review as a conference paper at ICLR 2018

but no other point, so n / Si+ : w~ i x(n) = 0 ,

If 1, 2 in eq. (9.1) are sufficiently small (n / Si+ : w~ i x(n) > 1) then we have



w~ i x(n)

=

1 0

, if n  Si+ . , else

Then we have

K


w~ i x(n)

=

1 0

, if n  S+ , else

i=1

(9.3)

which gives the correct classification on all the data points. Thus, from eq. (9.1), we can construct a

MNN with

d1 = 4K

hidden neurons which achieves zero error. This is straightforward to do if we have a bias in each

neuron. To construct this MNN even without bias, we first find a vector w^ i such that

w^ i XSi+ , w~ i = [1, . . . , 1, 1, 0] .

(9.4)

Note that this is possible since XSi+ , w~ i has full rank X-a.e. (the matrix XSi+  Rd0×d0-1 has, X-a.e., one zero left eigenvector, which is w~ i, according to eq. (9.2)). Additionally, we can set

w~ i = w^ i ,

(9.5)

since changing the scale of wi would not affect the validity of eq. (9.2). Then, we denote

wi(1) wi(3)

w~ i + 1w^ i ; wi(2) w~ i - 2w^ i ; wi(4)

w~ i + 2w^ i w~ i - 1w^ i .

Note, from eqs. (9.2) and (9.4) that this choice satisfies



1





n  Si+ : wi(j)

x(n)

=

2 -


2



- 1

, if j = 1 , if j = 2
. , if j = 3 , if j = 4

(9.6)

Also, to ensure that n / Si+ the sign of wi(j) x(n) does not change for different j, for some ,  < 1

we define

1

=



minn/Si+ maxn/Si+

w~ i x(n) w^ i x(n)

,

2 = 1,

(9.7)

where with probability 1, minn/Si+ w~ i x(n) > 0 and maxn/Si+ w^ i x(n) > 0. Defining

Wi wi(1), wi(2), wi(3), wi(4) zi [1, -1, -1, 1]  R4
and combining all the above facts, we have

 R4K×d0

(9.8)

f Wix(n) zi

=

11 1- 21-

f

wi(1) x(n)

-f

wi(2) x(n)

-f

wi(3) x(n)

+f

wi(3) x(n)

=

11 1- 21-

f

w~ i x(n) + 1w^ i x(n)

-f

w~ i x + 2w^ i x(n)

- f w~ i x(n) - 2w^ i x(n) + f w~ i x(n) - 1w^ i x(n)

= 1 , if n  Si+ . 0 , else

16

Under review as a conference paper at ICLR 2018

Thus, for

W = W1 , . . . , WK  R4×d0

z =

1 1-

2

1

1 -



·

[z1,

.

.

.

,

zK ]



R4K

we obtain a MNN that implements

f Wx(n) z = 1 , if n  S+ 0 , else

and thus achieves zero error. Clearly, from this construction, if wi is a row of W, then n  Si+,i : wi x(n)  2, and with probability 1 n / Si+,i : wi x(n) > 0, so this construction does not touch any non-differentiable region of the MSE.

10 GLOBAL MINIMA: PROOF OF THEOREM 9

Theorem

2d10/2

 log

19. (Theorem 9 restated). d0/N and d1 = 2N/d0 , or

if

Given assumptions 1-3, assumption 5 holds, we set

we set d1 as in

= this

8 

d-0 1/2

+

assumption.

Then, with probability 1 - , the angular volume of global minima is lower bounded as,

V (G (X, y)) > exp (-d1d0 log N )  exp (-2N log N ) .

In this section we lower bound the angular volume of G (definition 4), i.e., differentiable regions in which there exist DLMs with MCE = 0. We lower bound V (G) using the angular volume
corresponding to the differentiable region containing a single global minimum.

From assumption 4, we have d0d1> N , so we can apply Theorem 8 and say that the labels are generated using a (X, y) -dependent MNN: y = f (WX) z with target weights W =
w1 , . . . , wd1  Rd1×d0 and z  Rd1 . If, in addition, assumption 5 holds then we can assume W and z are independent from (X, y). In both cases, the following differentiable region

G~ (X, W) W  Rd1×d0 |i  d1 : sign wi X = sign wi X ,

(10.1)

also contains i > d1), and

a differentiable therefore X, y

global minimum (just set wi and their corresponding W,

= we

wi, zi have

=

zi

i



d1,

and

zi

=

0

G (X, y)  G~ (X, W)

(10.2)

Also, we will make use of the following definition. Definition 20. Let X have an angular margin  from W if all datapoints (columns in X) are at an angle of at least  from all the weight hyperplanes (rows of W) , i.e., X is in the set

M (W)

X  Rd0×N |i, n :

x(n) wi x(n) wi

> sin .

(10.3)

Using the definitions in eqs. (10.3) and (10.1), we prove the Theorem using the following three Lemmas.
First, In appendix section 13.2 we prove Lemma 21. For any , if W is independent from W then, in the limit N  , X  M (W) with log sin > d-0 1 log d0
V G~ = PWN W  G~ (X, W)  exp (d0d1 log sin ) .

Second, in appendix section 13.3 we prove

17

Under review as a conference paper at ICLR 2018

Lemma 22. Let W  Rd1×d0 a fixed matrix independent of X. Then, in the limit N   with d1 d0 N , the probability of not having an angular margin sin  = 1/ (d1d0N ) (eq. (10.3)) is upper bounded by

P (X / M (W)) 

2 

d-0 1/2

Lastly, in appendix section 13.4 we prove

Lemma 23. Let X  Rd0×N be a standard random Gaussian matrix of datapoints. Then we

can find, with probability 1, (X, y)-dependent matrices W and z as in Theorem 8 (where d1 4 N/ (2d0 - 2) ). Moreover, in the limit N  , where N/d0 d0 N , for any y, we can bound the probability of not having an angular margin (eq. (10.3)) with sin  = 1/ (d1d0N ) by

P (X / M (W))

8 

d0-1/2

+

2d01/2log d0 N

Recall that X, y and their corresponding W, we have G (X, y)  G~ (X, W) (eq. (10.2)). Thus, combining Lemmas 21 with sin  = 1/ (d1d0N ) together with either Lemma 22 or 23, we prove the first (left) inequality of Theorem 9:

V (G (X, y))  exp (-d1d0 log N ) Next, if d1 = 2N/d0 or d1< N/d0 (is assumption 5 holds), we obtain the second (right) inequality
exp (-d1d0 log N )  exp (-2N log N ) .

11 VOLUME RATIO OF GLOBAL AND LOCAL MINIMA: PROOF OF THEOREM 10

Theorem 24. (Theorem 10

2d10/2

 log

d0

/N

.

Then,

with

restated) probability

Given 1 - ,

assumptions 1-3, we the angular volume of

set  =. sub-optimal

8 

d0-1/2

+

DLMs, with

MCE > > 0, is exponentially vanishing in N, in comparison to the angular volume of global

minima with MCE = 0

V (L V (G

(X, y)) (X, y))



exp

- N 3/4 [d1d0]1/4

 exp (- N log N ) .

To prove this theorem we first calculate the expectation of the angular volume ratio given the X-event that the bound in Theorem 9 holds (given assumptions 1-3), i.e., V (G (X, y))  exp (-2N log N ). Denoting this event6 as M, we find:

EXN

V (L

(X, y)) |M

(1)
 EXN

[V

(L

(X, y)) |M]

(2)


V (G (X, y))

exp (-2N log N )

EXN [V (L (X, y))]

(3) exp - N 3/4 [d1d0]1/4 

(4)


PXN (M) exp (-2N log N ) PXN (M) exp (-2N log N )

exp - N 3/4 [d1d0]1/4 exp (-2N log N )

(5)
 exp

- N 3/4 [d1d0]1/4

(11.1)

where

1. We apply Theorem 9. 2. We use the following fact
6This event was previously denoted as X  M (W) in the proof of Theorem 9, but this is not important for this proof, so we simplified the notation.

18

Under review as a conference paper at ICLR 2018

Fact 25. For any variable X  0 and event A (where A¯ is its complement) E [X] = E [X|A] P (A) + E X|A¯ (1 - P (A))  E [X|A] P (A)

3. We apply Theorem 6. 4. We apply Theorem 9. 5. We use assumption 4, which implies  N 3/4 [d1d0]1/4 > 2N log N .

For simplicity, in the reminder of the proof we denote

V (L (X, y)) R (X) V (G (X, y)) .

From Markov inequality (Fact 11), since R (X)  0, we have  (N ) > 0:

PXN

[R (X)



 (N ) |M]



EXN [R (X) |M]  (N )

On the other hand, from fact 25, we have

1

-

PXN

[R

(X)

<



(N )

|M]



1

-

PXN [R (X) <  PXN (M)

(N )]

.

Combining Eqs. (11.2)-(11.3) we obtain

and so

EXN [R (X) |M]  (N )



1

-

PXN [R (X) <  PXN (M)

(N )]

,

PXN

(M)

- PXN

(M)

EXN [R (X) |M]  (N )



PXN

[R (X)

<

 (N )]

.

We choose

 (N ) = N PXN (M) EXN [R (X) |M] = exp - N 3/4 [d1d0]1/4

(11.2) (11.3)

so that

PXN

(M) -

1 N

 PXN

R (X)  exp

- N 3/4 [d1d0]1/4

Then, from Theorem 9 we have

1 - PXN (M)

8 

d-0 1/2

+

2d10/2log N

d0

.

so we obtain the first (left) inequality in the Theorem (10)

.

(11.4)

8 

d0-1/2

+

2d10/2log d0  1 N

-

PXN

V (L V (G

(X, y)) (X, y))



exp

- N 3/4 [d1d0]1/4

.

Lastly, we note that assumption 4 implies  N 3/4 [d1d0]1/4 > N log N , which proves the second (right) inequality of the theorem.

19

Under review as a conference paper at ICLR 2018

Part II
Proofs of technical results

In this part we prove the technical results used in part I.

12 UPPER BOUNDING THE ANGULAR VOLUME OF SUB-OPTIMAL DIFFERENTIABLE LOCAL MINIMA: PROOFS OF LEMMAS USED IN SECTION 8

12.1 PROOF OF LEMMA 14

In this section we will prove Lemma 14 in subsection 12.3.3. Recall the following definition Definition 26. Let

A = [a1, . . . , aN ] ; X = [x1, . . . , xN ] ,

where X  Rd0×N and A  Rd1×N . The Khatari-Rao product between the two matrices is defined as

AX

[a1  x1, a2  x2, ...aN  xN ]

 a11x1 a12x2 . . . 

=

 

a21x1

a22x2

...

 

.

 ...

... ... 

(12.1)

Lemma 27. (Lemma 14 restated) Let X  Rd0×N , A  {, 1}d1×N , S  [N ] and d0d1  N . Then, simultaneously for every possible A and S such that

|S|  rank (AS) d0 , we have that, X-a.e., v  RN such that vn = 0 n  S and (A  X) v = 0 .

Proof. We examine specific A  {, 1}d1×N and S  [N ], and such that |S|  dSd0, where we
defined dS rank (AS). We assume that dS  1, since otherwise the proof is trivial. Also, we assume by contradiction that v  RN such that vi = 0 i  S and (A  X) v = 0 . Without loss of generality, assume that S = {1, 2, ..., |S|} and that a1, a2, ..., adS are linearly independent. Then

|S|
(A  X) v = vnak,nxn = 0
n=1

(12.2)

for every 1  k  d1. From the definition of S we must have vn = 0 for every 1  n  |S|. Since
a1, a2, ..., adS are linearly independent, the rows of AdS = [a1, a2, ..., adS ] span a dS-dimensional space. Therefore, it is possible to find a matrix R such that RAdS = [IdS×dS , 0dS×(d1-dS)] , where 0i×j is the all zeros matrix with i columns and j rows. Consider now AS  XS, i.e., the matrix composed of the columns of A  X in S. Applying R = R  Id0 to AS  XS, turns (12.2) into d0dS equations in the variables v1, ..., v|S|, of the form

|S|

vkxk +

vna~k,nxn = 0

(12.3)

n=dS +1

for every 1  k  dS. We prove by induction that for every 1  d  dS, the first d0d equations are linearly independent, except for a set of matrices X of measure 0. This will immediately imply |S| > dSd0, or else eq. 12.2 cannot be true for v = 0. which will contradict our assumption, as required. The induction can be viewed as carrying out Gaussian elimination of the system of
equations described by (12.3), where in each elimination step we characterize the set of matrices X
that for which that step is impossible, and show it has measure 0.

20

Under review as a conference paper at ICLR 2018

For d = 1, the first d0 equations read v1x1 +

|S| n=dS

+1

vna~1,nxn

=

0,

and

since v1

=

0,

we

must have x1  Span a~1,dS+1xdS+1, ..., a~1,|S|x|S| . However, except for a set of measure 0

with respect to x1 (a linear subspace of Rd0 with dimension less than d0), this can only happen if

dim Span a~1,dS+1xdS+1, ..., a~1,|S|x|S| = d0, which implies |S|  dS - 1 + d0 > d0 and also

that the first d0 rows are linearly independent (since there are d0 independent columns).

For a general d, we begin by performing Gaussian elimination on the first (d - 1) d0 equations,
resulting in a new set of rd equations, such that every new equation contains one variable that appears
in no other new equation. Let C be the set of the indices (equivalently, columns) of these variables rd variables. From (12.3) it is clear none of the variables vd, vd+1, ..., vdS appear in the first (d - 1) d0 equations, and therefore C  S = S \ {d, d + 1, ..., dS}. By our induction assumptions, except for a set of measure 0, the first (d - 1) d0 are independent, which means that |C| = rd = (d - 1) d0.
We now extend the Gaussian elimination to the next d0 equations, and eliminate all the variables in C
from them. The result of the elimination can be written down as,

vdxd +

vn (a~d,nId0 - Y) xn = 0 ,

nS \C

(12.4)

where Y is a square matrix of size d0 whose coefficients depend only on {a~k,n}nC,d>k1 and on {xn}nC , and in particular do not depend on xd and {xn}nS \C .
Now set x~n = (a~d,nId0 - Y)xn for n  S \ C. As in the case of d = 1, since vd = 0, xd  Span{x~n}nS \C . Therefore, for all values of xd  Rd0 but a set of measure zero (linear subspace of with dimension less than d0), we must have dim Span{x~n}nS \C = d0. From the independence of {x~n}nS \C on xd it follows that dim Span{x~n}nS \C = d0 holds a.e. with respect to the Lebesgue measure over x.

Whenever dim Span{x~n}nS \C = d0 we must have |S \ C|  d0 and therefore

|S| > |S | = |C| + |S \ C|  (d - 1) d0 + d0 = d0d .

(12.5)

Moreover, dim Span{x~n}nS \C = d0 implies that the d0 equations vdxd + nS \C vnx~n = 0
are independent. Thus, we may perform another step of Gaussian elimination on these d0 equations, forming d0 new equations each with a variable unique to it. Denoting by C the set of these d0 variables, it is seen from (12.4) that C  (S  {d}) \ C and in particular C is disjoint from C.
Thus, considering the first (d - 1) d0 equations together with the new d0 equations, we see that there is a set C  C of d0d variables, such that each variable in C  C appears only in one of the d0d equations, and each of the d0d contains only a single variable in C  C . This means that the first d0d must be linearly independent for all values of X except for a set of Lebesgue measure zero,
completing the induction.

Thus, we have proven, that for some A  {, 1}d1×N and S  [N ] such that |S|  rank (AS) d0 the event

E (A, S) = X  Rd0×N |v  RN : (A  X) v = 0 and vn = 0, n  S

has zero measure. The event discussed in the theorem is a union of these events: 

E0 

E (A, S) ,

A{,1}d1×N S[N ]:|S|rank(AS )d0

and it also has zero measure, since it is a finite union of zero measure events.

For completeness we note the following corollary, which is not necessary for a our main results. Corollary 28. If N  d1d0, then rank (A  X) = N , X-a.e., if and only if,
S  [N ] : |S|  rank (AS) d0 .

Proof. We define dS rank (AS) and A  X. The necessity of the condition |S|  d0dS holds for every X, as can be seen from the following counting argument. Since the matrix AS has rank dS,

21

Under review as a conference paper at ICLR 2018

there exists an invertible row transformation matrix R, such that RAS has only dS non-zero rows. Consider now GS = AS  XS, i.e., the matrix composed of the columns of G in S. We have

GS = (RAS)  XS = R (AS  XS) = R GS ,

(12.6)

where R = R  Id0 is also an invertible row transformation matrix, which applies R separately on the d0 sub-matrices of GS that are constructed by taking one every d0 rows. Since GS has at most d0dS non-zero rows, the rank of GS cannot exceed d0dS. Therefore, if |S| > d0dS, GS will not have full column rank, and hence neither will G. To demonstrate sufficiency a.e., suppose G does
not have full column rank. Let S be the minimum set of columns of G which are linearly dependent. Since the columns of GS are assumed linearly dependent there exists v  R|S| such v 0 = |S| and GSv = 0. Using Lemma 28 we complete the proof.

12.2 PROOF OF LEMMA 15
In this section we will prove Lemma 15 in subsection 12.3.3. This proof relies on two rather basic results, which we first prove in subsections 12.2.1 and 12.2.2.

12.2.1 NUMBER OF DICHOTOMIES INDUCED BY A HYPERPLANE
Fact 29. A hyperplane w  d0 can separate a given set of points X = x(1), . . . , x(N)  Rd0×N into several different dichotomies, i.e., different results for sign w X . The number of dichotomies is upper bounded as follows:

I w : sign w X = h
h{-1,1}N

d0 -1
2
k=0

N -1 k

 2N d0 .

(12.7)

Proof. See (Cover, 1965, Theorem 1) for a proof of the left inequality as equality (the Schläfli Theorem) in the case that the columns of X are in "general position" (which holds X-a.e, see definition in (Cover, 1965)) . If X is not in general position then this result becomes an upper bound, since some dichotomies might not be possible.
Next, we prove the right inequality. For N = 1 and N = 2 the inequality trivially holds. For N  3, we have

d0 -1
2
k=0

N -1 k

(1)


d0 -1
2 (N

- 1)k

(2)


(N 2

- 1)d0

-1



2N d0

.

N -2

k=0

where in (1) we used the bound

N k

 N k , in (2) we used the sum of a geometric series.

12.2.2 A BASIC PROBABILISTIC BOUND
Lemma 30. Let H = h1 , . . . , hd1  {-1, 1}d1×k be a deterministic binary matrix, W = w1 , . . . , wd1  Rd1×d0 be an independent standard random Gaussian matrix, and X  Rd0×k be a random matrix with independent and identically distributed columns.

P (sign (WX) = H) 

k k/2

P WX[ k/2 ] > 0 .

22

Under review as a conference paper at ICLR 2018

Proof. By direct calculation

d1

P (sign (WX) = H) = E [P (sign (WX) = H|X)] (=1) E

P sign wi X = hi |X

i=1

(2) d1

E

P wi XS^(hi) > 0|X

i=1

(3) d1

E

P wi XS > 0|X

i=1



(=4)

E [P (WXS

>

0|X)]

(5)


E

 P (WXS > 0|X)

S[k]:|S|= k/2

=

E [P (WXS

>

0|X)]

(6)
=

k k/2

P WX[ k/2 ] > 0 .

S[k]:|S|= k/2

where

1. We used the independence of the wi.

2. We define S^± (h)

S  [k] : ±hS > 0 as the sets in which h is always posi-

tive/negative, and S^ (h) as the maximal set between these two. Note that wi has a stan-

dard normal distribution which is symmetric to sign flips, so S : P wi XS > 0|X =

P wi XS < 0|X .

3. Note that S^ (h)  k/2 . Therefore, we define S = argmax P wi XS > 0|X .
S[k]:|S|= k/2

4. We used the independence of the wi.

5. The maximum is a single term in the following sum of non-negative terms.

6. Taking the expectation over X, since the columns of X are independent and identically distributed, the location of S does not affect the probability. Therefore, we can set without loss of generality S = [ k/2 ].

12.2.3 MAIN PROOF: BOUND ON THE NUMBER OF CONFIGURATIONS FOR A BINARY MATRIX
WITH CERTAIN RANK

Recall the function a (·) from eq. (2.1):

a (u)

1 , if , u > 0 .
 , if u < 0

where  = 1. Lemma 31. (Lemma 15 restated). Let X  Rd0×k be a random matrix with independent and identically distributed columns, and W  Rd1×d0 an independent standard random Gaussian matrix. Then, in the limit min [k, d0, d1] > r,
P (rank (a (WX)) = r)  2k+rd0(log d1+log k)+r2 P WX[ k/2 ] > 0 .

Proof. We denote A = a (WX)  {, 1}d1×k. For any such A for which rank (A) = r, we have a

collection of r rows that span the remaining rows. There are

d1 r

possible locations for these r

spanning rows. In these rows there exist a collection of r columns that span the remaining columns.

There are

k r

possible locations for these r spanning columns. At the intersection of the spanning

23

Under review as a conference paper at ICLR 2018

rows and columns, there exist a full rank sub-matrix D. We denote A~ as the matrix A which rows and columns are permuted so that D is the lower right block

A~

ZB CD

=a

W1X1 W1X2 W2X1 W2X2

,

(12.8)

where D is an invertible r × r matrix, and we divided X and W to the corresponding block matrices

W W1 , W2 with W2  Rr×d0 rows and X2  Rd0×r.

, X [X1, X2] ,

Since rank A~ = r, the first d1 - r rows are contained in the span of the last r rows. Therefore,

there exists a matrix Q such that QC = Z and QD = B. Since D is invertible, this implies that

Q = BD-1 and therefore

Z = BD-1C ,

(12.9)

i.e., B, C and D uniquely determine Z.

Using the union bound over all possible permutations from A to A~ , and eq. (12.9), we have

P (rank (A) = r)



d1 r

k r

P rank A~ = r

(12.10)



d1 r

k r

P Z = BD-1C

=

d1 r

k r

P a (W1X2) [a (W2X2)]-1 a (W2X1) = a (W1X1)

=

d1 r

k r

P a (W1X2)[a (W2X2)]-1a (W2X1) = a (H) |sign (W1X1) = H P(sign (W1X1) = H)

H{-1,1}(d1 -r)×(k-r)

Using Lemma 30, we have

P (sign (W1X1) = H) 

k-r (k - r) /2

P W1X[ (k-r)/2 ] > 0 ,

(12.11)

an upper bound which does not depend on H. So all that remains is to compute the sum:

P a (W1X2) [a (W2X2)]-1 a (W2X1) = a (H) |sign (W1X1) = H
H{-1,1}(d1 -r)×(k-r)

= E P a (W1X2) [a (W2X2)]-1 a (W2X1) = a (H) |W1, X1 |sign (W1X1) = H

H{-1,1}(d1 -r)×(k-r)


(1)
E

 I  (W2, X2) : a (W1X2) [a (W2X2)]-1 a (W2X1) = a (H) sign (W1X1) = H

H{-1,1}(d1 -r)×(k-r)

(12.12)



(2)
E



2r2



 I (X2 : sign (W1X2) = H) 

 I (W2 : sign (W2X1) = H) sign (W1X1) = H

H{-1,1}(d1 -r)×r

H{-1,1}r×(k-r)



r 

r 

E  2r2  I (x : sign (W1x) = h) 

I w : sign w X1 = h  sign (W1X1) = H

h{-1,1}(d1 -r)

h{-1,1}(k-r)

(3)
E

2r2 2rd0 log(d1-r)+r2rd0 log(k-r)+r sign (W1X1) = H

=2rd0[log(d1-r)+log(k-r)]+r2+2r ,

(12.13)

where

24

Under review as a conference paper at ICLR 2018

1. Given (W1, X1), and eq. (12.8), the indicator function in eq. (12.12) is equal to zero only if P a (W1X2) [a (W2X2)]-1 a (W2X1) = A|W1, X1 = 0, and one otherwise.
2. This sum counts the number of values of H consistent with W1 and X1. Conditioned on (W1, X1), D = [a (W2X2)]-1,B = a (W1X2) and C = a (W2X1) can have multiple values, depending on W2 and X2. Also, any single value for (D, B, C) results in a single value of H. Therefore, the number of possible values of H in eq. (12.12) is upper bounded by the product of the number of possible values of D, B and C, which is product in the following equation.
3. The function Ih{-1,1}(k-r) w : sign w X1 = h counts the number of dichotomies that can be induced by the linear classifier w on X1. Using eq. (12.7) we can bound this number by 2 (k - r)d0 . Similarly, the other sum can be bounded by 2 (d1 - r)r.

Combining eqs. (12.10), (12.11) and (12.13) we obtain

P (rank (A) = r) 

d1 k rr

k-r (k - r) /2

2rd0[log(d1-r)+log(k-r)]+r2+2rP W1X[ (k-r)/2 ] > 0 .

Next, we take the log. To upper bound

N k

, for small k we use

k = N/2, we use

N N/2

 2N . Thus, we obtain

N k

 N k, while for

log P (rank (A) = r)  rd0 (log (d1 - r) + log (k - r)) + r2 + 2r log 2

(12.14)

+ r log d1 + r log k + (k - r) log 2 + log P W1X[ (k-r)/2 ] > 0 .

Recalling that W1  R(d1-r)×d0 while W  Rd1×d0 , we obtain from Jensen's inequality

log P W1X[ (k-r)/2 ] > 0



(k - r) /2 d1 - r k/2 d1

log P WX[ k/2 ] > 0

.

(12.15)

Taking the limit min [k, d0, d1] > r on eqs. (12.14) and (12.15) we obtain

P (rank (A) = r) 2k+rd0(log d1+log k)+t2 P WX[ k/2 ] > 0 .

12.3 PROOF OF LEMMA 16
In this section we will prove Lemma 16 in subsection 12.3.3. This proof relies on more elementary results, which we first prove in subsections 12.3.1 and 12.3.2.

12.3.1 ORTHANT PROBABILITY OF A RANDOM GAUSSIAN VECTOR

Recall that  (x) and  (x) are, respectively, the probability density function and cumulative distribution function for a scalar standard normal random variable.
Definition 32. We define the following functions x  0

g (x)  (x)

x (x) ,
 (x)

g-1 (x) 2 - log



g-1 (x)

2x

,

(12.16) (12.17)

where the inverse function g-1 (x) : [0, )  [0, ) is well defined since g (x) monotonically increase from 0 to , for x  0.

25

Under review as a conference paper at ICLR 2018

Lemma 33. Let z  N (0, ) be a random Gaussian vector in RK, with a covariance matrix ij = 1 - K-1 mn + K-1 where K  > 0. Then, recalling  () in eq. (12.17), we have
log P (i : zi > 0)  -K () + O (log K) .

Proof. Note that we can write z = u+, where u  N 0, 1 - K-1 IK , and   N 0, K-1 . Using this notation, we have

P (i : zi > 0)

 K

= d

duiI

-

i=1 -

 1 - K-1ui + K-1 > 0  (ui)  ()


= d 
-

K -1 1 - K-1 

K
 ()

(=1)

 2 (K - )


d [ ()]K exp
-

(K - ) 2 -
2

 =


d exp

2

exp K

log  () - 2

2 (K - ) -

2

2

,

(12.18)

where in (1) we changed the variable of integration to  = / (K - ). We denote, for a fixed ,

q () h ()

2 log  () -
2  2 (K - ) exp

2 2

(12.19) (12.20)

and 0 as its global maximum. Since q is twice differentiable, we can use Laplace's method (e.g., (Butler, 2007)) to simplify eq. (12.18)


log h () exp (Kq ()) d = Kq (0) + O (log K) .
-
To find 0, we differentiate q () and equate to zero to obtain

(12.21)

q

() =

 ()

-

1 

=

0.

 () 

(12.22)

which implies (recall eq. (12.16))

 () g () =  .
 ()

(12.23)

This is a monotonically increasing function from 0 to  in the range   0. Its inverse function can also be defined in that range g-1 () : [0, ]  [0, ]. This implies that this equation has only one solution, 0 = g-1 (). Since lim q () = -, this 0 is indeed the global maximum of q (). Substituting this solution into q (), we get (recall eq. (12.17))

 > 0 : q (0) = - () = q g-1 () = log  g-1 () Using eq. (12.18), (12.21) and (12.24) we obtain:

g-1 () 2 -.
2

(12.24)

log P (i : zi > 0)

 2

= log

d exp

exp K

- 2

= -K () + O (log K) .

log  () - 2 2

+ O (log K)

26

Under review as a conference paper at ICLR 2018

Next, we generalize the previous Lemma to a general covariance matrix. Corollary 34. Let u  N (0, ) be a random Gaussian vector in RK for which n : nn = 1, and   K maxn,m: n=m nm > 0 . Then, again, for large K
log P (i : ui > 0)  -K () + O (log K) .

Proof. We define u~  N 0, ~ , with ~ mn = 1 - K-1 mn + K-1. Note that n : nn = ~ nn = 1 and m = n: mn  ~ mn. Therefore, from Slepian's Lemma (Slepian, 1962, Lemma 1),
P (n : u~n > 0)  P (n : un > 0) .
Using Lemma 33 on u~ completes the proof.

12.3.2 MUTUAL COHERENCE BOUNDS

Definition 35. We define the mutual coherence of the columns of a matrix A = [a1, · · · , aN ]  RM×N as the maximal angle between different columns

 (A) max ai aj . i,j:i=j ai aj

Note that  (A)  1 and from (Welch, 1974), for N  M ,  (A) 

N -M M (N -1)

.

Lemma 36. Let A = [a1, · · · , aN ]  RM×N be a standard random Gaussian matrix, and  (A) is the mutual coherence of it columns (see definition 35). Then

P ( (A) > )  2N 2 exp

-M 2 24

.

Proof. In this case, we have from (Chen & Peng, 2016, Appendix 1):

P ( (A) > )  N (N - 1) exp

M a2 2 -
4 (1 + /2)

+ exp

- M (1 - a)2 4

for any a  (0, 1). Setting a = 1 - /2

,

P ( (A) > )  N (N - 1) exp

- M (1 - /2)2 2 4 (1 + /2)

+ exp - M 2 16

(1)
 N (N - 1) exp

-M 2

24

 2N 2 exp - M 2 , 24

+ exp - M 2 16

where in (1) we can assume that  1, since for  1, we have P ( (A) > ) = 0 (recall  (A)  1).

Lemma 37. Let B = [b1, · · · , bL]  RM×L be a standard random Gaussian matrix and mutual coherence  as in definition 35. Then,  > 0 and K  [L]:

P min  (BS) >
S[N ]:|S|=K

 exp 2 log (2K) - M 2 24

L -1 . K

Proof. We upper bound this probability by partitioning the set of column vectors into L/K subsets Si of size |Si| = K and require that in each subset the mutual coherence is lower bounded by .

27

Under review as a conference paper at ICLR 2018

Since the columns are independent, we have

P min  (BS) >
S[N ]:|S|=K

L/K

 P (S = {1 + (i - 1) K, 2 + (1 - i) K, . . . , iK} :  (BS) > )

i=1

(1) L/K-1
 2K2 exp

-M 2

24

i=1

 exp 2 log (2K) - M 2 24

L -1 , K

where in (1) we used the bound from Lemma 36.

12.3.3 MAIN PROOF: ORTHANT PROBABILITY OF A PRODUCT GAUSSIAN MATRICES
Lemma 38. (Lemma 16 restated). Let C = [c1, · · · , cN ]  RN×M and B  RM×L be two independent random Gaussian matrices. Without loss of generality, assume N  L, and denote  M L/N . Then, in the regime M  N and in the limit min [N, M, L] > > 1, we have
P (CB > 0)  exp -0.4N 1/4 .

Proof. For some  > 0, and subset S such that |S| = K < L, we have
P (CB > 0) P (CBS > 0| (BS)  ) P ( (BS)  ) + P (CBS > 0| (BS) > ) P ( (BS) > ) P (CBS > 0| (BS)  ) + P ( (BS) > ) =E P c1 BS > 0|BS,  (BS)  N | (BS)  + P ( (BS) > ) ,

where in the last equality we used the fact that the rows of C are independent and identically distributed.

We choose a specific subset

S = argminS[L]:|S|=K  (BS )

to minimize the second term and then upper bound it using Lemma 37 with  = K ; additionally, we

apply Corollary 34 on the first term with the components of the vector u being

ui = BS c1 i / BS BS ii  RK ,

which is a Gaussian random vector with mean zero and covariance  for which i : ii = 1 and i = j : ij  = K-1. Thus, we obtain

P (CB > 0)  exp (-N K () + O (N log K)) + exp

log

(2K )2

-

M 2 24K 2

L -1 , K
(12.25)

where we recall  () is defined in eq. (12.17).

Next, we wish to select good values for  and K, which minimize this bound for large (M, N, L, K). Thus, keeping only the first order terms in each exponent (assuming L K 1), we aim to minimize the function as much as possible

f (K, )

exp (-N K ()) + exp

-

M 2L 24K 3

.

(12.26)

Note that the first term is decreasing in K, while the second term increases. Therefore, for any  the minimum of this function in K would be approximately achieved when both terms are equal, i.e.,

M 2L N K () = 24K3 ,

28

Under review as a conference paper at ICLR 2018

so we choose

K () =

Substituting K () into f (K, ) yields

2M L 24 () N

1/4
.

3 () 2M L 1/4

f (K () , ) = 2 exp -N

.

24N

(12.27)

To minimize this function in , we need to maximize the function 3 () 2 (which has a single maximum). Doing this numerically gives us

  23.25 ;  ()  0.1062; 3 () 2  0.6478 . Substituting eqs. (12.27) and (12.28) into eq. (12.25), we obtain

(12.28)

P (CB > 0)

 exp -N

M L 1/4 + O (N log K)

37.05N

+ exp

-N

ML 37.05N

1/4 log K M 2

+ 2L K

+ 24K2 - log

2K 2

M L 1/4

ML

 exp -N

+ O N log

37.05N

N

,

where in the last line we used N  L,N  M and min [N, M, L] > > 1. Taking the log, and denoting  M L/N , we thus obtain
log P (CB > 0)  -0.4N 1/4 + O (N log ) , Therefore, in the limit that N   and  (N )  , with  (N ) < N , we have

P (CB > 0)  exp -0.4N 1/4 .

13 LOWER BOUNDING THE ANGULAR VOLUME OF GLOBAL MINIMA: PROOF OF LEMMAS USED IN SECTION 10

13.1 ANGLES BETWEEN RANDOM GAUSSIAN VECTORS

To prove the results in the next appendix sections, we will rely on the following basic Lemma.

Lemma 39. For any vector y and x  N (0, Id0 ), we have

xy

2 sin ( )d0-1

P

x

y

> cos ( )

 (d0 - 1) B

1 2

,

d0 -1 2

(13.1)

xy

P

<u xy

where we recall that B (x, y) is the beta function.

 B

2u

1 2

,

d0 -1 2

,

(13.2)

Proof. Since N (0, Id0 ) is spherically symmetric, we can set y = [1, 0 . . . , 0] , without loss of generality. Therefore,

xy xy

2

=

x12 +

x21

d0 i=2

x2i

B

1 , d0 - 1 22

,

29

Under review as a conference paper at ICLR 2018

the Beta distribution, since x12  2 (1) and

d0 i=2

xi2



2 (d0

- 1)

are

independent

chi-square

random variables.

Suppose Z  B (, ),   (0, 1), and  > 1 .

P (Z > u) =

1 u

x-1

(1

-

x)-1

dx

B (, )



1 u

1-1

(1

-

x)-1

dx

=

B (, )

1-u 0

x-1

dx

=

(1 - u)

.

B (, )

B (, )

Therefore, for > 0,

P

xy xy

2
> cos2 ( )

2

1 - cos2 ( )

d0 -1 2

 (d0 - 1) B

1 2

,

d0 -1 2

2 sin ( )d0-1

= (d0 - 1) B

1 2

,

d0 -1 2

,

which proves eq. (13.1).

Similarly, for   (0, 1) and  > 1

P (Z < u) =

u 0

x-1

(1 - x)-1 dx



B (, )

u 0

x-11-1dx

=

u

.

B (, )

B (, )

Therefore, for > 0,

P

xy xy

2
< u2

2u

 B

1 2

,

d0 -1 2

,

which proves eq. (13.2).

13.2 PROOF OF LEMMA 21:

Given three matrices: datapoints, X = x(1), . . . , x(N)  Rd0×N , weights W =

w1 , . . . , wd1  Rd1×d0 , and target weights W = w1 , . . . , wd1 d1  d1,we recall the following definitions:

 Rd1×d0 , with

M (W)

X  Rd0×N |i, n :

x(n) wi x(n) wi

> sin

(13.3)

and

G~ (X, W) W  Rd1×d0 |i  d1 : sign wi X = sign wi X .

(13.4)

Using these definitions, in this section we prove the following Lemma.

Lemma 40. (Lemma 21 restated). For any , if W is independent from W then, in the limit N  , X  M (W) with log sin > d0-1 log d0

PWN W  G~ (X, W)  exp (d0d1 log sin ) .

Proof. To lower bound PWN W  G~ (X, W) X  M (W), we define the event that all

weight hyperplanes (with normals wi) have an angle of at least  from the corresponding target hyperplanes (with normals wi).

G~i (W) =

W  Rd1×d0 |

wi wi wi wi

< cos () .

In order that sign wi x(n) = sign w1 x(n) , wi must be rotated in respect to wi by an angle greater then the angular margin , which is the minimal the angle between x(n) and the solution hyperplanes (with normals wi). Therefore, we have that, given X  M (W),

d1
 : G~i (W)  G~ (X, W) .
i=1

(13.5)

30

Under review as a conference paper at ICLR 2018

And so, X  M (W) :

PWN

W  G~ (X, W)

(1)  d1



 PWN W  G~i (W)

i=1

(13.6)

d1 (2)
= PWN
i=1

W  G~i (W)

(3)


2 sin ()d0-1

(d0 - 1) B

1 2

,

d0 -1 2

d1
,

where in (1) we used eq. (13.5), in (2) we used the independence of {wi}id=1 1 and in (3) we used eq. (13.1) from Lemma 39. Lastly, to simplify this equation we use the asymptotic expansion of the beta

function B

1 2

,

x

=

/x + O x-3/2 for large x:

log PWN W  G~ (X, W)  d0d1 log sin  + O (d1 log d0) .

We obtain the Lemma in the limit N   when log sin > d0-1 log d0.

13.3 PROOF OF LEMMA 22:

Lemma 41. (Lemma 22 restated). Let W = w1 , . . . , wd1  Rd1×d0 a fixed matrix independent of X. Then, in the limit N   with d1 d0 N , the probability of not having an angular margin sin  = 1/ (d1d0N ) (eq. (13.3)) is upper bounded by

P (X / M (W)) 

2 

d0-1/2

Proof. We define

Mn,i (W)

X  Rd0×N |

x(n) wi x(n) wi

> sin ()

,

and Mn (W)

d1 i=1

Mn,i

(W).

Since

M (W)

=

N n=1

Mn

(W

),

we

have

NN

P

(X



M

(W))

(1)
=

P (X  Mn (W)) =

[1 - P (X / Mn (W))]

n=1

n=1


(2) N

d1


(3)

 1 - P X / Mn,i (W)  

n=1

i=1

1

-

d1

2 B

sin ()

1 2

,

d0 -1 2

N
,

where in (1) we used the independence of x(n) nN=1, in (2) we use the union bound, and in (3) we use eq. (13.2) from Lemma 39. Taking the log and we using the asymptotic expansion of the beta

function B

1 2

,

x

=

/x + O x-3/2 for large x, we get

log P (X  M (W))  N log 1 -

2 

d0d1

sin



+

O

d1d0-1/2 sin 

=-

2 

d0-1/2

+

O

d0-3/2/N + d-0 1N -2

,

where in the last line we recalled sin  = 1/N . Recalling that d1 d0 N , we find

P (X / M (W))  1 - exp

-

2 

d0-1/2



2 

d0-1/2

31

Under review as a conference paper at ICLR 2018

13.4 PROOF OF LEMMA 23:

Lemma 42. (Lemma 23 restated). Let X  Rd0×N be a standard random Gaussian matrix of datapoints. Then we can find, with probability 1, (X, y)-dependent matrices W and z as in

Theorem 8 (where d1 4 N/ (2d0 - 2) ). Moreover, in the limit N  , where N/d0 d0 N , for any y, we can bound the probability of not having an angular margin (eq. (13.3)) with sin  =

1/ (d1d0N ) by

P (X / M (W))

8 

d-0 1/2

+

2d10/2log d0 N

Proof. In this proof we heavily rely on the notation and results from the proof of in appendix section

9. Without loss of generality we assume S1+ = [d0 - this proof is significantly more complicated since the

1]. Unfortunately, we can't use Lemma constructed solution W depends on X

41 ­ (we

keep this dependence implicit, for brevity). Similarly to the proof of Lemma 41, we define,

Mi,n (W)

X  Rd0×N |

x(n) wi x(n) wi

> sin ()

and Mi (W)

N n=1

Mi,n

(W),

so

M

(W)

=

d1 i=1

Mi

(W).

We

have

(1) d1
P (X  M (W)) = 1 - P (X / M (W))  1 - P (X / Mi (W))

i=1

(=2) 1 - d1P (X / M1 (W)) = 1 - d1 (1 - P (X  M1 (W))) , (13.7)
where in (1) we used the union bound, and in (2) we used the fact that, from symmetry, i : P (X / Mi (W)) = P (X / M1 (W)). Next, we examine the minimal angular margin in M1,n: separately for n < d0 and n  d0. Recalling the construction of W in appendix section 9, we have, for n < d0:

min
i,n<d0

x(n) wi x(n) wi

(w~ 1 ± 2w^ 1) x(n) = min
n<d0,± w~ 1 ± 2w^ 1 x(n)

(1)
= min

2

n<d0,± w~ 1 ± 2w^ 1

(2)
=

 1/

1 + 2

2 1

,

x(n)

w^ 1 maxn<d0 x(n)

(13.8)

where in (1) we used n < d0: x(n) w^ 1 = 1 and x(n) w~ 1 = 0 , from the construction of w~ 1 and
w^ 1 (eqs. (9.2), (9.5), and (9.4)), and in (2) we used the fact that w^ 1 w~ 1 = 0 from eq. (9.4) together with w~ 1 = w^ 1 from eq. (9.5), and 2 =  1 from eq. (9.7).

For n  d0 :

min
i,nd0

x(n) wi x(n) wi

= min (w~ 1 ± 1w^ 1) x(n)  (1 - ) 1 min w^ 1 x(n) ,

nd0,± w~ 1 ± 1w^ 1 x(n)



1+

2 1

nd0

w^ 1

x(n)

(13.9)

where we used the fact that n  d0 : 2 w^ 1 x(n)   w~ 1 x(n) , from eq. (9.7), and also that

w^ 1 w~ 1 = 0 from eq. (9.4).

We substitute eqs. (13.8) and (13.9) into P (X  M1 (W)): P (X  M1 (W))

P
(1)
P

 1/

1 + 2

2 1

w^ 1 maxn<d0 x(n)

 w^ 1 maxn<d0 x(n)

(1 - ) > sin ,

1

min

w^ 1 x(n)

> sin 



1+

2 1

nd0

w^ 1

x(n)

(1 - )

> sin ,

 min

x(1n)

> sin ,

 nd0 x(n)

1 >

1+

2 1

(13.10)

(2)
P

 >
 sin 

w^ 1

,  > max
n<d0

x(n)

P

(1 - )  min

x1(n)

 nd0 x(n)

> sin ,

1 >

1+

2 1

,

32

Under review as a conference paper at ICLR 2018

where in (1) we rotate the axes so that w^ 1  [1, 0, 0 . . . , 0] axes w~ 1  [0, 1, 0, 0 . . . , 0] ­ this is possible due to the spherical symmetry of x(n), and the fact that w^ 1 and w~ 1 are functions of x(n) for n < d0 (from eqs. (9.4) and (9.2)), and as such, they are independent from x(n) for n  d0, in (2) we use that fact that w^ 1 and maxn<d0 x(n) are functions of x(n) for n < d0 , and as such, they
are independent from x(n) for n  d0. Thus,

P (X  M1 (W))



1-P



  sin 

w^ 1

or   max x(n)
n<d0

·

1-P

(1 - )  min
 nd0

x1(n) x(n)

 sin  or

1 

1+

2 1

(1) 



1-P

  sin 

w^ 1

- P   max x(n)
n<d0

·

1-P

(1 - )  min
 nd0

x1(n) x(n)

 sin 

-P

1 

1+

2 1

= P  > max x(n)
n<d0



-P

  sin 

w^ 1

(13.11)

·

P

(1 - )  min
 nd0

x1(n) x(n)

> sin 

-P

1 

1+

2 1

,

where in (1) we use the union bound on both probability terms.

All that remains is to calculate each remaining probability term in eq. (13.11). First, we have

P

1 

1+

2 1

=1-P

 < 1 - 2

1

(=1) 1 - P

min w~ i x(n) >   1

nd0 w^ i x(n)

1 - 2 

(=2) 1 - P

min x(2n) >  

1

nd0 x(1n)

1 - 2 

(=3) 1 - P

x(21) >  

1

x(11) 1 - 2 

N -d0-1

(4)
 1-

1 - 2 arctan



1

 1 - 2 

N
,
(13.12)

where in (1) we used eq. (9.7), in (2) we recall that in eq. (13.10) we rotated the axes so that

w^ 1  [1, 0, 0 . . . , 0] axes w~ 1  [0, 1, 0, 0 . . . , 0], in (3) we used the independence of different x(n), and in (4) we used the fact that the ratio of two independent Gaussian variables is distributed

according to the symmetric Cauchy distribution, which has the cumulative distribution function

P (X

>

x)

=

1 2

-

1 

arctan (x),

and

therefore

P (|X|

>

x)

=

1-

2 

arctan (x).

Second, we use eq. (13.2)

P

min
nd0

x(1n) x(n)

 sin  > (1 - ) 

2 sin 

>

1- (1 - ) B

1 2

,

d0 -1 2

N
.

(13.13)

Third, x(n) 2 is distributed according to the chi-square distribution of order d0, so for 2 > d0,

P

2
x(n)  2

 2 exp 1 - 2/d0 /d0 d0/2 .

Therefore,

P

2
max x(n) < 2

>

1-

2 exp

1 - 2/d0

/d0

d0 /2

d0 -1
.

n<d0

(13.14)

33

Under review as a conference paper at ICLR 2018

Lastly, we bound w~ 1 = w^ 1 (from eq. (9.5)). From eq. (9.4), we have

w^ 1 X[d0-1] = [1, . . . , 1, 1] , where X[d0-1] has a singular value decomposition

(13.15)

d0
X[d0-1] = iuivi ,
i=1
with i being the singular values, and ui and vi being the singular vectors. The singular values are ordered from smallest to largest, and 1 = 0 with u1 = w~ 1, from eq. (9.2). With probability 1, the other d0 - 1 singular value are non-zero: they are the square roots of the eigenvalues of the random matrix X[d0-1]X[d0-1]  Rd0-1×d0-1. Taking the squared norm of eq. (13.15), we have

d0

d0 - 1 = w^ 1 X[d0-1]X[d0-1]w^ 1 =

i2 ui w^ 1 2  22 w^ 1 2 ,

i=1

(13.16)

where the last inequality stems from the fact that u1 w^ 1 = w~ 1 w^ 1 = 0 (from eq. (9.4)), so the minimal possible value is attained when u2 w^ 1 = w^ 1 . The minimal nonzero singular value, 2,
can be bounded using the following result from (Rudelson & Vershynin, 2010, eq. (3.2))

Since

P

min
rRd0

X[d0]r

 d-0 1/2

 .

we have,

2 = min
rRd0 -1

X[d0-1]r

 min
rRd0

X[d0]r

P 2 < d-0 1/2  .

Combining this with eq. (13.16) we get



P

<  sin 

w1

 d0 sin . 

(13.17)

Lastly, combining eqs. (13.12), (13.13), (13.14) and (13.17) into eqs. (13.7) and (13.11), we get, for 2 > d0,

P (X  M (W))

 1 - d1 1 -

1-

2 exp

1 - 2/d0

/d0

d0 /2

d0-1 - d0 sin  



2 sin 

·

1- (1 - ) B

1 2

,

d0 -1 2

N
- 1 - 2 arctan   1  1 - 2 


N


 1 - d1 1 -

1 - (log d0 exp (1 - log d0))d0/2

d0 -1

-

2d30/2log d1N

d0

  1-

81

1

 d1d10/2N + O N d1d03/2

N  - 0.45N  ,

where

in

the

last

line

we

take



=



=



=

 1/ 2,



=

d01/2log d0,

sin 

=

1/ (d1d0N ).

Using

the asymptotic expansion of the beta function B

1 2

,

x

=

/x + O x-3/2 for large x, we obtain,

34

Under review as a conference paper at ICLR 2018

for sin  = 1/ (d1d0N ) 1 - P (X  M (W))

 d1 1 -

1 - exp - d0 log d0 2 e log d0

d0 -1

-

2d01/2log d0 d1N

 N 

· 1-

81

1

 N d1d10/2 + O N d1d30/2

- 2-N 

= d1

1-

1

-

2d10/2log d0 d1N

+

O

d0 exp

- d0 log 2

d0 log d0

· 1-

81 +O
 d1d10/2

1 d1 d03/2

+

1 d12d0N

+ d12-N

+ d1d0 exp

- d0 log 2

d0 log d0

=

8 

1 d10/2

+

2d10/2log N

d0

+

O

1 d30/2

+

d01/4 d1N

+ d12-N

+ d1d0 exp

- d0 log 2

d0 log d0

.

Thus, taking the log, and using log (1 - x) = -x + O x2 , we obtain, forsin  = 1/ (d1d0N )

log P (X  M (W))

 log 1 -

8 

1 d01/2

-

2d01/2

 log

d0

N

+O

1 d03/2

+

d01/4 d1 N

+ d12-N

+ d0 exp

- d0 log 2

=-

8 1 - 2d01/2log d0 + O

 d01/2

N

1 d03/2

+

d10/4 d1N

+ d12-N

+ d0 exp

- d0 log 2

d0 log d0

d0 log d0
.

Recall that d1 4 N/ (2d0 - 2) = N/d0. Taking the limit N  , d0   with d1 d0 N , we

have

P (X / M (W))  1 - exp

-

8 

d0-1/2

-

2d01/2

 log

N

d0



8 

d0-1/2

+

2d10/2

 log

d0

N

Part III
Numerical Experiments - implementation details
Code and trained models for CIFAR and ImageNet results is available here https://github. com/MNNsMinima/Paper. In MNIST, CIFAR and ImageNet we performed binary classification on between the original odd and even class numbers. In we performed this binary classification between digits 0 - 4 and 5 - 9. Weights were initialized to be uniform with mean zero and variance 2/d, where d is fan-in (here the width of the previous neuron layer), as suggested in (He et al., 2015). In each epoch we randomly permuted the dataset and used the Adam (Kingma & Ba, 2015) optimization method (a variant of SGD) with 1 = 0.9, 2 = 0.99,  = 10-8. Different learning rates and mini-batch sizes were selected for each dataset and architecture. In CIFAR10 and ImageNet we used a learning-rate of  = 10-3 and a mini-batch size of 1024; also, ZCA whitening of the training samples was done to remove correlations between the input dimensions, allowing faster convergence. We define L as the number of weight layers. For the random dataset we use a mini-batch size of min (N/2, d/2) with learning rate  = 0.1 and 0.05, for L = 2 and 3, respectively. In the random data parameter scans the training was done for no more than 4000 epochs ­ we stopped if MCE = 0 was reached.
35

