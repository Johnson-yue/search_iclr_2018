Under review as a conference paper at ICLR 2018
LOSS-AWARE WEIGHT QUANTIZATION OF DEEP NET-
WORKS
Anonymous authors Paper under double-blind review
ABSTRACT
The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the fullprecision network.
1 INTRODUCTION
The last decade has witnessed huge success of deep neural networks in various domains such as computer vision, speech recognition, and natural language processing (LeCun et al., 2015). However, their huge size often hinders deployment to small computing devices such as cell phones and the internet of things. Recently, many attempts have been made to reduce the model size. One common approach is to prune an already-trained dense network (Han et al., 2016; Kim et al., 2016). However, the weights removed may mostly come from the fully-connected layers where computations are cheap, and the resultant time reduction is insignificant. Li et al. (2017b) and Molchanov et al. (2017) proposed to prune filters in the convolutional neural networks based on their magnitudes or significance to the loss. However, the pruned network has to be retrained, which is again expensive.
Another direction is to use more compact models. GoogleNet (Szegedy et al., 2015) and ResNet (He et al., 2016) replace the fully-connected layers with simpler global average pooling. However, they are also deeper. SqueezeNet (Howard et al., 2017) reduces the model size by replacing most of the 3 × 3 filters by 1 × 1 filters. This is less efficient on smaller networks because of the dense 1 × 1 convolutions are costly. MobileNet (Iandola et al., 2016) compresses the model using separable depth-wise convolution. ShuffleNet (Zhang et al., 2017) utilizes pointwise group convolution and channel shuffle to reduce the computation cost while maintaining accuracy. However, highly optimized group convolution and depth-wise convolution implementations are required. Alternatively, Novikov et al. (2015) compressed the model by using a compact multilinear format to represent the dense weight matrix. The CP and Tucker decompositions have also been used on the kernel tensor in CNNs (Lebedev et al., 2014; Kim et al., 2016). However, they often need expensive fine-tuning.
Another approach to effectively compress the network and accelerate training is by quantizing each full-precision weight to a small number of bits. This can be divided into two sub-categories, depending on whether pre-trained models are used (Lin et al., 2016a; Mellempudi et al., 2017) or the quantized model is trained from scratch (Courbariaux et al., 2015; Li et al., 2017a). Some of these also directly learn with the low-precision weights, but they usually suffer from severe accuracy deterioration (Li et al., 2017a; Miyashita et al., 2016). By keeping the full-precision weights during learning, Courbariaux et al. (2015) pioneered the BinaryConnect algorithm, which uses only one bit for each weight while still achieving state-of-the-art classification results. Rastegari et al. (2016) further incorporated weight scaling, and obtained better results. Instead of simply finding the closest binary approximation of the full-precision weights, a loss-aware scheme is proposed in (Hou et al., 2017). Beyond binarization, Ternary-Connect (Lin et al., 2016b) quantizes each weight to {-1, 0, 1}. Li & Liu (2016) and Zhu et al. (2016) added scaling to the ternarized weights, and DoReFa-Net (Zhou et al., 2016) further extended the quantization to more than three levels. How-
1

Under review as a conference paper at ICLR 2018

ever, these methods do not consider the effect of ternarization on the loss, and rely on heuristics in their procedures (Zhou et al., 2016; Zhu et al., 2016). Recently, a loss-aware low-bit quantized neural network is proposed in (Leng et al., 2017). However, it uses full-precision weights in the forward pass and the extra-gradient method for update, which are both expensive.
In this paper, we will first propose an efficient and disciplined ternarization scheme for network compression. Inspired by (Hou et al., 2017), we explicitly consider the effect of ternarization on the loss. This is formulated as an optimization problem which is then solved efficiently by the proximal Newton algorithm. When the loss surface's curvature is ignored, the proposed method reduces to that of (Li & Liu, 2016), and is also related to the projection step of (Leng et al., 2017). Next, we extend it to (i) allow the use of different scaling parameters for the positive and negative weights; and (ii) the use of m bits for weight quantization. Experiments on both feedforward and recurrent neural networks show that the proposed quantization scheme outperforms state-of-the-art algorithms.

NOTATIONS

For

a

vector

x,

 x

denotes

the

element-wise

square

root

(i.e.,

 [ x]i

=

 xi),

|x|

is

the

element-wise

absolute value, x p = (

i

|xi

|p

)

1 p

is

its

p-norm,

and

Diag(x)

returns

a

diagonal

matrix

with

x

on

the diagonal. For two vectors x and y, x y denotes the element-wise multiplication and x y

the element-wise division. Given a threshold , I(x) returns a vector such that [I(x)]i = 1 if xi > , -1 if xi < -, and 0 otherwise. I+(x) considers only the positive threshold, i.e., [I+(x)]i = 1 if xi > , and 0 otherwise. Similarly, [I-(x)]i = -1 if xi < -, and 0 otherwise. For a matrix X, vec(X) returns a vector by stacking all the columns of X, and diag(X) returns a

vector whose entries are from the diagonal of X.

2 RELATED WORK

Let the full-precision weights from all L layers be w = [w1 , w2 . . . wL ] , where wl = vec(Wl), and Wl is the weight matrix at layer l. The corresponding quantized weights will be denoted w^ = [w^ 1 , w^ 2 . . . w^ L ] .

2.1 WEIGHT BINARIZED NETWORKS
In BinaryConnect (Courbariaux et al., 2015), each element of wl is binarized to -1 or +1 by using the sign function: Binarize(wl) = +1 if wl  0, and -1 otherwise. In the Binary-Weight-Network (BWN) (Rastegari et al., 2016), a scaling parameter l > 0 is also included, i.e., Binarize(wl) = lbl, where bl  {-1, +1}nl and nl is the number of weights in wl. By minimizing the difference between wl and lbl, the optimal l, bl have the simple form: l = wl 1/nl, and bl = sign(wl).
Instead of simply finding the best binary approximation for the full-precision weight wlt at iteration t, the loss-aware binarized network (LAB) directly minimizes the loss w.r.t. the binarized weight ltblt (Hou et al., 2017). Let dlt-1 be a vector containing the diagonal of an approximate Hessian of the loss. It can be shown that lt = dtl-1 wlt 1/ dlt-1 1 and blt = sign(wlt).

2.2 WEIGHT TERNARIZED NETWORKS

In a weight ternarized network, zero is used as an additional quantized value. In ternary-connect (Lin

et al., 2016b), 0 otherwise).

Wa nhoenn-[nweglta]itiivsenwegeaigtihvte[,witlti]si

is stochastically quantized to -1

quantized to 1 with probability with probability -[wlt]i, and 0

[wlt]i (and otherwise.

In the ternary weight network (TWN) (Li & Liu, 2016), wlt is quantized to w^ lt = ltIlt (wlt), where

tllt

is a and

threshold lt, TWN

(i.e., [w^ lt]i = lt tries to minimize

if [wlt]i > lt, the 2-distance

-lt if [wlt]i < -lt and 0 otherwise). To obtain between the full-precision and ternarized weights,

leading to

lt

=

arg

max
>0

 2

1 I(wlt)

 |[wlt]i|
1 i:|[wlt]i|>lt

,

lt =

1 Ilt (wlt)

|[wlt]i|.
1 i:|[wlt]i|>tl

(1)

2

Under review as a conference paper at ICLR 2018

However, lt in (1) is difficult to solve. Instead, TWN simply sets tl = 0.7 · E(|wlt|) in practice. In TWN, one scaling parameter (lt) is used for both the positive and negative weights. In the trained ternary quantization (TTQ) network (Zhu et al., 2016), different scaling parameters (lt and lt) are used. The weight is thus quantized to w^ lt = ltI+lt (wlt) + ltI-tl (wlt). The scaling parameters are learned by gradient descent. As for lt, two heuristics are used. The first one sets tl to be a constant fraction of max(|wlt|), while the second one sets tl such that the sparsity at all layers are the same.

2.3 WEIGHT QUANTIZED NETWORKS

In a weight quantized network, m bits are used to represent each weight. Let Q be a set of

(2k + 1) quantized values, where k = 2m-1 - 1. The two popular choices are linear quan-

tization, with Q =

-1,

-

k-1 k

,

.

.

.

,

-

1 k

,

0,

1 k

,

.

.

.

,

k-1 k

,

1

,

and

logarithmic

quantization,

with

Q=

-1,

-

1 2

,

.

.

.

,

-1
2k-1

,

0,

1 2k-1

,

.

.

.

,

1 2

,

1

.

By limiting the quantized values to powers of

two, logarithmic quantization is advantageous in that expensive floating-point multiplications are

replaced by cheaper bit-shift operations. When m = 2, both schemes reduce to Q = {-1, 0, 1}.

In the DoReFa-Net (Zhou et al., 2016), weight wlt is heuristically quantized to m-bit, with:1

[w^ lt]i = 2 · quantizem

tanh([wlt]i) 2 max(| tanh([wlt]i)|)

+

1 2

-1

in

{-1,

-

2m -2 2m -1

,

.

.

.

,

-

1 2m -1

,

1 2m -1

,

.

.

.

,

2m 2m

-2 -1

,

1},

where

quantizem(x)

=

1 2m -1

round((2m

-

1)x). Similar to loss-aware binarization (Hou et al., 2017), Leng et al. (2017) proposed a loss-aware

quantized network called low-bit neural network (LBNN). The alternating direction method of mul-

tipliers (ADMM) (Boyd et al., 2011) is used for optimization. At the tth iteration, the full-precision

weight wlt is first updated by the method of extra-gradient (Vasilyev et al., 2010):

w~ lt = wlt-1 - tlL(wlt-1), wlt = wlt-1 - tlL(w~ lt),

(2)

where L is the augmented Lagrangian in the ADMM formulation, and t is the stepsize. Next, wlt

is projected to the space of m-bit quantized weights, so that w^ lt is of the form lbl, where l > 0,

and bl 

-1,

-

1 2

,

.

.

.

,

-

1 2k-1

,

0,

1 2k-1

,

.

.

.

,

1 2

,

1

.

3 LOSS-AWARE QUANTIZATION

TWN (Li & Liu, 2016) simply finds the closest ternary approximation of the full precision weights at each iteration, while TTQ (Zhu et al., 2016) sets the ternarization threshold heuristically. Inspired by the LAB (Hou et al., 2017), we consider the loss explicitly during quantization and obtain the quantization thresholds and scaling parameter by solving an optimization problem.

3.1 TERNARIZATION USING PROXIMAL NEWTON ALGORITHM

As in TWN, the weight wl is ternarized as w^ l = lbl, where l > 0 and bl  {-1, 0, 1}nl . Given a loss function , we formulate weight ternarization as the following optimization problem:

min (w^ ) : w^ l = lbl, l > 0, bl  Qnl , l = 1, . . . , L,
w^

(3)

where Q is the set of desired quantized values. As in LAB, we will solve this using the proximal Newton method (Lee et al., 2014; Rakotomamonjy et al., 2016). At iteration t, the objective is replaced by the second-order expansion

(w^ t-1) +  (w^ t-1)

(w^

-

w^ t-1)

+

1 (w^

-

w^ t-1)

Ht-1(w^ - w^ t-1),

2

(4)

where Ht-1 is an estimate of the Hessian of at w^ t-1. We use the diagonal equilibration preconditioner (Dauphin et al., 2015), which is robust in the presence of saddle points and also readily available in popular stochastic deep network optimizers such as Adam (Kingma & Ba, 2015). Let Dl

1Note that the quantized value of 0 is not used in DoReFa-Net.

3

Under review as a conference paper at ICLR 2018

be the approximate diagonal Hessian at layer l, we use D = Diag([diag(D1) , . . . , diag(DL) ] ) as an estimate of H. Substituting (4) into (3), we solve the following subproblem at the t-th iteration:

minw^ t

 (w^ t-1) (w^ t - w^ t-1) + 1 (w^ t - w^ t-1) Dt-1(w^ t - w^ t-1) 2

(5)

s.t. w^ lt = ltblt, lt > 0, btl  Qnl , l = 1, . . . , L.

Proposition 3.1 The objective in (5) can be written as

1L min w^ t 2
l=1

2
dlt-1 (w^ lt - wlt) ,

(6)

where dlt-1  diag(Dtl-1), and wlt  w^ lt-1 - l (w^ t-1)

dtl-1.

(7)

Obviously, this objective can be minimized layer by layer. Each proximal Newton iteration thus

consists of two steps: (i) Obtain wlt in (7) by gradient descent along l (w^ t-1), which is precondi-

tioned by the (ii) Quantize

adaptive learning rate 1 wlt to w^ lt by minimizing

dthlte-1scsaoletdhadtitfhfeerreensccealbeedtwdiemenenws^ilot nasndhawvelt

similar in (6).

curvatures; Intuitively,

when the curvature is low ([dlt-1]i is small), the loss is not sensitive to the weight and ternarization

error can be less penalized. When the loss surface is steep, ternarization has to be more accurate.

Though the constraint in (6) is more complicated than that in LAB, interestingly the following simple relationship can still be obtained for weight ternarization.

Proposition 3.2 With Q = {-1, 0, 1}, and the optimal w^ lt in (6) of the form b. For a fixed b,

=

b

dlt-1 wlt b dlt-1 1

1 ; whereas when  is fixed, b = I/2(wlt).

Equivalently, b can ment to the nearest

be written element in

as Q.

Q(wlt/), where Q(·) projects each Further discussions on how to solve for

entry of lt will

the input argube presented in

Sections 3.1.1 and 3.1.2. When the curvature is the same for all dimensions at layer l, the following

Corollary shows that the solution above reduces that of TWN.

Corollary 3.1 When Dtl-1 = I, lt reduces to the TWN solution in (1) with tl = lt/2.
In other words, TWN corresponds to using the proximal gradient algorithm, while the proposed method corresponds to using the proximal Newton algorithm with diagonal Hessian. In composite optimization, it is known that the proximal Newton algorithm is more efficient than the proximal gradient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016). Moreover, note that the interesting relationship tl = lt/2 is not observed in TWN, while TTQ completely neglects this relationship.
In LBNN (Leng et al., 2017), its projection step uses an objective which is similar to (6), but without using the curvature information. Besides, their wlt is updated with the extra-gradient in (2), which doubles the number of forward, backward and update steps, and can be costly. Moreover, LBNN uses full-precision weights in the forward pass, while all other quantization methods including ours use quantized weights (which eliminates most of the multiplications and thus faster training).

It can be shown that the proximal Newton algorithm converges (Hou et al., 2017). In practice, it is
important to keep the full-precision weights during update (Courbariaux et al., 2015). Hence, we replace (7) by wlt  wlt-1 - l (w^ t-1) dlt-1. The whole procedure which will be called LossAware Ternarization (LAT), is shown in Algorithm 3 of Appendix B. It is similar to Algorithm 1 of LAB (Hou et al., 2017), except that lt and btl are computed differently. In step 4, following (Li & Liu, 2016), we first rescale input xlt-1 with l, so that multiplications in dot products and convolutions become additions. Algorithm 3 can also be easily extended to ternarize weights in recurrent
networks. Interested readers are referred to (Hou et al., 2017) for details.

3.1.1 EXACT SOLUTION OF lt

To simplify notations, we drop the superscripts and subscripts. From Proposition 3.2,

=

b d w 1, b d1

b = I/2(w).

(8)

4

Under review as a conference paper at ICLR 2018

We now consider how to solve for . First, we introduce some notations. Given a vector

x = [x1, x2, . . . , xn], and an indexing vector s  Rn whose entries are a permutation of {1, . . . , n},

perms(x) returns the vector [xs1 , xs2 , . . . xsn ], and cum(x) = [x1,

2 i=1

xi,

.

.

.

,

n i=1

xi]

returns

partial sums for elements in x. For example, let a = [1, -1, -2], and b = [3, 1, 2]. Then,

permb(a) = [-2, 1, -1] and cum(a) = [1, 0, -2].

We sort elements in |w| in descending order, and let the vector containing the sorted indices be s. For example, if w = [1, 0, -2], then s = [3, 1, 2]. From (8),

=

I/2(w) d I/2(w)

w d1

1

=

[cum(perms(|d w|))]j [cum(perms(|d|))]j

= 2cj,

(9)

where c = cum(perms(|d w|)) cum(perms(d)) 2, and j is the index such that

[perms(|w|)]j > cj > [perms(|w|)]j+1.

(10)

For simplicity of notations, let the dimensionality of w (and thus also c) be n, and the operation

find(condition(x)) returns all indices in x that satisfies the condition. It is easy to see that any j
satisfying (10) is in S  find([perms(|w|)][1:(n-1)] - c[1:(n-1)]) ([perms(|w|)][2:n] - c[1:n-1]) < 0), where c[1:(n-1)] is the subvector of c with elements in the index range 1 to n - 1. The optimal  (= 2cj) is then the one which yields the smallest objective in (6), which can be simplified by
Proposition 3.3 below. The procedure is shown in Algorithm 1.

Proposition 3.3 The optimal lt of (6) equals 2 arg maxcj:jS cj2 · [cum(perms(dlt-1))]j.

Algorithm 1 Exact solver of (6)

1: 2:

Input: full-precision s = arg sort(|wlt|);

weight

wlt,

diagonal

entries

of

the

approximate

Hessian

dlt-1.

3: c = cum(perms(|dtl-1 wlt|)) cum(perms(dtl-1)) 2;

4: S = find(([perms(|wlt|)][1:(n-1)] - c[1:(n-1)]) ([perms(|wlt|)][2:n] - c[1:n-1]) < 0);

5: lt = 2 arg maxcj:jS c2j · [cum(perms(dlt-1))]j ;

6: btl = Ilt/2(wlt);

7: Output: w^ lt = ltblt.

3.1.2 APPROXIMATE SOLUTION OF lt
In case the sorting operation in step 2 is expensive, lt and btl can be obtained by alternating the iteration in Proposition 3.2 (Algorithm 2). Empirically, it converges very fast, usually in 5 iterations.
Algorithm 2 Approximate solver for (6). 1: Input: blt-1, full-precision weight wlt, diagonal entries of the approximate Hessian dlt-1. 2: Initialize:  = 1.0, old = 0.0, b = blt-1, = 10-6; 3: while | - old| > do 4: old = ;  = ;5: b dlt-1 wlt 1
b dlt-1 1
6: b = I/2(wlt); 7: end while 8: Output: w^ lt = b.

3.2 EXTENSION TO TERNARIZATION WITH TWO SCALING PARAMETERS

As in TTQ (Zhu et al., 2016), we can use different scaling parameters for the positive and negative weights in each layer. The optimization subproblem at the tth iteration then becomes:

minw^ t s.t.

 (w^ t-1) (w^ t - w^ t-1) + 1 (w^ t - w^ t-1) Dt-1(w^ t - w^ t-1) 2
w^ lt  {-lt, 0, lt}nl , lt > 0, lt > 0, l = 1, . . . , L.

(11)

5

Under review as a conference paper at ICLR 2018

Proposition 3.4 The optimal w^ lt in (5) is of the form w^ lt = ltptl + ltqlt, where lt

plt dlt-1 wlt plt dlt-1 1

1 , ptl

= I+tl /2(wlt), lt =

qlt dlt-1 wlt qtl dlt-1 1

1 , and qlt

= I-lt/2(wlt).

=

The exact and approximate solutions for lt and lt can be obtained in a similar way as in Sections 3.1.1 and 3.1.2. Details are in Appendix C.

3.3 EXTENSION TO LOW-BIT QUANTIZATION

For m-bit quantization, where m > 2, the optimization problem can be derived by simply changing the set of desired quantized values in (3) as Q with k = 2m-1 - 1 quantized values. The preconditioned gradient descent step is still a gradient descent step with adaptive learning rates like LAT. Similar to the procedure in Algorithm 2, the optimization problem can be solved efficiently by alternating minimization over b and  by using the following Proposition.

Proposition 3.5 With Q =

-1,

-

k-1 k

,

.

.

.

,

-

1 k

,

0,

1 k

,

.

.

.

,

k-1 k

,

1

for linear quantization or Q =

-1,

-

1 2

,

.

.

.

,

-1
2k-1

,

0,

1 2k-1

,

.

.

.

,

1 2

,

1

for logarithmic quantization, and the optimal w^ lt in (6) of

the form b. For a fixed b,  =

b

dtl-1 wlt b dlt-1 1

1

;

whereas

when



is

fixed,

b

=

Q(

wlt 

).

4 EXPERIMENTS

In this section, we perform experiments on both feedforward and recurrent neural networks. The following methods are compared: (i) the original full-precision network; (ii) weight-binarized networks, including BinaryConnect (Courbariaux et al., 2015), Binary-Weight-Network (BWN) (Rastegari et al., 2016), and Loss-Aware Binarized network (LAB) (Hou et al., 2017); (iii) weight ternarized networks, including Ternary Weight Networks (TWN) (Li & Liu, 2016), Trained Ternary Quantization (TTQ)2 (Zhu et al., 2016), Low-Bit Neural Network (LBNN) (Leng et al., 2017), the proposed Loss-Aware Ternarized network with exact solution (LAT e), approximate solution (LAT a), and with two scaling parameters (LAT2 e and LAT2 a); (iv) m-bit-quantized networks (with m > 2), including DoReFa-Net (Zhou et al., 2016), the proposed loss-aware quantized network with linear quantization (LAQk(linear)), and logarithmic quantization (LAQk(log)). We follow the setup in (Hou et al., 2017). Since weight quantization can be viewed as a form of regularization (Courbariaux et al., 2015), we do not use other regularizers such as Dropout and weight decay.

4.1 FEEDFORWARD NETWORKS
In this section, we perform experiments on four popular data sets using multilayer perceptron (MNIST) and convolutional neural networks (CIFAR-10, CIFAR-100 and SVHN). For MNIST, CIFAR-10, and SVHN, the setup is similar to that in (Courbariaux et al., 2015; Hou et al., 2017). Details can be found in Appendix D. For CIFAR-100, we use 45, 000 images for training, another 5, 000 for validation, and the remaining 10, 000 for testing. The testing errors are shown in Table 1.
Ternarization: On MNIST, CIFAR100 and SVHN, the weight-ternarized networks perform better than weight-binarized networks, and are comparable to the full-precision networks. Among the weight-ternarized networks, the proposed LAT and its variants have the lowest errors. On CIFAR-10, LAT a has similar performance as the full-precision network, but is outperformed by BinaryConnect. The code of LBNN is not publicly available. Results based on our re-implementation shows that it suffers from slow training and may not converge.
Figure 1(a) shows convergence of the training loss for LAT a on CIFAR-10, and Figure 1(b) shows the scaling parameter obtained at each CNN layer. As can be seen, the scaling parameters for the first and last layers (conv1 and linear3, respectively) are larger than the others. This agrees with the finding that, to maintain the activation variance and back-propagated gradients variance during forward and backward propagations, the variance of the weights between the lth and (l + 1)th layers should roughly follow 2/(nl +nl+1) (Glorot & Bengio, 2010). Hence, as the input and output layers are small, larger scaling parameters are needed for their high-variance weights.
2For TTQ, we follow the CIFAR-10 setting in (Zhu et al., 2016), and set tl = 0.005 max(|wlt|).

6

Under review as a conference paper at ICLR 2018

Table 1: Testing errors (%) on the feedforward networks. Algorithm with the lowest error in each

group is highlighted.

MNIST CIFAR-10 CIFAR-100 SVHN

no binarization

full-precision 1.11

10.38

39.06

2.28

binarization

BinaryConnect BWN LAB

1.28 1.31 1.18

9.86 10.51 10.50

46.42 43.62 43.06

2.45 2.54 2.35

1 scaling ternarization
2 scaling

TWN LBNN LAT e LAT a TTQ LAT2 e LAT2 a

1.23 10.64 2.12 35.84 1.15 10.47 1.14 10.38 1.20 10.59 1.20 10.45 1.19 10.48

43.49 -
39.10 39.19 42.09 39.01 38.84

2.37 -
2.30 2.30 2.38 2.34 2.35

3-bit quantization

DoReFa-Net LAQ3 (linear) LAQ3 (log)

1.31 1.20 1.16

10.54 10.67 10.52

45.05 38.70 38.50

2.39 2.34 2.29

(a) Training loss.

(b) Scaling parameter .

Figure 1: Convergence of training loss and scaling parameter obtained by LAT a on CIFAR-10.

Using Two Scaling Parameters: Compared to TTQ, the proposed LAT2 always has better performance. However, the extra flexibility of using two scaling parameters does not always translate to lower testing error. As can be seen, it outperforms algorithms with one scaling parameter only on CIFAR-100. We speculate this is because the capacities of deep networks are often larger than needed, and so the limited expressiveness of quantized weights may not significantly deteriorate performance. Indeed, as pointed out by (Courbariaux et al., 2015), weight quantization is a form of regularization, and can contribute positively to the performance.
Using More Bits: Among the 3-bit quantization algorithms, the proposed scheme with logarithmic quantization has the best performance. It also outperforms the other quantization algorithms on CIFAR-100 and SVHN. However, as discussed above, more quantization flexibility is useful only when the weight-quantized network does not have enough capacity.
4.2 RECURRENT NETWORKS
In this section, we follow (Hou et al., 2017) and perform character-level language modeling experiments on the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997). The training objective is the cross-entropy loss over all target sequences. Experiments are performed on three data sets: (i) Leo Tolstoy's War and Peace; (ii) source code of the Linux Kernel; and (iii) Penn Treebank Corpus (Taylor et al., 2003). For the first two, we follow the setting in (Karpathy et al., 2016; Hou et al., 2017). For Penn Treebank, we follow the setting in (Mikolov & Zweig, 2012). Crossentropy values on the test set are shown in Table 2. TTQ, when used on LSTM, is very sensitive to the initial values of its scaling parameters. In the experiment, we tried different initializations for TTQ and then report the best.
7

Under review as a conference paper at ICLR 2018

Ternarization: As in Section 4.1, the proposed LAT e and LAT a outperform other weight ternarization schemes, and are even better than the full-precision network on all three data sets. Figure 2 shows convergence of the training and validation losses on War and Peace. Among the ternarization methods, LAT and its variants converge faster than both TWN and TTQ.

Table 2: Testing cross-entropy values on the LSTM. Algorithm with the lowest cross-entropy value

in each group is highlighted.

War and Peace Linux Kernel Penn Treebank

no binarization

full-precision

1.268

1.326

1.083

binarization

BinaryConnect BWN LAB

2.942 1.313 1.291

3.532 1.307 1.305

1.737 1.078 1.081

1 scaling ternarization
2 scaling

TWN LAT e LAT a TTQ LAT2 e LAT2 a

1.290 1.248 1.253 1.272 1.239 1.245

1.280 1.256 1.264 1.302 1.258 1.258

1.045 1.022 1.024 1.031 1.018 1.015

3-bit quantization

DoReFa-Net LAQ3 (linear) LAQ3 (log)

1.349 1.282 1.268

1.276 1.327 1.273

1.017 1.017 1.009

4-bit quantization

DoReFa-Net LAQ4 (linear) LAQ4 (log)

1.328 1.294 1.272

1.320 1.337 1.319

1.019 1.046 1.016

(a) Training loss.

(b) Validation loss.

Figure 2: Convergence of the training and validation losses on War and Peace.

Using Two Scaling Parameters: LAT2 e and LAT2 a outperform TTQ on all three data sets. They also perform better than using one scaling parameter on War and Peace and Penn Treebank.
Using More Bits: The proposed LAQ always outperforms DoReFa-Net when 3 or 4 bits are used. Moreover, logarithmic quantization is better than linear quantization. Figure 3 shows distributions of the full-precision and quantized input-to-hidden weights of the input gate on War and Peace. As can be seen, the distributions are bell-shaped. Hence, logarithmic quantization can give finer resolutions to many of the weights which have small magnitudes.
As noted in Section 4.1, using more quantization bits may not be needed, and ternarization (using 2 bits) yields the lowest cross-entropy on War and Peace and Linux Kernel. On the other hand, using more bits can reduce the objective in (6), and thus fewer training epochs for convergence (Figure 2).
8

Under review as a conference paper at ICLR 2018

(a) Full-precision weights.

(b) Quantized weights.

Figure 3: Distributions of the full-precision and quantized weights by LAQ3(log) on War and Peace.

5 CONCLUSION
In this paper, we introduce a loss-aware weight ternarization algorithm that directly considers its effect on the loss during ternarization. The problem is solved using the proximal Newton algorithm. Each iteration consists of a gradient descent step with adaptive learning rates and a ternarization step that project onto a ternary set. An exact solution as well as an efficient approximate solution are provided. The procedure is also extended to the use of different scaling parameters for positive/negative weights and arbitrary m-bit quantization. Experiments on both feedforward and recurrent networks show that the proposed quantization scheme outperforms the current state-of-the-art.
REFERENCES
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1­122, 2011.
M. Courbariaux, Y. Bengio, and J.P. David. BinaryConnect: Training deep neural networks with binary weights during propagations. In NIPS, pp. 3105­3113, 2015.
Y. Dauphin, H. de Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex optimization. In NIPS, pp. 1504­1512, 2015.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTAT, pp. 249­256, 2010.
S. Han, H. Mao, and W.J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and Huffman coding. In ICLR, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, pp. 1735­1780, 1997.
L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of deep networks. In ICLR, 2017.
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. Preprint arXiv:1704.04861, 2017.
F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5MB model size. Preprint arXiv:1602.07360, 2016.
9

Under review as a conference paper at ICLR 2018
A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks. In ICLR, 2016.
Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. In ICLR, 2016.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. Preprint arXiv:1412.6553, 2014.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
J.D. Lee, Y. Sun, and M.A. Saunders. Proximal Newton-type methods for minimizing composite functions. SIAM Journal on Optimization, 24(3):1420­1443, 2014.
C. Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural network: Squeeze the last bit out with admm. Preprint arXiv:1707.09870, 2017.
F. Li and B. Liu. Ternary weight networks. Preprint arXiv:1605.04711, 2016.
H. Li, S. De, Z. Xu, C. Studer, H. Samet, and Goldstein T. Training quantized nets: A deeper understanding. In NIPS, 2017a.
H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. ICLR, 2017b.
D. Lin, S. Talathi, and S. Annapureddy. Fixed point quantization of deep convolutional networks. In ICML, pp. 2849­2858, 2016a.
Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio. Neural networks with few multiplications. In ICLR, 2016b.
N. Mellempudi, A. Kundu, D. Mudigere, D. Das, B. Kaul, and P. Dubey. Ternary neural networks with fine-grained quantization. Preprint arXiv:1705.01462, 2017.
T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. SLT, 12: 234­239, 2012.
D. Miyashita, E.H. Lee, and B. Murmann. Convolutional neural networks using logarithmic data representation. Preprint arXiv:1603.01025, 2016.
P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efficient transfer learning. ICLR, 2017.
A. Novikov, D. Podoprikhin, A. Osokin, and D.P. Vetrov. Tensorizing neural networks. In NIPS, pp. 442­450, 2015.
A. Rakotomamonjy, R. Flamary, and G. Gasso. DC proximal Newton for nonconvex optimization problems. IEEE Transactions on Neural Networks and Learning Systems, 27(3):636­647, 2016.
M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classification using binary convolutional neural networks. In ECCV, 2016.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, pp. 1­9, 2015.
A. Taylor, M. Marcus, and B. Santorini. The Penn treebank: An overview. In Treebanks, pp. 5­22. Springer, 2003.
F. P. Vasilyev, E. V. Khoroshilova, and A. S. Antipin. An extragradient method for finding the saddle point in an optimal control problem. Moscow University Computational Mathematics and Cybernetics, 34(3):113­118, 2010.
X. Zhang, X. Zhou, M. Lin, and J. Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. Preprint arXiv:1707.01083, 2017.
10

Under review as a conference paper at ICLR 2018 S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. DoReFa-Net: Training low bitwidth convolu-
tional neural networks with low bitwidth gradients. Preprint arXiv:1606.06160, 2016. C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary quantization. Preprint arXiv:1612.01064,
2016.
11

Under review as a conference paper at ICLR 2018

A PROOFS

A.1 PROOF OF PROPOSITION 3.1

With wlt in (7), the objective in (5) can be rewritten as

 (w^ t-1) (w^ t - w^ t-1) + 1 (w^ t - w^ t-1) Dt-1(w^ t - w^ t-1) 2

1L =
2
l=1

2
dlt-1 (w^ lt - (w^ lt-1 - l (w^ t-1) dtl-1)) + c1

1L =
2
l=1

2
dlt-1 (w^ lt - wlt) + c1

1L =
2
l=1

2
dlt-1 (ltbtl - wlt) + c1

=

1L 2

nl
[dtl-1]i(lt[btl ]i - [wlt]i)2 + c1,

l=1 i=1



where

c1

=

-

1 2

(

dlt-1

(l (w^ t-1)

dlt-1))2 is independent of lt and btl .

A.2 PROOF OF PROPOSITION 3.2

To simplify notations, we drop the subscript and superscript. Considering one particular layer, problem (6) is of the form:

min,b s.t.

1 2

n

di(bi - wi)2

i=1

 > 0, bi  {-1, 0, 1}.

When  is fixed,

bi

=

arg min
bi

1 2 di(bi

-

wi)2

=

1 2

di2

(bi

-

wi/)2

=

I/2(wi).

When b is fixed,



=

1 arg min
2

n

di(bi - wi)2

i=1

1 = arg min b
2

b

d 12 - b

d

w 1 + c2,

1 = arg min b
2

b

d1

-

b b

d b

w 1 2-1 b d1 2 b

d b

w d

2 1 1

+ c2

= b d w1 b b d1

= b d w 1. b d1

A.3 PROOF OF COROLLARY 3.1

When Dtl-1 = I, i.e., the curvature is the same for all dimensions in the lth layer, From Proposition 3.2,

lt =

b

dlt-1 wlt b dtl-1 1

1

=

Itl /2(wlt) wlt Itl /2(wlt) 1

1

=

1 Itl (wlt)

1

|[wlt]i|,
i:[wlt ]i >lt

12

Under review as a conference paper at ICLR 2018

tl

=

1 2

Ilt/2 wlt 1 = arg max

Itl /2 1

>0

This is the same as the TWN solution in (1).

 2

1 I(wlt)

 |[wlt]i|
1 i:[wlt]i>

.

A.4 PROOF OF PROPOSITION 3.3

For simplicity of notations, we drop the subscript and superscript. For each layer, we have an

optimization problem of the form 
arg min( d (b - w))2


= arg min b


b

b d 1 - b

d b

w1

2
-

b

d1 b

d b

w

2 1

d1

=

arg min


I/2(w)

I/2(w)

d1

-

I/2(w) d w 1 I/2(w) I/2(w) d 1

2
-

I/2(w) I/2(w)

I/2(w) I/2(w)

= arg min - I/2(w)

d

w

2
1,

 I/2(w) d 1

where the second equality holds as b = I/2(w). From (9), we have

- I/2(w)

d

w

2 1

I/2(w) d 1

=

- I/2(w) d w 1 · I/2(w) d 1

I/2(w) d w I/2(w) d 1

1·

I/2(w)

d1

= -2cj · 2cj · [cum(perms(d))]j = -2cj2 · [cum(perms(d))]j.
 Thus, the  that minimizes ( d (b - w))2 is  = 2 arg maxcj,jS cj2 · [cum(perms(d))]j.

w

2 1

d1

A.5 PROOF FOR PROPOSITION 3.4

For simplicity of notations, we drop the subscript and superscript, and consider the optimization

problem:

min,b

1 2

n

di(w^i - wi)2

i=1

s.t. w^i  {-, 0, +}.

Let f (w^i) = (w^i - wi)2. Then, f () = ( - wi)2, f (0) = wi2, and f (-) = ( + wi)2. It is easy to see that (i) if wi > /2, f () is the smallest; (ii) if wi < -/2, f (-1) is the smallest; (iii) if -/2  wi  /2, f (0) is the smallest. In other words, the optimal w^i satisfies

w^i = I+/2(wi) + I-/2(wi),

or equivalently, w^ = p + q, where p = I+/2(w), and q = I-(w).

Define w+ and w- such that [w+]i =

wi 0

wi > 0 otherwise,

and

[w-]i

=

wi 0

wi < 0 . Then, otherwise.

1 2

n

di(w^i

-

wi)2

=

1 2

n

di(pi

-

wi+)2

+

1 2

n

di(qi - wi-)2.

i=1 i=1

i=1

(12)

The objective in (12) has two parts, and each part can be viewed as a special case of the ternarization

step in Proposition 3.1 (considering only with positive or negative weights). Similar to the proof for

Proposition 3.2, we can obtain that the optimal w^ = p + q satisfies

= =

pdw p d1

1,

q

dw q d1

1,

p = I+/2(w), q = I-/2(w).

13

Under review as a conference paper at ICLR 2018

A.6 PROOF OF PROPOSITION 3.5



For simplicity of notations, we drop the subscript and superscript.

Since

1 2

(

d

(b - w))2 =

1 2

n i=1

di(bi

-

wi)2,

for

each

layer,

we

simply

consider

the

optimization

problem:

min,b

1 2

n

di(bi - wi)2

i=1

s.t.  > 0, bi  Q.

When  is fixed,

bi

=

arg min
bi

1 2 di(bi

- wi)2

=

1 2

di2

(bi

-

wi/)2

=

Q

wi 

.

When b is fixed,



=

1 arg min
2

n

di(bi - wi)2

i=1

1 = arg min b
2

b

d 12 - b

d

w 1 + c2

1 = arg min b
2

b

b d 1 - b

d b

w1

21 -

b

d1 2 b

d b

w

2 1

d1

= b d w1 b b d1

= b d w 1. b d1

B LOSS-AWARE TERNARIZATION ALGORITHM (LAT)

The whole procedure of LAT is shown in Algorithm 3.

C EXACT AND APPROXIMATE SOLUTIONS FOR TERNARIZATION WITH TWO SCALING PARAMETERS
Let there be n1 positive elements and n2 negative elements in w. For a n-dimensional vector x = [x1, x2, . . . , xn], define inverse(x) = [xn, xn-1, . . . , x1]. As is shown in (12), the objective can be separated into two parts, and each part can be viewed as a special case of ternarization step in Proposition 3.1, dealing only with positive or negative weights. Thus the exact and approximate solutions for lt and lt can separately be derived in a similar way as that of using one scaling parameter. The exact and approximate solutions for  and  for each layer at the tth time step are shown in Algorithms 4 and 5.

D EXPERIMENT DETAILS
D.1 SETUP FOR EXPERIMENTS ON FEEDFORWARD NETWORKS
The setup for the four data sets are as follows:
1. MNIST: This contains 28 × 28 gray images from 10 digit classes. We use 50, 000 images for training, another 10, 000 for validation, and the remaining 10, 000 for testing. We use the 4-layer model: 784F C - 2048F C - 2048F C - 2048F C - 10SV M, where F C is a fully-connected layer, and SV M is a L2-SVM output layer using the square hinge loss. Batch normalization with a minibatch size 100, is used to accelerate learning. The maximum number of epochs is 50. The learning rate for all the networks starts at 0.01, and decays by a factor of 0.1 at epochs 15 and 25.

14

Under review as a conference paper at ICLR 2018

Algorithm 3 Loss-Aware Ternarization (LAT) for training a feedforward neural network.

Input: Minibatch {(xt0, yt)}, current full-precision weights {wlt}, first moment {mtl-1}, second moment {vlt-1}, and learning rate t. 1: Forward Propagation

2: for l = 1 to L do

3: compute lt and blt using Algorithm 1 or 2; 4: rescale the layer-l input: x~lt-1 = ltxlt-1;

5: compute ztl with input x~lt-1 and binary weight btl ;

6: apply batch-normalization and nonlinear activation to ztl to obtain xlt;

7: end for

8: compute the loss using xLt and yt;

9: Backward Propagation

10:

initialize output layer's activation's gradient

  xtL

;

11: for l = L to 2 do

12:

compute

  xlt-1

using

  xtl

,

lt

and btl ;

13: end for

14: Update parameters using Adam

15: for l = 1 to L do

16:

compute gradients l

(w^ t) using

  xlt

and xtl-1;

17: update first moment mlt = 1mlt-1 + (1 - 1)l (w^ t);

18: 19:

update second moment compute unbiased first

vlt = 2 moment

vm^lt-tl 1=+m(1lt/-(1-2)(1t )l;

(w^ t)

l (w^ t));

20: compute unbiased second moment v^lt = vlt/(1 - 2t);

21:

compute

current

curvature

matrix

dtl

=

1 t

1 + v^lt ;

22: update full-precision weights wlt+1 = wlt - m^ lt dtl ; 23: update learning rate t+1 = UpdateLearningrate(t, t + 1);

24: end for

Algorithm 4 Exact solver for w^ lt with two scaling parameters.

1: 2:

Input: full-precision s1 = arg sort(wlt);

weight

wlt,

diagonal

entries

of

the

approximate

Hessian

dlt-1.

3: c1 = cum(perms1 (|dtl-1 wlt|)) cum(perms1 (|dlt-1|)) 2;

4: S1 = find[([perms1 (wlt)][1:(n1-1)] - [c1][1:(n1-1)]) [perms1 (wlt)][2:n1] - [c1][1:n1-1]) < 0);

5: lt = 2 arg maxci,iS1 [c1]i2 · [cum(perms1 (|dlt-1|))]i;

6: ptl = I+/2(wlt);

7: s2 = inverse(s1); 8: c2 = cum(perms2 (|dtl-1 wlt|)) cum(perms2 (|dtl-1|)) 2; 9: S2 = find(([-perms2 (wlt)][1:(n2-1)] - [c2][1:(n2-1)]) ([-perms2 (wlt)][2:n2] - [c2][1:n2-1]) <
0);

10: lt = 2 arg maxci,iS2 [c2]2i 11: qlt = I-/2(wlt);

[cum(perms2 (|dlt-1|))]i;

12: Output: w^ lt = ltptl + ltqlt.

2. CIFAR-10: This contains 32 × 32 color images from 10 object classes. We use 45, 000 images for training, another 5, 000 for validation, and the remaining 10, 000 for testing. The images are preprocessed with global contrast normalization and ZCA whitening. We use the VGG-like architecture:
(2×128C3)-M P 2-(2×256C3)-M P 2-(2×512C3)-M P 2-(2×1024F C)-10SV M,
where C3 is a 3 × 3 ReLU convolution layer, and M P 2 is a 2 × 2 max-pooling layer. Batch normalization with a minibatch size of 50, is used. The maximum number of epochs
15

Under review as a conference paper at ICLR 2018

Algorithm 5 Approximate solver for w^ lt with two scaling parameters
1: Input: btl-1, full-precision weight wlt, and diagonal entries of approximate Hessian dtl-1. 2: Initialize:  = 1.0, old = 0.0,  = 1.0, o = 0.0, b = btl-1, p = I+0 (b), q = I-0 (b),
10-6. 3: While | - old| > and | - old| > : 4: old = , old = ;  = ;5: p dlt-1 wlt 1
p dlt-1 1
6: p = I+/2(wlt);  = ;7: q dlt-1 wlt 1
q dlt-1 1
8: q = I-/2(wlt); 9: end While 10: Output: w^ lt = p + q.

=

is 200. The learning rate for the weight-binarized network starts at 0.03 while for all the other networks starts at 0.002, and decays by a factor of 0.5 after every 15 epochs. 3. CIFAR-100: This contains 32 × 32 color images from 100 object classes. We use 45, 000 images for training, another 5, 000 for validation, and the remaining 10, 000 for testing. The images are preprocessed with global contrast normalization and ZCA whitening. We use the VGG-like architecture:
(2×128C3)-M P 2-(2×256C3)-M P 2-(2×512C3)-M P 2-(2×1024F C)-10SV M,
where C3 is a 3 × 3 ReLU convolution layer, and M P 2 is a 2 × 2 max-pooling layer. Batch normalization with a minibatch size of 100, is used. The maximum number of epochs is 200. The learning rate for all the networks starts at 0.0005, and decays by a factor of 0.5 after every 15 epochs. 4. SVHN: This contains 32 × 32 color images from 10 digit classes. We use 598, 388 images for training, another 6, 000 for validation, and the remaining 26, 032 for testing. The images are preprocessed with global and local contrast normalization. The model used is:
(2×64C3)-M P 2-(2×128C3)-M P 2-(2×256C3)-M P 2-(2×1024F C)-10SV M.
Batch normalization with a minibatch size of 50, is used. The maximum number of epochs is 50. The learning rate for the weight-binarized network starts at 0.001 while for all the other networks starts at 0.0005, and decays by a factor of 0.1 at epochs 15 and 25.
D.2 SETUP FOR EXPERIMENTS ON RECURRENT NETWORKS
The setup for the three data sets are as follows:
1. Leo Tolstoy's War and Peace, which consists of 3258K characters of almost entirely English text with minimal markup and has a vocabulary size of 87. We use the same training/validation/test set split as in (Karpathy et al., 2016; Hou et al., 2017).
2. The source code of the Linux Kernel, which consists of 621K characters and has a vocabulary size of 101. We use the same training/validation/test set split as in (Karpathy et al., 2016; Hou et al., 2017).
3. The Penn Treebank data set (Taylor et al., 2003) which has been frequently used for language modeling. It contains 50 different characters, including English characters, numbers, and punctuations. We follow the setting in (Mikolov & Zweig, 2012), with 5,017K characters for training, 393K for validation, and 442K characters for testing.
We use a one-layer LSTM with 512 cells. The maximum number of epochs is 200, and the number of time steps is 100. The initial learning rate is 0.002. After 10 epochs, it is decayed by a factor of 0.98 after each epoch. The weights are initialized uniformly in [0.08, 0.08]. After each iteration, the gradients are clipped to the range [-5, 5]. For the weight-and-activation-binarized networks, we do
16

Under review as a conference paper at ICLR 2018 not binarize the inputs, as they are one-hot vectors in this language modeling task. All the updated weights are clipped to [-1, 1] for binarization and ternarization methods, but not for m-bit (m > 2) quantization methods.
17

