Under review as a conference paper at ICLR 2018
CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS
Anonymous authors Paper under double-blind review
ABSTRACT
The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach on spectral image classification, community detection, vertex classification and matrix completion tasks.
1 INTRODUCTION
In many domains, one has to deal with large-scale data with underlying non-Euclidean structure. Prominent examples of such data are social networks, genetic regulatory networks, functional networks of the brain, and 3D shapes represented as discrete manifolds. The recent success of deep neural networks and, in particular, convolutional neural networks (CNNs) LeCun et al. (1998) have raised the interest in geometric deep learning techniques trying to extend these models to data residing on graphs and manifolds. Geometric deep learning approaches have been successfully applied to computer graphics and vision Masci et al. (2015); Boscaini et al. (2015; 2016b;a); Monti et al. (2017a), brain imaging Ktena et al. (2017), and drug design Duvenaud et al. (2015) problems, to mention a few. For a comprehensive presentation of methods and applications of deep learning on graphs and manifolds, we refer the reader to the review paper Bronstein et al. (2016).
Related work. The earliest neural network formulation on graphs was proposed by Gori et al. (2005) and Scarselli et al. (2009), combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest in Li et al. (2015); Sukhbaatar et al. (2016)). The first CNN-type architecture on graphs was proposed by Bruna et al. (2013). One of the key challenges of extending CNNs to graphs is the lack of vector-space structure and shift-invariance making the classical notion of convolution elusive. Bruna et al. formulated convolution-like operations in the spectral domain, using the graph Laplacian eigenbasis as an analogy of the Fourier transform (Shuman et al. (2013)). Henaff et al. (2015) used smooth parametric spectral filters in order to achieve localization in the spatial domain and keep the number of filter parameters independent of the input size. Defferrard et al. (2016) proposed an efficient filtering scheme using recurrent Chebyshev polynomials applied on the Laplacian operator. Kipf & Welling (2016) simplified this architecture using filters operating on 1-hop neighborhoods of the graph. Atwood & Towsley (2016) proposed a Diffusion CNN architecture based on random walks on graphs. Monti et al. (2017a) (and later, Hechtlinger et al. (2017)) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs. In Monti et al. (2017b), spectral graph CNNs were extended to multiple graphs and applied to matrix completion and recommender system problems.
1

Under review as a conference paper at ICLR 2018

Main contribution. In this paper, we construct graph CNNs employing an efficient spectral filtering scheme based on Cayley polynomials that enjoys similar advantages of the Chebyshev filters (Defferrard et al. (2016)) such as localization and linear complexity. The main advantage of our filters over Defferrard et al. (2016) is their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph. We demonstrate experimentally that this affords our method greater flexibility, making it perform better on a broad range of graph learning problems.
Notation. We use a, a, and A to denote scalars, vectors, and matrices, respectively. z¯ denotes the conjugate of a complex number, Re{z} its real part, and i is the imaginary unit. diag(a1, . . . , an) denotes an n×n diagonal matrix with diagonal elements a1, . . . , an. Diag(A) = diag(a11, . . . , ann) denotes an n × n diagonal matrix obtained by setting to zero the off-diagonal elements of A. Off(A) = A - Diag(A) denotes the matrix containing only the off-diagonal elements of A. I is the identity matrix and A  B denotes the Hadamard (element-wise) product of matrices A and B. Proofs are given in the appendix.

2 SPECTRAL TECHNIQUES FOR DEEP LEARNING ON GRAPHS

Spectral graph theory. Let G = ({1, . . . , n}, E, W) be an undirected weighted graph, represented by a symmetric adjacency matrix W = (wij). We define wij = 0 if (i, j) / E and wij > 0 if (i, j)  E. We denote by Nk,m the k-hop neighborhood of vertex m, containing vertices that are at most k edges away from m. The unnormalized graph Laplacian is an n × n symmetric positive-semidefinite matrix u = D - W, where D = diag( j=i wij) is the degree matrix. The normalized graph Laplacian is defined as n = D-1/2D-1/2 = I - D-1/2WD-1/2. In the following, we use the generic notation  to refer to some Laplacian.
Since both normalized and unnormalized Laplacian are symmetric and positive semi-definite matrices,
they admit an eigendecomposition  =  , where  = (1, . . . n) are the orthonormal eigenvectors and  = diag(1, . . . , n) is the diagonal matrix of corresponding non-negative eigenvalues (spectrum) 0 = 1  2  . . .  n. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as (the square of) frequencies. Given a signal f = (f1, . . . , fn) on the vertices of graph G, its graph Fourier transform is given by ^f =  f . Given two signals f , g on the graph, their spectral convolution can be defined as the element-wise product of the Fourier transforms, f g =  ( g)( f ) =  diag(g^1, . . . , g^n)^f , which corresponds to the property referred to as the Convolution Theorem in the Euclidean case.

Spectral CNNs. Bruna et al. (2013) used the spectral definition of convolution to generalize CNNs on graphs, with a spectral convolutional layer of the form

flout = 

p
kG^ l,l k flin
l =1

.

(1)

Here the n × p and n × q matrices Fin = (f1in, . . . , fpin) and Fout = (f1out, . . . , fqout) represent respectively the p- and q-dimensional input and output signals on the vertices of the graph, k = (1, . . . , k) is an n × k matrix of the first eigenvectors, G^ l,l = diag(g^l,l ,1, . . . , g^l,l ,k) is a k × k diagonal matrix of spectral multipliers representing a learnable filter in the frequency domain, and
 is a nonlinearity (e.g., ReLU) applied on the vertex-wise function values. Pooling is performed
by means of graph coarsening, which, given a graph with n vertices, produces a graph with n < n
vertices and transfers signals from the vertices of the fine graph to those of the coarse one.

This framework has several major drawbacks. First, the spectral filter coefficients are basis dependent, and consequently, a spectral CNN model learned on one graph cannot be transferred to another graph. Second, the computation of the forward and inverse graph Fourier transforms incur expensive O(n2)
multiplication by the matrices ,  , as there is no FFT-like algorithms on general graphs. Third, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain (locality property simulates local reception fields, Coates & Ng (2011)); assuming k = O(n) Laplacian eigenvectors are used, a spectral convolutional layer requires O(pqk) = O(n) parameters to train.

2

Under review as a conference paper at ICLR 2018

To address the latter issues, Henaff et al. (2015) argued that smooth spectral filter coefficients result

in spatially-localized filters (an argument similar to vanishing moments). The filter coefficients are

represented as g^i = g(i), where g() is a smooth transfer function of frequency . Applying such

filter to signal f can be expressed as Gf = g()f = g() f =  diag(g(1), . . . , g(n)) f ,

where applying a function to a matrix is understood in the operator functional calculus sense (applying

the function to the matrix eigenvalues). Henaff et al. (2015) used parametric functions of the form

g() =

r j=1

j

j

(),

where

1(),

.

.

.

,

r

()

are

some

fixed

interpolation

kernels

such

as

splines,

and  = (1, . . . , r) are the interpolation coefficients used as the optimization variables during

the network training. In matrix notation, the filter is expressed as Gf = diag(B) f , where

B = (bij) = (j(i)) is a k × r matrix. Such a construction results in filters with r = O(1)

parameters, independent of the input size. However, the authors explicitly computed the Laplacian

eigenvectors , resulting in high complexity.

ChebNet. Defferrard et al. (2016) used polynomial filters represented in the Chebyshev basis

r
g(~) = jTj(~)
j=0

(2)

applied to rescaled frequency ~  [-1, 1]; here,  is the (r + 1)-dimensional vector of polynomial
coefficients parametrizing the filter and optimized for during the training, and Tj() = 2Tj-1() - Tj-2() denotes the Chebyshev polynomial of degree j defined in a recursive manner with T1() =  and T0() = 1. Chebyshev polynomials form an orthogonal basis for the space of polynomials of order r on [-1, 1]. Applying the filter is performed by g(~ )f , where ~ = 2-n 1 - I is the rescaled Laplacian such that its eigenvalues ~ = 2-n 1 - I are in the interval [-1, 1].

Such an approach has several important advantages. First, since g(~ ) =

r j=0

j

Tj

(~ )

contains

only matrix powers, additions, and multiplications by scalar, it can be computed avoiding the explicit

expensive O(n3) computation of the Laplacian eigenvectors. Furthermore, due to the recursive

definition of the Chebyshev polynomials, the computation of the filter g()f entails applying the Laplacian r times, resulting in O(rn) operations assuming that the Laplacian is a sparse matrix with

O(1) non-zero elements in each row (a valid hypothesis for most real-world graphs that are sparsely

connected). Second, the number of parameters is O(1) as r is independent of the graph size n. Third,

since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and a polynomial of

degree r of the Laplacian affects only r-hops, the resulting filters have guaranteed spatial localization.

A key disadvantage of Chebyshev filters is the fact that using polynomials makes it hard to produce narrow-band filters, as such filters require very high order r, and produce unwanted non-local filters. This deficiency is especially pronounced when the Laplacian has clusters of eigenvalues concentrated around a few frequencies with large spectral gap (Figure 3, middle right). Such a behavior is characteristic of graphs with community structures, which is very common in many real-world graphs, for instance, social networks. To overcome this major drawback, we need a new class of filters, that are both localized in space, and are able to specialize in narrow bands in frequency.

3 CAYLEY FILTERS

A key construction of this paper is a family of complex filters that enjoy the advantages of Chebyshev filters while avoiding some of their drawbacks. A Cayley polynomial of order r is a real-valued function with complex coefficients,

r

gc,h() = c0 + 2Re

cj(h - i)j(h + i)-j

(3)

j=1

where c = (c0, . . . , cr) is a vector of one real coefficient and r complex coefficients and h > 0 is the spectral zoom parameter, that will be discussed later. A Cayley filter G is a spectral filter defined on
real signals f by

r
Gf = gc,h()f = c0f + 2Re{ cj(h - iI)j(h + iI)-jf },

(4)

j=1

3

Under review as a conference paper at ICLR 2018

where the parameters c and h are optimized for during training. Similarly to the Chebyshev filters, Cayley filters involve basic matrix operations such as powers, additions, multiplications by scalars, and also inversions. This implies that application of the filter Gf can be performed without explicit expensive eigendecomposition of the Laplacian operator. In the following, we show that Cayley filters are analytically well behaved; in particular, any smooth spectral filter can be represented as a Cayley polynomial, and low-order filters are localized in the spatial domain. We also discuss numerical implementation and compare Cayley and Chebyshev filters.

Analytic properties. Cayley filters are best understood through the Cayley transform, from which

their name derives. Denote by eiR = {ei :   R} the unit complex circle. The Cayley transform

C(x)

=

x-i x+i

is

a

smooth

bijection

between

R

and

eiR

\

{1}.

The complex matrix C(h) =

(h - iI)(h + iI)-1 obtained by applying the Cayley transform to the scaled Laplacian h has

its spectrum in eiR and is thus unitary. Since z-1 = z for z  eiR, we can write cjCj(h) = cjC-j(h). Therefore, using 2Re{z} = z + z, any Cayley filter (4) can be written as a conjugate-
even Laurent polynomial w.r.t. C(h),

r
G = c0I + cjCj(h) + cjC-j(h).
j=1

(5)

Since the spectrum of C(h) is in eiR, the operator Cj(h) can be thought of as a multiplication by a pure harmonic in the frequency domain eiR for any integer power j,
Cj(h) = diag C(h1) j, . . . , C(hn) j  .

A Cayley filter can be thus seen as a multiplication by a finite Fourier expansions in the frequency domain eiR. Since (5) is conjugate-even, it is a (real-valued) trigonometric polynomial.

Note that any spectral filter can be formulated as a Cayley filter. Indeed, spectral filters g() are specified by the finite sequence of values g(1), . . . , g(n), which can be interpolated by a trigonometric polynomial. Moreover, since trigonometric polynomials are smooth, we expect low order Cayley filters to be well localized in some sense on the graph, as discussed later.

Finally, in definition (4) we use complex coefficients. If cj  R then (5) is an even cosine polynomial, and if cj  iR then (5) is an odd sine polynomial. Since the spectrum of h is in R+, it is mapped to the lower half-circle by C, on which both cosine and sine polynomials are complete and can represent
any spectral filter. However, it is beneficial to use general complex coefficients, since complex Fourier
expansions are overcomplete in the lower half-circle, thus describing a larger variety of spectral filters
of the same order without increasing the computational complexity of the filter.

Cayley r = 3

Chebyshev r = 3

Chebyshev r = 7

1

0 min

max min

max min

max

Figure 1: Filters (spatial domain, top and spectral domain, bottom) learned by CayleyNet (left) and ChebNet (center, right) on the MNIST dataset. Cayley filters are able to realize larger supports for the same order r.

Spectral zoom. To understand the essential role of the parameter h in the Cayley filter, consider C(h). Multiplying  by h dilates its spectrum, and applying C on the result maps the non-negative
spectrum to the complex half-circle. The greater h is, the more the spectrum of h is spread apart in R+, resulting in better spacing of the smaller eigenvalues of C(h). On the other hand, the smaller h is, the further away the high frequencies of h are from , the better spread apart are the high frequencies of C(h) in eiR (see Figure 2). Tuning the parameter h allows thus to `zoom' in to
different parts of the spectrum, resulting in filters specialized in different frequency bands.

4

|g()|

Under review as a conference paper at ICLR 2018

0 -0.5

0 -0.5

0 -0.5

Im Im Im

-1 -1

-0.5 0
Re

0.5

1

-1 -1

-0.5

0
Re

0.5

1

-1 -1

-0.5 0
Re

0.5

1

Figure 2: Eigenvalues of the unnormalized Laplacian hu of the 15-communities graph mapped on the complex unit half-circle by means of Cayley transform with spectral zoom values (left-to-right) h = 0.1, 1, and 10. The first 15 frequencies carrying most of the information about the communities are marked in red. Larger values of h zoom (right) on the low frequency band.

Numerical properties. The numerical core of the Cayley filter is the computation of Cj(h)f for j = 1, . . . , r, performed in a sequential manner. Let y0, . . . , yr denote the solutions of the following linear recursive system,

y0 = f , (h + iI)yj = (h - iI)yj-1 , j = 1, . . . , r.

(6)

Note that sequentially approximating yj in (6) using the approximation of yj-1 in the rhs is stable, since C(h) is unitary and thus has condition number 1.

Equations (6) can be solved with matrix inversion exactly, but it costs O(n3). An alternative is to
use the Jacobi method,1 which provides approximate solutions y~j  yj. Let J = -(Diag(h + iI))-1Off(h + iI) be the Jacobi iteration matrix associated with equation (6). For the unnormalized Laplacian, J = (hD + iI)-1hW. Jacobi iterations for approximating (6) for a given j have the form

y~j(k+1) = Jy~j(k) + bj , bj = (Diag(h + iI))-1(h - iI)y~j-1,

(7)

initialized with y~j(0) = bj and terminated after K iterations, yielding y~j = y~j(K). The application of

the approximate Cayley filter is given by Gf =

r j=0

cj

y~j



Gf ,

and

takes

O(rK n)

operations

under the previous assumption of a sparse Laplacian. The method can be improved by normalizing

y~j 2 = f 2.

Next, we give an error bound for the approximate filter. For the unnormalized Laplacian, let

d = maxj{dj,j} and  =

J



=

 hd h2 d2 +1

<

1.

For the normalized Laplacian, we assume that

(hn + iI) is dominant diagonal, which gives  = J  < 1.

Pthreogpeonseitriaolnc1a.seU, anndderMthe=abovjr=e 1ajss|ucmj|pitfitohnes,graGpfh-f iGs2fre2gulaMr. K ,

where

M

=

 n

r j=1

j

|cj |

in

Proposition 1 is pessimistic in the general case, while requires strong assumptions in the regular case. We find that in most real life situations the behavior is closer to the regular case. It also follows from Proposition 1 that smaller values of the spectral zoom h result in faster convergence, giving this parameter an additional numerical role of accelerating convergence.

Complexity. We study the computational complexity of our method, as the number of edges n of the graph tends to infinity. For every constant of a graph, e.g d, , we add the subscript n, indicating the number of edges of the graph. For the unnormalized Laplacian, we assume that dn is bounded, which gives n < a < 1 for some a independent of n. For the normalized Laplacian, we assume that n < a < 1. By Theorem 1, fixing the number of Jacobi iterations K and the order of the filter r, independently of n, keeps the Jacobi error controlled. As a result, the number of parameters is O(1), and for a Laplacian modeled as a sparse matrix, applying a Cayley filter on a signal takes O(n)
operations.

Localization. Unlike Chebyshev filters that have the small r-hop support, Cayley filters are rational functions supported on the whole graph. However, it is still true that Cayley filters are well localized on the graph. Let G be a Cayley filter and m denote a delta-function on the graph, defined as one at vertex m and zero elsewhere. We show that Gm decays fast, in the following sense:
1We remind that the Jacobi method for solving Ax = b consists in decomposing A = Diag(A) + Off(A) and obtaining the solution iteratively as x(k+1) = -(Diag(A))-1Off(A)x(k) + (Diag(A))-1b.

5

Under review as a conference paper at ICLR 2018

Definition 2 (Exponential decay on graphs). Let f be a signal on the vertices of graph G, 1  p  , and 0 < < 1. Denote by S  {1, . . . , n} a subset of the vertices and by Sc its complement. We
say that the Lp-mass of f is supported in S up to if f |Sc p  f p, where f |Sc = (fl)lSc is the restriction of f to Sc. We say that f has (graph) exponential decay about vertex m, if there exists some   (0, 1) and c > 0 such that for any k, the Lp-mass of f is supported in Nk,m up to ck. Here, Nk,m is the k-hop neighborhood of m.

Remark 3. Note that Definition 2 is analogous to classical exponential decay on Euclidean space:

|f (x)|  R-x iff for every ball B of radius  about 0,

f |Bc   c-

f

 with c =

R f

.

Theorem 4. Let G be a Cayley filter of order r. Then, Gm has exponential decay about m in L2,

with constants c = 2M

1 Gm

and  = 1/r (where M and  are from Proposition 1).
2

Cayley vs Chebyshev. Below, we compare the two classes of filters: Spectral zoom and stability. Generally, both Chebyshev polynomials and trigonometric polynomials give stable approximations, optimal for smooth functions. However, this crude statement is oversimplified. One of the drawbacks in Chebyshev filters is the fact that the spectrum of  is always mapped to [-1, 1] in a linear manner, making it hard to specialize in small frequency bands. In Cayley filters, this problem is mitigated with the help of the spectral zoom parameter h. As an example, consider the community detection problem discussed in the next section. A graph with strong communities has a cluster of small eigenvalues near zero. Ideal filters g() for extracting the community information should be able to focus on this band of frequencies. Approximating such filters with Cayley polynomials, we zoom in to the band of interest by choosing the right h, and then project g onto the space of trigonometric polynomials of order r, getting a good and stable approximation (Figure 3, bottom right). However, if we project g onto the space of Chebyshev polynomials of order r, the interesting part of g concentrated on a small band is smoothed out and lost (Figure 3, middle right). Thus, projections are not the right way to approximate such filters, and the stability of orthogonal polynomials cannot be invoked. When approximating g on the small band using polynomials, the approximation will be unstable away from this band; small perturbations in g will result in big perturbations in the Chebyshev filter away from the band. For this reason, we say that Cayley filters are more stable than Chebyshev filters.
Regularity. We found that in practice, low-order Cayley filters are able to model both very concentrated impulse-like filters, and wider Gabor-like filters. Cayley filters are able to achieve a wider range of filter supports with less coefficients than Chebyshev filters (Figure 1), making the Cayley class more regular than Chebyshev.
Complexity. Under the assumption of sparse Laplacians, both Cayley and Chebyshev filters incur linear complexity O(n). Besides, the new filters are equally simple to implement as Chebyshev filters; as seen in Eq.7, they boil down to simple sparse matrix-vector multiplications providing a GPU friendly implementation.

4 RESULTS
Experimental settings. We test the proposed CayleyNets reproducing the experiments of Defferrard et al. (2016); Kipf & Welling (2016); Monti et al. (2017a;a) and using ChebNet (Defferrard et al. (2016)) as our main baseline method. All the methods were implemented in TensorFlow of M. Abadi et. al. (2016). The experiments were executed on a machine with a 3.5GHz Intel Core i7 CPU, 64GB of RAM, and NVIDIA Titan X GPU with 12GB of RAM. SGD+Momentum and Adam (Kingma & Ba (2014)) optimization methods were used to train the models in MNIST and the rest of the experiments, respectively. Training and testing were always done on disjoint sets.
Community detection. We start with an experiment on a synthetic graph consisting of 15 communities with strong connectivity within each community and sparse connectivity across communities (Figure 3, left). Though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings. On this graph, we generate noisy step signals, defined as fi = 1 + i if i belongs to the community, and fi = i otherwise, where i  N (0, 0.3) is Gaussian i.i.d. noise. The goal is to classify each such signal according to the community it belongs to. The neural network architecture used for this task consisted of a spectral convolutional layer (based on Chebyshev or Cayley filters) with 32 output features, a mean pooling layer, and a softmax classifier for producing

6

Under review as a conference paper at ICLR 2018

the final classification into one of the 15 classes. The classification accuracy is shown in Figure 3 (right, top) along with examples of learned filters (right, bottom). We observe that CayleyNet significantly outperforms ChebNet for smaller filter orders, with an improvement as large as 80%. Studying the filter responses, we note that due to the capability to learn the spectral zoom parameter, CayleyNet allows to generate band-pass filters in the low-frequency band that discriminate well the communities ( Figure 3 bottom right).

Accuracy %

100 80
60 ChebNet
40
20 CayleyNet
2 4 6 8 10 12
Order r
1

|g()|

0.5

0 min
1

max

|g()|

0.5

0

min

max

Figure 3: Left: synthetic 15-communities graph. Right: community detection accuracy of ChebNet
and CayleyNet (top); normalized responses of four different filters learned by ChebNet (middle)
and CayleyNet (bottom). Grey vertical lines represent the frequencies of the normalized Laplacian (~ = 2n-1 - 1 for ChebNet and C() = (h - i)/(h + i) unrolled to a real line for CayleyNet). Note how thanks to spectral zoom property Cayley filters can focus on the band of low frequencies
(dark grey lines) containing most of the information about communities.

Complexity. We experimentally validated the computational complexity of our model applying filters of different order r to synthetic 15-community graphs of different size n using exact matrix inversion and approximation with different number of Jacobi iterations (Figure 4, center and right). As expected, approximate inversion guarantees O(n) complexity. We further conclude that typically very few Jacobi iterations are required (Figure 4, left shows that our model with just one Jacobi iteration outperforms ChebNet for low-order filters on the community detection problem).

100 80 13 9 60 5 40 1 20
3 6 9 12
Order r

0.1 0.05
0

13 9 5 1
5 10
Order r

0.1

0.05 0

13 9
5
1

200 600 1,000
#Vertices n

Figure 4: Left: community detection test accuracy as function of filter order r. Center and right: computational complexity (test times on batches of 100 samples) as function of filter order r and graph size n. Shown are exact matrix inversion (dashed) and approximate Jacobi with different number of iterations (colored). For reference, ChebNet is shown (dotted).

MNIST. Following Defferrard et al. (2016); Monti et al. (2017a), we approached the classical MNIST digits classification as a learning problem on graphs. Each pixel of an image is a vertex of a graph (regular grid with 8-neighbor connectivity), and pixel color is a signal on the graph. We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm (Dhillon et al. (2007)), and finally a fullyconnected layer (this architecture replicates the classical LeNet5, LeCun et al. (1998), architecture,

7

Accuracy % Time (sec) Time (sec)

Under review as a conference paper at ICLR 2018

which is shown for comparison). MNIST classification results are reported in Table 1. CayleyNet achieves the same (near perfect) accuracy as ChebNet with filters of lower order (r = 12 vs 25). Examples of filters learned by ChebNet and CayleyNet are shown in Figure 1.

Table 1: Test accuracy obtained with different methods on the MNIST dataset.
Model Order Accuracy #Params
LeNet5 - 99.33% 1.66M ChebNet 25 99.14% 1.66M CayleyNet 12 99.18% 1.66M

Table 2: Test accuracy of different methods on the CORA dataset. Chebyshev polynomials of order 3 and Cayley polynomials of order 1 have been exploited for ChebNet and CayleyNet respectively.

Method

Accuracy #Params

ChebNet (Defferrard et al. (2016)) 86.22%

DCNN (Atwood & Towsley (2016)) 86.30%

GCN (Kipf & Welling (2016))

86.50%

CayleyNet

87.90%

92K 91K 92K 92K

Citation network. Next, we address the problem of vertex classification on graphs using the popular CORA citation graph, Sen et al. (2008). Each of the 2708 vertices of the CORA graph represents a scientific paper, and an undirected unweighted edge represents a citation (5429 edges in total). For each vertex, a 1433-dimensional binary feature vector representing the content of the paper is given. The task is to classify each vertex into one of the 7 groundtruth classes. We split the graph into training (1,708 vertices), validation (500 vertices) and test (500 vertices) sets, for simulating the labeled and unlabeled information. We train ChebNet and CayleyNet with the architecture presented in Kipf & Welling (2016); Monti et al. (2017a) (two spectral convolutional layers with 16 and 7 outputs), DCNN (Atwood & Towsley (2016)) with 2 diffusion layer (20 hidden features and 2 diffusion hops) and GCN (Kipf & Welling (2016)) with 3 convolutional layer (64 and 16 hidden features). Figure 5 compares ChebNets and CayleyNets instructed with different polynomial orders, Table 2 presents a comparison of the performance obtained with all the considered solutions (all architectures roughly present same amount of parameters). CayleyNet consistently outperforms all the considered competitors. This experiment also shows that the proposed approach is much less sensitive to the different types of Laplacian operators, unlike ChebNet which is unable to handle unnormalized Laplacians.

Accuracy %

Normalized Laplacian

88
86 84 87.1 87.9
1

86.6 86.9
2
Order r

86.2 87.1 3

Unnormalized Laplacian

100 80 60 40 20 0 85.5 87.7 1

27.0 86.0 2
Order r

14.3 86.8 3

Figure 5: ChebNet (blue) and CayleyNet (orange) test accuracies obtained on the CORA dataset. Unlike ChebNet, our CayleyNet is able to handle both types of Laplacians.

Recommender system. In our final experiment, we applied CayleyNet to recommendation system, formulated as matrix completion problem on user and item graphs, Monti et al. (2017a). The task is, given a sparsely sampled matrix of scores assigned by users (columns) to items (rows), to fill in the missing scores. The similarities between users and items are given in the form of column and row graphs, respectively. Monti et al. (2017a) approached this problem as learning with a Recurrent Graph CNN (RGCNN) architecture, using an extension of ChebNets to matrices defined on multiple graphs in order to extract spatial features from the score matrix; these features are then fed into an RNN producing a sequential estimation of the missing scores. Here, we repeated verbatim their experiment on the MovieLens dataset (Miller et al. (2003)), replacing Chebyshev filters with Cayley filters. We used separable RGCNN architecture with two CayleyNets of order r = 4 employing 15 Jacobi iterations. The results are reported in Table 3. Our version of sRGCNN outperforms all the competing methods, including the previous result with Chebyshev filters reported in Monti et al. (2017a).

8

Under review as a conference paper at ICLR 2018

Table 3: Performance (RMSE) of different matrix completion methods on the MovieLens dataset.

Method

RMSE

MC (Candes & Recht (2012))

0.973

IMC (Jain & Dhillon (2013); Xu et al. (2013)) 1.653

GMC (Kalofolias et al. (2014))

0.996

GRALS (Rao et al. (2015))

0.945

sRGCNNCheby (Monti et al. (2017a))

0.929

sRGCNNCayley

0.922

5 CONCLUSIONS
In this paper, we introduced a new efficient spectral graph CNN architecture that scales linearly with the dimension of the input data. Our architecture is based on a new class of complex rational Cayley filters that are localized in space, can represent any smooth spectral transfer function, and are highly regular. The key property of our model is its ability to specialize in narrow frequency bands with a small number of filter parameters, while still preserving locality in the spatial domain. We validated these theoretical properties experimentally, demonstrating the superior performance of our model in a broad range of graph learning problems.
REFERENCES
J. Atwood and D. Towsley. Diffusion-convolutional neural networks. arXiv:1511.02136v2, 2016.
D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst. Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks. Computer Graphics Forum, 34(5):13­23, 2015. ISSN 1467-8659.
D. Boscaini, J. Masci, E. Rodolà, and M. M. Bronstein. Learning shape correspondence with anisotropic convolutional neural networks. In Proc. NIPS, 2016a.
D. Boscaini, J. Masci, E. Rodolà, M. M. Bronstein, and D. Cremers. Anisotropic diffusion descriptors. Computer Graphics Forum, 35(2):431­441, 2016b.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. arXiv:1611.08097, 2016.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. Proc. ICLR, 2013.
E. Candes and B. Recht. Exact matrix completion via convex optimization. Comm. ACM, 55(6): 111­119, 2012.
A. Coates and A.Y. Ng. Selecting Receptive Fields in Deep Networks. In Proc. NIPS, 2011.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Proc. NIPS, 2016.
Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a multilevel approach. Trans. PAMI, 29(11), 2007.
D. K. Duvenaud et al. Convolutional networks on graphs for learning molecular fingerprints. In Proc. NIPS, 2015.
M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc. IJCNN, 2005.
Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural networks to graph-structured data. arXiv:1704.08165, 2017.
M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv:1506.05163, 2015.

9

Under review as a conference paper at ICLR 2018
P. Jain and I. S. Dhillon. Provable inductive matrix completion. arXiv:1306.0626, 2013. V. Kalofolias, X. Bresson, M. M. Bronstein, and P. Vandergheynst. Matrix completion on graphs.
arXiv:1408.1717, 2014. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
arXiv:1609.02907, 2016. Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and Daniel
Rueckert. Distance metric learning using graph convolutional networks: Application to functional brain networks. arXiv:1703.02161, 2017. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278­2324, 1998. Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv:1511.05493, 2015. M. Abadi et. al. Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467, 2016. J. Masci, D. Boscaini, M. M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural networks on Riemannian manifolds. In Proc. 3DRR, 2015. B. N. Miller et al. MovieLens unplugged: experiences with an occasionally connected recommender system. In Proc. Intelligent User Interfaces, 2003. F. Monti, D. Boscaini, J. Masci, E. Rodolà, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proc. CVPR, 2017a. Federico Monti, Michael M Bronstein, and Xavier Bresson. Geometric matrix completion with recurrent multi-graph neural networks. Proc. NIPS, 2017b. N. Rao, H.-F. Yu, P. K. Ravikumar, and I. S. Dhillon. Collaborative filtering with graph information: Consistency and scalable methods. In Proc. NIPS, 2015. F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61­80, 2009. P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3):93, 2008. D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Sig. Proc. Magazine, 30(3):83­98, 2013. S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation. arXiv:1605.07736, 2016. M. Xu, R Jin, and Z.-H. Zhou. Speedup matrix completion with side information: Application to multi-label learning. In Proc. NIPS, 2013.
10

Under review as a conference paper at ICLR 2018

APPENDIX

PROOF OF PROPOSITION 1

First note the following classical result for the approximation of Ax = b using the Jacobi method: if the initial condition is x(0) = 0, then (x - x(k)) = Jkx. In our case, note that if we start with initial condition y~j(0) = 0, the next iteration gives y~j(0) = bj, which is the initial condition from our construction. Therefore, since we are approximating yj = C(h)y~j-1 by y~j = y~j(K), we have

C(h)y~j-1 - y~j = JK+1C(h)y~j-1

(8)

Define the approximation error in C(h)jf by

ej =

Cj(h)f - y~j Cj(h)f 2

2.

By the triangle inequality, by the fact that Cj(h) is unitary, and by (8)

ej 

Cj(h)f - C(h)y~j-1 Cj(h)f 2

2+

C(h)y~j-1 - y~j 2 Cj(h)f 2

=

Cj-1(h)f - y~j-1 Cj-1(h)f 2

2+

JK+1C(h)y~j-1 2 f2

ej-1 +

JK+1 2

C(h)y~j-1 f2

2

= ej-1 +

JK+1 2

ej-1 + JK+1 2 (1 + ej-1)

y~j-1 2 f2

(9)

where the last inequality is due to

y~j-1 2  Cj-1(h)f 2 + Cj-1(h)f - y~j-1 2 = f 2 + f 2 ej-1.

Now, using standard norm bounds, in the general case we have

JK+1




2

n

JK+1

. Thus,

by  = J  we have

 ej  ej-1 + n

J

K +1 

(1

+

ej-1)

=

(1

+

nK+1)ej-1

+

nK+1.

The solution of this recurrent sequence is ej  (1 + nK+1)j - 1 = jnK+1 + O(2K+2).

If nweKu+se1

the version of the algorithm, in which each . The solution of this recurrent sequence is

y~j

is

normalized,

we

get

by

(9)

ej



ej-1

+

 We denote in this case Mj = j n

ej  jnK+1.

In case the graph is regular, we have D = dI. In the non-normalized Laplacian case,

J = -(hdI + iI)-1h( - dI) =

h (dI - ) =

h W.

hd + i

hd + i

(10)

The spectral radius of  is bounded by 2d. This can be shown as follows. a value  is not an

eigenvalue of  (namely it is a regular value) if and only if ( - I) is invertible. Moreover, the

matrix ( - I) is strictly dominant diagonal for any || > 2d. By Levy­Desplanques theorem, any

strictly dominant diagonal matrix is invertible, which means that all of the eigenvalues of  are less

than 2d in their absolute value. As a result, the spectral radius of (dI - ) is realized on the smallest

eigenvalue

of

,

namely

it

is

|d

-

0|

=

d.

This

means

that

the

specral

radius

of

J

is

 hd h2 d2

+1

.

As

a

result

J

2

=

 hd h2 d2 +1

= .

We can now continue from (9) to get

ej  ej-1 +

J

K+1 2

(1

+

ej-1)

=

ej-1

+

K+1(1

+

ej-1).

11

Under review as a conference paper at ICLR 2018

As before, we get ej  jK+1 + O(2K+2), and ej  jK+1 if each y~j is normalized. We denote in this case Mj = j.

In the case of the normalized Laplacian of a regular graph, the spectral radius of n is bounded by

2,

and

the

diagonal

entries

are

all

1.

Equation

(10)

in

this

case

reads

J

=

h h+i

(I

-

n),

and

J

has

spectral

radius

h h2 +1

.

Thus

J

2

=

h h2 +1

=  and we continue as before to get ej

 jK+1 and

Mj = j.

In all cases, we have by the triangle inequality

Gf - Gf

r

f2

2  |cj |
j=1

Cj(h)f - y~j Cj(h)f 2

rr
2 = |cj | ej  Mj |cj | K+1.

j=1

j=1

PROOF OF THEOREM 4

In this proof we approximate Gm by Gm. Note that the signal m is supported on one vertex,
and in the calculation of Gm, each Jacobi iteration increases the support of the signal by 1-hop. Therefore, the support of Gm is the r(K + 1)-hop neighborhood Nr(K+1),m of m. Denoting l = r(K + 1), and using Proposition 1, we get

Gm - Gm|Nl,m


2

Gm - Gm +
2

Gm - Gm|Nl,m

2

 Gm - Gm + Gm - Gm
22

=2

Gm - Gm

 2M K+1
2

m

2

= 2M (1/r)l.

(11)

12

