Under review as a conference paper at ICLR 2018
SELF-ENSEMBLING FOR VISUAL DOMAIN ADAPTATION
Anonymous authors Paper under double-blind review
ABSTRACT
This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen & Valpola (2017)) of temporal ensembling (Laine & Aila (2017)), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.
1 INTRODUCTION
The state of the art performance of deep learning in computer vision tasks comes at the cost of requiring large datasets with corresponding ground truth labels for training. Such datasets are often expensive to produce, owing to the cost of the human labour required to produce the ground truth labels.
Semi-supervised learning is an active area of research that aims to reduce the quantity of ground truth labels required for training. It is aimed at common practical scenarios in which only a small subset of a large dataset has corresponding ground truth labels. Unsupervised domain adaptation is a closely related problem in which one attempts to transfer knowledge gained from a labeled source dataset to a distinct unlabeled target dataset, within the constraint that the objective (e.g.digit classification) must remain the same. Domain adaptation offers the potential to train a model using labeled synthetic data ­ that is often abundantly available ­ and unlabeled real data. The scale of the problem can be seen in the VisDA-17 domain adaptation challenge images shown in Figure 1. We will present our winning solution in Section 4.2.
Recent work (Tarvainen & Valpola (2017)) has demonstrated the effectiveness of self-ensembling with random image augmentations to achieve state of the art performance in semi-supervised learning benchmarks.
We have developed the approach proposed by Tarvainen & Valpola (2017) to work in a domain adaptation scenario. We will show that this can achieve excellent results in specific small image domain adaptation benchmarks. More challenging scenarios, notably MNIST  SVHN and the VisDA-17 domain adaptation challenge required further modifications. To this end, we developed confidence thresholding and class balancing that allowed us to achieve state of the art results in a variety of benchmarks, with some of our results coming close to those achieved by traditional supervised learning. Our approach is sufficiently flexble to be applicable to a variety of network architectures, both randomly initialized and pre-trained.
Our paper is organised as follows; in Section 2 we will discuss related work that provides context and forms the basis of our technique; our approach is described in Section 3 with our experiments and results in Section 4; and finally we present our conclusions in Section 5.
2 RELATED WORK
In this section we will cover self-ensembling based semi-supervised methods that form the basis of our approach and domain adaptation techniques to which our work can be compared.
1

Under review as a conference paper at ICLR 2018
(a) VisDa-17 training set images; the labeled source domain
(b) VisDa-17 validation set images; the unlabeled target domain
Figure 1: Images from the VisDA-17 domain adaptation challenge
2.1 SELF-ENSEMBLING FOR SEMI-SUPERVISED LEARNING
The following three related approaches use loss functions that are composed of supervised and unsupervised components. The supervised component is stanrdard cross-entropy loss, while the unsupervised component penalises the network for generating different predictions for the same sample under different conditions. Sajjadi et al. (2016) apply rich data augmentation during training and record the history of the network predictions for each training sample at each epoch. At epoch k their unsupervised loss encourages the network to minimise the sum of squared differences between the prediction for sample i at epoch k and all of the predictions recorded in the history for sample i from epoch 0 to k - 1. They refer to this as transformation/stability loss. Laine & Aila (2017) present two models; their -model and their temporal model. The -model passes each unlabeled sample through a classifier twice, each time with different dropout, noise and image translation parameters. Their unsupervised loss is the mean of the squared difference in class probability predictions resulting from the two presentations of each sample. Their temporal model is more similar to that of Sajjadi et al. (2016), except that it maintains a per-sample moving average of the historical network predictions. The unsupervised loss is the mean squared difference between the current prediction and the moving average prediction, rather than the difference between two predictions in the current epoch only, as with the -model. Their approach achieved state of the art results in the SVHN and CIFAR-10 semi-supervised classification benchmarks. Tarvainen & Valpola (2017) further improved on the work of Laine & Aila (2017). They use an exponential moving average of the network weights as a teacher network rather than an exponential moving average of class prediction as in the temporal model. Their approach uses two networks; a student network and a teacher network, where the student is trained using gradient descent and the weigthts of the teacher are the exponential moving average of those of the student. The unsupervised loss used to train the student is the mean square difference between the predictions of the student and the teacher, under different dropout, noise and image translation parameters.
2.2 DOMAIN ADAPTATION
Ganin & Lempitsky (2015) use a bifurcated network that starts with feature extraction layers before splitting into two branches; a label classification branch and a domain classification branch that predicts the dataset from which a sample was drawn. A gradient reversal layer placed between the feature sub-network and the domain classification branch inverts the gradients propagating from the domain classifier to the feature sub-network, encouraging it to extract domain independent features. During training, samples from the source domain train both the label classification branch and the domain classification branch. Target domain samples train only the domain classification branch. An alternative and simpler implementation described in their appendix minimises the label crossentropy loss in the feature and label classification layers, minimises the domain cross-entropy in the domain classification layers but maximises it in the feature layers. The model of Ghifary et al. (2016) is an autoencoder that predicts class labels by connecting fullyconnected classification layers to the latent representation generated by the encoder part of the network. The network is trained to classify samples from the source domain ­ for which ground truths
2

Under review as a conference paper at ICLR 2018
are available ­ and reconstruct samples from both the source and the target domain. It is interesting to contrast it with the model of Ganin & Lempitsky (2015); both are bifurcated networks. Ganin & Lempitsky (2015) encourage domain independent features through the use of gradient reversal where as Ghifary et al. (2016) use autoencoder reconstruction.
Sankaranarayanan et al. (2017) present a hybrid model that consists of an encoder and a conditional Generative Adversarial Network (GAN; Goodfellow et al. (2014)). The encoder transforms samples from the source domain into a latent space. Given the latent representation and random noise, the GAN's generator learns to create a sample that matches the distribution of the source dataset. The GAN's discriminator attempts to classify a sample as real (from the source distribution) or fake. Finally, given a target sample, the encoder is trained to produce a latent representation that induces the generator to create an equivalent sample from the source distribution, thereby encouraging the encoder to extract domain independent features.
The model of Tzeng et al. (2017) operates along similar lines but is simpler. A source classification network is trained to classify samples from the source domain. A domain classification network is then trained to determine which domain a sample originated from, but with source samples passed through the feature extraction layers from the first network and target samples passed through a new set of feature extraction layers for the target domain. These new feature extraction layers are trained in order to trick the final domain classification layers into predicting that the target samples come from the source dataset, while the final domain classification layers are trained to discriminate between the two. Once complete, the label classifier trained in the first step is attached to the target domain feature extraction layers.
Russo et al. (2017) present a bi-directional GAN composed of two generators that transform samples from the source to the target domain and vice versa. Pseudo-labels are learned for target domain samples by transforming samples to the source domain and using a classifier trained on the source domain. Label class consistency is achieved by encouraging consistent label predictions when transforming a labelled source sample to the target domain and back to the source domain again. This work bears similarities to CycleGAN, by Zhu et al. (2017).
Saito et al. (2017) use tri-training (Zhou & Li (2005)); feature extraction layers are used to drive three classifier sub-networks. The first two are trained on samples from the source domain, while a weight similarity penalty encourages them to learn different weights. Pseudo-labels generated for target domain samples by these source domain classifiers are used to train the final classifier to operate on the target domain.
Li et al. (2016) described adaptive batch normalization, a variant of batch normalization (Ioffe & Szegedy (2015)) that learns separate batch normalization statistics for the source and target domains in a two-pass process, establishing new state-of-the-art results. In the first pass standard supervised learning is used to train a classifier for samples from the source domain. In the second pass, normalization statistics for target domain samples are computed for each batch normalization layer in the network, leaving the network weights as they are.
3 METHOD
Our model builds upon the mean teacher semi-supervised learning model of Tarvainen & Valpola (2017), which we will describe. Subsequently we will present our modifications that enable domain adaptation.
The structure of the mean teacher model of Tarvainen & Valpola (2017) ­ also discussed in section 2.1 ­ is shown in Figure 2a. The student network is trained using gradient descent, while the weights of the teacher network are an exponential moving average of those of the student. During training each input sample xi is passed through both the student and teacher networks, generating predicted class probability vectors zi (student) and z~i (teacher). Different dropout, noise and image translation parameters are used for the student and teacher pathways.
During each training iteration a mini-batch of samples is drawn from the dataset, consisting of both labeled and unlabeled samples. The training loss is the sum of a supervised and an unsupervised component. The supervised loss is cross-entropy loss computed using zi (student prediction). It is masked to 0 for unlabeled samples for which no ground truth is available. The unsupervised com-
3

Under review as a conference paper at ICLR 2018
Figure 2: The network structures of the original mean teacher model and our model. Dashed lines in the mean teacher model indicate that ground truth labels ­ and therefore cross-entropy classification loss ­ are only available for labeled samples.
ponent is the self-ensembling loss. It penalises the difference in class predictions between student (zi) and teacher (z~i) networks for the same input sample. It is computed using the mean squared difference between the class probability predictions zi and z~i. Laine & Aila (2017) and Tarvainen & Valpola (2017) found that it was necessary to scale up the unsupervised loss in a time dependent manner during training. Using unsupervised loss at full strength from the start results in the network getting stuck in a degenerate state that gives very poor classification performance. To address this, they weighted the unsupervised loss using a time dependent function that follows a Gaussian curve from 0 to 1 during the first 80 epochs. In the following subsections we will describe our contributions in detail along with the motivations for introducing them.
3.1 ADAPTING TO DOMAIN ADAPTATION
The models of Tarvainen & Valpola (2017) and of Laine & Aila (2017) were designed for semisupervised learning problems in which a subset of the samples in a dataset have ground truth labels. During training both models mix labeled and unlabeled samples together in a mini-batch. In contrast, unsupervised domain adaptation problems use two datasets; labeled source and unlabeled target. Our variant of the mean teacher model ­ shown in Figure 2b ­ has separate source (XSi) and target (XT i) paths. It processes two mini-batches - one source and one target ­ per training iteration. We apply cross-entropy loss to the source mini-batch and unsupervised self-ensembling loss to the target mini-batch. As in Tarvainen & Valpola (2017), self-ensembling loss is computed as the meansquared difference between predictions produced by the student (zT i) and teacher (z~T i) networks with different augmentation, dropout and noise parameters. Following the work of Li et al. (2016) we use different batch normalization statistics for the source and target domains. We do not use their approach as-is, as they handle the source and target datasets separtely in two distinct training phases, where our approach must train using both simultaneously. During each training iteration we apply batch normalization to source and target mini-batches separately; the means and variances used to normalize source samples differ from those used for target samples (this is simple to implement using most neural network toolkits; evaluate the network once for source samples and a second time for target samples). We do not however maintain separate exponential moving averages of the means and variances for each dataset for use at test time. We should note that each iteration does not consist of distinct gradient descent steps for source and target samples, rather we compute supervised loss for the source mini-batch and unsupervised loss for the target mini-batch and perform a gradient descent step using the combined loss. As seen in the 'mean teacher' row of Table 1, the model described thus far achieves state of the art results in 5 out of 8 small image benchmarks. The MNIST  SVHN, STL  CIFAR-10 and Syndigits  SVHN benchmarks however require additional modifications to achieve good performance. These will now be described.
4

Under review as a conference paper at ICLR 2018

3.2 CONFIDENCE THRESHOLDING
We found that replacing the Gaussian ramp-up factor that scales the unsupervised loss with confidence thresholding stabilized training in more challenging domain adaptation scenarios. For each unlabeled sample xT i the teacher network produces the predicted class probabilty vector z~T ij ­ where j is the class index drawn from the set of classes C ­ from which we compute the confidence f~T i = maxjC (z~T ij); the predicted probability of the predicted class of the sample. If f~T i is below the confidence threshold (a parameter search found 0.968 to be an effective value for small image benchmarks), the self-ensembling loss for the sample xi is masked to 0.
Our working hypothesis is that confidence thresholding acts as a filter, shifting the balance in favour of the student learning correct labels from the teacher. While high network prediction confidence does not guarantee correctness there is a positive correlation. Given the tolerance to incorrect labels reported by Laine & Aila (2017), we believe that the higher signal-to-noise ratio underlies the success of this component of our approach.
3.3 DATA AUGMENTATION
The augmentation used in Laine & Aila (2017) consisted of horizontal flips (for some experiments), integer valued translations and Gaussian noise. In addition, we apply random affine transformations whose matrices are generated using the matrix shown in (1), where N (0, 0.1) denotes a real value drawn from a normal distribution with mean 0 and standard deviation 0.1. Our translations are real valued and drawn from the range [-2, 2].

1 + N (0, 0.1) N (0, 0.1) N (0, 0.1) 1 + N (0, 0.1)

(1)

The addition of confidence thresholding and affine augmentation achieves state of the art results in the STL  CIFAR-10 and Syn-digits  SVHN benchmarks, as seen in the 'full model' row of
Table 1. The impact of each of these components is explored in more detail in Table 2.

3.4 CLASS BALANCE LOSS
With the adaptations made so far the challenging MNIST  SVHN benchmark remains undefeated due to training instabilities. During training we noticed that the prediction error rate on the SVHN test set decreases at first, then starts to rise, reaching high values before training completes. We diagnosed the problem by recording the preditions for all SVHN training set samples after each epoch. We observed that the rise in error rate during training correlated with the predictions evolving toward a condition in which most samples are predicted as belonging to the '1' class; the most populous class in the SVHN dataset. We hypothesize that the class imbalance in the SVHN dataset caused the unsupervised loss to reinforce the '1' class more often than the others, resulting in the network settling in a degenerate local minimum. Rather than distinguish between digits as intended it learned to distinguish MNIST samples from SVHN samples and assign the latter to the '1' class.
We addressed this problem by introducing a class balance loss term that penalises the network for making predictions that exhibit large class imbalance. For each target domain mini-batch we compute the mean of the predicted sample class probabilities over the sample dimension, resulting in the mini-batch mean per-class probability. The loss is computed as the binary cross entropy between the mean class probability vector and a uniform probability vector. We balance the strength of the class balance loss with that of the self-ensembling loss by multiplying the class balance loss by the average of the confidence threshold mask (e.g. if 75% of samples in a mini-batch pass the confidence threshold, then the class balance loss is multiplied by 0.75).

4 EXPERIMENTS
Our implementation was developed using PyTorch (Chintala et al.) and is available at TBC.

5

Under review as a conference paper at ICLR 2018

USPS

C onvsrc Convsrc (aug)b

­ MNIST
91.97 ±2.15 ­

RevGrada [1] DCRN [2] G2A [3] ADDA [4] ATT [5] SBADA-GAN [6] OUR RESULTS Mean teacher
Full model
Specific aug

74.01 73.67 90.8 90.1 ­ 97.60
98.07 ±2.82
99.54 ±0.04
­

C onvtgt Convtgt (aug)b

99.62 ±0.03
­

MNIST
­ USPS 96.25 ±0.54 ­
91.11 91.8 92.5 89.4 ­ 95.04
98.26 ±0.11 98.23 ±0.13 ­
97.83 ±0.17 ­

SVHN
­ MNIST 73.00 ±5.47 ­
73.91 81.97 84.70 76.00 86.20 76.14
99.18 ±0.12 99.22c ±0.30 ­
99.66 ±0.02 ­

MNIST
­ SVHN
28.78 ±2.52 64.82 ±1.99 35.67 40.05 36.4 ­ 52.8 61.08
13.96c ±4.41 41.98c ±5.67
96.6 ±0.15 96.68 ±0.07 97.3 ±1.99

CIFAR
­ STL 70.07 ±0.49 ­
66.12 66.37 ­ ­ ­ ­
75.80 ±0.1 75.51 ±0.15 ­
66.62 ±1.70 ­

STL
­ CIFAR 50.78 ±0.55 ­
56.91 58.65 ­ ­ ­ ­
11.11 ±0.0 69.15 ±0.37 ­
85.45 ±0.33 ­

Syn Digits ­ SVHN 86.96 ±0.77 ­
91.09 ­ ­ ­ 93.1 ­
15.94 ±0.0
96.00 ±0.11
­
95.55 ±0.1 ­

Syn Signs ­ GTSRB 96.72 ±0.11 ­
88.65 ­ ­ ­ 96.2 ­
98.43 ±0.1
98.32 ±0.27
­
98.54 ±0.26 ­

[1] Ganin & Lempitsky (2015), [2] Ghifary et al. (2016), [3] Sankaranarayanan et al. (2017), [4] Tzeng
et al. (2017), [5] Saito et al. (2017), [6] Russo et al. (2017) a Combined results from Ganin & Lempitsky (2015) and Ghifary et al. (2016), taking best result if more
than one available. b Used additional augmentation as described in Section 4.1, paragraph 'MNIST  SVHN'. c MNIST  SVHN experiments and the SVHN  MNIST full model experiment used class balance loss.

Table 1: Small image benchmark classification accuracy; each result is presented as mean ± standard deviation that were computed from 5 independent runs

Model MT (mean teacher) MT without class balance loss MT + CT MT + AA MT + CT + AA (full model) full model + SA

USPS ­ MNIST 98.07 ±2.82
­ 97.28 ±2.74 99.49 ±0.02 99.54 ±0.04
­

MNIST ­ SVHN 13.96 ±4.41 12.52 ±4.42 36.48 ±1.97 7.94 ±1.49 41.979 ±5.67 96.6 ±0.15

STL ­ CIFAR 11.11 ±0.0
­ 68.99 ±0.51 38.25 ±21.57 69.15 ±0.37
­

Syn Digits ­ SVHN 15.94 ±0.0
­ 96.07 ±0.08 15.94 ±0.0 96.00 ±0.11
­

Table 2: Effects of components of our approach on challenging small image benchmarks. MT = mean teacher, CT = confidence threshold, AA = affine augmentation, SA = MNIST  SVHN
specific augmentation

4.1 SMALL IMAGE DATASETS
Our results can be seen in Table 1. The `Convsrc' and `Convtgt' results are the accuracies obtained from training a neural network in supervised fashion on the source and target domains respectively, therefore representing the exepected baseline and best achievable result. These results were obtained using the same neural network architectures and augmentation parameters as were used for our domain adaptation experiments. The `Convsrc (aug)' and `Convtgt (aug)' results demonstrate
6

Under review as a conference paper at ICLR 2018

(a) MNIST  USPS (c) Syn-digits  SVHN (e) SVHN  MNIST

(b) CIFAR-10  STL (d) Sin-signs  GTSRB (f) MNIST (specific augmentation)  SVHN

Figure 3: Small image domain adaptation example images

the effect of additional data augmentation used for the MNIST  SVHN adaptation paths that is discussed further down.
The small datasets are described in Appendix A Table 4. Our training procedure is described in Appendix B and our network architectures are described in Appendix D. It is worth noting that in each of these cases, only the training sets of the respective datasets were used during training; the test sets used for reporting scores were not used at all ­ even in an unsupervised fashion - during training.
MNIST  USPS (see Figure 3a). MNIST and USPS are both greyscale hand-written digit datasets. The USPS images were up-scaled from 16 × 16 to 28 × 28 resolution. In both adaptation directions our approach not only demonstrates a significant improvement over prior art but nearly achieves the performance of supervised learning using the target domain ground truths. The strong performance of the mean teacher model can be attributed to the similarity of the datasets to one another. Our MNIST  USPS performance is strong in comparison to prior work, but is less marked when contrasted to the baseline performance of a standard network trained on MNIST only. It is worth noting that our baseline performance in both directions is also stronger than the domain adaptation methods used in much of the prior art, most likely due to our affine data augmentation.
CIFAR-10  STL (see Figure 3b). CIFAR-10 and STL are both 10-class image datasets. The STL images were down-scaled to 32 × 32 resolution to match that of CIFAR-10. The 'frog' class in CIFAR-10 and the 'monkey' class in STL were removed as they have no equivalent in the other dataset, resulting in a 9-class problem with 10% less samples in each dataset. We obtained strong performance in the STL  CIFAR-10 path, likely due to the similar nature of the datasets and the strong performance exhibited in semi-supervised settings as seen in the USPS  MNIST path earlier. The CIFAR-10  STL results are more interesting; the baseline performance of a standard network trained on the CIFAR-10 source domain outperforms that of a network trained on the STL target domain, most likely due to the small size of the STL training set. Our self-ensembling results outpace both the baseline performance and the `theoretical maximum' of `Convtgt', lending further evidence to the view of Sajjadi et al. (2016) and Laine & Aila (2017) that self-ensembling acts as an effective regulariser.
Syn-Digits  SVHN (see Figure 3c). The Syn-Digits dataset is a synthetic dataset designed by Ganin & Lempitsky (2015) to be used as a source dataset in domain adaptation experiments for the purpose of targeting SVHN. Other approaches have achieved good scores on this benchmark, beating the baseline by a significant margin. Our result improves on them, reducing the error rate from 6.9% to 4%; approaching the 2.7% error rate achieved using supervised learning.
Syn-Signs  GTSRB (see Figure 3d). Syn-Signs is another synthetic dataset designed by Ganin & Lempitsky (2015) to target the 43-class GTSRB (German Traffic Signs Recognition Benchmark; Stallkamp et al. (2011)) dataset. GTSRB is composed of images that vary in size and come with annotations that provide region of interest (bounding box around the sign) and ground truth classification. We extracted the region of interest from each image and scaled them to a resolution of 40×40 to match those of Syn-Signs. Our approach halved the best error rate of competing approaches. As seen in the MNIST  USPS and CIFAR-10  STL results, our `Convsrc'baseline outouperforms existing domain adaptation algorithms, most likely due to our use of data augmentation.
SVHN  MNIST (see Figure 3e). Google's SVHN (Street View House Numbers) is a colour digits dataset of house number plates. The SVHN images were converted to grey-scale ­ as in Ghifary
7

Under review as a conference paper at ICLR 2018

VALIDATION PHASE Team
OTHER TEAMS bchidlovski BUPT OVERFIT Uni. Tokyo MIL OUR RESULTS ResNet-50 model

Mean class acc.
83.1 77.8 75.4
82.8

TEST PHASE Team
NLE-DA BUPT OVERFIT Uni. Tokyo MIL
ResNet-152 model

Mean class acc.
87.7 85.4 82.4
92.8

Table 3: VisDA-17 performance. Full results are presented in Appendix C Table 5

et al. (2016) ­ and the MNIST images were padded to 32×32 resolution. Our approach significantly outpaces other techniques and achieves an accuracy close to that of supervised learning. We should note that in 2 out of 5 runs, the full model converged on an accuracy of around 90% when class balancing loss was disabled, so we enabled it for that configuration.
MNIST  SVHN (see Figure 3f). This adaptation path is somewhat more challenging as MNIST digits are uniform in terms of size, aspect ratio and intensity range, where the target SVHN digits are more varied. As a consequence, adapting from MNIST to SVHN required additional work. Class balancing loss was necessary to ensure training stability and additional experiment specific data augmentation was required to achieve good accuracy. The default augmentation parameters presented in section 3.3 result in an accuracy score of 42%. Achieving the performance figures shown required additional data augmentation in the form of random intensity flips (negative image), scaling the intensity by a random value drawn from the range [0.25, 1.5] and offsetting it by a random value in the range [-0.5, 0.5]. These hyper-parameters were selected in order to augment MNIST samples to match the intensity variations present in SVHN; the effect of this is illustrated in Figure 3f. With these additional modifications, we achieve a result that significantly outperforms prior art and nearly achieves the accuracy of a supervised classifier. This said, it should be noted that applying these additional augmentations to the source MNIST dataset was essential, where applying them to the target SVHN dataset was helpful but had far less effect. That said, it should be noted that this augmentation scheme raises the performance of a supervised classifier trained on MNIST to just above that of much of the prior art.
We would also like to note that the our approach does not sacrifice performance on source domain samples in order to achieve good performance on target domain samples; our networks perform well on both without requiring any indication as to the source.
4.2 VISDA-2017 VISUAL DOMAIN ADAPTATION CHALLENGE
The VisDA-2017 image classification challenge is a 12-class domain adaptation problem consisting of three datasets: a training set consisting of 3D renderings of sketchup models, and validation and test sets consisting of photographic images (see Figure 1). The objective is to learn from labeled computer generated images and correctly predict the class of photographic images. Ground truth labels were made available for the training and validation sets only; test set scores were computed by a server operated by the competition organisers.
While the algorithm as that presented above, we base our network on the pretrained ResNet-152 (He et al. (2016)) network provided by PyTorch (Chintala et al.), rather than using a randomly initialised network as before. The final 1000-class classification layer is removed and replaced with two fullyconnected layers; the first has 512 units with a ReLU non-linearity while the final layer has 12 units with a softmax non-linearity. Our hyper-parameters and richer data augmentation scheme are described in Appendix C.1. It is worth noting that in order to achieve the accuracy as seen in Table 3 we applied data augmentation to the target set samples, averaging the results from 16 differently augmented images. Our final VisDA test set score is also the result of ensembling the predictions of 5 different networks.
8

Under review as a conference paper at ICLR 2018
5 CONCLUSIONS
We have presented an effective domain adaptation algorithm that has achieved state of the art results in a number of benchmarks and has achieved accuracies that are almost on par with traditional supervised learning on digit recognition benchmarks targeting the MNIST and SVHN datasets. The resulting networks will exhibit strong performance on samples from both the source and target domains. check Our approach is sufficiently flexible to be usable for a variety of network architectures, including those based on randomly initialised and pre-trained networks. Miyato et al. (2017) stated that the self-ensembling methods presented by Laine & Aila (2017) ­ on which our algorithm is based ­ operate by label propagation. This view is supported by our results, in particular our MNIST  SVHN experiment. The latter requires additional intensity augmentation in order to sufficiently align the dataset distributions, after which good quality label predictions are propagated throughout the target dataset. In cases where data augmentation is insufficient to align the dataset distrubitions, a pre-trained network may be used to bridge the gap, as in our solution to the VisDA-17 challenge. This leads us to conclude that effective domain adaptation can be achieved by first aligning the distributions of the source and target datasets ­ the focus of much prior art in the field ­ and then refining their correspondance; a task to which self-ensembling is well suited.
ACKNOWLEDGMENTS
This research was funded by a grant from Marine Scotland. We would also like to thank nVidia corporation for their generous donation of a Titan X GPU.
REFERENCES
S. Chintala et al. Pytorch. URL http://pytorch.org.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1180­ 1189, 2015.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European Conference on Computer Vision, pp. 597­613. Springer, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations, 2017.
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.
Takeru Miyato, Schi-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.
9

Under review as a conference paper at ICLR 2018

Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pp. 55­69. Springer, 1998.
Paolo Russo, Fabio Maria Carlucci, Tatiana Tommasi, and Barbara Caputo. From source to target and back: symmetric bi-directional adaptive gan. arXiv preprint arXiv:1705.08824, 2017.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, 2017.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163­1171, 2016.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. arXiv preprint arXiv:1704.01705, 2017.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In IEEE International Joint Conference on Neural Networks, pp. 1453­1460, 2011.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. 2017.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, 2017.
Zhi-Hua Zhou and Ming Li. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on knowledge and Data Engineering, 17(11):1529­1541, 2005.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.

A DATASETS

The datasets used in this paper are described in Table 4.

USPSa
MNIST
SVHN
CIFAR-10 STLb Syn-Digitsc
Syn-Signs
GTSRB

# train 7,291 60,000 73,257 50,000
5,000 479,400 100,000 32,209

# test 2,007 10,000 26,032 10,000
8,000 9,553 ­ 12,630

# classes 10 10 10 10
10 10 43 43

Resolution 16 × 16 28 × 28 32 × 32 32 × 32
96 × 96 32 × 32 40 × 40
varies

Channels Mono Mono RGB RGB
RGB RGB RGB RGB

a Available from http://statweb.stanford.edu/~tibs/ ElemStatLearn/datasets/zip.train.gz and http:

//statweb.stanford.edu/~tibs/ElemStatLearn/
datasets/zip.test.gz b Available from http://ai.stanford.edu/~acoates/stl10/ c Available from Ganin's website at http://yaroslav.ganin.net/

Table 4: datasets

B SMALL IMAGE EXPERIMENT TRAINING
B.1 TRAINING PROCEDURE Our networks were trained for 300 epochs. We used the Adam Kingma & Ba (2015) gradient descent algorithm with a learning rate of 0.001. We trained using mini-batches composed of 256 samples,
10

Under review as a conference paper at ICLR 2018

OUR RESULTS Validation Test
OUR RESULTS Validation Test

Plane ­
96.3 96.9 M.cycle ­
88.6 86.3

Bicycle ­
87.9 92.4 Person ­
77.4 75.3

Bus ­
84.7 92.0 Plant ­
93.3 97.7

Car ­
55.7 97.2 Sk.brd ­
92.8 93.3

Horse ­
95.9 95.2 Train ­
87.5 94.5

Knife ­
95.2 98.8 Truck ­
38.2 93.3

MEAN CLASS ACC ­
82.8 92.8

Table 5: Full VisDA-17 results

except in the Syn-digits  SVHN and Syn-signs  GTSRB experiments where we used 128 in order to reduce memory usage. The self-ensembling loss was weighted by a factor of 3 and the class balancing loss was weighted by 0.005. Our teacher network weights ti were updated so as to be an exponential moving average of those of the student si using the formula ti = ti-1 + (1 - )si, with a value of 0.99 for . A complete pass over the target dataset was considered to be one epoch in all experiments except the MNIST  USPS and CIFAR-10  STL experiments due to the small size of the target datasets, in which case one epoch was considered to be a pass over the larger soure dataset.
We found that using the proportion of samples that passed the confidence threshold could be used to drive early stopping (Prechelt (1998)). The final score was the target test set performance at the epoch at which the high confidence threshold pass rate was obtained.
C VISDA-17
C.1 HYPER-PARAMETERS
Our training procedure was the same as that used in the small image experiments, except that we used 160 × 160 images, a batch size of 56 (reduced from 64 to fit within the memory of an nVidia 1080-Ti), a self-ensembling weight of 10 (instead of 3), a confidence threshold of 0.9 (instead of 0.968) and a class balancing weight of 0.01. We used the Adam Kingma & Ba (2015) gradient descent algorithm with a learning rate of 10-5 for the final two randomly initialized layers and 10-6 for the pre-trained layers. The first convolutional layer and the first group of convolutional layers (with 64 feature channels) of the pre-trained ResNet were left unmodified during training.
Data augmentation:
· scale image so that its smallest dimension is 176 pixels, then randomly crop a 160 × 160 section from the scaled image
· No random affine transformations as they increase confusion between the car and truck classes in the validation set
· random uniform scaling in the range [0.75, 1.333]
· random intensity/brightness scaling in the range [0.75, 1.333]
· random rotations, normally distributed with a standard deviation of 0.2
· horizontal flipping
· random desaturation in which the colours in an image are randomly desaturated to greyscale by a factor between 0% and 100%
· rotations in colour space, around a randomly chosen axes with a standard deviation of 0.05
· random offset in colour space, after standardisation using parameters specified by PyTorch implementation of ResNet-152

11

Under review as a conference paper at ICLR 2018

D NETWORK ARCHITECTURES

Our network architectures are shown in Tables 6 - 9.

Description 28 × 28 Mono image Conv 5 × 5 × 32, batch norm Max-pool, 2x2 Conv 3 × 3 × 64, batch norm Conv 3 × 3 × 64, batch norm Max-pool, 2x2 Dropout, 50% Fully connected, 256 units Fully connected, 10 units, softmax

Shape 28 × 28 × 1 24 × 24 × 32 12 × 12 × 32 10 × 10 × 64 8 × 8 × 64 4 × 4 × 64 4 × 4 × 64
256
10

Table 6: MNIST  USPS architecture

Description 32 × 32 Mono image Conv 3 × 3 × 32, pad 1, batch norm Conv 3 × 3 × 32, pad 1, batch norm
Max-pool, 2x2 Conv 3 × 3 × 64, pad 1, batch norm Conv 3 × 3 × 64, pad 1, batch norm Conv 3 × 3 × 64, pad 1, batch norm
Max-pool, 2x2 Conv 3 × 3 × 128, pad 1, batch norm Conv 3 × 3 × 128, pad 1, batch norm Conv 3 × 3 × 128, pad 1, batch norm
Max-pool, 2x2
Global pooling layer
Dropout, 50%
Fully connected, 128 units
Fully connected, 10 units, softmax

Shape 32 × 32 × 1 32 × 32 × 32 32 × 32 × 32 16 × 16 × 32 16 × 16 × 64 16 × 16 × 64 16 × 16 × 64 8 × 8 × 64 8 × 8 × 128 8 × 8 × 128 8 × 8 × 128 4 × 4 × 128 1 × 1 × 128 1 × 1 × 128
128
10

Table 7: MNIST  SVHN architecture

Description 32 × 32 RGB image Conv 3 × 3 × 128, pad 1, batch norm Conv 3 × 3 × 128, pad 1, batch norm Conv 3 × 3 × 128, pad 1, batch norm
Max-pool, 2x2
Dropout, 50% Conv 3 × 3 × 256, pad 1, batch norm Conv 3 × 3 × 256, pad 1, batch norm Conv 3 × 3 × 256, pad 1, batch norm
Max-pool, 2x2
Dropout, 50% Conv 3 × 3 × 512, pad 0, batch norm Conv 1 × 1 × 256, batch norm Conv 1 × 1 × 128, batch norm
Global pooling layer
Fully connected, 10 units, softmax

Shape 32 × 32 × 3 32 × 32 × 128 32 × 32 × 128 32 × 32 × 128 16 × 16 × 128 16 × 16 × 128 16 × 16 × 256 16 × 16 × 256 16 × 16 × 256 8 × 8 × 256 8 × 8 × 256 6 × 6 × 512 6 × 6 × 256 6 × 6 × 128 1 × 1 × 128
10

Table 8: CIFAR-10  STL and Syn-Digits  SVHN architecture

12

Under review as a conference paper at ICLR 2018

Description 40 × 40 RGB image Conv 3 × 3 × 96, pad 1, batch norm Conv 3 × 3 × 96, pad 1, batch norm Conv 3 × 3 × 96, pad 1, batch norm
Max-pool, 2x2
Dropout, 50% Conv 3 × 3 × 192, pad 1, batch norm Conv 3 × 3 × 192, pad 1, batch norm Conv 3 × 3 × 192, pad 1, batch norm
Max-pool, 2x2
Dropout, 50% Conv 3 × 3 × 384, pad 1, batch norm Conv 3 × 3 × 384, pad 1, batch norm Conv 3 × 3 × 384, pad 1, batch norm
Max-pool, 2x2
Dropout, 50%
Global pooling layer
Fully connected, 43 units, softmax

Shape 40 × 40 × 3 40 × 40 × 96 40 × 40 × 96 40 × 40 × 96 20 × 20 × 96 20 × 20 × 96 20 × 20 × 192 20 × 20 × 192 20 × 20 × 192 10 × 10 × 192 10 × 10 × 192 10 × 10 × 384 10 × 10 × 384 10 × 10 × 384 5 × 5 × 384 5 × 5 × 384 1 × 1 × 384
43

Table 9: Syn-signs  GTSRB architecture

13

