Under review as a conference paper at ICLR 2018
FRATERNAL DROPOUT
Anonymous authors Paper under double-blind review
ABSTRACT
Recurrent neural networks (RNNs) are an important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets ­ Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised tasks (CIFAR-10).
1 INTRODUCTION
Recurrent neural networks (RNNs) like long short-term memory (LSTM; Hochreiter & Schmidhuber (1997)) networks and gated recurrent unit (GRU; Chung et al. (2014)) are popular architectures for sequence modeling tasks like language generation, translation, speech synthesis, and machine comprehension. However, they are harder to optimize compared to feed-forward networks due to challenges like variable length input sequences, repeated application of the same transition operator at each time step, and largely-dense embedding matrix that depends on the vocabulary size. Due to these aforementioned challenges in optimizing RNNs compared with feed-forward networks, the application of batch normalization and its variants (layer normalization, recurrent batch normalization, recurrent normalization propagation) have not been as successful as their counterparts in feed-forward networks (Laurent et al., 2016), although they do considerably provide performance gains. Similarly, naive application of dropout (Srivastava et al., 2014) has been shown to be ineffective in RNNs (Zaremba et al., 2014). Therefore, regularization techniques for RNNs is an active area of research.
To address these challenges, Zaremba et al. (2014) proposed to apply dropout only to the nonrecurrent connections in multi-layer RNNs. Variational dropout (Gal & Ghahramani (2016)) uses the same dropout mask throughout a sequence during training. DropConnect (Wan et al., 2013) applies the dropout operation on the weight matrices. Zoneout (Krueger et al. (2016)), in a similar spirit with dropout, randomly chooses to use the previous time step hidden state instead of using the current one. Similarly as a substitute for batch normalization, layer normalization normalizes the hidden units within each sample to have zero mean and unit standard deviation. Recurrent batch normalization on the other hand applies batch normalization but with unshared mini-batch statistics for each time step (Cooijmans et al., 2016).
Merity et al. (2017a) and Merity et al. (2017b) on the other hand showed that activity regularization (AR) and temporal activation regularization (TAR)1 are also effective methods for regularizing LSTMs.
1TAR and Zoneout are similar in their motivations because both leads to adjacent time step hidden states to be close on average.
1

Under review as a conference paper at ICLR 2018

In this paper we propose a simple regularization based on dropout that we call fraternal dropout, where we minimize an equally weighted sum of prediction losses from two identical copies of the same LSTM with different dropout masks, and add as a regularization the 2 difference between the predictions (pre-softmax) of the two networks. We analytically show that our regularization objective is equivalent to minimizing the variance in predictions from different i.i.d. dropout masks. Our approach thus encourages the predictions to be invariant to dropout masks. We also discuss how our regularization is related to expectation linear dropout Ma et al. (2016), -model Laine & Aila (2016) and activity regularization Merity et al. (2017b), and empirically show that our method provides non-trivial gains over these related methods which we will explain furthermore in our ablation study (section 5).

2 FRATERNAL DROPOUT

Dropout is a powerful regularization for neural networks. It is usually more effective on densely connected layers because they suffer more from overfitting compared with convolution layers where the parameters are shared. For this reason dropout is an important regularization for RNNs. However, dropout has a gap between its training and inferencing phase since the later phase assumes linear activations to correct for the factor by which the expected value of each activation would be different Ma et al. (2016). In addition, the prediction of models with dropout generally vary with different dropout mask. However, the desirable property in such cases would be to have final predictions invariant to the dropout mask.
As such, the idea behind fraternal dropout is to train a neural network model in a way that encourages the variance in predictions under different dropout masks to be as small as possible. Specifically, consider we have an RNN model denoted by M() that takes as input X, where  denotes the model parameters. Let pt(zt, sti; )  Rm be the prediction of the model for input sample X at time t, for dropout mask sit and current input zt, where zt is a function of X and the hidden states corresponding to the previous time steps. Similarly, let t(pt(zt, sit; ), Y) be the corresponding tth time step loss value for the overall input-target sample pair (X, Y).
Then in fraternal dropout, we simultaneously feed-forward the input sample X through two identical copies of the RNN that share the same parameters  but with different dropout masks sti and stj at each time step t. This yields two loss values at each time step t given by t(pt(zt, sit; ), Y), and t(pt(zt, stj; ), Y). Then the overall loss function of fraternal dropout is given by,

T1 F D(X, Y) = 2
t=1

t(pt(zt, sti; ), Y) +

t(pt(zt, sjt ; ), Y)

 +
m

T

RF D(zt; )

t=1

(1)

where  is the regularization coefficient, m is the dimensions of pt(zt, sit; ) and RF D(zt; ) is the fraternal dropout regularization given by,

RF D(zt; ) := Esit,stj

pt(zt, sti; ) - pt(zt, sjt ; )

2 2

.

(2)

We approximate RF D(zt; ) using Monte Carlo where pt(zt, sti; ) and pt(zt, stj; ) are the same as the one used to calculate t values. Hence, the additional computation is negligible.
We note that the regularization term of our objective is equivalent to minimizing the variance in the prediction function with different dropout masks as shown below (proof in Appendix).
Remark 1. Let sit and sjt be i.i.d. dropout masks and pt(zt, sit; )  Rm be the prediction function as described above. Then,

RF D(zt; ) = Esti,sjt

m

pt(zt, sti; ) - pt(zt, stj ; )

2 2

=2

varsti (ptq(zt, sti; )).

q=1

(3)

2

Under review as a conference paper at ICLR 2018

3 RELATED WORK

3.1 RELATION TO EXPECTATION LINEAR DROPOUT (ELD)

Ma et al. (2016) analytically showed that the expected (over samples) error between a model's expected prediction over all dropout masks, and the prediction using the average mask, is upper bounded. Based on this result, they propose to explicitly minimize the difference (we have adapted their regularization to our notations),

RELD(zt; ) = Es pt(zt, s; ) - pt(zt, Es[s]; ) 2

(4)

where s is the dropout mask. However, due to feasibility consideration, they instead propose to use the following regularization in practice,

R~ ELD(zt; ) = Esi

pt(zt, si; ) - pt(zt, Es[s]; )

2 2

.

(5)

Specifically, this is achieved by feed-forwarding the input twice through the network, with and without dropout mask, and minimizing the main network loss (with dropout) along with the regularization term specified above (but without back-propagating gradients through the network without dropout). The goal of Ma et al. (2016) is to minimize the network loss along with the expected difference between the prediction from individual dropout mask and the prediction from the expected dropout mask. We note that our regularization objective is upper bounded by the expectation-linear dropout regularization as shown below (proof in Appendix).

Proposition 1. RF D(zt; )  4R~ELD(zt; ).

Finally as indicated above, they apply the target loss only on the network with dropout. In fact, in our own ablation studies (see section 5) we found that back-propagating target loss through the network (without dropout) makes optimizing the model harder. However, in our setting, simultaneously backpropagating target loss through both networks yields both performance gain as well as convergence gain. We believe convergence is faster for our regularization because network weights are more likely to get target based updates from back-propagation in our case. This is especially true in weight dropout (Wan et al., 2013) since in this case dropped weights do not get updated in the training iteration.

3.2 RELATION TO -MODEL

Laine & Aila (2016) propose -model with the goal of improving performance on classification

tasks in the semi-supervised setting. They propose a model similar to ours (considering the deep

feed-forward setting for our model) except they apply target loss on one of the networks and use

time-dependent

weighting

function

(t)

(while

we

use

constant

 m

).

The intuition in their case

is to leverage unlabeled data by using them to minimize the difference in prediction between the

two copies of the network with different dropout masks. Further, they also test their model in the

supervised setting but fail to explain the improvements they obtained by using this regularization.

We note that in our case we analytically show that our regularizer (also used in -model) is equal

to the variance in the model predictions (remark 1). Furthermore, we also show the relation of

our regularizer to expectation linear dropout (proposition 1). In section 5, we study the effect of

target based loss on both networks, which is not used in the -model. We find that applying target

loss on both the networks leads to significantly faster convergence. Finally, we bring to attention that

temporal embedding (another model proposed by Laine & Aila (2016), claimed to be a better version

of -model for semi-supervised, learning) is intractable in natural language processing applications

because storing averaged predictions over all of the time steps would be memory exhaustive (since

predictions are usually huge - tens of thousands values). On a final note, we argue that in the

supervised

case

using

a

time-dependent

weighting

function

(t)

instead

of

a

constant

value

 m

is

not

needed. Since ground true labels are known we have not observed the problem mentioned by Laine

& Aila (2016), that the network gets stuck in a degenerate solution when (t) is too large in earlier

epochs of training. We note that it is much easier to search for an optimal constant value, which is

true in our case, as opposed to tuning the time-dependent function.

Similarity to -model makes our method related to other semi-supervised works, mainly Rasmus et al. (2015) and Sajjadi et al. (2016). Since semi-supervised learning is not a primary focus of this paper, we refer to Laine & Aila (2016) for more details.

3

Under review as a conference paper at ICLR 2018

Model Zaremba et al. (2014) - LSTM (medium)
Zaremba et al. (2014) - LSTM (large) Gal & Ghahramani (2016) - Variational LSTM (medium)
Gal & Ghahramani (2016) - Variational LSTM (large) Inan et al. (2016) - Variational LSTM Inan et al. (2016) - Variational RHN Zilly et al. (2016) - Variational RHN Melis et al. (2017) - 5-layer RHN
Melis et al. (2017) - 4-layer skip connection LSTM Merity et al. (2017a) - AWD-LSTM 3-layer fraternal dropout + AWD-LSTM 3-layer

Parameters 10M 24M 20M 66M 51M 24M 23M 24M 24M 24M 24M

Validation 86.2 82.2 81.9 77.9 71.1 68.1 67.9 64.8 60.9 60.0 58.9

Table 1: Perplexity on Penn Treebank word level language modeling task.

Test 82.7 78.4 79.7 75.2 68.5 66.0 65.4 62.2 58.3 57.3 56.8

Model Merity et al. (2016) - Variational LSTM + Zoneout
Merity et al. (2016) - Variational LSTM Inan et al. (2016) - Variational LSTM
Melis et al. (2017) - 5-layer RHN Melis et al. (2017) - 1-layer LSTM Melis et al. (2017) - 2-layer skip connection LSTM Merity et al. (2017a) - AWD-LSTM 3-layer fraternal dropout + AWD-LSTM 3-layer

Parameters 20M 20M 28M 24M 24M 24M 34M 34M

Validation 108.7 101.7 91.5 78.1 69.3 69.1 68.6 66.8

Table 2: Perplexity on WikiText-2 word level language modeling task.

Test 100.9 96.3 87.0 75.6 65.9 65.9 65.8 64.1

4 EXPERIMENTS
4.1 LANGUAGE MODELS
In the case of language modeling we test our model on two benchmark datasets ­ Penn Tree-bank (PTB) dataset (Marcus et al., 1993) and WikiText-2 (WT2) dataset (Merity et al., 2016). Preprocessing from Mikolov et al. (2010) (for PTB corpus) and Moses tokenizer (Koehn et al., 2007) (for the WT2 dataset) were used. For both datasets we applied a AWD-LSTM 3-layer architecture described in Merity et al. (2017a)2. The number of parameters in the model used for PTB is 24 million as compared to 34 million in the case of WT2 because WT2 has a larger vocabulary size for which we use a larger embedding matrix. Apart from those differences, the architectures are identical.
Word level Penn Treebank (PTB)
We evaluate our model using the perplexity metric and compare the results that we have obtained against the existing state-of-the-art results. The results are reported in Table 1. Our approach achieves the state-of-the-art performance compared with existing benchmarks.
Word level WikiText-2
In a case of the WikiText-2 language modeling task we outperform the current state-of-the-art by a significant margin. The final results are presented in Table 2.
More details about the experiment may be found in the subsection 5.4.
4.2 IMAGE CAPTIONING
We also apply fraternal dropout on an image captioning task. We use the well-known show and tell model as a baseline3 (Vinyals et al., 2014). We emphasize that in the image captioning task, the
2We used the official Github repository code for this paper. 3We used PyTorch implementation with default hyper-parameters from github.com/ruotianluo/ neuraltalk2.pytorch.

4

Under review as a conference paper at ICLR 2018

Model Show and Tell Xu et al. (2015)
Baseline Fraternal dropout,  = 0.015 Fraternal dropout,  = 0.005

BLEU-1 66.6 68.8 69.3 69.3

BLEU-2 46.1 50.8 51.4 51.5

BLEU-3 32.9 36.1 36.6 36.9

BLEU-4 24.6 25.6 26.1 26.3

Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is the only difference between models. The rest of hyper-parameters are the same.

image encoder and sentence decoder architectures are usually learned together. Since we want to focus on the benefits of using fraternal dropout in RNNs we use frozen pretrained ResNet-101 (He et al., 2015) model as our image encoder. It means that our results are not directly comparable with other state-of-the-art methods, however we report results for the original methods so readers can see that our baseline performs well. The final results are presented in Table 3.
We argue that in this task smaller  values are optimal because the image captioning encoder is given all information in the beginning and hence the variance of consecutive predictions is smaller that in unconditioned natural language processing tasks. Fraternal dropout may benefits here mainly due to averaging gradients for different mask and hence updating weights more frequently.

5 ABLATION STUDIES
In this section, the goal is to study existing methods closely related to ours ­ expectation linear dropout Ma et al. (2016), -model Laine & Aila (2016) and activity regularization Merity et al. (2017b). All of our experiments for ablation studies, which apply a single layer LSTM, use the same hyper-parameters and model architecture4.

5.1 EXPECTATION-LINEAR DROPOUT (ELD)
The relation with expectation-linear dropout Ma et al. (2016) has been discussed in the section 2. Here we perform experiments to study the difference in performance when using the ELD regularization versus our regularization (FD). In addition to ELD, we also study a modification (ELDM) of ELD which applies target loss to both copies of LSTMs in ELD similar to FD (notice in their case they only have dropout on one LSTM). Finally we have a baseline model without any of these regularizations. The learning dynamics curves are shown in Figure 1. Our regularization performs better in terms of convergence compared with other methods. In terms of generalization, we find that FD is similar to ELD, but baseline and ELDM are much worse. Interestingly, looking at the train and validation curves together, ELDM seems to be suffering from optimization problems.

5.2 -MODEL

Since -model Laine & Aila (2016) is similar to our algorithm (even though it is designed for
semi-supervised learning in feed-forward networks), we study the difference in performance with -model5 both qualitatively and quantitatively to establish the advantage of our approach. First,
we run both single layer LSTM and 3-layer AWD-LSTM on PTB task to check how their model
compares with ours in the case of language modeling. The results are shown in Figure 1 and 2. We find that our model converges significantly faster than -model. We believe this result is achieved

4We use a batch size of 64, truncated back-propagation with 35 time steps, a constant zero state is provided

as the initial state with probability 0.01 (similar to Melis et al. (2017)), SGD with learning rate 30 (no mo-

mentum) which is multiplied by 0.1 whenever validation performance does not improve ever during 20 epochs,

weight dropout on the hidden to hidden matrix 0.5, dropout every word in a mini-batch with probability 0.1,

embedding dropout 0.65, output dropout 0.4 (final value of LSTM before embedding), input embedding size

of 655, the input/output size of LSTM is same as embedding size 655 (and the embedding weights are tied), gradient clipping of 0.25 and weight decay 1.2 × 10-6.

5We used constant function (t)

=

 m

since we want to focus on the difference when target loss is back-

propagated through one or two networks. Additionally, we find tuning a function instead of a constant infeasi-

ble.

5

Under review as a conference paper at ICLR 2018

Figure 1: Ablation study: Train (left) and validation (right) perplexity on PTB word level modeling with single layer LSTM (10M parameters). These curves study the learning dynamics of the baseline model, -model, Expectation-linear dropout (ELD), Expectation-linear dropout modification (ELDM) and fraternal dropout (FD, our algorithm). We find that FD converges faster than the regularizers in comparison, and generalizes at par.

Figure 2: Validation perplexity on PTB word level modeling for -model and fraternal dropout. We find that FD converges faster and generalizes at par.

Figure 3: Average hidden state activation is re-

duced when any of the regularizer described is

used.

The

y-axis

is

the

value

of

1 d

m · ht

2 2

.

because we back-propagate the target loss through both networks (in contrast to -model) that leads to weights getting updated using target-based gradients more often.
Even though we designed our algorithm specifically to address problems in RNNs, to have a fair comparison, we compare with -model on a semi-supervised task which is their goal. Specifically, we used the CIFAR-10 dataset that is consisted of 32 × 32 images from 10 classes. Following usual splits in semi-supervised field we decided to use 4 thousands labeled and 41 thousands unlabeled samples for training, 5 thousands labeled samples for validation and 10 thousands labeled samples for test set. We used the original ResNet-56 architecture. We run grid search on   {0.05, 0.1, 0.15, 0.2}, dropout rates in {0.05, 0.1, 0.15, 0.2} and leave the rest of the hyperparameters unchanged. We additionally checked importance of using unlabeled data. The results are reported in Table 4. We find that our algorithm performs at par with -model. When unlabeled data is not used, fraternal dropout provides slightly better results as compared to ordinal dropout.

5.3 ACTIVITY REGULARIZATION AND TEMPORAL ACTIVITY REGULARIZATION ANALYSIS

The authors of Merity et al. (2017b) study the importance of activity regularization (AR)6 and tem-

poral activity regularization (TAR) in LSTMs given as,

RAR(zt; )

=

 d

ht

2 2

(6)

RT AR(zt; )

=

 d

ht - ht-1

2 2

(7)

6We used m · ht 22, where m is the dropout mask, in our actual experiments with AR because it was implemented as such in the original paper's Github repository Merity et al. (2017a).

6

Under review as a conference paper at ICLR 2018

Model Ordinary
None Fraternal ( = 0.05) Ordinary + -model Fraternal ( = 0.15)

Dropout rate 0.1 0.0 0.05 0.1 0.1

Unlabeled data used No No No Yes Yes

Validation 78.4 (± 0.25) 78.8 (± 0.59) 79.3 (± 0.38) 80.2 (± 0.33) 80.5 (± 0.18)

Test 76.9 (± 0.31) 77.1 (± 0.3) 77.6 (± 0.35) 78.5 (± 0.46) 79.1 (± 0.37)

Table 4: Accuracy on altered (semi-supervised) CIFAR-10 dataset for ResNet-56 based models. We find that our algorithm performs at par with -model. When unlabeled data is not used ordinary dropout hurts performance while fraternal dropout provides slightly better results. It means that our methods may be beneficial when we lack data and have to use additional regularizing methods.

Figure 4: Ablation study: Train (left) and validation (right) perplexity on PTB word level modeling with single layer LSTM (10M parameters). These curves study the learning dynamics of the baseline model, temporal activity regularization (TAR), prediction regularization (PR), activity regularization (AR) and fraternal dropout (FD, our algorithm). We find that FD both converges faster and generalizes better than the regularizers in comparison.

where ht  Rd is the LSTM's output activation at time step t (hence depends on both current input zt and the model parameters ). Notice that AR and TAR regularizations are applied on the output of
the LSTM, while our regularization is applied on the pre-softmax output pt(zt, sti; ) of the LSTM.
However, since our regularization can be decomposed as

RF D(zt; ) = Esi,sj = Esi,sj

pt(zt, sit; ) - pt(ht, stj ; )

2 2

(8)

pt(zt, sti; )

2 2

+

pt(zt, stj ; )

2 2

-

2pt(zt,

sit;

)T

pt(zt,

stj

;

)

(9)

and encapsulates an 2 term along with the dot product term, we perform experiments to confirm that the gains in our approach is not due to the 2 regularization alone. A similar argument goes

for the TAR objective. We run a grid search on   {1, 2, . . . , 12},   {1, 2, . . . , 12}, which

include the hyper-parameters mentioned in Merity et al. (2017a). For our regularization, we use

  {0.05, 0.1, . . . , 0.4}. Furthermore, we also compare with a regularization (PR) that regularizes

pt(zt, sti; )

2 2

to

further

rule-out

any

gains

only

from

2 regularization. Based on this grid search,

we pick the best model on the validation set for all the regularizations, and additionally report a

baseline model without any of these four mentioned regularizations. The learning dynamics curves

are shown in Figure 4. Our regularization performs better both in terms of convergence and gener-

alization compared with other methods. Average hidden state activation is reduced when any of the

regularizer described is applied (see Figure 3).

5.4 LANGUAGE MODELING FAIR COMPARISON
As mentioned in the subsection 4.1, influenced by Melis et al. (2017), we want to make sure that fraternal dropout outperform existing methods not simply because of extensive hyper-parameter grid search. Hence, in our experiments we left a vast majority of hyper-parameters mentioned in the original paper unchanged i.e. embedding and hidden states sizes, gradient clipping value, weight decay and the values used for all dropout layers (dropout on the word vectors, the output between

7

Under review as a conference paper at ICLR 2018

Dropout Ordinary Ordinary Fraternal Fraternal Fraternal

Fine-tuning None One None One Two

PTB Validation
60.7 60.0 59.8 58.9 58.5

Test 58.8 57.3 58.0 56.8 56.2

WT2 Validation Test
69.1 66.0 68.6 65.8 68.3 65.3 66.8 64.1
­­

Table 5: Importance of fine-tuning for AWD-LSTM 3-layer model. Perplexity for the Penn Treebank and WikiText-2 language modeling task.

LSTM layers, the output of the final LSTM, and embedding dropout). However, a few changes were necessary:
· the coefficients for AR and TAR have to be altered because fraternal dropout also affects RNNs activation (as explained in the subsection 5.3) ­ we have not run grid search to obtain the best values but simply deactivated AR and TAR regularizers;
· since we need twice as much memory, the batch size is halved so the model needs approximately the same amount of memory and hence fits on the same GPU.
The last change is altering ASGD non-monotone interval n hyper-parameter. We run a grid search on n  {5, 25, 40, 50, 60} and obtained very similar results for the largest values (40, 50 and 60). Hence, our model is trained longer using ordinary SGD optimizer as compared to the original model.
To double check that we do not obtain trivial gains, we run ten learning procedures for the original hyper-parameters with different seeds (without fine-tuning) for PTB dataset to compute confidence intervals. The average best validation perplexity is 60.64 ± 0.15 with the minimum value equals 60.33. The same for test perplexity is 58.32 ± 0.14 and 58.05, respectively. Our score (59.8 validation and 58.0 test perplexity) beats ordinal dropout minimum values.
Due to lack of computational power, we ran a single training procedure for fraternal dropout on WT2 dataset. In this experiment, we decided to use the best hyper-parameters found for PTB dataset ( = 0.1, non-monotone interval n = 60 and halved batch size).
We confirm that ASGD benefits when the fine-tuning step is used (Merity et al., 2017a). However, this is a very time-consuming practice and since different hyper-parameters may be used in this additional part of the learning procedure, the probability of obtaining better results due to the extensive grid search is higher. Hence, in our experiments we use the same fine-tuning procedure as implemented in the official repository (even fraternal dropout was not used). We present the importance of fine-tuning in Table 5.
We argue that running grid search for all hyper-parameters jointly may lead to better results (altering dropout rates may be especially beneficial since our method explicitly takes advantage of using dropout). However, our goal here is to rule out the possibility of outperforming just because of using better hyper-parameters.
6 CONCLUSION
In this paper we propose a simple regularization method for RNNs called fraternal dropout that acts as a regularization by reducing the variance in model predictions across different dropout masks. We show that our model achieves state-of-the-art results on benchmark language modeling tasks along with faster convergence. We also analytically study the relationship between our regularization and expectation linear dropout Ma et al. (2016). We perform a number of ablation studies to evaluate our model from different aspects and carefully compare it with related methods both qualitatively and quantitatively.
REFERENCES
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

8

Under review as a conference paper at ICLR 2018
Tim Cooijmans, Nicolas Ballas, Ce´sar Laurent, and Aaron C. Courville. Recurrent batch normalization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019­1027, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735­1780, 1997.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pp. 177­180. Association for Computational Linguistics, 2007.
David Krueger, Tegan Maharaj, Ja´nos Krama´r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.
Ce´sar Laurent, Gabriel Pereyra, Phile´mon Brakel, Ying Zhang, and Yoshua Bengio. Batch normalized recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 2657­2661. IEEE, 2016.
Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, and Eduard Hovy. Dropout with expectation-linear regularization. arXiv preprint arXiv:1609.08017, 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313­330, 1993.
Ga´bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182, 2017a.
Stephen Merity, Bryan McCann, and Richard Socher. Revisiting activation regularization for language rnns. arXiv preprint arXiv:1708.01009, 2017b.
Tomas Mikolov, Martin Karafia´t, Lukas Burget, Jan Cernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning with ladder network. CoRR, abs/1507.02672, 2015. URL http://arxiv.org/abs/1507.02672.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163­1171, 2016.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1): 1929­1958, 2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2014. URL http://arxiv.org/abs/1411.4555.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1058­1066, 2013.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.03044.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ik, and Ju¨rgen Schmidhuber. Recurrent highway networks. arXiv preprint arXiv:1607.03474, 2016.
9

Under review as a conference paper at ICLR 2018

APPENDIX

Remark 1. Let sit and stj be i.i.d. dropout masks and pt(zt, sti; )  Rm be the prediction function as described above. Then,

RF D(zt; ) = Esit,sjt

m

pt(zt, sti; ) - pt(zt, sjt ; )

2 2

=2

varsti (pqt (zt, sti; )). (10)

q=1

Proof. For simplicity of notation, we omit the time index t.

RF D(z; ) = Esi,sj

p(z, si; ) - p(z, sj; )

2 2

= Esi

p(z, si; )

2 2

+ Esj

p(z, sj; )

2 2

- 2Esi,sj p(z, si; )T p(z, sj ; )

m
= 2 Esi pq(z, si; )2 - Esi,sj [pq(z, si; )pq(z, sj ; )]

q=1

m
= 2 Esi pq(z, si; )2 - Esi [pq(z, si; )] Esj [pq(z, si; )]

q=1

m
= 2 Esi pq(z, si; )2 - Esi [pq(z, si; )]2

q=1

m

= 2 varsi (pq(z, si; )).

q=1

(11) (12)
(13) (14) (15) (16)

Proposition 1. RF D(zt; )  4R~ELD(zt; ).

Proof. Let s¯ := Es[s], then

Rt(zt) := Esit,sjt

pt(zt, sit; ) - pt(zt, sjt ; )

2 2

= Esti,stj

pt(zt, sti; ) - pt(zt, s¯; ) + pt(zt, s¯; ) - pt(zt, stj; )

2 2

= 4Esit,sjt

pt(zt, sit; ) - pt(zt, s¯; ) + pt(zt, s¯; ) - pt(zt, stj; ) 22

2 2

.

(17) (18)
(19)

Then using Jensen's inequality,

Rt(zt)  4Esit,sjt

1 2

pt(zt, sit; ) - pt(zt, s¯; )

2 2

+

1 2

pt(zt, s¯; ) - pt(zt, stj; )

2 2

(20)

= 4Esit

pt(zt, sit; ) - pt(zt, s¯; )

2 2

= 4R~ELD(zt; ).

(21)

10

