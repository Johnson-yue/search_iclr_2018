Under review as a conference paper at ICLR 2018
BIAS-VARIANCE DECOMPOSITION FOR BOLTZMANN MACHINES
Anonymous authors Paper under double-blind review
ABSTRACT
We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation. Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases. Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.
1 INTRODUCTION
Why the deep learning architectures can generalize well despite their high representation power with a large number of parameters is one of crucial problems in theoretical deep learning analysis, and there are a number of attempts to solve the problem with focusing on several aspect such as sharpness and robustness (Dinh et al., 2017; Wu et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Kawaguchi et al., 2017). However, the complete understanding of this phenomenon is not achieved yet due to the complex structure of deep learning architectures.
To theoretically analyze the generalizability of the architectures, in this paper, we focus on a family of Boltzmann machines (Ackley et al., 1985) including RBMs (Restricted Boltzmann Machines) (Hinton, 2002; Smolensky, 1986) and DBMs (Deep Boltzmann Machines) (Salakhutdinov & Hinton, 2012), the fundamental probabilistic model of deep learning (Hinton et al., 2006), and we firstly present bias-variance decomposition for Boltzmann machines. The key to achieve this analysis is to employ an information geometric formulation of a hierarchical probabilistic model, which was firstly explored by Amari (2001); Nakahara & Amari (2002); Nakahara et al. (2006). In particular, the recent advances of the formulation by Sugiyama et al. (2016; 2017) enables us to analytically obtain the Fisher information of parameters in Boltzmann machines, which is essential to give the lower bound of variances in bias-variance decomposition.
We show an interesting phenomenon revealed by our bias-variance decomposition: The variance does not necessarily increase while the bias always decreases when we include more parameters in Boltzmann machines, which is caused by its hierarchical structure. Our result indicates the possibility of designing a deep learning architecture that can reduce both of bias and variance, leading to better generalization ability with keeping the representation power.
The remainder of this paper is organized as follows: First we formulate Boltzmann machines using an information geometric formulation in Section 2, which includes the traditional Boltzmann machines (Section 2.2), arbitrary-order Boltzmann machines (Section 2.3), and Boltzmann machines with hidden nodes such as RBMs and DBMs (Section 2.4). Then we present the main result of this paper, bias-variance decomposition for Boltzmann machines, in Section 3 and discuss its property. Finally, we conclude with summarizing the contribution of this paper in Section 4.
2 FORMULATION
To theoretically analyze a family of Boltzmann machines (Ackley et al., 1985) including RBMs (Restricted Boltzmann Machines) (Hinton, 2002; Smolensky, 1986) and DBMs (Deep Boltzmann Machines) (Salakhutdinov & Hinton, 2012), we introduce an information geometric formulation of hierarchical probabilistic models that include the above family of Boltzmann machines.
1

Under review as a conference paper at ICLR 2018

2.1 PRELIMINARY: LOG-LINEAR MODEL

First we prepare a log-linear probabilistic model on a partial order structure, which has been intro-
duced by Sugiyama et al. (2016; 2017). Let (S, ) be a partially ordered set, or a poset (Gierz et al., 2003), where a partial order  is a relation between elements in S satisfying the following three properties for all x, y, z  S: (1) x  x (reflexivity), (2) x  y, y  x  x = y (antisymmetry), and (3) x  y, y  z  x  z (transitivity). We assume that S is always finite and includes the least element (bottom)   S; that is,   x for all x  S. We denote S \ {} by S+.

We use two functions, the zeta function  : S × S  {0, 1} and the Mo¨bius function µ : S × S  Z.

The zeta function  is defined as (s, x) = 1 if s  x and (s, x) = 0 otherwise, while the Mo¨bius

function µ is its convolutional inverse, that is,

 (x, s)µ(s, y) =



{

µ(s, y) =

1 0

if x = y, otherwise,

sS

xsy

which is inductively defined as



1

µ(x, y)

=



- 0

xs<y µ(x, s)

if x = y, if x < y,
otherwise.

For any functions f , g, and h with the domain S such that





g(x) = (s, x)f (s) = f (s), h(x) = (x, s)f (s) = f (s),

sS

sx

sS

sx

f is uniquely recovered using the Mo¨bius function:
 f (x) = µ(s, x)g(s), f (x) = µ(x, s)h(s).

sS

sS

This is the Mo¨bius inversion formula and is fundamental in enumerative combinatorics (Ito, 1993).

Sugiyama et al. (2017) introduced a log-linear model on S, which gives a discrete probability distri-

bution with the structured outcome space (S, ). Let P denote a probability distribution that assigns a probability p(x) for each x  S satisfying xS p(x) = 1. Each probability p(x) for x  S is defined as


log p(x) := (s, x)(s) = (s).

(1)

sS

sx

From the Mo¨bius inversion formula,  is obtained as 
(x) = µ(s, x) log p(s).

sS

In addition, we introduce  : S  R as 
(x) := (x, s)p(s) = p(s),

sS


sx

p(x) = µ(x, s)(s).

sS

(2)
(3) (4)

The second equation is from the Mo¨bius inversion formula. Sugiyama et al. (2017) showed that the set of distributions S = {P | 0 < p(x) < 1 and p(x) = 1} always becomes the dually flat Riemannian manifold. This is why two functions  and  are dual coordinate systems of S
connected with the Legendre transformation, that is,

 = (),  = ()

with two convex functions () := -() = - log p(),

 () := p(x) log p(x).
xS

2

Under review as a conference paper at ICLR 2018

Moreover, the Riemannian metric g() ( =  or ) such that

g() = (), g() = (),

which corresponds to the gradient of  or , is given as

gxy ()

=

(x) (y)

=

[ log p(s)
E (x)

] log p(s)
(y)

=

 (x, s)(y, s)p(s) -

(x)(y),

gxy ()

=

(x) (y)

=

[ log p(s)
E (x)

] log p(s)
(y)

=

sS
 µ(s, x)µ(s, y)p(s)-1.

sS

(5) (6)

for all x, y  S+. Furthermore, S is in the exponential family (Sugiyama et al., 2016), where  coincides with the natural parameter and  with the expectation parameter.

Let us consider two types of submanifolds:

S = { P  S | (x) = (x) for all x  dom() } ,
S = { P  S | (x) = (x) for all x  dom() }
specified by two functions ,  with dom(), dom()  S+, where the former submanifold S has constraints on  while the latter S has those on . It is known in information geometry that S is e-flat and S is m-flat, respectively (Amari, 2016). Suppose that dom()  dom() = S+ and dom()  dom() = . Then the intersection S  S is always the singleton, that is, the distribution Q satisfying Q  S and Q  S always uniquely exists, and the following Pythagorean theorem holds:

DKL(P, R) = DKL(P, Q) + DKL(Q, R), DKL(R, P ) = DKL(R, Q) + DKL(Q, P ) for any P  S and R  S.

(7) (8)

2.2 STANDARD BOLTZMANN MACHINES

A Boltzmann machine is represented as an undirected graph G = (V, E) with a vertex set V = {1, 2, . . . , n} and an edge set E  {{i, j} | i, j  V }. The energy function :{0, 1}n  R of the

Boltzmann machine G is defined as



(x; ) = - ixi - ijxixj

iV {i,j}E

for each x = (x1, x2, . . . , xn)  {0, 1}n, where  = (1, 2, . . . , n, 12, 13, . . . , n-1n) is a parameter vector for vertices (bias) and edges (weight) such that ij = 0 if {i, j}  E. The probability p(x; ) of the Boltzmann machine G is obtained for each x  {0, 1}n as

exp(-(x, )) p(x; ) =
Z ()

(9)

with a partition function Z() such that 
Z() = exp(-(x; ))
x{0,1}n
 to ensure the condition x{0,1}n p(x) = 1.

(10)

It is clear that a Boltzmann machine is a special case of the log-linear model in Equation (1) with

S = 2V , the power set of V , and  = . Let each x  S be the set of indices of "1" of x  {0, 1}n

and  be the inclusion relation, that is, x  y if and only if x  y. Suppose that

B = { x  S+

} |x| = 1 or x  E ,

(11)

where |x| is the cardinality of x, which we call a parameter set. The Boltzmann distribution in Equations (9) and (10) directly corresponds to the log-linear model in Equation (1):
 log p(x) = (s, x)(s) - (), () = -() = log Z().
sB

3

Under review as a conference paper at ICLR 2018

This means that the set of Boltzmann distributions S(B) that can be represented by a parameter set B is a submanifold of S given as
S(B) := { P  S | (x) = 0 for all x  B } .

Given an empirical distribution P^. Learning of a Boltzmann machine is to find the best approximation of P^ from the Boltzmann distributions S(B), which is formulated as a minimization problem
of the KL (Kullback­Leibler) divergence:

min
PB S(B)

DKL(P^,

PB )

=

min
PB S(B)


sS

p^(s)

log

p^(s) .
pB (s)



This is equivalent to maximize the log-likelihood L(PB) = N sS p^(s) log pB(s) with the sample

size N . Since we have



 B (x)

DKL(P^,

PB

)

=

  B (x)

 p^(s) log pB(s)
sS 



=

  B (x)



p^(s)

B (u)

sS

<us

-

 B(x) (B) sS p^(s)

= ^(x) - B(x),

the KL divergence DKL(P^, PB) is minimized when ^(x) = B(x) for all x  B, which is well known as the learning equation of Boltzmann machines. Thus the minimizer PB  S(B) of the KL divergence DKL(P^, PB) is the distribution given as
{ B(x) = ^(x) if x  B  {},
B(x) = 0 otherwise.

This distribution PB is known as m-projection of P^ onto S(B) (Sugiyama et al., 2017), which is unique and always exists as S has the dually flat structure with respect to (, ).

2.3 ARBITRARY-ORDER BOLTZMANN MACHINES
The parameter set B is fixed in Equation (11) in the traditional Boltzmann machines, but our loglinear formulation allows us to include or remove any element in S+ = 2V \ {} as a parameter. This attempt was partially studied by Sejnowski (1986); Min et al. (2014) that include higher order interactions to increase the representation power of Boltzmann machines.
Let B1, B2, . . . , Bm be a sequence of parameter sets such that B1  B2  · · ·  Bm-1  Bm = S.
Since we have a hierarchy of submanifolds S(B1)  S(B2)  · · ·  S(Bm-1)  S(Bm) = S,
we obtain the decreasing sequence of KL divergences: DKL(P^, PB1 )  DKL(P^, PB2 )  · · ·  DKL(P^, PBm-1 )  DKL(P^, PBm ) = 0,
where each PBi = argminP S(Bi) DKL(P^, P ), the best approximation of P^ using Bi.
2.4 BOLTZMANN MACHINES WITH HIDDEN NODES
Another strategy to increase the representation power is to use hidden nodes (Le Roux & Bengio, 2008). A Boltzmann machine with hidden nodes is represented as G = (V  H, E), where V and H correspond to visible and hidden nodes, respectively, and the resulting domain S = 2V H . In particular, restricted Boltzmann machines (RBMs) (Smolensky, 1986; Hinton, 2002) are often used in applications, where the edge set is given as
E = { {i, j} | i  V, j  H }

4

Under review as a conference paper at ICLR 2018

Input Hidden Hidden layer layer 1 layer 2
1 34
2

{1,2,3,4}
{1,2,3} {1,2,4} {1,3,4} {2,3,4} {1,2} {1,3} {1,4} {2,3} {2,4} {3,4}
{1} {2} {3} {4} Ø

Figure 1: An example of a deep Boltzmann machine (left) with an input layer (visible nodes) V = {1, 2} with two hidden layers H1 = {3} and H2 = {4}, and the corresponding domain set SV H (right). In the right-hand side, the colored objects {1}, {2}, {3}, {4}, {1, 2}, {2, 3}, and {3, 4} denote the parameter set B, which correspond to nodes and edges of the DBM in the left-hand
side.

Moreover, in a deep Boltzmann machine (DBM) (Salakhutdinov & Hinton, 2009; 2012), which is the beginning of the recent trend of deep learning (LeCun et al., 2015; Goodfellow et al., 2016), the hidden nodes H is divided into k disjoint subsets (layers) H1, H2, . . . , Hk and
E = { {i, j} | i  Hl-1, j  Hl, l  {1, . . . , k} } ,
where V = H0 for simplicity. Let S = 2V and S = 2V H and S and S be the set of distributions with the domains S and S, respectively. In both cases of RBMs and DBMs, we have
B = { x  S | |x| = 1 or x  E } ,

(see Figure 1) and the set of Boltzmann distributions is obtained as

S(B) = { P  S

} (x) = 0 for all x  B .

Since the objective of learning Boltzmann machines with hidden nodes is MLE (maximum likeli-

hood estimation) with respect to the marginal probabilities of the visible part, the target empirical distribution P^  S is extended to the submanifold S(P^) such that

S  (P^ )

=

{ P



S

} (x) = ^(x) for all x  S ,

and the process of learning Boltzmann machines with hidden nodes is formulated as double minimization of the KL divergence such that

min DKL(P, PB).
P S(P^),PB S(B)

(12)

Since two submanifolds S(B) and S(P^) are e-flat and m-flat, respectively, it is known that the EM-algorithm can obtain the global optimum of Equation (12) (Amari, 2016, Section 8.1.3), which was first analyzed by Amari et al. (1992). Since this computation is infeasible due to combinatorial explosion of the domain S = 2V H , a number of approximation methods such as Gibbs sampling have been proposed (Salakhutdinov & Hinton, 2012).

3 BIAS-VARIANCE DECOMPOSITION
Here we present the main result of this paper, bias-variance decomposition for Boltzmann machines. We focus on the expectation of the squared KL divergence E[DKL(P , P^B)2] from the true (unknown) distribution P  to the MLE P^B of an empirical distribution P^ by a Boltzmann machine with a parameter set B, and decompose it using information geometric properties.

5

Under review as a conference paper at ICLR 2018

PB* P^B '(B) (x) = 0, x  B

DKL(P*, P^B) = DKL(P*, PB*) + DKL(PB* , P^B) P*
'(P*) (x) = *(x), x  B
Figure 2: Illustration of the Pythagorean theorem.

Theorem 1 (Bias-variance decomposition of the KL divergence). Given a Boltzmann machine with

a parameter set B. Let P   S be the true (unknown) distribution, PB , P^B  S(B) be the MLEs of P  and an empirical distribution P^, respectively. If the Boltzmann machine includes hidden nodes,

let P  be a distribution in the submanifold S(P ) achieved in Equation (12) and PB , P^B  S(B).

We [have

]

[]

E DKL(P , P^B)2 = DKL(P , PB )2 + E DKL(PB , P^B)2  DKL(P , PB )2 + var(PB , B),

var(PB , B)

=

1 N

( 

)2

pB (x)-1

µ(x, s)B (s) ,

xS

sB

bias2

variance

where the equality holds when the sample size N  .

Proof. From the Pythagorean theorem illustrated in Figure 2,

[ E DKL(P

,

] P^B )2

=

[( E DKL(P

,

PB )

+

DKL(PB

,

P^B ))2 ]

[]

= E DKL(P , PB )2 + 2DKL(P , PB )DKL(PB , P^B) + DKL(PB , P^B)2

[ ][

]

= DKL(P , PB )2 + 2DKL(P , PB )E DKL(PB , P^B) + E DKL(PB , P^B)2 .

Since ^B(s) is an unbiased estimator of

[

E[DKL(PB

,

P^B

] )

=

E

 pB (x)

xS

loBg(spp^)BBf((oxxr))e]ve=ryxsSpSB,(ixt )hsoldxsEth[atB

(s)

-

] ^B (s)

=

0.

Hence we have [

]

[]

E DKL(P , P^B)2 = DKL(P , PB )2 + E DKL(PB , P^B)2 .

(13)

The [second term is ]

E DKL(PB , P^B)2

=

E

( 

xS

pB (x)

log

pB

(x)

)2

 

p^B (x)

=

E

 

xS


yS

pB (x)pB (y)

log

pB (x) p^B (x)

log

 pB (y)  p^B (y)







(

)(

)

= E

pB (x)pB (y) 

B (s) - ^B(s) - (B ) - (^B) 

xS yS

sB,sx

 

(

)(

)

 B (u) - ^B(u) - (B ) - (^B) 

uB,uy

6

Under review as a conference paper at ICLR 2018



 

( )(

)

= E

pB(x)pB (y)(s, x)(u, y) B (s) - ^B(s) B (u) - ^B(u) 

xS yS sB uB

-2

E

 


pB

(x)



( B

(s)

-

^B

) (s)

( (B

)

-

(^B

 ) )

+

E

[( (B

)

-

(^B

)2 )

]

xS

sB,sx

 

[( ) (

)]

= pB (x)pB (y)(s, x)(u, y)E B (s) - ^B(s) B (u) - ^B(u)

xS yS sB uB
 -2 pB (x)

(s,

x)E

[( B

(s)

-

^B

) (s)

( (B

)

-

(^B

)] )

+

E

[( (B

)

-

(^B

)2 )

] .

xS sB

Since (s) and () = -() are orthogonal for all s  S, that is, we have from Equation (5)

E

[ 

log p(s) (s)



] log p(s) ()

=



 (x,

s)p(s)

-

(x)

=

(x)

-

(x)

=

0,

sS

it follows that

[( ) (

)] [

]

E B (s) - ^B(s) (B ) - (^B) = E ^B(s)(^B) - B (s)(B )

E

[( (B )

-

(^B

)2] )

= =

B (s)(B ) - B (s)(B ) = 0, []
E (^B)2 - (B )2 = (B )2

-

(B )2

=

0.

Thus

[]

E DKL(PB , P^B)2

 

[( ) (

)]

= pB (x)pB(y)(s, x)(u, y)E B (s) - ^B(s) B (u) - ^B(u) .

xS yS sB uB

Using the Crame´r­Rao bound,

[( )( )] E B (s) - ^B(s) B (u) - ^B(u) 

1 N

 µ(w, s)µ(w, u)pB(w)-1,

wS

where the Fisher information is given in Equation (6), we obtain

[] E DKL(PB , P^B)2



1 N









pB(x)pB(y)(s, x)(u, y)µ(w, s)µ(w, u)pB(w)-1,

xS yS sB uB wS

where the equality holds if the sample size N  . Here we have

1 N







pB (x)pB(y)(s, x)(u, y)µ(w, s)µ(w, u)pB(w)-1

xS yS sB uB wS

=

1 N

 



pB (w)-1

µ(w, s)µ(w, u) (s, x)pB(x) (u, y)pB (y)

wS

sB uB

xS

yS

=

1 N

 

pB (w)-1

µ(w, s)µ(w, u)B (s)B (u)

wS

sB uB

=

1 N

( 

)2

pB (w)-1

µ(w, s)B (s)

wS

sB

Therefore, from Equation (13), it follows that

[]

[]

E DKL(P , P^B)2 = DKL(P , PB )2 + E DKL(PB , P^B)2

7

Under review as a conference paper at ICLR 2018



DKL(P , PB )2

+

1 N

( 

)2

pB (x)-1

µ(x, s)B (s)

xS

sB

= DKL(P , PB )2 + var(PB , B)

with the equality holding when N  .

Let B, B  S = 2V H such that B  B, that is, B has more parameters than B. Then it is trivial that the bias is always reduced, that is,

DKL(P , PB )  DKL(P , PB  )

because S(B)  S(B). However, this monotonicity does not always hold for the variance. We illustrate this non-monotonicity in the following example. Let S = 2V with V = {1, 2, 3} and assume that P  is the uniform distribution, i.e., p(x) = 1/8 for all x  S. Suppose that B = {{1}} and B = {{1}, {1, 2}, {1, 3}}. We have

( )2 (

)2

var(P , B) · N = p()-1 µ(, {1})({1}) + p({1})-1 µ({1}, {1})({1})

= 8(-1 · 1/2)2 + 8(1 · 1/2)2 = 4,
and
var(P , B) · N ( )2
= p()-1 µ(, {1})({1}) + µ(, {1, 2})({1, 2}) + µ(, {1, 3})({1, 3})
( )2 + p({1})-1 µ({1}, {1})({1}) + µ({1}, {1, 2})({1, 2}) + µ({1}, {1, 3})({1, 3})
( )2 ( )2 + p({1, 2})-1 µ({1, 2}, {1, 2})({1, 2}) + p({1, 3})-1 µ({1, 3}, {1, 3})({1, 3})

= 8(-(1/2) + (1/4) + (1/4))2 + 8((1/2) - (1/4) - (1/4))2 + 8(1 · (1/4))2 + 8(1 · (1/4))2
= 0 + 0 + 1/2 + 1/2 = 1.
Thus the variance decreases when we include more parameters in B. This interesting property, non-monotonicity of the variance with respect to the growth of parameter sets, comes from the hierarchical structure of S realized by the mo¨bius function µ.

4 CONCLUSION
In this paper, we have firstly achieved bias-variance decomposition of the KL divergence for Boltzmann machines using the information geometric formulation of hierarchical probability distributions. Our model includes various types of Boltzmann machines such as the traditional Boltzmann machines, generalized Boltzmann machines with arbitrary order interactions, and Boltzmann machines hidden nodes such as RBMs and DBMs. Our bias-variance decomposition reveals the nonmonotonicity of the variance with respect to increase of the number of parameters. This result indicates that it is possible to reduce both bias and variance when we include more parameters in the deep learning architectures. To solve the open problem of the generalizability of the deep learning architectures, our finding can be fundamental for further theoretical development.

REFERENCES
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive science, 9(1):147­169, 1985.
S. Amari. Information geometry on hierarchy of probability distributions. IEEE Transactions on Information Theory, 47(5):1701­1711, 2001.
S. Amari. Information Geometry and Its Applications. Springer, 2016.

8

Under review as a conference paper at ICLR 2018
S. Amari, K. Kurata, and H. Nagaoka. Information geometry of Boltzmann machines. IEEE Transactions on Neural Networks, 3(2):260­271, 1992.
L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning, pp. 1019­1028, 2017.
G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, and D. S. Scott. Continuous Lattices and Domains. Cambridge University Press, 2003.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771­1800, 2002.
G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527­1554, 2006.
K. Ito (ed.). Encyclopedic Dictionary of Mathematics. The MIT Press, 2 edition, 1993.
K. Kawaguchi, L. P. Kaelbling, and Y. Bengio. Generalization in deep learning. arXiv:1710.05468, 2017.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In Proceedings of 5th International Conference on Learning Representations, 2017.
N. Le Roux and Y. Bengio. Representational power of restricted Boltzmann machines and deep belief networks. Neural computation, 20(6):1631­1649, 2008.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436­444, 2015.
M. R. Min, X. Ning, C. Cheng, and M. Gerstein. Interpretable sparse high-order Boltzmann machines. In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, pp. 614­622, 2014.
H. Nakahara and S. Amari. Information-geometric measure for neural spikes. Neural Computation, 14(10):2269­2316, 2002.
H. Nakahara, S. Amari, and B. J. Richmond. A comparison of descriptive models of a single spike train by information-geometric measure. Neural computation, 18(3):545­568, 2006.
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems 30, 2017.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics, pp. 448­455, 2009.
R. Salakhutdinov and G. E. Hinton. An efficient learning procedure for deep Boltzmann machines. Neural Computation, 24(8):1967­2006, 2012.
T. J. Sejnowski. Higher-order Boltzmann machines. In AIP Conference Proceedings, volume 151, pp. 398­403, 1986.
P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart, J. L. McClelland, and PDP Research Group (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1, pp. 194­281. MIT Press, 1986.
M. Sugiyama, H. Nakahara, and K. Tsuda. Information decomposition on structured space. In IEEE International Symposium on Information Theory, pp. 575­579, 2016.
M. Sugiyama, H. Nakahara, and K. Tsuda. Tensor balancing on statistical manifold. In Proceedings of the 34th International Conference on Machine Learning, pp. 3270­3279, 2017.
L. Wu, Z. Zhu, and W. E. Towards understanding generalization of deep learning: Perspective of loss landscapes. In ICML 2017 Workshop on Principled Approaches to Deep Learning, 2017.
9

