Under review as a conference paper at ICLR 2018

NETWORK OF GRAPH CONVOLUTIONAL NETWORKS TRAINED ON RANDOM WALKS
Anonymous authors Paper under double-blind review

ABSTRACT
Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs). At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.

1 INTRODUCTION

Semi-supervised learning on graphs is important in many real-world applications, where the goal is to recover labels for all nodes given only a fraction of labeled ones. Significant amounts of data in diverse fields can be naturally represented as graphs with discrete node relationships. Some applications include social networks, where one wishes to predict user interests, or in health care, where one wishes to predict whether a patient should be screened for cancer. In many such cases, collecting node labels can be prohibitive. However, edges between nodes can be easier to obtain, either using an explicit graph (e.g. social network) or implicitly by calculating pairwise similarities (e.g. using a patient-patient similarity kernel Merdan et al., 2017).

Convolutional Neural Networks (LeCun et al., 1998) learn location-invariant hierarchical filters, enabling significant improvements on Computer Vision tasks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016). This success has motivated researchers (Bruna et al., 2014) to extend convolutions from spatial (i.e. regular lattice) domains to graph-structured (i.e. irregular) domains, yielding a class of algorithms known as Graph Convolutional Networks (GCNs).

Formally, we are interested in semi-supervised learning where we are given a graph G = (V, E) with N = |V| nodes; adjacency matrix A; and matrix X  RN×F of node features. Labels for only a subset of nodes VL  V observed. In general, |VL| |V|. Our goal is to recover labels for all unlabeled nodes VU = V - VL, using the feature matrix X, the known labels for nodes in VL, and the graph G. In this setting, one treats the graph as the "unsupervised" and labels of VL as the
"supervised" portions of the data.

Depicted in Figure 1, our model for semi-supervised node classification builds on the GCN mod-

ule proposed by Kipf & Welling (2017), which operates on the normalized adjacency matrix A^,

as in GCN(A^), where A^ =

D-

1 2

AD-

1 2

,

and

D

is

diagonal

matrix

of

node

degrees.

Our pro-

posed extension of GCNs is inspired by the recent advancements in random walk based graph

embeddings (e.g. Perozzi et al., 2014; Grover & Leskovec, 2016; Abu-El-Haija et al., 2017). We

make a Network of GCN modules (NGCN), feeding each module a different power of A^, as in

{GCN(A^0), GCN(A^1), GCN(A^2), . . . }. The k'th power contains statistics from the k-th step of a

random walk on the graph. Therefore, our NGCN model is able to combine information from var-

ious step-sizes. We then combine the output of all GCN modules into a classification sub-network,

which for this work consists of either fully-connected layers or a softmax attention vector. We use

the trained weights of the classification sub-network to better understand how the NGCN model

works and to gain further insight on the underlying graph. For instance, in the presence of input per-

1

Under review as a conference paper at ICLR 2018

A^ · · . . .

I · × × ...

X

GCGNCGNCN

GCGNCGNCN

GCGNCGNCN

...

N ... r C0 C1 C2

K-1 k=0

rCk

concatenate

N

· × GCGNCGNCN
CK-1

fully-connected

N
C
(a) NGCN Architecture

(b) t-SNE visualization of fully-connected hidden layer of NGCN when trained over Cora graph.

Figure 1: Right: Model architecture, where A^ is the normalized normalized adjacency matrix, I is the identity matrix, X is node features matrix, and × is matrix-matrix multiply operator. We calculate K powers of the A^, feeding power into r GCNs, along with X. The output of all K × r GCNs
can be concatenated along the column dimension, then fed into fully-connected layers, outputting C channels per node, where C is size of label space. We calculate cross entropy error, between rows prediction N × C with known labels, and use them to update parameters of classification sub-
network and all GCNs. Left: pre-relu activations of the last fully-connected hidden layer, PCA-ed
down to 50 dimensions, visualized using t-SNE.

turbations, we observe that the weights shift towards GCN modules utilizing higher powers of the adjacency matrix, effectively widening the "receptive field" of the (spectral) convolutional filters. We achieve state-of-the-art on several semi-supervised graph learning tasks, showing that explicit random walks enhance the representational power of vanilla GCN's.
The rest of this paper is organized as follows. Section 2 reviews background work that provides the foundation for this paper. In Section 3, we describe our proposed method, followed by experimental evaluation in Section 4. We compare our work with recent closely-related methods in Section 5. Finally, we conclude with our contributions and future work in Section 6.

2 BACKGROUND

2.1 SEMI-SUPERVISED GRAPH LEARNING

Traditional label propagation algorithms (Weston et al., 2012; Belkin et al., 2006a) learn a model that transforms node features into node labels and uses the graph to add a regularizer term:

Llabel.propagation = Lclassification + Lreg = Lclassification + f (X)T f (X),

(1)

where f : RN×d0  RN×C is the model,  is the graph Laplacian, and   R is regularization coefficient hyperparameter.

2.2 GRAPH CONVOLUTIONAL NETWORKS
Graph Convolution (Bruna et al., 2014) generalizes convolution from Euclidean domains to graphstructured data. Convolving a "filter" over a signal on graph nodes can be calculated by transforming both the filter and the signal to the Fourier domain, multiplying them, and then transforming the result back into the discrete domain. The signal transform is achieved by multiplying with the eigenvectors of the graph Laplacian. The transformation requires a quadratic eigendecomposition of the symmetric Laplacian; however, it is possible to approximate the decomposition by using truncated Chebyshev polynomials (Hammond et al., 2011). Kipf & Welling (2017) propose using multi-layer Graph Convolutional Networks (GCNs) for semi-supervised graph learning. Every layer

2

Under review as a conference paper at ICLR 2018

computes the transformation:

H(l+1) =  A^H(l)W (l) ,

(2)

where H(l)  RN×dl is the input activation matrix to the l-th hidden layer with row Hi(l) containing a dl-dimensional feature vector for vertex i  V, and W (l)  Rdl×dl+1 is the layer's trainable

weights. The first hidden layer H(0) is set to the input features X. A softmax on the last layer is

used to classify labels. All layers use the same "normalized adjacency" A^, obtained by the "renor-

malization

trick"

introduced

by

Kipf

&

Welling

(2017).

Specifically,

A^

=

D-

1 2

AD

-

1 2

.1

Eq. (2) is a first order approximation of convolving filter W (l) over signal H(l) (Hammond et al., 2011; Kipf & Welling, 2017). The left-multiplication with A^ averages node features with their direct neighbors; this signal is then passed through a non-linearity function (·) (e.g, ReLU(z) =
max(0, z)). Successive layers effectively diffuse signals from nodes to neighbors.

Thus, a two-layer GCN model can be defined in terms of vertex features X and normalized adjacency A^ as:

GCN2-layer(A^, X; ) = softmax A^(A^XW (0))W (1) ,

(3)

where the GCN parameters  = W (0), W (1) are trained to minimize the cross-entropy error over labeled examples. The output of the GCN model is a matrix RN×C , where N is the number of nodes and C is the number of labels. Each row contains the label scores for one node, assuming there are C classes.

2.3 GRAPH EMBEDDINGS

Node Embedding methods represent graph nodes in a continuous vector space. They learn a dictionary Z  RN×d, with one d-dimensional embedding per node. Traditional methods use the adjacency matrix to learn embeddings. For example, Eigenmaps (Belkin & Niyogi, 2003) calculates the following constrained optimization:

||Aij(Zi - ZJ )|| s.t. ZT DZ = I,
i,j

(4)

where I is identity vector. Skipgram models on text corpora (Mikolov et al., 2013) inspired modern graph embedding methods, which simulate random walks to learn node embeddings (Perozzi et al., 2014; Grover & Leskovec, 2016). Each random walk generates a sequence of nodes. Sequences are converted to textual paragraphs, and are passed to a word2vec-style embedding learning algorithm (Mikolov et al., 2013). As shown in Abu-El-Haija et al. (2017), this learning-by-simulation is equivalent, in expectation, to the decomposition of a random walk co-occurrence statistics matrix D. The expectation on D can be written as:

E[D]  EqQ [(T )q] = EqQ D-1A q ,

(5)

where T = D-1A is the row-normalized transition matrix (a.k.a right-stochastic adjacency matrix), and Q is a "context distribution" that is determined by random walk hyperparameters, such as the length of the random walk. The expectation therefore weights the importance of one node on another as a function of the distance between them. The main difference between traditional node embedding methods and random walk methods is the optimization criteria: the former minimizes a loss on representing the adjacency matrix A (see Eq. 4), while the latter minimizes a loss on representing random walk co-occurrence statistics D.

3 OUR METHOD
3.1 MOTIVATION
Graph Convolutional Networks and random walk graph embeddings are individually powerful. Kipf & Welling (2017) uses GCNs for semi-supervised node classification. Instead of following traditional methods that use the graph for regularization (e.g. Eq. 4), Kipf & Welling (2017) use the
1with added self-connections added like Aii = 1, similar to Kipf & Welling (2017)

3

Under review as a conference paper at ICLR 2018

adjacency matrix for training and inference, effectively diffusing information across edges at all GCN layers (see Eq. 6). Separately, recent work has showed that random walk statistics can be very powerful for learning an unsupervised representation of nodes that can preserve the structure of the graph (Perozzi et al., 2014; Grover & Leskovec, 2016; Abu-El-Haija et al., 2017).

Under special conditions, it is possible for the GCN model to learn random walks. In particular, consider a two-layer GCN defined in Eq. 6 with the assumption that first-layer activation is identity as (z) = z, and weight W (0) is an identity matrix (either explicitly set or learned to satisfy the upstream objective). Under these two identity conditions, the model reduces to:

GCN2-layer-special(A^, X) = softmax A^A^XW (1) = softmax A^2XW (1) ,

(6)

where A^2 can be expanded as:

A^2 =

D-

1 2

AD-

1 2

D

-

1 2

AD-

1 2

=

D-

1 2

A

D-1A

D-

1 2

=

D-

1 2

AT

D-

1 2

.

(7)

By multiplying the adjacency A with the transition matrix T before normalization, the GCN is effectively doing a one-step random walk.

3.2 EXPLICIT RANDOM WALKS

The special conditions described above are not true in practice. Although stacking hidden GCN
layers allows information to flow through graph edges, this flow is indirect as the information goes through feature reduction (matrix multiplication) and a non-linearity (activation function (·)). Therefore, the vanilla GCN cannot directly learn high powers of A^, and could struggle with mod-
eling information across distant nodes. We hypothesize that making the GCN directly operate on
random walk statistics will allow the network to better utilize information across distant nodes, in the same way that node embedding methods (e.g. DeepWalk, Perozzi et al. (2014)) operating on D
are superior to traditional embedding methods operating on the adjacency matrix (e.g. Eigenmaps, Belkin & Niyogi (2003)). Therefore, rather than feeding only A^ to the GCN model as proposed by Kipf & Welling (2017) (see Eq. 6), we feed a K-degree polynomial of A^ to K instantiations of
GCN. Generalizing Eq. (7) gives:

A^k

=

D

-

1 2

AT

k-1

D

-

1 2

.

(8)

We also define A^0 to be the identity matrix. Note that A^k is symmetric if and only if A^ is symmet-
ric. Similar to Kipf & Welling (2017), we convert directed graphs to undirected ones. Therefore the eigendecomposition of A^k is real, and the approximations derived by Kipf & Welling (2017); Hammond et al. (2011) are still valid under symmetric A^k.

3.3 NETWORK OF GCNS
Consider K instantiations of {GCN(A^0, X), GCN(A^1, X), . . . , GCN(A^K-1, X)}. Each GCN outputs a matrix RN×Ck , where the v-th row describes a latent representation of that particular GCN for node v  V, and where Ck is the latent dimensionality. Though Ck can be different for each GCN, we set all Ck to be the same for simplicity. We then combine the output of all K GCN and feed them into a classification sub-network, allowing us to jointly train all GCNs and the classification sub-network via backpropagation. This should allow the classification sub-network to choose features from the various GCNs, effectively allowing the overall model to learn a combination of features using the raw (normalized) adjacency, different steps of random walks, and the input features X (as they are multiplied by identity A^0).

3.3.1 FULLY-CONNECTED CLASSIFICATION NETWORK
From a deep learning prospective, it is straight-forward to represent the classification network as a fully-connected layer. We can concatenate the output of the K GCNs along the column dimension, i.e. concatenating all GCN(X, A^k), each  RN×Ck into matrix  RN×CK where CK = k Ck. We add a fully-connected layer ffc : RN×CK  RN×C , with trainable parameter matrix Wfc  RCK×C , written as:
NGCNfc(A^, A; Wfc, ) = softmax GCN(A^0, X; (0)) GCN(A^1, X; (1)) . . . Wfc . (9)

4

Under review as a conference paper at ICLR 2018

The classifier parameters Wfc are jointly trained with GCN parameters  = {(0), (1), . . . }.

3.3.2 ATTENTION CLASSIFICATION NETWORK

We also propose a classification networks based on "softmax attention", in which the classification network learns a convex combination of the GCN instantiations. Our attention model (NGCNa) is parametrized by vector m  RK, one scalar for each GCN. It can be written as:

NGCNa(A^, X; m, ) = mkGCN(A^k, X; (k))
k

(10)

where m is output of a softmax: m = softmax(m).

This softmax attention is similar to "Mixture of Experts" model, especially if we set the number of

output channels for all GCNs equal to the number of classes, as in C0 = C1 = · · · = C. This

allows us to add cross entropy loss terms on all GCN outputs in addition to the loss applied at the

output NGCN, forcing all GCN's to be independently useful. It is possible to set the m  RK

parameter vector "by hand" using the validation split, especially for reasonable K such as K  6.

One possible choice might be setting m0 to some small value and remaining m1, . . . , mK-1 to the

harmonic

series

1 k

;

another

choice

may

be

linear

decay

K -k K -1

.

These

are

respectively

similar

to

the

context distributions of GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013; Levy

et al., 2015). We note that if on average a node's information is captured by its direct or nearby

neighbors, then the output of GCNs consuming lower powers of A^ should be weighted highly.

3.4 TRAINING

We minimize the cross entropy between our model output and the known training labels Y as:

min diag(VL) Y  log NGCN(X, A^) ,

(11)

where  is Hadamard product, and diag(VL) denotes a diagonal matrix, with entry at (i, i) set to 1 if i  VL and 0 otherwise. In addition, we can apply intermediate supervision for the NGCNa to
attempt make all GCN become independently useful, yielding minimization objective:

min diag(VL) Y  log NGCNa(A^, X; m, ) + Y  log GCN(A^k, X; (k)) .
m, k

(12)

3.5 GCN REPLICATION
To simplify notation, our NGCN derivations (e.g. Eq. 9) assumes that there is one GCN per A^ power. However, our implementation feeds every A^ to r GCN modules, as shown in Fig. 1.

4 EXPERIMENTS
We follow the experimental setup by Kipf & Welling (2017) and Yang et al. (2016), including the provided dataset splits (train, validattion, test) produced by Yang et al. (2016).
4.1 DATASETS
We experiment on three citation datasets: Pubmed, Citeseer, and Cora. Their statistics are summarized in table 1. In each dataset, a node represents an article published in the corresponding journal; an edge between two nodes represents a citation from one article to another; and a label represents the category or subject of the article. Each dataset contains a binary Bag-of-Words (BoW) feature vector for each node. The BoW are extracted from the article abstract. Therefore, the task is to predict the subject of articles, given the BoW of their abstract the citations to other (possibly labeled) articles. In order to calculate adjacency matrix, we follow Kipf & Welling (2017) and convert directed graphs to undirected ones. We choose the aforementioned datasets because the (train, validation, test) splits used by Yang et al. (2016) are available online and our baseline method, (GCN,

5

Under review as a conference paper at ICLR 2018

Dataset Nodes Edges Classes Features Labeled nodes

|V| |E|

C

F

|VL|

Citeseer 3,327 4,732 Cora 2,708 5,429 Pubmed 19,717 44,338

6 7 3

3,703 1,433 500

120 140 60

Table 1: Dataset statistics used for experiments. The number of nodes with observed labels is small (|VL| = 20 × C)

Kipf & Welling, 2017), uses the same splits. Following Kipf & Welling (2017), we use 500 nodes for validation for each dataset. We note that the validation set is larger than training |VL| for all datasets!
4.2 BASELINE METHODS
We compare against the baseline methods used by Kipf & Welling (2017). These include label propagation (LP, Zhu et al. (2003)); semi-supervised embedding (SemiEmb, Weston et al. (2012)); manifold regularization (ManiReg, Belkin et al. (2006b)); skip-gram graph embeddings (DeepWalk, Perozzi et al. (2014)), iterative classification algorithms (ICA, Lu & Getoor (2003)); and Planetoid (Yang et al., 2016). In addition, we compare against an optimized two layer GCN (Kipf & Welling (2017)), which is equivalent to our model if we only use A^1 via NGCNa.K. The accuracy numbers from all baseline methods are copied from Kipf & Welling (2017).
4.3 IMPLEMENTATION
In practice, we implement our NGCN using TensorFlow (et al, 2015). We make use of the Adam optimizer (Ba & Kingma, 2015) with a learning rate of 0.01. We use a two-layer GCN with hidden dimensionality of 16 (same as Kipf & Welling (2017)). We use 50% dropout on all GCN layers and L2 regularization of 10-5. We experiment with replication factors r = 1 and r = 4 and K = 6 (i.e. using A^0, A^1, . . . , A^5).
We implement early stopping as follows. If the validation accuracy does not improve over 10 steps, we halt training. We save the model parameters at peak validation accuracy. We invoke many runs over all datasets, each with random initialization of parameters. We rank the models using validation accuracy, we choose the top 3 performers, then report their metrics (mean and standard deviation) on the test set.
4.4 NODE CLASSIFICATION ACCURACY
We summarize results in Table 2. Our NGCN methods outperforms GCN (Kipf & Welling, 2017) and all prior baselines. Our experimental results indicate that there is useful information in the random walk statistics that help in semi-supervised learning.
4.5 CALIBRATION
The majority of NGCN parameters live in the GCN modules, and only a handful live in the fullyconnected layer (e.g. for K = 6, r = 1, NGCNa.K contains 6 parameters). Since the size of training data is small (|V| = 20 × C), all models overfit and quickly reach  100% classification accuracy on the training set. Therefore, model selection on the validation set is important.
One can set the K parameters of "by hand" on the validation partition, and select the best performing model. For example, fixing attention weights to harmonic series, does better than "training them" end-to-end, on validation and test. To save time, we use an analytic approach similar to what has been recently proposed by Guo et al. (2017). In particular, we calibrate the classification subnetwork parameters (e.g. m  R6 for NGCNa.K) by using gradient descent to minimize the cross entropy loss on a subset of validation nodes. We use 120 nodes of the validation set to perform this calibration.
6

Under review as a conference paper at ICLR 2018

Method

Pubmed

Citeseer

Cora

ManiReg (Belkin et al., 2006b) SemiEmb (Weston et al., 2012) LP (Zhu et al., 2003) DeepWalk (Perozzi et al., 2014) ICA (Lu & Getoor, 2003) Planetoid (Yang et al., 2016) GCN (Kipf & Welling, 2017)
GCN (our implementation) NGCNa (ours) NGCNfc (ours)
NGCNa (ours, calibrated) NGCNfc (ours, calibrated)

70.7 71.1 63.0 65.3 73.9 77.2 79.0
79.2 ± 0.35 79.5 ± 0.30 79.8 ± 0.80
79.6 ± 0.20 80.9 ± 0.35

60.1 59.6 45.3 43.2 69.1 64.7 70.3
71.5 ± 0.70 71.9 ± 0.05 71.4 ± 0.35
71.6 ± 0.10 74.3 ± 0.35

59.5 59.0 68.0 67.2 75.1 75.7 81.5
81.2 ± 0.65 82.4 ± 0.30 82.1 ± 0.30
82.8 ± 0.20 84.5 ± 0.30

Table 2: Node classification accuracy (in %) for various graph datasets. Results above the first double-line are copied from Kipf & Welling (2017), and below are produced using our implementation. Below the second double line uses Calibration (Section 4.5). One might think it cannot be directly compared to baselines, however we believe it is a smart way of doing model selection than "by hand".

Features Removed

Train Nodes per class

10% 30% 50% 70% 90% 20 10

5

GCN 81.2 79.5 75.8 70.9 59.4
NGCNa 81.5 80.3 78.3 74.8 69.1 NGCNa (calibrated) 81.8 80.4 78.8 75.4 70.6

81.4 73.2 82.4 73.3 82.8 74.0

61.2 61.4 64.8

Table 3: Node classification accuracy (in %) for the Cora dataset with some percentage of features removed (left) and scarce number of training examples (right). For each column, we generate 5 graphs and use them for each algorithm. We ran each algorithm only once (i.e. one random initialization) on each of the 5 graphs.

This analytical approach is more effective than setting the parameters "by hand" through grid-search. We clearly mark these models as calibrated in all result tables.
4.6 TOLERANCE TO FEATURE NOISE
We test our method under feature noise perturbations by removing node features at random. This is practical, as article authors might forget to include important terms in their abstract, and more generally not all nodes will have the same amount of detailed information. Table 3 shows that our NGCNa model outperforms vanilla GCN when features are removed, and that the performance gap widens as we remove more features. This suggests that our method can somewhat recover removed features by directly pulling-in features from nearby and distant neighbors, according to the attention vector m. We visualize in Figure 2 the attention weights as a function of % features removed. With little feature removal, there is some weight on A^0, and the attention weights for A^1, A^2, . . . follow some decay function. Maliciously dropping features causes our model to shift its attention weights towards higher powers of A^.
5 RELATED WORK
The field of graph learning algorithms is quickly evolving. This section focuses on work that is most similar to ours.
7

Under review as a conference paper at ICLR 2018

mi mi

Calibrated = no

Feats Removed = 10% 0.4 0.3 0.2 0.1 0.0 0.4 0.3 0.2 0.1 0.0
0 1 2i3 4 5

Feats Removed = 30%
0 1 2i3 4 5

Feats Removed = 50%
0 1 2i3 4 5

Feats Removed = 70%
0 1 2i3 4 5

Feats Removed = 90%
0 1 2i3 4 5

Calibrated = yes

Figure 2: Attention weights (m) for NGCNa when trained with feature removal perturbation, with (bottom) and without (top) calibration. Removing features shifts the attention weights to the right,
suggesting the model is relying on long range dependencies.

Defferrard et al. (2016) define graph convolutions as a K-degree polynomial of the Laplacian, where the polynomial coefficients are learned. In their setup, the K-th degree Laplacian is a sparse square matrix where entry at (i, j) will be zero if nodes i and j are more than K hops apart. This sparsity is like ours. A minor difference between our work is the matrix: we use the normalized adjacency A^ whereas they use the Laplacian defined as I - A^, where I is identity matrix. Raising A^ to power K will produce a square matrix with entry (i, j) being the probability of random walker ending at node i after K steps from node j. The major difference is the order of random walk versus non-linearity. In particular, their model calculates f ( k qkAk), while our model calculates k qkf (Ak), where A is A^ in our model and I - A^ in theirs, and our f can be the NGCNa.K attention model. In fact, Defferrard et al. (2016) is also similar to work by Abu-El-Haija et al. (2017), as they both learn polynomial coefficients to some normalized adjacency matrix.
Atwood & Towsley (2016) calculate powers of the transition matrix and keep each power "in a separate channel" until the classification sub-network at the end. Their model is therefore similar to our work in that it also falls under k qkf (Ak). However, where their model multiplies features with each power Ak once, our model makes use of GCN's (Kipf & Welling, 2017) that multiply by Ak at every GCN layer (see Eq. 2). Thus, our model generalizes Atwood & Towsley (2016); in the special case that all GCNs in our NGCN are a single layer, our model becomes equivalent to theirs.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we extend Graph Convolutional Networks (GCNs) with random walk methods. Traditional GCNs operate on the normalized adjacency matrix. We make multiple instantiations of GCNs, feeding each a power of the adjacency matrix, and then feed the total output into a classification sub-network. Our model, Network of GCNs, is end-to-end trainable, and is able to directly learn information across near or distant neighbors. We inspect the distribution of parameter weights in our classification sub-network, and show that our model is effectively able to circumvent adverserial perturbations on the input by shifting weights towards GCNs consuming higher powers of the adjacency matrix. For future work, we plan to extend our methods to a stochastic implementation and tackle other (larger) graph datasets. We also plan on using our implementation to gain valuable insight in real world graph learning tasks.
ACKNOWLEDGMENTS
We would like to thank meaningful discussions with anonymous researchers which had a significant impact on the quality of our work.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alex Alemi. Watch your step: Learning graph embeddings through attention. In arxiv, 2017.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2016.
Jimmy Ba and Diederik Kingma. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In Neural Computation, 2003.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. In Journal of machine learning research (JMLR), 2006a.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. In Journal of machine learning research (JMLR), 2006b.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations, 2014.
Michae¨l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NIPS), 2016.
Mart´in Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015.
A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning (ICML), 2017.
David K. Hammond, Pierre Vandergheynst, and R. Gribonval. Wavelets on graphs via spectral graph theory. In Applied and Computational Harmonic Analysis, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
T. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, 1998.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. In Transactions of the Association for Computational Linguistics (TACL), 2015.
Qing Lu and Lise Getoor. Link-based classification. In International Conference on Machine Learning (ICML), 2003.
Selin Merdan, Christine L. Barnett, and Brian T. Denton. Data analytics for optimal detection of metastatic prostate cancer. 2017.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, 2013.
9

Under review as a conference paper at ICLR 2018
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014.
B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Knowledge Discovery and Data Mining, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deeplearning via semisupervised embedding. In Neural Networks: Tricks of the Trade, pp. 639­655, 2012.
Z. Yang, W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning (ICML), 2016.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In International Conference on Machine Learning (ICML), 2003.
10

