Under review as a conference paper at ICLR 2018
MAINTAINING COOPERATION IN COMPLEX SOCIAL
DILEMMAS USING DEEP REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.
1 INTRODUCTION
Constructing agents that can achieve good outcomes in complex multi-agent environments is an important component of modern AI research. A common way to construct agents for such environments is deep reinforcement learning combined with `self play.' In self play agents learn how to behave by simulating the game repeatedly while controlling all the players and iteratively improving their strategies.1 When faced with a real partner, the agent uses the strategies it has learned at training time.
In practice self-play methods have led to very strong performance in games such as Backgammon (Tesauro, 1995), Go (Silver et al., 2016; 2017), Poker (Brown et al., 2015), and video games (Kempka et al., 2016; Wu & Tian, 2016; Usunier et al., 2016). Self-play is very powerful because if it converges then it converges to an equilibrium of the game (Fudenberg & Levine, 1998) and in two player zero-sum games every equilibrium strategy yields at least the minimax payoff (Neumann, 1928). Thus, self-play is a general way to compute a good strategy in any two player zero-sum game. Here we ask how to go beyond zero-sum games and design agents that can achieve good outcomes in social dilemmas.2
In general-sum games, if we apply the self-play procedure of computing some equilibrium at training time and then playing our `half' of it at test time we no longer have good guarantees (Fudenberg & Levine, 1998; Sandholm & Crites, 1996). In particular, in social dilemmas constant mutual exploitation achieves bad outcomes yet is an equilibrium outcome. By contrast, mutual unconditional
1Early game theorists conceptualized a similar strategy. In particular Brown (1951) proposed that to decide how to behave in a game players could engage in mental `fictitious play' of the game and in that way discover a good strategy.
2There has been recent interest in non zero-sum games in the deep RL community. A growing body of research focuses on coordination games (Lowe et al., 2017; Foerster et al., 2017a; Riedmiller et al., 2009; Tampuu et al., 2017; Peysakhovich & Lerer, 2017) of which language emergence games are an important special case (Lazaridou et al., 2017; Das et al., 2017; Evtimova et al., 2017; Havrylov & Titov, 2017; Jorge et al., 2016). More closely related to our study Leibo et al. (2017); Perolat et al. (2017) study standard RL agents in Markov social dilemmas but are primarily concerned with describing how incentives affect the emergence of cooperative or non-cooperative behavior. Kleiman-Weiner et al. (2016) use related RL methods to model human inference of the cooperative (or non-cooperative) intentions of a partner. Lewis et al. (2017) apply deep RL to bargaining which can be thought of as an imperfect information general-sum game.
1

Under review as a conference paper at ICLR 2018
cooperation achieves good outcomes but is not an equilibrium because it leaves the agent open to exploitation.3
The repeated Prisoner's Dilemma is perhaps the most studied social dilemma. It is well known that strategies such as Tit-for-Tat (Axelrod, 2006) or Win-Stay-Lose-Shift (Nowak & Sigmund, 1993) perform well in this environment.4 These strategies are studied so heavily because they have intuitively appealing properties. They are nice (begin by cooperating), are simple to explain to a partner, cooperate with cooperators, do not get exploited by defectors, are forgiving (eventually return to cooperation if it breaks down), and, importantly, if one can commit to them, create incentives for a partner to behave cooperatively. Because one of our main goals is to preserve these properties we call our strategy approximate Markov tit-for-tat (amTFT).
Expanding such strategies to Markov social dilemmas is non-trivial because `cooperation' and `defection' are no longer single acts, but rather sequences of choices. Existing work has studied finding cooperative equilibria in repeated games (Littman & Stone, 2005; De Cote & Littman, 2012). The constructions in this literature find cooperative policies and punish a partner's deviations from them by breaking away from cooperation in the next iteration of the game. This methodology cannot be directly applied to our problem. First, our agents only live once at at test time and must maintain cooperation by behaving intelligently within the confines of a single game rather than threats across games. Second, in many cases of interest there can be many value equivalent sequences of actions (eg. moving left and then down or down and then left to reach a goal in a grid world) as well as noise in implementation. In these situations we will show that using the deviation-based strategies above will quickly lead to a breakdown in cooperation even against a well-meaning partner.
amTFT addresses both of these issues by using modified self-play to learn two policies at training time: a fully cooperative policy and a `safe' policy (we refer to this as defection)5 which forms an equilibrium with lower payoffs than cooperation. An advantage of amTFT is that it requires no additional machinery beyond what is required by standard self-play, thus if we can construct competitive agents in some environment (eg. Atari, Mnih et al. (2015)) then we can also construct agents that solve social dilemmas in that environment.
At test time, the amTFT agent is matched with a partner whose policy is unknown. At each time step the amTFT agent computes the gain from the action their partner actually chose compared to the one prescribed by the cooperative policy. This can be done either using a learned Q function or via policy rollouts. We refer to this as a per period debit. If the total debit is below a threshold the agent behaves according to the cooperative policy. If the debit at some time period is above the threshold, the agent switches to the defecting policy for k turns and then returns to cooperation. This k is computed such that the partner's gains (debit) are smaller than the losses they incur (k lost turns of cooperation).
We show analytically that such a construction cooperates with cooperators, is not exploited by defectors, and incentivizes cooperation from the rest all within the confines of a single test game. In addition, because a partner's `defection' is computed in terms of value and not actions it is robust to a partner using one of a class of outcome-equivalent cooperative policies or to function approximation on the part of the amTFT agent. We show the effectiveness of amTFT in two Markov games: a grid-world, Coins, and a modification of an Atari game where players must learn from pixels, the Pong Player's Dilemma.
3Because it is not an equilibrium, standard self-play will not converge to these strategies. However, there exist modified learning procedures which will find cooperative strategies (eg. policy gradient using the joint reward).
4In the repeated 2 action Prisoner's Dilemma standard tit-for-tat (TFT) works by copying the prior behavior of their partner. If a partner cooperated last period, TFT cooperates, otherwise it defects. Win-Stay-Lose-Shift cooperates after both have cooperated or both have defected and defects otherwise. Both of these strategies create consequences for defection today and thus stabilize cooperation.
5We note that in the PD this action is `defect' but in most real social dilemmas this is not the case. For example social dilemmas occur naturally in economic situations where agents face a choice of trading and realizing gains from trade or simply producing everything they need on their own. In this case a safe policy is the outside option of `stop transacting with this agent.'
2

Under review as a conference paper at ICLR 2018
2 THE BASIC MODEL
We now turn to formalizing our main idea. We will work with a generalization of Markov decision problems:
Definition 1 (Shapley (1953)) A (finite, 2-player) Markov game consists of a set of states S = {s1, . . . , sn}; a set of actions for each player A1 = {a11, . . . , ak1}, A2 = {a21, . . . , ak2}; a transition function  : S × A1 × A2  (S) which tells us the probability distribution on the next state as a function of current state and actions; a reward function for each player Ri : S × A1 × A2  R which tells us the utility that player gains from a state, action tuple. We assume rewards are bounded.
Players can choose between policies which are maps from states to probability distributions on actions i : S  (Ai). We denote by i the set of all policies for a player.
Definition 2 A value function for a player i inputs a state and a pair of policies V i(s, 1, 2) and gives the expected discounted reward to that player from starting in state s. We assume agents discount the future with rate  which we subsume into the value function. A related object is the Q function for a player i inputs a state, action, and a pair of policies Qi(s, 1, 2) and gives the expected discounted reward to that player from starting in state s taking action a and then continuing according to 1, 2 afterwards.
We will be talking about strategic agents so we often refer to the concept of a best response:
Definition 3 A policy for agent j denoted j is a best response starting at state s to a policy i if for any j and any s along the trajectory generated by these policies we have V j(s , i, j)  V j(s , i, j). We denote the set of such best responses as BRj(i, s). If j obeys the inequality above for any choice of state s we call it a perfect best response.
The set of stable states in a game is the set of equilibria. We call a policy for player 1 and a policy for player 2 a Nash equilibrium if they are best responses to each other. We call them a Markov perfect equilibrium if they are perfect best responses.
We are interested in a special set of policies:
Definition 4 Cooperative Markov policies starting from state s (1C , 2C ) are those which, starting from state s, maximize V 1(s, 1, 2) + V 2(s, 1, 2). We let the set of cooperative policies be denoted by iC(c). Let the set of policies which are cooperative from any state be the set of perfectly cooperative policies.
A social dilemma is a game where there are no cooperative policies which form equilibria. In other words, if one player commits to play a cooperative policy at every state, there is a way for their partner to exploit them and earn higher rewards at their expense. Note that in a social dilemma there may be policies which achieve the payoffs of cooperative policies because they cooperate on the trajectory of play and prevent exploitation by threatening non-cooperation on states which are never reached by the trajectory. If such policies exist, we call the social dilemma solvable.
The state representation used plays an important role in determining whether policies that solve the social dilemma exist. Specifically, a policy which rewards cooperation today with cooperation tomorrow must be able to remember whether cooperation happened yesterday. In both of our example games, Coins and the PPD, if the game is played from the pixels without memory maintaining cooperation is impossible. This is because the current state does not contain information about past behavior of one's partner.
Thus, some memory is required to create policies which maintain cooperation. This memory can be learned (eg. an RNN) or it can be an explicitly designed summary statistic (our approach). However, adding memory does not remove equilibria where both players always defect, so adding memory does not imply that self-play will find policies that maintain cooperation (Foerster et al., 2017b; Sandholm & Crites, 1996). In the appendix we show that even in the simplest situation, the one memory repeated PD, always defecting equilibria can be more robust attractors than ones which maintain cooperation. amTFT is designed to get around this problem by using modified self-play to explicitly construct the cooperative and cooperation maintaining strategies as well as then switching rule. We begin with the theory behind amTFT.
3

Under review as a conference paper at ICLR 2018

3 APPROXIMATE MARKOV TFT

We begin with a social dilemma where pure cooperators can be exploited. We aim to construct a simple meta-policy which incentivizes cooperation along the path of play by switching intelligently between policies in response to its partner.
We assume that cooperative polices are exchangeable. That is, for any pair (1C , 2C ), (1C , 2C )  iC (s) any combination of the two (eg. (1C , 2C ) is also in Ci (s)) and that all pairs give a unique distribution of the total rewards between the two players.
If policies are not exchangeable or can give different distributions of the total payoff then in addition to having a cooperation problem, we also have a coordination problem (ie. in which particular way should agents cooperate? how should gains from cooperation be split?). This is an important question, especially if we want our agents to interact with humans, and is related to the notion of choosing focal points in coordination/bargaining games. However, a complete solution is beyond the scope of this work and will often depend on contextual factors. See eg. Schelling (1980); Roth et al. (1991); Kleiman-Weiner et al. (2016); Peysakhovich & Lerer (2017); Bicchieri (2005) for more detailed discussion.
For the social dilemma to be solvable, there must be strategies with worse payoffs to both players. Consider an equilibrium (1D, 2D) which has worse payoffs for player 2. We assume that (1D, 2D) is an equilibrium even if played for a finite time, which we call D-dominance. We use D-dominance to bound the payoffs of a partner during the execution of a punishment phase, thus it is a sufficient but not necessary condition. We discuss in the Appendix how this assumption can be relaxed. To define this formally, we first introduce the notation of a compound policy XkZ which is a policy that behaves according to X for k turns and then Z afterwards.

Definition 5 We say a game is D dominant (for player i) if for any k, any state s, and any policy A we have V i(s, ^1DkC , ^2DkC )  V 2(s, ^1DkC , ^2AkC ).

In theory, with access to C, D, their Q functions, and no noise or function approximation, we can construct amTFT as follows. Suppose the amTFT agent plays as player 1 (the reverse is symmetric).
At the start of the game the amTFT agent begins in phase C. If the phase is C then the agent plays according to C. At each time step, if the agent is in a C phase, the agent looks at the action a2 chosen by their partner. The agent computes
d = QC2 C (s, 1C (s), a2) - Q2CC (s, 1C (s), 2C (s)).
If d > 0 then starting at the next time step when state s is reached the agent enters into a D phase where they choose according to D for k periods. k is computed such that
V2(s , 1DkC , 2DkC ) - V2(s , 1C , 2C ) > d.
Here  > 1 controls how often an agent can be exploited by a pure defector. After this k is over the agent returns to the C phase. The amTFT strategy gives a nice guarantee:

Theorem 1 Define d = maxA2,s(QC2 C (s, 1C (s), a) - Q2CC (s, 1C (s), 2C (s))). If for any state

s we have that V2(s, 1C , 2C ) - V2(s, 1D, 2D)

>

d 

then if player 1 is an amTFT agent, a

fully

omniscient player 2 maximizes their payoffs by behaving according to 2C when 1 is in a C phase and

2D when 1 is in a D-phase. Thus, if agents start in the C phase and there is no noise, they cooperate

forever. If they start in a D phase, they eventually return to a C phase.

The proof is quite simple and we relegate it to the Appendix. However, we now see that amTFT has the desiderata we have asked for: it is easy to explain, it cooperates with a pure cooperator, it does not get completely exploited by a pure defector,6 and incentivizes cooperation along the trajectory of play.7
6In an infinite length game amTFT will get exploited an infinite number of times as it tries to return to cooperation after each D phase. One potential way to avoid this to avoid this is to increase  at each D phase.
7This makes amTFT subtly different from TFT. TFT requires one's partner to cooperate even during the D phase for the system return to cooperation. By contrast, amTFT allows any action during the D phase, this makes it similar to the rPD strategy of Win-Stay-Lose-Shift or Pavlov (Nowak & Sigmund, 1993).

4

Under review as a conference paper at ICLR 2018

4 CONSTRUCTING AN AMTFT AGENT

We now use RL methods to construct the components required for amTFT by approximating the cooperative and defect policies as well as the switching policy. To construct the required policies we use self-play and two reward schedules: selfish and cooperative.
In the selfish reward schedule each agent i treats the other agent just as a part of their environment and tries to maximize their own reward. We assume that RL training converges and we call the converged policies under the selfish reward schedule ^iD and the associated Q function approximations Q^iDD. If policies converge with this training then ^D is a Markov equilibrium (up to function approximation).
In the cooperative reward schedule each agent gets rewards both from their own payoff and the rewards the other agent receives. That is, we modify the reward function so that it is RiCC (s, a1, a2) = R1(s, a1, a2) + R2(s, a1, a2). We call the converged policy and value function approximations ^iC and Q^Ci C . In this paper we are agnostic to which learning algorithm is used to compute policies.
In general there can be convergence issues with selfish self-play (Fudenberg & Levine, 1998; Conitzer & Sandholm, 2007; Papadimitriou, 2007) while in the cooperative reward schedule the standard RL convergence guarantees apply. The latter is because cooperative training is equivalent to one super-agent controlling both players and trying to optimize for a single scalar reward.
With the value functions and policies in hand from the procedure above, we can construct an amTFT meta-policy. For the purposes of this construction, we consider agent 1 as the amTFT agent (but everything is symmetric). The amTFT agent keeps a memory state (Wt, bt) which both start at 0.
The amTFT agent sees the action a of their partner at time t and approximates the gain from this deviation as Dt = Q^2CC (s, at2) - Q^2CC (s, 2C (s)). To compute this debit we can either use learned Q functions or we can simply use rollouts.
The amTFT agent accumulates the total payoff balance of their partner as Wt = Wt-1 + Dt. If Wt is below a fixed threshold T the amTFT agent chooses actions according to C. If Wt crosses a threshold T the mTFT agent uses rollouts to compute a k such that the partner loses more from ^DkC relative to cooperation than some constant  times the current debit. The hyperparameters T and  trade off robustness to approximation error and noise. Raising T allows for more approximation error in the calculation of the debit but relaxes the incentive constraints on the agent's partner. Raising  makes the cost of defection higher but makes false positives more costly. The algorithm is formalized below:

Algorithm 1 Approximate Markov Tit For Tat (for Agent 1)

Input: ^C , ^D and their Q^; , T b  0, W  0
while Game do D  Q^2CC (s, a2) - Q^C2 C (s, ^2C (s)) if b = 0 then Choose a  ^1C (s) W =W +D
if b > 0 then Choose a  ^1D(s) b=b-1
if W > T then b = k^(s, T )
W =0

Q^ comes from model or rollouts k^(s, W ) uses rollouts to compute length of D phase

A key component of the amTFT strategy is the computation of the per period debit Dt. In our experiments we do this via use batched policy rollouts. This procedure is unbiased in the limit of large batches and long rollouts but is computationally intensive at test time. Another way is to use a model to predict Q directly with a model. This is difficult in practice since any bias in the model is accumulated across periods and because the model needs to be accurate everywhere, not just on the trajectory of C. In the appendix we discuss some results on learning a model Q^ and improving the efficiency of such procedures is an important direction for future work.

5

Under review as a conference paper at ICLR 2018

5 EXPERIMENTS
We test amTFT in two environments: one grid-world and one where agents must learn from raw pixels. In the grid-world game Coins two players move on a 5 × 5 board. The game has a small probability of ending in every time step, we set this so the average game length is 500 time steps. Coins of different colors appear on the board periodically, and a player receives a reward of 1 for collecting (moving over) any coin. However, if a player picks up a coin of the other player's color, the other player loses 2 points. The payoff for each agent at the end of each game is just their own point total. The strategy which maximizes total payoff is for each player to only pick up coins of their own color; however each player is tempted to pick up the coins of the other player's color.
We also look at an environment where strategies must be learned from raw pixels. We use the method of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives -2. We refer to this game as the Pong Player's Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully cooperative agent can be exploited by a defector.
We are interested in constructing general strategies which scale beyond tabular games so we use deep neural networks for state representation for both setups. We use standard setups so we relegate the details of the networks as well as the training to the appendix.
We perform both Selfish (self play with reactive agents receiving own rewards) and Cooperative (self play with both agents receiving sum of rewards) training for both games. We train 100 replicates for Coins and 18 replicates for the PPD. In both games Selfish training leads to suboptimal behavior while Cooperative training does find policies that implement socially optimal outcomes. In Coins ^D agents converge to picking up coins of all colors while social ^C agents learn to only pick up matching coins. In PPD selfishly trained agents learn to compete and try to score while prosocially trained agents gently hit the ball back and forth.

(a) Coins

(b) PPD

Coins Defections

Coins Total Payoff

PPD Total Payoff 0

60 30
-10

20 40 -20

10 20 -30

00

0

35000 0

35000

0

Number Games Played

35000

Training Cooperative Selfish

(c) Training

(d) Coins Results

(e) PPD Results

Figure 1: In two Markov social dilemmas we find that standard self-play converges to defecting strategies while modified self-play finds cooperative, but exploitable strategies. We find that amTFT avoids being exploited by defectors while also incentivizing cooperative strategies from it's partner. We also see that inferring a partner's cooperation using the value function (amTFT) is much more stable than inferring it via actions (Grim).

6

Under review as a conference paper at ICLR 2018

We evaluate the performance of various Markov social dilemma strategies in a tournament. To construct a matchup between two strategies we construct agents and have them play a fixed length iteration of the game. Note that at training time we use a random length game but at test time we use a fixed length one so that we can compare payoffs more efficiently. We use 1000 replicates per strategy pair to compute the average expected payoff. We compare ^C, ^D, and amTFT.
We also compare the direct adaptation of the construction in De Cote & Littman (2012). Recall that this algorithm maintains equilibria by threat of deviation later: if either agent's behavior in game iteration t does not accord with the cooperative policy, both agents switch to a different policy in the next repetition of the game. We adapt this to the single test game setting as follows: the agent computes policies ^C , ^D and if their partner j takes an action a in a state s where a = ^jC (s) the agent switches to ^D forever. We call this the Grim Trigger Strategy due to its resemblance to the rPD strategy of the same name.
In both games we find that cooperators can be exploited by defectors, two defectors together get low payoffs as they compete for coins, amTFT cooperates with cooperators (and itself) while avoiding exploitation, and the Grim Trigger strategy acts like ^D most of the time because of it is incredibly sensitive to a partner's trivial deviations from its preferred cooperative strategy.

6 AMTFT AS TEACHER
The results above show that amTFT is a good strategy to employ in a mixed environment which includes some cooperators, some tit-for-tat agents and some defectors. However, what happens when amTFT's partner is themselves a learning agent?
To answer this question we consider what happens if we fix the one player (the Teacher) to use a fixed policy but let the other player be a selfish reactive deep RL agent (the Learner). We perform the retraining in the domain of Coins.8 Recall that when selfish RL agents played with each other, they converged to the Selfish `grab all coins' strategy. We see that Learners paired with purely cooperative teachers learn to exploit the teachers, learners paired with ^D also learn to exploit (this learning happens much slower because a fully trained ^D policy is able to grab coins very quickly and thus it is hard for a blank slate agent to learn at all), however learners paired with amTFT learn to cooperate. Note that choosing amTFT as a strategy leads to higher payoffs for both the Learner and the Teacher, thus even if we only care about the payoffs accrued to our own agent we can do better with amTFT than a purely greedy strategy.

Teacher Payoff

Total Payoff

50

Payoff

25

0

-25 0

10000 20000 30000

0 10000 20000 30000

Number Games Played

Training amTFT Prosocial Selfish

Figure 2: Both purely selfish and purely cooperative Teachers lead Learners to exploitative strategies. However, amTFT Teachers lead Learners to cooperate and thus both agents reach a higher payoff in the long-run.

8We tried to perform the retraining in the PPD but incentivizing cooperation via a shift to D later requires a low discount rate and we found A3C to be unstable in this regime.
7

Under review as a conference paper at ICLR 2018
7 CONCLUSION
Humans are remarkably adapted to solving bilateral social dilemmas. We have focused on how to give artificial agents this capability. We have shown that amTFT can maintain cooperation and avoid exploitation in Markov games. In addition we have provided a simple construction for this strategy that requires no more than modified self-play. Thus, amTFT can be applied to social dilemmas in many environments.
Since the initial reporting of the results in this paper there has been more work on using deep RL to construct good agents for general-sum social dilemmas. An alternative approach uses end-to-end learning which takes into account that one's partner is a reactive (rather than static) agent to construct good strategies. This has been shown to construct cooperation maintaining strategies in the repeated PD and in Coins (Foerster et al., 2017b). This approach has some drawbacks: it is computationally challenging, it has no known theoretical guarantees, and it is not clear that the strategies learned via this approach will be simple to explain (eg. to a human partner). Despite these drawbacks we believe such end-to-end approaches are an incredibly interesting direction for future research and that explicit constructions like amTFT are a complement to, not a substitute for, end-to-end ideas.
This work further emphasizes the importance of treating agents as fundamentally different than other parts of the environment. In particular, agents have beliefs, desires, learn, and use some form of optimization while objects follow simple fixed rules. An important future direction for constructing cooperative agents is to continue to incorporate ideas from inverse reinforcement learning (Abbeel & Ng, 2004; Ng et al., 2000) and cognitive science (Baker et al., 2009; Kleiman-Weiner et al., 2016) to construct agents that exhibit some theory of mind.
There is a growing literature on hybrid systems which include both human and artificial agents (Crandall et al., 2017; Shirado & Christakis, 2017). We have focused on implementing outcomes which maximize total payoff, perhaps at the cost of distributional considerations (eg. equity) while being robust to perfectly selfish partners. Many of our techniques have focused on making the policy robust to approximation error that allows a selfish partner to cheat. However, human preferences are far more complex and include considerations of fairness (Fehr & Gächter, 2000), altruistic cooperation (Peysakhovich et al., 2014) and social norms (Bicchieri, 2005). By changing the cooperative training, the amTFT construction is capable of maintaining behavior that take these other concerns into consideration as well. An important future direction is to incorporate what is known about human social preferences into the design of better agents and mechanisms (Kraft-Todd et al., 2015; Yoeli et al., 2013; Hauser et al., 2014; Ouss & Peysakhovich, 2015).
REFERENCES
Abbeel, Pieter and Ng, Andrew Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Axelrod, Robert M. The evolution of cooperation: revised edition. Basic books, 2006.
Baker, Chris L, Saxe, Rebecca, and Tenenbaum, Joshua B. Action understanding as inverse planning. Cognition, 113(3):329­349, 2009.
Bicchieri, Cristina. The grammar of society: The nature and dynamics of social norms. Cambridge University Press, 2005.
Bó, Pedro Dal and Fréchette, Guillaume R. The evolution of cooperation in infinitely repeated games: Experimental evidence. The American Economic Review, 101(1):411­429, 2011.
Brown, George W. Iterative solution of games by fictitious play. Activity analysis of production and allocation, 13(1):374­376, 1951.
Brown, Noam, Ganzfried, Sam, and Sandholm, Tuomas. Hierarchical abstraction, distributed equilibrium computation, and post-processing, with application to a champion no-limit texas hold'em agent. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 7­15. International Foundation for Autonomous Agents and Multiagent Systems, 2015.
8

Under review as a conference paper at ICLR 2018
Conitzer, Vincent and Sandholm, Tuomas. Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents. Machine Learning, 67(1-2):23­43, 2007.
Crandall, Jacob W, Oudah, Mayada, Ishowo-Oloko, Fatimah, Abdallah, Sherief, Bonnefon, JeanFrançois, Cebrian, Manuel, Shariff, Azim, Goodrich, Michael A, Rahwan, Iyad, et al. Cooperating with machines. arXiv preprint arXiv:1703.06207, 2017.
Das, Abhishek, Kottur, Satwik, Moura, José MF, Lee, Stefan, and Batra, Dhruv. Learning cooperative visual dialog agents with deep reinforcement learning. arXiv preprint arXiv:1703.06585, 2017.
De Cote, Enrique Munoz and Littman, Michael L. A polynomial-time nash equilibrium algorithm for repeated stochastic games. arXiv preprint arXiv:1206.3277, 2012.
Evtimova, Katrina, Drozdov, Andrew, Kiela, Douwe, and Cho, Kyunghyun. Emergent language in a multi-modal, multi-step referential game. arXiv preprint arXiv:1705.10369, 2017.
Fehr, Ernst and Gächter, Simon. Fairness and retaliation: The economics of reciprocity. The journal of economic perspectives, 14(3):159­181, 2000.
Foerster, Jakob, Nardelli, Nantas, Farquhar, Gregory, Torr, Philip, Kohli, Pushmeet, Whiteson, Shimon, et al. Stabilising experience replay for deep multi-agent reinforcement learning. arXiv preprint arXiv:1702.08887, 2017a.
Foerster, Jakob N, Chen, Richard Y, Al-Shedivat, Maruan, Whiteson, Shimon, Abbeel, Pieter, and Mordatch, Igor. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017b.
Fudenberg, Drew and Levine, David K. The theory of learning in games, volume 2. MIT press, 1998.
Hauser, Oliver P, Rand, David G, Peysakhovich, Alexander, and Nowak, Martin A. Cooperating with the future. Nature, 511(7508):220­223, 2014.
Havrylov, Serhii and Titov, Ivan. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. arXiv preprint arXiv:1705.11192, 2017.
Jorge, Emilio, Kågebäck, Mikael, and Gustavsson, Emil. Learning to play guess who? and inventing a grounded language as a consequence. arXiv preprint arXiv:1611.03218, 2016.
Kempka, Michal, Wydmuch, Marek, Runc, Grzegorz, Toczek, Jakub, and Jas´kowski, Wojciech. Vizdoom: A doom-based ai research platform for visual reinforcement learning. arXiv preprint arXiv:1605.02097, 2016.
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Kleiman-Weiner, Max, Ho, Mark K, Austerweil, Joe L, Michael L, Littman, and Tenenbaum, Joshua B. Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction. In Proceedings of the 38th Annual Conference of the Cognitive Science Society, 2016.
Kraft-Todd, Gordon, Yoeli, Erez, Bhanot, Syon, and Rand, David. Promoting cooperation in the field. Current Opinion in Behavioral Sciences, 3:96­101, 2015.
Lazaridou, Angeliki, Peysakhovich, Alexander, and Baroni, Marco. Multi-agent cooperation and the emergence of (natural) language. In International Conference on Learning Representations, 2017.
Leibo, Joel Z, Zambaldi, Vinicius, Lanctot, Marc, Marecki, Janusz, and Graepel, Thore. Multi-agent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 464­473. International Foundation for Autonomous Agents and Multiagent Systems, 2017.
Lewis, Mike, Yarats, Denis, Dauphin, Yann N, Parikh, Devi, and Batra, Dhruv. Deal or no deal? end-to-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125, 2017.
9

Under review as a conference paper at ICLR 2018
Littman, Michael L and Stone, Peter. A polynomial-time nash equilibrium algorithm for repeated games. Decision Support Systems, 39(1):55­66, 2005.
Lowe, Ryan, Wu, Yi, Tamar, Aviv, Harb, Jean, Abbeel, Pieter, and Mordatch, Igor. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Humanlevel control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timothy, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
Neumann, J v. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295­320, 1928.
Ng, Andrew Y, Russell, Stuart J, et al. Algorithms for inverse reinforcement learning. In Icml, pp. 663­670, 2000.
Nowak, Martin and Sigmund, Karl. A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner's dilemma game. Nature, 364(6432):56, 1993.
Nowak, Martin A. Evolutionary dynamics. Harvard University Press, 2006.
Ouss, Aurélie and Peysakhovich, Alexander. When punishment doesn't pay: 'cold glow' and decisions to punish. Journal of Law and Economics, 58(3), 2015.
Papadimitriou, Christos H. The complexity of finding nash equilibria. Algorithmic Game Theory, pp. 29­51, 2007.
Perolat, Julien, Leibo, Joel Z, Zambaldi, Vinicius, Beattie, Charles, Tuyls, Karl, and Graepel, Thore. A multi-agent reinforcement learning model of common-pool resource appropriation. arXiv preprint arXiv:1707.06600, 2017.
Peysakhovich, Alexander and Lerer, Adam. Prosocial learning agents solve generalized stag hunts better than selfish ones. arXiv preprint arXiv:1709.02865, 2017.
Peysakhovich, Alexander, Nowak, Martin A, and Rand, David G. Humans display a 'cooperative phenotype' that is domain general and temporally stable. Nature Communications, 2014.
Riedmiller, Martin, Gabel, Thomas, Hafner, Roland, and Lange, Sascha. Reinforcement learning for robot soccer. Autonomous Robots, 27(1):55­73, 2009.
Roth, Alvin E, Prasnikar, Vesna, Okuno-Fujiwara, Masahiro, and Zamir, Shmuel. Bargaining and market behavior in jerusalem, ljubljana, pittsburgh, and tokyo: An experimental study. The American Economic Review, pp. 1068­1095, 1991.
Sandholm, Tuomas W and Crites, Robert H. Multiagent reinforcement learning in the iterated prisoner's dilemma. Biosystems, 37(1-2):147­166, 1996.
Schelling, Thomas C. The strategy of conflict. Harvard university press, 1980.
Shapley, Lloyd S. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095­1100, 1953.
Shirado, Hirokazu and Christakis, Nicholas A. Locally noisy autonomous agents improve global human coordination in network experiments. Nature, 545(7654):370­374, 2017.
Silver, David, Huang, Aja, Maddison, Chris J, Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
10

Under review as a conference paper at ICLR 2018
Silver, David, Schrittwieser, Julian, Simonyan, Karen, Antonoglou, Ioannis, Huang, Aja, Guez, Arthur, Hubert, Thomas, Baker, Lucas, Lai, Matthew, Bolton, Adrian, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354­359, 2017.
Tampuu, Ardi, Matiisen, Tambet, Kodelja, Dorian, Kuzovkin, Ilya, Korjus, Kristjan, Aru, Juhan, Aru, Jaan, and Vicente, Raul. Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4):e0172395, 2017.
Tesauro, Gerald. Temporal difference learning and td-gammon. Communications of the ACM, 38(3): 58­68, 1995.
Usunier, Nicolas, Synnaeve, Gabriel, Lin, Zeming, and Chintala, Soumith. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint arXiv:1609.02993, 2016.
Williams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Wu, Yuxin and Tian, Yuandong. Training agent for first-person shooter game with actor-critic curriculum learning. 2016.
Yoeli, Erez, Hoffman, Moshe, Rand, David G, and Nowak, Martin A. Powering up with indirect reciprocity in a large-scale field experiment. Proceedings of the National Academy of Sciences, 110(Supplement 2):10424­10429, 2013.
8 APPENDIX
8.1 STANDARD SELF-PLAY FAILS TO DISCOVER COOPERATIVE STRATEGIES IN THE REPEATED PD
In a social dilemma there exists an equilibrium of mutual defection, and there may exist additional equilibria of conditional cooperation. Standard self-play may converge to any of these equilibria. When policy spaces are large, it is often the case that simple equilibria of constant mutual defection have larger basins of attraction than policies which maintain cooperation.
We can illustrate this with the simple example of the repeated Prisoner's Dilemma. Consider a PD with payoffs of 0 to mutual defection, 1 for mutual cooperation, w > 1 for defecting on a cooperative partner and -s for being defected on while cooperating. Consider the simplest possible state representation where the set of states is the pair of actions played last period and let the initial state be (C, C) (this is the most optimistic possible setup). We consider RL agents that use policy gradient (results displayed here come from using Adam (Kingma & Ba, 2014), similar results were obtained with SGD though convergence speed was much more sensitive to the setting of the learning rate) to learn policies from states (last period actions) to behavior.
Note that this policy space contains TFT (cooperate after (C, C), (D, C), defect otherwise), Grim Trigger (cooperate after (C, C), defect otherwise) and Pavlov or Win-Stay-Lose-Shift (cooperate after (C, C), (D, D), defect otherwise (Nowak & Sigmund, 1993)) which are all cooperation maintaining strategies (though only Grim and WSLS are themselves full equilibria).
Each episode is defined as one repeated PD game which lasts a random number of periods with stopping probability of stopping .05 after each period. Policies in the game are maps from the onememory state space {(C, C), (D, C), (C, D), (D, D)} to either cooperation or not. These policies are trained using policy gradient and the REINFORCE algorithm (Williams, 1992). We vary w and set s = 1.5w such that (C, C) is the most efficient strategy always. Note that all of these parameters are well within the range where humans discover cooperative strategies in experimental applications of the repeated PD (Bó & Fréchette, 2011).
Figure 3 shows that cooperation only robustly occurs when it is a dominant strategy for both players (w < 0) and thus the game is no longer a social dilemma.9.
9Note that these results use pairwise learning and therefore are different from evolutionary game theoretic results on the emergence of cooperation (Nowak, 2006). Those results show that indeed cooperation can robustly
11

Under review as a conference paper at ICLR 2018

0.75

Avg Cooperation

0.50

0.25

0.00 0

250 500 750
Episodes of Training
Temptation Payoff 0.5 1.3 1.5 1.9

1000

Figure 3: Results from training one-memory strategies using policy gradient in the repeated Prisoner's Dilemma. Even in extremely favorable conditions self-play fails to discover cooperation maintaining strategies. Note that temptation payoff .5 is not a PD and here C is a dominant strategy in the stage game.

8.2 PROOF OF MAIN THEOREM

To prove the theorem we will apply the one deviation principle. To show this, we fix player 1 to be an
amTFT agent and look at player 2. Note that from the point of view of player 2 this is now a Markov game with a state representation of (s, k) where if k = 0 player 1 behaves according to C and if k > 0 player 1 is in the D phase and thus behaves according to DkC .

We consider the policy for player 2 of `play 2C when player 1 is in the C phase and play 2D when player 1 is in the D phase.' Recall by the Principle of Optimality if there does not exist a one shot
deviation a at any state under which player 2 earns a higher payoff, then there does not exist a better
policy than the one prescribed.

Consider starting at k > 0. The suggested policy has player 2 play 2DkC . By D-dominance this is the best response to 1DkC so there are no one-shot deviations in the D phase.

Let us consider what happens in the C phase (k = 0). By the assumption of the theorem at any state

s we know that

V2(s, 1C , 2C ) -

V2(s, 1D, 2D)

>

d .


Let {r2t (s, 1, 2)} be the per-period reward stream (note here each r is a random variable) for player

2 induced by the policies 1, 2. Since



V2(s, 1, 2) = E t r2t (s, 1, 2)

t=0

where  is the discount rate. Because rewards are bounded then for any > 0 there exists k such that

k
|V2(s, 1, 2) - tr2t | < .
t=0

emerge in these kinds of strategy spaces under evolutionary processes. Those results differ because they rely on the following argument: suppose we have a population of defectors. This can be invaded by mutants of TFT because TFT can try cooperation in the first round. If it is matched with a defector, it loses once but it then defects for the rest of the time, if it is matched with another TFT then they cooperate for a long time. Thus, for sufficiently long games the risk of one round of loss is far smaller than the potential fitness gain of meeting another mutant. Thus TFT can eventually gain a foothold. It is clear why in learning scenarios such arguments cannot apply.

12

Under review as a conference paper at ICLR 2018

That is, the first k terms time steps approximate the full discounted expectation arbitrarily well. This also means that for some k

V2(s, 1C , 2C ) - V2(s, 1DkC , 2DkC )

>

d .


From any state, with an amTFT

the highest profit an agent can partner is d. However we have

make from shown that

deviating from 2C with a single action there exists a length k such that moving

to

D

for

k

turns

costs

the

agent

more

than

d 

.

Therefore

there

is

no

time

during

the

C

phase

they

wish to deviate. This completes the proof.

8.3 ASIDE ON D-DOMINANCE
The reason we made the D-dominance assumption is to bound the expected payoff of an agent playing against DkC and therefore bound the necessary length of a D phase after a particular deviation. However, in order to compute what the length of the D phase the amTFT agent needs access to the best response policy to DkC , or its associated value function. With D-dominance we assume that D is that best response. Even if D-dominance does not strictly hold, it is likely a sufficient approximation. If necessary however, one can train an RL agent on episodes where their partner plays DkC , where k is observed. This allows one to approximate the best response policy to DkC which will then give us what we need to compute the responses to deviations from C in the D phase that incentivize full cooperation.

8.4 EXPERIMENTAL DETAILS
We used rollouts to calculate the debit to the amTFT's partner at each time period. This estimator has good performance for both PPD and Coins given their reward structure. It is also possible to use a learned model of Q. Learning a sufficiently accurate model Q^ is challenging for several reasons. First, it has to have very low bias, since any bias in Q^ will be accumulated over periods. Second, the one-shot deviation principle demands that Q^ be accurate for all state-action pairs, not just those sampled by the policies (C, C). Standard on-policy value function estimation will only produce accurate estimates of Q at states sampled by the cooperative policies. As an example, in Coins, since the cooperative policies never collect their partner's coins Q^ for these state-action pairs may be inaccurate.
We found that it was possible in Coins to learn a model Q^ to calculate debit without policy rollouts using the same neural network architecture that was used to train the policies. However, we found that in order to train a Q^ model accurate enough to work well we had to use a modified training procedure. After finishing Selfish and Cooperative training, we perform a second step of training using a fixed (converged) ^C. In order to sample states off the path of ^C during this step, the learner behaves according to a mixture of C, D, and random policies while the partner continues according to ^C. Q^ is updated via off-policy Bellman iteration. We found this modified procedure produced a Q^ function that was good enough to maintain cooperation (though still not as efficient as rollouts). For more complex games, an important area for future work is to develop methodologies to compute more accurate approximations of Q or combine a Q^ model with rollouts effectively.

8.4.1 COINS GAME AND TRAINING
For Coins there are four actions (up, down, left, right), and S is represented as a 4 × 5 × 5 binary tensor where the first two channels encode the location of the each agent and the other two channels encode the location of the coin (if any exist). At each time step if there is no coin on the board a coin is generated at a random location with a random color, with probability 0.1.
A policy (s; ) : s  (a) is learned via the advantage actor critic algorithm. We use a multi-layer convolutional neural network to jointly approximate the policy  and state-value function V^ . For this small game, a simpler model could be used, but this model generalizes directly to games with higher-dimensional 2D state spaces (e.g. environments with obstacles). For a given board size k, the model has log2(k) + 1 repeated layers, each consisting of a 2D convolution with kernel size 3, followed by batch normalization and ReLU. The first layer has stride 1, while the successive layers

13

Under review as a conference paper at ICLR 2018

each have stride 2, which decreases the width and height from k to k/2 while doubling the number of channels. For the 5 × 5 board, channel sizes are 13, 26, 52, 104. From these 104 features,  is computed via a linear layer with 4 outputs with softmax, to compute a distribution over actions, while the value function is computed via a single-output linear layer.

The actor and critic are updated episodically with a common learning rate - at the end of each game we update the model on a batch of episodes via

i = 

At

V (st) i

+

A~t

log

(st,

at

)



(st, i

at)

where A is the advantage

At = rt + V (st+1) - V (st)

and A~ is the advantaged normalized over all episodes and periods in the batch

A~t

=

At - |A| . (A)

We train with a learning rate of 0.001, continuation probability .998 (i.e. games last on average 500 steps), discount rate 0.98, and a batch size of 32. We train for a total of 40, 000 games.

8.4.2 PONG PLAYER DILEMMA TRAINING
We use the arcade learning environment modified for 2-player play as proposed in Tampuu et al. (2017), with modified rewards of +1 for scoring a point and -2 for being scored on. We train policies directly from pixels, using the pytorch-a3c package https://github.com/ikostrikov/ pytorch-a3c.
Policies are trained directly from pixels via A3C (Mnih et al., 2016). Inputs are rescaled to 42x42 and normalized, and we augment the state with the difference between successive frames with a frame skip of 8. We use 38 threads for A3C, over a total of 38,000 games (1,000 per thread). We use the default settings from pytorch-a3c: a discount rate of 0.99, learning rate of 0.0001, 20-step returns, and entropy regularization weight of 0.01.
The policy is implemented as a convolutional neural network with four layers, following pytorch-a3c. Each layer uses a 3x3 kernel with stride 2, followed by ELU. The network has two heads for the actor and critic. We elide the LSTM layer used in the pytorch-a3c library, as we found it to be unnecessary.

14

