Under review as a conference paper at ICLR 2018
LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Many state-of-the-art word embedding techniques involve factorization of a cooccurrence based matrix. We aim to extend this approach by studying word embedding techniques that involve factorization of co-occurrence based tensors (N way arrays). We present two new word embedding techniques based on tensor factorization and show that they outperform common methods when used for several semantic NLP tasks when trained with the same data. To train one of the embeddings, we present a new joint tensor factorization problem and an approach for solving it. Furthermore, we modify the performance metrics for the Outlier Detection Camacho-Collados & Navigli (2016) task to measure the quality of higher-order relationships that a word embedding captures. Our tensor-based methods significantly outperform existing methods at this task when using our new metric. Finally, we demonstrate that vectors in our embeddings can be composed multiplicatively to create different vector representations for each meaning of a polysemous word in a way that cannot be done with other common embeddings. We show that this property stems from the higher order information that the vectors contain, and thus is unique to our tensor based embeddings.
INTRODUCTION
Word embeddings have been used to improve the performance of many NLP tasks including language modelling (Bengio et al. (2003)), machine translation (Bahdanau et al. (2014)), and sentiment analysis (Kim (2014)). The broad applicability of word embeddings to NLP implies that improvements to their quality will likely have widespread benefits for the field. The word embedding problem is to learn a mapping  : V  Rk (k  100-300 in most applications) that encodes meaningful semantic and/or syntactic information. For instance, in many word embeddings, (car)  (truck), since the words are semantically similar.
More complex relationships than similarity can also be encoded in word embeddings. For example, we can answer analogy queries of the form a : b :: c : ? using simple arithmetic in many stateof-the-art embeddings (Mikolov et al. (2013)). The answer to bed : sleep :: chair : x is given by the word whose vector representation is closest to (sleep) - (bed) + (chair) ( (sit)). Other embeddings may encode such information in a nonlinear way (Jastrzebski et al. (2017)).
Mikolov et al. (2013) demonstrates the additive compositionality of their word2vec vectors: one can sum vectors produced by their embedding to compute vectors for certain phrases rather than just vectors for words. Later in this paper, we will show that our embeddings naturally give rise to a form of multiplicative compositionality that has not yet been explored in the literature.
Almost all recent word embeddings rely on the distributional hypothesis (Harris), which states that a word's meaning can be inferred from the words that tend to surround it. To utilize the distributional hypothesis, many embeddings are given by a low-rank factor of a matrix derived from co-occurrences in a large unsupervised corpus. For examples of this, see Pennington et al. (2014); Murphy et al. (2012); Levy & Goldberg (2014) and Salle et al. (2016). Approaches that rely on matrix factorization only utilize pairwise co-occurrence information in the corpus. We aim to generalize this approach by creating word embeddings given by factors of tensors containing higher order co-occurrence data.
1

Under review as a conference paper at ICLR 2018

RELATED WORK
Some common word embeddings related to co-occurrence based matrix factorization include GloVe (Pennington et al. (2014)), word2vec (Levy & Goldberg (2014)), LexVec (Salle et al. (2016)), and NNSE (Murphy et al. (2012)). In contrast, our work studies word embeddings given by factorization of tensors. An overview of tensor factorization methods is given in Kolda & Bader (2009).
Our work uses factorization of symmetric nonnegative tensors, which has been studied in the past (Wang & Qi (2007); Comon et al. (2008)). In general, factorization of tensors has been applied to NLP in Van de Cruys et al. (2013); Zhang et al. (2014) and factorization of nonnegative tensors in Van de Cruys (2009). Recently, factorization of symmetric tensors has been used to create a generic word embedding (Sharan & Valiant (2017)) but the idea was not explored extensively. Our work studies this idea in much greater detail, fully demonstrating the viability of tensor factorization as a technique for training word embeddings.
Composition of word vectors to create novel representations has been studied in depth, including additive, multiplicative, and tensor-based methods Mitchell & Lapata (2010); Blacoe & Lapata (2012). Typically, composition is used to create vectors that represent phrases or sentences. Our work, instead, shows that pairs of word vectors can be composed multiplicatively to create different vector representations for the various meanings of a single polysemous word.

MATHEMATICAL PRELIMINARIES

NOTATION
Throughout this paper we will write scalars in lowercase italics , vectors in lowercase bold letters v, matrices with uppercase bold letters M, and tensors (of order N > 2) with Euler script notation X, as is standard in the literature.

POINTWISE MUTUAL INFORMATION
Pointwise mutual information (PMI) is a useful property in NLP that quantifies the likelihood that two words co-occur (Levy & Goldberg (2014)). It is defined as:
p(x, y) P M I(x, y) = log p(x)p(y)

where p(x, y) is the probability that x and y occur together in a given fixed-length context window in the corpus, irrespective of order.

It is often useful to consider the positive PMI (PPMI), defined as:

P P M I(x, y) := max(0, P M I(x, y))

since negative PMI values have little grounded interpretation (Bullinaria & Levy (2007); Levy & Goldberg (2014); Van de Cruys (2009)).

Given an indexed vocabulary V = {w1, . . . , w|V |}, one can construct a |V | × |V | PPMI matrix M where mij = P P M I(wi, wj). Many existing word embedding techniques involve factorizing this PPMI matrix: Levy & Goldberg (2014); Murphy et al. (2012); Salle et al. (2016).

PMI can be generalized to N variables. While there are many ways to do so (Van de Cruys (2011)), in this paper we use the form defined by:

P

M

I (x1N

)

=

log

p(x1, . . . , xN ) p(x1) · · · p(xN )

where p(x1, . . . , xN ) is the probability that all of x1, . . . , xN occur together in a given fixed-length context window in the corpus, irrespective of their order.

In this paper we study 3-way PPMI tensors M, where mijk = P P M I(wi, wj, wk), as this is the natural higher-order generalization of the PPMI matrix. We leave the study of creating word embeddings with N -dimensional PPMI tensors (N > 3) to future work.

2

Under review as a conference paper at ICLR 2018

TENSOR FACTORIZATION

Just as the rank-R matrix decomposition is defined to be the product of two factor matrices (M  UV ), the canonical rank-R tensor decomposition for a third order tensor is defined to be the product of three factor matrices (Kolda & Bader (2009)):

R

X  ur  vr  wr =: U, V, W ,

(1)

r=1
where  is the outer product: (a  b  c)ijk = aibjck. This is also commonly referred to as the rank-R CP Decomposition. Elementwise, this is written as:

R

xijk  uirvjrwkr = u:,i  v:,j , w:,k ,

r=1
where  is elementwise vector multiplication and u:,i is the ith row of U. In our later section on multiplicative compositionality, we will see this formulation gives rise to a meaningful interpretation of the elementwise product between vectors in our word embeddings.

Symmetric CP Decomposition. In this paper, we will consider symmetric CP decomposition of
nonnegative tensors (Lim (2005); Kolda & Bader (2009)). Since our N -way PPMI is nonnegative and invariant under permutation, the PPMI tensor M is nonnegative and supersymmetric, i.e. mijk = m(i)(j)(k)  0 for any permutation   S3.

In the symmetric CP decomposition, instead of factorizing M  U, V, W , we factorize M as the triple product of a single factor matrix U  R|V |×R such that
M  U, U, U

In this formulation, we use U to be the word embedding so the vector for wi is the ith row of U similar to the formulations in: Levy & Goldberg (2014); Murphy et al. (2012); Pennington et al. (2014).
It is known that the optimal rank-k CP decomposition exists for symmetric nonnegative tensors such as the PPMI tensor (Lim (2005)). However, finding such a decomposition is NP hard in general (Hstad (1990)) so we must consider approximate methods.
In this work, we only consider the symmetric CP decomposition, leaving the study of other tensor decompositions (such as the Tensor Train or HOSVD Oseledets (2011); Kolda & Bader (2009)) to future work.

WHY FACTORIZE THE THIRD MOMENT?
There are several reasons that we believe can justify the performance of factorizing the PPMI tensor of the third moment of co-occurence. Assuming that NLP tasks such as semantic analogy depend on how the embedding vectors cluster, one can think of word embeddings as finding an embedding so that semantically similar words form a cluster. For identifying the clusters of a planted partition model such as the Stochastic Block Model (SBM), the spectral factorization of node interactions completely derived from pair-wise interactions is sufficient for discovering the disjoint clusters (Belkin & Niyogi (2001); Krzakala et al. (2013); Spielman (2007)). However, polysemous words necessitate the assumption of a Mixed Membership (MM) model, since polysemous words belong to multiple different clusters of meaning (Foulds (2017)). In this case, it is well-known that factorizing the third moment provably recovers the parameters of planted Mixed Membership-SBM model (Anandkumar et al. (2013)) in a way that only capturing pair-wise interactions, i.e. the second order moment (uniquely) cannot. Further from the perspective of using Gaussian mixture models for capturing polysemy in word embeddings (Athiwaratkun & Wilson (2017)) it is also known that factorizing the third moments can provably identify the isotropic Gaussian mixture models (Anandkumar et al. (2014)).
For an analysis of the degree to which our tensor factorization-based embeddings capture polysemy, we refer the reader to Section on multiplicative compositionality on page 9. Another perspective is that considering the third order moment further contextualizes the co-occurance matrix, adding information that was lost by only considering matrix factorization.

3

Under review as a conference paper at ICLR 2018

METHODOLOGIES

COMPUTING THE SYMMETRIC CP DECOMPOSITION
The (|V |3) size of the third order PPMI tensor presents a number of computational challenges. In practice, |V | can vary from 104 to 106, resulting in a tensor whose naive representation requires at least 4  10, 0003 bytes = 4 TB of floats. Even the sparse representation of the tensor takes up such a large fraction of memory that standard algorithms such as successive rank-1 approximation (Wang & Qi (2007); Mu et al. (2015)) and alternating least-squares (Kolda & Bader (2009)) are infeasible for our uses. Thus, in this paper we will consider a stochastic online formulation similar to that of (Maehara et al. (2016)).
We optimize the CP decomposition in an online fashion, using small random subsets Mt of the nonzero tensor entries to update the decomposition at time t. In this minibatch setting, we optimize the decomposition based on the current minibatch and the previous decomposition at time t - 1. To update U (and thus the symmetric decomposition), we first define a decomposition loss L(Mt, U) and minimize this loss with respect to U using Adam (Kingma & Ba (2014)).
At each time t, we take Mt to be all co-occurrence triples (weighted by PPMI) in a fixed number of sentences (around 1,000) from the corpus. We continue training until we have depleted the entire corpus.
For Mt to accurately model M, we also include a certain proportion of elements with zero PPMI (or "negative samples") in Mt, similar to that of Salle et al. (2016). We use an empirically found proportion of negative samples for training, and leave theoretical discovery of the optimal negative sample proportion to future work.

WORD EMBEDDING PROPOSALS

CP-S. The first embedding we propose is based on symmetic CP decomposition of the PPMI tensor M as discussed in the mathematical preliminaries section. The optimal setting for the word embedding W is:
W := argmin ||M - U, U, U ||F
U
Since we cannot feasibly compute this exactly, we minimize the loss function defined as the squared error between the values in Mt and their predicted values:

R

L(Mt, U) =

(mtijk - uirujrukr)2

mtijk Mt

r=1

using the techniques discussed in the previous section.

JCP-S. A potential problem with CP-S is that it is only trained on third order information. To rectify
this issue, we propose a novel joint tensor factorization problem we call Joint Symmetric Rank-R CP
Decomposition. In this problem, the input is the fixed rank R and a list of supersymmetric tensors Mn of different orders but whose axis lengths all equal |V |. Each tensor Mn is to be factorized via rank-R symmetric CP decomposition using a single |V | × R factor matrix U.

To produce a solution, we first define the loss at time t to be the sum of the reconstruction losses of

each different tensor:

N

Ljoint((Mt)Nn=2, U) = L(Mnt , U),

n=2

where Mn is an n-dimensional supersymmetric PPMI tensor. We then minimize the loss with respect to U. Since we are using at most third order tensors in this work, we assign our word

embedding W to be:

W := argmin Ljoint(M2, M3, U)
U

= argmin L(M2, U) + L(M3, U)
U

4

Under review as a conference paper at ICLR 2018

This problem is a specific instance of Coupled Tensor Decomposition, which has been studied in the past (Acar et al. (2011); Naskovska & Haardt (2016)). In this problem, the goal is to factorize multiple tensors using at least one factor matrix in common. A similar formulation to our problem can be found in Comon et al. (2015), which studies blind source separation using the algebraic geometric aspects of jointly factorizing numerous supersymmetric tensors (to unknown rank). In contrast to our work, they outline some generic rank properties of such a decomposition rather than attacking the problem numerically. Also, in our formulation the rank is fixed and an approximate solution must be found. Exploring the connection between the theoretical aspects of joint decomposition and quality of word embeddings would be an interesting avenue for future work.
To the best of our knowledge this is the first study of Joint Symmetric Rank-R CP Decomposition, and the first application of Coupled Tensor Decomposition to word embedding.
SHIFTED PMI
In the same way Levy & Goldberg (2014) considers factorization of positive shifted PMI matrices, we consider factorization of positive shifted PMI tensors M, where mijk = max(P M I(wi, wj, wk) - , 0) for some constant shift . We empirically found that different levels of shifting resulted in different qualities of word embeddings ­ the best shift we found for CP-S was a shift of   2.7, whereas any nonzero shift for JCP-S resulted in a worse embedding across the board. When we discuss evaluation we report the results given by factorization of the PPMI tensors shifted by the best value we found for each specific embedding.
COMPUTATIONAL NOTES
When considering going from two dimensions to three, it is perhaps necessary to discuss the computational issues in such a problem size increase. However, it should be noted that the creation of pre-trained embeddings can be seen as a pre-processing step for many future NLP tasks, so if the training can be completed once, it can be used forever thereafter without having to take training time into account. Despite this, we found that the training of our embeddings was not considerably slower than the training of order-2 equivalents such as SGNS. Explicitly, our GPU trained CBOW vectors (using the experimental settings found below) in 3568 seconds, whereas training CP-S and JCP-S took 6786 and 8686 seconds respectively.

EVALUATION

In this section we present a quantitative evaluation comparing our embeddings to an informationless embedding and two strong baselines. Our baselines are:

1.

Random (random vectors with I.I.D. entries normally distributed with mean 0 and variance

1 2

),

for comparing against a model with no meaningful information encoded

2. word2vec (CBOW1) Mikolov et al. (2013), for comparison against the most commonly used embedding method, as well as for comparison against a technique related to PPMI matrix factorization Levy & Goldberg (2014)

3. NNSE2 Murphy et al. (2012), for comparison against a technique that relies on an explicit PPMI matrix factorization

1We intentionally choose not to include results from other related/empirically similar models to CBOW in our evaluation (such as SGNS or GloVe) for the sake of readability. We believe that this is not a significant omission to our evaluation since empirically these models perform competitively with one another (Ruder (2016)). Further, these methods are intrinsically interrelated since SG can be seen as a form of co-occurrence count matrix factorization Levy & Goldberg (2014), which GloVe also is, and CBOW is solving a nearly identical objective to that of SG. In any case, the performance of these models are competitive enough with each other across various word embeddings tasks that we choose to only include one of them.
2The input to NNSE is an m × n matrix, where there are m words and n co-occurrence patterns. In our experiments, we set m = n = |V | and set the co-occurrence information to be the number of times wi appears within a window of 5 words of wj. As stated in the paper, the matrix entries are weighted by PPMI.

5

Under review as a conference paper at ICLR 2018

For a fair comparison, we trained each model on the same corpus of 10 million sentences gathered from Wikipedia. We removed stopwords and words appearing fewer than 2,000 times (130 million tokens total) to reduce noise and uninformative words. The baselines were trained using the recommended hyperparameters from their original publications, and all optimizers were using using the default settings. Hyperparameters are always consistent across evaluations.
Because of the dataset size, the results shown should be considered a proof of concept rather than an objective comparison to state-of-the-art pre-trained embeddings. Due to the natural computational challenges arising from working with tensors, we leave creation of a full-scale production ready embedding based on tensor factorization to future work.
As is common in the literature (Mikolov et al. (2013); Murphy et al. (2012)), we use 300-dimensional vectors for our embeddings and all word vectors are normalized to unit length prior to evaluation.

QUANTITATIVE TASKS

Outlier Detection. The Outlier Detection task (Camacho-Collados & Navigli (2016)) is to determine which word in a list L of n+1 words is unrelated to the other n which were chosen to be related. For each w  L, one can compute its compactness score c(w), which is the compactness of L \ {w}. c(w) is explicitly computed as the mean similarity of all word pairs (wi, wj) : wi, wj  L \ {w}. The predicted outlier is argmaxwLc(w), as the n related words should form a compact cluster with high mean similarity.
We use the WikiSem500 dataset (Blair et al. (2016)) which includes sets of n = 8 words per group gathered based on semantic similarity. Thus, performance on this task is correlated with the amount of semantic information encoded in a word embedding. Performance on this dataset was shown to be well-correlated with performance at the common NLP task of sentiment analysis (Blair et al. (2016)).
The two metrics associated with this task are accuracy and Outlier Position Percentage (OPP). Accuracy is the fraction of cases in which the true outlier correctly had the highest compactness score. OPP measures how close the true outlier was to having the highest compactness score, rewarding embeddings more for predicting the outlier to be in 2nd place rather than nth when sorting the words by their compactness score c(w).
3-way Outlier Detection. As our tensor-based embeddings encode higher order relationships between words, we introduce a new way to compute c(w) based on groups of 3 words rather than pairs of words. We define the compactness score for a word w to be:

c(w) =

sim(vi1 , vi2 , vi3 ),

vi1 =vw vi2 =vw ,vi1 vi3 =vw ,vi1 ,vi2

where sim(·) denotes similarity between a group of 3 vectors. sim(·) is defined as:

sim(v1, v2, v3) =

13

13

-1

3 ||vi - 3 vj||2

i=1 j=1

We call this evaluation method OD3.
The purpose of OD3 is to evaluate the extent to which an embedding captures 3rd order relationships between words. As we will see in the results of our quantitative experiments, our tensor methods outperform the baselines on OD3, which validates our approach.

This approach can easily be generalized to ODN (N > 3), but again we leave the study of higher order relationships to future work.

Simple supervised tasks. Jastrzebski et al. (2017) points out that the primary application of word embeddings is transfer learning to actual NLP tasks. They argue that to evaluate an embedding's ability to transfer information to a relevant task, one must measure the embedding's accessibility of information for actual downstream tasks. To do so, one must cite the performance of simple supervised tasks as training set size increases, which is commonly done in transfer learning evaluation (Jastrzebski et al. (2017)). If an algorithm using a word embedding performs well with just a small amount of training data, then the information encoded in the embedding is easily accessible.

6

Under review as a conference paper at ICLR 2018
The simple supervised downstream tasks we use to evaluate the embeddings are as follows: 1. Supervised Analogy Recovery. We consider the task of solving queries of the form a : b :: c
: ? using a simple neural network as suggested in Jastrzebski et al. (2017). The analogy dataset we use is from the Google analogy testbed (Mikolov et al. (2013)). 2. Sentiment analysis. We also consider sentiment analysis as described by Schnabel et al. (2015). We use the suggested Large Movie Review dataset (Maas et al. (2011)), containing 50,000 movie reviews. All code is implemented using scikit-learn or TensorFlow and uses the suggested train/test split. Word similarity. To present results on the standard word embedding evaluation technique, we evaluate the embeddings using word similarity on the common MEN and MTurk datasets (Bruni et al. (2014); Radinsky et al. (2011)). For an overview of word similarity evaluation, see Schnabel et al. (2015). QUANTITATIVE RESULTS
Figure 1: Analogy task performance vs. % training data
Figure 2: Sentiment analysis task performance vs. % training data Outlier Detection results. The results are shown in Table 1. The first thing to note is that CP-S outperforms the other methods across each Outlier Detection metric. Since the WikiSem500 dataset
7

Under review as a conference paper at ICLR 2018

Table 1: Outlier Detection scores across all embeddings

(Method) Random CBOW NNSE
CP-S JCP-S

OD2 OPP 0.6123 0.6542 0.6998 0.7078 0.7017

OD2 acc 0.2765 0.3731 0.4288 0.4370 0.4242

OD3 OPP 0.5345 0.6162 0.6292 0.6741 0.6666

OD3 acc 0.1950 0.3034 0.3190 0.3597 0.3201

is semantically focused, performance at this task demonstrates the quality of semantic information encoded in our embeddings.
On OD2, the baselines perform more competitively with our CP Decomposition based models, but when OD3 is considered our methods clearly excel. Since the tensor-based methods are trained directly on third order information and perform much better at OD3, we see that OD3 scores reflect the amount of third order information in a word embedding. This is a validation of OD3, as our 3rd order embeddings would naturally out perform 2nd order embeddings at a task that requires third order information. Still, the superiority of our tensor-based embeddings at OD2 demonstrates the quality of the semantic information they encode.
Supervised analogy results. The results are shown in Figure 1. At the supervised semantic analogy task, CP-S vastly outperforms the baselines at all levels of training data, further signifying the amount of semantic information encoded by this embedding technique.
Further, when only 10% of the training data is presented, our tensor methods are the only ones that attain nonzero performance ­ even in such a limited data setting, use of CP-S's vectors results in nearly 40% accuracy. This phenomenon is also observed in the syntactic analogy tasks: our embeddings consistently outperform the others until 100% of the training data is presented. These two observations demonstrate the accessibility of the information encoded in our word embeddings. We can thus conclude that this relational information encoded in the tensor-based embeddings is more easily accessible than that of CBOW and NNSE. Thus, our methods would likely be better suited for transfer learning to actual NLP tasks, particularly those in data-sparse settings.
Sentiment analysis results. The results are shown in Figure 2. In this task, JCP-S is the dominant method across all levels of training data, but again, the difference is more obvious when training data is limited. This again indicates that for this specific task the information encoded by our tensorbased methods is more readily available as that of the baselines. It is thus evident that exploiting both second and third order co-occurrence data leads to higher quality semantic information being encoded in the embedding. Further theoretical research is needed to understand why JCP-S outperforms CP-S at this task, but its superiority to the other strong baselines demonstrates the quality of information encoded by JCP-S. This discrepancy is also illustrative of the fact that there is no single "best word embedding" (Jastrzebski et al. (2017)) ­ different embeddings encode different types of information, and thus should be used where they shine rather than for every NLP task.
Based on these three semantically-focused NLP tasks (OD2, Supervised analogy recovery, and Sentiment analysis), it is clear that using tensor factorization to create word representations has the propensity to encode a higher level of semantic information than that of pairwise or matrix-based word embedding methods.
Word Similarity results.

Table 2: Word Similarity Scores (Spearman's )

(Method) Random CBOW NNSE
CP-S JCP-S

MEN -0.028 0.601 0.717 0.630 0.621

MTurk -0.150 0.498 0.686 0.631 0.669

8

Under review as a conference paper at ICLR 2018

We show the results in Table 2. As we can see, our embeddings very clearly outperform the random embedding at this task. They even outperform CBOW on both of these datasets. It is worth including these results as the word similarity task is a very common way of evaluating embedding quality in the literature. However, due to the many intrinsic problems with evaluating word embeddings using word similarity (Faruqui et al. (2016)), we do not read further into these results.

MULTIPLICATIVE COMPOSITIONALITY

We find that even though they are not explicitly trained to do so, our tensor-based embeddings capture polysemy information naturally through multiplicative compositionality. We demonstrate this property qualitatively and provide proper motivation for it, leaving automated utilization to future work.
In our tensor-based embeddings, we found that one can create a vector that represents a word w in the context of another word w by taking the elementwise product vw  vw . We call vw  vw a "meaning vector" for the polysemous word w.
For example, consider the word star, which can denote a lead performer or a celestial body. We can create a vector for star in the "lead performer" sense by taking the elementwise product vstar  vactor. This produces a vector that lies near vectors for words related to lead performers and far from those related to star's other senses.
Table 3: Nearest neighbors (in cosine similarity) to elementwise products of word vectors

Composition star  actor star + actor star  planet star + planet tank  fuel tank + fuel tank  weapon tank + weapon

Nearest neighbors (CP-S)
oscar, award-winning, supporting stars, movie, actress
planets, constellation, trek sun, earth, galaxy
liquid, injection, tanks tanks, engines, injection gun, ammunition, tanks
tanks, armor, rifle

Nearest neighbors (JCP-S)
roles, drama, musical actress, trek, picture galaxy, earth, minor galaxy, dwarf, constellation vehicles, motors, vehicle vehicles, tanks, powered brigade, cavalry, battalion tanks, battery, batteries

Nearest neighbors (CBOW)
DNA, younger, tip actress, comedian, starred
fingers, layer, arm galaxy, planets, earth armored, tanks, armoured tanks, engine, diesel persian, age, rapid tanks, cannon, armored

To motivate why this works, recall that the values in a third order PPMI tensor M are given by:
R
mijk = P P M I(wi, wj , wk)  virvjrvkr = vi  vj , vk ,
r=1
where vi is the word vector for wi. If words wi, wj, wk have a high PPMI, then vi  vj, vk will also be high, meaning vi  vj will be close to vk in the vector space by cosine similarity.
For example, even though galaxy is likely to appear in the context of the word star in in the "celestial body" sense, vstar  vactor, vgalaxy  PPMI(star, actor, galaxy) is low whereas vstar  vactor, vdrama  PPMI(star, actor, drama) is high. Thus , vstar  vactor represents the meaning of star in the "lead performer" sense.
In Table 3 we present the nearest neighbors of multiplicative and additive composed vectors for a variety of polysemous words. As we can see, the words corresponding to the nearest neighbors of the composed vectors for our tensor methods are semantically related to the intended sense both for multiplicative and additive composition. In contrast, for CBOW, only additive composition yields vectors whose nearest neighbors are semantically related to the intended sense. Thus, our embeddings can produce complementary sets of polysemous word representations that are qualitatively valid whereas CBOW (seemingly) only guarantees meaningful additive compositionality. We leave automated usage of this property to future work.

CONCLUSION
Our key contributions are as follows:

9

Under review as a conference paper at ICLR 2018
1. Two novel tensor factorization based word embeddings. We presented CP-S and JCP-S, which are word embedding techniques based on symmetric CP decomposition. We experimentally demonstrated that these embeddings outperform existing matrix-based techniques on a number of downstream semantic tasks when trained on the same data.
2. A novel joint symmetric tensor factorization problem. We introduced and utilized Joint Symmetric Rank-R CP Decomposition to train JCP-S. In this problem, multiple supersymmetric tensors must be decomposed using a single rank-R factor matrix. This technique allows for utilization of both second and third order co-occurrence information in word embedding training.
3. A new embedding evaluation metric to measure amount of third order information. We produce a 3-way analogue of Outlier Detection (Camacho-Collados & Navigli (2016)) that we call OD3. This metric evaluates the degree to which third order information is captured by a given word embedding. We demonstrated this by showing our tensor based techniques, which naturally encode third information, perform better at OD3 compared to existing second order models.
4. Word vector multiplicative compositionality for polysemous word representation. We showed that our word vectors can be meaningfully composed multiplicatively to create a "meaning vector" for each different sense of a polysemous word. This property is a consequence of the higher order information used to train our embeddings, and was empirically shown to be unique to our tensor-based embeddings.
Tensor factorization appears to be a highly applicable and effective tool for learning word embeddings, with many areas of potential future work. Leveraging higher order data in training word embeddings is useful for encoding new types of information and semantic relationships compared to models that are trained using only pairwise data. This indicates that such techniques will prove useful for training word embeddings to be used in downstream NLP tasks.
10

Under review as a conference paper at ICLR 2018
REFERENCES
Evrim Acar, Tamara G. Kolda, and Daniel M. Dunlavy. All-at-once optimization for coupled matrix and tensor factorizations. CoRR, abs/1105.3422, 2011. URL http://dblp.uni-trier. de/db/journals/corr/corr1105.html#abs-1105-3422.
Animashree Anandkumar, Rong Ge 0001, Daniel J Hsu, and Sham Kakade. A Tensor Spectral Approach to Learning Mixed Membership Community Models. COLT, 2013. URL https: //arxiv.org/pdf/1302.2684.pdf.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. J. Mach. Learn. Res., 15(1):2773­2832, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=2627435. 2697055.
B. Athiwaratkun and A. G. Wilson. Multimodal Word Distributions. ArXiv e-prints, April 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/ abs/1409.0473.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS'01, pp. 585­591, Cambridge, MA, USA, 2001. MIT Press. URL http://dl.acm.org/citation.cfm?id=2980539.2980616.
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137­1155, March 2003. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=944919.944966.
William Blacoe and Mirella Lapata. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL '12, pp. 546­556, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics. URL http: //dl.acm.org/citation.cfm?id=2390948.2391011.
Philip Blair, Yuval Merhav, and Joel Barry. Automated generation of multilingual clusters for the evaluation of distributed representations. CoRR, abs/1611.01547, 2016. URL http://arxiv. org/abs/1611.01547.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. Multimodal distributional semantics. J. Artif. Int. Res., 49(1):1­47, January 2014. ISSN 1076-9757. URL http://dl.acm.org/citation. cfm?id=2655713.2655714.
John A. Bullinaria and Joseph P. Levy. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3):510­526, 2007. ISSN 15543528. doi: 10.3758/BF03193020. URL http://dx.doi.org/10.3758/BF03193020.
Jose´ Camacho-Collados and Roberto Navigli. Find the word that does not belong: A framework for an intrinsic evaluation of word vector representations. In ACL Workshop on Evaluating Vector Space Representations for NLP, pp. 43­50. Association for Computational Linguistics, 2016.
P. Comon, Y. Qi, and K. Usevich. A polynomial formulation for joint decomposition of symmetric tensors of different orders. In LVA-ICA'2015, volume 9237 of Lecture Notes in Computer Science, Liberec, Czech Republic, August 25-28 2015. Springer. doi: http://link.springer.com/chapter/10. 1007/978-3-319-22482-4 3. Special session on tensors. hal-01168992.
Pierre Comon, Gene Golub, Lek-Heng Lim, and Bernard Mourrain. Symmetric tensors and symmetric tensor rank. SIAM Journal on Matrix Analysis and Applications, 30(3):1254­1279, September 2008. URL https://hal.archives-ouvertes.fr/hal-00327599.
Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. Problems with evaluation of word embeddings using word similarity tasks. CoRR, abs/1605.02276, 2016. URL http: //arxiv.org/abs/1605.02276.
11

Under review as a conference paper at ICLR 2018
James R. Foulds. Mixed membership word embeddings for computational social science. CoRR, abs/1705.07368, 2017. URL http://arxiv.org/abs/1705.07368.
Zellig Harris. Distributional structure. Word, 10(23):146­162, 1954.
Johan Hstad. Tensor rank is np-complete. Journal of Algorithms, 11(4):644 ­ 654, 1990. ISSN 0196-6774. doi: http://dx.doi.org/10.1016/0196-6774(90)90014-6. URL http://www. sciencedirect.com/science/article/pii/0196677490900146.
Stanisaw Jastrzebski, Damian Lesniak, and Wojciech Marian Czarnecki. How to evaluate word embeddings? on importance of data efficiency and simple supervised tasks, 2017. URL https: //arxiv.org/abs/1702.02170.
Yoon Kim. Convolutional neural networks for sentence classification. CoRR, abs/1408.5882, 2014. URL http://arxiv.org/abs/1408.5882.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM REVIEW, 51 (3):455­500, 2009.
Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborova´, and Pan Zhang. Spectral redemption - clustering sparse networks. CoRR, 2013. URL http: //dblp.org/rec/journals/corr/KrzakalaMMNSZZ13.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS'14, pp. 2177­2185, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm. org/citation.cfm?id=2969033.2969070.
Lek-Heng Lim. Optimal solutions to non-negative parafac/multilinear nmf always exist. 2005.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT '11, pp. 142­150, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. ISBN 978-1-932432-87-9. URL http://dl.acm.org/citation.cfm?id=2002472. 2002491.
Takanori Maehara, Kohei Hayashi, and Ken-ichi Kawarabayashi. Expected tensor decomposition with stochastic gradient descent. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pp. 1919­1925. AAAI Press, 2016. URL http://dl.acm.org/ citation.cfm?id=3016100.3016167.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546, 2013. URL http: //dblp.uni-trier.de/db/journals/corr/corr1310.html#MikolovSCCD13.
Jeff Mitchell and Mirella Lapata. Composition in Distributional Models of Semantics. Cognitive Science, 34(8):1388­1429, 2010.
Cun Mu, Daniel Hsu, and Donald Goldfarb. Successive rank-one approximations for nearly orthogonally decomposable symmetric tensors. SIAM Journal on Matrix Analysis and Applications, 36 (4):1638­1659, 2015. doi: 10.1137/15M1010890. URL http://dx.doi.org/10.1137/ 15M1010890.
Brian Murphy, Partha Pratim Talukdar, and Tom M. Mitchell. Learning effective and interpretable semantic models using non-negative sparse embedding. In Martin Kay and Christian Boitet (eds.), COLING, pp. 1933­1950. Indian Institute of Technology Bombay, 2012. URL http://dblp. uni-trier.de/db/conf/coling/coling2012.html#MurphyTM12.
12

Under review as a conference paper at ICLR 2018

Kristina Naskovska and Martin Haardt. Extension of the semi-algebraic framework for approximate CP decompositions via simultaneous matrix diagonalization to the efficient calculation of coupled CP decompositions. In 50th Asilomar Conference on Signals, Systems and Computers, ACSSC 2016, Pacific Grove, CA, USA, November 6-9, 2016, pp. 1728­1732, 2016. doi: 10.1109/ACSSC. 2016.7869678. URL http://dx.doi.org/10.1109/ACSSC.2016.7869678.

I. V. Oseledets. Tensor-train decomposition. SIAM J. Sci. Comput., 33(5):2295­2317, September 2011. ISSN 1064-8275. doi: 10.1137/090752286. URL http://dx.doi.org/10.1137/ 090752286.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532­1543, 2014. URL http://www.aclweb.org/anthology/D14-1162.

Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. A word at a time: Computing word relatedness using temporal semantic analysis. In Proceedings of the 20th International Conference on World Wide Web, WWW '11, pp. 337­346, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0632-4. doi: 10.1145/1963405.1963455. URL http: //doi.acm.org/10.1145/1963405.1963455.

Sebastian Ruder. An overview of word embeddings and their connection to dis-

tributional semantic models.

2016.

URL http://blog.aylien.com/

overview-word-embeddings-history-word2vec-cbow-glove/.

Alexandre Salle, Aline Villavicencio, and Marco Idiart. Matrix factorization using window sampling and negative sampling for improved word representations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers, 2016. URL http://aclweb.org/anthology/ P/P16/P16-2068.pdf.

Tobias Schnabel, Igor Labutov, David M. Mimno, and Thorsten Joachims. Evaluation methods for unsupervised word embeddings. In Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.), EMNLP, pp. 298­307. The Association for Computational Linguistics, 2015. ISBN 978-1-941643-32-7. URL http://dblp.uni-trier.de/db/conf/ emnlp/emnlp2015.html#SchnabelLMJ15.

Vatsal Sharan and Gregory Valiant. Orthogonalized als: A theoretically principled tensor decomposition algorithm for practical use. arXiv preprint arXiv:1703.01804, 2017.

Daniel A Spielman. Spectral Graph Theory and its Applications. In Foundations of Computer Science, 2007. FOCS '07. 48th Annual IEEE Symposium on, pp. 29­38. IEEE, 2007. doi: 10. 1109/FOCS.2007.56. URL http://ieeexplore.ieee.org/xpl/articleDetails. jsp?tp=&arnumber=4389477&contentType=Conference+Publications.

Tim Van de Cruys. A non-negative tensor factorization model for selectional preference induction. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS '09, pp. 83­90, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics. URL http://dl.acm.org/citation.cfm?id=1705415.1705426.

Tim Van de Cruys. Two multivariate generalizations of pointwise mutual information. In Proceedings of the Workshop on Distributional Semantics and Compositionality, DiSCo '11, pp. 16­20, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. ISBN 9781937284022. URL http://dl.acm.org/citation.cfm?id=2043121.2043124.

Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. A tensor-based factorization model of semantic compositionality. In Proc. of NAACL-HLT, pp. 1142­1151, 2013.

Yiju Wang and Liqun Qi. On the successive supersymmetric rank-1 decomposition of higher-order supersymmetric tensors. Numerical Lin. Alg. with Applic., 14(6):503­519, 2007. doi: 10.1002/ nla.537. URL http://dx.doi.org/10.1002/nla.537.

Jingwei Zhang, Jeremy Salwen, Michael R Glass, and Alfio Massimiliano Gliozzo. Word semantic representations using bayesian probabilistic tensor factorization. 2014.

13

