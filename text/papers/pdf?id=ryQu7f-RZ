Under review as a conference paper at ICLR 2018
ON THE CONVERGENCE OF ADAM AND BEYOND
Anonymous authors Paper under double-blind review
ABSTRACT
Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.
1 INTRODUCTION
Stochastic gradient descent (SGD) is the dominant method to train deep networks today. This method iteratively updates the parameters of a model by moving them in the direction of the negative gradient of the loss evaluated on a minibatch. In particular, variants of SGD that scale coordinates of the gradient by square roots of some form of averaging of the squared coordinates in the past gradients have been particularly successful, because they automatically adjust the learning rate on a per-feature basis. The first popular algorithm in this line of research is ADAGRAD (Duchi et al., 2011; McMahan & Streeter, 2010), which can achieve significantly better performance compared to vanilla SGD when the gradients are sparse, or in general small.
Although ADAGRAD works well for sparse settings, its performance has been observed to deteriorate in settings where the loss functions are nonconvex and gradients are dense due to rapid decay of the learning rate in these settings since it uses all the past gradients in the update. This problem is especially exacerbated in high dimensional problems arising in deep learning. To tackle this issue, several variants of ADAGRAD, such as RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma & Ba, 2015), ADADELTA (Zeiler, 2012), NADAM (Dozat, 2016), etc, have been proposed which mitigate the rapid decay of the learning rate using the exponential moving averages of squared past gradients, essentially limiting the reliance of the update to only the past few gradients. While these algorithms have been successfully employed in several practical applications, they have also been observed to not converge in some other settings. It has been typically observed that in these settings some minibatches provide large gradients but only quite rarely, and while these large gradients are quite informative, their influence dies out rather quickly due to the exponential averaging, thus leading to poor convergence.
In this paper, we analyze this situation in detail. We rigorously prove that the intuition conveyed in the above paragraph is indeed correct; that limiting the reliance of the update on essentially only the past few gradients can indeed cause significant convergence issues. In particular, we make the following key contributions:
· We elucidate how the exponential moving average in the RMSPROP and ADAM algorithms can cause non-convergence by providing an example of simple convex optimization problem where RMSPROP and ADAM provably do not converge to an optimal solution. Our analysis easily extends to other algorithms using exponential moving averages such as ADADELTA and NADAM as well, but we omit this for the sake of clarity. In fact, the
1

Under review as a conference paper at ICLR 2018

analysis is flexible enough to extend to other algorithms that employ averaging squared gradients over essentially a fixed size window (for exponential moving averages, the influences of gradients beyond a fixed window size becomes negligibly small) in the immediate past. We omit the general analysis in this paper for the sake of clarity.
· The above result indicates that in order to have guaranteed convergence the optimization algorithm must have "long-term memory" of past gradients. Specifically, we point out a problem with the proof of convergence of the ADAM algorithm given by Kingma & Ba (2015). To resolve this issue, we propose new variants of ADAM which rely on long-term memory of past gradients, but can be implemented in the same time and space requirements as the original ADAM algorithm. We provide a convergence analysis for the new variants in the convex setting, based on the analysis of Kingma & Ba (2015), and show a datadependent regret bound similar to the one in ADAGRAD.
· We provide a preliminary empirical study of one of the variants we proposed and show that it either performs similarly, or better, on some commonly used problems in machine learning.

2 PRELIMINARIES

Notation. We use Sd+ to denote the set of all positive definite d × d matrices. With slight abuse of

notation, for a vector a  Rd and a M -1a and M to represent M 1/2.

positive definite Furthermore, for

matrix M  Rd any vectors a, b

×RRdd,,wweeuusseeaa/Mforteoledmeneontte-

wise square root, a2 for element-wise square, a/b to denote element-wise division and max(a, b)

to denote element-wise maximum. For any vector i  Rd, i,j denotes its jth coordinate where

j  [d]. The projection operation F,A(y) for A  S+d is defined as arg minxF A1/2(x - y) for

y  Rd. Finally, we say F has bounded diameter D if x - y   D for all x, y  F .

Optimization setup. A flexible framework to analyze iterative optimization methods is the online

optimization problem in the full information feedback setting. In this online setup, at each time

step t, the optimization algorithm picks a point (i.e. the parameters of the model to be learned)

xt  F , where F  Rd is the feasible set of points. A loss function ft (to be interpreted as

the loss of the model with the chosen parameters in the next minibatch) is then revealed, and the

algorithm incurs loss ft(xt). The algorithm's regret at the end of T rounds of this process is given

by RT =

T i=1

ft(xt)

-

minxF

T i=1

ft(x).

Our aim to is to devise an algorithm that ensures RT = o(T ), which implies that on average, the model's performance converges to the optimal one. The simplest algorithm for this setting is
the standard online gradient descent algorithm (Zinkevich, 2003), which moves the point xt in the opposite direction of the gradient gt = ft(xt) while maintaining the feasibility by projecting onto the set F via the update rule xt+1 = F (xt - tgt), where F (y) denotes theprojection of y  Rd onto the set F i.e., F (y) = minxF x - y , and t is typically set to / t for some constant . The aforementioned online learning problem is closely related to the empirical risk minimization
(ERM): minxF Ez[f (x, z)], where z is a training example drawn training sample over which a model with parameters x is to be learned, and f (x, z) is the loss of the model with parameters x on
the sample z. In particular, an online optimization algorithm with vanishing average regret yields a
stochastic optimization algorithm for the ERM problem (Cesa-Bianchi et al., 2004). Thus, we use
online gradient descent and stochastic gradient descent (SGD) synonymously.

Generic adaptive methods setup. We now provide a framework of adaptive methods that gives
us insights into the differences between different adaptive methods and is useful for understanding
the flaws in a few popular adaptive methods. Algorithm 1 provides a generic adaptive framework
that encapsulates many popular adaptive methods. Note the algorithm is still abstract because the "averaging" functions t and t have not been specified. Here t : F t  Rd and t : F t  S+d . For ease of exposition, we refer to t as step size and tVt-1/2 as learning rate of the algorithm and furthermore, restrict ourselves to diagonal variants of adaptive methods encapsulated by Algorithm 1
where Vt = diag(vt) . We first observe that standard stochastic gradient algorithm falls in this

2

Under review as a conference paper at ICLR 2018

Algorithm 1 Generic Adaptive Method Setup

Input: x1  F , step size {t > 0}Tt=1, sequence of functions {t, t}tT=1

for t = 1 to T do

gt = ft(xt)

mt = t(g1, . . . , gt)and Vt = t(g1  g1, . . . , gt  gt)

x^t+1 = xt - tmt/ Vt

xt+1

=

F

 ,V

t

(x^t+1

)

end for

framework by using:

t(g1, . . . , gt) = gt and t(g1, . . . , gt) = I,

(SGD)



and t = / t for all t  [T ]. While the decreasing step size is required for convergence, such an

aggressive decay of learning rate typically translates into poor empirical performance. The key idea

of adaptive methods is to choose averaging functions appropriately so as to entail good convergence.

For instance, the first adaptive method ADAGRAD (Duchi et al., 2011), which propelled the research

on adaptive methods, uses the following averaging functions:

diag( t(g1, . . . , gt) = gt and t(g1, . . . , gt) =

t i=1

gi2)

,

t

(ADAGRAD)

 and step size t = / t for all t  [T ]. In contrast to a learning rate of / t in SGD, such a setting

effectively implies a modest learning rate decay of / i gi2,j for j  [d]. When the gradients are

sparse, this can potentially lead to huge gains in terms of convergence (see Duchi et al. (2011)).

These gains have also been observed in practice for even few non-sparse settings.

Adaptive methods based on Exponential Moving Averages. Exponential moving average variants of ADAGRAD are popular in the deep learning community. RMSPROP, ADAM, NADAM, and ADADELTA are some prominent algorithms that fall in this category. The key difference is to use an exponential moving average as function t instead of the simple average function used in ADAGRAD. ADAM1, a particularly popular variant, uses the following averaging functions:

tt
t(g1, . . . , gt) = (1 - 1) 1t-igi and t(g1, . . . , gt) = (1 - 2)diag( 2t-igi2), (ADAM)
i=1 i=1

for some 1, 2  [0, 1). This update can alternatively be stated by the following simple recursion:

mt,i = 1mt-1,i + (1 - 1)gt,i and vt,i = 2vt-1,i + (1 - 2)gt2,i

(1)

and m0,i = 0 and v0,i = 0 for all i  [d]. and t  [T ]. Also, note the additional projection operation in Algorithm 1 in comparison to ADAM. When F = Rd, the projection operation is an identity operation and this corresponds to the algorithm in (Kingma & Ba, 2015). For theoretical analysis, one requires t = 1/ t for t  [T ], although, a more aggressive choice of constant step size seems to work well in practice. RMSPROP, which appeared in an earlier unpublished work
(Tieleman & Hinton, 2012) is essentially a variant of ADAM with 1 = 0. In practice, especially in deep learning applications, the momentum term arising due to non-zero 1 appears to significantly boost the performance. We will mainly focus on ADAM algorithm due to this generality but our
arguments also apply to RMSPROP.

3 THE NON-CONVERGENCE OF ADAM
With the problem setup in the previous section, we discuss fundamental flaw in the current exponential moving average methods like ADAM. We show that ADAM can fail to converge to an optimal
1Here, for simplicity, we remove the debiasing step used in the version of ADAM used in the original paper by Kingma & Ba (2015). However, our arguments also apply to the debiased version as well.

3

Under review as a conference paper at ICLR 2018

solution even in simple one-dimensional convex settings. These examples of non-convergence con-

tradict the claim of convergence in (Kingma & Ba, 2015), and the main issue lies in the following

quantity of interest:

t+1 =



Vt+1 - Vt

t+1

t

.

(2)

This quantity essentially measures the change in the inverse of learning rate of the adaptive method with respect to time. One key observation is that for SGD and ADAGRAD, t 0 for all t  [T ]. This simply follows from update rules of SGD and ADAGRAD in the previous section. In
particular, update rules for these algorithms lead to "non-increasing" learning rates. However, this
is not necessarily the case for exponential moving average variants like ADAM and RMSPROP i.e., t can potentially be indefinite for t  [T ] . We show that this violation of positive definiteness can lead to undesirable convergence behavior for ADAM and RMSPROP. Consider the following simple sequence of linear functions for F = [-1, 1]:

ft(x) =

C x, -x,

for t mod 3 = 1 otherwise,

where C > 2. For this function sequence, it is easy to see that the point x = -1 provides the minimum regret. The following result shows that when 1 = 0 and 2 = 1/(1 + C2), ADAM converges to a highly suboptimal solution of x = +1. Intuitively, the reasoning is as follows. The
algorithm obtains the large gradient C once every 3 steps, and while the other 2 steps it observes the gradient -1, which moves the algorithm in the wrong direction. The large gradient C is unable to
counteract this effect since it is scaled down by a factor of almost C for the given value of 2, and hence the algorithm converges to 1 rather than -1.

Below we will also present a more general result for general constant 1 and 2. We note here that while these examples use constant 1 and 2, the proof of convergence of ADAM in (Kingma & Ba, 2015) actually relies on decreasing 1 over time. It is quite easy to extend our examples to the case where 1 is decreased over time, since the critical parameter is 2 rather than 1, and as long as 2 is bounded away from 1, our analysis goes through. For the sake of clarity, we only analyze the setting where 1 is held constant (as is routinely done in practice) and prove non-convergence of ADAM.
Theorem 1. ADAM with parameter setting such that all the conditions in (Kingma & Ba, 2015) are satisfied can have non-zero average regret i.e., RT /T 0 as T   for convex {fi}i=1 with bounded gradients on a feasible set F having bounded D diameter.

We relegate all proofs to the appendix. A few remarks are in order. One might wonder if adding a small constant in the denominator of the update helps in circumventing this problem i.e., the update for ADAM in Algorithm 1 of x^t+1 is modified as follows:

x^t+1 = xt - tmt/ Vt + I.

(3)

The algorithm in (Kingma & Ba, 2015) uses such an update in practice, although their analysis does not. In practice, selection of the parameter appears to be critical for the performance of the algorithm. However, we show that for any constant > 0, there exists an online optimization setting where, again, ADAM has non-zero average regret asymptotically (see Theorem 6 in Section B of the appendix).

The above examples of non-convergence are catastrophic insofar that ADAM and RMSPROP con-

verge to a point that is worst amongst all points in the set [-1, 1]. Note that above example also

holds for constant step size t = . Also note that classic SGD and ADAGRAD do not suffer from

this problem and for these algorithms, average regret asymptotically goes to 0. This problem is espe-

cially aggravated in high dimensional settings and when the variance of the gradients with respect to

time is large. This example also provides intuition for why large 2 is advisable while using ADAM

algorithm, and indeed in practice using large 2 helps. However the following result shows that for any constant 1 and 2 with 1 < 2, we can design an example where ADAM has non-zero

average rate asymptotically.



Theorem 2. For any constant 0  1, 2  1 such that 1 < 2, ADAM with parameter setting

such that all the conditions in (Kingma & Ba, 2015) are satisfied can have non-zero average regret
i.e., RT /T 0 as T   for convex {fi}i=1 with bounded gradients on a feasible set F having bounded D diameter.

4

Under review as a conference paper at ICLR 2018

Algorithm 2 AMSGRAD

Input: x1  F , step size {t}Tt=1, {1t}tT=1, 2

Set m0 = 0 and v0 = 0

for t = 1 to T do

gt = ft(xt)

mt = 1tmt-1 + (1 - 1t)gt

vt = 2vt-1 + (1 - 2)gt2

v^t = xt+1

max(v^t-1, vt)

=

 F


, V^t

(xt

and V^t =diag(v^t) - tmt/ v^t)

end for

The above results show that with constant 1 and 2, momentum or regularization via epsilonwill help in convergence of the algorithm to the optimal solution. Note that the condition 1 < 2 is benign and is typically satisfied in the parameter settings used in practice. Furthermore, such condition is typically assumed for convergence of the algorithm (see (Kingma & Ba, 2015)). We can strengthen this result by providing a similar example of non-convergence even in the easier stochastic optimization setting:
 Theorem 3. For any constant 0  1, 2  1 such that 1 < 2, there is a stochastic convex optimization problem for which ADAM, with parameter settings such that all the conditions in (Kingma & Ba, 2015) are satisfied, does not converge to the optimal solution.
These results have important consequences insofar that one has to use "problem-dependent" , 1 and 2 in order to avoid bad convergence behavior. In high-dimensional problems, this typically amounts to using, unlike the update in Equation (3), a different , 1 and 2 for each dimension. However, this defeats the purpose of adaptive methods since it requires tuning a large set of parameters. We would also like to emphasize that while the example of non-convergence is carefully constructed to demonstrate the problems in ADAM, it is not unrealistic to imagine scenarios where such an issue can at the very least slow down convergence.
4 A NEW EXPONENTIAL MOVING AVERAGE VARIANT: AMSGRAD
In this section, we develop a new principled exponential moving average variant and provide its convergence analysis. Our aim is to devise a new strategy with guaranteed convergence while preserving the practical benefits of ADAM and RMSPROP. To understand the design of our algorithms, let us revisit the quantity t in (2). For ADAM and RMSPROP, this quantity can potentially be negative. The proof in the original paper of ADAM erroneously assumes that t is positive semi-definite and is hence, incorrect (refer to Appendix E for more details). For the first part, we modify these algorithms to satisfy this additional constraint. Later on, we also explore an alternative approach where t can be made positive semi-definite by using values of 1 and 2 that change with t.
AMSGRAD uses a smaller learning rate in comparison to ADAM and yet incorporates the intuition of slowly decaying the effect of past gradients on the learning rate as long as t is positive semidefinite. Algorithm 2 presents the pseudocode for the algorithm. The key difference of AMSGRAD with ADAM is that it maintains the maximum of all vt until the present time step and uses this maximum value for normalizing the running average of the gradient instead of vt in ADAM. By doing this, AMSGRAD results in a non-increasing step size and avoids the pitfalls of ADAM and RMSPROP i.e., t 0 for all t  [T ] even with constant 2. Also, in Algorithm 2, one typically uses a constant 1t in practice (although, the proof requires a decreasing schedule for proving convergence of the algorithm).
To gain more intuition for the updates of AMSGRAD, it is instructive to compare its update with ADAM and ADAGRAD. Suppose at particular time step t and coordinate i  [d], we have vt-1,i > gt2,i > 0, then ADAM aggressively increases the learning rate, however, as we have seen in the previous section, this can be detrimental to the overall performance of the algorithm. On the other hand, ADAGRAD slightly decreases the learning rate, which often leads to poor performance in practice since such an accumulation of gradients over a large time period can significantly decrease the learning rate. In contrast, AMSGRAD neither increases nor decreases the learning rate and
5

Under review as a conference paper at ICLR 2018

Figure 1: Performance comparison of ADAM and AMSGRAD on synthetic example on a simple one dimensional convex problem inspired by our examples of non-convergence. The first two plots (left and center) are for the online setting and the the last one (right) is for the stochastic setting.

furthermore, decreases vt which can potentially lead to non-decreasing learning rate even if gradient is large in the future iterations. We prove the following key result for AMSGRAD algorithm.
 Theorem 4. Let {xt} and {vt} be the sequences obtained from Algorithm 2, t = / t, 1 = 11, 1t  1 for all t  [T ] and  = 1/ 2 < 1. Assume that F has bounded diameter D and ft(x)   G for all t  [T ] and x  F . For xt generated using the AMSGRAD
(Algorithm 2), we have the following bound on the regret

RT



 D2 T (1 - 1)

d i=1

v^T1/,i2+

D2 2(1 - 1)

T t=1

d i=1

1tv^t1,/i2 +

  1 + log T

t (1 - 1)2(1 - ) (1 - 2)

d i=1

g1:T,i 2.

The following result falls as an immediate corollary of the above result.

Corollary 1. Suppose 1t = 1t-1 in Theorem 4, then we have

RT



 D2 T (1 - 1)

d
v^T1/,i2
i=1

+

2(1

1D2 G - 1)(1 -

)2

+

(1

  1 + log T - 1)2(1 - ) (1 -

2)

d i=1

g1:T ,i

2.

 The above bound can be considerably better than O( T ) regret of SGD when

d i=1

v^T1/,i2

dG

and

d i=1

g1:T,i 2

dG. Furthermore, in Theorem 4, one can use a much more modest mo-

mentum decay of 1t = 1/t and still ensure a regret of O( T ). We would also like to point out

that one could consider taking simple average of smoothed gradients instead of using the maximum

of all the previous values of vt. The resulting algorithm is very similar to ADAGRAD except for nor-

malization with smoothed gradients rather than actual gradients and can be shown to have similar

convergence as ADAGRAD.

5 EXPERIMENTS
In this section, we present empirical results on both synthetic and real-world datasets. For our experiments, we study the problem of multiclass classification using logistic regression and neural networks, representing convex and nonconvex settings, respectively.
Synthetic Experiments: To demonstrate the convergence issue of ADAM, we first consider the following simple convex setting inspired from our examples of non-convergence:
1010x, for t mod 101 = 1 ft(x) = -10x, otherwise,
with the constraint set F = [-1, 1]. We first observe that, similar to the examples of nonconvergence we have considered, the optimal solution is x = -1; thus, for convergence, we expect the algorithms to converge to x = -1. For this sequence of functions, we investigate the regret and the value of the iterate xt for ADAM and AMSGRAD. To enable fair comparison, we set 1 = 0.9 and 2 = 0.99 for ADAM and AMSGRAD algorithm, which are typically the parameters settings used for ADAM in practice. Figure 1 shows the average regret (Rt/t) and value of the iterate (xt) for

6

Under review as a conference paper at ICLR 2018

Figure 2: Performance comparison of ADAM and AMSGRAD for logistic regression, feedforward neural network and CIFARNET. The top row shows performance of ADAM and AMSGRAD on logistic regression (left and center) and 1-hidden layer feedforward neural network (right) on MNIST. In the bottom row, the two plots compare the training and test loss of ADAM and AMSGRAD with respect to the iterations for CIFARNET.

this problem. We first note that the average regret of ADAM does not converge to 0 with increasing t. Furthermore, its iterates xt converge to x = 1, which unfortunately has the largest regret amongst all points in the domain. On the other hand, the average regret of AMSGRAD converges to 0 and its iterate converges to the optimal solution. Figure 1 also shows the stochastic optimization setting:

ft(x) =

1010x, -10x,

with probability 0.01 otherwise.

Similar to the aforementioned online setting, the optimal solution for this problem is x = -1. Again, we see that the iterate xt of ADAM converges to the highly suboptimal solution x = 1.
Logistic Regression: To investigate the performance of the algorithm on convex problems, we compare AMSGRAD with ADAM on logistic regression problem. We use MNIST dataset for this experiment, the classification is based on 784 dimensional image vector to one of the 10 class labels. The step size parameter t is set to / t for both ADAM and AMSGRAD in for our experiments, consistent with the theory. We use a minibatch version of these algorithms with minibatch size set to 128. We set 1 = 0.9 and 2 is chosen from the set {0.99, 0.999}, but they are fixed throughout the experiment. The parameters  and 2 are chosen by grid search. We report the train and test loss with respect to iterations in Figure 2. We can see that AMSGRAD performs better than ADAM with respect to both train and test loss. We also observed that AMSGRAD is relatively more robust to parameter changes in comparison to ADAM.
Neural Networks: For our first experiment, we trained a simple 1-hidden fully connected layer neural network for the multiclass classification problem on MNIST. Similar to the previous experiment, we use 1 = 0.9 and 2 is chosen from {0.99, 0.999}. We use a fully connected 100 rectified linear units (ReLU) as the hidden layer for this experiment. Furthermore, we use constant t =  throughout all our experiments on neural networks. Such a parameter setting choice of ADAM is consistent with the ones typically used in the deep learning community for training neural networks. A grid search is used to determine parameters that provides the best performance for the algorithm.
Finally, we consider the multiclass classification problem on the standard CIFAR-10 dataset, which consists of 60,000 labeled examples of 32 × 32 images. We use CIFARNET, a convolutional neural network (CNN) with several layers of convolution, pooling and non-linear units, for training a multiclass classifer for this problem. In particular, this architecture has 2 convolutional layers with 64 channels and kernel size of 6 × 6 followed by 2 fully connected layers of size 384 and 192. The network uses 2 × 2 max pooling and layer response normalization between the convolutional layers (Krizhevsky et al., 2012). A dropout layer with keep probability of 0.5 is applied in between the fully connected layers (Srivastava et al., 2014). The minibatch size is also set to 128 similar to previous experiments. The results for this problem are reported in Figure 2. The parameters for ADAM and AMSGRAD are selected in a way similar to the previous experiments. We can see that

7

Under review as a conference paper at ICLR 2018

AMSGRAD performs considerably better than ADAM on train loss and accuracy. Furthermore, this performance gain also translates into good performance on test loss.

5.1 EXTENSION: ADAMNC ALGORITHM

An alternative approach is to use an increasing schedule of 2 in ADAM. This approach, unlike
Algorithm 2 does not require changing the structure of ADAM but rather uses a non-constant 1 and
2. The pseudocode for the algorithm, ADAMNC, is provided in the appendix (Algorithm 3). We
show that by appropriate selection of 1t and 2t, we can achieve good convergence rates. Theorem 5. Let {xt} and {vt} be the sequences obtained from Algorithm 3, t = / t, 1 = 11 and 1t  1 for all t  [T ]. Assume that F has bounded diameter D and ft(x)   G for all t  [T ] and x  F. Furthermore, let {2t} be such that the following conditions are satisfied:

1.

1 T

t j=1

kt-=j12(t-k+1)(1

-

2j )gj2,i



1 

j  [d].

t j=1

gj2,i

for

some



>

0

and

all

t



[T ],

2.

vt1,/i2 t



vt1-/21,i t-1

for all t  {2, · · ·

, T } and i  [d].

Then for xt generated using the ADAMNC (Algorithm 3), we have the following bound on the regret

RT



D2 2(1 - 1)

d

 T

vT1/,i2

+

i=1

D2 2(1 - 1)

T t=1

d i=1

1tvt1,/i2 t

+

2 (1 - 1)3

d i=1

g1:T,i 2.

The above result assumes selection of {(t, 2t)} such that t 0 for all t  {2, · · · , T }. However, one can generalize the result to deal with the case where this constraint is violated as long as the
violation is not too large or frequent. Following is an immediate consequence of the above result.

Corollary 2. Suppose 1t = 1t-1 and 2t = 1 - 1/t in Theorem 5, then we have

D2

d

2(1 - 1) i=1

g1:T ,i

2

+

2(1

1D2 G - 1)(1 -

)2

+

(1

2 - 1)3

d i=1

g1:T,i 2.

The above corollary follows from a trivial fact that vt,i =

t j=1

gj2,i

/t

for

all

i



[d] when

2t = 1 - 1/t. This corollary is interesting insofar that such a parameter setting effectively yields

a momentum based can be considerably

variant of ADAGRA better than O( T )

D. Similar regret of S

to
GD

ADAGR when

AD, the regret

d i=1

g1:T,i 2

is

data-dependent and dG. It is easy to

generalize this result for setting similar settings of 2t. Similar to Corollary 1, one canagain use a

much more modest decay of 1t = 1/t and still ensure a data-dependent regret of O( T ).

6 DISCUSSION
In this paper, we study exponential moving variants of ADAGRAD and identify an important flaw in these algorithms which can lead to undesirable convergence behavior. We demonstrate these problems through carefully constructed examples where RMSPROP and ADAM converge to highly suboptimal solutions. In general, any algorithm that relies on an essentially fixed sized window of past gradients to scale the gradient updates will suffer from this problem.
We proposed fixes to this problem by slightly modifying the algorithms, essentially endowing the algorithms with a long-term memory of past gradients. These fixes retain the good practical performance of the original algorithms, and in some cases actually show improvements.
The primary goal of this paper is to highlight the problems with popular exponential moving average variants of ADAGRAD from a theoretical perspective. RMSPROP and ADAM have been immensely successful in development of several state-of-the-art solutions for a wide range of problems. Thus, it is important to understand their behavior in a rigorous manner and be aware of potential pitfalls while using them in practice. We believe this paper is a first step in this direction and suggests good design principles for faster and better stochastic optimization.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Peter Auer and Claudio Gentile. Adaptive and self-confident on-line learning algorithms. In Proceedings of the 13th Annual Conference on Learning Theory, pp. 107­117, 2000.
Nicol Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50:2050­2057, 2004.
Timothy Dozat. Incorporating Nesterov Momentum into Adam. In Proceedings of 4th International Conference on Learning Representations, Workshop Track, 2016.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of 3rd International Conference on Learning Representations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097­ 1105, 2012.
H. Brendan McMahan and Matthew J. Streeter. In Proceedings of the 23rd Annual Conference On Learning Theory, pp. 244­256, 2010.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014.
T. Tieleman and G. Hinton. RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701, 2012. Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning, pp. 928­936, 2003.
9

Under review as a conference paper at ICLR 2018

APPENDIX

A PROOF OF THEOREM 1

Proof. We consider the setting where ft are linear functions and F = [-1, 1]. In particular, we define the following function sequence:

Cx, for t mod 3 = 1 ft(x) = -x, otherwise,

where C  2. For this function sequence, it is easy to see that the point x = -1 provides the minimum regret. Without loss of generality, assume that the initial point is x1 = 1. Also, since the problem is one-dimensional, we drop indices representing coordinates from all quantities in Algorithm 1. Consider the execution of ADAM algorithm for this sequence of functions with

1

=

0, 2

=

1

1 + C2

and

t

=

 t



where  < 1 - 2.Note that since gradients of these functions are bounded, F has bounded L diameter and 12/ 2 < 1. Hence, the conditions on the parameters required for ADAM are

satisfied (refer to (Kingma & Ba, 2015) for more details).

Our main claim is that for iterates {xt}t=1 arising from the updates of ADAM, we have xt > 0 for all t  N and furthermore, x3t+1 = 1 for all t  N  {0}. For proving this, we resort to the principle of mathematical induction. Since x1 = 1, both the aforementioned conditions hold for the base case. Suppose for some t  N  {0}, we have xi > 0 for all i  [3t + 1] and x3t+1 = 1. Our aim is to prove that x3t+2 and x3t+3 are positive and x3t+4 = 1. We first observe that the gradients have
the following form:

fi(x) =

C, -1,

for i mod 3 = 1 otherwise

From (3t + 1)th update of ADAM in Equation (1), we obtain

x^3t+2 = x3t+1 -

C = 1 - (3t + 1)(2v3t + (1 - 2)C2)

C .
(3t + 1)(2v3t + (1 - 2)C2)

The equality follows from the induction hypothesis. We observe the following:

C  C

(3t + 1)(2v3t + (1 - 2)C2)

(3t + 1)(1 - 2)C2

 = < 1.
(3t + 1)(1 - 2))

(4)

 The second inequality follows from the step size choice that  < 1 - 2. Therefore, we have 0 < x^3t+2 < 1 and hence x3t+2 = x^3t+2 > 0. Furthermore, after the (3t + 2)th and (3t + 3)th

updates of ADAM in Equation (1), we have the following:

x^3t+3 = x3t+2 + x^3t+4 = x3t+3 +

 ,
(3t + 2)(2v3t+1 + (1 - 2))  .
(3t + 3)(2v3t+2 + (1 - 2))

Since x3t+2 > 0, it is easy to see that x3t+3 > 0. To complete the proof, we need to show

that x3t+4 = 1. In order to prove this claim, we show that x^3t+4  1, which readily translates

to x3t+4 = 1 because x3t+4 = F (x^3t+4) and F = [-1, 1] here F is the simple Euclidean

projection

(note

that

in

one-dimension,

F

 , Vt

=

F ).

We

observe

the

following:



x^3t+4 = min(x^3t+3, 1) +

. (3t + 3)(2v3t+2 + (1 - 2))

The above equality is due to the fact that x^3t+3 > 0 and property of projection operation onto the set F = [-1, 1]. We consider the following two cases:

10

Under review as a conference paper at ICLR 2018

1. Suppose x^3t+3  1, then it is easy to see from the above equality that x^3t+4 > 1.

2. Suppose x^3t+3 < 1, then we have the following:

x^3t+4 = x^3t+3 + = x3t+2 +


(3t + 3)(2v3t+2 + (1 - 2))  +
(3t + 2)(2v3t+1 + (1 - 2))

 (3t + 3)(2v3t+2 + (1 - 2))

=1-

C +
(3t + 1)(2v3t + (1 - 2)C2)


(3t + 2)(2v3t+1 + (1 - 2)) 
+. (3t + 3)(2v3t+2 + (1 - 2))

The third equality is due to the fact that x3t+2 = x^3t+2. Thus, to prove x^3t+4 > 1, it is enough to the prove:

C  (3t + 1)(2v3t + (1 - 2)C2)
T1

 (3t + 2)(2v3t+1 + (1 - 2))
 +
(3t + 3)(2v3t+2 + (1 - 2))
T2

We have the following bound on term T1 from Equation (4):



T1 

. (3t + 1)(1 - 2))

(5)

Furthermore, we lower bound T2 in the following manner:

T2 = 



+

(3t + 2)(2v3t+1 + (1 - 2))

(3t + 3)(2v3t+2 + (1 - 2))

  1 + 1 2C2 + (1 - 2) 3t + 2 3t + 3

 11

+

2C2 + (1 - 2) 2(3t + 1)

2(3t + 1)



=

2 =
(3t + 1)(2C2 + (1 - 2))

(3t +

 1)(1

- 2)



T1.

(6)

The first inequality is due to the fact that vt  C2 for all t  N. The last inequality follows from inequality in Equation (5). The last equality is due to following fact:

2C2 + (1 - 2) = 2

1 - 2

for the choice of 2 = 1/(1 + C2). Therefore, we have T2  T1 and hence, x^3t+4  1.

Therefore, from both the cases, we see that x3t+4 = 1. Therefore, by the principle of mathematical induction it holds for all t  N  {0}. Thus, we have
f3t+1(x3t+1)+f3t+2(x3t+2)+f3t+2(x3t+2)-f3t+1(-1)-f3t+2(-1)-f3t+3(-1)  2C-4 = 2C-4.
Therefore, for every 3 steps, ADAM suffers a loss of at least 2C - 4. More specifically, RT  (2C - 4)T /3. Since C  2, this loss can be very large and furthermore, RT /T 0 as T  , which completes the proof.

11

Under review as a conference paper at ICLR 2018

B PROOF OF THEOREM 6

Theorem 6. For any > 0, ADAM with the modified update in Equation (3) and with parameter
setting such that all the conditions in (Kingma & Ba, 2015) are satisfied can have non-zero average regret i.e., RT /T 0 as T   for convex {fi}i=1 with bounded gradients on a feasible set F having bounded D diameter.

Proof. Let us first consider the case where = 1 (in fact, the same setting works for any 1). The general case can be proved by simply rescaling the sequence of functions by a factor of . We show that the same optimization setting in Theorem 1 where ft are linear functions and F = [-1, 1], hence, we only discuss the details that differ from the proof of Theorem 1. In particular, we define
the following function sequence:

Cx, for t mod 3 = 1 ft(x) = -x, otherwise,

where C  2. Similar to the proof of Theorem 1, we assume that the initial point is x1 = 1 and the

parameters are:

1

=

0, 2

=

2 (1 + C2)C2

and

t

=

 t



where  < 1 - 2. The proof essentially follows along the lines of that of Theorem 1 and is

through principle of mathematical induction. Our aim is to prove that x3t+2 and x3t+3 are positive and x3t+4 = 1. The base case holds trivially. Suppose for some t  N  {0}, we have xi > 0 for all i  [3t + 1] and x3t+1 = 1. For (3t + 1)th update, the only change from the update of in Equation (1)

is the additional in the denominator i.e., we have

x^3t+2 = x3t+1 -

C (3t + 1)(2v3t + (1 - 2)C2 + )

C

1-

 0.

(3t + 1)(2v3t + (1 - 2)C2)

 The last inequality follows by simply dropping v3t term and using the relation that  < 1 - 2. Therefore, we have 0 < x^3t+2 < 1 and hence x3t+2 = x^3t+2 > 0. Furthermore, after the (3t + 2)th and (3t + 3)th updates of ADAM in Equation (1), we have the following:

x^3t+3 = x3t+2 + x^3t+4 = x3t+3 +

 ,
(3t + 2)(2v3t+1 + (1 - 2) + )  .
(3t + 3)(2v3t+2 + (1 - 2) + )

Since x3t+2 > 0, it is easy to see that x3t+3 > 0. To complete the proof, we need to show that x3t+4 = 1. The only change here from the proof of Theorem 1 is that we need to show the following:



+

(3t + 2)(2v3t+1 + (1 - 2) + )

(3t + 3)(2v3t+2 + (1 - 2) + )

 2C2 + (1 - 2) +

 1 + 1 3t + 2 3t + 3



11 +

2C2 + (1 - 2) + 

2(3t + 1) 2(3t + 1)

2 C

==

(3t + 1)(2C2 + (1 - 2) + )

(3t + 1)((1 - 2)C2 + )

 C . (3t + 1)(2v3t + (1 - 2)C2 + )

(7)

12

Under review as a conference paper at ICLR 2018

The first inequality is due to the fact that vt  C2 for all t  N. The last equality is due to following fact:

2C2 + (1 - 2) = 2

1 - 2 + C2 .

for the choice of 2 = 2/[(1 + C2)C2] and = 1. Therefore, we see that x3t+4 = 1. Therefore, by the principle of mathematical induction it holds for all t  N  {0}. Thus, we have

f3t+1(x3t+1)+f3t+2(x3t+2)+f3t+2(x3t+2)-f3t+1(-1)-f3t+2(-1)-f3t+3(-1)  2C-4 = 2C-4.

Therefore, for every 3 steps, ADAM suffers a loss of at least 2C - 4. More specifically, RT  (2C - 4)T /3. Since C  2, this loss can be very large and furthermore, RT /T 0 as T  ,

which completes the proof of the case where = 1. For the general case, we consider the following

sequence of functions:



ft(x) =

C x, - x,

for t mod 3 = 1 otherwise,

The functions are essentially rescaled in a manner so that the resultant updates of ADAM correspond
to the one in the optimization setting described above. Using essentially the same argument as above, it is easy to show that the regret RT  (2C - 4) T /3 and thus, the average regret is non-zero asymptotically, which completes the proof.

C PROOF OF THEOREM 2

Proof. The proof generalizes the optimization setting used in Theorem 1. Throughout the proof, we assume 1 < 2, which is also a condition (Kingma & Ba, 2015) assume in their paper. In this proof, we consider the setting where ft are linear functions and F = [-1, 1]. In particular, we
define the following function sequence:

ft(x) =

C x, -x,

for t mod C = 1 otherwise,

where C  N, C mod 2 = 0 satisfies the following:

(1 - 1)1C-1C  1 - 1C-1,

2(C-2)/2C2  1,

3(1 - 1) 2 1 - 2

(1 - C-1) 1+ 1-

+

1C/2-1 1 - 1

<

C ,
3

(8)



where  = 1/ 2 < 1. It is not hard to see that these conditions hold for large constant C

that depends on 1 and 2. Since the problem is one-dimensional, we drop indices representing

coordinates from all quantities in Algorithm 1. For this function sequence, it is easy to see that

the point x = -1 provides the minimum regret since C  2. Furthermore, the gradients have the

following form:

fi(x) =

C, -1,

for t mod C = 1 otherwise

Our first observation is that mkC  0 for all k  N  {0}. For k = 0, this holds trivially due to our initialization. For the general case, observe the following:

mkC+C = -(1 - 1) - (1 - 1)1 - · · · - (1 - 1)1C-2 + (1 - 1)1C-1C + 1C mkC (9)

= -(1 - 1C-1) + (1 - 1)1C-1C + 1C mkC .

(10)

If mkC  0, it can be easily shown that mkC+C  0 for our selection of C in Equation (8) by using
the principle of mathematical induction. With this observation we continue to the main part of the proof. Let T be such that t + C   2t for all t  T where   3/2. All our analysis focuses on
iterations t  T . Note that any regret before T is just a constant because T is independent of T

13

Under review as a conference paper at ICLR 2018

and thus, the average regret is negligible as T  . Consider an iterate at time step t of the form kC after T . Our claim is that

xt+C  min{xt + ct, 1}

(11)

for some ct > 0. To see this, consider the updates of ADAM for the particular sequence of functions we considered are:

xt+1 = F

xt

-

 t

(1 - 1)C + 1mt (1 - 2)C2 + 2vt

,

xt+i = F

xt+i-1

-

 t

 +i

-

1

-(1 - 1) + 1mt+i-1 (1 - 2) + 2vt+i-1

for i  {2, · · · , C}.

For i  {2, · · · , C}, we use the following notation:

t

=

-  t

(1 - 1)C + 1mt , (1 - 2)C2 + 2vt

t+i

=

-  t+i

-(1 - 1) + 1mt+i (1 - 2) + 2vt+i

for

i



{1, · · ·

,C

- 1}.

Note that if t+j  0 for some j  {1, · · · , C - 1} then t+l  0 for all l  {j, · · · , C - 1}. This follows from the fact that the gradient is negative for all time steps i  {2, · · · , C}. Using Lemma 6
for {xt+1, · · · , xt+C } and {t, · · · , t+C-1}, we have the following:

xt+C  min

t+C -1

1, xt +

i

i=t

.

Let i = C/2. In order to prove our claim in Equation (11), we need to prove the following:

To this end, we observe the following:

t+C -1
 = i > 0.
i=t

t+C -1 i=t+1

i

=

C -1 i=1

-  t+

i

-(1 - 1) + 1mt+i (1 - 2) + 2vt+i

C
= -



-(1 - 1) + (1 - 1)

i-2 j=1

1j

(-1)

+ (1 - 1)1i-1C + 1i mt

i=2 t + i - 1

(1 - 2) + 2vt+i-1

C




(1 - 1) + (1 - 1)

i-2 j=1

1j

- (1 - 1)1i-1C

i=2 t + i - 1

(1 - 2) + 2vt+i-1

C


 (1 - 1) + (1 - 1)

i-2 j=1

1j

C
-



i=2  t

(1 - 2) + 2vt+i-1

i=2 t

(1 - 1)1i-1C (1 - 2) + 2vt+i-1

C


 (1 - 1) + (1 - 1)

i-2 j=1

1j

C
-



i=2  t

(1 - 2) + 2vt+i-1

i=2 t

(1 - 1)1i-1C (1 - 2) + 2i-1(1 - 2)C2

  C t
i=i

1 - 1i-1

-  (1 - 1)(1 - C-1)

(1 - 2) + 2(1 - 2) t (1 - ) (1 - 2)





C - i - 1i -1 -  (1 - 1)(1 - C-1)  0.

 t (1 - 2) + 2(1 - 2)

1 - 1

t (1 - ) (1 - 2)

The first equality follows from the definition of mt+i+1. The first inequality follows from the fact that mt  0 when t mod C = 0 (see Equation (10) and arguments based on it). The second

14

Under review as a conference paper at ICLR 2018

inequality follows from the definition of  that t + C   2t for all t  T . The third inequality
is due to the fact that vt+i-1  (1 - 2)2i-2C2. The fourth inequality is due to the fact that 2i -1C2  1. The last inequality follows from our choice of C. Furthermore, from the above inequality, we have

t+C -1
i  t +  i=t  t

 (1 - 2) + 2(1 - 2)

C -i

-

1i -1 1 - 1

-  (1 - 1)(1 - C-1) t (1 - ) (1 - 2)

= -  (1 - 1)C + 1mt +  t (1 - 2)C2 + 2vt  t

 (1 - 2) + 2(1 - 2)

C

-i

-

1i -1 1 - 1

-  (1 - 1)(1 - C-1) t (1 - ) (1 - 2)

 -  (1 - 1)C + 



C - i - 1i -1 -  (1 - 1)(1 - C-1)

t (1 - 2)C2  t (1 - 2) + 2(1 - 2)

1 - 1

t (1 - ) (1 - 2)

 

C - 1C/2-1 - 3(1 - 1)

(1 - C-1) 1+

 t 3 1 - 1 2 1 - 2

1-

=   t

Note that from our choice of C, it is easy to see that   0. Also, observe that  is independent of t. Thus, xt+C  min{1, xt + / t}. From this fact, we also see the following:

1. If xt = 1, then xt+C = 1 for all t  T such that t mod C = 0.

2. There exists constant T1  T such that xT1 = 1 where T1 mod C = 0.



The due

first point simply follows from to divergent nature of the sum

tht=e tre1la/tiotn.

xt+C  min{1, xt Therefore, we have

+

/

t}. The second point is

CC
f(kC+i)(xkC+i) - f(kC+i)(-1)  2C - 2(C - 1) = 2.
i=1 i=1

where kC  T1. Thus, when t  T1, for every C steps, ADAM suffers a loss of at least 2. More specifically, RT  2(T - T1)/C. Thus, RT /T 0 as T  , which completes the proof.

D PROOF OF THEOREM 3

Proof. Let  be an arbitrary small positive constant, and C be a large enough constant chosen as a function of 1, 2,  that will be determined in the proof.
Consider the following one dimensional stochastic optimization setting over the domain [-1, 1]. At each time step t, the function ft(x) is chosen i.i.d. as follows:

ft(x) =

Cx

with

probability

p

:=

1+ C +1

-x with probability 1 - p

The expected function is F (x) = x; thus the optimum point over [-1, 1] is x = -1. At each time

step t the gradient gt equals C with probability p and -1 with probability 1 - p. Thus, the step taken

by ADAM is

t

=

-t(1mt-1 + (1 - 1)gt) . 2vt-1 + (1 - 2)gt2

We now show that for a large enough constant C, E[t]  0, which implies that the ADAM's steps keep drifting away from the optimal solution x = -1.

Lemma 1. For a large enough constant C (as a function of 1, 2, ), we have E[t]  0.

15

Under review as a conference paper at ICLR 2018

Proof. Let Et[·] denote expectation conditioned on all randomness up to and including time t - 1. Taking conditional expectation of the step, we have

1 t

Et[t]

=

p

·

-(1mt-1 + (1 - 1)C) 2vt-1 + (1 - 2)C2

+

(1

-

p)

·

-(1mt-1 2vt-1

- (1 - 1)) + (1 - 2)

= p · -(1mt-1 + (1 - 1)C) +(1 - p) ·

-1mt-1

+(1 - p) ·

1 - 1

2vt-1 + (1 - 2)C2

2vt-1 + (1 - 2)

2vt-1 + (1 - 2)

T1 T2 T3
(12)

We will bound the expectation of the terms T1, T2 and T3 above separately.

First, for T1, we have

T1



-(1C + (1 - 1)C) (1 - 2)C2



- 1 . 1 - 2

(13)

Next, we bound E[T2]. Define k =

log(C +1) log(1/1 )

. This choice of k ensures that 1kC  1 - 1k. Now,

note that

t-1

mt-1 = (1 - 1) 1t-1-igi.

i=1

Let E denote the event that for every i = t - 1, t - 2, . . . , max{t - k, 1}, gi = -1. Note that Pr[E]  1 - kp. Assuming E happens, we can bound mt-1 as follows:

t-1 max{t-k,1}-1

mt-1  (1-1)

1t-1-i·-1+(1-1)

1t-1-i·C  -(1-1k)+1kC  0,

i=max{t-k,1}

i=1

and so T2  0.

With probability at most kp, the event E doesn't happen. In this case, we bound T2 as follows. We first bound mt-1 in terms of vt-1 using the Cauchy-Schwarz inequality as follows:

t-1
mt-1 = (1 - 1) 1t-1-igi  (1 - 1)
i=1

t-1 i=1

2t-1-igi2

t-1 i=1

(

12 2

)t-1-i

 (1 - 1)

(1

-

2 2)(2

-

12)

·vt-1.

Thus, vt-1  m2t-1/A2. Thus, we have

T2 =

-1mt-1



2vt-1 + (1 - 2)

A
-1|mt-1| = 2(mt2-1/A2)

-1(1 - 1) . (1 - 2)(2 - 12)

Hence, we have E[T2]  0 · (1 - kp) +

-1(1 - 1) · kp = (1 - 2)(2 - 12)

-1(1 - 1)kp (1 - 2)(2 - 12)

(14)

Finally,

we

lower

bound

E[T3]

using

Jensen's

inequality

applied

to

the

convex

function

1 x

:

E[T3] 

(1 - 1)



2E[vt-1] + (1 - 2)

(1 - 1)

.

2(1 + )C + (1 - 2)

The last inequality follows by using the facts vt-1 = (1 - 2)

t-1 i=1

2t-1-igi,

and

the

random

variables g12, g22, . . . , gt2-1 are i.i.d., and so

E[vt-1] = (1-2t-1)E[g12] = (1-2t-1)(Cp+(1-p)) = (1-2t-1)(1+)C -  (1+)C. (15)

16

Under review as a conference paper at ICLR 2018

Combining the bounds in (13), (14), and (15) in the expression for ADAM's step, (12), and plugging in the values of the parameters k and p we get the following lower bound on E[t]:

-1+ C +1



·



 1

1 -

2

+

-1(1 - 1)

log(C+1) log(1/1 )

(1 - 2)(2 - 12)

 +

1- 1+ C +1

·

(1 - 1)

.

2(1 + )C + (1 - 2)

It is evident that for C large enough (as a function of , 1, 2), the above expression can be made non-negative.

For the sake of simplicity, let us assume, as is routinely done in practice, that we are using a version of ADAM that doesn't perform any projection steps2. Then the lemma implies that E[xt+1]  E[xt]. Via a simple induction, we conclude that E[xt]  x1 for all t. Thus, if we assume that the starting point x1  0, then E[xt]  0. Since F is a monotonically increasing function, we have E[F (xt)]  F (0) = 0, whereas F (-1) = -. Thus the expected suboptimality gap is always  > 0, which
implies that ADAM doesn't converge to the optimal solution.

E PROOF OF THEOREM 4
The proof of Theorem 4 presented below is along the lines of the Theorem 4.1 in (Kingma & Ba, 2015) which provides a claim of convergence for ADAM. As our examples showing nonconvergence of ADAM indicate, the proof in (Kingma & Ba, 2015) has problems. The main issue in their proof is the incorrect assumption that t defined in their equation (3) is positive semidefinite, and we also identified problems in lemmas 10.3 and 10.4 in their paper. The following proof fixes these issues and provides a proof of convergence for AMSGRAD.

Proof. We begin with the following observation:

xt+1

=


F


, V^t

(xt

-

tV^t-1/2mt)

=

min
xF

V^t1/4(x - (xt - tV^t-1/2mt)) .

Furthermore,


F


, V^t

(x

)

=

x

for all x



F.

In this proof, we will use xi

to denote the ith

coordinate of x. Using Lemma 4 with u1 = xt+1 and u2 = x, we have the following:

V^t1/4(xt+1 - x) 2  = =

V^t1/4(xt - tV^t-1/2mt - x) 2 V^t1/4(xt - x) 2 + t2 V^t-1/4mt 2 - 2t mt, xt - x V^t1/4(xt - x) 2 + t2 V^t-1/4mt 2 - 2t 1tmt-1 + (1 - 1t)gt, xt - x

Rearranging the above inequality, we have

gt, xt - x

1 
2t(1 - 1t)

1 2t(1 - 1t)

V^t1/4(xt - x) 2 -

V^t1/4(xt+1 - x) 2

+

2(1

t - 1t)

V^t-1/4mt 2

+

1

1t - 1t

mt-1, xt - x

V^t1/4(xt - x) 2 -

V^t1/4(xt+1 - x) 2

+ t 2(1 - 1t)

V^t-1/4mt 2

+

2(1

1t - 1t

)

t

V^t-1/4mt-1

2

+

1t 2t(1 -

1t)

V^t1/4(xt - x)

2.

(16)

The second inequality follows from simple application of Cauchy-Schwarz and Young's inequality. We now use the standard approach of bounding the loss at each step using convexity of the function

2Projections can be easily handled with a little bit of work but the analysis becomes more messy.

17

Under review as a conference paper at ICLR 2018

ft in the following manner:

TT
ft(xt) - ft(x)  gt, xt - x

t=1 t=1

T


1

t=1 2t(1 - 1t)

V^t1/4(xt - x) 2 -

V^t1/4(xt+1 - x) 2

+ t 2(1 - 1t)

V^t-1/4mt 2

+

2(1

1t - 1t)

t

V^t-1/4mt-1

2

+

1t 2t(1 -

1t)

V^t1/4(xt - x)

2

.

(17)

The first inequality is due to convexity of function ft. The second inequality follows from the bound in Equation (16). For further bounding this inequality, we need the following intermediate result.

Lemma 2. For the parameter settings and conditions assumed in Theorem 4, we have

T
t
t=1

V^t-1/4mt

2

  1 + log T

d

(1 - 1)(1 - ) (1 - 2) i=1

g1:T,i 2

Proof. We start with the following:

T
t
t=1

V^t-1/4mt

T -1
2 = t
t=1

V^t-1/4mt

d
2 + T
i=1

m2T ,i v^T ,i

T -1
 t
t=1

V^t-1/4mt

2 + T

d i=1

mvT2T,,ii

T -1
 t
t=1

V^t-1/4mt

d
2+
i=1

(

T j=1

Tk=-1j

1(T

-k+1)gj,i)2

T ((1 - 2)

T j=1

2T

-j

gj2,i)

The first inequality follows from the definition of v^T,i, which is maximum of all vT,i until the current time step. The second inequality follows from the update rule of Algorithm 2. We further bound the
above inequality in the following manner:

T

T -1

t V^t-1/4mt 2 

t V^t-1/4mt 2 + 

d

(

t=1 t=1

i=1

T j=1

kT=-1j

1(T

-k+1))(

T ((1 - 2)

T j=1

Tk=-1j

1(T

-k+1)gj2,i)

T j=1

2T

-j

gj2,i)

T -1
 t V^t-1/4mt 2 + 

d

(

T j=1

1T

-j

)(

t=1 i=1 T ((1 - 2)

T j=1

1T

-j

gj2,i)

T j=1

2T

-j

gj2,i

)

T -1
 t
t=1

V^t-1/4mt

2+  1 - 1

d i=1

T j=1

1T

-j

gj2,i

T ((1 - 2)

T j=1

2T

-j

gj2,i)

T -1
 t
t=1

V^t-1/4mt

2+ (1 - 1)

 dT T (1 - 2) i=1 j=1

1T -j gj2,i 2T -j gj2,i

T -1
 t
t=1

V^t-1/4mt

2+ (1 - 1)

 T (1

-

2)

d i=1

T j=1

T -j |gj,i|

(18)

The first inequality follows from Cauchy-Schwarz inequality. The second inequality is due to the

fact that 1k  1 for all k  [T ]. The third inequality follows from the inequality

T j=1

1T

-j



1/(1 - 1). By using similar upper bounds for all time steps, the quantity in Equation (18) can

18

Under review as a conference paper at ICLR 2018

further be bounded as follows:

T
t
t=1

V^t-1/4mt

2



T t=1

(1

- 1)

 t(1 -

2)

d i=1

t
t-j |gj,i|
j=1

=

 (1 - 1) (1 - 2)

d i=1

T t=1

1 t

t
t-j |gj,i|
j=1

=

 (1 - 1) (1 - 2)

d i=1

TT
|gt,i|
t=1 j=t

j-t j



(1 -

 1) (1 -

2)

d i=1

T t=1

|gt,i|

T j=t

j-t t



(1

-

 1) (1

-

2)

d i=1

T t=1

|gt,i| (1

1 - ) t

 (1 - 1)(1 - )

d
(1 - 2) i=1 g1:T,i 2

T 1

  1 + log T

d

t
t=1

(1 - 1)(1 - ) (1 - 2) i=1

g1:T,i 2

The third inequality follows from the fact that

T j=t

 j -t



1/(1 - ).

The

fourth inequality is

due to simple application of Cauchy-Schwarz inequality. The final inequality is due to the following

bound on harmonic sum:

T t=1

1/t



(1

+

log

T

).

This

completes

the

proof

of

the

lemma.

We now return to the proof of Theorem 4. Using the above lemma in Equation (17) , we have:

TT
ft(xt) - ft(x) 
t=1 t=1

1 2t(1 - 1t)

V^t1/4(xt - x) 2 - V^t1/4(xt+1 - x) 2

+

1t 2t(1 -

1t)

V^t1/4(xt - x)

2

  1 + log T

d

+

(1 - 1)2(1 - ) (1 - 2) i=1

g1:T,i 2

1 
21(1 - 1)

V^11/4(x1 - x)

2

+

2(1

1 -

1)

T t=2

V^t1/4(xt - x) 2 - V^t1-/14(xt - x) 2 t t-1

T
+
t=1

1t 2t(1 - 1)

V^t1/4(xt - x)

2

  1 + log T

d

+

(1 - 1)2(1 - ) (1 - 2) i=1

g1:T,i 2

=

1 21(1 - 1)

d
v^11,/i2(x1,i
i=1

- xi )2 +

1 2(1 - 1)

T t=2

d
(xt,i
i=1

- xi)2

v^t1,/i2 - v^t1-/21,i t t-1

+

1 2(1 -

1)

T t=1

d i=1

1t(xt,i

- xi)2v^t1,/i2 t

+

(1 -

  1 + log T 1)2(1 - ) (1

- 2)

d i=1

g1:T,i 2.

(19)

The first inequality and second inequality use the fact that 1t  1. In order to further simplify the bound in Equation (19), we need to use telescopy sum. We observe that, by definition of v^t,i, we have
v^t1,/i2  v^t1-/21,i . t t-1
19

Under review as a conference paper at ICLR 2018

Algorithm 3 ADAMNC

Input: x1  F , step size {t > 0}tT=1, {(1t, 2t)}tT=1

Set m0 = 0 and v0 = 0

for t = 1 to T do

gt = ft(xt)

mt = 1tmt-1 + (1 - 1t)gt

vt = xt+1

2tvt-1 +

=

F

 , Vt

((x1t--2ttm)gtt2/andvtV) t

=

diag(vt)

end for

Using the L bound on the feasible region and making use of the above property in Equation (19), we have:

T
ft(xt) - ft(x)
t=1



1 21(1 - 1)

d
v^11,/i2D2
i=1

+

1 2(1 - 1)

T t=2

d
D2
i=1

v^t1,/i2 - v^t1-/21,i t t-1

+

1 2(1 - 1)

T t=1

d i=1

D2 1tv^t1,/i2 t

+

  1 + log T (1 - 1)2(1 - ) (1 - 2)

d i=1

g1:T,i 2

=

D2 2T (1 - 1)

d
v^T1/,i2
i=1

+

D2 2(1 - 1)

T t=1

d i=1

1tv^t1,/i2 t

+

  1 + log T (1 - 1)2(1 - ) (1 - 2)

d i=1

g1:T,i 2.

The equality follows from simple telescopic sum, which yields the desired result.

F PROOF OF THEOREM 5

Proof. Using similar argument to proof of Theorem 4 until Equation (16), we have the following

gt, xt - x

1 2t(1 - 1t)

Vt1/4(xt - x) 2 -

Vt1/4(xt+1 - x) 2

+ t 2(1 - 1t)

Vt-1/4mt 2

+

2(1

1t - 1t

)

t

Vt-1/4mt-1

2 + 1t 2t(1 - 1t)

Vt1/4(xt - x)

2.

(20)

The second inequality follows from simple application of Cauchy-Schwarz and Young's inequality. We now use the standard approach of bounding the loss at each step using convexity of the function ft in the following manner:

TT
ft(xt) - ft(x)  gt, xt - x

t=1 t=1

T


1

t=1 2t(1 - 1t)

Vt1/4(xt - x) 2 -

Vt1/4(xt+1 - x) 2

+

2(1

t - 1t)

Vt-1/4mt 2

+

2(1

1t - 1t)

t

Vt-1/4mt-1

2 + 1t 2t(1 - 1t)

Vt1/4(xt - x)

2

.

(21)

The inequalities follow due to convexity of function ft and Equation (20). For further bounding this inequality, we need the following intermediate result.

Lemma 3. For the parameter settings and conditions assumed in Theorem 5, we have

T
t
t=1

Vt-1/4mt

2

2 d (1 - 1)2 i=1

g1:T,i 2.

20

Under review as a conference paper at ICLR 2018

Proof. We start with the following:

T
t
t=1

Vt-1/4mt

T -1
2 = t
t=1

Vt-1/4mt

2 + T

d i=1

mvT2T,,ii

T -1

d

 t V^t-1/4mt 2 + T

t=1 i=1

(

T j=1

kT=-1j 1(T

-k+1)gj,i)2

(

T j=1

Tk=-1j

2(T

-k+1)(1

-

2j )gj2,i)

The first inequality follows from the update rule of Algorithm 2. We further bound the above inequality in the following manner:

T

T -1

t Vt-1/4mt 2 

t Vt-1/4mt 2 + T

d

(

t=1 t=1

i=1

T j=1

kT=-1j

1(T

-k+1))(

T j=1

kT=-1j 1(T

-k+1)gj2,i)

T j=1

Tk=-1j

2(T

-k+1)

(1

-

2j

)gj2,i

T -1

d

 t Vt-1/4mt 2 + T

t=1 i=1

(

T j=1

1T

-j

)(

T j=1

1T

-j

gj2,i)

T j=1

Tk=-1j

2(T

-k+1)(1

-

2j

)gj2,i

T -1
 t
t=1

Vt-1/4mt

2 + T 1 - 1

d i=1

T j=1

1T

-j

gj2,i

T j=1

Tk=-1j

2(T

-k+1)

(1

-

2j

)gj2,i

T -1
 t
t=1

Vt-1/4mt

2

+

1

 -

1

d i=1

T j=1

1T

-j

gj2,i

T j=1

gj2,i

T -1
 t
t=1

Vt-1/4mt

2

+

1

 -

1

dT i=1 j=1

1T -j gj2,i

j k=1

gk2,i

(22)

The first inequality follows from Cauchy-Schwarz inequality. The second inequality is due to the

fact that 1k  1 for all k  [T ]. The third inequality follows from the inequality

T j=1

1T

-j



1/(1 - 1). Using similar argument for all time steps, the quantity in Equation (22) can be bounded

as follows:

T
t
t=1

Vt-1/4mt

2  1 - 1

dT i=1 j=1

T -j l=0

1l gj2,i

j k=1

gk2,i

 dT  (1 - 1)2 i=1 j=1

gj2,i

j k=1

gk2,i



2 (1 - 1)2

d i=1

g1:T,i 2.

The final inequality is due to Lemma 5. This completes the proof of the lemma. 21

Under review as a conference paper at ICLR 2018

Using the above lemma in Equation (21) , we have:

TT
ft(xt) - ft(x) 
t=1 t=1

1 21(1 - 1t)

Vt1/4(xt - x) 2 - Vt1/4(xt+1 - x) 2

+

1t 2t(1 -

1t)

Vt1/4(xt - x)

2

2 d + (1 - 1)3 i=1

g1:T,i 2



1 21(1 - 1)

V11/4(x1 - x)

2

+

2(1

1 -

1)

T t=2

Vt1/4(xt - x) 2 - Vt1-/14(xt-1 - x) 2 t t

T
+
t=1

1t 2t(1 - 1)

Vt1/4(xt - x)

2

2 d + (1 - 1)3 i=1 g1:T,i 2

=

1 21(1 - 1)

d
v11,/i2(x1,i
i=1

- xi)2 +

1 2(1 - 1)

T t=2

d
(xt,i
i=1

- xi)2

vt1,/i2 - vt1-/21,i t t-1

+

1 2(1 - 1)

T t=1

d i=1

1t(xt,i

- xi)2vt1,/i2 t

+

2 (1 - 1)3

d i=1

g1:T,i 2.

(23)

The first inequality and second inequality use the fact that 1t  1. Furthermore, from the theorem statement, we know that that {(t.2t)} are selected such that the following holds:

vt1,/i2  vt1-/21,i . t t-1

Using the L bound on the feasible region and making use of the above property in Equation (23), we have:

T
ft(xt) - ft(x)
t=1



1 21(1 - 1)

d i=1

v11,/i2D2

+

1 2(1 -

1)

T t=2

d i=1

D2

vt1,/i2 - vt1-/21,i t t-1

+

2(1

1 - 1)

T t=1

d i=1

D2 1tvt1,/i2 t

+

(1

2 - 1)3

d i=1

g1:T,i 2

=

D2 2T (1 - 1)

d
vT1/,i2 +
i=1

D2 2(1 - 1)

T t=1

d i=1

1tvt1,/i2 t

+

2 (1 - 1)3

d i=1

g1:T,i 2.

The equality follows from simple telescopy sum, which yields the desired result.

G AUXILIARY LEMMA

Lemma 4 ((McMahan & Streeter, 2010)). For any Q  S+d and convex feasible set F  Rd, suppose u1 = minxF Q1/2(x-z1) and u2 = minxF Q1/2(x-z2) then we have Q1/2(u1 - u2)  Q1/2(x1 - x2) .

Proof. We provide the proof here for completeness. Since u1 = minxF Q1/2(x - z1) and u2 = minxF Q1/2(x - z2) and from the property of projection operator we have the following:

x1 - u1, Q(x2 - x1)  0 and x2 - u2, Q(x1 - x2)  0.

Combining the above inequalities, we have

u2 - u1, Q(x2 - x1)  x2 - x1, Q(x2 - x1) .

(24)

Also, observe the following:

1

u2 - u1, Q(x2 - x1)



[ 2

u2

-

u1, Q(u2

-

u1)

+

x2 - x1, Q(x2 - x1) ]

The above inequality can be obtained from the fact that (u2 - u1) - (x2 - x1), Q((u2 - u1) - (x2 - x1))  0 as Q  S+d and rearranging the terms. Combining the above inequality with Equation (24), we have the required result.

22

Under review as a conference paper at ICLR 2018

Lemma 5 ((Auer & Gentile, 2000)). For any non-negative real numbers y1, · · · , yt, the following holds:

t

yi

t
 2 yi.

i=1

i j=1

yj

i=1

Lemma 6. Suppose F = [a, b] for a, b  R and

yt+1 = F (yt + t)

for all the t  [T ], y1  F and furthermore, there exists i  [T ] such that j  0 for all j  i and j > 0 for all j > i. Then we have,

T
yT +1  min{b, y1 + j}.
j=1

Proof. It is first easy to see that yi+1  y1 +

i j=1

j

since

j



0

for

all

j



i.

Furthermore,

also

observe that yT +1  min{b, yi+1 +

T j=i+1

j }

since

j



0

for

all

j

>

i.

Combining

the

above

two inequalities gives us the desired result.

23

