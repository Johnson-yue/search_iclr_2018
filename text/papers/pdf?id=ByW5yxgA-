Under review as a conference paper at ICLR 2018
MULTISCALE HIDDEN MARKOV MODELS FOR COVARIANCE PREDICTION
Anonymous authors Paper under double-blind review
ABSTRACT
This paper presents a novel variant of hierarchical hidden Markov models (HMMs), the multiscale hidden Markov model (MSHMM), and an associated spectral estimation and prediction scheme that is consistent, finds global optima, and is computationally efficient. Our MSHMM is a generative model of multiple HMMs evolving at different rates where the observation is a result of the additive emissions of the HMMs. While estimation is relatively straightforward, prediction for the MSHMM poses a unique challenge, which we address in this paper. Further, we show that spectral estimation of the MSHMM outperforms standard methods of predicting the asset covariance of stock prices, a widely addressed problem that is multiscale, non-stationary, and requires processing huge amounts of data.
1 INTRODUCTION
Hidden Markov models (HMMs) are a critical tool in many fields, including speech recognition (Huang et al., 1990), robotics (Thrun et al., 1998), and natural language processing (NLP) (Rodu et al., 2013). Spectral estimation of HMMs, used in place of the expectation maximization (EM) algorithm or Gibbs sampling, now allows for fast estimation, making HMMs feasible for large datasets (Hsu et al., 2012; Foster et al., 2012). However, modeling complex structure with a standard HMM, if even possible, can require a prohibitively large state space. Previous work using HMMs to predict volatility fail to capture multiple timescales, and additionally are not able to scale to large datasets because they are estimated by EM (Rossi & Gallo, 2006; Nystrup et al., 2016). To specifically address datasets with multiple timescales, we propose the multiscale hidden Markov model (MSHMM), which utilizes computationally efficient spectral estimation to accommodate large, high-dimensional datasets, and can be usefully applied to multiscale non-stationary problems of industrial scale.
In order to motivate our method we apply the MSHMM to high-frequency asset pricing data. For US equities, sampling once per second yields roughly 5 million observations per year per stock, on thousands of stocks. Prediction of asset covariance has been recognized as an extremely important task (Wu & Xiao, 2012), and continues to be of interest (Nystrup et al., 2017), but modeling at that scale has proven challenging and often intractable. Since asset covariance is a non-stationary stochastic process, the MSHMM structure is well suited for this problem.
Our main contribution is a novel variant of multiscale hidden Markov model that uses a unique, structured hierarchical form to force the model to evolve at multiple timescales. The prediction phase of MSHMM differs markedly from other hierarchical HMMs due to the additive emission and multiscale noise structure. The resulting MSHMM provides a model that allows for spectral estimation, thus avoiding computationally costly EM or Gibbs sampling. Spectral estimation's ability to offer additional speed, while avoiding local minima, allows us to use our method to model large-scale multivariate time series such as high-frequency stock trades. We prove that MSHMM is consistent under spectral estimation, and we provide an efficient estimation algorithm. We show that MSHMM can achieve excellent performance in the real world task of predicting realized covariance and compare to existing methods.
1

Under review as a conference paper at ICLR 2018

2 MULTISCALE HIDDEN MARKOV MODEL

A hidden Markov model characterizes a sequence of discrete-time observations (in this paper we consider high-dimensional, continuously distributed observations) as being emitted by a chain of discrete hidden states. There are two main assumptions, 1) that observations are conditionally independent given the hidden states, and 2) that the hidden states are governed by a Markov process, meaning that the distribution over hidden states at time t given the entire latent state history depends only on the previous hidden state at time t - 1, so
Pr[ht | ht-1, . . . , h1] = Pr[ht | ht-1].
We can fully specify the HMM by a transition matrix T  Rm×m where m is the number of hidden states, an emission probability  that specifies the conditional probability of an observation x given a hidden state h, and an initial distribution  over hidden states.
i = Pr[h1 = i], Ti,j = Pr[ht+1 = i | ht = j], [(x)]i = Pr[xt|ht = i].
It is convenient to think of hidden states as being an indicator vector in Rm, and  as associating with each observation x a vector (x) where (x)i = Pr[x|h = i]. For readers familiar with HMMs, x is often discrete, allowing  to be easily interpreted as a matrix with finitely many rows. As previously mentioned, we assume our observations are high-dimensional x  Rn, and we assume that m n.
HMMs can model many behaviors exhibited by data. Flexibility is obtained by increasing the size of the hidden state space, at the cost of an often prohibitive increase in sample complexity. This can be avoided by factoring the hidden state space into multiple layers and placing a nontrivial dependence structure on these layers and emitted observations (Ghahramani & Jordan, 1997), known as a factorial HMM (FHMM). Figure 5 (a) displays the dependence structure of the original FHMM.

h(t2-)1

h(t2)

ht(2+)1

ht(2-)1

(2) = 2

h(t2+)1

h(t1-)1

h(t1)

ht(1+)1

ht(1-)1

h(t1)

h(t1+)1

Xt-1 Xt Xt+1

Xt(-1)1
+

Xt(-2)1

Xt(1)
+

Xt(+1)1
+

Xt(+2)1

Xt-1 Xt Xt+1
Figure 1: (a) Factorial HMM with 2 hidden state layers. (b) An MSHMM with two HMMs where h(t2) is the hidden state sequence of the slower moving HMM with transitions happening at time steps (2) = 2. The observations Xt are the sum of the emissions of the two HMMs, so Xt = Xt(1) + Xt(-2)1.
In contrast to the FHMM, MSHMM has multiple, independently evolving HMMs with additive emissions as seen in Figure 5 (b). Consider a system with M hidden state chains {h(1), . . . , h(M)}. Each hidden state chain h(i) transitions at different time periods (i), so that at a given time t, chains transition by time t + 1 only if they satisfy (t + 1 modulo (i)) = 0, with self-transitions allowed. In this paper, we assume that the parameters {(1), . . . , (M)} are unique, and thus without loss of generality ordered from smallest to largest, so that the hidden state chain h(M) is the slowest evolving chain. Unlike in Ghahramani & Jordan (1997), where observations are sampled from a

2

Under review as a conference paper at ICLR 2018

Gaussian centered at a linear combination of the hidden states of each chain, in our model each hidden state chain emits an unobserved output with noise, and the system-wide observations are linear combinations of the output. In other words, we can think of each layer as an actual HMM with unobserved, though otherwise typical, output. Critically, we assume the noise terms are resampled only when there is a (possibly self-) transition.

2.1 SPECIFICATION OF MULTISCALE HIDDEN MARKOV MODEL

Recalling that an HMM can be fully specified with parameters , T , and (x), we denote the HMM parameters of each hidden process i with a superscript (i), so (i), T (i), and (i)(x). As above, h(i) represents the hidden state chain for hidden process i, and ht(i) its value at time t. Furthermore, m(i) denotes the size of the hidden state space for process i. Letting ht without the superscript i denote the global hidden state of the entire system, we can characterize the independence between hidden states as
M
Pr[ht+1 | ht] = Pr[ht(+i)1 | ht(i)].
i=1
Denote the expectation of the continuous emission for model i at time t,

q(ht(i)) = E[Xt(i)|h(ti)] and let q(ht) be a matrix collecting q(h(ti)) on the columns, so
q(ht) = q(ht(1)), q(ht(2)), · · · , q(h(tM)) .

To incorporate noise at each level into our model, we define a matrix q~(ht) as

q~(ht) =

q(ht(1)) +

t(1), · · · , q(h(tM)) +

(M ) t

,

where

(i) t



N (0, (i));

however,

the

model

is

robust

to

other

distributions.

The

observable

emis-

sion is then Xt = q~(ht)w where w is a vector of size M . Figure 5 displays the representation of

MSHMM. Note that, as a consequence of the stickiness property in our model,

Pr[q(ht(+i)1) +

(i) t+1

=

q(h(ti))

+

t(i)] = 1 if (t + 1 modulo (i)) = 0.

Finally, we let h~(ti) = E[h(ti)|Xt-1 . . . X1] denote the belief state of HMM i at time t.

2.2 PREDICTION MULTISCALE HIDDEN MARKOV MODEL

The prediction procedure for our model differs substantially from standard hierarchical and factorial

HMMs. Typically for HMMs and variants of HMMs, it is only necessary to estimate the belief state h~t. But for the MSHMM, the correction term

~t(i)

q(ht(i)) +

(i) t

-

q (h~ (ti) )

from HMM i acts as persistent bias for HMM i-1 for a period (i). Therefore, in order to eliminate

this bias, we must estimate ~(ti).

We require one more definition in order to specify the prediction procedure. Let g(i)(t) to be a step function for the ith hidden state chain that steps when HMM process i has a transition. For example, if t = {0, 1, 2, 3, 4, 5, 6, · · · }, and (3) = 3 then g(3)(t) = {0, 0, 0, 3, 3, 3, 6, · · · } which corresponds to when process 3 has a state transition.

Prediction proceeds in two steps. First, we obtain predictions q(h~t(i)) from each component HMM. Second, in order to eliminate the persistent bias effect mentioned above, we estimate the noise terms
~t(i) for i = 2 . . . M and add these to q(h~t(i)) to obtain an estimate of q(ht(i)) + (ti). We first estimate ~(t+M1), where M is the slowest HMM process, as

E[~t(M)|ht,Xt, · · · , Xg(M)(t)] =

t

-

1 g(M ) (t)

t-g(M ) (t)

Xt-j

-

q(h~ t(M ) )w(M )

,

j=0

3

Under review as a conference paper at ICLR 2018

and subsequently estimate

t-g (i) (t)

E[~(ti)|ht,Xt, · · ·

, Xt-g(i)(t)]

=

1 t - g(i)(t)

Xr(ie)sid,t-j - q(h~t(i))w(i) ,

j=0

and finally predict

Xt+1 = q~(ht)w = q(h~(t1)) + ~t(1), · · · , q(h~t(M)) + ~t(M) w.

This correction term estimation procedure must be repeated at all times t for all component HMMs. Algorithm 1 details the prediction procedure.

Algorithm 1 Prediction using MSHMM
1: Input: Observed emissions X. 2: Let XrMesid = X, and set i = M . 3: while i > 0 do 4: Predict Xt(+i)1 using spectral estimation HMM, q(h~t(i)) (see Rodu et al. (2013) for details) from the
observations Xr(eMsid) . 5: Compute the error X - q(h~t(i))w(i) over (gM (t), t]. 6: Estimate new ~t(i) by averaging the error over this period. 7: Compute Xr(eis-id1) = Xr(eis)id - q(h~t(i))w(i) - ~(ti) and set i = i - 1. 8: end while 9: Sum over all predictions from each process Xt(+i)1.

2.3 ESTIMATION ALGORITHM FOR MULTISCALE HIDDEN MARKOV MODELS
We divide the algorithm into two components. The first estimates the HMM parameters through the spectral method of moments (see Rodu (2014) for details). The second, presented in algorithm 2, estimates the weight vector that combines the emissions of the component HMMs in the MSHMM.
The critical assumptions that allow our model to be estimated using fast and consistent spectral methods are 1) the independence of the component HMM processes and 2) the fact that the emissions Xt are linear combinations of the latent emissions from the component HMM processes. Note that the decoupling of the HMMs is possible because they evolve at different rates.

Algorithm 2 Computing weights for the linear combination of HMMs

1: Input: Training set of observed emissions X.

2: Let Xr(eMsid) = X, and set i = M . 3: while i > 0 do

4: Using spectral estimation method from Foster et al. (2012) (see supplementary materials), estimate the

HMM process i with a binned average

X (i) avg((i) )

of bin width (i) over time series Xr(eis)id, and

compute observation expectations q(h(ti)).

5:

In

order

to

recenter,

compute

Xr(eis-id1)

=

Xr(eis)id

-

X (i) avg((i) )

and

set

i

=

i

-

1.

6: end while

7: Estimate w with equation Xt = q(ht)w + t with q(ht) = q(h(t1)), q(ht(2)), . . . , q(ht(M)) .

2.4 THEORETICAL FOUNDATIONS OF MULTISCALE HIDDEN MARKOV MODELS
In our estimation scheme, observations for HMM process M are obtained by segmenting the time series into bins of size (M). A residual time series is then constructed by subtracting the derived observations from their respective observations. We then obtain observations for HMM process M - 1 by binning the residual time series in a similar fashion, and proceed recursively for every other process. Each time we bin, however, there is noise in the estimated observation, and that noise gets propagated to faster chains. Critically, the only effect of this noise on consistency is bias in the
4

Under review as a conference paper at ICLR 2018

estimation of the diagonal elements of the second moment, which only appears in the method of moments estimation in its inverted form.
More technically, note that in our estimation scheme we assume that E[Xt(i)] = 0 for i < M . In an MSHMM with only two processes, for instance, this implies that E[Xt(1)] = 0. Let q(1) be a matrix such that
q(1)h(t1) = q(ht(1)) = E[Xt(1)|ht(1)]
then under stationarity conditions we have that q(1)(1) = 0. This implies that E[Xt ] = Xt(2) for t  [t, t + (2)), which suggests that we first estimate observations Xt(+2)k(2) of the slow-moving HMM by calculating the expected value of Xt for t  [t, t + (2)), then estimating the slowmoving model using these calculated observations and the fast-moving model using the residuals of observations from their expected value. But because we have only S = (2)/(1) observations Xt for t  [t, t+(2)) our estimate is noisy. The main problem is that the bias in the empirical estimate of Xt(2) is reflected in Xt - Xt(2) for all t  [t, t + (2)). This, in turn, biases the estimation of the diagonal elements of the second moment matrix of the faster HMM 1. It does not, however, bias estimation of HMM 2, as the error between two successive observations for that HMM are independent.
In practice, the bias seems to have no real effect on the performance of the MSHMM, and even might serve as a natural regularization. It is possible, however, to estimate the spectral parameters so as to eliminate this bias, which we lay out in theorem 1. This provides a practical mechanism for obtaining consistent estimates that effectively restricts the sample size of each component HMM process to be on the order of that of the slowest HMM. On the other hand, if we let the ratio between sampling intervals become arbitrarily large as in theorem 2, while impractical, this allows observations estimates from each HMM component to become arbitrarily precise, thus again providing consistent estimators. Theorem 1. Assume we have an MSHMM. Let ki(M) and ki(M) + 1 be the last observation in HMM k before the ith state transition of HMM M and the first observation in HMM k immediately following the ith transition of HMM M , respectively. Note that for HMM k the emission value at time t = i(M) is equal to the previous (k) time steps. Assume that the second order observables for all chains k  {1, . . . , M } are estimated only using the bigram ki(M) and ki(M) + 1. Then the estimated spectral MSHMM parameters converge to the true spectral MSHMM parameters.

Proof. We consider the two layer process, though this generalizes to an arbitrary number of layers. As a reminder, we can decompose the theoretical contributions from the faster-moving HMM 1 into two components: the state-conditional means and a mean 0 noise term that we consider to be drawn iid at each time step. Similarly, empirical contributions can be decomposed as the state-conditional means, an iid noise term, and an estimation bias propagated from HMM 2 that persist for (2) time steps. Note that from the central limit theorem the bias terms are drawn iid with mean 0. Therefore when estimating the second order observable as suggested in theorem 1 the bias term simply acts as an additional noise term. It is straightforward to see, then, that all the necessary observables converge to their true parameters.
Theorem 2. Assume we have an MSHMM as defined above, and let J be the total number of observations. Let J   and (i)/(i-1)   for 1 < i  M . Then the estimated spectral MSHMM parameters converge to the true spectral MSHMM parameters (in the sense that the probability forecasts are consistently estimated.)

Proof. Considering the two layer MSHMM, we show that as (i)/(i-1)  , X^t(2)  Xt(2). We write

X^t(2)

=

1 S

Xt

=

Xt(2)

+

q(1)^

+

1 S

(1) t

t [t,t+(2))

t (t,t+(2))

From

the

law

of

large

numbers,

as

S



,

1 S

q(1)(1) = 0 we have X^t(2)  Xt(2).

t (t,t+(2))

(1) t



0,

and

^(1)



(1),

and

since

5

Under review as a conference paper at ICLR 2018

While theorem 2 is impractical, it shows that as the separation of time scales between HMM processes increases, the empirical estimates of the spectral parameters converge to their theoretical counterparts. If a comfortably large enough separation is not feasible or desirable, theorem 1 provides an estimation method that inherently enjoys the usual consistency of spectral HMM estimation. In practice, however, the effect of the bias is minimal.

3 EXPERIMENTS WITH SYNTHETIC DATA

We assessed the estimation speed and accuracy of MSHMMs using synthetic data generated from an MSHMM process containing 3 component HMMs each with 5 hidden states (MSHMM-3-5) which emits a vector of continuous observations. Computation time for training the MSHMM was linear in the number of observations and quadratic in the number of states. For our application, we found no benefit beyond 15 hidden states. Our computational times are consistent with Parikh et al. (2012). Furthermore, because of the MSHMM specification, the HMM with the highest resolution dominated the computational time for estimating the model. For comparison, a simple HMM with 5 hidden states using EM1 required 1255 seconds to estimate parameters for 900,000 observations while our MSHMM-3-5 took 25 seconds.

Figure 2: Model RMSE for MSHMM normalized by the RMSE of the oracle. Thus 1 is the absolute minimum.

We compared the correct MSHMM to HMMs with 5 and 15 hidden states. We also compared a 3 Markov component MSHMM which has 10 hidden states instead of 5. Finally, as an upper bound on performance we used the "oracle" MSHMM which makes prediction using the true hidden state transition, emission, and weight parameters, where only the hidden state sequence is unknown. Surprisingly, the MSHMM with too many hidden states still performs well. The relative root-mean-squared error (RMSE) is the RMSE normalized by the oracle RMSE.

4 ESTIMATING STOCK PRICE COVARIANCE
4.1 DATA DESCRIPTION
Daily stock closing prices were extracted from the Center for Research in Securities Prices (CRSP) database.2 All intraday data was extracted from the NYSE Trade and Quote (TAQ) database.3 All stock returns are calculated using the difference of log prices. The calculation of realized covariance uses the same procedure as Sheppard & Xu (2014).
We sampled the realized covariance at daily, 17-minute and 1-minute rates. We chose the 17-minute sample rate since Bollerslev et al. (2008) have shown that microstructure effects on realized variance are mostly mitigated. We chose six exchange-traded funds (ETFs), including SPY (which tracks the S&P 500 index), TLT, FEZ, USO, UNG, and GLD. Each realized covariance matrix has 21 unique entries. We calculated realized variance over 10 years of historical daily data, 2 years of 17-minute intraday data, and 1 year of 1-minute intraday data. To be exact, for training we used 2268 daily,  22000 17-minute, and  20000 1-minute observations, and for testing we used 252 daily,  2600 17-minute, and  4000 1-minute observations. Due to market conditions such as stock halts, the number of intraday data points is not the same for all stocks, so we reported approximate number of observations.
1http://cran.r-project.org/web/packages/HMMCont/HMMCont.pdf 2http://www.crsp.com/products/research-products/crsp-us-stock-databases 3http://www.nyxdata.com/doc/2549

6

Under review as a conference paper at ICLR 2018

4.2 EMPIRICAL RESULTS

Our model was applied to high-frequency equity data for covariance estimation. We estimated the slowest process using 9 years of data, while the minute-level process was estimated using shorter periods of time. This is another useful feature of our model versus other hierarchical HMMs: we can estimate the daily covariance process using a longer overlapping time period.
We used the standard Generalized Autoregressive Conditional Heteroskedastic model (GARCH) (Bollerslev, 1986) as our baseline. We chose the baseline model to be GARCH(1,1), such that
t = htvt, vt  N (0, 1) and ht =  + t2-1 + ht-1.
The prediction from GARCH(1,1) is the estimate of volatility t2. GARCH is a standard baseline for volatility prediction. We also compared our model to two types of models. The first is a generalization of the multivariate GARCH, Bayesian Multivariate Dynamic Covariance model (BMDC)4 (Wu et al., 2013). The second type of model is a generalization of regime-switching models to high-frequency data, High-Frequency-Based Volatility (HEAVY)5 models (Noureldin et al., 2012; Sheppard & Xu, 2014). BDMC and HEAVY have been shown to be computationally efficient to estimate as well as to perform at state-of-the-art levels. The model seeks to predict the covariance either a day, 17 minutes, or a minute forward.

Horizon daily 17-minute 1-minute

BDMC 0.39 ( 976 s) 0.77 ( 9134 s) 0.75 ( 9576 s)

HEAVY 0.42 ( 103 s) 0.76 ( 717 s) 0.80 ( 1879 s)

MSHMM-3-5 0.93 ( 73 s) 0.73 ( 51 s) 0.51 ( 47 s)

Table 1: The RMSE of the out of sample predicted versus realized stock covariance which is normalized by the GARCH(1,1) RMSE. Run times are in paranthesis in seconds.

We report the RMSE of the models normalized with respect to the GARCH baseline. Table 1 shows the results of the prediction on ETFs. MSHMM is clearly superior at higher frequency, where the realized covariance matrix tends to be sparse. This result is consistent with our simulated data. When we used a daily prediction horizon, MSHMM underperformed BDMC and HEAVY models; this should be expected, however, since high-frequency data does not aid estimation very much when estimating the covariance only at the market close. We outperformed both models when predicting at 17-minute intervals and 1-minute realized covariance. MSHMM is also significantly faster than BDMC and HEAVY models 6. Finally, we again draw the reader's attention to the fact that without the estimation of noise as done in algorithm 1, MSHMM underperformed even GARCH(1,1).

5 RELATED WORK
The MSHMM is a specialized hierarchical HMM, in a similar vein proposed by El Hihi & Bengio (1995), and is distinct from the factorial hidden Markov models (FHMMs) of Ghahramani & Jordan (1997). While FHMMs and MSHMMs are both capable of learning short- and long-term structure, the MSHMM explicitly differentiates time-scale evolution.
Spectral estimation of latent state models Recently there has been significant interest in spectral estimation of latent state models, beginning with the seminal work in spectral estimation of discrete HMMs (Hsu et al., 2012) which has been extended to fully reduced methods (Foster et al., 2012), to continuous output HMMs (Song et al., 2010; Rodu et al., 2013), to trees, (Dhillon et al., 2012), and to more general latent variable structures (Anandkumar et al., 2014; Parikh et al., 2012). In this paper, we extend spectral estimation techniques to MSHMMs.
Hierarchical HMM variants A multitude of hierarchical HMM variants have been proposed since hierarchical HMMs were first introduced (El Hihi & Bengio, 1995). Hierarchical HMMs are characterized by multiple interconnected hidden states, Pr[ht+1 | ht, ht, · · · ] where ht is not in the same
4Code is from https://bitbucket.org/jmh233/bmdc_icml2013 5Code is from https://www.kevinsheppard.com/MFE_Toolbox 6We used the highly optimized C++ code of HEAVY and the Matlab code from BDMC. Our model is written in R and not yet optimized for performance.

7

Under review as a conference paper at ICLR 2018
hidden state sequence, or by multiple hidden states that affect an emission, Pr[xt | ht, ht, · · · ]. FHMMs are a subclass of hierarchical HMMs in which multiple independent hidden state processes affect an observable emission. There are a multitude of variants on FHMMs. Among these are multirate HMMs (Cetin & Ostendorf, 2004) and additive FHMMs (Kolter & Jaakkola, 2012), which differ in structure from our model.
Multirate HMMs have multiple hidden state processes, which we refer to as "levels" or "components," each evolving at different rates. The main structural difference between the multirate HMM and our model is that only the first level emits an observable; all subsequent "higher" levels output to the level "below," which has a faster hidden state process. MSHMM has additive emissions results, whereas, multirate HMM will propagate the same error upstream through different levels of hidden states.
By way of contrast, in additive FHMMs all hidden state processes evolve at the same rate, but the higher level process emits to both observations at t and t + 1. It is possible to formulate an MSHMM and more generally hierarchical HMM as a latent junction tree, on which spectral algorithms can be estimated (Parikh et al., 2012). However, latent junction trees are inefficient representations for estimating FHMMs and MSHMMs (Jordan et al., 1999) as they couple the hidden state paths, leading to an explosion in the number of parameters to be estimated.
Financial Covariance Estimation Models The standard methods for modeling stock price variation trade off between parameter accuracy and computational efficiency. The modern classes of models are multivariate versions of generalized autoregressive conditional heteroscedasticity (GARCH) (Engle & Kroner, 1995; Wu et al., 2013), stochastic volatility models (Gourie´roux et al., 2009), and Markov-switching processes (Calvet & Fisher, 2008), (which, while a hidden state model, their formulation and estimation are significantly different). Furrer & Bengtsson (2007) used a Kalman filter approach in order to estimate covariance matrices. More recently, Nystrup et al. (2016) modelled covariance using HMMs. Most of these models do not scale to large amounts of data and are used to predict daily returns or even longer periods. For a more detailed overview of models for large covariance matrix estimation models see Fan et al. (2016).
With the emergence of high-frequency data containing detailed records of quotes and transaction prices with nanosecond time resolution, a new class of models has emerged, such as multivariate high-frequency-based-models (HEAVY) (Noureldin et al., 2012). Our model extends the work of Rossi & Gallo (2006) to multiple timescales, covariance estimation, and the use of computationally efficient spectral methods instead of EM.
6 CONCLUSION
Our MSHMM is a specialized hierarchical HMM that captures multiscale structure by adding emissions from multiple component HMMs. Unlike standard factorial HMMs, MSHMM handles noise at multiple time scales in a manner that cleanly supports spectral estimation. This estimation is straightforward if one accounts for the noise that propagates from the slower to the faster time scales. However, prediction requires accounting for the fact that estimation error due to noise at slower time scales acts as a bias term for the faster component HMMs. We have provided fast estimation schemes with provable asymptotic properties.
Because we cast MSHMMs in the spectral estimation framework, unlike other hierarchical HMMs, they scale gracefully with the size and dimensionality of the data, increasing the range of problems they can address. They apply naturally to high-frequency financial time series, as they capture both the short- and long-term covariances between stock prices, which are used as the basis for decisionmaking about portfolio management and risk assessment, and they rapidly detect market changes. The MSHMM can also potentially be applied to other types of multi-time scale data in domains such as natural language processing, vision, speech recognition, neuroscience, and macroeconomics.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. JMLR, 15(1):2773­2832, 2014.
Tim Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3): 307­327, 1986.
Tim Bollerslev, Tzuo Hann Law, and George Tauchen. Risk, jumps, and diversification. Journal of Econometrics, 144(1):234­256, 2008.
Laurent E Calvet and Adlai J Fisher. Multifractal volatility: theory, forecasting, and pricing. Academic Press, 2008.
Ozgu¨r Cetin and Mari Ostendorf. Multi-rate hidden markov models and their application to machining toolwear classification. In ICASSP, volume 5, pp. V­837. IEEE, 2004.
Paramveer S. Dhillon, Jordan Rodu, Michael Collins, Dean P. Foster, and Lyle H. Ungar. Spectral dependency parsing with latent variables. In EMNLP-CoNLL 2012, pp. 205­213, 2012.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In NIPS, pp. 493­499. Citeseer, 1995.
Robert F Engle and Kenneth F Kroner. Multivariate simultaneous generalized arch. Econometric theory, 11 (01):122­150, 1995.
Jianqing Fan, Yuan Liao, and Han Liu. An overview of the estimation of large covariance and precision matrices. The Econometrics Journal, 19(1):C1­C32, 2016.
Dean P Foster, Jordan Rodu, and Lyle H Ungar. Spectral dimensionality reduction for hmms. arXiv preprint arXiv:1203.6130, 2012.
Reinhard Furrer and Thomas Bengtsson. Estimation of high-dimensional prior and posterior covariance matrices in kalman filter variants. Journal of Multivariate Analysis, 98(2):227­255, 2007.
Zoubin Ghahramani and Michael I Jordan. Factorial hidden markov models. Machine learning, 29(2-3):245­ 273, 1997.
Christian Gourie´roux, Joann Jasiak, and Razvan Sufana. The wishart autoregressive process of multivariate stochastic volatility. Journal of Econometrics, 150(2):167­181, 2009.
Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460­1480, 2012.
Xuedong D Huang, Yasuo Ariki, and Mervyn A Jack. Hidden Markov models for speech recognition, volume 2004. Edinburgh university press Edinburgh, 1990.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
J Zico Kolter and Tommi Jaakkola. Approximate inference in additive factorial hmms with application to energy disaggregation. In International conference on artificial intelligence and statistics, pp. 1472­1482, 2012.
Diaa Noureldin, Neil Shephard, and Kevin Sheppard. Multivariate high-frequency-based volatility (heavy) models. Journal of Applied Econometrics, 27(6):907­933, 2012.
Peter Nystrup, Henrik Madsen, and Erik Lindstro¨m. Long memory of financial time series and hidden markov models with time-varying parameters. Journal of Forecasting, 2016.
Peter Nystrup, Henrik Madsen, and Erik Lindstro¨m. Dynamic portfolio optimization across hidden market regimes. Quantitative Finance, pp. 1­13, 2017.
Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, and Eric P. Xing. A spectral algorithm for latent junction trees. In Nando de Freitas and Kevin P. Murphy (eds.), UAI, pp. 675­684. AUAI Press, 2012.
J. Rodu. Dissertation: Spectral Estimation of Hidden Markov Models. University of Pennsylvania, 2014. URL http://books.google.com/books?id=2SztoQEACAAJ.
9

Under review as a conference paper at ICLR 2018 Jordan Rodu, Dean P Foster, Weichen Wu, and Lyle H Ungar. Using regression for spectral estimation of
hmms. In Statistical Language and Speech Processing, pp. 212­223. Springer, 2013. Alessandro Rossi and Giampiero M Gallo. Volatility estimation via hidden markov models. Journal of Empir-
ical Finance, 13(2):203­230, 2006. Kevin Sheppard and Wen Xu. Factor high-frequency based volatility (heavy) models. Available at SSRN
2442230, 2014. L. Song, B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola. Hilbert space embeddings of hidden Markov
models. In Proceedings of the 27th International Conference on Machine Learning (ICML-210), 2010. Sebastian Thrun, Wolfram Burgard, and Dieter Fox. A probabilistic approach to concurrent mapping and
localization for mobile robots. Autonomous Robots, 5(3-4):253­271, 1998. Wei Biao Wu and Han Xiao. Covariance matrix estimation in time series. Handbook of Statistics, 30:187­209,
2012. Yue Wu, Jose M Hernandez-lobato, and Ghahramani Zoubin. Dynamic covariance models for multivariate
financial time series. In ICML, pp. 558­566, 2013.
10

Under review as a conference paper at ICLR 2018

Supplemental Material

A SPECTRAL ESTIMATION ALGORITHM

Algorithm 3 Computing observables for spectral estimation of an HMM, fully reduced third mo-

ment

1: Input: Training examples- {x1i , x2i , x3i } for i  {1, . . . , N T }.

2:

Compute E^[x2  x1] =

1 NT

NT i=1

x2i

x1i

.

3: Compute7 the left k singular vectors corresponding to the top k singular values of E^[x2  x1]. Call the

matrix of these singular vectors U^ .

4: Reduce data: y^ = U^ x.

5:

Compute µ^ =

1 NT

NT i=1

y1i

,

^

=

1 NT

6: Set ^b1 = µ^ and b = b1 ^ -1.

NT i=1

y2i

y1i

and

tensor

C^

=

1 NT

NT i=1

y3i



y1i



y2i .

7: Right multiply each slice of the tensor in the y2 direction (so y2 is being sliced up, leaving the y3y1 matrices intact) by ^ -1 to form B^() = C^()^ -1.

B COMPARISON TO RELATED HIDDEN MARKOV MODEL
Multirate HMMs have multiple hidden state processes, which we refer to as "levels" or "components," each evolving at different rates. The main structural difference between the multirate HMM (as seen in figure 1) and our model (figure 3) is that only the first level emits an observable; all subsequent "higher" levels output to the level "below," which has a faster hidden state process. MSHMM has additive emissions results, whereas, multirate HMM will propagate the same error upstream through different levels of hidden states.

h(t2-)1

ht(2+)1

ht(1-)1

ht(1)

ht(1+)1

ht(1+)2

Xt-1

Xt

Xt+1

Xt+2

Figure 3: Multi-rate HMM with 2 hidden state processes.
By way of contrast, in additive FHMMs (figure 2) all hidden state processes evolve at the same rate, but the higher level process emits to both observations at t and t + 1.
C SYNTHETIC DATA
The first 2500 observations for the first entry in the emission vector are plotted in figure 4. One can see the effect of the transitions of different component HMMs. The computational time for both training and prediction of the MSHMM is shown in figure 5.
D FURTHER DISCUSSION OF OTHER HIERARCHICAL HMMS
Further, latent junction trees are constrained to a fixed tree topology. This has two ramifications. First, a separate model needs to be estimated for each desired topology, and more importantly, in the application presented here, the tree topology would have to be extremely wide to accommodate the desired length of observation history, requiring an unmanageable computational burden.

11

Under review as a conference paper at ICLR 2018

ht(M-1)

h(tM )

h(tM+1)

ht(2-)1

h(t2)

h(t2+)1

h(t1-)1

h(t1)

ht(1+)1

Xt-1 Xt Xt+1
Figure 4: Factorial HMM with M hidden state processes.

h(t2-)1

(2) = 2

ht(2+)1

h(t1-)1

h(t1)

ht(1+)1

Xt(-1)1
+

Xt(-2)1

Xt(1)
+

Xt(+1)1
+

Xt(+2)1

Xt-1 Xt Xt+1

Figure 5: An MSHMM with two HMMs where h(t2) is the hidden state sequence of the slower moving HMM with transitions happening at time steps (2) = 2. The observations Xt are the sum
of the emissions of the two HMMs, so Xt = Xt(1) + Xt(-2)1.

12

Under review as a conference paper at ICLR 2018
Figure 6: Synthetic MSHMM data plot for 3 hidden state level and 5 hidden states for each (MSHMM-3-5)
Figure 7: Computational time for MSHMM
E FURTHER DETAILS OF DATA
Daily stock closing prices were extracted from the Center for Research in Securities Prices (CRSP) database8. Daily returns where adjusted for corporate actions such as dividends and stock splits. All intraday data was extracted from the NYSE Trade and Quote (TAQ) database9. Intraday prices where filtered using trade flags and incorrect price quotations. Our reference intraday prices used the average of the best bid price and best ask price, and all stock returns are calculated using the difference of log prices.
8http://www.crsp.com/products/research-products/crsp-us-stock-databases 9http://www.nyxdata.com/doc/2549
13

Under review as a conference paper at ICLR 2018
Figure 6 is illustrative of the multiple scales of stock variance of SPY. In particular, note that there are clearly different states for daily and intraday realized variance.

Figure 8: S&P 500 ETF (SPY) realized variance at daily, 17-minute, and 1-minute sampling rates over 10 years, 1 year, and 1 month respectively.
We chose the 30 stocks: TROW, AAPL, AA, CMA, FITB, BEN, MMC, HRB, BK, AFL, AXP, CB, IBM, MSFT, PNC, XOM, PGR, KEY, LM, SLM, AIG, STI, ALL, COF, UNH, SPY, GLD, TLT, FEZ, USO, UNG , as well as exchange trader funds (ETFs) such as SPY, which tracks the S&P 500 index, and GLD which tracks the value of gold prices. Each realized covariance matrix has 465 unique entries. We calculated realized variance over 10 years of historical daily data, 2 years of 17 minute intraday data, and 1 year of minute intraday data. To be exact, for training we used 2268 daily,  22000 17 minute, and  20000 minute datapoints, and for testing we used 252 daily,  2600 17 minute, and  4000 minute datapoints. Due to market conditions such as stock halts, the number of intraday data points is not the same for all stocks, so we reported approximate number of observations. Particularly, the top subplot of figure 7 corresponds to the adjusted daily prices of SPY and GLD.
Figure 7 also demonstrates the covariance between SPY and GLD, illustrating the multiscale nature of the covariance between SPY and GLD.

F FURTHER DISCUSSION OF EVALUATION

In the case

of covariance prediction we

define

MSE

=

1 T -k

T 1

-k

(C

-

E (C )))2 .

A close analysis of the results shows that HEAVY performs better at estimating covariance of stocks but not ETFs (SPY, GLD, USO, etc ). We surmised that this is because the covariance matrix for stocks tends to be close to a diagonal matrix, unlike ETFs, which have more significant off diagonal components. Since the variance is larger than the covariance, this accounted for the outperformance. We surmised that this is because companies are driven by very few factors, as a result the MSHMM hidden states focused on predicting covariance of ETF. However, at relatively high frequency timefames, our model outperformed. Again the intuition is that microstructure creates both fat tails as well as sparse covariance matrices, which are akin to discrete problems.

14

Under review as a conference paper at ICLR 2018
Figure 9: S&P 500 ETF (SPY) and SPDR Gold Shares ETF (GLD) daily prices, and their realized daily covariance.
15

