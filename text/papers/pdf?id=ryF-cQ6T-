Under review as a conference paper at ICLR 2018
MACHINE LEARNING BY TWO-DIMENSIONAL HIERARCHICAL TENSOR NETWORKS: A QUANTUM INFORMATION THEORETIC PERSPECTIVE ON DEEP ARCHI-
TECTURES
Anonymous authors Paper under double-blind review
ABSTRACT
The resemblance between the methods used in studying quantum-many body physics and in machine learning has drawn considerable attention. In particular, tensor networks (TNs) and deep learning architectures bear striking similarities to the extent that TNs can be used for machine learning. Previous results used one-dimensional TNs in image recognition, showing limited scalability and a request of high bond dimension. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multipartite entanglement renormalization ansatz (MERA). This approach overcomes scalability issues and implies novel mathematical connections among quantum many-body physics, quantum information theory, and machine learning. While keeping the TN unitary in the training phase, TN states can be defined, which optimally encodes each class of the images into a quantum many-body state. We study the quantum features of the TN states, including quantum entanglement and fidelity. We suggest these quantities could be novel properties that characterize the image classes, as well as the machine learning tasks. Our work could be further applied to identifying possible quantum properties of certain artificial intelligence methods.
1 INTRODUCTION
Over the past years, we have witnessed a booming progress in applying quantum theories and technologies to realistic problems. Paradigmatic examples include quantum simulators Trabesinger et al. (2012) and quantum computers Steane (1998); Knill (2010); Buluta et al. (2011) aimed at tackling challenging problems that are beyond the capability of classical digital computations. The power of these methods stems from the properties quantum many-body systems.
Tensor networks (TNs) belong to the most powerful numerical tools for studying quantum manybody systems Verstraete et al. (2008); Oru´s (2014a;b); Ran et al. (2017b). The main challenge lies in the exponential growth of the Hilbert space with the system size, making exact descriptions of such quantum states impossible even for systems as small as O(102) electrons. To break the "exponential wall", TNs were suggested as an efficient ansatz that lowers the computational cost to a polynomial dependence on the system size. Astonishing achievements have been made in studying, e.g. spins, bosons, fermions, anyons, gauge fields, and so on Verstraete et al. (2008); Cirac & Verstraete (2009); Oru´s (2014b); Ran et al. (2017b) Ran et al. (2017b). TNs are also exploited to predict interactions that are used to design quantum simulators Ran et al. (2017a).
As TNs allowed the numerical treatment of difficult physical systems by providing layers of abstraction, deep learning achieved similar striking advances in automated feature extraction and pattern recognition LeCun et al. (2015). The resemblance between the two approaches is beyond superficial. At theoretical level, there is a mapping between deep learning and the renormalization group Be´ny (2013), which in turn connects holography and deep learning You et al. (2017); Gan & Shu (2017), and also allows studying network design from the perspective of quantum entanglement Levine et al. (2017). In turn, neural networks can represent quantum states Carleo & Troyer (2017); Chen et al. (2017); Huang & Moore (2017); Glasser et al. (2017).
1

Under review as a conference paper at ICLR 2018
Most recently, TNs have been applied to solve machine learning problems such as dimensionality reduction Cichocki et al. (2016; 2017), handwriting recognition Stoudenmire & Schwab (2016); Han et al. (2017). Through a feature mapping, an image described as classical information is transferred into a product state defined in a Hilbert space. Then these states are acted onto a TN, giving an output vector that determines the classification of the images into a predefined number of classes. Going further with this clue, it can be seen that when using a vector space for solving image recognition problems, one faces a similar "exponential wall" as in quantum many-body systems. For recognizing an object in the real world, there exist infinite possibilities since the shapes and colors change, in principle, continuously. An image or a gray-scale photo provides an approximation, where the total number of possibilities is lowered to 256N per channel, with N describing the number of pixels, and it is assumed to be fixed for simplicity. Similar to the applications in quantum physics, TNs show a promising way to lower such an exponentially large space to a polynomial one. This work contributes in two aspects. Firstly, we derive an efficient quantum-inspired learning algorithm based on a hierarchical representation that is known as tree TN (TTN) (see, e.g., Murg et al. (2015)). Compared with Refs. Stoudenmire & Schwab (2016); Han et al. (2017) where a onedimensional (1D) TN (called matrix product state (MPS) O¨ stlund & Rommer (1995)) is used, TTN suits more the two-dimensional (2D) nature of images. The algorithm is inspired by the multipartite entanglement renormalization ansatz (MERA) approach Vidal (2007; 2008); Cincio et al. (2008); Evenbly & Vidal (2009), where the tensors in the TN are kept to be unitary during the training. We test the algorithm on both the MNIST (handwriting recognition with binary images) and CIFAR (recognition of color images) databases and obtain accuracies comparable to the performance of convolutional neural networks. More importantly, the TN states can then be defined that optimally encodes each class of images as a quantum many-body state, which is akin to the study of a duality between probabilistic graphical models and TNs Robeva & Seigal (2017). We contrast the bond dimension and model complexity, with results indicating that a growing bond dimension overfits the data. we study the representation in the different layers in the hierarchical TN with t-SNE Van der Maaten & Hinton (2008), and find that the level of abstraction changes the same way as in a deep convolutional neural network Krizhevsky et al. (2012) or a deep belief network Hinton et al. (2006), and the highest level of the hierarchy allows for a clear separation of the classes. Finally, we show that the fidelities between each two TN states from the two different image classes are low, and we calculate the entanglement entropy of each TN state, which gives an indication of the difficulty of each class.
(b)
(c)
(d)
Figure 1: (Color online) The left figure (a) shows the configuration of TTN. The squares at the bottom represent the vectors obtained from the pixels of one image through the feature map. The sphere at the top represents the label. The right figures are (b) the illustrations of the environment tensor, (c) the schematic diagram of fidelity and (d) entanglement entropy calculation.
2 PRELIMINARIES OF TENSOR NETWORK AND MACHINE LEARNING
A TN is defined as a group of tensors whose indexes are shared and contracted in a specific way. TN can represent the partition function of a classical system, and also of a quantum many-body state.
2

Under review as a conference paper at ICLR 2018

For the latter, one famous example is the MPS with the following mathematical representation:

 =s1s2···sN-1sN

As[11]1 As[22]12

·

·

·

A A .[N-1]

[N ]

sN -1 N -2N -1 sN N -1

1···N -1

(1)

When describing a physical state, the indexes {sn} are called physical bonds that represent the physical Hilbert space, and dummy indexes {m} are called virtual bonds that carry the quantum entanglement. MPS is essentially a 1D state representation. When applied to 2D systems, MPS
suffers severe restrictions since one has to choose a snake-like 1D path that covers the 2D manifold.
This issue is known as the area law of entanglement entropy Verstraete & Cirac (2006); Hastings
(2007); Schuch et al. (2008).

A TTN (Fig. 1 (a)) provides a natural expression for 2D states, which we can write as a hierarchical structure of K layers:

K Nk

 =1,1···N1,4

T ,[k,n]
k+1,n k,n,1 k,n,2k,n,3k,n,4

{} k=1 n=1

(2)

where Nk is the number of tensors in the k-th layer. For simplicity, we ignore the indexes by using bold capitals, and write Eq. (2) as

K Nk

=

T[k,n],

(3)

k=1 n=1

as long as no confusion is caused. The summation signs are also omitted by providing that all indexes that are shared by two tensors will be contracted. Meanwhile, all vectors are assumed to be column. We will follow these conventions throughout this paper.

In a TTN, each local tensor is chosen to have one upward index and four downward indexes. For representing a pure state, the tensor on the top only has four downward indexes. All the indexes except the downward ones of the tensors in the first layer are dummy and will be contracted. In our work, the TTN is slightly different from the pure state representation, by adding an upward index to the top tensor (Fig. 1 (a)). This added index corresponds to the labels in the supervised machine learning.

Before training, we need to prepare the data with a feature function that maps N scalars (N is the dimension of the images) to the space of N vectors. The choice of the feature function is arbitrary: we chose the one used in Ref. Stoudenmire & Schwab (2016), where the dimension of each vector (denoted by d) can be controlled. Then, the space is transformed from that of N scalars to a dN dimensional Hilbert space.
After "vectorizing" the j-th image in the dataset, the output for classification is a d~-dimensional vector obtained by contracting the vectors with the TTN, which reads as

N
L~[j] =  v[j,n],

(4)

n=1

where {v[j,n]} denotes the n-th vector given by the j-th sample. One can see that d~is the dimension of the upward index of the top tensor, and should equal to the number of the classes. We use the convention that the position of the maximum value gives the classification of the image predicted by the TTN, akin to a softmax layer in a deep learning network.

For training, the cost function to be minimized is the square error, which is defined as

J
f = |L~[j] - L[j]|2,

(5)

j=1

where J is the number of training samples. L[j] is a d~-dimensional vector corresponding to the j-th label. For example, if the j-th sample belongs to the p-th class, L[j] is defined as

L[j] =

1, if  = p 0, otherwise

(6)

3

Under review as a conference paper at ICLR 2018

3 MERA-INSPIRED TRAINING ALGORITHM

We use MERA to derive a highly efficient training algorithm. To proceed, let us rewrite the cost function in the following form

J
f = ( v[j,n]v[j,n] - 2

L[j]v[j,n] + 1).

j=1 nn

n

(7)

Here, the third term comes from the normalization of L[j], and we assume that the second term is always real.

The dominant cost comes from the first term. We borrow the idea from the MERA approach to
reduce this cost. The central idea of MERA is the renormalization group of the entanglement Vidal
(2007). The renormalization group flows are implemented by tensors that satisfy orthogonal condi-
tions. More specifically, the indexes of one tensor are grouped into to two kinds, say upward and
downward indexes. By contracting all the downward indexes of a tensor with its conjugate, one gets an identity, i.e., TT = I. On one hand, the orthogonality makes the state remain normalized, a basic requirement of quantum states. On the other hand, the renormalization group flows can be
considered as the compressions of the Hilbert space (from the downward to upward indexes). The orthogonality ensure that such compressions are unbiased with TT  I in the subspace. The difference from the identity characterizes the errors caused by the compressions.

In our case with the TTN, each tensor has one upward and four downward indexes, which gives a
non-square orthogonal matrix by grouping the downward indexes into a large one. Such tensors are
called isometries. When all the tensors are isometries, the TTN gives a unitary transformation that compresses a dN -dimensional space to a d~-dimensional one. One will approximately have   I in the subspace that optimizes the classification. For this reason, the first term can be considered as
a constant with the orthogonality, and the cost function becomes

J
f =-

L[j] v[j,n] .

j=1 n

(8)

Each term in f is simply the contraction of one TN, which can be efficiently computed. We stress that independent of Eq. (5), Eq. (8) can be directly used as the cost function. This will lead to a more interesting picture connected to the quantum information theory. The details are given in Sec. 5.

The tensors in the TTN are updated alternatively to minimize Eq. (8). To update the tensor T[k,n] for instance, we assume other tensors are fixed and define the environment tensor E[k,n], which is calculated by contracting everything in Eq. (8) after taking out T[k,n] (Fig. 1 (b)) Evenbly & Vidal (2009). Then the cost function becomes f = -Tr(T[k,n]E[k,n]). Under the constraint that T[k,n] is an isometry, the solution of the optimal point is given by T[k,n] = VU where V and U are calculated from the singular value decomposition E[k,n] = UV. At this point, we have f = - a a.
The update of one tensor becomes the calculation of the environment tensor and its singular value decomposition. In the alternating process for updating all the tensors, some tricks are used to accelerate the computations. The idea is to save some intermediate results to avoid repetitive calculations by taking advantage of the tree structure. Another trick is to normalize the vector each time obtained by contracting four vectors with a tensor.

The strategy for the training is the one-against-all classification scheme in machine learning (here
dubbed as Strategy-I). For each class, we train one TTN so that it recognizes whether an image
belongs to this class. The output of Eq. (4) is a two-dimensional vector. We fix the label for a yes answer as Lyes = [1, 0]. For P classes, we will accordingly have P TTNs, denoted by {(p)}. Then for recognizing an image (vectorized to {v[n]}), we define a P -dimensional vector F as

Fp = Lyes(p) v[n].
n

(9)

The position of its maximal element gives which class the image belongs to. For comparison, we also directly train the TTN by the samples labeled in P classes (dubbed as Strategy-II). In this case,

4

Under review as a conference paper at ICLR 2018

the output [Eq. (4)] is P -dimensional, where the position of its maximal element is expected to give the correct class.
The scaling of both time complexity and space complexity is O((bv5 + bi4)dNT ), where d is the dimension of input vector; bv the dimension of virtual bond; bi the dimension of input bond; NT the number of training inputs.

Training accuracy Training accuracy
Accuracy

1 0.95 0.9 0.85 0.8 0.75 0.7
2

34 Virtual bond
(a)

input bond=2 input bond=3 input bond=4 input bond=5
5

11

0.98 0.95
0.96

0.9 0.94

0.92 0.85 0.9

0.88 0.8

input bond=2

0.86

0.75

input bond=3

0.84

input bond=4 input bond=5

0.82

training testing

0.7 0.8

234523452

2 3 4 5 6 7 8 9 10

Virtual bond

Dimension of bond (input bond = virtual bond)

(b) (c)

Figure 2: (a) Binary classification accuracy on CIFAR-10 with number of training samples=200; (b) Binary classification accuracy on CIFAR-10 with number of training samples=600; (c) Training and test accuracy as the function of the dimension of indexes on the MNIST dataset. The number of training samples is 1000 for each pair of classes.

(a) (b) (c)

(d) (e) (f)
Figure 3: Embedding of data instances of CIFAR-10 by t-SNE corresponding to each layer in the TN. (a) Original data distribution; (b) 1st layer; (c) 2nd layer; (d) 3rd layer; (e) 4th layer; (f) 5th layer.

4 EXPERIMENTS ON IMAGE RECOGNITION
Our approach to classify image data begins by mapping each pixel xj to a d-component vector sj (xj ). This feature map was introduced by Stoudenmire & Schwab (2016)) and defined as Eq. (10)

sj (xj ) =

d-1 sj - 1

(cos(

 2

xj

))d-sj

(sin(

 2

xj

))sj

-1

5

(10)

Under review as a conference paper at ICLR 2018
where sj runs from 1 to d. By using a larger d, the TTN has the potential to approximate a richer class of functions. Our implementation is available under an open source license 1.
4.1 BENCHMARK ON CIFAR-10
To verify the representation power of TTNs, we used the CIFAR-10 dataset Krizhevsky & Hinton (2009). The dataset consists of 60,000 32 × 32 RGB images in 10 classes, with 6,000 instances per class. There are 50,000 training images and 10,000 test images. Each RGB image was originally 32 × 32 pixels: we transformed them to grayscale. Working with gray-scale images reduced the complexity of training, with the trade-off being that less information was available for learning.
We built a TTN with five layers and used the MERA-like algorithm (Section 3) to train the model. Specifically, we built a binary classification model to investigate key machine learning and quantum features, instead of constructing a complex multiclass model. We found both the input bond (physical indexes) and the virtual bond (geometrical indexes) had a great impact on the representation power of TTNs, as showed in Fig. 2. This indicates that the limitation of representation power (learnability) of the TTNs is related to the input bond. The same way, the virtual bond determine how accurately the TTNs approximate this limitation.
From the perspective of tensor algebra, the representation power of TTNs depends on the tensor contracted from the entire TTN. Thus the limitation of this relies on the input bond. Furthermore, the TTNs could be considered as a decomposition of this complete contraction, and the virtual bond determine how well the TTNs approximate this. Moreover, this phenomenon could be interpreted from the perspective of quantum many-body theory: the higher entanglement in a quantum manybody system, the more representation power this quantum system has.
The sequence of convolutional and pooling layers in the feature extraction part of a deep learning network is known to arrive at higher and higher levels of abstractions that helps separating the classes in a discriminative learner LeCun et al. (2015). This is often visualized by embedding the representation in two dimensions by t-SNE Van der Maaten & Hinton (2008), and by coloring the instances according to their classes. If the classes clearly separate in this embedding, the subsequent classifier will have an easy task performing classification at a high accuracy. We plotted this embedding for each layer in the TN in Fig. 3. We observe the same pattern as in deep learning, having a clear separation in the highest level of abstraction.
4.2 BENCHMARK ON MNIST
To test the generalization of TTNs on a benchmark dataset, we used the MNIST collection, which is widely used in handwritten digit recognition. The training set consists of 60,000 examples, and the test set of 10,000 examples. Each gray-scale image of MNIST was originally 28 × 28 pixels, and we rescaled them to 16 × 16 pixels for building TTNs with four layers on this scale. The MERA-like algorithm was used to train the model.
Similar to the last experiment, we built a binary model to show the performance of generalization. With the increase of bond dimension (both of the input bond and virtual bond), we found an apparent rise of training accuracy, which is consistent with the results in Fig. 2. At the same time, we observed the decline of testing accuracy. The increase of bond dimension leads to a sharp increase of the number of parameters and, as a result, it will give rise to overfitting and lower the performance of generalization. Therefore, one must pay attention to finding the optimal bond dimension ­ we can think of this as a hyperparameter controlling model complexity.
For multi-class learning, we choose the one-against-all strategy to build a 10-class model, which classify an input image by choosing the label for which the output is largest. To avoid overfitting and lower computing cost, we apply different feature map to each individual model. The parameters configuration and testing results are in Table 1. We repeated the t-SNE visualization on MNIST, but since the classes separate well even in the raw data, we did not include the corresponding figures here.
1The code of the implementation is available at https://github.com/dingliu0305/Tree-Tensor-Networks-in-Machine-Learning
6

Under review as a conference paper at ICLR 2018

model

Table 1: 10-class classification on MNIST 0 1 2 3 4 5 6 7 8 9 10-class

Training accuracy (%) 96 97 96 94 96 94 97 94 93 94

Testing accuracy (%) 97 97 95 93 95 95 96 94 93 93 92

Input bond

3334262664/

Virtual bond

3344363666/

5 ENCODING IMAGES IN QUANTUM STATES: FIDELITY AND ENTANGLEMENT

Taking a TTN  trained with Strategy-II, we define P TN state as

[p] = L[p].

(11)

In [p], the upward index of the top tensor is contracted with the label (L[p]), giving a TN state that represents a pure quantum state.

The quantum state representations allow us to use quantum theories to study images and the related issues. Let us begin with the cost function. In Section 3, we started from a frequently used cost function in Eq. (5), and derived a cost function in Eq. (8). In the following, we show that such a cost function can be understood by the notion of fidelity. With Eq. (11), the cost function in Eq. (8) can be rewritten as

f = - [p]T v[j,n].
jn

(12)

Knowing that the fidelity between two states is defined as their inner product, each term in the
summation is simply the fidelity Steane (1998); Bennett & DiVincenzo (2000) between a vectorized image and the corresponding state [p]. Considering the fidelity measure as the distance between two states, {[p]} are the P states, where the distance between each [p] and the corresponding
vectorized images are minimized. In other words, the cost function is in fact the total fidelity, and [p] is the quantum state that optimally encodes the p-th class of images.

Note that due to the orthogonality of the top tensor, such P states are orthogonal to each other, i.e., [p] [p] = Ipp. This might trap us to a bad local minimum. For this reason, we propose Strategy-
I. For each class, we train a TTN to give yes-or-no answers. Each TTN gives two TN states labeled yes and no, respectively. Then we will have 2P TN states. {[p]} are then defined by taking the
P yes-labeled TN states. The elements of F in Eq. (9) are defined by the summation of the fidelity between [p] and the class of vectorized images. In this scenario, the classification is decided by finding the [p] that gives the maximal fidelity with the input image, while the orthogonal conditions among {[p]} no longer exist.

Besides the algorithmic interpretation, fidelity may imply more intrinsic information. Without the orthogonality of {[p]}, the fidelity Fpp = [p][p] (Fig. 1 (c)) describes the differences between the quantum states that encode different classes of images. As shown in Fig. 4(a), Fpp remains quite small in most cases, indicating that the orthogonality still approximately holds using Strategy-I. Still, some results are still relatively large, e.g., F4,9 = 0.1353. We speculate this is closely related to the ways how the data are fed and processed in the TN. In our case, two image classes that have
similar shapes will result in a larger fidelity, because the TTN essentially provides a real-space
renormalization flow. In other words, the input vectors are still initially arranged and renormalized
layer by layer according to their spatial locations in the image; each tensor renormalizes four nearest-
neighboring vectors into one vector. Fidelity can be potentially applied to building a network, where the nodes are classes of images and the weights of the connections are given by the Fpp. This might provide a mathematical model on how different classes of images are associated to each other. We
leave these questions for future investigations.

Another important concept of quantum mechanics is (bipartite) entanglement, a quantum version of correlations Bennett & DiVincenzo (2000). It is one of the key characters that distinguishes

7

Under review as a conference paper at ICLR 2018

the quantum states from classical ones. Entanglement is usually given by a normalized positivedefined vector called entanglement spectrum (denoted as ), and is measured by the entanglement entropy S = - a 2a ln 2a. Having two subsystems, entanglement entropy measures the amount of information of one subsystem that can be gained by measuring the other subsystem. In the framework of TN, entanglement entropy determines the minimal dimensions of the dummy indexes needed for reaching a certain precision.
In our image recognition, entanglement entropy characterizes how much information of one part of the image we can gain by knowing the rest part of the image. In other words, if we only know a part of an image and want to predict the rest according to the trained TTN (the quantum state that encodes the corresponding class), the entanglement entropy measures how accurately this can be done. Here, an important analog is between knowing a part of the image and measuring the corresponding subsystem of the quantum state. Thus, the trained TTN might be used on image processing, e.g., to recover an image from a damaged or compressed lower-resolution version.
Fig. 4(b) shows the entanglement entropy for each class in the MNIST dataset. With the TTN, the entanglement spectrum is simply the singular values of the matrix M = LT[K,1] with L the label and T[K,1] the top tensor (Fig. 1 (d)). This is because the all the tensors in the TTN are orthogonal. Note that M has four indexes, of which each represents the effective space renormalized from one quarter of the vectorized image. Thus, the bipartition of the entanglement determines how the four indexes of M are grouped into two bigger indexes before calculating the SVD. We compute two kinds of entanglement entropy by cutting the system in the middle along the x or y direction. Our results suggest that the images of "0" and "4" are the easiest and hardest, respectively, to predict one part of the image by knowing the other part.

Handwritten Digit Entanglement Entropy

0 1 2 3 4 5 6 7 8 9
0123456789 Handwritten Digit
(a)

1 0.8
0.7 0.8
0.6

up-down left-right

0.6 0.5 0.4
0.4 0.3

0.2 0.2
0.1

0

0 0123456789

Handwritten Digit

(b)

Figure 4: (a) Fidelity Fpp between each two handwritten digits, which ranges from -0.0032 to 1. The diagonal terms Fpp = 1 because the quantum states are normalized; (b) Entanglement entropy
corresponding to each handwritten digit entropy.

6 CONCLUSION AND OUTLOOK
We continued the forays into using tensor networks for machine learning, focusing on hierarchical, two-dimensional tree tensor networks that we found a natural fit for image recognition problems. This proved a scalable approach that had a high precision, and we can conclude the following observations:
· The limitation of representation power (learnability) of the TTNs model strongly depends on the open indexes (physical indexes). And, the inner indexes (geometrical indexes) determine how well the TTNs approximate this limitation.
· A hierarchical tensor network exhibits the same increase level of abstraction as a deep convolutional neural network or a deep belief network.
· Fidelity can give us an insight how difficult it is to tell two classes apart. · Entanglement entropy can characterize the difficulty of representing a class of problems.
In future work, we plan to use fidelity-based training in an unsupervised setting and applying the trained TTN to recover damaged or compressed images and using entanglement entropy to characterize the accuracy.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Charles H. Bennett and David P. DiVincenzo. Quantum information and computation. Nature, 404: 247­255, 2000.
Ce´dric Be´ny. Deep learning and the renormalization group. January 2013.
Iulia Buluta, Sahel Ashhab, and Franco Nori. Natural and artificial atoms for quantum computation. Reports on Progress in Physics, 74(10):104401, 2011.
Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602­606, February 2017.
Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xiang. On the equivalence of restricted Boltzmann machines and tensor network states. 2017.
Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, Danilo P. Mandic, and Others. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 lowrank tensor decompositions. Foundations and Trends R in Machine Learning, 9(4-5):249­429, 2016.
Andrzej Cichocki, Anh-Huy Phan, Qibin Zhao, Namgil Lee, Ivan Oseledets, Masashi Sugiyama, Danilo P. Mandic, and Others. Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives. Foundations and Trends R in Machine Learning, 9(6):431­673, 2017.
Lukasz Cincio, Jacek Dziarmaga, and Marek M. Rams. Multiscale entanglement renormalization ansatz in two dimensions: quantum Ising model. Physical Review Letters, 100:240603, 2008.
J. Ignacio. Cirac and Frank Verstraete. Renormalization and tensor product states in spin chains and lattices. Journal of Physics A: Mathematical and Theoretical, 42:504004, 2009.
Glen Evenbly and Guifre Vidal. Algorithms for entanglement renormalization. Physical Review B, 79:144108, April 2009. doi: 10.1103/PhysRevB.79.144108.
Wen-Cong Gan and Fu-Wen Shu. Holography as deep learning. 2017.
Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, and J. Ignacio Cirac. Neural networks quantum states, string-bond states and chiral topological states. October 2017.
Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, and Pan Zhang. Unsupervised generative modeling using matrix product states. September 2017.
Matthew B. Hastings. An area law for one-dimensional quantum systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(08), 2007.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527­1554, July 2006. ISSN 1530-888X.
Yichen Huang and Joel E. Moore. Neural network representation of tensor network and chiral states. 2017.
Emanuel Knill. Physics: quantum computing. Nature, 463(7280):441­443, 2010.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, volume 25, pp. 1097­1105. 2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, May 2015. ISSN 1476-4687.
Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. 2017.
9

Under review as a conference paper at ICLR 2018
Valentin Murg, Frank Verstraete, Reinhold Schneider, Pe´ter R. Nagy, and O¨ rs Legeza. Tree tensor network state with variable tensor order: An efficient multireference method for strongly correlated systems. Journal of Chemical Theory and Computation, 11:1027­1036, 2015.
Roma´n Oru´s. A practical introduction to tensor networks: Matrix product states and projected entangled pair states. Annals of Physics, 349:117, 2014a.
Roma´n Oru´s. Advances on tensor network theory: symmetries, fermions, entanglement, and holography. The European Physical Journal B, 87(11):280, November 2014b.
Stellan O¨ stlund and Stefan Rommer. Thermodynamic limit of density matrix renormalization. Physical Review Letters, 75:3537, 1995.
Shi-Ju Ran, Angelo Piga, Cheng Peng, Gang Su, and Maciej Lewenstein. Few-body systems capture many-body physics: Tensor network approach. 2017a.
Shi-Ju Ran, Emanuele Tirrito, Cheng Peng, Xi Chen, Gang Su, and Maciej Lewenstein. Review of tensor network contraction approaches. 2017b.
Elina Robeva and Anna Seigal. Duality of graphical models and tensor networks. 2017. Norbert Schuch, Michael M. Wolf, Frank Verstraete, and J. Ignacio Cirac. Entropy scaling and
simulability by matrix product states. Physical Review Letters, 100:030504, January 2008. Andrew Steane. Quantum computing. Reports on Progress in Physics, 61(2):117­173, 1998. E. Miles Stoudenmire and David J. Schwab. Supervised learning with tensor networks. Advances in
Neural Information Processing Systems, 29:4799­4807, 2016. Andreas Trabesinger, J. Ignacio Cirac, Peter Zoller, Immanuel Bloch, Jean Dalibard, and Sylvain
Nascimbe´ne. Nature Physics Insight - Quantum Simulation. Nature Physics, 8, 2012. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(85):2579­2605, 2008. Frank Verstraete and J. Ignacio Cirac. Matrix product states represent ground states faithfully. Phys-
ical Review B, 73:094423, 2006. Frank Verstraete, Valentin Murg, and J. Ignacio Cirac. Matrix product states, projected entangled
pair states, and variational renormalization group methods for quantum spin systems. Advances in Physics, 57:143­224, 2008. Guifre Vidal. Entanglement renormalization. Physical Review Letters, 99:220405, 2007. Guifre Vidal. Class of quantum many-body states that can be efficiently simulated. Physical Review Letters, 101:110501, 2008. Yi-Zhuang You, Zhao Yang, and Xiao-Liang Qi. Machine learning spatial geometry from entanglement features. September 2017.
10

