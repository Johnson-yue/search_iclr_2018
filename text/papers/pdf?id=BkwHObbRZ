LEARNING ONE-HIDDEN-LAYER NEURAL NETWORKS WITH LANDSCAPE DESIGN
Anonymous authors Paper under double-blind review

ABSTRACT
We consider the problem of learning a one-hidden-layer neural network: we assume the input x  Rd is from Gaussian distribution and the label y = a (Bx) + , where a is a nonnegative vector in Rm with m  d, B  Rm×d is a full-rank weight matrix, and  is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. Inspired by the formula, we design a non-convex objective function G(·) whose landscape is guaranteed to have the following properties:
1. All local minima of G are also global minima. 2. All global minima of G correspond to the ground truth parameters. 3. The value and gradient of G can be estimated using samples.
With these properties, stochastic gradient descent on G provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations.

1 INTRODUCTION
Scalable optimization has been playing crucial roles in the success of deep learning, which has immense applications in artificial intelligence. Remarkably, optimization issues are often addressed through designing new models that make the resulting training objective functions easier to be optimized. For example, over-parameterization Livni et al. (2014), batch-normalization Ioffe & Szegedy (2015), and residual networks He et al. (2016a;b) are often considered as ways to improve the optimization landscape of the resulting objective functions.
How do we design models and objective functions that allow efficient optimization with guarantees? Towards understanding this question in a principled way, this paper studies learning neural networks with one hidden layer. Roughly speaking, we will show that when the input is from Gaussian distribution and under certain simplifying assumptions on the weights, we can design an objective function G(·), such that
[a] all local minima of G(·) are global minima
[b] all the global minima are the desired solutions, namely, the ground-truth parameters (up to permutation and some fixed transformation).
We note that designing such objective functions is challenging because 1) the natural 2 loss objective does have bad local minimum, and 2) due to the permutation invariance1, the objective function inherently has to contain an exponential number of isolated local minima.

1.1 SETUP AND KNOWN ISSUES WITH PROPER LEARNING

We aim to learn a neural network with a one-hidden-layer using a non-convex objective function. We assume input x comes from Gaussian distribution and the label y comes from the model

y = a (B x) + 

(1.1)

1Permuting the rows of B and the coordinates of a correspondingly preserves the functionality of the network.

1

where a  Rd, B  Rm×d are the ground-truth parameters, (·) is a element-wise non-linear function, and  is a noise vector with zero mean. Here we can without loss of generality assume x comes from spherical Gaussian distribution N (0, Idd×d). 2
For technical reasons, we will further assume m  d and that a has non-negative entries.

The most natural learning objective is perhaps the 2 loss function, given the additive noise. Concretely, we can parameterize with training parameters a  Rd, B  Rm×d of the same dimension as a and B correspondingly,

y^ = a (Bx) ,

(1.2)

and then use stochastic gradient descent to optimize the 2 loss function. When we have enough training examples, we are effectively minimizing the following population risk with stochastic updates,

f (a, B) = E y^ - y 2 .

(1.3)

However, empirically stochastic gradient descent cannot converge to the ground-truth parameters in the synthetic setting above when (x) = ReLU(x) = max{x, 0}, even if we have access to an infinite number of samples, and B is a orthogonal matrix. Such empirical results have been reported in Livni et al. (2014) previously, and we also provide our version in Figure 1 of Section 4. This is consistent with observations and theory that over-parameterization is crucial for training neural networks successfully Livni et al. (2014); Hardt et al. (2016); Soudry & Carmon (2016).
These empirical findings suggest that the population risk f (a, B) has spurious local minima with inferior error compared to that of the global minimum. This phenomenon occurs even if we assume we know a or a = 1 is merely just the all one's vector. Empirically, such landscape issues seem to be alleviated by over-parameterization. By contrast, our method described in the next section does not require over-parameterization and might be suitable for applications that demand the recovery of the true parameters.

1.2 OUR CONTRIBUTIONS
Towards learning with the same number of training parameters as the ground-truth model, we first study the landscape of the population risk f (·) and give an analytic formula for it -- as an explicit function of the ground-truth parameters and training parameters with the randomness of the data being marginalized out. The formula in equation (2.3) shows that f (·) is implicitly attempting to solve simultaneously a finite number of low-rank tensor decomposition problems with commonly shared components.
Inspired by the formula, we design a new training model whose associated loss function -- named f and formally defined in equation (2.5) -- corresponds to the loss function for decomposing a matrix (2-nd order tensor) and a 4-th order tensor (Theorem 2.2). Empirically, stochastic gradient descent on f learns the network as shown Section 4.
Despite the empirical success of f , we still lack a provable guarantee on the landscape of f . The second contribution of the paper is to design a more sophisticated objective G(·) whose landscape is provably nice -- all the local minima of G(·) are proven to be global, and they correspond to the permutation of the true parameters. See Theorem 2.3.
Moreover, the value and the gradient of G can be estimated using samples, and there are no constraints in the optimization. These allow us to use straightforward SGD (see guarantees in Ge et al. (2015); Jin et al. (2017)) to optimize G(·) and converge to a local minimum, which is also a global minimum (Corollary 2.4).
Finally, we also prove a finite-sample complexity result. We will show that with a polynomial number of samples, the empirical version of G share almost the same landscape properties as G itself (Theorem 2.7). Therefore, we can also use an empirical version of G as a surrogate in the optimization.

1.3 RELATED WORK
The work of Arora et al. (2014) is one of the early results on provable algorithms for learning deep neural networks, where the authors give an algorithm for learning deep generative models with sparse weights. Livni et al. (2014), Zhang et al. (2016; 2017b), and Daniely et al. (2016) study the learnability of special cases of neural networks using
2This is because if x  N (0, ), then we can whiten the data by taking x = -1/2x and define B = B1/2. We note that B x = Bx and therefore we maintain the functionality of the model.

2

ideas from kernel methods. Janzamin et al. (2015) give a polynomial-time algorithm for learning one-hidden-layer neural networks with twice-differential activation function and known input distributions, using the ideas from tensor decompositions.
A series of recent papers study the theoretical properties of non-convex optimization algorithms for one-hidden-layer neural networks. Brutzkus & Globerson (2017) and Tian (2017) analyze the landscape of the population risk for onehidden-layer neural networks with Gaussian inputs under the assumption that the weights vector associated to each hidden variable (that is, the filters) have disjoint supports. Li & Yuan (2017) proves that stochastic gradient descent recovers the ground-truth parameters when the parameters are known to be close to the identity matrix. Zhang et al. (2017a) study the optimization landscape of learning one-hidden-layer neural networks with a specific activation function, and they design a specific objective function that can recover a single column of the weight matrix. Zhong et al. (2017) study the convergence of non-convex optimization from a good initializer that is produced by tensor methods. Our algorithm works for a large family of activation functions (including ReLU) and any full-rank weight matrix. To our best knowledge, we give the first global convergence result for gradient-based methods for our general setting. 3
The optimization landscape properties have also been investigated on simplified neural networks models. Kawaguchi (2016) shows that the landscape of deep neural nets does not have bad local minima but has degenerate saddle points. Hardt & Ma (2017) show that re-parametrization using identity connection as in residual networks He et al. (2016a) can remove the degenerate saddle points in the optimization landscape of deep linear residual networks. Soudry & Carmon (2016) show that an over-parameterized neural network does not have bad differentiable local minimum. Hardt et al. (2016) analyze the power of over-parameterization in a linear recurrent network (which is equivalent to a linear dynamical system.)
The optimization landscape has also been analyzed for other machine learning problems, including SVD/PCA phase retrieval/synchronization, orthogonal tensor decomposition, dictionary learning, matrix completion, matrix sensing Baldi & Hornik (1989); Srebro & Jaakkola (2013); Ge et al. (2015); Sun et al. (2015); Bandeira et al. (2016); Ge et al. (2016); Bhojanapalli et al. (2016); Ge et al. (2017). Our analysis techniques build upon that for tensor decomposition in Ge et al. (2015) -- we add two additional regularization terms to deal with spurious local minimum caused by the weights a and to remove the constraints.
Notations: We use · to denote the Euclidean norm of a vector and spectral norm of a matrix. We use · F to denote the Frobenius/Euclidean norm of a matrix or high-order tensor. For a vector x, let x 0 denotes its infinity norm and for a matrix A, let |A|0 be a shorthand for vec(A) 0 where vec(A) is the vectorization of A.
We use A  B to denote the Kronecker product of A and B, and Ak is a shorthand for A  · · ·  A where A appears k times. For vectors a  b and ak denote the tensor product. We denote the identity matrix in dimension d × d by Idd×d, or Id when the dimension is clear from the context. We will define other notations when we first use them.

2 MAIN RESULTS

2.1 CONNECTING 2 POPULATION RISK WITH TENSOR DECOMPOSITION
We first show that a natural 2 loss for the one-hidden-layer neural network can be interpreted as simultaneously decomposing tensors of different orders. A straightforward approach of learning the model (1.1) is to parameterize the prediction by

y^ = a (Bx) ,

(2.1)

where a  Rd, B  Rm×d are the training parameters. Naturally, we can use 2 as the empirical loss, which means the population risk is

f (a, B) = E y^ - y 2 .

(2.2)

3The work of Janzamin et al. (2015); Zhong et al. (2017) are closely related, but they require tensor decomposition as the algorithm/initialization.

3

Throughout the paper, we use b1 , . . . , bm to denote the row vectors of B and similarly for B. That is, we have

b1 

 b1 

B

=

 

...

 and B 

= 

...

. 

Let

ai

and

ai

's

be

the

coordinates

of

a

and

a

respectively.

bm bm

We give the following analytic formula for the population risk defined above.

Theorem 2.1. Assume vectors bi, bi 's are unit vectors. Then, the population risk f defined in equation (2.2) satisfies that
2

f (a, B) = ^k2

ai bi k -

aibi k + const .

(2.3)

kN

i[m]

i[m]

F

where ^k is the k-th Hermite coefficient of the function . See section A.1 for a short introduction of Hermite polynomial basis. 4

Connection to tensor decomposition: We see from equation (2.3) that the population risk of f is essentially an average
of infinite number of loss functions for tensor decomposition. For a fixed k  N, we have that the k-th summand in equation (2.3) is equal to (up to the scaling factor ^k2)

fk Tk -

aibik

2 F

.

i[m]

(2.4)

where Tk = i[m] ai bi k is a k-th order tensor in (Rd)k. We note that the objective fk naturally attempts to decompose the k-order rank-m tensor Tk into m rank-1 components a1bi k, . . . , ambmk.

The proof of Theorem 2.1 follows from using techniques in Hermite Fourier analysis, which is deferred to Section A.2.

Issues with optimizing f :. It turns out that optimizing the population risk using stochastic gradient descent is empirically difficult. Figure 1 shows that in a synthetic setting where the noise is zero, the test error empirically doesn't converge to zero for sufficiently long time with various learning rate schemes, even if we are using fresh samples in iteration. This suggests that the landscape of the population risk has some spurious local minimum that is not a global minimum. See Section 4 for more details on the experiment setup.

An empirical fix:. Inspired by the connection to tensor decomposition objective described earlier in the subsection,

we can design a new objective function that takes exactly the same form as the tensor decomposition objective function

f2 + f4.

Concretely, let's define y^

=a

(Bx) where  = ^2h2 + ^4h4 and h2(t) =

1 (t2
2

-

1)

and

h4(t)

=

1 (t4 - 6t2 + 3) are the 2nd and 4th normalized probabilists' Hermite polynomials Wikipedia (2017a). We abuse
24

the notation slightly by using the same notation to denote the its element-wise application on a vector. Now for each

example we use y^ - y 2 as loss function. The corresponding population risk is

f (a, B) = E y^ - y 2 .

(2.5)

Now by an extension of Theorem 2.1, we have that the new population risk is equal to the ^22f2 + ^42f4.

Theorem 2.2. Let f be defined as in equation (2.5) and f2 and f4 be defined in equation (2.4). Assume bi, bi 's are unit vectors. Then, we have

f = ^22f2 + ^42f4 + const

(2.6)

It turns out stochastic gradient descent on the objective f (a, B) (with projection to the set of matrices B with row norm 1) converges empirically to the ground truth (a , B ) or one of its equivalent permutations. (See Figure 2.) However, we don't know of any existing work for analyzing the landscape of the objective f (or fk for any k  3). We conjecture that the landscape of f doesn't have any spurious local minimum under certain mild assumptions on (a , B ). Despite recent attempts on other loss functions for tensor decomposition Ge & Ma (2017), we believe that analyzing f is technically challenging and its resolution will be potentially enlightening for the understanding landscape of loss function with permutation invariance. See Section 4 for more experimental results.

4

When



=

ReLU ,

we

have

that

^0

=

1 , 2

^1

=

1 2

.

For

n



2

and

even,

^n

=

((n-3)!!)2 . 2n!

For

n



2

and

odd,

^n

=

0.

4

2.2 LANDSCAPE DESIGN FOR ORTHOGONAL B

The population risk defined in equation (2.5) -- though works empirically for randomly generated ground-truth (a , B ) -- doesn't have any theoretical guarantees. It's also possible that when (a , B ) are chosen adversarially or from a different distribution, SGD no longer converges to the true parameters.

To solve this problem, we design another objective function G(·), such that the optimizer of G(·) still corresponds to the ground-truth, and G() has provably nice landscape -- all local minima of G() are global minima.

In this subsection, for simplicity, we work with the case when B is an orthogonal matrix and state our main result. The discussion of the general case is deferred to the end of this Section and Section C.

We define our objective function G(B) as

G(B)

 
m

sign(^4) E y ·

(bj, bk, x) - µ sign(^4) E y · (bj, x) +  ( bi 2 - 1)2

j,k[d],j=k

j[d]

i=1

(2.7)

where (·, ·) is defined as

and (·, ·, ·) is defined as

1 (v, x) =

v

4

-

1 (v

x)2

v

2+

1 (v

x)4 .

84

24

(2.8)

(v, w, x) = 1 v 2 w 2 + v, w 2 - 1 w 2(v x)2 - 1 v 2(w x)2 2 22

+ 2(v

x)(w

x)v

1 w + (v

x)2(w

x)2 .

2

(2.9)

The rationale behind of the choices of  and  will only be clearer and relevant in later sections. For now, the only relevant property of them is that both are smooth functions whose derivatives are easily computable.

We remark that we can sample G(·) using the samples straightforwardly -- it's defined as an average of functions of examples and the parameters. We also note that only parameter B appears in the loss function. We will infer the value of a using straightforward linear regression after we get the (approximately) accurate value of B .

Due to technical reasons, our method only works for the case when ai > 0 for every i. We will assume this throughout the rest of the paper. The general case is left for future work. Let amax = max ai , amin = min ai , and  = max ai / min ai . Our result will depend on the value of  . Essentially we treat  as an absolute constant that doesn't scale in dimension. The following theorem characterizes the properties of the landscape of G(·).
Theorem 2.3. Let c be a sufficiently small universal constant (e.g. c = 0.01 suffices) and suppose the activation function  satisfies ^4 = 0. Assume µ  c/ ,   c-1amax, and B is an orthogonal matrix. The function G(·) defined as in equation (2.7) satisfies that

1. A matrix B is a local minimum of G if and only if B can be written as B = DP B where P is a permutation
matrix and D is a diagonal matrix with Dii  {±1 ± O(µamax/)}.5 Furthermore, this means that all local minima of G are also global.

2. Any saddle point B has a strictly negative curvature in the sense that min(2G(B))  -0 where 0 = c min{µamin/( d), }

3. Suppose B is an approximate local minimum in the sense that B satisfies

G(B)   and min(2G(B))  -0

Then B can be written as B = P DB + EB where P is a permutation matrix, D is a diagonal matrix satisfying the same bound as in bullet 1, and |E|  O(/(^4amin)).
As a direct consequence, B is Od()-close to a global minimum in Euclidean distance, where Od(·) hides polynomial dependency on d and other parameters.

5More precisely, |Dii| =

1 1-µ|^4|ai /( 6)

5

The theorem above implies that we can learn B (up to permutation of rows and sign-flip) if we take  to be sufficiently large and optimize G(·) using stochastic gradient descent. In this case, the diagonal matrix D in bullet 1 is sufficiently close to identity (up to sign flip) and therefore a local minimum B is close to B up to permutation of rows and sign flip. The sign of each bi can be recovered easily after we recover a (see Lemma 2.5 below.)
SGD converges to a local minimum Ge et al. (2015) (under the additional property as established in bullet 2 above), which is also a global minimum for the function G(·). We will prove the theorem in Section B as a direct corollary of Theorem B.1. The technical bullet 2 and 3 of the theorem is to ensure that we can use SGD to converge to a local minimum as stated below.6
Corollary 2.4. In the setting of Theorem 2.3, we can use stochastic gradient descent to optimize function G(·) (with fresh samples at each iteration) and converge to an approximate global minimum B that is -close to a global minimum in time poly(d, 1/).

After approximately recovering the matrix B , we can also recover the coefficient a easily. Note that fixing B, we can fit a using simply linear regression. For the ease of analysis, we analyze a slightly different algorithm. The lemma below is proved in Section D.
Lemma 2.5. Given a matrix B whose rows have unit norm, and are -close to B in Euclidean distance up to permutation and sign flip with   1/(2 ). Then, we can give estimates a, B (using e.g., Algorithm 1) such that there exists a permutation P where a - P a   amax and B is row-wise -close to P B .

The key step towards analyzing objective G(B) is the following theorem that gives an analytic formula for G(·). Theorem 2.6. The function G(·) satisfies

 G(B) = 2 6|^4| ·

ai

i[d] j,k[d],j=k

bi , bj

2

bi , bk

2

-

|^4|µ 6

ai
i,j[d]

bi , bj

m
4+ (
i=1

bi

2 - 1)2

(2.10)

Theorem 2.6 is proved in Section A. We will motivate our design choices with a brief overview in Section 3 and formally analyze the landscape of G in Section B (see Theorem B.1).

Finite sample complexity bounds. Extending Theorem 2.3, we can characterize the landscape of the empirical risk G, which implies that stochastic gradient on G also converges approximately to the ground-truth parameters with polynomial number of samples.
Theorem 2.7. In the setting of Theorem 2.3, suppose we use N empirical samples to approximate G and obtain empirical risk G. There exists a fixed polynomial poly(d, 1/) such that if N  poly(d, 1/), then with high probability the landscape of G has the properties to that of G in bullet 2 and 3 of Theorem 2.3.
All of the results above assume that B is orthogonal. Since the local minimum are preserved by linear transformation of the input space, these results can be extended to the general case when B is not orthogonal but full rank (with some additional technicality) or the case when the dimension is larger than the number of neurons (m < d). See Section C.

3 OVERVIEW: LANDSCAPE DESIGN AND ANALYSIS
In this section, we present a general overview of ideas behind the design of objective function G(·). Inspired by the formula (2.3), in Section 3.1, we envision a family of possible objective functions for which we have unbiased estimators via samples. In Section 3.2, we pick a specific function that feeds our needs: a) it has no spurious local minimum; b) the global minimum corresponds to the ground-truth parameters.
3.1 WHICH OBJECTIVE CAN BE ESTIMATED BY SAMPLES?
Recall that in equation (2.2) of Theorem 2.1 we give an analytic formula for the straightforward population risk f . Although the population risk f doesn't perform well empirically, the lesson that we learn from it help us design better
6In the most general setting, converging to a local minimum of a non-convex function is NP-hard.

6

objective functions. One of the key fact that leads to the proof of Theorem 2.1 is that for any continuous and bounded function , we have that

E y · (bi x) = ^k^k( aj bj , bi k) .

kN

j[d]

Here ^k and ^k are the k-th Hermite coefficient of the function  and . That is, letting hk the k-th normalized probabilists' Hermite polynomials Wikipedia (2017a) and ·, · be the standard inner product between functions, we
have ^k = hk,  .

Note that  can be chosen arbitrarily to extract different terms. For example, by choosing  = hk, we obtain that

E y · hk(bi x) = ^k

aj bj , bi k .

j[d]

(3.1)

That is, we can always access functions forms that involves weighted sum of the powers of bi , bi , as in RHS of equation (3.1). Using a bit more technical tools in Fourier analysis (see details in Section A), we claim that most of
the symmetric polynomials over variables bi , bj can be estimated by samples: Claim 3.1 (informal). For any polynomial p() over a single variable, there exits a corresponding function p such that

E [y · p(B, x)] = aj p( bj , bi )
ji
Moreover, for an any polynomial q(·, ·) over two variables, there exists corresponding q such that

(3.2)

E [y · q(B, x)] = aj q( bj , bi , bk, bi )
j i,k

(3.3)

We will not prove these two general claims. Instead, we only focus on the formulas in Theorem A.5 and Theorem A.6, which are two special cases of the claims above.
Motivated by Claim A.3, in the next subsection, we will pick an objective function which has no spurious local minimum among those functional forms on the right-hand sides of equation (3.2) and (3.3).

3.2 WHICH OBJECTIVE HAS NO SPURIOUS LOCAL MINIMA?

As discussed briefly in the introduction, one of the technical difficulties to design and analyze objective functions for neural networks comes from the permutation invariance -- if a matrix B is a good solution, then any permutation of the rows of B still gives an equally good solution (if we also permute the coefficients in a accordingly). We only know of a very limited number of objective functions that guarantee to enjoy permutation invariance and have no spurious local minima Ge et al. (2015). We start by considering the objective function used in Ge et al. (2015),

min P (B) =

bi , bj 2 bi , bk 2

i j=k

s.t. i  [d], bi = 1

(3.4)

Note that here we overload the notation by using bi 's to denote a set of fixed vectors that we wanted to recover and using bi's to denote the variables. Careful readers may notice that P (B) doesn't fall into the family of functions that
we described in the previous section (that is, RHS equation of (3.2) and (3.3)), because it lacks the weighting ai 's. We will fix this issue later in the subsection. Before that we first summarize the nice properties of the landscape of P (B).

For the simplicity of the discussion, let's assume B forms an orthonormal matrix in the rest of the subsection. Then, any permutation and sign-flip of the rows of B leads to a global minimum of P (·) -- when B = SQB with a permutation matrix Q and a sign matrix S (diagonal with ±1), we have that P (B) = 0 because one of bi , bj 2 and bi , bk 2 has to be zero for all i, j, k7).
It turns out that these permutations/sign-flips of B are also the only local minima8 of function P (·). To see this, notice that P (B) is a degree-2 polynomial of B. Thus if we pick an index s and fix every row except for bs, then P (B) is a

7Note that B is orthogonal, and j = k 8We note that since there are constraints here, we consider the local minimum on the manifold defined by the constraints.

7

quadratic function over unit vector bs ­ reduces to an smallest eigenvector problem. Eigenvector problems are known to have no spurious local minimum. Thus the corresponding function (w.r.t bs) has no spurious local minimum. It turns out the same property still holds when we treat all the rows as variables and add the row-wise norm constraints.

However, there are two issues with using objective function P (B). The obvious one is that it doesn't involve the coef-
ficients ai 's and thus doesn't fall into the forms of equation (3.3). Optimistically, we would hope that for nonnegative ai 's the weighted version of P below would also enjoy the similar landscape property

P (B) = ai

bi , bj 2 bi , bk 2

i j=k

When ai 's are positive, indeed the global minimum of P are still just all the permutations of the B .9 However, when max ai > 2 min ai , we found that P starts to have spurious local minima . It seems that spurious local minimum often occurs when a row of B is a linear combination of a smaller number of rows of B . See Section F for a concrete
example.

To remove such spurious local minima, we add a regularization term below that pushes each row of B to be close to one of the rows of B ,

R(B) = -µ ai

bi , bj 4

ij

(3.5)

We see that for each fixed j, the part in R(B) that involves bj has the form -µ i ai bi , bj 4 = -µ i ai bi 4, bj 4 This is commonly used objective function for decomposing tensor i ai bi 4. It's known that for orthogonal bi 's, the only local minima are ±b1, . . . , ±bd Ge et al. (2015). Therefore, intuitively R(B) pushes each of the bi's towards one of the bi 's. 10 Choosing µ to be small enough, it turns out that P (B) + R(B) doesn't have any spurious local
minimum as we will show in Section B.

Another issue with the choice of P (B) + R(B) is that we are still having a constraint minimization problem. Such row-wise norm constraints only make sense when the ground-truth B is orthogonal and thus has unit row norm. A straightforward generalization of P (B) to non-orthogonal case requires some special constraints that also depend
on the covariance matrix B B , which in turn requires a specialized procedure to estimate. Instead, we move the constraints into the objective function by considering adding another regularization term that approximately enforces the constraints.

It turns out the following regularizer suffices for the orthogonal case: S(B) =  i( bi 2 - 1)2 . Moreover, we can extend this easily to the non-orthogonal case (see Section C) without estimating any statistics of B in advance. We note that S(B) is not the Lagrangian multiplier and it does change the global minima slightly. We will take  to be large enough so that bi has to be close to 1. As a summary, we finally use the unconstrained objective
min G(B) P (B) + R(B) + S(B)
Since R(B) and S(B) are degree-4 polynomials of B, the analysis of G(B) is much more delicate, and we cannot use much linear algebra as we could for P (B). See Section B for details.

Finally we note that a feature of this objective G(·) is that it only takes B as variables. We will estimate the value of a after we recover the value of B. (see Section D).

4 SIMULATION

In this section, we provide simple simulation results that verify that minimizing G(B) with SGD recovers a permu-
tation of B ; however, minimizing Equation (2.2) with SGD results in finding spurious local minima. Based on the
formula for the population risk in Equation (2.3), we also verified empirically the conjecture that SGD would successfully recover B using the activation functions (z) = ^2h2(z) + ^4h4(z),11 even if the data were generated via a model with ReLU activation. (See Section 2.1 for the rationale behind such conjectures.)

9This is the main reason why we require a  0.

10However, note that R(B) by itself doesn't work because it does not prevent the solutions where all the bi's are equal to the

same bj .

11We also observed that using (z)

=

1 2

|z|

also

works

but

due

to

the

space

limitation

we

don't

report

the

experimental

results

here.

8

0.5 0.6

Parameter Error

Objective Value

0.4 0.5
0.4 0.3
0.3 0.2
0.2
0.1 0.1

0 1

23 Iterations

45 ×104

0 012345

Iterations

×104

Figure 1: Data are generated by a network with ReLU activation without noise. The training model uses the same architecture. Left: the estimated population risk doesn't converge to zero. Right: the parameter error using the surrogate in equation (4.1).

Objective Value

0.08 0.06 0.04 0.02

Parameter Error

0.8 0.6 0.4 0.2

0 0.5 1 1.5 2 2.5 3

Iterations

×105

0 0

12 Iterations

3 ×105

Figure 2: The labels are generated from a network with ReLU activation. We learn with ^2h2 + ^4h4 activation. Left: the test loss subtracted by the theoretical global minimum value. Right: the error in parameter space measured by
equation (4.1)

For all of our experiments, we chose B = Idd×d with dimension d = 50 and a = 1 for simplicity, and the data is generated from a one-hidden-layer network with ReLU activation without noise. We use stochastic gradient descent with fresh samples at each iteration, and we plot the (expected) population error (that is, the error on a fresh batch of examples).

To test whether SGD converges to a matrix B which is equivalent to B up to permutation of rows, we use a surrogate error metric to evaluate whether B -1B is close to a permutation matrix. Given a matrix Q with row norm 1, let

e(Q) = min{1 - min max |Qij|, 1 - min max |Qij|}.

ij

ji

(4.1)

 Then we have that if e(Q)   for some  < 1/3, then it implies that Q is 2-close to a permutation matrix in

infinity norm. On the other direction, we know that if e(Q) > , then Q is not -close to any permutation matrix in

infinity norm. The latter statement also holds when Q doesn't have row norm 1.

Figure 1 shows that without over-parameterization, using ReLU as an activation function, SGD doesn't converge to zero test error and the ground-truth parameters. We decreased step-size by a factor of 4 every 5000 number of iterations after the error plateaus at 10000 iterations. For the final 5000 iterations, the step-size is less than 10-9, so we can be

9

0.2 1

Objective Value Parameter Error

0.15 0.1
0.05 0
-0.05

0.8 0.6 0.4 0.2

-0.1 0

246 Iterations

8 104

0 02468

Iterations

104

Figure 3: Learning with objective function G(·). Left: the test loss. Right: the error in parameter space measured by equation (4.1)

confident that the non-zero objective value is not due to the variance of SGD. We see that none of the five runs of SGD converged to a global minimum.
Figure 2 shows that using ^2h2 + ^4h4 as the activation function, SGD with projection to the set of matrices B with row norm 1 converges to the ground-truth parameters. We also plot the loss function which converges the value of a global minimum. (We subtracted the constant term in equation (2.6) so that the global minimum has loss 0.)
Figure 3 shows that using our objective function G(B), the iterate converges to a permutation of the ground truth matrix B . The fact that the parameter error goes up and down is not surprising, because the algorithm first gets close to a saddle point and then breaks ties and converges to a one of the global minima.
Finally we note that using the loss function G(·) seems to require significantly larger batch (and sample complexity) to reduce the variance in the gradients estimation. We used batch size 262144 in the experiment for G(·). However, in contrast, for the ^2h2 + ^4h4 we used batch size 8192 and for relu we used batch size 256.
5 CONCLUSION
In this paper we first give an analytic formula for the population risk of the standard 2 loss, which empirically may converge to a spurious local minimum. We then design a novel population loss that is guaranteed to have no spurious local minimum.
Designing objective functions with well-behaved landscape is an intriguing and potentially fruitful direction. We hope that our techniques can be useful for characterizing and designing the optimization landscape for other settings. We conjecture that the objective f2 + f412 has no spurious local minimum when ,  are reasonable constants and the ground-truth parameters are in general position. We provided empirical evidence to support the conjecture.
Our results assume that the input distribution is Gaussian. Extending them to other input distributions is a very interesting open problem.
REFERENCES
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In International Conference on Machine Learning, pp. 584­592, 2014.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
12See equation (2.4) for the definition of fk and Theorem 2.2 for how to access f2 + f4 in the setting of one-hidden-layer neural nets.
10

Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. arXiv preprint arXiv:1602.04426, 2016.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873­3881, 2016.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp. 2253­2261, 2016.
R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Conference on Learning Theory, pp. 797­842, 2015.
Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in Neural Information Processing Systems (NIPS), 2016. URL http://arxiv.org/abs/1605.07272.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In 5th International Conference on Learning Representations (ICLR 2017), 2017.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. CoRR, abs/1609.05191, 2016. URL http://arxiv.org/abs/1609.05191.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630­645. Springer, 2016b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv preprint arXiv:1705.09886, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pp. 855­863, 2014.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Ryan O'Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In ICML, 2013.
Gilbert W Stewart. Matrix perturbation theory. 1990.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096, 2015.
11

Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Wikipedia. Hermite polynomials -- wikipedia, the free encyclopedia, 2017a. URL https://en.wikipedia.org/w/ index.php?title=Hermite_polynomials&oldid=796842411. [Online; accessed 1-September-2017 ].
Wikipedia. Formal power series -- wikipedia, the free encyclopedia, 2017b. URL https://en.wikipedia.org/w/ index.php?title=Formal_power_series&oldid=797671381. [Online; accessed 20-September-2017 ].
Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva. Electron-proton dynamics in deep learning. CoRR, abs/1702.00458, 2017a. URL http://arxiv.org/abs/1702.00458.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pp. 993­1001, 2016.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-connected neural networks. In Artificial Intelligence and Statistics, pp. 83­91, 2017b.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.

A ANALYTIC FORMULA FOR POPULATION RISKS

A.1 BASICS ON HERMITE POLYNOMIALS

In this section, we briefly review Hermite polynomials and Fourier analysis on Gaussian space. Let Hj be the prob-

abilists'

Hermite polynomial

Wikipedia

(2017a),

and let

hj

=

1 j!

Hj

be the

normalized

Hermite

polynomials.

The

normalized Hermite polynomial forms a complete orthonormal basis in the function space L2(R, e-x2/2) in the fol-

lowing sense13. For two functions f, g that map R to R, define the inner product f, g with respect to the Gaussian

measure as

f, g = E [f (x)g(x)] .
xN (0,1)

The polynomials h0, . . . , hm, . . . are orthogonal to each other under this inner product:

hi, hj = ij .

Here ij = 1 if i = j and otherwise ij = 0. Given a function   L2(R, e-x2/2) , let the k-th Hermite coefficient of  be defined as

^k = , hk .

Since h0, . . . , hm, . . . , forms a complete orthonormal basis, we have the expansion that

(x) = ^khk(x) .
kN

We will leverage several other nice properties of the Hermite polynomials in our proofs. The following claim connects the Hermite polynomial to the coefficients of Taylor expansion of a certain exponential function. It can also serve as a definition of Hermite polynomials.
Claim A.1 ((O'Donnell, 2014, Equation 11.8)). We have that for t, z  R,

exp(tz - 1 t2) = 2



1 k!

Hk

(z)tk

.

k=0

13We denote by L2(R, e-x2/2) the weighted L2 space, namely, L2(R, e-x2/2)

f:

 -

f (x)2e-x2/2dx

<



12

The following Claims shows that the expectation E [hn(x)hm(y)] can be computed easily when x, y are (correlated) Gaussian random variables.
Claim A.2 ((O'Donnell, 2014, Section 11.2)). Let (x, y) be -correlated standard normal variables (that is, both x,y have marginal distribution N (0, 1) and E[xy] = ). Then,
E [hm(x)hn(y)] = nmn .

As a direct corollary, we can compute ExN (0,Idd×d) (u x)(v x) by expanding in the Hermite basis and applying the Claim above.
Claim A.3. Let ,  be two functions from R to R such that 2, 2  L2(R, e-x2/2). Then, for any unit vectors u, v  Rd, we have that

E (u x)(v x) = ^i^i u, v i .

xN (0,Idd×d)

iN

Proof of Claim A.3. Let s = u x and t = v x. Then s, t are two spherical standard normal random variables that are u, v -correlated, and we have that

E (u x)(v x) = E [(s)(t)] .
xN (0,Idd×d)
We expand (s) and (t) in the Fourier basis and obtain that

E [(s)(t)] = E  ^ihi(s) ^jhj(t)
iN jN
= ^i^j E [hi(s)hj(t)]
i,j
= ^i^i u, v i
i

(by Claim A.2)

A.2 ANALYTIC FORMULA FOR POPULATION RISK f AND f

In this section we prove Theorem 2.1 and Theorem 2.2, which both follow from the following more general Theorem.
Theorem A.4. Let ,   L2(R, e-x2/2), and y^ = a (Bx) with parameter a  R and B  R ×d. Define the population risk f as

f(a, B) = E y - y^ 2 .

b1 

 b1 

Suppose

B

=

 

...

 

and

B

= 

...

 

and

bi's

and

bi

's

have

unit

2 norm. Then,

b bm

2

f (a, B) =

^k

ai bi k - ^k

aibi k + const,

kN

i[m]

i[ ]

F

where ^k, ^k are the k-th Hermite coefficients of the function  and  respectively.

We can see that Theorem 2.1 follows from choosing  =  and Theorem 2.2 follows from choosing  = ^2h2 + ^4h4. The key intuition here is that we can decompose  into a weighted combination of Hermite polynomials, and each Hermite polynomial influence the population risk more or less independently (because they are orthogonal polynomials with respect to the Gaussian measure).

13

Proof of Theorem A.4. We have

f = E

y^ - y 2 = E

a

2
(B x) - a (Bx)

 2

=

E

 

ai (bi x) -

ai(bi x)

 

i[m]

i[ ]

= E ai aj (bi x)(bj x) + E aiaj(bi x)(bj x)

i[m],j[ ]

i[m],j[ ]

- 2 E ai aj(bi x)(bj x)
i[m],j[ ]

=

ai aj ^k2 bi , bj k +

aiaj ^k2 bi, bj k

i,j[m]

kN

i,j[ ]

kN

- 2 ai aj ^k^k bi , bj k

i[m],j[ ]

kN

2

=

^k

ai bi k - ^k

aibi k .

kN

i[m]

i[ ]

F

(by Claim A.3)

A.3 ANALYTIC FORMULA FOR POPULATION RISK G

In this section we show that the population risk G(·) (defined as in equation (2.7)) has the following analytical formula:

 G(B) = 2 6|^4| ·

ai

bi , bj 2 bi , bk 2

i[d] j,k[d],j=k

-

|^4|µ 6

ai
i,j[d]

bi , bj

m
4+ (
i=1

bi

2 - 1)2 .

The formula will be crucial for the analysis of the landscape of G(·) in Section B. The formula follows straightforwardly from the following two theorems and the definition (2.7).
Theorem A.5. Let (·, ·, ·) be defined as in equation (2.9), we have that

 E y ·

 
(bj, bk, x) = 2 6^ ·

ai

bi , bj 2 bi , bk 2 .

j,k[d],j=k

i[d] j,k[d],j=k

Theorem A.6. Let (·, ·) be defined as in equation (2.8), then we have that



E y

·

(bj, x)
j[d]

=

^4 6

ai
i,j[d]

bi , bj

4.

In the rest of the section we prove Theorem A.5 and A.6.

We start with a simple but fundamental lemma. Essentially all the result in this section follows from expanding the two sides of equation (A.1) below.
Lemma A.7. Let u, v  Rd be two fixed vectors and x  N (0, Idd×d). Then, for any s, t  R,

exp( u, v st) = E

exp(u xt - 1 u 2t2) exp(v 2

xs - 1 v 2s2) 2

.

(A.1)

14

Proof. Using the fact that E

exp(v

x)

=

exp(

1 2

v

2), we have that,

E

exp(u

xt - 1 2

u 2t2) exp(v

xs - 1 2

v

2s2)

=E

exp((tu + sv)

x)

exp(- 1 2

u

2t2 - 1 2

v

2s2)

1 = exp(

tu + sv

2- 1

u

t2 - 1

v

2s2)

2 22

= exp( u, v st) .

(by the formula E

exp(v

x)

=

exp(

1 2

v

2))

Next we extend some of the results in the previous section to the setting with different scaling (such as when v in Claim A.3 is no longer a unit vector.)

Lemma A.8.

Let u be a fixed unit vector and v be an arbitrary vector in Rd.

Let (v, x) =

1 8

v

4

-

1 4

(v

x)2

v 2+

1 24

(v

x)4.

u, v 44,k = E Hk(u x)(v, x)

(A.2)

 As a sanity check, we can verify that when v is a unit vector, (v, x) = 24h4(v x) and th Lemma reduces to a special case of Claim A.2.

Proof. Let A, B be formal power series in variable s, t defined as A = exp( u, v st) and B =

E

exp(u

xt

-

1 2

u 2t2) exp(v

xs

-

1 2

v 2s2) . We refer the readers to Wikipedia (2017b) for more backgrounds

of power series. For casual readers, one can just think of A as B as two power series obtained by expanding the

exp(·) via Taylor expansion. For a formal power series A in variable x, let [x]A to denote coefficient in front of the

monomial x. By Lemma A.7, we have that A = B, and thus

[s4tk]A = [s4tk]B ,

(A.3)

which implies that

1 24

u, v 44,k = E

[tk ]

exp(u xt - 1 t2) 2

· [s4]

exp(v xs - 1 v 2s2) 2

1 = E k! Hk(u x)(v, x) .

(A.4)

where the last line is by the fact that (v, x) = [s4] exp(v

xs

-

1 2

v

2s2). This can be verified by applying Claim

A.1 with t = s v

and

z

=

vT v

x

,

and

noting

that

H4(x)

=

x4

- 6x2

+

3.

Now we are ready prove Theorem A.6 using Lemma A.8.

Proof of Theorem A.6. Using the fact that (v x) =

 k=0

^k

hk

(v

x), we have that



E y · (bj, x) =

ai E (bi x)(bj, x)

j i,j[d]



= ai E ^khk(bi x)(bj, x)

i,j[d]

k

= ai E ^4h4(bi
i,j[d]

x)(bj, x)

=

^4 6

i,j[d]

ai

bi , bj

4

(by

Lemma

A.8

and

hj

=

1 j

Hj

)

15

Towards proving Theorem A.5, we start with the following Lemma. Inspired by the proofs above, we design a function (v, w, x) such that we can estimate u, v 2 u, w 2 by taking expectation of E (u x)(v, w, x) .
Lemma A.9. Let a be a fixed unit vector in Rd and v, w two fixed vectors in Rd. Let (·, ·) be defined as in Lemma A.8. Define (v, w, x) as

Then, we have that

(v, w, x) = (v + w, x) + (v - w, x) - 2(v, x) - 2(w, x)

1 =

v

2

w 2+

v, w 2 - 1

w 2(v

x)2 - 1

v

2(w

x)2

2 22

- 2(v

x)(w

x)v

1 w + (v

x)2(w

x)2 .

2

 E (u x)(v, w, x) = 2 6^4 u, v 2 u, w 2 .

(A.5) (A.6)

Proof. Using the fact that u, v + w 2 + u, v - w 4 - 2 u, v 2 - 2 u, w 4 = 12 u, v 2 u, w 2 and Lemma A.8, we have that

12 u, v 2 u, w 24,k = E Hk(u x)((v + w, x) + (v - w, x) - 2(v, x) - 2(w, x))

= E Hk(u x)(v, w, x) .

(A.7)

Using the fact that (u x) =

 k=0

^k hk (u

x), we conclude that



E (u x)(v, w, x) = ^k E hk(u x)(v, w, x)

k=0

=

^4 6

E

H4(u

x)(v, w, x)

= 2 6^4 u, v 2 u, w 2

(by

Lemma

A.8

and

hj

=

1 j

Hj

)

(by Lemma A.8 again)

Now we are ready to prove Theorem A.5 by using Lemma A.9 for every summand.

Proof of Theorem A.5. We have that



E y · (bj, bk, x) = ai E (bi x)(bj, bk, x)

j,k i j,k



= 2 6^4 ai

bi , bj 2 bi , bk 2 .

i j,k

(by Lemma A.9)

B LANDSCAPE OF POPULATION RISK G(·)

In this section we prove Theorem 2.3. Since the landscape property is invariant with respect to rotations of parameters, without loss of generality we assume B is the identity matrix Id throughout this section. (See Section C for a precise statement for the invariance.) Recall that by Theorem 2.6, the population risk G(·) in the case of B = Id is equal to
 G(B) = 2 6|^4| ai (bj ei)2(bk ei)2
i j=k

-

|^4|µ 6

d i=1

ai

d
(bj
j=1

ei)4

+

d

j=1

bj 2 - 1 2 .

(B.1)

16

In the rest of section we work with the formula above for G(·) instead of the original definition. In fact, for future reference, we study a more general version of the function G. For nonnegative vectors ,  and nonnegative number µ, let G,,µ be defined as

d

dd

d

G,,µ(B) = i (bj ei)2(bk ei)2 - µ i (bj ei)4 +  ( bj 2 - 1)2

i=1 j=k

i=1 j=1

j=1

(B.2)

Here ei denotes the i-th natural basis vector. We see that G is sub-case of G,,µ and we prove the following extension of Theorem 2.3. Let max = maxi i and min = mini i.
Theorem B.1. Let  = max/min and c be a sufficiently small universal constant (e.g. c = 10-2 suffices). Suppose µ  cmin/max and   4 max(µmax, max). Then, the function G,,µ(B) defined as in equation (B.2) satisfies that

1. A matrix B is a local minimum of G,,µ if and only if B can be written as B = DP where P is a permutation

matrix and D is a diagonal matrix with Dii 

±

1 1-µi /

.

2. Any saddle point B has strictly negative curvature in the sense that min(2G,,µ(B))  -0 where 0 = c min{µmin/(d), µm2 in/max, }

3. Suppose B is an approximate local minimum in the sense that B satisfies

g(B)   and min(2g(B))  -0

Then B can be written as B = DP + E where P is a permutation matrix, D is a diagonal matrix with the

entries satisfying

1

1

-

µi 

1

-

18d2 m2 in

-

 2



Di2i



1

1

-

µi 

 1+
2

and E is an error matrix satisfying

|E|  3/min.

As a direct consequence, B is Od()-close to a global minimum in Euclidean distance, where Od(·) hides polynomial dependency on d and other parameters.

Here we recall that |E| denotes the largest entries in the matrix E. Theorem 2.3 follows straightforwardly from Theorem B.1 by setting  = 2 6|^4|a and  = |^4|a / 6. In the rest of the section we prove Theorem B.1.

b1 

Note

that

our

variable

B

is

a

matrix

of

dimension

d×d

and

we

use

bi

to

denote

the

rows

of

B,

that

is,

B

=

 

...

. 

bd Naturally, towards analyzing the properties of a local minimum B, the first step is that we pick a row bs of B and treat
only bs as variables and others rows as fixed. We will show that local optimality of bs will imply that bs is equal to one
of the basis vector ej up to some scaling factor. This step is done in Section B.1. Then in Section B.2 we show that
the local optimality of all the variables in B implies that each of the rows of B corresponds to different basis vector,

which implies that B is a permutation matrix (up to scaling of the rows).

B.1 STEP 1: ANALYSIS OF LOCAL OPTIMALITY OF A SINGLE ROW

Suppose we fix b1, · · · , bs-1, bs+1, · · · , bd, and optimize only over bs, we obtain the objective h of the following form:

dd

h,,(x) =

ix2i -

ixi4 +  x 2 - 1 2

i=1 i=1

(B.3)

We can see that setting i = ai k=s(bk ei)2, i = ai , and x = bs gives us the original objective G(B). In this subsection, we will work with h(·) and analyze the properties of the local minima of h(·).

The following lemma shows that a local minimum x of the objective h(·) must be a scaling of a basis vector. For a vector x, let |x|2nd denotes the second largest absolute values of the entries for x. We note that |·|2nd is not a norm. The

17

lemma deals generally an approximate local minimum, though we suggest casual readers simply think of ,  = 0 in the lemma.
Lemma B.2. Let h(·) be defined in equation (B.3) with non-negative vectors  and  in Rd. Suppose parameters ,   0 satisfy that    3/min. If some point x satisfies h(x)   and min(2h(x))  - , then we have

|x|2nd 

 .
min

Proof. Without loss of generality, we can take  =  3/min which means  = 2/3m1/i3n. The gradient and Hessian of function h(·) are

h(x) = 2 diag()x - 4 diag()x 3 + x 2h(x) = 2 diag() - 12 diag( x 2) + Id + 8xx .

(B.4)

where  4( x 2 - 1).

1/3

Let S = {i : |xi|  } be the indices of the coordinates that are significantly away from zero, where  =

 min

.

Since h(x)  , we have that |h(x)i|   for every i  [d], which implies that

i  [d], 2ixi + xi - 4ixi3  

(B.5)

which further implies that

i  S,

2i +  - 4ix2i

 

(B.6)

If |S| = 1, then we are done because |x|2nd  . Next we prove that |S|  2. For the sake of contradiction, we assume that |S|  2. Moreover, WLOG, we assume that |x|1  |x|2 are the two largest entries of |x| in absolute values.

We take v  Rd such that v1 = -x2/ x21 + x22, and v2 = x1/ x21 + x22, and vj = 0 for j  2. Then we have that v x = 0 and v = 1. We evaluate the quadratic form and have that

v 2h(x)v = v (2 diag() + Id)v - 12v diag( x 2)v

(since v x = 0)

= (21 + )v12 + (22 + )v22 - 121v12x21 - 122v22x22



-81v12x21

-

82v22x22

+

 

(by equation (B.6) and v = 1)



-8(1

+

2

)

x22x21 x12 + x22

+

 



-8minx22

+

 

.

1/3

Recall that  =

 min

. Then we conclude that

v 2h(x)v  -6m1/i3n2/3 = -6.

This contradicts with the assumption that min(2h(x))  -m1/i3n2/3 =  and that v = 1. Therefore we have |S| = 1 and

|x|2nd   =



1/3


min

 min

(using    3/min)

For future reference, we can also show that for a sufficiently strong regularization term (sufficiently large ), the norm of a local minimum x should be bounded from below and above by 1/2 and 2. This are rather coarse bounds that suffice for our purpose in this subsection. In Section B.2 we will show that all the rows of a local minimum B of G have norm close to 1.
18

Lemma B.3. In the setting of Lemma B.2, 1. Suppose in addition that   4 max(max,  ) and   0.1mind-3/2, then x 2  2.

2. Let i = arg maxi |xi|. In addition to the previous conditions in bullet 1, assume that   4i . Then,

x

2

1 .

2

We remark that we have to state the conditions for the upperbounds and lowerbounds separately since they will be used with these different conditions.

Proof. Let S = {i : |xi|  } be the indices of the coordinates that are significantly away from zero, where  =

1/3

 min

. We first show that x 2  2. We divide into two cases:



1

S

is empty.

Since   0.1mind-3/2, then 



2 .
d

We conclude that

x 2  2.

2 S is non-empty. For i  S, recall equation (B.6) which implies that

4(

x

2 - 1) 

 

+ 4ix2i



 

+ 4max

x

2

x 2   + max x 2 + 1 4 

Since





4max,

so





m1/i3n2/3



3 4

,

and

thus

from

the

display

above

we

have

that

x 2  2.

Next we show that

x

2



1 2

.

Again we divide into two cases:

1. S is empty. For the sake of contradiction, assume that

x

2



1 2

,

then





-2.

We show that there is

sufficient negative curvature. Recall that

2h(x) = 2 diag() - 12 diag( x 2) + I + 8xx 2 diag() - 12 diag( x 2) - 2I + 8xx

Choose index j so that j = min, then

ej 2h(x)ej

= 2min - 12j xj2 - 2 + 8x2j

 2min + 82 - 2l

 2min - (2 - 82)



2min

-

4 
3

 - 5   -3 6

(by

2



1 12

)

(by   4 max{min,  })

This contradicts with the fact that min(2h(x))  - . Thus when S is empty,

x

2

1 2

.

2. S is non-empty. Recall that i = arg maxi |xi|, and by definition i  S. Using Equation (B.6)

  -2i

- 

19

which implies that

x 2  1 - i -  .  4

Since 



4i

,

and 



m1/i3n2/3



 

,

we

conclude

that

x 2  1/2.

We have shown that a local minimum x of h should be a scaling of the basis vector ei . The following lemma strengthens the result by demonstrating that not all basis vector can be a local minimum -- the corresponding coefficient i has to be reasonably small for ei being a local minimum. The key intuition here is that if i is very large compared to other entries of , then if we move locally the mass of ei from entry i to some other index j, the objective function will be likely to decrease because jx2j is likely to be smaller than i x2i . (Indeed, we will show that such movement will cause a second-order decrease of the objective function in the proof.)
Lemma B.4. In the setting of Lemma B.2, let i = arg maxi |xi|. If h(x)  , and min(2h(x)) > - for 0    0.1min/d and    3/min, then
i  min + 2 + 2 + 4i .

Proof. For the ease of notation, assume WLOG that i = 1. Let  = ( /min)1/2. By the assumptions, we have that

  1 . By Lemma B.2, we have
6d

x

2



1 2

,

which

implies

that

x12 

x

2 - (d - 1) |x|22nd



1 2

- d |x|22nd

 1 - d2



1 .
3

(B.7)

Define v = -

xk x1

e1 + ek. Since x1 is the largest entry of x, we can verify that 1 

v

2

=

1

+

x2k x21

 2.

By the

assumption, we have that

v 2h(x)v  - v 2  -4 .

(B.8)

On the other hand, recall the form of Hessian (equation (B.4)), by straightforward algebraic manipulation, we have that

v 2h(x)v = v 2 diag() - 12 diag( x 2) + Id + 8xx . v

=

21(

xk x1

)2

+

2k

-

121x2k

-

12k xk2

+

(

xk x1

)2

+





(21

+

)( xk x1

)2

-

12(1

+

k )xk2

+

2k

+

(41x21

-

21

+

 |x1| )



(41x12

+

 |x1|

)(

xk x1

)2

-

12(1

+

k )x2k

+

2k

+

(41x12

-

21

+

 |x1| )

(by v x = 0) (by equation (B.6)) (by equation (B.6))

= -81xk2 - 12kx2k + 41x12 + 2k - 21 + 4

(by |xk|  |x1| and |x1|2  1/3)

 2k - 21 + 4 + 81 .

(by x  2 using Lemma B.2)

Combining equation (B.8) and the equation above gives 1  k + 2 + 2 + 41 .

Since k is arbitrary we complete the proof.

The previous lemma implies that it's very likely that the local minimum x can be written as x = xi ei and the index i is also likely to be the argmin of . The following technical lemma shows that when this indeed happens, then
we can strengthen Lemma B.2 in terms of the error bound's dependency on  and  . In Lemma B.2, we have that |x|2nd is bounded by a function of  . Here we strengthen the bound to be a function that only depends on . Thus

20

as long as  be small enough so that we can apply Lemma B.2 and Lemma B.4 to meet the condition of the lemma below, then we get an error bound that goes to zero as  goes to zero. This translates to the error bound in bullet 3 of Theorem B.1 where the bound on E only depends on . For casual readers we suggest to skip this Lemma since its precise functionality will only be clearer in the proof of Theorem B.1.
Lemma B.5. In the setting of Lemma B.2, in addition we assume that i = argmink|k| and that x can be written as x = xiei + x-i satisfying
 x-i   0.1 min{1/ d, min/(max)} .

Then, we can strengthen the bound to

x-i



3 .
min

Proof. WLOG, let i = 1. Let xj be the second largest entry of x in absolute value. Define v1 = 41x21 - 21 - , and

similarly vj = 4jxj2 - 2j - . Since

h(x)



, by

equation

(B.6),

we

have that |v1|



 |x1 |

and |v2|

=

 |xj

|

.

Subtracting 41x21 = 21 +  + v1 and 4jx2j = 2j +  + vj, we obtain,

41x12 = 4j x2j - 2(j - 1) + v1 - vj  4j xj2 + (v1 - vj )

(since j - 1  0)

Since

x

2

1 2

,

then

x12



1 2

- d2 

1 3

.

Since

|xj |



,

4j x2j  4max2

Combining the above two displays,

(v1 - vj )  41x21 - 4j 2

 41x12 - 4max2

42  3 1 - 3 min



2 3 min

Since

|v1|



 |x1 |

and

|v2|

=

 |x2

|

,

2 2 |xj |  3 min,

and

re-arranging

gives

|xj

|



3

 min

.

(using

x21



1 3

and





min/(6max))

(B.9)

(B.10)

B.2 LOCAL OPTIMALITY OF ALL THE VARIABLES

In this section we prove Theorem B.1. Results in Subsection B.1 have established that if B is a local minimum, then each row bs of B has to be a scaling of a basis vector. In this section we show that these basis vectors need to be distinct from each other. The following proposition summaries such a claim (with a weak error analysis).
Proposition B.6. In the setting of Theorem B.1, suppose B satisfies

g(B)   and min(2g(B))  -

for parameters ,  satisfying 0    c min{µmin/(d), } and   c min{min,  3/min}. Then, the matrix B can be written as
B = DP + E ,

where D is diagonal such that i, |Dii|  [1/4, 2], and P is a permutation matrix, and |E|   with  =

1/2

 µmin

.

21

As alluded before, in the proof we will first apply the results in Section B.1 to show that when B is a local minimum, each row bs has a unique large entry. Then we will show that the largest entries of each row sit on different columns. The key intuition behind the proof is that if two rows, say row s, t, have their large entries on the same column, then it means that there exists a column-- say column k -- that doesn't contain largest entry of any row. Then either row s or t will violate Lemma B.4. Or in other words, either row s or t can move their mass into the column k to decrease the function value. This contradicts the assumption that B is a local minimum.

Proof. As pointed in the paragraph below equation (B.3), when we restrict our attention to a particular row of B and fix the rest of the rows the function G,,µ reduces to the function h(·) in equation (B.3) so that we can apply lemmas in Section B.1.
Concretely, fix an index s  [d] and let x = bs. For all i  [d], let ¯i = i j=s(bj ei)2, and ¯i = µi. Then we have that

d
G,,µ(B) = ¯ixi2 -

¯ix4i +  x 2 - 1 2

i=1 i

(B.11)

We view the function above as h(x). Now we apply Lemma B.2 (by replacing ,  in Lemma B.2 by ¯, ¯). The assumption that min(2g,,µ(B))  - implies that min(2h(x))  - since 2h(x) is a submatrix of
2g(B). Moreover, h(x)  G(B)     3/(µmin)

Hence by Lemma B.2, we have that the second largest entry of |bs| satisfies

s, |bs|2nd  .

(B.12)

1/2

where 

 µmin

for the ease of notation. We can check that   1 by the assumption. Therefore, we have
4 d

essentially shown that each row of B has only one single large entry, since the second largest entry is at most .

Next we show that each row of B has largest entries on distinct columns. For each row j  [d], let ij = arg maxi |ei bj| be the index of the largest entry of bj. We will show that i1, . . . , id are distinct.

For the sake of contradiction, suppose they are not distinct, that is, there are two distinct rows s, t that have the same

largest entries on column l, that is, we assume that is = it = l. This implies that {i1, . . . , id} = [d] and let k  [d]

1/2

be the index such that k / {i1, . . . , id}. We note that by the assumption  =

 µmin

 1  1 . We first
4 d 4 d

bound from above ¯k

¯k = k

(bj

ek )2



k d 2



1 16 min.

j=s

(by





1 4 

d

)

Assume in addition without loss of generality that |bs el|  |bt el|. Let
zl (bj el)2
j=s

(B.13)

be the sum of squares of the entries on the column l without entry bj el, and that ¯l = lzl . We first prove that zl  1/3.

For the sake of contradiction, assume zl

<

1/3. Then we have that ¯l

=

lz



1 3

l

.

This

implies

that





4 max{¯l,  }, and since l is the index of the largest column of bs we can invoke Lemma B.3 and conclude that

bs 2  1/2. This further implies that

(bs el)2  bs 2 - d |bs|2nd  1/2 - d2  1/3

 (by   1/(4 d))

Since we have assumed that |bs el|  |bt el|. Then we obtain that

zl  |bt el|2  |bs el|2  1/3 ,

22

which contradicts the assumption. Therefore, we conclude that zl  1/3. Then we are ready to bound ¯l from below:

¯l

=

lzl



1 3 l

.

The display above and Equation (B.13) implies that

¯l

-

¯k



1 4 min.

(B.14)

Note that l is the largest entry in absolute value in the vector bs. We will apply Lemma B.4. We fix every row of B except bs and consider the objective as a function of bs only. Again let ¯i = i j=s(bj ei)2, and ¯i = µi and we have the equation (B.11). (Note that now ¯ depends on the choice of s which we fixed.) Lemma B.4 gives us that

¯l  ¯k + 2 + 2 + 4¯ .

Since





1 50

min,





1 50

min

and

¯l

=

µl



1 50

min

,

we

obtain

that

¯l



¯k

+

1 5 min

(B.15)

which contradicts equation (B.14). Thus we have established that i1, . . . , id are distinct.

Finally, let Q be the matrix that only contain the largest entries (in absolute value) of each columns of B. Since

i1, . . . , id are distinct, we have that Q contains exactly one entry per row and per column. Therefore Q can be

written as DP where P is a permutation matrix and D is a diagonal matrix.

Moreover, we have that

bs

2 



bs 2 - d |bs|22nd  1/4 and bs 2  2. Therefore, the largest entry of each row has absolute value between 1/4 and

2. Therefore |D|ii  [1/4, 2]. Let E = B - P D. Then we have that |E|  maxs |bs|2nd  ,which completes the

proof.

Applying Lemma B.5, we can further strengthen Proposition B.6 with better error bounds and better control of the largest entries of each column.
Proposition B.7 (Strengthen of Proposition B.6). In the setting of Proposition B.6. Suppose in addition that  satisfies   cµm2 in/max. Then, the matrix B can be written as
B = DP + E ,

where P is a permutation matrix, D is diagonal such that

1

i  [d],

1

-

µi 

1

-

18d2 m2 in

-

 2



|Dii|2



1

1

-

µi 

 1+
2

and

3

|E|



. min

1/2

Proof. By Proposition B.6, we know that |E|   =

 µmin

. Now we use Lemma B.5 to strength the error

bound.

As we have done in the proof of Proposition B.6, we again fix an arbitrary s  [d] and all the rows except bs and view G,,µ as a function of bs. For all i  [d], let ¯i = i j=s(bj ei)2, and ¯i = µi and view G,,µ as a function of the form h(x) with ,  replaced by ¯, ¯, namely,

h(x) = ¯kxk2 - ¯kx4k +  x 2 - 1 2 + const
kk

23

We will verify the condition of Lemma B.5. Let i be the index of the largest entry in absolute value of the vector bs. Since we have shown that the largest entry in each row sits on different columns, and the second largest entry is always less than , we have that,

¯i = i

(bj

ei)2



id2



1 16 min.

j=s

(by   1 )
4 d

For any k = i, we know that the column k contains some entry (k, jk) which is the largest entry of some row, and we also have that jk = s since the largest entry of row s is on column i. Therefore, we have that

¯k = k (bj ek)2  l(bjk ek)2  l( bk 2 - d2)
j=s
1  3 l

 (by   1/(4 d))

Therefore, ¯k  ¯i for any k = i and thus i = argmink|¯k|. By the fact that |E|  , we have that x-i    

0.1 min{1/ d,

min/(max)}.

Now we are ready to apply Lemma B.5 and obtain that |bs|2nd



.3
min

Applying

the

argument

for

every

row

s

gives

|E|



.3
min

Finally, we give the bound for the entires in D. Let v be a short hand for h(bs) which is equal to the s-th column of G(B). Since B is an -approximate stationary point, then we have that v   and by straightforward calculation
of the gradient, we have

d
vi = 2¯ixi - 4µixi3 + 4( x2j - 1)xi .
j=1

Since xi = 0, dividing by xi gives,

0

=

2¯i

-

4µix2i

+

d
4(
j=1

x2j

-

1)

-

vi xi

=

(4

-

4µi)xi2

+

4

j=i

x2j

+

2¯i

-

4

-

vi xi

Rearranging the equation above gives,



xi2

=

4

1 - 4µi

4

-

2¯i

-

4

j=i

x2j

-

vi 
xi



=

1

1

-

µi 

1 -

¯i 2

-

xj2
j=i

-

vi 4xi 

To upper bound x2i , we note that |vi| < , ¯i > 0, and j=i xj2 > 0, so

x2i 

1

1

-

µi 

 1+

 1 + 2µi + 

2 

(since   4µi)

For

the

lower

bound

of

x2i ,

we

note

that

|E|





=

3 min

implies

j=i xj2  d2. Moreover, we have proved that

each rows has largest entry at different columns. Also note that the largest entry of row bs is on column i. Therefore,

we have ¯i = i

j=s(bTj ei)2



maxd2.

Using

these

two

estimates

and



=

,3
min

we

have

xi2



1

1

-

µi 

(1

-

( max 2

+ 1)d2

-

 )
2

1 18d2 

=

1-

µi 

1-

m2 in

- 2

24

Finally we are ready to prove Theorem B.1 by applying Proposition B.6.

Proof of Theorem B.1. By setting  = 0,  = 0 in Proposition B.6, we have that any local minimum B satisfies that B = DP where P is a permutation matrix and D is a diagonal and the precise diagonal entries of D. It can be verified that all these points have the same function value, so that they are all global minimizers.

Towards proving the second bullet, we note that a saddle point B satisfies that G(B) = 0. We will prove that min(2G(B))  -0. For the sake of contradiction, suppose min(2G(B))  -0. Then setting  = 0 and

 = 0 in Propostion B.7, we have that B = DP and Dii =

±

1 1-µi /

, which by bullet 1 implies that B is a

local minimum. This contradicts the assumption that B is a saddle point.

The 3rd bullet is a just a rephrasing of Proposition B.7.

C HANDLING NON-ORTHOGONAL WEIGHTS

In this section, we first show that when the weight vectors {bi } s are not orthonormal, the local optimum of a slight variant of G(B) still allow us to recover B . The main observation is that the set of local minima are preserved (in a certain sense) by linear transformation of the variables. We design an objective function F (B) that is equivalent to G(B) up to a linear transformation. This allows us to use Theorem 2.3 as a black box to characterize all the local minima of F .
We use max(·), min(·) to denote the largest and smallest eigenvalues of a square matrix. Similarly, max(·) and min(·) are used to denote the largest and smallest singular values.

C.1 LOCAL MINIMUM AFTER A LINEAR TRANSFORMATION

Given a function f (y), we say function g(·) is a linear transformation of f (·) if there is a matrix W such that g(x) = f (W x). If W has full rank, the local minima of f are closely related to the local minima of g.

We recall some standard notation in calculus first. We use f (t) to denote the gradient of f evaluated at t. For

example,

f (W x)

is

a

shorthand

for

f (y) y

|y=W

x,

and

similarly

2f (W x)

is

2f (y) (y)2

|y=W

x

.

The following theorem then connects the gradients and Hessians of f (W x) and g(x). Essentially, it shows that the set of local minima and saddle points have a 1-1 mapping between f and g, and the corresponding norms/eigenvalues only differ multiplicatively by quantities related to the spectrum of W .
Theorem C.1. Let W  Rd×m(d  m) be a full rank matrix. Suppose g : Rm  R and f : Rd  R are twice-differentiable functions such that g(x) = f (W x) for any x  Rm. Then, for all x  Rm, the following three properties hold:

1. min(W ) f (W x)  g(x)  max(W ) f (W x) . 2. If min(2g(x)) < 0, then
max(W )2min(2f (W x))  min(2g(x))  min(W )2min(2f (W x)).

3. The point x satisfies the first and second order optimality condition for g iff y = W x also satisfy the first and second order optimality condition for f .

Proof. The proof follows from the relationship between the gradients of g and the gradients of f . By basic calculus,

we have

g(x) = f (W x) = W f (y)

= W f (W x)

x y y=W x

which immediately implies bullet 1. Similarly, we can compute the second order derivative:

2g(x) = W [2f (W x)]W.

25

To simplify notation, let A = 2f (W x). Let x = arg min x =1 x W AW x, and y = (W x)/ W x . Therefore
min(A)  y Ay  min(W AW )/ W x 2  min(W AW )/ W 2.
On the other hand, let y be the unit vector that minimizes y Ay, we know y is in column span of W because f is only defined on the row span, so there must exist a unit vector x such that W x = y where   min(W ). For this x we have min(W AW )  x W AW x = 2min(A)  m2 in(W )min(A). This finishes the proof for 2. Finally, notice that W is full rank, so g(x) = W f (W x) = 0 iff f (W x) = 0. Also, 2g(x) = W [2f (W x)]W 0 iff 2f (W x) 0.

C.2 OBJECTIVE FOR NON-ORTHOGONAL WEIGHTS
Now we will design a new objective function that can be linearly transformed to the orthonormal case. The main idea is to view the rows of B as the new basis that we work on (which is not necessarily orthogonal). Note that this is already the case for the first two terms of the objective function G(B), we change the objective function as follows: More concretely, we define

 F,µ,(B) = 2 6^ ·

i

bi , bj 2 bi , bk 2

i[d] j,k[d]

-

^4µ 6

i
i,j[d]

bi , bj

mm
4 +  (( i
j=1 i=1

bj , bi

2 - 1)2 - 1)2 .

Note that the only change in the objective is the regularizer for the norm of bj. It is now replaced by

((

m i=1

i

bj , bi

2

- 1)2 - 1)2,

which

tries

to

ensure

the "norm"

of bj

in

the

basis defined by

row

of B

to be

1. The objective function that we will optimize corresponds to choosing i = ai .

Similar as before, this function can be computed as expectations







Fa ,µ,(B) = E y ·

(bj, bk, x) - µ E y · (bj, x)

j,k[d],j=k

j[d]

m

+ E(x,y),(x ,y )[ y · 2(bi, x) · y · 2(bi, x )],

i=1

(C.1)

where (x , y ) is an independent sample, and 2(v, x) = (v x)2 - v 2.

Intuitively, if we can find a linear transformation that makes {bi }'s orthonormal, that will reduce the problem to the orthonormal case. This is in fact the whitening matrix:

Let M =

m i=1

ai

bi

(bi

)

be the weighted covariance matrix of bi 's. Suppose the SVD of M is U DU

and let

W = U D-1/2. We apply the transformation W to the vectors ai bi's and obtain that oi = W ai bi . We can

verify that oi's are orthogonal vectors because

oioi = W M W = Id
i[m]

(C.2)

For notational convenience, let's extend the definition of the G(B) in equation by using the putting the relevant information in the subscript

 G,,,o(B) = 6^ ·

ai

oi, ¯bj

i[d] j,k[d],j=k

2

oi, ¯bk

2

-

^4µ 6

ai
i,j[d]

oi, ¯bj

4.

26

m
+  ( ¯bi 2 - 1)2
i=1
(That is, the index o denotes the ground-truth solution with respect to which G is defined.)
The next Theorem shows that we can rotate the objective function F properly so that it matches the objective G with a ground-truth vector oi's. Theorem C.2. Let W be defined as above, and let 1/a be the vector whose i-th entry is 1/ai . Then, we have that
G1/a ,µ,,oi (B) = Fa ,µ,(BW )).
Note this can be interpreted as a linear transformation as in vector format BW is equal to B · (W  Idd×d).

¯b1 

Proof. The equality can be obtained by straightforward calculation.

We

note

that

since

B

=

 

...

, 

the

rows

of

¯bm

B · (W  Idd×d) are W ¯b1, . . . , W ¯bm.

Therefore, we have that

Fa ,µ,(B · (W  Idd×d))

 = 2 6^ ·

ai

bi , W ¯bj 2 bi , W ¯bk 2

(C.3)

i[d] j=k[d]

-

^4µ 6

ai
i,j[d]

bi , W ¯bj

mm
4 +  ( ai
j=1 i=1

W ¯bj, bi

2 - 1)2 .

1 = 2 6^ ·
i[d] ai j,k[d]

ai W bi , ¯bj 2

ai W bi , ¯bk 2

- ^4µ

1

6 i,j[d] ai

mm
ai W bi , ¯bj 4 +  ( ¯bj,
j=1 i=1

 = 2 6^ ·

1

i[d] ai

j,k[d]

oi, ¯bj

2

oi, ¯bk

2

ai W bi 2 - 1)2 .

- ^4µ

1

6 i,j[d] ai

mm
oi, ¯bj 4 +  ( ¯bj , oi 2 - 1)2 .
j=1 i=1

(by the definition of oi's)

From Theorem 2.3 we can immediately get the following Corollary (note that the only difference is that the coefficients

now are 1/ai instead of ai ). Recall amax = maxi ai and amin = mini amin, we have

Cµ orocl/larayaCn.d3.Let(caam=ina)m-a1x.

/amin. Let c The function

be a sufficiently small universal constant (e.g. c = 0.01 G1/a ,µ,,oi (·) defined as in Theorem C.2 satisfies that

suffices).

Assume

1. A matrix B is a local minimum of G if and only if B can be written as B = P DO where O is a matrix whose rows are oi's, P is a permutation matrix and D is a diagonal matrix with Dii  {±1 ± O(µ/amin)}.
2. Any saddle point B has a strictly negative curvature in the sense that min(2G(B))  -0 where 0 = c min{µ/(aamaxd), }

3. Suppose B is an approximate local minimum in the sense that B satisfies
g(B)   and min(2g(B))  -0
Then B can be written as B = P DO + E where P is a permutation matrix, D is a diagonal matrix and |E|  O(amax/^4).

27

Finally, we can combine the theorem above and Theorem B.1 to give a guarantee for optimizing F . Let  be a diagonal matrix with ii = ai . Let M = B 2B and (M ) = M /min(M ).
Theorem C.4. Let c be a sufficiently small universal constant (e.g. c = 0.01 suffices). Let a = amax/amin. Assume µ  c/a and   1/(c · amin). The function F (·) defined as in Theorem C.2 satisfies that

1. A matrix B is a local minimum of F if and only if B satisfy B- = P DB where P is a permutation matrix,  is a diagonal matrix with ii = ai , and D is a diagonal matrix with Dii  {±1 ± O(µ/amin)}. Furthermore, this means that all local minima of F are also global.

2. Any saddle point B has a strictly negative curvature in the sense that min(2F (B))  -0 where 0 = c min{µ/(adamax), }min(M ).

3. Suppose B is an approximate local minimum in the sense that B satisfies F (B)   and min(2F (B))  -0
Then B can be written as B- = P DB + E where , D, P are as in 1, the error term E O(amax md · (M )1/2/^4) (when amax md · (M )1/2/^4 < c).



Proof. Note that we can immediately apply Theorem 2.3 to G1/a ,µ,,oi (B) to characterize all its local minima. See Corollary C.3.

Next we will transform the properties for local minima of G (stated in Corollary C.3) to F using Theorem C.1. First we note that the transformation matrix W and M are closely related:

W W = M, min(W )2 = 1/ M , W 2 = 1/min(M ).

(C.4)

This is because according to the definition of W , the SVD of M is M = U DU and W = U D-1/2, so W W = U D-1U = M -1. The claims of the singular values follow immediately from the SVD of M and W .

As a result, all local minimum of F are of the form BW where B is a local minimum of G. For B = BW , the gradient and Hessian of F (B) and G(B) are also related by Theorem C.1.

Let us first prove 1. By Corollary C.3, we know every local minimum of G is of the form B = P DO. According to the definition of O in Theorem C.2, we know each row vector oi is equal to W (ai )1/2bi , therefore O = B W . As a result, all local minima of G are of the form B = P DB W . By Theorem C.1 and Theorem C.2, we know all local minima of F must be of the form B = BW = P DB W W = P DB M -1.
Now we try to compute B- . To do that observe that [B ]M -1[B ] = I. Therefore [B M -1]- = B , and for any local minimum B, we have

B- = (P DB M -1)- = P - D- (B M -1)- = P - D- B .

Note that P is still a permutation matrix, and D- is still a matrix whose diagonal entries are {±1 ± O(µ/amin)}, so this is exactly the form we stated in 1. More concretely, the rows of B- are permutations of ai bi . For bullet 2, it follows immediately from Property 2 in Theorem C.1. Note that by property 2,

min(2F (BW

))



min(2G(B)) W2

=

min(2G(B))min(M ).

Finally we will prove 3. Let B = BW - , so that G(B) = F (B). We will prove properties of B using the properties of B from Corollary C.3.
First we observe that by Theorem C.1,
min(2G(B))  W 2min(2F (B))  -c min{µ/(adamax, }.
Therefore the second order condition for Claim 3 in Corollary C.3 is satisfied. Now when F (B)  , we have G(B)   W = /min(M )1/2. By Corollary C.3, we know B can be expressed as P DO + E where D is

28

the diagonal matrix, P is a permutation matrix and |E |  O(amax/(^4min(M )1/2)). We will apply perturbation Theorem C.9 for matrix inversion. Since min(P DO)  1/2, we know when E  1/4,
 (P DO + E )-1 - (P DO)-1  8 2 E .  Here E is bounded by E F  md|E |  O(amax md/(^4min(M )1/2)), which is smaller than 1/4 when  is small enough.
The corresponding point in F is B = BW , and in 1 we have already proved (P DOW )- is of the form we want, therefore we can define E = B- - (P DOW )- = (B - P DO)- W -1, and
 E = W -1 (P DO + E )-1 - (P DO)-1 = O(amax md · (M )1/2/^4).
This finishes the proof.
C.3 HANDLE UNDERCOMPLETE CASE
The objective function F can handle the case when the weights bi 's are not orthogonal, but still requires the number of components m to be equal to the number of dimensions d. In this section we show how to use similar ideas for the case when the number of components is smaller than the dimension (m < d).
Note that all the terms in F (B) only depends on the inner-products bj, bi . Let S be the span of {bi }'s and PS be the projection matrix to this subspace, it is easy to see that F (B) satisfies
F (B) = F (BPS ).
That is, the previous objective function only depends on the projection of B in the space S. Using similar argument as Theorem C.4, it is not hard to show the only local optimum in S satisfies the same conditions, and allow us to recover B . However, without modifying the objective, the local optimum of F (B) can have arbitrary components in the orthogonal subspace S.
In order to prevent the components from S, we add an additional 2 regularizer: define F,µ,, as follows:

F,µ,, (B )

=

F,µ,(B)

+

 2

B

2 F

(C.5)

Intuitively, since the first term F,µ,(B) only cares about the projection BPS , minimizing

B

2 F

will remove the

components in the orthogonal subspace of S. We will choose  carefully to make sure that the additional term does

not change the local optima of F,µ,(B) by too much, while still ensuring a small projection on S.

In this case we will consider pseudo-inverse instead of inverse. In particular, for a m × d matrix B, define its pseudoinverse B to be the matrix such that BB = Idm×m and BB is the projection to the row span of B.

Let M =

m i=1

ai

bi

(bi

)

, (M ) =

M

/m(M ).

Theorem C.5. For any desired accuracy 0, we can choose parameters , , 0, µ, , such that for the objective function Fa ,µ,,(B), for any B such that

F (B)  , 2F (B)  -0/2,

we have [B] = B DP + E where  is a diagonal matrix with entries ai , D is a diagonal matrix with entries close to 1, P is a permutation matrix and E  0.

To choose the parameters, let c be a sufficiently small universal constant (e.g. c = 0.01 suffices). Assume µ  c/

and   1/(c · amin).

Let 0

= c min{µ/(damax), }min(M ).

Let 



min{

amax

c^4 0 ·m d1/2

(M

)

,

0

/2},

and

 = min{min(M )1/2, c/ M , c0min(M )}.

We first show that if the gradient is small, then the point cannot have a large component in S. Lemma C.6. If Fa ,µ,,(B)  , then PS B F  /.

29

Proof. Since Fa ,µ,(B) only depends BPS , we know Fa ,µ,(B)PS = 0. Therefore   Fa ,µ,,(B)PS F = (B)PS F =  PS B F , and we have BPS F  / as desired.

Next we show that if the gradient of Fa ,µ,,(B) is small, and  is also small, then the gradient of Fa ,µ,(B) can be bounded. Lemma C.7. In the setting of Theorem C.5, if Fa ,µ,,(B)    min(M )1/2, then we have
Fa ,µ,(B)   +  2m/min(M ).

Towards proving Lemma C.7, we first bound the norm of B by the following claim: Claim C.8. If Fa ,µ,,(B)  min(M )1/2, then each row bi must satisfy bi M bi  2.

Proof. We prove by contradiction. Assume towards contradiction that there is a column bi such that bi M bi  2. We consider the quantity,



Fa 

,µ,,
bi

(B

),

bi

.



Note that Fa ,µ,, has 4 terms: (1) 2 6^ · i[d] i



m j=1

((

m i=1

i

bj , bi

2

-

1)2

-

1)2,

(4)

 2

B

2 F

.

j,k[d]

bi , bj

2

bi , bk

2,

(2)

- ^4µ
6

i,j[d] i bi , bj 4, (3)

Among these 4 terms, the first, third and forth terms all contribute positively to this inner-product (because when

bi

is

moved

to

(1 - )bi

all

those

terms

clearly

decrease).

Term

2

- ^4µ
6

i,j[m] ai bi , bj 4 contribute negatively.

Therefore we can ignore terms 1 and 4:



Fa 

,µ,,
bi

(B

),

bi



 bi

[-

^4µ 6

i[m]

ai

bi , bj

4 + (

ai

i[m]

bi , bj

2 - 1)2], bi

.

Let bi M bi = C  2, we know

i[m] ai

bi , bj

4 1
amin

i[m](ai )2 bi , bj 4  C2/amin. Therefore,

 bi

[- ^4µ 6

i[m]

ai

bi , bj 4], bi

=

- 4^4µ 6

i[m]

ai

bi , bj

4

 - 4^4µ 6

·

C2 amin

On the other hand,



bi

[(
i[m]

ai

bi , bj 2 - 1)2, bi

= 4(bi M bi - 1)(bi M bi) = 4C(C - 1).

By the choice of , µ, we can see that the negative term is negligible, and we know

Fa 

,µ,,
bi

(B),

bi

 2C(C - 1)

Since bi M bi = C, we have bi  C/min(M ). Therefore the norm of the gradient is at least 2C(C -1)/ bi|  2 2min(M ), this contradicts with the assumption. The norm of the rows must all be bounded.

Proof of Lemma C.7. We have that bi M bi  2 implies bi 2  2/min(M ). The norm of the whole matrix is

bounded by B F 

m i=1

bi

2

2m/min(M ), so by triangle inequality we have

Fa ,µ,(B)  Fa ,µ,,(B) +  B F   +  2m/min(M ).

Finally we are ready to prove Theorem C.5.

30

Proof of Theorem C.5. We will separate B into two components BS = BPS and B = BPS . We will first show that BS is close to the desirable solution. To do that we will use Theorem C.4 14. By the choice of , , we know from Lemma C.7 that Fa ,µ,(BS )  2 2m/min(M ). Also, 2Fa ,µ,(BS )  2Fa ,µ,(B) -   -0. Therefore we know BS must be of the form
[BS ] = P DB + E1,
where E1 < 0/2. Also at the same time from the proof of Theorem C.4 we know BS = (P DO + E )W where P DO + E has singular values close to 1. Therefore min(BS )  min(W )/2 = 1/2 M .
By Lemma C.6 we know B F  /. We apply inverse matrix perturbation (Theorem C.9) again, using B = BS + B, therefore we know
B = BS + E2, where E2  O( B F /m2 in(BS ))  0/2.
Combining these two perturbations we know
[B] = P DB + E1 + E2 ,
and the error term E1 + E2 has spectral norm at most 0.

C.4 TOOLBOX: MATRIX PERTURBATION

In the proof we used the following theorem for the perturbation of matrices.

Theorem C.9 (Stewart and Sun Stewart (1990)). Consider the perturbation of a matrix A: if B = A + E,then we

have

 B - A  2 A B E .

As a corollary, if E  min(A)/2, then we have 
B - A  2 2min(A)-2 E .

D RECOVERING THE LINEAR LAYER
We will show in this section that if we have are given a -approximation of B , then it is easy to recover a . The key observation here is that the correlation between the bi , x and the output y is exactly proportional to ai . We also note that there could be multiple other ways to recover a , e.g., using linear regression with the (Bx) as input and the y as output. We chose this algorithm mostly because of the ease of analysis.
Algorithm 1 Recovering a Input: A matrix B with unit row norms that is row-wise -close to B in Euclidean distance. Return: Let ai = 2E[y x, bi ] where E means the empirical average. Set ai  |ai| and bi  bisgn(ai)

Lemma (Restatement of Lemma 2.5). Given a matrix B whose rows are -close to B in Euclidean distance up to permutation and sign flip with   1/(2 ). Then, we can give estimates a, B (using e.g., Algorithm 1) such that there exists a permutation P where a - P a   amax and B is row-wise -close to P B .

To see why this simple algorithm works for recovering a , we need the following simple claim.

Claim D.1. For any vector v we have

1m E[y x, v ] = 2 ai bi , v .
i=1

14If we restrict all the vectors to the subspace S, we can still apply Theorem C.4 as long as we replace all inverses with pseudoinverses.

31

The proof of this claim follows immediately from the property of Hermite polynomials. Now we are ready to prove the corollary.
Proof. Without loss of generality we assume B is close to a sign flip of B . The unknown permutation does not change the proof.
Since bi is  close to Bi , let u be the vector where uj = bj , bi - bi , we have
mm
ai = ai bi , bi = ai ( bi , bi + bi , bi - bi ) = ai + ai , u  ai ± amax.
i=1 i=1
Therefore ai is always positive, ai is in the desirable range and Bi - Bi  . Similarly, if -bi is  close to Bi , we have ai  -ai ± amax, and the conclusion still holds.

For the settings considered in Section C, the vectors bi are not necessarily orthogonal. In this case we use the following algorithm:

Algorithm 2 Recovering a for general case
Input: A matrix B with unit row norms, and B is -close to B in spectral norm up to permutation and sign flip.
Let ui = 2E[y x, bi ] where E means the empirical average. Let a = (BB )-1u. Return: Set ai  |ai| and bi  bisgn(ai)

Lemma D.2. Given a matrix B whose rows have unit norm, and B - SP B   for some permutation matrix P

and diagonal matrix S with ±1 entries on diagonals.If m2 in(B) , we can give estimates a, B (using e.g., Algorithm 2)



4 2 m

such that a - P a



2

2amax m m-2in (B )

·

and

B -PB

 .

Proof. We again use Claim D.1: in this case we know the vector u satisfies u = B(B ) a . As a result, for the vector

a , we have

a = (BB )-1(B(B ) )a = (B) (B ) a = (B B) a .

By assumption we know B = SP B + E where E  . By the perturbation ofmatrix inverse (Theorem C.9), we know if E    min(B)/2, then B = (B )P -1S-1 + E where E  2 2min(B)-2. Therefore

a = (P -1S-1 + E ) a = S- P - a + (E ) a = SP a + (E ) a .

(Here the last equality is because for both permutation matrix P and sign flip matrix S, P - = P and S- = S.) Therefore, coordinates of a are permutation and sign flips of a , up to an error term (E ) a .

When   m2 in(B) , we know (E ) a
4 2 m

  E amax m  amin/2, therefore the signs are all recovered


correctly. After fixing the sign, we have a - P a  (E ) a

2

2amax m-2in (B )

m,

and

B -PB

 .

E SAMPLE COMPLEXITY

In this section we will show that our algorithm only requires polynomially many samples to find the desired solution. Note that we did not try to optimize the polynomial dependency.
Theorem E.1 (Theorem 2.7 Restated). In the setting of Theorem 2.3, suppose we use N empirical samples to ap-
proximate G and obtain function G. There exists a fixed polynomial such that if N  poly(d, amax/amin, 1/), with high probability for any point B with min(2G(B))  -0/2 and G(B)  /2, then B can be written as B = DP + E where P is a permutation matrix, D is a diagonal matrix and |E|  O(/(^4amin)).

32

In order to bound the sample complexity, we will prove a uniform convergence result: we show that with polynomially many samples, the gradient and Hessian of G are point-wise close to the gradient and Hessian of G, therefore any approximate local minimum of G must also be an approximate local minimum of G.
However, there are two technical issues in showing the uniform convergence result. The first issue is that when the norm of B is very large, both the gradient and Hessian of G and G are very large and we cannot hope for good concentration. We deal with this issue by showing when B has a large norm, the empirical gradient G(B) must also have large norm, and therefore it can never be an approximate local minimum (we do this later in Lemma E.5). The second issue is that our objective function involves high-degree polynomials over Gaussian variables x, y, and is therefore not sub-Gaussian or sub-exponential. We use a standard truncation argument to show that the function does not change by too much if we restrict to the event that the Gaussian variables have bounded norm.
Lemma E.2. Suppose P (B) + R(B) = E(x,y)[f (x, y, B)] where f is a polynomial of degree at most 5 in x, y and at most 4 in B. Also assume that the sum of absolute values of coefficients is bounded by . For any   /2, let R = Cd log(amax/) for a large enough constant C, let F be the event that x 2  R, and let Gtrunc = E(x,y)[f (x, y, B)1F ]. For any B such that bi  2 for all rows, we have
G(B) - Gtrunc(B)  ,
and 2G(B) - 2Gtrunc(B)  ,
Proof. By standard 2 concentration bounds, for large enough C and any z > R, the probability that x 2  z is at most exp(-10z).
By simple calculation, it is easy to check that Bf (x, y, B)  4d1.5amax x 5, and 2Bf (x, y, B)  12d2amax x 5. We know G(B) - Gtrunc(B) = E[Bf (x, y, B)(1 - 1F ) . The expectation between x 2  [2iR, 2i+1R], for i = 0, 1, 2, ..., is always bounded by 4d1.5amax 2i+1R 5 exp(-2iR) < /2i+1. Therefore

G(B) - Gtrunc(B)  /2i+1  .
i=0
The bound for the Hessian follows from the same argument.

Finally, we combine this truncation with a result of Mei et al. (2016) that proves universal convergence of gradient and Hessian. For completeness here we state a version of their theorem with bounded gradient/Hessian:
Theorem E.3 (Theorem 1 in Mei et al. (2016)). Let f () be a function from Rp  R and f^ be its empirical version. If the norm of the gradient and Hessian of a function is always bounded by  , for variables in a ball of radius r in p dimensions, there exists a universal constant C0 such that for C = C0 max{log r /, 1}, the following hold:

(a) The sample gradient converges to the population gradient. Namely if N  Cp log p we have

Pr[ sup f () - f^  
 r

Cp log n ]  1 - . n

(b) The sample Hessian converges to the empirical Hessian. Namely if N  Cp log p we have

Pr[ sup f () - f^  
 r

Cp log n ]  1 - . n

As an immediate corollary of this theorem and Lemma E.2, we have
Corollary E.4. In the setting of Theorem 2.7, for every B whose rows have norm at most 2, we have with high probability,
G(B) - G(B)  /2,
and 2G(B) - 2G(B)  0/2.

33

Proof. On the other hand, for all such matrices B, by Lemma E.2 we know the gradient and Hessian of G is close to

the gradient and Hessian of Gtrunc.

G(B) - Gtrunc(B)  /4,

and 2G(B) - 2Gtrunc(B)  0/4.

Now, the gradient and Hessian for individual samples for estimating Gtrunc are bounded by some poly(d, 1/), there-
fore by Theorem E.3 we know the gradient and Hessian of G are close to those of Gtrunc. When N  poly(d, 1/) for a large enough polynomial, we have with high probability, for all B with all rows bi  2,

Gtrunc(B) - G(B)  /4, and
2Gtrunc(B) - 2G(B)  0/4. The corollary then follows from triangle inequality.

Finally we handle the case when B has a row with large norm. We will show that in this case G(B) must also be large, so B cannot be an approximate local minimum.
Lemma E.5. If bi is the row with largest norm and bi  2, then when N  poly(d, amax/amin) for some fixed polynomial, we have with high probability G(B), bi  c bi 4 for some universal constant c > 0.

Proof. The proof of this Lemma is very similar to Claim C.8. Note that by equation (2.7) there are three terms in G(B):(1) sign(^4)E^ y · j,k[d],j=k (bj, bk, x) , (2) -µ sign(^4)E^ y · j[d] (bj, x) , (3)  im=1( bi 2 -1)2. Here E^ is the empirical average over the samples.

Note that the first two terms are homogeneous degree 4 polynomials over B, and the third term does not depend on the

sample. By argument similar to Corollary E.4, we know for any B where bi has the largest row norm, with the number

of samples we choose the gradient of the first two terms is camin bi 3 close to the gradient of their expectations,

where c < 0.01 is a small constant.



By Theorem 2.6, we know the expectation of the first two terms are equal to A1(B) = 2 6|^4| ·

i[d] ai

j,k[d],j=k

bi , bj

2

bi , bk

2

and

A2(B)

=

- |^4|µ
6

i,j[d] ai bi , bj 4. Here the gradient of the first

term always have positive correlation with bi, so we can ignore it. For the second term, we know the gradient

 [A2(B)] = - |^4|µ bi 6

j

aj bj , bi 3bj .

Taking the inner-product with bi, and use the fact that bi form an orthonormal basis, we know

 bi [A2(B)], bi

 -camin

bi

4.

On the other hand, when bi  2, we have for the third term

 [(
bi

bi

2 - 1)2], bi

 ( bi

- 1)4   bi 4/16.

Since  is larger than amax, we know the negative contribution from A2 and the difference between the empirical version and G are both negligible. Therefore we have G(B), bi  c bi 4 as desired.

Now we are ready to prove Theorem 2.7:

Proof. By Lemma E.5, any point B with G(B)   must have bi  2 for all i. Now by Corollary E.4, we know the point B we have must satisfy
G(B)  ; 2G(B) -0Id.
By point 3 in Theorem 2.3, this implies the guarantee on B.

34

F SPURIOUS LOCAL MINIMUM FOR FUNCTION P

In this section we give an example where the function P does have spurious local minimum.

In this example, d = 4, and the true vectors are the standard basis vectors bi = ei. We will set a1 = 1, and

a2 = a3 = a4 = 2 +  (where  > 0 is an arbitrary positive constant).



The spurious local minimum that we consider is b1 = b2 = e1 = b1, b3 = e2 = b2, b4 =

2 2

e3

+

2 2

e4.

That

is,

1 0 0 0 

B

=

 

1 0



0 1

0 0

0 0

 

.



00

2 2

2 2

The objective P (B) = 1 and the only non-zero term is a1 b1, b1 2 b1, b2 2. In order to improve the objective locally, we need to change either b1 or b2, otherwise the term a1 b1, b1 2 b1, b2 2 is still 1, and all other terms (ai bi , bj 2 bi , bk 2) are non-negative.

Assume we have a local perturbation B , where b1 = 1 - 21e1 +1u1, b2 = 1 - 22e1 +2u2. Here u1, u2 are unit vectors that are orthogonal to e1. Also, since this is a local perturbation, we make sure 1, 2  , and b3(2)  1 - , [b4(3)]2, [b4(4)]2  0.5 - . We will show that when  is small enough, the objective function P (B )  1.

To see this, notice that the term a1 b1, b1 2 b1, b2 2 is now equal to (1 - 12)(1 - 22). On the other hand, for b1, we have

44

44

ai bi , b1 2 bi , bk 2 = 21(2 + )

bi , u1 2 bi , bk 2

i=2 k=3

i=2 k=3

44
= 12(2 + ) bi , u1 2( bi , bk 2)

i=2 k=3

4 44

 21(2 + )

bi , u1

2

· min{
i=2

bi , bk 2}

i=2 k=3

 21(2 + )(0.5 - ).

Similarly we have the same equation for b2. Note that all the terms we analyzed are disjoint, therefore
P (B )  (1 - 12)(1 - 22) + 12(2 + )(0.5 - ) + 22(2 + )(0.5 - ).
By removing higher order terms of , it is easy to see that P (B )  1 when  is small enough. Therefore B is a local minima of P .

35

