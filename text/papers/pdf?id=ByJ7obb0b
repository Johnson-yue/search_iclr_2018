Under review as a conference paper at ICLR 2018
UNDERSTANDING AND EXPLOITING THE LOW-RANK STRUCTURE OF DEEP NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Training methods for deep networks are primarily variants on stochastic gradient descent. Techniques that use (approximate) second-order information are rarely used because of the computational cost and noise associated with those approaches in deep learning contexts. However, in this paper, we show how feedforward deep networks exhibit a low-rank derivative structure. This low-rank structure makes it possible to use second-order information without needing approximations and without incurring a significantly greater computational cost than gradient descent. To demonstrate this capability, we implement Cubic Regularization (CR) on a feedforward deep network with stochastic gradient descent and two of its variants. There, we use CR to calculate learning rates on a per-iteration basis while training on the MNIST and CIFAR-10 datasets. CR proved particularly successful in escaping plateau regions of the objective function. We also found that this approach requires less problem-specific information (e.g. an optimal initial learning rate) than other first-order methods in order to perform well.
1 INTRODUCTION
1.1 GRADIENT-BASED OPTIMIZATION AND DEEP LEARNING
Gradient-based optimization methods use derivative information to determine intelligent search directions when minimizing a continuous objective function. The steepest descent method is the most basic of these optimization techniques, but it is known to converge very slowly in ill-conditioned systems. Even outside of these cases, it still only has a linear rate of convergence. Newton's method is a more sophisticated approach ­ one that uses second-order derivative information, which allows the optimizer to model the error surface more accurately and thus take more efficient update steps. When it converges, it does so quadratically, but Newton's method also has limitations of its own. Firstly, it does not scale well: it can be very expensive to calculate, store, and invert the objective function Hessian. Secondly, the method may fail if the Hessian is indefinite or singular.
A variety of methods have been developed to try and appropriate the strengths of each approach while avoiding their weaknesses. The conjugate gradient method, for example, uses only firstorder information but uses the history of past steps taken to produce a better convergence rate than steepest descent. Quasi-Newton methods, on the other hand, approximate the Hessian (or its inverse) using first-order information and may enforce positive-definiteness on its approximation. Other approaches like trust region methods use second-order information without requiring convexity. For further information about gradient-based optimization, see Nocedal & Wright (2006).
Deep learning (DL) provides a set of problems that can be tackled with gradient-based optimization methods, but it has a number of unique features and challenges. Firstly, DL problems can be extremely large, and storing the Hessian, or even a full matrix approximation thereto, is not feasible for such problems. Secondly, DL problems are often highly nonconvex. Thirdly, training deep networks via mini-batch sampling results in a stochastic optimization problem. Even if the necessary expectations can be calculated (in an unbiased way), the variance associated with the batch sample calculations produces noise, and this noise can make it more difficult to perform the optimization. Finally, deep networks consist of the composition of analytic functions whose forms are known. As such, we can calculate derivative information analytically via back-propagation (i.e. the chain rule).
1

Under review as a conference paper at ICLR 2018
1.2 TRAINING METHODS FOR DEEP LEARNING
These special characteristics of DL have motivated researchers to develop training methods specifically designed to overcome the challenges with training a deep neural network. One such approach is layer-wise pretraining (Bengio et al., 2007), where pretraining a neural network layer-by-layer encourages the weights to initialize close to a optimal minimum. Transfer learning (Yosinski et al., 2014) works by a similar mechanism, relying on knowledge gained through previous tasks to encourage nice training on a novel task. Outside of pretraining, a class of optimization algorithms have been specifically designed for training deep networks. The Adam, Adagrad, and Adamax set of algorithms provide examples of using history-dependent learning rate adjustment (Kingma & Ba, 2014). Similarly, Nesterov momentum provides a method for leveraging history dependence in stochastic gradient descent (Sutskever et al., 2013). One could possibly argue that these methods implicitly leverage second order information via their history dependence, but the stochastic nature of mini-batching prevents this from becoming explicit.
Some researchers have sought to use second-order information explicitly to improve the training process. Most of these methods have used an approximation to the Hessian. For example, the LBFGS method can estimate the Hessian (or its inverse) in a way that is feasible with respect to memory requirements; however, the noise associated with the sampling techniques can either overwhelm the estimation or require special modifications to the L-BFGS method to prevent it from diverging (Byrd et al., 2016). There have been two primary ways to deal with this: subsampling (Byrd et al., 2016; Moritz et al., 2016) and mini-batch reuse (Schraudolph et al., 2007; Mokhtari & Ribeiro, 2014). Subsampling involves updating the Hessian approximation every L iterations rather than every iteration, as would normally be done. Mini-batch reuse consists of using the same minibatch on subsequent iterations when calculating the difference in gradients between those two iterations. These approximate second-order methods typically have a computational cost that is higher than, though on the same order of, gradient descent, and that cost can be further reduced by using a smaller mini-batch for the Hessian approximation calculations than for the gradient calculation (Byrd et al., 2011). There is also the question of bias: it is possible to produce unbiased low-rank Hessian approximations (Martens et al., 2012), but if the Hessian is indefinite, then quasi-Newton methods will prefer biased estimates ­ ones that are positive definite. Other work has foregone these kinds of Hessian approximations in favor of using finite differences (Martens, 2010).
1.3 CONTRIBUTIONS
In this paper, we prove, by construction, that the first and second derivatives of feedforward deep learning networks exhibit a low-rank, outer product structure. This structure allows us to use and manipulate second-order derivative information, without requiring approximation, in a computationally feasible way. As an application of this low-rank structure, we implement Cubic Regularization (CR) to exploit Hessian information in calculating learning rates while training a feedforward deep network. Finally, we show that calculating learning rates in this fashion can improve existing training methods' ability to exit plateau regions during the training process.
2 THE LOW-RANK STRUCTURE OF DEEP NETWORK DERIVATIVES
Second-order derivatives are not widely used in DL, and where they are used, they are typically estimated. These derivatives can be calculated analytically, but this is not often done because of the scalability constraints described in Section 1.1. If we write out the first and second derivatives, though, we can see that they have a low-rank structure to them ­ an outer product structure, in fact. When a matrix has low rank (or less than full rank), it means that the information contained in that matrix (or the operations performed by that matrix) can be fully represented without needing to know every entry of that matrix. An outer product structure is a special case of this, where an mxn matrix A can be fully represented by two vectors A = uvT . We can then calculate, store, and use secondorder derivatives exactly in an efficient manner by only dealing with the components needed to represent the full Hessians rather than dealing with those Hessians themselves. Doing this involves some extra calculations, but the storage costs are comparable to those of gradient calculations.
In this section, we will illustrate the low-rank structure for a feedforward network, of arbitrary depth and layer widths, consisting of ReLUs in the hidden layers and a softmax at the output layer. A
2

Under review as a conference paper at ICLR 2018

feedforward network with arbitrary activation functions has somewhat more complicated derivative formulae, but those derivatives still exhibit a low-rank structure. That structure also does not depend on the form of the objective function or whether a softmax is used, and it is present for convolutional and recurrent layers as well. The complete derivations for these cases are given in Appendix B.
In our calculations, we make extensive use of index notation with the summation convention (Ivancevic & Ivancevic, 2007). In index notation, a scalar has no indices (v), a vector has one index (v as vi or vi), a matrix has two (V as V ij, Vji, or Vij), and so on. The summation convention holds that repeated indices in a given expression are summed over unless otherwise indicated. For example, aT b = i aibi = aibi. The pair of indices being summed over will often consist of a superscript and a subscript; this is a bookkeeping technique used in differential geometry, but in this context, the subscripting or superscripting of indices will not indicate covariance or contravariance. We have also adapted index notation slightly to suit the structure of deep networks better: indices placed in brackets (e.g. the k in v(k),j) are not summed over, even if repeated, unless explicitly indicated by a summation sign. A tensor convention that we will use, however, is the Kronecker delta: ij, ji , or ij. The Kronecker delta is the identity matrix represented in index notation: it is 1 for i = j and 0 otherwise. The summation convention can sometimes be employed to simplify expressions containing Kronecker deltas. For example, ijvi = vj and ijVjk = Vik.
Let us consider a generic feedforward network with ReLU activation functions in n hidden layers, a softmax at the output layer, and categorical cross-entropy as the objective function (defined in more detail in Appendix B. The first derivatives, on a per-sample basis, for this deep network are

f uji

=

f pi

v(n),j

f  wj(k),i

=

f pl

uml i(n,k),mv(k-1),j

(1) (2)

where f is the per-sample objective function, v(k),j is the vector output of layer k, uji is the matrix

of weights in the softmax, and pj = uji v(n),i (which is the vector quantity evaluated by the soft-

max). For the full derivation, and the definition of the matrix quantity i(n,k),m, see Appendix B. In

calculating these expressions, we have deliberately left

f pj

unevaluated.

This keeps the expression

relatively simple, and programs like TensorFlow (Abadi et al., 2015) can easily calculate this for

us. Leaving it in this form also preserves the generality of the expression ­ there is no low-rank

structure

contained

in

f pj

,

and

the

low-rank

structure

of

the

network

as

a

whole

is

therefore

shown

to be independent of the objective function and whether or not a softmax is used. In fact, as long

as Equation 13 holds, any sufficiently smooth function of pj may be used in place of a softmax

without disrupting the low-rank structure. The one quantity that needs to be stored here is i(n,k),j for k = 1, 2, . . . , n - 1; it will be needed in the second derivative calculations. Note, however, that

this is roughly the same size as the gradient itself.

We can now see the low-rank structure:

f  uji

is the outer product (or tensor product) of the vectors

f pi

and v(n),j ,

and

f  wj(k),i

is the outer

product of

f pl

uml

i(n,k),m

(which ends up

being a

rank-1

tensor) and v(k-1),j. The index notation makes the outer product structure clear. It is important to

note that this low-rank structure only exists for each sample ­ a weighted sum of low-rank matrices

is not necessarily (and generally, will not be) low rank. In other words, even if the gradient of f is

low rank, the gradient of the expectation, F = E [f ], will not be, because the gradient of F is the

weighted sum of the gradients of f . The second-order objective function derivatives are then

2f uij ust

=



2f pips

v(n),j

v(n),t

2f uij wt(k),s

=

f pi

s(n,k),j v(k-1),t

+



2f pipl

v(n),j

ulm

s(n,k),m

v(k-1),t

(3) (4)

3

Under review as a conference paper at ICLR 2018

2f  wj(k),i  wt(r),s

=



2f plpq

ulmi(n,k),mv(k-1),j

uaq

s(n,r),av(r-1),t



+

f pl

uml

×

 

s(n,r),mi(r-1,k),tv(k-1),j 0
i(n,k),ms(k-1,r),j v(r-1),t

r>k r=k
r<k

(5)

Calculating

all

of

these

second

derivatives

requires

the

repeated

use

of

2f p2

.

Evaluating

that

Hessian

is straightforward given knowledge of the activation functions and objective used in the network,

and storing it is also likely not an issue as long as the number of categories is small relative to the

number of weights. For example, consider a small network with 10 categories and 1000 weights. In

such

a

case,

2f p2

would

only

contain

100

entries

­

the

gradient

would

be

10

times

larger.

We

now

find

that

we

have

to

store

j(n,k),i

values

in

order

to

calculate

the

derivatives.

In

2f w2

,

we

also

end

up

needing

j(r,k),i

for

r

=

n.

In

a

network

with

n

hidden

layers,

we

would

then

have

n(n-1) 2

of

the

j(r,k),i matrices to store. For n = 10, this would be 45, for n = 20, this would be 190, and so on.

This aspect of the calculations does not seem to scale well, but in practice, it is relatively simple to

work around. It is still necessary to store j(n,k),i, k < n, but j(r,k),i, r < n, only actually shows

up in one place, and thus it is possible to calculate each j(r,k),i matrix, use it, and discard it without needing to store it for future calculations. The key thing to note about these second derivatives is

that they retain a low-rank structure ­ they are now tensor products (or the sums of tensor products)

of matrices and vectors. For example,

2f uji wt(k),s =

f pi

× s(n,k),j

×

v(k-1),t

+ ais × v(n),j × v(k-1),t

ais

=



2f pipl

ulms(n,k),m

(6) (7)

With these expressions, it would be relatively straightforward to extract the diagonal of the Hessian and store or manipulate it as a vector. The rank of the weighted sum of low rank components (as occurs with mini-batch sampling) is generally larger than the rank of the summed components, however. As such, manipulating the entire Hessian may not be as computationally feasible; this will depend on how large the mini-batch size is relative to the number of weights. The low rank properties that we highlight here for the Hessian exist on a per-sample basis, as they did for the gradient, and therefore, the computational savings provided by this approach will be most salient when calculating scalar or vector quantities on a sample-by-sample basis and then taking a weighted sum of the results. In principle, we could calculate third derivatives, but the formulae would likely become unwieldy, and they may require memory usage significantly greater than that involved in storing gradient information. Second derivatives should suffice for now, but of course if a use arose for third derivatives, calculating them would be a real option. Thus far, we have not included bias terms. Including bias terms as trainable weights would increase the overall size of the gradient (by adding additional variables), but it would not change the overall low-rank structure. Using the calculations provided in Appendix B, it would not be difficult to produce the appropriate derivations.

3 CUBIC REGULARIZATION IN DEEP LEARNING

Cubic Regularization (CR) is a trust region method that uses a cubic model of the objective function:

f

(x)



mj

(sj )

=

f

(xj )

+

sjT

f x

+

1 2

sjT

Hj sj

+

1 6 j

sj 3

(8)

at the j-th iteration, where Hj is the objective function Hessian and sj = x - xj. The cubic term makes it possible to use information in the Hessian without requiring convexity, and the weight j on

4

Under review as a conference paper at ICLR 2018

that cubic term can have its own update scheme (based on how well m (sj) approximates f (Kohler & Lucchi, 2017)). Solving for an optimal sj value then involves finding the root of a univariate nonlinear equation (Nesterov & Polyak, 2006). CR is not commonly used in deep learning; we have seen only one example of CR applied to machine learning (Kohler & Lucchi, 2017) and no examples with deep learning. This is likely the case because of two computationally expensive operations: calculating the Hessian and solving for sj. We can overcome the first by using the lowrank properties described above. The second is more challenging, but we can bypass it by using CR to calculate a step length (i.e. the learning rate) for a given search direction rather than calculating the search direction itself.

3.1 IMPLEMENTATION

Our approach in this paper is to use CR as a metamethod ­ a technique that sits on top of existing

training algorithms. The algorithm calculates a search direction, and then CR calculates a learning

rate for that search direction. For a general iterative optimization process, this would look like

xj+1 = xj + jgj, where gj is the search direction (which need not be normalized), j is the

learning rate, and the subscript refers to the iteration. With the search direction fixed, m would then

be

a

cubic

function

of



at

each

iteration.

Solving

m 

=

0

as

a

quadratic

equation

in



then

yields

-gT Hg ± =

(gT Hg)2 - 2 (gT f )  g 3  g3

(9)

If we assume that gT f < 0 (i.e. g is a descent direction), then  is guaranteed to be real. Continuing under that assumption, of the two possible  values, we choose the one guaranteed to
be positive. The sampling involved in mini-batch training means that there are a number of possible ways to get a final jgj result. One option would be to calculate E [jgj]. This would involve calculating an  value with respect to the search direction produced by each sample point and then averaging the product g over all of the sample points. Doing this should produce an unbiased estimate of jgj, but in practice, we found that this approach resulted in a great deal sampling noise and thus was not effective. The second approach would calculate E [j] × E [gj]. To do this, we would calculate an  value with respect to the search direction produced by each sample point, as in the first option, calculate an average  value, and multiply the overall search direction by that
average. This approach, too, suffered from excessive noise. In the interest of reducing noise and
increasing simplicity, we chose a third option: once the step direction had been determined, we considered that fixed, took the average of gT Hg and gT f over all of the sample points to produce m () and then solved for a single j value. This approach was the most effective of the three.

3.2 COMPUTATIONAL RESULTS
To test CR computationally, we created deep feedforward networks using ReLU activations in the hidden layers, softmax in the output layer, and categorical cross-entropy as the error function; we then trained them on the MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009) data sets. This paper shows results from networks with 12 hidden layers, each 128 nodes wide. For the purposes of this paper, we treat network training strictly as an optimization process, and thus we are not interested in network performance measures such as accuracy and validation error ­ the sole consideration is minimizing the error function presented to the network. As we consider that minimization progress, we will also focus on optimization iteration rather than wall clock time: the former indicates the behaviour of the algorithm itself, whereas the latter is strongly dependent upon implementation (which we do not want to address at this juncture). Overall computational cost per iteration matters, and we will discuss it, but it will not be our primary interest. Further implementation details are found in Appendix A.
Figure 1 shows an example of CR (applied on top of SGD). In this case, using CR provided little to no benefit. The average learning rate with CR was around 0.05 (a moving average with a period of 100 is shown in green on the learning rate plot both here and in the rest of the paper), which was close to our initial choice of learning rate. This suggests that 0.02 was a good choice of learning rate. Another reason the results were similar, though, is that the optimization process did not run into any

5

Under review as a conference paper at ICLR 2018

(a) Error (SGD in blue, SGD with CR in red)

(b) Learning rate (calculated rate in red, period-100 moving average in green)

Figure 1: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learning rate = 0.01,  = 100

(a) Error (SGD in blue, SGD with CR in red)

(b) Learning rate (calculated rate in red, period-100 moving average in green)

Figure 2: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learning rate = 0.02,  = 100

plateaus. We would expect CR to provide the greatest benefit when the optimization gets stuck on a plateau ­ having information about the objective function curvature would enable the algorithm to increase the learning rate while on the plateau and then return it to a more typical value once it leaves the plateau. To test this, we deliberately initialized our weights so that they lay on a plateau: the objective function is very flat near the origin, and we found that setting the network weights to random values uniformly sampled between 0.1 and -0.1 was sufficient.

(a) Error (SGD in blue, SGD with CR in red)

(b) Learning rate (calculated rate in red, period-100 moving average in green)

Figure 3: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD) on the CIFAR10 Dataset; initial learning rate = 0.01,  = 1000

6

Under review as a conference paper at ICLR 2018
Figure 2 shows the results of SGD with and without CR when stuck on a plateau. There, we see a hundred-fold increase in the learning rate while the optimization is on the plateau, but this rate drops rapidly as the optimization exits the plateau, and once it returns to a more normal descent, the learning rate also returns to an average of about 0.05 as before. The CR calculation enables the training process to recognize the flat space and take significantly larger steps as a result. Applying CR to SGD when training on CIFAR-10 (Figure 3) produced results similar to those seen on MNIST. We then considered if this behaviour would hold true on other training algorithms: we employed CR with Adagrad (Duchi et al., 2011) and Adadelta (Zeiler, 2012)on MNIST. The results were similar. CR did not provide a meaningful difference when the algorithms performed well, but when those algorithms were stuck on plateaus, CR increased the learning rate and caused the algorithms to exit the plateau more quickly than they otherwise would have (Figures 4 and 5). The relative magnitudes of those increases were smaller than for SGD, but Adagrad and Adadelta already incorporate some adaptive learning rate behaviour, and good choices for the initial learning rate varied significantly from algorithm to algorithm. We also used a larger value for  to account for the increased variability due to those algorithms' adaptive nature. The result with Adadelta showed some interesting learning rate changes: the learning rate calculated by CR dropped steadily as the algorithm exited the plateau, but it jumped again around iteration 1200 as it apparently found itself in a flat region of space.
(a) Error (Adagrad in blue, Adagrad with CR in red) (b) Learning rate (calculated rate in red, period-100 moving average in green)
Figure 4: Cubic Regularization (CR) applied to Adagrad; initial learning rate = 0.1,  = 1000
(a) Error (Adadelta in blue, Adadelta with CR in red) (b) Learning rate (calculated rate in red, period-100 moving average in green)
Figure 5: Cubic Regularization (CR) applied to Adadelta; initial learning rate = 1.0,  = 1000
4 DISCUSSION
We see this CR approach as an addition to, not a replacement for, existing training methods. It could potentially replace existing methods, but it does not have to in order to be used. Because of the low-rank structure of the Hessian, we can use CR to supplement existing optimizers that do not explicitly leverage second order information. The CR technique used here is most useful when the optimization is stuck on a plateau prior to convergence: CR makes it possible to determine whether
7

Under review as a conference paper at ICLR 2018
the optimization has converged (perhaps to a local minimum) or is simply bogged down in a flat region. It may eventually be possible to calculate a search direction as well as a step length, which would likely be a significant advancement, but this would be a completely separate algorithm.
We found that applying CR to Adagrad and Adadelta provided the same kinds of improvements that applying CR to SGD did. However, using CR with Adam (Kingma & Ba, 2014) did not provide gains as it did with the other methods. Adam generally demonstrates a greater degree of adaptivity than Adagrad or Adadelta; in our experiments, we found that Adam was better than Adagrad or Adadelta in escaping the plateau region. We suspect that trying to overlay an additional calculated learning rate on top of the variable-specific learning rate produced by Adam may create interference in both sets of learning rate calculations. Analyzing each algorithm's update scheme in conjunction with the CR calculations could provide insight into the nature and extent of this interference, and provide ways to further improve both algorithms. In future work, though, it would not be difficult to adapt the CR approach to calculate layer- or variable-specific learning rates, and doing that could address this problem. Calculating a variable-specific learning rate would essentially involve rescaling each variable's step by the corresponding diagonal entry in the Hessian; calculating a layer-specific learning rate would involve rescaling the step of each variable in that layer by some measure of the block diagonal component of the Hessian corresponding to those variables. The calculations for variable-specific learning rates with CR are given in Appendix B.
There are two aspects of the computational cost to consider in evaluating the use of CR. The first aspect is storage cost. In this regard, the second-order calculations are relatively inexpensive (comparable to storing gradient information). The second aspect is the number of operations, and the second-order calculations circumvent the storage issue by increasing the number of operations. The number of matrix multiplications involved in calculating the components of Equation 9, for example, scales quadratically with the number of layers (see the derivations in Appendix B). Although the number of matrix multiplications will not change with an increase in width, the cost of na¨ive matrix multiplication scales cubically with matrix size. That being said, these calculations are parallelizable and as such, the effect of the computation cost will be implementation-dependent.
A significant distinction between CR and methods like SGD has to do with the degree of knowledge about the problem required prior to optimization. SGD requires an initial learning rate and (usually) a learning rate decay scheme; an optimal value for the former can be very problem-dependent and may be different for other algorithms when applied to the same problem. For CR, it is necessary to specify , but optimization performance is relatively insensitive to this ­ order of magnitude estimates seem to be sufficient ­ and varying  has a stronger affect on the variability of the learning rate than it does on the magnitude (though it does affect both). If the space is very curved, the choice of  matters little because the step size determination is dominated by the curvature, and if the space if flat, it bounds the step length. It is also possible to employ an adaptive approach for updating  (Kohler & Lucchi, 2017), but we did not pursue that here. Essentially, using CR is roughly equivalent to using the optimal learning rate (for SGD).
5 CONCLUSIONS
In this paper, we showed that feedforward networks exhibit a low-rank derivative structure. We demonstrate that this structure provides a way to represent the Hessian efficiently; we can exploit this structure to obtain higher-order derivative information at relatively low computational cost and without massive storage requirements. We then used second-order derivative information to implement CR in calculating a learning rate when supplied with a search direction. The CR method has a higher per-iteration cost than SGD, for example, but it is also highly parallelizable. When SGD converged well, CR showed comparable optimization performance (on a per-iteration basis), but the adaptive learning rate that CR provided proved to be capable of driving the optimization away from plateaus that SGD would stagnate on. The results were similar with Adagrad and Adadelta, though not with Adam. CR also required less problem-specific knowledge (such as an optimal initial learning rate) to perform well. At this point, we see it as a valuable technique that can be incorporated into existing methods, but there is room for further work on exploiting the low-rank derivative structure to enable CR to calculate search directions as well as step sizes.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pp. 153­160, 2007.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic Hessian information in optimization methods for machine learning. SIAM Journal on Optimization, 21 (3):977­995, 2011.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008­1031, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Vladimir G Ivancevic and Tijana T Ivancevic. Applied differential geometry: a modern introduction. World Scientific, 2007.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex optimization. arXiv preprint arXiv:1705.05933, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 735­742, 2010.
James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the Hessian by back-propagating curvature. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 963­970. Omnipress, 2012.
Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Transactions on Signal Processing, 62(23):6089­6104, 2014.
Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-BFGS algorithm. In Artificial Intelligence and Statistics, pp. 249­258, 2016.
Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathematical Programming, 108(1):177­205, 2006.
Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 2006.
Nicol N Schraudolph, Jin Yu, and Simon Gu¨nter. A stochastic quasi-Newton method for online convex optimization. In Artificial Intelligence and Statistics, pp. 436­443, 2007.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013.
9

Under review as a conference paper at ICLR 2018

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320­3328, 2014.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.

A EXPERIMENTAL PROCEDURE
Starting at a point far from the origin resulted in extremely large derivative and curvature values (not to mention extremely large objective function values), and this could sometimes cause difficulties for the CR method. This was easy to solve by choosing an initialization point relatively near the origin; choosing an initialization relatively near the origin also provided a significantly better initial objective function value. We initialized the networks' weights to random values between an upper and lower bound: to induce plateau effects, we set, the bounds to ±0.1, otherwise, we set them to ±0.2.
All of the networks used a mini-batch size of 32 and were implemented in TensorFlow (Abadi et al., 2015). The initial learning rate varied with network size; we chose learning rates that were large and reasonable but perhaps not optimal, and for optimization algorithms with other parameters governing the optimization, we used the default TensorFlow values for those parameters. For the learning rate decay, we used an exponential decay with a decay rate of 0.95 per 100 iterations. The  value used is specified along with the initial learning rate for each network's results. This value was also not optimized but was instead set to a reasonable power of 10.

B LOW-RANK DERIVATIONS FOR DEEP NETWORKS

B.1 FEEDFORWARD NETWORK WITH RELU ACTIVATIONS Table 1 provides a nomenclature for our deep network definition.

Table 1: Nomenclature ­ Formulation

Quantity
n xi v(k),j wi(k),j A (·) uij pj y^l yl
f
F

Description Number of hidden layers Vector of inputs for a single sample Vector output of layer k
Matrix of weights for layer k Activation function
Matrix of output layer weights Vector of intermediate variables for the output layer
Vector of outputs for a single sample Vector of labels for a single sample Scalar objective function value for a single sample
Scalar objective function

Equations 10-16 define a generic feedforward network with ReLU activation functions in the hidden layers, n hidden layers, a softmax at the output layer, and categorical cross-entropy as the objective function.
10

Under review as a conference paper at ICLR 2018

v(k),j = A wi(k),j v(k-1),i , k = 1, . . . , n

A (z) = max (z, 0)

v(0),i = xi

pj = uji v(n),i

y^j =

exp pj exp (pl)

l

f = -yl ln y^l

F = E [f ]

The relevant first derivatives for this deep network are

(10) (11) (12) (13)
(14)
(15) (16)

A (z) =

1 z>0 0 z<0

0



 v (k),j

 

=

sj A wi(k),j v(k-1),i v(k-1),t

 wt(l),s

  

A

w v w(k),j (k-1),i
i

(k),j v(k-1),q q wt(l),s

l>k l=k
l<k

(17) (18)

where there is no summation over j in Equation 18. We now define several intermediate quantities to simplify the derivation process:

s(k),j  sj A wi(k),j v(k-1),i

i(k),j  A wl(k),j v(k-1),l wi(k),j = s(k),j wi(k),s

j(kl ,l),jk


   
 

k (i),ji
ji-1 i=l+1
ji

0

k>l
k=l k<l

j(kl ,l),jk j(lm,m),jl = j(km,m),jk i(k,l),j  s(k,l),j i(k),s

(19) (20)
(21)
(22) (23)

where there is no summation over j in Equations 19 and 20. We can now complete our calculations of the first derivatives.

 v (k),j wt(l),s =

0 s(k,l),j v(l-1),t

pj ulk

= lj v(n),k

pj  v (n),i

= uij

l>k lk

f uji

=

f pk

pk uij

=

f pi

v(n),j

f  wj(k),i

=

f pl v(n),m pl v(n),m wj(k),i

=

f pl

uml i(n,k),m

v(k-1),j

(24) (25) (26) (27) (28)

11

Under review as a conference paper at ICLR 2018

We then start our second derivative calculations by considering some intermediate quantities:

A (z) = 0

 q(n,k),m  wt(r),s

= a(n,r),mA

wp(r),av(r-1),p sabtq(r-1,k),b = s(n,r),mq(r-1,k),t s(k),j = 0  (·)

 i(n,k),m  wt(r),s

=

 q(n,k),m  wt(r),s

i(k),q





 2 v (n),m

 

 wj(k),i  wt(r),s

=
 



 v(qn,k),m (k),q (k-1),j
wt(r),s i
0 (n,k),m v(k-1),j
i wt(l),s

r>k r=k r<k

 

s(n,r),mi(r-1,k),tv(k-1),j

r>k

= 0 r=k

 i(n,k),ms(k-1,r),j v(r-1),t r < k

(29) (30) (31) (32)
(33)

The second derivative of the ReLU vanishes, which simplifies the second derivative calculations significantly. Technically, the second derivative is undefined at the origin, but the singularity is removable, and thus we can define the second derivative to be 0 at the origin. We can then calculate the second-order objective function derivatives:

2f uij ust

=

2f pk pkpl uij

pl uts

=



2f pips

v

(n),j

v(n),t

(34)

2f uji wt(k),s

=

f v(n),j pi wt(k),s

+

2f pipl

v(n),j

pl  v (n),m

 v (n),m  wt(k),s

=

f pi

s(n,k),j

v(k-1),t

+

2f pipl

v(n),j

ulms(n,k),mv(k-1),t

(35)

2f

2f pl v(n),m pq v(n),a f pl

 2 v (n),m

wj(k),iwt(r),s = plpq v(n),m wj(k),i v(n),a wt(r),s + pl v(n),m wj(k),iwt(r),s

=



2f plpq

uml i(n,k),mv(k-1),j

uqa

s(n,r),a

v(r-1),t



+

f pl

uml

×

 

s(n,r),mi(r-1,k),tv(k-1),j 0
i(n,k),ms(k-1,r),j v(r-1),t

r>k r=k
r<k

(36)

To use CR, we calculate  as

-gT Hg + =

(gT Hg)2 - 2 (gT f )  g 3  g3

(37)

For a given iteration for the deep network described above (dropping the subscript j's so as not to interfere with the index notation), the quantities in this equation are

12

Under review as a conference paper at ICLR 2018

gT f

=

F pi

v(n),j

ji

+

 

F pl

uml i(n,k),mv(k-1),j

j(k),i

k

gT

Hg

=

ji

2F pips

v(n),j v(n),tts

+2 ji
k

F pi

s(n,k),j

v(k-1),t

+

2F pipl

v(n),j

uml s(n,k),mv(k-1),t

t(k),s

+

j(k),i

2F plpq

ulm

i(n,k),m

v(k-1),j

uaq

s(n,k),a

v(k-1),tt(k),s

k

+2

n

k-1

(jk),i

F pl

ulmi(n,k),m

s(k-1,r),j

v(r-1),t(tr),s

k=2 r=1

g 3=

ji ji +

j(k),ij(k),i

k

3 2

(38)
(39) (40)

 With these formulae in hand, and using the generalized binomial theorem ( 1 +  1 + /2 for
1), we can consider what happens to  in limiting cases:

 |gT f|

   


gT Hg
2gT f g3





 

2

gT Hg g3

+

gT f gT Hg

gT Hg gT Hg gT Hg

gT f gT f gT f

 g3  g3  g3

and gT Hg > 0 and gT Hg > 0 and gT Hg < 0

The weight update scheme for a single learning rate at each iteration is

(41)

uj+1 = uj + j j wj(k+)1 = wj(k) + j (jk)

(42) (43)

We could instead consider a weight-specific learning rate. If we assume that the baseline g vector
for each variable is a unit step in the direction of that variable, and we ignore superscripts indicating the iteration number, the calculations for variable-specific learning rates j(u),i for uij and j(w,k),i are as shown in Table 2:

Table 2: Variable-Specific Learning Rate Calculations

Learning Rate
j(u),i j(w,k),i

gtf

F pi

v

(n),j

F pl

uml

i(n,k),m

v(k-1),j

gT Hg



2F pi  ps

v(n),j

v(n),t

2F  pl  pq

ulmi(n,k),mv(k-1),j

uqas(n,k),av(k-1),t

g3 1 1

B.2 CONVOLUTIONAL AND RECURRENT LAYERS
Convolutional and recurrent layers preserve the low-rank derivative structure of the fully connected feedforward layers considered above, and we will show this in the following sections. Because we are only considering a single layer of each, we calculate the derivatives of the layer outputs with respect to the layer inputs ­ in a larger network, those derivatives will be necessary for calculating total derivatives via back-propagation.
13

Under review as a conference paper at ICLR 2018
B.2.1 CONVOLUTIONAL LAYER We can define a convolutional layer as

vts = A (x~ts)kl wkl (x~st )lk = x ts++kl--11

(44) (45)

where xji is the layer input,  is the vertical stride,  is the horizontal stride, A is the activation function, and vts is the layer output. A convolutional structure can make the expressions somewhat complicated when expressed in index notation, but we can simplify matters by using the simplifica-
tion ztskl = xts++kl--11. The layer definition is then

vts = A ztskl wkl

(46)

The derivatives of the convolutional layer are

vts xji

=A

ztskl wkl

wpm

 ztsqm xij

 ztsqm xji

=

1 i = s + m - 1 and j =  t + p - 1 0 else

vts wqp

=

A

ztskl wkl ztsqp

2vts xqpxji

=A

2vts wqpxji

=A

ztskl wkl

ztskl wkl

wrm

 ztsrm xji

wba

 ztsba xqp

 ztsqp xij

+A

ztskl wkl

ztsqpwrm

 ztsrm xji

2vts wqpwba

=A

ztskl wkl ztsqpztsba

(47) (48) (49) (50) (51) (52)

with no summation over s and t in any of the expressions above. Using the simplification with ztskl makes it significantly easier to see the low rank structure in these derivatives, but that structure is still noticeable without the simplification.
14

Under review as a conference paper at ICLR 2018

vts wqp

=

A

(x~ts)lk wkl (x~st )qp = A

(x~st )kl wkl xts++qp--11

(53)

vts xij

=

A (x~ts)lk wkl wji++11--st i + 1 > s, j + 1 >  t 0 else

(54)


A      
=
      

(x~ts)lk wkl

2vts wqpwba

=A

(x~st )kl wkl xts++qp--11xts++ba--11
2vts wqpxij

(55)

+ A (x~ts)kl wkl wji++11--st xts++qp--11 i + 1 - s = p, j + 1 -  t = q

A (x~ts)lk wkl wji++11--st x ts++qp--11

i + 1 - s > 0, i + 1 - s = p, j + 1 -  t > 0, j + 1 -  t = q

0 else (56)



2vts xqpxij

 
=
 

A

(x~st )kl wkl wji++11--st wqp++11--ts 0

p + 1 > s, i + 1 > s q + 1 >  t, j + 1 >  t
else

(57)

The conditional form of the expressions is more complicated, but it is also possible to see how the derivatives relate to wji and submatrices of xij.
B.2.2 RECURSIVE LAYER
We can define our recursive layer as

v(jt) = A wij v(it-1) v(j0) = xj

(58) (59)

where t indicates the number of times that the recursion has been looped through. If we inspect this
carefully, we can actually see that this is almost identical to the hidden layers of the feedforward
network: they are identical if we stipulate that the weights of the feedforward network are identical at each layer (i.e. wj(k),i = wji k) and if we treat the recursive loops like layers. This observation allows us to reuse some of our previous derivations. Primarily, we will use the fact that

 (·) wji =

k

 (·) wj(k),i wp(k),m wji

=

k

 (·)  wj(k),i

The first-order derivatives are then

(60)

 v(mt) wrs

=

k

s(t,k),mv(rt-1)

 v(mt) xi

= s(t,1),mwis

If A is a ReLU, then the second derivatives are relatively simple

(61) (62)

15

Under review as a conference paper at ICLR 2018

 v(mt) wji wrq

=

k,p

 v(mt)  wj(k),i  wr(p),q

t k-1

=2

i(t,k),mq(k-1,p),j v(rp-1)

k=2 p=1

 v(mt) xixl

=

0

 v(mt)  wrq  xi

=

s(t,1),j is

+

k

 q(t,1),m  wt(k),s

wis

= s(t,1),j is +

q(t,k),m s(k-1,1),r wis

k

(63) (64)
(65)

If A is not a ReLU, then we would use the results in the next section to calculate the second derivatives. Regardless of the exact form of A, though, we retain the low-rank structure as long as A is an
entry-wise function of its arguments.

B.3 DEEP NETWORK WITH GENERAL ACTIVATION FUNCTIONS
For a deep network with general entry-wise functions, the first derivatives are all identical to the derivations given in Section 2 save that A will be different. Before calculating second-order derivatives, though, we do some preliminary calculations that were not necessary before because A = 0 for ReLUs. First, we calculate the derivatives of r(l),m:

0

 r(l),m

  
=

rmA wj(l),mv(l-1),j smv(l-1),t

 wt(q),s

  

rmA

wj(l),mv(l-1),j wa(l),ms(l-1,q),av(q-1),t

l<q l=q
l>q

(rls),m  rmsmA wj(l),mv(l-1),j

 r(l),m  wt(q),s


 =


0
(rls),mv(l-1),t r(lp),mwa(l),ps(l-1,q),av(q-1),t

l<q l=q l>q

(66) (67) (68)

where there is no summation over m in any of these equations. Next, we calculate the derivatives of m(k,l),j :
16

Under review as a conference paper at ICLR 2018

m(k,l),j = b(k,r),j a(r,r-1),bm(r-1,l),a a(r,r-1),b = a(r),b = s(r),bwa(r),s

 a(r),b  wt(q),s


 =


0 (prs),bwa(r),pv(r-1),t + s(r),bat (drp),bwa(r),dwm(r),ps(r-1,q),mv(q-1),t

r<q r=q r>q

 m(k,l),j  wt(q),s

=

k r=l+1

b(k,r),j

 a(r),b  wt(q),s

m(r-1,l),a


           
=
           

0
k b(k,r),j (drp),bwa(r),dwc(r),ps(r-1,q),cv(q-1),tm(r-1,l),a
r=q+1
+b(k,q),j p(qs),bwa(q),pv(q-1),t + s(q),bat m(q-1,l),a
k b(k,r),j d(rp),bwa(r),dwc(r),ps(r-1,q),cv(q-1),tm(r-1,l),a
r=l+1

k<q l<qk
ql

Thirdly, we calculate the derivatives of l(n,k),j:

(69) (70) (71)
(72)

 l(n,k),j  wt(q),s

=

 m(n,k),j  wt(q),s

l(k),m

+

m(n,k),j

 l(k),m  wt(q),s

 l(n,k),j

 wt(q),s


                 
=
                 

n b(n,r),j d(rp),bwa(r),dwc(r),ps(r-1,q),cv(q-1),tm(r-1,k),al(k),m
r=q+1
+b(n,q),j p(qs),bwa(q),pv(q-1),t + s(q),bat m(q-1,k),al(k),m
n b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tm(r-1,k),al(k),m
r=k+1
+m(n,k),j l(sk),mv(k-1),t
n b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tm(r-1,k),al(k),m
r=k+1
+m(n,k),j (lpk),mwa(k),ps(k-1,q),av(q-1),t


               
=
               

n b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,q),cv(q-1),tl(r-1,k),a
r=q+1
+b(n,q),j p(qs),bwa(r),pv(q-1),tl(q-1,k),a + s(n,q),j l(q-1,k),t

n b(n,r),j d(rp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tl(r-1,k),a

r=k+1

n

+m(n,k),j l(sk),mv(k-1),t b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tl(r-1,k),a

r=k+1

+m(n,k),j (lpk),mwa(k),ps(k-1,q),av(q-1),t

q>k q=k q<k q>k q=k q<k

17

(73) (74) (75)

Under review as a conference paper at ICLR 2018



 2 v (n),j

 

 wi(k),l  wt(q),s

=
 

vl(n,k),j (k-1),i
 wt(q),s

v + l(n,k),j (k-1),i
 wt(q),s

(n,k),j v(k-1),i l wt(q),s

qk q<k

(76)


                 
=
                 

n b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,q),cv(q-1),tl(r-1,k),av(k-1),i
r=q+1
+b(n,q),j (pqs),bwa(q),pv(q-1),tl(q-1,k),av(k-1),i + s(n,q),j l(q-1,k),tv(k-1),i
n b(n,r),j (drp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tl(r-1,k),av(k-1),i
r=k+1
+m(n,k),j l(sk),mv(k-1),tv(k-1),i
n b(n,r),j d(rp),bwa(r),dwc(r),ps(r-1,k),cv(k-1),tl(r-1,k),av(k-1),i
r=k+1
+m(n,k),j l(pk),mwa(k),ps(k-1,q),av(q-1),tv(k-1),i + l(n,k),j s(k-1,q),iv(q-1),t

q>k
q=k
q<k (77)

The second-order derivatives of the objective function are then

2F uij ust

=

2F pk pkpl uij

pl uts

=

2F pips

v

(n),j

v(n),t

(78)

2F uji wt(k),s

=

F v(n),j pi wt(k),s

+

2F pipl

v(n),j

pl  v (n),m

 v (n),m  wt(k),s

=

F pi

s(n,k),j

v(k-1),t

+

2F pipl

v(n),j

ulms(n,k),mv(k-1),t

(79)

2F

2F pa v(n),j pb v(n),c F pm

 2 v (n),j

wi(k),lwt(q),s = papb v(n),j wi(k),l v(n),c wt(q),s + pm v(n),j wi(k),lwt(q),s

=

2F papb

uja

l(n,k),j

v(k-1),i

ucb

s(n,q),cv(q-1),t

+

F pa

uaj



 2 v (n),j wi(k),l  wt(q),s

(80)

18

