Under review as a conference paper at ICLR 2018

DISTRIBUTED RESTARTING NEWTONCG METHOD FOR LARGE-SCALE EMPIRICAL RISK MINIMIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
In this paper, we propose a distributed damped Newton method in which sample size is gradually increasing to quickly obtain a solution whose empirical loss is under satisfactory statistical accuracy. Our proposed method is multistage in which the solution of one stage serves as a warm start for the next stage which contains more samples (including the samples in the previous stage). This overall multistage algorithm reduce the number of passes over data. Moreover, our algorithm in nature is easy to be distributed and shares the strong scaling property indicating that acceleration is always expected by using more computing nodes. Various iteration complexity results regarding descent direction computation and stopping criteria are analyzed under convex setting. Our results of experiments illustrate that the proposed algorithm can outperform other comparable methods for training machine learning tasks including neural networks.

1 INTRODUCTION

In the field of machine learning, solving the expected risk minimization problem has received lots of attentions over decades, which is in the form

min L(w) = min Ez[f (w, z)],

wRd

wRd

(1)

where z is a d + 1 dimensional random variable containing both feature variables and a response variable. f (w, z) is a loss function with respect to w and any fixed value of z. In most practical problems, the distribution of z is either unknown or leading great difficulties to evaluate the expected loss. One general idea is to estimate the expectation with a statistical average over a large number of independent and identically distributed data samples of z, which is denoted by {z1, z2, . . . , zN } where N is the total number of samples. Thus, the problem in (1) can be rewritten as the Empirical Risk Minimization (ERM) problem

1N

min LN (w) = min

wRd

wRd

N

fi(w),
i=1

(2)

where fi(w) = f (w, zi).
A lot of studies have been done on developing optimization algorithms to find an optimal solution of above problem under different setting. For example, Beck & Teboulle (2009); Nesterov (2013); Drusvyatskiy et al. (2016); Ma et al. (2017) are some of the gradient-based methods which require at least one pass over all data samples to evaluate the gradient LN (w). As the sample size N becomes larger, these methods would be less efficient compared to stochastic gradient methods where the gradient is approximated based on a small number of samples Johnson & Zhang (2013); Roux et al. (2012); Defazio et al. (2014); Shalev-Shwartz & Zhang (2013); Konecny` & Richta´rik (2017); Nguyen et al. (2017).

Second order methods are well known to share faster convergence rate by utilizing the Hessian information. Recently, several papers Byrd et al. (2015); Schraudolph et al. (2007); Mokhtari & Ribeiro (2015) have studied how to apply second orders methods to solve ERM problem. However, getting the inverse of Hessian matrix of a good approximation of it is always quite expensive, leading to a significant difficulty on applying these methods on large scale problems. Following the idea of adaptive sample size discussed in Mokhtari & Ribeiro (2017); Eisen et al.

1

Under review as a conference paper at ICLR 2018

(2017); Mokhtari & Ribeiro (2016), the complexity of Newton's method can be reduced (Mokhtari & Ribeiro, 2016) if the dimension d is small, but it is impractical to compute the Hessian inverse for large dimensional problems. In order to decrease the cost of computing the Hessian inverse, Eisen et al. (2017) proposed the k-Truncated Adaptive Newton (k-TAN) approach. In this method, the inverse of such approximated Hessian is calculated by increasing the sample size adaptively and using a rank-k approximation of the Hessian. The cost per iteration is O((log k +n)d2). Again, note that either when d is large, or in the case when k is close to d, this method can be quite inefficient.
In this paper, we propose an increasing sample size second-order method which solves the Newton step in ERM problems more efficiently. Our proposed method, called Restarting NewtonCG (RNC) method, starts with a tiny number of samples and only considering the corresponding empirical risk based on these samples. This problem is solved up to some accuracy, and the solution of this stage is a warm start for the next stage in which we solve the next empirical risk with a larger number of samples, which contains all the previous samples. Such procedure is run iteratively until either all the samples have been included, or we find that it is unnecessary to further increase the sample size. Our RNC method combines the idea of increasing sample size and the inexact damped Newton method discussed in Zhang & Xiao (2015) and Ma & Taka´c (2016). Instead of solving the Newton system directly, we apply preconditioned conjugate gradient (PCG) method as the solver for each Newton step. We show the required number of PCG steps in order to reach the statistical accuracy of the full dataset.
Also, it is always a challenging problem of running first order algorithms such as SGD and Adam Kingma & Ba (2014) in a distributed way. However, our algorithm is designed naturally to be easily parallelized and shares the strong scaling property. While splitting the gradient and Hessian-vector product computation based on local data stored across different machines, it is always expected to get extra acceleration via increasing the number of computational nodes. We show that, under distributed setting, our RNC algorithm is communication efficient in both theory and experiments.
We organize this paper as following. In Section 2, we introduce the necessary assumptions and the definition of statistical accuracy. Section 3 describes the proposed algorithm and its distributed version. Section 4 explores the theoretical guarantees on complexity. In Section 5, we demonstrate the outstanding performance of our algorithm in practice. Section 6 is the concludes our contribution.

2 PROBLEM FORMULATION

In this paper, we focus on finding the optimal solution w of (1). As described earlier, we are
trying to find a solution for the empirical loss function LN (w), which is the statistical mean over N samples. Now, consider the empirical loss Ln(w) associated with n  N samples. In Bousquet & Bottou (2007) and Bottou (2010s), the error between the expected loss and the empirical loss Ln is calculated. In Mokhtari & Ribeiro (2016), it is mentioned that Ln approximates the expected loss with statistical accuracy Vn for all w  Rd with high probability(w.h.p),

sup |L(w) - Ln(w)|  Vn.
wRd

(3)

In other words, there exists a constant  such that the inequality (3) holds with probability of at least

1 - . Generally speaking, statistical accuracy Vn depends on n (although it depends on  too, but

for

simplicity

in

notation

we

just

consider

the

size

of

the

samples),

and

is

of

the

order

Vn

=

O(

1 n

)

where   [0.5, 1] (Vapnik (2013); Bousquet (2002); Bartlett et al. (2006)).

For problem (2), if we've found an approximate solution wn which satisfies the inequality Ln(wn)- Ln(w^n)  Vn, where w^n is the true minimizer of Ln, it is not necessary to go further and find a better solution (a solution with less optimization error). The reason comes from the fact that for
a more accurate solution the summation of estimation and optimization errors does not become
smaller than Vn. Therefore, when we say that wn is an Vn-suboptimal solution for the risk Ln, it means that Ln(wn) - Ln(w^n)  Vn. In other words, wn solves problem (2) to reach its statistical accuracy.

It is crucial to note that if we add an additional term in the magnitude of Vn to the empirical loss Ln,

the new solution is also in the similar magnitude as Vn to the expected loss L. Therefore, we can

regularize

the

non-strongly

convex

loss

function

Ln

by

cVn 2

w

2 and consider it as the following

2

Under review as a conference paper at ICLR 2018

problem:

min Rn(w)
wRd

:=

1 n

ni=1fi(w)

+

cVn 2

w

2.

(4)

The noticeable feature of the new empirical risk Rn is that Rn is cVn-strongly convex, where c is a positive constant. Thus, we can utilize any practitioner-favorite algorithm. Specifically, we are

willing to apply the inexact damped Newton method, which will be discussed in the next section.

Due to the fact that a larger strong-convexity parameter leads to a faster convergence, we could

expect that the first few steps would converge fast since the values of cVn in these steps are large
(larger statistical accuracy), as discussed in Theorem 1. From now on, when we say wn is an Vnsuboptimal solution of the risk Rn, it means that Rn(wn) - Rn(wn )  Vn, where wn is the true optimal solution of the risk Rn. Our final aim is to find wN which is VN -optimal solution for the
risk RN which is the risk over the whole dataset.

3 RESTARTING NEWTONCG METHOD WITH INCREASING SAMPLE SIZE

The inexact damped Newton method, which is discussed in the study of Zhang & Xiao (2015), is to find the next iterate based on an approximated Newton-type update. It has two important differences comparing to exact Newton method. First, as it clear from the word "damped", the learning rate of the inexact damped Newton type update is not 1, since it depends on the approximation of Newton decrement. The second distinction is that there is no need to compute exact Newton direction (which is very expensive to calculate in one step). Alternatively, an approximated inexact Newton type direction is calculated by applying an iterative process to obtain a direction with desirable accuracy under some measurement.

In order to utilize the important features of ERM, we combine the idea of increasing sample size
and the inexact damped Newton method. In our proposed method, we start with handling a small
number of samples, assume m0 samples. We then solve its corresponding ERM to its statistical accuracy, i.e. Vm0 , by inexact damped Newton algorithm. At the next iteration, we increase the number of samples geometrically with rate of , i.e., m0 samples. The approximated solution of the previous ERM can be used as a warm start point to find the solution of the new ERM. The sample
size increases until it equals the number of full samples.

Consider the iterate wm within the statistical accuracy of the set with m samples, i.e. Sm for the risk
Rm. The inexact damped Newton method with increasing sample size finds the iterate wn which is Vn-suboptimal solution for the sample set Sn, i.e. Rn(wn) - Rn(wn )  Vn after Tn iterations. We initialize w~0 = wm and consider the following update:

w~k+1

=

w~k

-

1 1+n (w~k

)

vk

,

(5)

where vk is k-Newton direction defined as Definition 1 ( k-Newton direction).

2Rn(w~k)vk - Rn(w~k)  k.

(6)

Note that k has a crucial effect in the speed of the algorithm. We use preconditioned CG (PCG)

(by considering the preconditioned matrix P

=

H~n + µI, where H~n

=

1 |A|

iA 2Rni (w)

and A  Sn) in order to find the vector vk, which is an approximate solution of the system

P -12Rn(w~k)vk = P -1Rn(w~k). Moreover, n(w~k) = vkT 2Rn(w~k)vk is the approxi-

mation of (exact) Newton decrement. Also, we have

n

Rn(w)

=

1 n

fi(w) + cVnw,

i=1

n

2Rn(w)

=

1 n

2fi(w) + cVnI.

i=1

(7)

Thus, after Tn-PCG iterations, wn = w~Tn (see Theorem 1). Also, because of k = 0, vk is the exact Newton direction, and the update in (5) is the exact damped Newton step. Furthermore, in Theorem
1 we show that the number of total PCG iterations to reach VN -optimal solution for the risk RN is TN . It means that when we start with the iterate wm0 with corresponding m0 samples, after TN PCG iterations, we reach the point wN with statistical accuracy of VN for the whole dataset. Our proposed method is summarized in Algorithm 1. In the inner for loop of Algorithm 1, in order

3

Under review as a conference paper at ICLR 2018

to calculate the approximate Newton direction and approximate Newton decrement, we use PCG algorithm which is shown in Algorithm 2.
Thus, after Tn-PCG iterations, wn = w~Tn (see Theorem 1). Also, we can note that when k = 0, then vk is the exact Newton direction, and the update in (5) is the exact damped Newton step. Furthermore, in Theorem 1 we show that the number of total PCG iterations to reach VN -optimal solution for the risk RN is TN . It means that when we start with the iterate wm0 with corresponding m0 samples, after TN PCG iterations, we reach the point wN with statistical accuracy of VN for the whole dataset.
Our proposed method is summarized in Algorithm 1. In the inner for loop of Algorithm 1, in order to calculate the approximate Newton direction and approximate Newton decrement, we use PCG algorithm which is shown in Algorithm 2.

Stopping Criterion Here we discuss two stopping criterions to fulfill the 10th line from Algo-

rithm 1. At first, considering wn is unknown in practice, we can use strong convexity inequality as Rn(w~k) - Rn(wn ) 2c1Vn Rn(w~k) 2 to find a stopping criterion for the inner loop, which satisfies Rn(w~k) < ( 2c)Vn. However, this stopping criterion can be too conservative in practice.

Another stopping criterion is discussed by Zhang & Xiao (2015), using the fact that the risk Rn is

self-concordant. This criterion can be written as n(w~k)  (1 - ) Vn (see section 7.1), where





1 20

.

The

later stopping criterion

implies

that Rn(w~k) - Rn(wn )



Vn

whenever

Vn



0.682.

To compare these two criterions, the later criterion is more practical due to the fact that we have

n(w~k) in every iteration. While we need to calculate the gradient of the risk Rn in each iteration of

the inner loop to use the first criterion.

Algorithm 1 Restarting NewtonCG algorithm

1: Initialization: Sample size increase constant , initial sample size n = m0 and wn = wm0 with Rn(wn) < ( 2c)Vn
2: while n  N do

3: Update wm = wn and m = n 4: Increase sample size: n = max{m, N }

5: Set w~0 = wm and set k = 0

6: repeat

7: Calculate vk and n(w~k) by Algorithm 2 PCG

8:

Set

w~k+1

=

w~k

-

1 1+n (w~k

)

vk

9: k = k + 1

10: until a stopping criterion is satisfied

11: Set wn = w~k

12: end while

Algorithm 2 PCG - Algorithm 2 in Zhang & Xiao (2015)

1: Input: w~k  Rd, k, and µ  0

2:

Let H

denote the Hessian 2Rn(w~k) and P

=

1 |A|

iA 2Rni (w~k) + µI

3: Set r(0) = Rn(w~k), s(0) = P -1r(0), v(0) = 0, u(0) = s(0), t = 0

4: repeat

5: Calculate Hu(t) and Hv(t)

6:

Compute t =

r(t) ,s(t) u(t) ,H u(t)

7: Set v(t+1) = v(t) + tu(t), r(t+1) = r(t) - tHu(t)

8:

Compute t =

r(t+1) ,s(t+1) r(t) ,s(t)

9: Set s(t+1) = P -1r(t+1), u(t+1) = s(t+1) + tu(t)

10: Set t = t + 1

11: until rt+1  k

12: return vk = v(t+1) and n(w~k) = vkT Hv(t) + tvkT Hu(t)

4

Under review as a conference paper at ICLR 2018

Distributed Implementation Similar to the algorithm in Zhang & Xiao (2015), Algorithm 1 and

2 can also be implemented in a distributed environment. Suppose the entire dataset is stored across

K machines, i.e., each machine stores Ni data samples such that

K i=1

Ni

=

N.

Under

this

setting,

each iteration in Algorithm 1 can be executed on different machines in parallel with

K i=1

ni

=

n, where ni is the batch-size on ith machine. To implement Algorithm 2 in a distributed way, a

broadcast operation is needed at each iteration to guarantee that each machine will share the same

w~k value. Moreover, the gradient and Hessian-vector product can be computed locally and later

reduce to the master machine. With the increasing of batch size, computation work on each machine

will increase while we still have the same amount of communication need. As a consequence,

the computation expense will gradually dominate the communication expense before the algorithm

terminates. Therefore the proposed algorithm could take advantage of utilizing more machines to

shorten the running time of Algorithm 2.

4 CONVERGENCE ANALYSIS

In this section, first we define the self-concordant function. This kind of function has the property

that its third derivative can be controlled by its second derivative. By assuming that function f :

Rd  R has continuous third derivative, we define self-concordant function as follows. Definition 2. A convex function f : Rd  R is Mf -self-concordant if for any w  dom(f ) and

u  Rd we have

|uT (f

(w)[u])u|



Mf

(uT

2f

(w)u)

3 2

,

(8)

where f

(w)[u]

:=

limt0

1 t

(2

f

(w

+

tu)

-

2f (w)).

As

it

is

discussed

in

Nesterov

(2013),

any

self-concordant function f with parameter Mf can be rescaled to become standard self-concordant

(with parameter 2). There are many well-known empirical loss functions which are self-concordant

such as linear regression, Logistic regression and squared hinge loss. In order to prove our results

the following conditions are considered in our analysis.

Assumption 1. The loss functions f (w, z) are convex w.r.t w for all values of z. In addition, their

gradients f (w, z) are L-smooth

f (w, z) - f (w , z)  L w - w , z.

(9)

Assumption 2. The loss functions f (w, z) are self-concordant w.r.t w for all values of z.

The immediate conclusion of Assumption 1 is that both L(w) and Ln(w) are convex and L-smooth. Also, we can note that Rn(w) is cVn-strongly convex and (cVn + L)-smooth. As it is discussed in Zhang & Xiao (2015) we use the following auxiliary functions, which will be used in the analysis of the self-concordant functions:
(t) = t - log(1 + t), t  0. and (t) = -t - log(1 - t), 0  t < 1. (10)
The above functions can be very helpful in analyzing the self-concordant functions. Also, for the risk Rn, the same as Zhang & Xiao (2015) we can define the following auxiliary vectors:
u~n(w~k) = [2Rn(w~k)]-1/2Rn(w~k) and v~n(w~k) = [2Rn(w~k)]1/2vn. (11)

We can note that u~n(w~k) = Rn(w~k)[2Rn(w~k)]-1Rn(w~k), which is the exact Newton decrement, and, the norm v~n(w~k) = n(w~k) which is the approximation of Newton decrement (and u~n(w~k) = v~n(w~k) in the case when k = 0).

In the rest of this section, we analyze the upper bound for the number of iterations needed to solve every subproblem up to its statistical accuracy.

We prove a linear convergence rate for our algorithm. We analyze the case when we have wm which
is a Vm-suboptimal solution of the risk Rm, and we are interested in deriving a bound for the number
of required iterations, Tn, to ensure that wn is a Vn-suboptimal solution for the risk Rn. We use the
analysis of DiSCO algorithm discussed in Zhang & Xiao (2015) to find the bound for Tn. Theorem 1. Suppose that Assumptions 1 and 2 hold. Consider wm which satisfies Rm(wm) - Rm(wm )  Vm and also the risk Rn corresponding to sample set Sn  Sm where n = m,  > 1. Set the parameter k (the error in (6)) as following

k

=



(

cVn L+cVn

)1/2

Rn (w~k )

,

(12)

5

Under review as a conference paper at ICLR 2018

where  Rn(wn )

 

1 20

.

Then

the

variable

wn

is

an

Vn-suboptimal

Vn if the number of iterations Tn satisfies in the

solution for following:

the

risk

Rn,

i.e

Rn(wn)

-

Tn 

Rn(wm)-Rn(wn )

1 2

(1/6)

+

log2

(

2(1/6) Vn

)

1

+

2µ cVn

)

log2

2(cVn +L) cVn

, w.h.p. (13)

Here t shows the smallest nonnegative integer larger than or equal to t.

By utilizing Theorem 1, the following number of iterations, TN , is needed to reach the statistical accuracy of VN of the full training set with high probability:

|P |
TN 
i=2

+ log ( )RP[i](wP[i-1])-RP[i](wP [i])

1 2

(1/6)

2(1/6) 2 VP[i]

1

+

2µ cVP [i]

)

log2

2(cVP [i] +L)  cVP [i]

,

(14)

where P = {m0, m0, 2m0, . . . , N }. Also, based on the result in (13), by considering the risk

Rn, we can note that when the strong-convexity parameter for the mentioned risk (cVn) is large, less

number of iterations are needed (or equally faster convergence is achieved) to reach the iterate with

Vn-suboptimal solution; and this happens in the first steps.

Corollary 1. Suppose that Assumptions 1 and 2 hold. By assuming that wm is Vm-suboptimal solution and also consider the risk Rn corresponding to sample set Sn  Sm where n = 2m. If we set parameter k (the error in (6)) as (12), then after T~n iterations, where with high probability:

T~n 

3+

1-

1 2

2+

c 2

w

2

1 2

(1/6)

Vm

+

log2

(

2(1/6) Vn

)

1

+

2µ cVn

)

log2

2(cVn +L) cVn

,

(15) we reach the point wn with statistical accuracy of Vn for the risk Rn. Moreover, after T~N iterations

we reach a point with the statistical accuracy of VN of the full training set:

T~N 

2 log2

N m0

+

3+

1-

1 2

2+

c 2

w

2

1 2

(1/6)

N

V1-(

1 2

)log2

m0

1-

1 2

m0

+

log2

N m0

log2

(

2(1/6) VN

)

(1

+

2µ cVN

) log2

2 

+

2L c

.

1 VN

, w.h.p, (16)

where m0 is the number of initial sample.



By T~N

Corollary 1, we can notice = O((log2 N )3 N ), and

that T~N for  =

= O((log2 N )2 N  log2 N ), and when

0.5,

the

result

is

T~N

=

O((log2

N )3N

1 4

).



=

1,

we

have

5 NUMERICAL EXPERIMENTS

In this section, we present numerical experiments on several large real-world datasets to show that our restarting NewtonCG algorithm can outperform other existed methods on solving both convex and non-convex problems. Also, we compare the results from utilizing different number of machines to demonstrate the nice scaling property for our algorithm. All the experiments are performed on a cluster with 24 Xeon E5-2620 CPUs (2.40GHz), and all the algorithms are implemented in Python with PyTorch library. In the plots, we use the pink vertical dashed lines to represent when restarting happens in our NewtonCG algorithm.

Convex case First, we compare our restarting NewtonCG algorithm with two other distributed optimization algorithms CoCoA Smith et al. (2016) and Disco Zhang & Xiao (2015), on solving convex problems. We choose these two algorithms in consideration of attaining a fair comparison between distributed first-order (CoCoA) method and distributed second-order (DiSCO) approach. Binary classification tasks based on two datasets rcv1 and news20 chosen from Chang & Lin (2011) are solved using logistic regression model. We choose this two datasets following the principle from Zhang & Xiao (2015), where those two datasets show different relations between number of features and number of data samples (larger and smaller). The empirical loss function we are trying to minimize is stated as in (4). We use logistic loss function defined as fi(w) := log(1 +

6

Under review as a conference paper at ICLR 2018

loss loss loss loss

ERM, rcv_train
0.7 CoCoA 0.6 Disco
NewtonCG 0.5 Restart
0.4
0.3
0.2
0.1
0.0 0 5 10 15 20 25
epochs ERM, rcv_train
0.7 CoCoA 0.6 Disco
NewtonCG
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300
running_time ERM, news20
0.7 CoCoA 0.6 Disco
NewtonCG 0.5 Restart
0.4
0.3
0.2
0.1
0.0 0 5 10 15 20 25 30 35 40
epochs ERM, news20
0.7 CoCoA 0.6 Disco
NewtonCG
0.5
0.4
0.3
0.2
0.1
0.0 0 500 1000 1500 2000 2500 3000 3500 4000
running_time

train_acc

train_acc

train_acc

train_acc

ERM, rcv_train
1.0
0.8
0.6
0.4 CoCoA Disco NewtonCG
0.2 Restart
0 5 10 15 20 25
epochs ERM, rcv_train
1.0
0.8
0.6
0.4
CoCoA Disco 0.2 NewtonCG
0 50 100 150 200 250 300
running_time ERM, news20
1.0
0.8
0.6
0.4 CoCoA Disco NewtonCG
0.2 Restart
0 5 10 15 20 25 30 35 40
epochs ERM, news20
1.0
0.8
0.6
0.4
CoCoA Disco 0.2 NewtonCG
0 500 1000 1500 2000 2500 3000 3500 4000
running_time

test_acc

test_acc

test_acc

test_acc

ERM, rcv_train
1.0
0.8
0.6
0.4 CoCoA Disco NewtonCG
0.2 Restart
0 5 10 15 20 25
epochs ERM, rcv_train
1.0
0.8
0.6
0.4
CoCoA Disco 0.2 NewtonCG
0 50 100 150 200 250 300
running_time ERM, news20
1.0
0.8
0.6
0.4 CoCoA Disco NewtonCG
0.2 Restart
0 5 10 15 20 25 30 35 40
epochs ERM, news20
1.0
0.8
0.6
0.4
CoCoA Disco 0.2 NewtonCG
0 500 1000 1500 2000 2500 3000 3500 4000
running_time

Figure 1: Restarting Newton-CG v.s. DiSCO v.s CoCoA for Logistic Regression

exp(-yiwT xi)), where xi  Rd is data sample and yi  {-1, 1} are binary label. Note that there is a fixed regularization parameter 10-6 in DiSCO and CoCoA, while our restarting NewtonCG has Regularization of 1/ m which depends on the size of samples m.
We run all these three algorithms using 8 nodes. The starting batch-size on each node for restarting NewtonCG is set to 16 for a faster beginning, while other two will go over the whole dataset at each iteration. For restarting NewtonCG implementation, number of samples used to form the new ERM loss are doubled from previous iteration after each restarting. Furthermore, restarting happens whenever norm of loss gradient is lower than 1/ m.
From Figure 1, we observe consistently that the restarting NewtonCG algorithm has a better performance over the other two in the begin stages. Both loss value and training accuracy under our restarting NewtonCG algorithm converges to optimality by passing a very small number of samples, which suggests that the restarting NewtonCG can find a good solution in a warm starting manner. Compared with DiSCO, our restarting approach helps to get rid of spending too much computation at the beginning iterations, where second order methods are usually less efficient than first order methods. Also, our algorithm can still converge fast when we are close to optimal solution, while first order methods become weak since the gradient becomes more and more close to zero.
Non-convex case Even though the iteration complexity analysis only covers the convex case, we want to point out that our algorithm is also able to handle nonconvex problems efficiently. In this section, we compare the performance of increasing sample size method with the well known Adam Kingma & Ba (2014) method on solving convolution neural network. We do experiments on the standard image classification dataset Mnist with a 5-layer convolutional neural network. In Fig-

7

Under review as a conference paper at ICLR 2018

loss loss

NaiveCNet, Mnist
Adam 2.0 NewtonCG
Restart
1.5
1.0
0.5
0.0
epochs0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 NaiveCNet, Mnist
Adam 2.0 NewtonCG
1.5
1.0
0.5
0.0
0 50 1r00unnin15g0 _tim2e00 250 300

train_acc

train_acc

NaiveCNet, Mnist
1.0
0.8
0.6
0.4
Adam NewtonCG 0.2 Restart
epochs0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 NaiveCNet, Mnist
1.0
0.8
0.6
0.4
Adam 0.2 NewtonCG
0 50 1r00unnin15g0 _tim2e00 250 300

test_acc

test_acc

NaiveCNet, Mnist
1.0
0.8
0.6
0.4
Adam NewtonCG 0.2 Restart
epochs0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 NaiveCNet, Mnist
1.0
0.8
0.6
0.4
Adam 0.2 NewtonCG
0 50 1r00unnin15g0 _tim2e00 250 300

Figure 2: Restarting Newton-CG v.s. Adam using NaiveCNet on Mnist dataset

ure 2, we compare Adam with our restarting NewtonCG approach. The Adam is implemented using bulid-in optimizer in pytorch library, and we choose the best batch-size as 64 from the range {16, 32, 64, 128} and initial learning rate as 0.005 from the range {0.001, 0.005, 0.01, 0.05}. Regarding our restarting NewtonCG, we experiment on 32 nodes. The initial batch size are set to 8 for each node, i.e, set 256 as our initial total batch size across all nodes. As it is clear shown in Figure 2, our restarting NewtonCG with 32 nodes could outperform serial Adam. Note that Adam, i.e., the stochastic first-order method variant, can not be distributed easily, since a small batch size is require to have start-of-art performance He et al. (2016). While we could further improve our restarting approach by utilizing more nodes.

epochs loss
test_acc

4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.00

1 nodes 2 nodes 4 nodes 8 nodes 16 nodes 32 nodes
20

NaiveCNet, Mnist 40 60 running_time

80

100

NaiveCNet, Mnist
1 nodes 2 nodes
2.0 4 nodes
8 nodes 16 nodes
1.5 32 nodes
1.0
0.5
0.0 0 20 40 60 80 100 running_time

NaiveCNet, Mnist 1.0

0.8

0.6

0.4 0.2
0

1 nodes 2 nodes 4 nodes 8 nodes 16 nodes 32 nodes
50 100 150 200 250 300 running_time

Figure 3: Performance of restarting NewtonCG algorithm with different computing threads.

Strong scaling As the last experiment, we demonstrate that our restarting NewtonCG algorithm shares a strong scaling property. As shown in Figure 3, whenever we increase the number of nodes, we can always obtain acceleration towards optimality. The leftmost plot in Figure 3 shows that the speed of passing over data increases along the increase of number of nodes used, since distributed computation among nodes will obtain the gradient and Hessian-vector product faster. As it is shown in rightmost plot in Figure 3, to reach 0.96 testing accuracy, it is about 12 times slower by only using 1 node than using 32 nodes.
6 CONCLUSION
We propose a restarting NewtonCG method with increasing sample size strategy to solve the expected risk minimization problem. Our algorithm can converge to a low statistical accuracy in very few epochs and also be implemented in a distributed environment naturally. We show linear convergence rate for convex empirical risk minimization under mild assumptions. Numerical experiments are presented to demonstrate the advantages of our proposed algorithm on both convex and nonconvex problems.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138­156, 2006.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2009.
Le´on Bottou. Large-scale machine learning with stochastic gradient descent. In Proc. of COMPSTAT, 2010s.
olivier Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of Learning Algorithms. PhD thesis, Biologische Kybernetik, 2002.
Olivier Bousquet and Le´on Bottou. The tradeoffs of large scale learning. NIPS, 2007.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Richard H. Byrd, Samantha L. Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton method for large-scale optimization. SIAM J. Optim., 26(2), 10081031. (24 pages), 2015.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. NIPS, 2014.
Dmitriy Drusvyatskiy, Maryam Fazel, and Scott Roy. An optimal first order method based on optimal quadratic averaging. arXiv preprint arXiv:1604.06543, 2016.
Mark Eisen, Aryan Mokhtari, and Alejandro Ribeiro. Large scale empirical risk minimization via truncated adaptive newton method. arXiv preprint arXiv:1705.07957, 2017.
Xi He, Dheevatsa Mudigere, Mikhail Smelyanskiy, and Martin Taka´c. Large scale distributed hessian-free optimization for deep neural network. arXiv preprint arXiv:1606.00511, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. NIPS, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jakub Konecny` and Peter Richta´rik. Semi-stochastic gradient descent methods. Frontiers in Applied Mathematics and Statistics, 2017.
Chenxin Ma and Martin Taka´c. Distributed inexact damped newton method: Data partitioning and load-balancing. arXiv preprint arXiv:1603.05191, 2016.
Chenxin Ma, Naga Venkata C Gudapati, Majid Jahani, Rachael Tappenden, and Martin Taka´c. Underestimate sequences via quadratic averaging. arXiv preprint arXiv:1710.03695, 2017.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. Journal of Machine Learning Research, 16(1):3151­3181, 2015.
Aryan Mokhtari and Alejandro Ribeiro. Adaptive newton method for empirical risk minimization to statistical accuracy. arXiv preprint arXiv:1605.07659, 2016.
Aryan Mokhtari and Alejandro Ribeiro. First-order adaptive sample size methods to reduce complexity of empirical risk minimization. arXiv preprint arXiv:1709.00599, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.
Lam Nguyen, Jie Liu, Katya Scheinberg, and Martin Taka´c. Sarah: A novel method for machine learning problems using stochastic recursive gradient. arXiv preprint arXiv:1703.00102, 2017.
9

Under review as a conference paper at ICLR 2018

Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems, pp. 2663­2671, 2012.
Nicol N Schraudolph, Jin Yu, and Simon Gu¨nter. A stochastic quasi-newton method for online convex optimization. In Artificial Intelligence and Statistics, pp. 436­443, 2007.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 2013.
Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I Jordan, and Martin Jaggi. Cocoa: A general framework for communication-efficient distributed optimization. arXiv preprint arXiv:1611.02189, 2016.
Vladimir Vapnik. The nature of statistical learning theory. Springer, 2013.
Yuchen Zhang and Lin Xiao. Communication-efficient distributed optimization of self-concordant empirical loss. arXiv preprint arXiv:1501.00263, 2015.

7 APPENDIX

Before talking about the main results, two following lemmas are used in our analysis. Lemma 1. (Lemma 4 in Zhang & Xiao (2015)) Suppose Assumption 1 holds and H~n - 2Rn(w~k)  µ. Then, Algorithm 2, after Tµ iterations calculates vn such that 2Rn(w~k)vn - Rn(w~k)  k, where

Tµ =

1

+

2µ cVn

)

log

2

cVn +L cVn

Rn (w~k )

k

.

(17)

Lemma 2. (Proposition 5 in Mokhtari & Ribeiro (2016)) Consider the sample sets Sm with size m and Sn with size n such that Sm  Sn. Let wm is Vm-suboptimal solution of the risk Rm. If
assumptions 1 and 2 hold, then the following is true:

Rn(wm) - Rn(wn )



Vm

+

2(n-m) n

(Vn-m

+ Vm) + 2(Vm

- Vn) +

c(Vm -Vn ) 2

w

2,

w.h.p.

(18)

If

we

consider

Vn

=

O(

1 n

)

where





[0.5,

1],

and

assume

that

n

=

2m

(or



=

2),

then

(18)

can

be written as:

Rn(wm) - Rn(wn ) 

3+

1

-

1 2

2

+

c 2

w

2

Vm.

(19)

7.1 PRACTICAL STOPPING CRITERION

As a result of Theorem 1 in the study Zhang & Xiao (2015), we have:

(1 - ) u~n(w~k)  v~n(w~k)  (1 + ) u~n(w~k) ,

(20)

where





1 20

.

Also,

by

the

equation

in

(11),

we

know

that

v~n (w~k )

= n(w~k). As it is discussed

in the section 9.6.3. of the study Boyd & Vandenberghe (2004), we have (t)  t2 for 0  t 

0.68.

According to Theorem 4.1.13 in the study Nesterov (2013), if u~n(w~k) < 1 we have:

( u~n(w~k) )  Rn(w~k) - Rn(wn )  ( u~n(w~k) ).

(21)

Therefore, if u~n(w~k)  0.68, we have:

Rn(w~k) - Rn(wn )  ( u~n(w~k) )  u~n(w~k) 2

(20)



1 (1-)2

v~n (w~k )

2=

1 (1-

)2

n2

(w~k

)

(22)

Therefore,

we

can

note

that

n (w~k )



(1

-

 ) Vn

concludes

that

Rn (w~k )

-

Rn(wn )



Vn

when

Vn  0.682.

10

Under review as a conference paper at ICLR 2018

7.2 PROOF OF THEOREM 1

According to the Theorem 1 in Zhang & Xiao (2015), we can derive the iteration complexity by
starting from wm as a good warm start, to reach wn which is Vn-suboptimal solution for the risk Rn. By considering assumption 2, we can assume that Rn is a standard self-concordant function. According to the Corollary 1 in Zhang & Xiao (2015), we can note that if we set k the same as (12), after K iterations we reach the solution wn such that Rn(wn) - Rn(wn )  Vn where

K=

Rn(wm)-Rn(wn )

1 2

(1/6)

+

log2

(

2(1/6) Vn

)

.

(23)

Also, according to Lemma 1, we can note that the number of PCG steps needed to reach the approximation of Newton direction with precision k is as following:

Tµ = (=12)

1

+

2µ cVn

)

log2

2

cVn +L cVn

Rn (w~k )

k

1

+

2µ cVn

)

log2

2(cVn +L) cVn

.

(24)

Therefore, we can note that when we start from wm, which is Vm-suboptimal solution for the risk Rm, after Tn PCG steps, where Tn  KTµ, we reach the point wn which is Vn-suboptimal solution
of the risk Rn, which follows (13). Suppose the initial sample set contains m0 samples, and consider the set P = {m0, m0, 2m0, . . . , N }, then after TN PCG steps, we reach VN -optimal solution for the whole
data set:

|P |
TN 
i=2

+ log ( )RP[i](wP[i-1])-RP[i](wP [i])

1 2

(1/6)

2(1/6) 2 VP[i]

1

+

2µ cVP [i]

)

log2

2(cVP [i] +L)  cVP [i]

.

(25)

7.3 PROOF OF COROLLARY 1

The proof of the first part is trivial. According to Lemma 2, we can find the upper bound for Rn(wm) - Rn(wn ), and when  = 2, by utilizing the bound (19) we have:

K=

Rn(wm)-Rn(wn )

1 2

(1/6)

+

log2

(

2(1/6) Vn

)

(19)


3+

1-

1 2

2+

c 2

w

2

1 2

(1/6)

Vm

+

log2

(

2(1/6) Vn

)

.

:=K~

(26)

Therefore, we can note that when we start from wm, which is Vm-suboptimal solution for the risk Rm, after T~n PCG steps, where T~n  K~ Tµ, Tµ is defined in (24), we reach the point wn which is
Vn-suboptimal solution of the risk Rn, which follows (15).
Suppose the initial sample set contains m0 samples, and consider the set P = {m0, 2m0, 4m0, . . . , N }, then the total number of PCG steps, T~N , to reach VN -optimal solution

11

Under review as a conference paper at ICLR 2018

for the whole data set is as following:

|P |
i=2
log2

3+

1-

1 2

2(cVP [i] +L)  cVP [i]

2+

c 2

w

2

1 2

(1/6)

VP [i-1]

+

log2

(

2(1/6) VP [i]

)



log2

N m0

+

3+

1-

1 2

2+

c 2

w

2

1 2

(1/6)

N

V1-(

1 2

)log2

m0

1-

1 2

m0

|P |
+
i=2

log2

(

2(1/6) VP [i]

)

1

+

2µ cVN

) log2

2 

+

2L c

.

1 VN



2 log2

N m0

+

3+

1-

1 2

2+

c 2

w

2

1 2

(1/6)

N

V1-(

1 2

)log2

m0

1-

1 2

m0

1

+

)2µ
cVP [i]

+

log2

N m0

log2

(

2(1/6) VN

)

1

+

2µ cVN

) log2

2 

+

2L c

.

1 VN

 T~N .

(27)

12

