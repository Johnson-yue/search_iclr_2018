Under review as a conference paper at ICLR 2018
LEARNING DEEP MODELS: CRITICAL POINTS AND LOCAL OPENNESS
Anonymous authors Paper under double-blind review
ABSTRACT
With the increasing interest in deeper understanding of the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models, we provide sufficient conditions under which any local optimum of the resulting optimization problem is global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are global minima. Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global. Unlike many existing results in the literature, our result assumes no assumption on the target data matrix Y . For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.
1 INTRODUCTION
Deep learning models have recently led to significant practical successes in various fields ranging from computer vision to natural language processing. Despite these significant empirical successes, the theoretical understanding of the behavior of these models is still very limited. While some works have tried to explain these successes through the lenses of expressivity of these models by showing their power in learning large class of mappings, other works find the root of the success in the generalizability of these models from learning perspective.
From optimization perspective, training deep models typically requires solving non-convex optimization problem due to the "deep" structure of the model. In fact, it has been shown by Blum & Rivest (1989) that training neural networks to global optimality is NP-complete in the worst case even for the simple case of three node networks. Despite this worst case barrier, the practical success of deep learning may suggest that most of the local optima of these models are close to the global optima. In particular, Choromanska et al. (2015) use spin glass theory and empirical experiments to show that the local optima of deep neural netrwork optimization problem are close to the global optima.
In an effort to better understand the landscape of training deep neural networks, Kawaguchi (2016); Lu & Kawaguchi (2017); Yun et al. (2017); Hardt & Ma (2016) studied the linear neural networks and provide sufficient conditions under which critical points (or local optima) of the optimization problem are globally optimal. For non-linear neural networks, various works have shown that when the number of parameters of the model is larger than the data dimension, then the local optima of the resulting optimization problems are easy and they can be found through local search procedures; see, e.g., Soltanolkotabi et al. (2017); Soudry & Carmon (2016); Nguyen & Hein (2017); Xie et al. (2017).
Despite the growing interest in studying the landscape of deep optimization problems, many of the results and mathematical analyses are problem specific and cannot be generalized to other problems
1

Under review as a conference paper at ICLR 2018

and network structures easily. As a first step toward reaching a unifying theory for these results, we propose the use of open mappings for characterizing the properties of the local optima of an optimization problem.

To study the landscape of shallow/deep models, let us start by the general optimization problem

minimize (F(w))
wW

(1)

for training of a learning model. Here (·) is the loss function and F(·) represents a statistical model with parameter w which needs to be learned by solving the above optimization problem. A simple example is the popular linear regression problem:

minimize
w

Xw - y 22,

where y is a given constant response vector and X is a given constant feature matrix. In this example, the loss function is the 2 loss, i.e., (z) = z - y 22, and the fitted model F is a linear model, i.e., F(w) = Xw. While this linear regression problem is convex and easy, fitting many practical models, such as deep neural networks, requires solving non-trivial non-convex optimization problems. In addition to the training of deep neural networks, the well-studied matrix completion problem also lies in this category of non-convex problems. For this matrix completion problem, Park et al. (2016) shows that the non-convex matrix factorization formulation of the non-square matrix sensing problem has no spurious local optimum under restricted isometry property (RIP) conditions. Similar results were obtained for the symmetric matrix multiplication problem by Ge et al. (2016), and the non-convex factorized low-rank matrix recovery problem by Bhojanapalli et al. (2016). In this paper, we use the local openness of the mapping F to provide sufficient conditions under which every local optimum is in fact global.

To proceed, let us define our notations that will be used throughout the paper. We use the notation Al,:, and A:,l to denote the lth row and column of matrix A respectively. Let A , N (A), C(A), rA be the respective Frobenius norm, null space, column space, and rank of A. Given subspaces U and V , we say U  V if U is orthogonal to V , and U = V  if U is the orthogonal complement of V . We say matrix A  Rd1×d0 is rank deficient if rank(A) < min(d1, d0), and full rank if rank(A) = min(d1, d0). We call a point W = (Wh, . . . , W1) non-degenerate if rank(W ) = min0ih di, and degenerate if rank(W )< min0ih di. We also say a point W = (Wh, . . . , W1) is a second order saddle point if the hessian of the loss function at W has a negative eigenvalue.

Let us start by briefly highlighting two problems which will be used as a motivation for our analysis:

Feedforward Neural Networks: Consider the following multiple layer feedforward neural network optimization problem:

minimize
W

1 2

Fh(W ) - Y

2 F

with

F1(W )

1 (W1 X ),

Fk(W )

k WkFk-1(W ) ,

for k  [2, h], where k(·), k = 1, . . . , h, are the activation functions for different layers, W = Wi hi=1, Wi  Rdi×di-1 are the weight matrices, X  Rd0×n is the input training data, and Y  Rdh×n is the target training data, see Goodfellow & Courville (2016). To obtain the representation
in (1), we need to set our loss function to the 2 loss, and set F = Fh.

A special instance of this optimization problem was studied in Nguyen & Hein (2017), which considers the non-linear neural network with pyramidal structure (i.e. di  di-1  i = 1, . . . , h and d0  n). (Nguyen & Hein, 2017, Theorem 3.8) shows that under some conditions, among which are the differentiability of the loss function (·) and the activation function (·), if W is a critical point
with Wi's being full row rank then it is a global minimum. In our paper, we show that by relaxing the differentiability condition on both (·) and (·), we can still obtain a similar result under very
minimal set of assumptions.

Another special case is the linear feedforward networks where the mapping k(·) is the identity map

in all layers:

minimize
W

1 2

Wh · · · W1X

-Y

2 F

.

(2)

For this optimization problem Lu & Kawaguchi (2017) showed that every local optimum of the ob-

jective function is globally optimal under some assumptions. More precisely, by using perturbation

2

Under review as a conference paper at ICLR 2018

analysis, (Lu & Kawaguchi, 2017, Theorem 2.2) prove that when X and Y are full row rank, every local optimum in problem (2) is a local optimum in problem (3).

minimum
Z Rdh ×n
subject to

1 ||ZX 2

-

Y ||2F

rank(Z)  dp min0ih di

(3)

Moreover, they show that when X is full row rank, every local optimum of problem (3) is a global
optimum. Thus, with the sufficient condition of X and Y being both full row rank, every local
optimum of problem (2) is a global optimum. Yun et al. (2017) also show that the same result hold when XXT and Y XT are both full rank. It is in fact not hard to see that one cannot relax the full
rankness assumption of Y due to the following simple example:

X = I W3 =

1 0

,

W2 = [ 0 ] ,

W1 = [ 1 0 ] ,

Y=

00 01

which is a local optimum of 3-layer deep linear model problem (2) with h = 3 that is not global. However, if a given local optima is non-degenerate (which is a simple checkable condition), the full rankness of Y can be relaxed. In particular, we will show that if X is full row rank, then every non-degenerate critical point is either a global optimum or a saddle point, thus relaxing the full row rank assumption on Y .

Non-symmetric Matrix Completion and Matrix Factorization: Consider the following nonsymmetric matrix completion optimization problem:

minimize
W1Rd1×d0 ,W2Rd2×d1

1 2

 (W2W1 - Y )

2 F

(4)

where  is a linear mapping that represents the sensing process and Y  Rd2×d0 is a low rank target matrix. When  only selects a subset of entries, we get the famous Netflix prize problem, see Koren (2009). When  is the identity mapping, this problem becomes the low rank matrix estimation problem described in Srebro & Jaakkola (2003), which can be seen as a 2-layer linear neural network optimization problem

1

minimize
W1Rd1×d0 ,W2Rd2×d1

2

W2W1X

-Y

2 F

(5)

with X = I. In problem (5), the loss function is the 2 loss, and the mapping F is defined as F (W1, W2) = W2W1. In this paper, we show the following results for problem (5):

· Every degenerate critical point of (5) is either a global minimum or a second-order saddle point. This result can be generalized to general loss function (·).
· We show that for problem (5), if X is full row rank, then every non-degenerate critical point is either a global minimum or a saddle point.

In addition to these results, we completely characterize the local openness of the matrix product map in its range. This result could be used in many other optimization problems for characterizing the local/global equivalence.

2 MATHEMATICAL FRAMEWORK
As discussed in the previous section, we are interested in solving

minimize (F(w))
wW

(6)

where F : W  S is a mapping and : S  R is a loss function. Here we assume the set W is closed and the mapping F is continuous. In non-convex scenarios, this optimization problem can
only be solved up to "local optima" by local search procedures; see Lee et al. (2016) for an example.
In this paper, we study problems (6) and (7), and provide sufficient conditions under which any

3

Under review as a conference paper at ICLR 2018

local optimum is in fact global. To proceed with our analysis, we define the auxiliary optimization

problem

minimize (s)
sS

(7)

where S is the range of the mapping F. Since problem (7) minimizes the function (·) over the range of the mapping F, the global optimal objective values for problems (6) and (7) are the same. Moreover, there is a connection between the global optima of the two optimization problem through the mapping F. However, the connection between the local optima of the two optimization problem is not clear. This connection is in particular important when the local optima of (7) are "nice" (e.g. globally optimal or close to optimal). In what follows, we establish the connection between the local optima of the optimization problems (6) and (7) under some simple sufficient conditions. This connection is then used to study the relation between local and global optima of (6) and (7) for various deep learning models. Let us first define the following concepts:

Definition: A mapping F : W  S is said to be open, if for every open set U  W, F(U ) is (relatively) open in S. The mapping F(w) is said to be locally open at w if for every > 0, there
exists  > 0 such that B F (w)  F B (w) , where B(w)  W is a ball with radius  centered at w, and B (F(w))  S is a ball of radius centered at F(w). A useful property of (locally) open mappings is that the composition of two (locally) open maps is (locally) open.

We now state a simple intuitive result that allows us to establish a connection between the local optima of (6) and (7).
Observation 1. Suppose F( w¯ ) is locally open at w¯. If w¯ is a local minimum of problem (6), then s¯ = F(w¯) is a local minimum of problem (7).

Figure 1: Sketch of the Proof of Observation 1.

Proof. Let w¯ be a local minimum of problem (6). Then there exists > 0 such that (F(w¯))  (F(w)), w  B (w¯). By the definition of local openness,
  > 0 such that B(s¯)  F B (w¯)  (s¯)  (s), s  B(s)
which implies s¯ is a local minimum of problem (7).

Furthermore, if every local optimum of (7) is global, the above result implies that any local optimum of (6) is also global. This simple Lemma motivates us to study the local openness of some popular mappings, through which we can establish the local/global equivalence for various classes of optimization problems.

An example of a mapping that is widely used in many optimization problems, such as deep neural networks (2) and matrix completion (5), is the matrix multiplication mapping defined as

M : Rm×k × Rk×n  RuM { Z  Rm×n with rank(Z)  u min(m, n, k)} with M(X, Y ) XY.

(8)

Although, the matrix multiplication mappings M( X, Y ) appears naturally in deep models and is widely used as a non-convex factorization for rank constrained problems, see Wang et al. (2016); Bhojanapalli et al. (2016); Ge et al. (2016); Srebro & Jaakkola (2003); Sun (2015), to our knowledge,

4

Under review as a conference paper at ICLR 2018

the complete characterization of the openness of this mapping has not been studied in the optimization literature before. This motivated us to study the openness/local openness of the mapping M as
one of our initial steps.

While the classical open mapping theorem in Rudin (1973) states that surjective continuous linear operators are open, this is not true in general for bilinear mappings such as matrix product. In fact, by providing a simple counterexample of a bilinear mapping that is not open, Horowitz (1975) shows that the linear case cannot be generally extended to multilinear maps. Several papers, see Balcerzak et al. (2013; 2005); Behrends (2011), investigate this bilinear mapping and provide a characterization of the points where this mapping is open. Moreover, Behrends (2017) studies the matrix multiplication mapping M which is a special example of bilinear mappings and also provides a complete characterization of the points where the mapping is locally open. However, in their study they consider the range of the mapping to be Rm×n, and not RMu ; which due to the constraint of problem (7) that defines the feasible region to be the range of the mapping F, does not allow us to establish the connection between local optima of problems (6) and (7). For that reason, we study the local openness of the mapping M in its range RuM. An intuitive definition of local openness of M(X, Y ) at (X, Y ) in RuM is as follows. We say the multiplication mapping is locally open at (X, Y ) if for any small perturbation Z~  RuM of Z = XY , there exists a pair (X~ , Y~ ), small perturbations of (X, Y ), such that Z~ = X~ Y~ .

Notice that when k  min(m, n), then RmMin(m,n) = Rm×n. However, in the case where k < min(m, n) the mapping is definitely not locally open in Rm×n, but can still be locally open in RMk .

As a simple example, consider X =

1 2

and Y = [ 1 1 ], then there does not exist X~ , Y~

perturbations of X and Y respectively such that X~ Y~ = Z~ when Z~ is a full rank perturbation of Z = XY ; however, for any rank 1 perturbation Z~ of Z = XY , we can find a perturbed pair (X~ , Y~ )

such that Z~ = X~ Y~ . Motivated by Observation 1, we study in the next section the local openness of

the mapping M. We later use these results to analyze the behavior of local optima of deep neural

networks.

3 LOCAL OPENNESS OF MATRIX MULTIPLICATION MAPPING
Consider X  Rm×k and Y  Rk×n with k  min(m, n). Then the range of the mapping M is the entire space Rm×n. In this case, (Behrends, 2017, Theorem 2.5) provides a complete characterization of the pairs (X, Y ) where the mapping is locally open. However, when k  min(m, n), i.e. the product of the matrix is rank deficient, the characterization of the set of points for which the mapping is locally open remains an unresolved problem. We settled this question in Theorem 3 in this section which provides a complete characterization of points (X, Y ) for which the mapping M is locally open when k < min(m, n). Let us start by restating the main result in Behrends (2017): Proposition 2. [Behrends (2017)]: Assume k  min(m, n), then the the following statements are equivalent:
 X such that X Y = 0 and X + X is full row rank.  1. or  Y such that XY = 0 and Y + Y is full column rank.
2. dim N (X)  C(Y )  k - m or n - (rank(Y ) - dim N (X)  C(Y )  k - rank(X).
3. M(X, Y ) is locally open at (X, Y ).
The above proposition provides a checkable condition which completely characterizes the local openness of the mapping M at different points when the range of the mapping is the entire space. Now, let us state our result that characterizes the local openness of the mapping M in its range when k < min{m, n}. Theorem 3. Let X  Rm×k, Y  Rk×n, and k < min(m, n). Then if rank(X) = rank(Y ), M(X, Y ) is not locally open at (X, Y ). Else if rank(X) = rank(Y ), then the following statements are equivalent:

5

Under review as a conference paper at ICLR 2018

(AX ) :  X such that X Y = 0 and X + X is full column rank. i) and
(AY ) :  Y such that XY = 0 and Y + Y is full row rank.

ii) dim N (X)  C(Y ) = 0  dim N (Y T )  C(XT ) = 0.

iii) Z = M(X, Y ) is locally open at (X, Y ) in RMu .

As previously mentioned, local openness can be described in terms of perturbation analysis. For example, M(X, Y ) is locally open at (X, Y ) if for a given > 0, there exists  > 0 such that for any Z~ = Z + R  RMu with ||R||  , there exists X0, Y0 with ||X0||  , ||Y0||  , such that Z~ = (X + X0)(Y + Y0). As a perturbation bound on , we show that for any locally open pair (X, Y ), given an , we need to choose  to be of order . The details of our analysis can be found in
the proof of Theorem 3 in Appendix A.2.

Remark 1 It follows from Theorem 3 that when X is full column rank, and Y is full row rank, the mapping M(X, Y ) is locally open at (X, Y ). This result was observed in other works; see, e.g., (Sun, 2015, Proposition 4.2). Also when k < min(m, n) if only one of the two matrices is full rank, then the mapping is not locally open. We have showed this result in the proof of Theorem 3, and below is a simple example:

Let

X1 =

1 1

,

Y1 = [ 0 , 0 ] ,

X1Y1 =

00 00

,

R =

0 00

,

then X1Y1 + R is feasible. On the other hand, for a perturbation X1 =

1 2

we have

(X1 + X1)(Y1 + Y1 ) =

(1 + 1) 3 (1 + 1) 4 (1 + 2) 3 (1 + 2) 4

.

and Y1 = [ 3 , 4 ],

Hence, in order for this perturbation to be equal to X1Y1 + R, we need 3 to be different from zero. However, when 3 is different from zero, for small enough 2, there does not exist such an X1, Y1 , or equivalently, M(X1, Y1) is not locally open at (X1, Y1). Similarly, X2 = Y1T and Y2 = X1T constitutes an example of a rank deficient X and full rank Y for which M(X, Y ) is not locally open.

In the next sections, we use our local openness result to characterize the cases where the local optima of various training optimization problem of the form (6) are globally optimal.

4 NON-LINEAR DEEP NEURAL NETWORK WITH A PYRAMIDAL STRUCTURE:

Consider the non-linear deep neural network optimization problem with a pyramidal structure minimize Fh(W ) with F1(W ) 1(W1X); Fk(W ) k WkFk-1(W ) , (9)
W

for k  [2, h], where k(·) is the activation function applied component-wise, i.e.,

k(B) = [k(Bij)]i,j with k : R  R. Here W =

Wi

h i=1

where

Wi

 Rdi×di-1 is

the weight matrix of layer i, and X  Rd0×n is the input training data. In this section, we consider

the pyramidal network structure with d0 > n and di  di-1 for 1  i  h; see Nguyen & Hein

(2017).

First notice that when X is full column rank, the image of the mapping Fh is in fact the entire space Rdh×n and hence every local optima of the auxiliary optimization problem (7) is global. We now show that when Wi's are all full row rank and (·) is invertible, the mapping Fh is locally open at W.
Lemma 4. Assume the functions k(·) : R  R are invertible. Then the mapping Fh defined in (9) is locally open at the point W = (W1, . . . , Wh) if Wi's are all full row rank.

6

Under review as a conference paper at ICLR 2018

Before proving this result, we would like to remark that many of the popular activation functions such as logsitic, tangent hyperbolic, and leaky ReLu are invertible and satisfy the assumptions of this lemma.

Proof. Let us prove by induction. Since linear mappings are open, and since 1(·) is invertible; by using the composition property of open maps, we get that F1 is open.

Assume Fk-1

Wi

k-1 i=1

is locally open at Wi ik=-11, then using Proposition 2, due to the full row

rankness of Wk, the mapping WkFk-1

Wi

k-1 i=1

is locally open at Wk, (Wi)ki=-11 . Using the

composition property of open maps and invertibility of k(·), we get Fk (Wi)ki=1 is locally open at Wi ki=1.

Thus, by Observation 1, if W is a local optimum of problem (9) with Wi's being full row rank, then Z = Fh(W ) is a local optimum of the corresponding auxiliary problem:
minimize (Z)
Z Rdh ×n
and is consequently a global optimum of problem (9) when the loss function (·) is convex. Nguyen & Hein (2017) show that every critical point W of problem (9) with Wi's being full row rank is a global optimum when both (·) and (·) are differentiable. Our result relaxes the differentiability assumption on both the activation and loss functions; however, we can only show all local optima are global.

5 TWO-LAYER LINEAR NEURAL NETWORK

Consider the two layer linear neural network optimization problem

minimize
W

1 2

W2W1X

-Y

2 F

(10)

where W2  Rd2×d1 and W1  Rd1×d0 are weight matrices, X  Rd0×n is the input data, and Y  Rd2×n is the target training data. Using our transformation, the corresponding auxiliary optimization problem can be written as

minimum
Z

1 2

||Z X

-

Y

||2F

subject to rank(Z)  min(d2, d1, d0)

(11)

(Kawaguchi, 2016, Theorem 2.3) shows that when XXT and Y XT are full rank, d2  d0, and when Y XT (XXT )-1XY T has d2 distinct eigenvalues, every local optimum is global and all saddle points are second order saddles. While the local/global equivalence result holds for deeper networks, the property that all saddles are second order does not hold in that case. Another related result by (Yun et al., 2017, Theorem 2.2) shows that when XXT , Y XT , and Y XT (XXT )-1XY T are full rank, every local optimum of a linear deep network is global. Moreover, they provide necessary and sufficient conditions for a critical point to be a global minimum. However, we notice that the full rankness assumption on Y XT was not used in showing the result for non-degenerate critical points and thus can be relaxed in that case. In this section, without any assumption on Y , we reconstruct the proof that shows the latter result for 2-layer networks using local openness, and then show a similar result for the degenerate case. The result for the degenerate case hold when replacing the square loss error by a general convex loss function as we will see in Colorollary 6. The proofs of the theorem and corollary stated below can be found in Appendix A.1
Theorem 5. Every degenerate critical point of problem (10) is either a global minimum or a second order saddle. If X is full row rank, then every non-degenerate critical point of problem (10) is either a global minimum or a saddle point.
Corollary 6. Let the square loss error in (10) be replaced by a general convex loss function (·). Then every degenerate critical point is either a global minimum or a second order saddle.

7

Under review as a conference paper at ICLR 2018

Baldi & Hornik (1989) and Srebro & Jaakkola (2003) show the same result when both X and Y are full row rank. Theorem 5 generalizes their results by relaxing the assumptions on Y . Another implication of this theorem is the problem of the fully observed non-symmetric matrix completion. Ge et al. (2016) studied the symmetric matrix completion problem and showed that every local minimum is global (but not the non-symmetric case). By setting X = I, the theorem above extends the results in Ge et al. (2016) to the non-symmetric case. We summarize our results in the following chart:
Critical points of Two Layer Linear Neural Network problem: (10)

Non-Degenerate

Degenerate

If X is full row rank, then by Theorem 5

If X is full row rank, then by proof of (Yun et al., 2017, Theorem 2.2)

Saddle point or Global minimum of problem: (10)

By Theorem 5
Second order saddle point of problem: (10) or Global Optimum

6 MULTI-LAYER DEEP LINEAR NEURAL NETWORK

Consider the training problem of multi-layer deep linear neural networks:

1

minimize
W

2

Wh · · · W1X

-Y

2 F

.

(12)

Here W = Wi hi=1, Wi  Rdi×di-1 are the weight matrices, X  Rd0×n is the input training data, and Y  Rdh×n is the target training data. Based on our general framework, the corresponding auxiliary optimization problem is given by

minimum
Z Rdh ×n
subject to

1 ||ZX 2

-

Y ||2F

rank(Z)  dp min0ih di

(13)

Lu & Kawaguchi (2017) attempted to prove that when X and Y are full row rank, every local min-

imum is global. However, the derivation does not constitute a formal proof of the desired result; this is because it uses Lemma 3.3 which states that given a full rank matrix M¯ with singular value decomposition (SVD) M¯ = U¯ ¯ V¯ T , for any perturbation M of M¯ , there exists U , , and V perturbations of U¯ , ¯ , and V¯ respectively, such that U V T = M is an SVD in M . This statement is not

true in general due to the follwowing counterexample provided by Stewart (1998):

Let

M¯ =

10 0 1+

and M = 1 1 .

Then the right singular vectors of M¯ are given by

V¯ =

10 01

,

while the right singular vectors of M are given by

V = 1 2

11 1 -1

.

8

Under review as a conference paper at ICLR 2018

However, this Lemma was only used to prove (Lu & Kawaguchi, 2017, Theorem 3.1) which, as we will show next, can be derived using Proposition 2 and Theorem 3. Before proceeding to the proof we define the following mapping:

Mi,j (Wi, . . . , Wj ) : {Wi, . . . , Wj }  RMi,j

{Z  Rdi×dj-1 | rank(Z)  min dl }
j-1li

with Mi,j(Wi, . . . , Wj) = Wi · · · Wj for i > j

Now let Z = WhWh-1 . . . W1 = M(Mh,p+1, Mp,1) be a rank dp matrix, we state Theorem 3.1 of Lu & Kawaguchi (2017) using our notation.
Lemma 7. If W is non-degenerate, then Mh,1(W ) = Wh · · · W1 is locally open at W .

Proof. We construct a proof by induction on h to show the desired result. When h = 2, we either have d1 < min(d2, d0) or d1  min(d2, d0). In the first case, since

d1 = rank(W¯ 2W¯ 1)  rank(W¯ 1)  d1, and d1 = rank(W¯ 2W¯ 1)  rank(W¯ 2)  d1,

then by Theorem 3, we get M2,1 is locally open at (W2, W1). In the second case, and since either

d2 = rank(W¯ 2W¯ 1)  rank(W¯ 2)  d2, or d0 = rank(W¯ 2W¯ 1)  rank(W¯ 1)  d0,

then by Proposition 2, M2,1 is locally open at (W2, W1).

Now assume the result holds for the product of h matrices Mh,1(W ), we show it is true for Mh+1,1(W ).

Since

dp = rank(W¯ h, . . . , W¯ 1)  rank(Wp+1Wp)  dp

then using Proposition 2, we get Mp+1,p is locally open at (Wp+1, Wp). So we can replace Wp+1Wp by a new matrix Zp with rank dp. Then by induction hypothesis, the product mapping Mh+1,1 = Wh+1 · · · Wp+2ZpWp-1 · · · W1 is locally open at W . Since the composition of locally
open maps is locally open, the result follows.

We now demonstrate our main results for this optimization problem which shows that under a set of necessary conditions, every critical point of problem (12) is either a saddle or a global minimum. Although the result for the non-degenerate case directly follows from (Yun et al., 2017, Theorem 2.2), we provide in Lemma 8 a more intuitive proof that uses local openness of M.
Lemma 8. Assume X is full row rank, then every non-degenerate critical point of (12) is either a saddle or a global minimum.
Proof. Suppose W = (Wh, . . . , W1) is a non-degenerate local minimum. Then it follows by Lemma 7 that Mh,1 is locally open at W . Then by Lemma 1, Z = Mh(Wh, . . . , W1) is a local optimum of problem (13) which is in fact global by (Lu & Kawaguchi, 2017, Theorem 2.2).

9

Under review as a conference paper at ICLR 2018
REFERENCES
M. Balcerzak, A. Wachowicz, and W. Wilczyn´ski. Multiplying balls in the space of continuous functions on [0, 1]. Studia Mathematica, 170:203­209, 2005.
M. Balcerzak, A. Majchrzycki, and A. Wachowicz. Openness of multiplication in some function spaces. Taiwanese J. Math, 17:1115­1126, 2013.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
E. Behrends. Products of n open subsets in the space of continuous functions on [0, 1]. Studia Mathematica, 204:73­95, 2011.
E. Behrends. Where is matrix multiplication locally open? Linear Algebra and its Applications, 517:167­176, 2017.
S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873­3881, 2016.
A. Blum and R. L. Rivest. Training a 3-node neural network is np-complete. In Advances in neural information processing systems, pp. 494­501, 1989.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015.
R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pp. 2973­2981, 2016.
I. Goodfellow and A. Courville. Deep learning. Book in preparation for MIT Press, Cambridge, 2016.
M. Hardt and T. Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
C. Horowitz. An elementary counterexample to the open mapping principle for bilinear maps. Proceedings of the American Mathematical Society, 53(2):293­294, 1975.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Y. Koren. The bellkor solution to the netflix grand prize. Netflix prize documentation, 81:1­10, 2009.
J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pp. 1246­1257, 2016.
H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.
D. Park, A. Kyrillidis, C. Caramanis, and S. Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. arXiv preprint arXiv:1609.03240, 2016.
W. Rudin. Functional analysis, mcgraw-hill series in higher mathematics. 1973.
M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape of overparameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
N. Srebro and T. Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 720­727, 2003.
G. W. Stewart. Perturbation theory for the singular value decomposition. Technical report, 1998.
R. Sun. Matrix completion via nonconvex factorization: Algorithms and theory. PhD thesis, University of Minnesota, 2015.
L. Wang, X. Zhang, and Q. Gu. A unified computational and statistical framework for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275, 2016.
10

Under review as a conference paper at ICLR 2018 B. Xie, Y. Liang, and L. Song. Diverse neural network learns true target functions. In Artificial Intelligence
and Statistics, pp. 1216­1224, 2017. C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. arXiv preprint
arXiv:1707.02444, 2017.
11

Under review as a conference paper at ICLR 2018

7 APPENDIX A.1

7.1 PROOFS OF THE THEOREM 5

Proof. The proof for the degenerate case is done by constructing a descent direction if the point is critical but not global. Let (W¯ 2, W¯ 1) be a degenerate critical point, i.e. rank(W¯ 2W¯ 1) < min(d2, d1, d0). Then, based on the dimensions of d0, d1, and d2, we have one of the following cases:
d2 < d1 then  b = 0 such that b  N W¯ 2 d0 < d1 then  b = 0 such that b  N W¯ 1T d1  d2, and d1  d0 then either W¯ 2 is rank deficient and  b = 0 such that b  N W¯ 2 or W¯ 1 is rank deficient and  b = 0 such that b  N W¯ 1T
So in all cases either N W¯ 2 = 0 or N W¯ 1T = 0. Also, let  = W¯ 2W¯ 1X - Y . If XT = 0, then by convexity of the square loss error function, the point (W¯ 2, W¯ 1) is a global minimum of (11). Else, there exists (i, j) such that Xi,:, j,: = 0. We now use first and second order optimality conditions to construct a descent direction when the current critical point is not global.

First order optimality condition: By considering perturbation in the directions A  Rd2×d1 and B  Rd1×d0 for the optimization problem

minimize
t

1 2

(W2 + tA)(W1 + tB) - Y

2 F

(14)

we obtain

AW¯ 1X + W¯ 2BX ,  = 0, A  Rd2×d1 , B  Rd1×d0

Second order optimality condition: 2 ABX,  + ||AW¯ 1X + W¯ 2BX||2F  0 A  Rd2×d1 , B  Rd1×d0

Suppose (W¯ 1, W¯ 2) is a critical point and there exists b = 0, b  N (W¯ 2). We define

b if l = i, B:,l = 0 otherwise

Al,: =

esT 0

if l = j, otherwise

where  is a scalar constant, s = argmax |bk| and es is the sth unit vector. Then, using the second
k
order optimality condition, for c = ||AW¯ 1X||2F , we get
 bs Xi,:, j,: + c  0

=0 =0

Since this is true for every value of , bs should be zero which contradicts the assumption on the choice of b. Hence N (W¯ 2) = . Similarly, suppose (W¯ 1, W¯ 2) is a critical point and exists aT = 0, aT  N (W¯ 1). Let

aT if l = j, Al,: = 0 otherwise

B:,l =

es 0

if l = i, otherwise

where  is a constant, s = argmax |ai| and es is the sth unit vector. Then, for c = ||W¯ 2BX||F2 , we
i
get  as Xi,:, j,: + c  0

=0
Using the same argument, we can show that (W¯ 2, W¯ 1) is a second order saddle point of problem (10).

We now show the result for the degenerate case. Let (W¯ 2, W¯ 1) be a non-degenerate local optimum, i.e. rank(W¯ 2W¯ 1) = min(d2, d1, d0). Then it follows by Lemma 7 that the matrix multiplication M(W¯ 2W¯ 1) is locally open at (W¯ 2, W¯ 1). Then by Lemma 1, Z = W¯ 2W¯ 1 is a local optimum of problem (13) which is in fact global by (Lu & Kawaguchi, 2017, Theorem 2.2).

12

Under review as a conference paper at ICLR 2018
7.2 PROOF OF COROLLARY 6
Proof. First order optimality condition: AW¯ 1X + W¯ 2BX,  (W¯ 2W¯ 1X - Y ) = 0 A  Rd2×d1 , B  Rd1×d0
Second order optimality condition: 2 ABX,  (W¯ 2W¯ 1X - Y ) + h AW¯ 1X, W¯ 2BX, W¯ 2W¯ 1X)  0 A  Rd2×d1 , B  Rd1×d0 where h(·) is a function that has a tensor representation. But we only need to know that it is a function of AW¯ 1X, W¯ 2BX, and W¯ 2W¯ 1X.
If  (W¯ 2W¯ 1X -Y )XT , then by convexity of (·), (W¯ 2, W¯ 1) is a global minimum. Otherwise, there exists (i, j) such that Xi,:,  (W¯ 2W¯ 1X - Y ) j,: = 0. Using the same former argument in proof of Theorem 5, we choose A and B such that h(AW¯ 1X, W¯ 2BX, W¯ 2W¯ 1X) is some constant that does not depend on , and ABX,  (W¯ 2W¯ 1X - Y ) =  Xi,:,  (W¯ 2W¯ 1X - Y ) j,: = 0.
=0
Then by proper choice of  we show that the point (W¯ 2, W¯ 1) is a second order saddle point.
8 APPENDIX A.2
8.1 PROOF OF THEOREM 3
In this section, we prove Theorem 3 which provides a complete characterization of points (X  Rm×k, Y  Rk×n) where the matrix multiplication mapping M(X, Y ) is locally open in its range RMu = {Z  Rm×n with rank(Z)  u min(m, k, n)}. Our result shows that when u = k < min(m, n), M is locally open at a point (X, Y ) if and only if the following two conditions both hold:
(AX ) :  X such that X Y = 0 and X + X is full column rank. 
and (AY ) :  Y such that XY = 0 and Y + Y is full row rank. To do so, we first show that the local openness of M(X, Y ) at (X, Y ) is equivalent to the local openness of M(U T X, Y V ) at (U T X, Y V ) where U  Rm×m and V  Rn×n are left and right singular vectors of the product XY respectively. This allows us to focus our study on the local openness of the mapping M to matrix pairs whose product is a diagonal matrix. We then show in Lemma 10 that when u = k and rX = rY , (AX ) holds if and only if (AY ) holds. Finally, we show in Proposition 13, that these conditions hold if and only if the mapping M(X, Y ) is locally open at (X, Y ). Lemma 9. Consider X  Rm×k, Y  Rk×n, and let XY = U V T with U  Rm×m, V  Rn×n, and   Rm×n be an SVD decomposition of the matrix product XY . Then, M(X, Y ) is locally open at (X, Y )  M(U T X, Y V ) is locally open at (U T X, Y V )
Proof. We first show the direction "  ". Suppose M(X, Y ) is locally open at (X, Y ), then by definition of local openness, for any given > 0, there exists  > 0 such that
IB(XY )  RMu  M IB (X), IB (Y ) = {(X + X )(Y + Y ) | X  , Y  } (15) We now show that
IB(U T XY V )  RuM  M IB (U T X), IB (Y V ) . Consider ~  IB(U T XY V )  RuM, i.e. ~ = U T XY V + R with rank(~ ) u and R  . Since U and V are orthonormal left and right singular vectors, we get U ~ V T = XY + U RV T with rank(U ~ V T )=rank(~ ) u and U RV T = R  . According to (15), this translates to the following:
U ~ V T  IB(XY )  RuM  {(X + X )(Y + Y ) | X  , Y  }
13

Under review as a conference paper at ICLR 2018

which implies, ~  {(U T X + U T X )(Y V + Y V ) | U T X  , Y V  } = {(U T X + U T X )(Y V + Y V ) | X  , Y  }
Since ~ was arbitrarily chosen, we get IB(U T XY V )  RuM  M IB (U T X), IB (Y V ) .

Proving the converse direction "  " is similar. Suppose M(U T X, Y V ) is locally open at (U T X, Y V ), then by definition of local openness, for any given > 0, there exists  > 0 such
that

IB(U T XY V )RuM  M IB (U T X), IB (Y V ) = {(U T X+X )(Y V +Y ) | X

We now show that

IB(XY )  RMu  M IB (X), IB (Y ) .

, Y } (16)

Consider Z~  IB(XY )  RuM, i.e. Z~ = XY + R with rank(Z~) u and R  . Since U and V are orthonormal left and right singular vectors, we get U T Z~V = U T XY V + U T RV with rank(U T Z~V )=rank(Z~) u and U T RV = R  . According to (16), this translates to the
following:
U T Z~V  IB(U T XY V )  RuM  M IB (U T X), IB (Y V )

which implies,

Z~  {(X + U X )(Y + Y V T ) | U X  , Y V T  } = {(X + U X )(Y + Y V T ) | X  , Y  }

Since Z~ was arbitrarily chosen, we get IB(XY )  RuM  M IB (X), IB (Y ) , which completes the proof.

We next derive some facts that we will later need to prove the main result in Theorem 3. Lemma 10. Consider X  Rm×k, Y  Rk×n, k < min(m, n) with rank(X) = rank(Y ) = r, then
i) (AY ):  Y such that XY = 0 and Y + Y is full row rank  dim N (X)  C(Y ) = 0.
ii) (AX ):  X such that X Y = 0 and X +X is full column rank  dim N ( Y T )  C(XT ) = 0.
iii) dim N (X)  C(Y ) = 0  dim N (Y T )  C(XT ) = 0.

Proof. i) We first show the direction "  ". Consider X  Rm×k, Y  Rk×n both rank r matrices. Suppose (AY ) holds, then

C(Y )  N (X)  rank(Y )  dim N (X) = k - r.

(17)

Also,

k = rank(Y + Y )  rank(Y ) + rank(Y ) = r + rank(Y )

(18)

From inequalities (17) and (18), we get

k - r  rank(Y )  k - r  rank(Y ) = k - r

Note that dim C(Y ) = dim N (X) and C( Y )  N ( X ), which implies that C( Y ) = N ( X ).

Then since rank(Y + Y ) = rank(Y ) + rank(Y ), we get  = C( Y )  C( Y ) = N ( X )  C( Y )  dim N (X)  C( Y ) = 0.

14

Under review as a conference paper at ICLR 2018

We now show the other direction "  ". Without loss of generality, let Y = Y0k×rAr×n-r Y0k×r where columns of Y0 are linearly independent and let Y = y1, . . . , yk-r, 0, . . . , 0  Rk×n be a rank k - r matrix where yi are unit basis of N ( X ) which yields C( Y ) = N ( X ). Then since
dim N (X)  C(Y ) = 0, rank(Y + Y ) = k for some value of . This completes the proof.

ii) Note that by setting X = Y T and Y = XT , the same proof can be used to show (ii).

iii) Notice that

dim span N ( X )  C( Y ) = dim N ( X ) + dim C( Y ) - dim N ( X )  C( Y )

Thus,

= k - r + r - dim N ( X )  C( Y ) = k - dim N ( X )  C( Y )

dim N ( X )  C( Y ) = 0  dim span N ( X )  C( Y ) < k

  a such that a  C( Y ), and a  N ( X )   a such that a  N ( Y T ), and a  C( XT )  dim N ( Y T  C( XT ) = 0.

Interestingly, Lemma 10 shows that when u = k < min(m, n) and X and Y have the same rank, (AX ) and (AY ) are equivalent. The next result shows that if these conditions hold, then rX is equal to rY , and the last n - r rows of U T X and last n - r columns of Y V , where U  Rm×m and V  Rn×n are the left and right singular vectors of the product XY respectively, are all zeros.

Lemma 11. Consider X  Rm×k, Y  Rk×n with k < min(m, n), and let XY = U V T with U  Rm×m, and V  Rn×n, and   Rm×n be an SVD decomposition of the product XY . If

(AX ) :  X such that X Y = 0 and X + X is full column rank. 
and
(AY ) :  Y such that XY = 0 and Y + Y is full row rank.

then

rX = rY r,

Y V :,r+1:n = 0, and U T X r+1:n,: = 0

Proof. Suppose that (AY ) holds, then C( Y )  N ( X )  rank(Y )  dim N ( X ) = k - rX .
Also, k = rank(Y + Y )  rank(Y ) + rank(Y ) = rY + rank(Y )
From inequalities (19) and (20), we get k - rY  rank(Y )  k - rX  rY  rX
Suppose that (AX ) holds, then C( XT )  N ( Y T )  rank(X )  dim N ( Y T ) = k - rY .
Also, k = rank(X + X )  rank(X) + rank(X ) = rX + rank(X )
From inequalities (22) and (23), we get k - rX  rank(X )  k - rY  rX  rY

(19) (20) (21) (22) (23) (24)

15

Under review as a conference paper at ICLR 2018

From inequalities (21) and (24), we get rX = rY , which by Lemma 10 implies dim N ( X ) 
C( Y ) = 0. Let r = rX = rY be the rank of the matrices X and Y , then the rank of the product matrix XY is less then or equal to r. Then it directly follows from the SVD decomposition of the matrix XY , that U T X Y V ):,r+1:n = :,r+1:n = 0, or equivalently XY V:,r+1:n = 0. On the other
hand, since C Y V:,r+1:n  C Y and N X  C Y = , we conclude that Y V ):,r+1:n = 0. Similarly, one can show that U T X)r+1:n,: = 0.

We now state and prove the following lemma which will be used latter in the proof of Proposition 13. Lemma 12. Consider a matrix V  Rm×n with rank(V ) = r. Then there exists a basis B = {i1, . . . , ir} and a matrix A  Rr×(m-r) with bounded norm A  m2m, such that
VBc = AVB
where VB is a matrix with rows {Vi,:}iB and VBc is a matrix with rows {Vi,:}iBc

Proof. To ease the notation, we denote the ith row of V by vi. We use induction on m to show that there exists a basis B = {i1, . . . , ir} such that  j  Bc,

vj = aj,ivi
iB

with |aj,i|  2m  i  B.

Base step m = r + 1:

Without consider

loss generality assume B vr+1 = 0. By the property

= of

{1, . . basis,

. , r}. there

Since the exists ar+1

case =0

of vr+1 such that

=0 vr+1

trivially holds, we

=

r i=1

ar+1,ivi

.

Let i = argmin|ar+1,i|. We only need to consider the case when |ar+1,i |  2r+1. In that case,
iB

vi

=

1 ar+1,i

vr+1

r
-
i=1; i=i

ar+1,i ar+1,i

vi

a¯r+1,r+1

a¯r+1,i

= a¯r+1,ivi
iB

where B = B - {i} + {r + 1}

Since a¯r+1,i  1 < 2r+1, the induction step holds.

Assume it is true for m > r, we show the result for m + 1. Without loss of generality assume B = {1, . . . , r}. By induction hypothesis,

r
vj = aj,i vi
i=1

with |aj,i|  2m  j = {r + 1, . . . , m}; i  B

Since exists

the case of am+1 = 0

vm+1 = 0 trivially such that vm+1 =

holds,
r i=1

we consider vm+1 = 0. By the property of basis, there am+1,i vi. Let i = argmin|am+1,i|. We only need to

iB

consider the case when |am+1,i |  2m+1. In that case,

vi

=

1 am+1,i

vm+1

-

r i=1; i=i

am+1,i am+1,i

vi

a¯m+1,m+1

a¯m+1,i

= a¯m+1,ivi
iB

16

Under review as a conference paper at ICLR 2018

where B = B - {i} + {m + 1} and |a¯m+1,i|  1  i  B. For all j  {r + 1, . . . , m}

r

vj =

aj,i vi + aj,i vi

i=1; i=i

=

r
aj,i
i=1; i=i

vi

+

aj,i am+1,i

vm+1

-

r i=1; i=i

am+1,i aj,i am+1,i

vi

r
=
i=1; i=i

aj,i

-

aj,i am+1,i am+1,i

vi

+

aj,i am+1,i

vm+1

a¯j,i

a¯j,m+1

= a¯j,ivi

iB

It remains to show that |a¯j,i|  2m+1 for all i  B, j  Bc. Lets first consider i  B - {m + 1} and j  Bc.

|a¯j,i|  |aj,i| +

aj,i

am+1,i am+1,i

 2m + 2m am+1,i am+1,i

by induction hypothesis

 2m+1

For i = m + 1, |a¯j,m+1| =

aj,i am+1,i

 2m. This completes the induction proof.

By setting the rows of A to a¯j jB, it directly follows that VBc = AVB with A  |B||Bc|2m  m22m = m2m

Note that the bound we got on A is not strict, but we only need to use the fact that it is bounded. We are now ready to state and prove Proposition 13 which is the main block in Theorem 3. Proposition 13. Consider X  Rm×k, Y  Rk×n, k < min(m, n) then

(AX ) :  X such that X Y = 0 and X + X is full column rank.

M(X, Y ) is locally open at (X, Y ) 

and

(AY ) :  Y such that XY = 0 and Y + Y is full row rank.

Proof. Let U V T be the singular value decomposition of Z with U  Rm×m, V  Rn×n, and   Rm×n, and define X¯ = U T X, Y¯ = Y V . Then using Lemma 9, Z = M(X, Y ) is locally open at (X, Y ) if and only if  = M(X¯ , Y¯ ) is locally open at (X¯ , Y¯ ). Notice that XY = 0  X¯ Y¯ = U T XY V = 0, and Y + Y is full row rank  Y¯ + Y¯ is full row rank A similar equivalence relation holds for the condition on X and X¯ .

We now choosing

prove X¯ =

the Y¯

direction "  = 0, we show it

". is

While the result true when at least

obviously one of the

holds when rX¯ matrices is rank

= rY¯ = deficient.

k

by

X¯ is full column rank and Y¯ is rank deficient: Suppose M(X¯ , Y¯ ) is locally open at (X¯ , Y¯ ), it follows from the definition of openness that M(X¯ , Y¯1) is locally open at (X¯ , Y¯1) where Y¯1 = Y¯:,1:k is the first k columns of Y¯ . Since the range of the mapping M at (X¯ , Y¯1) is RkM = Rm×k, using Proposition 2, we get that either
 X¯ such that X¯ Y¯1 = 0 and X¯ + X¯ is full row rank. 
or
 Y¯ 1 such that X¯ Y¯ 1 = 0 and Y¯1 + Y¯ 1 is full column rank.

17

Under review as a conference paper at ICLR 2018

But since X¯  Rm×k and m > k, it is impossible to have X¯ such that X¯ + X¯ full row rank. Moreover, since X¯ is full rank, for any Y¯ 1 satisfying X¯ Y¯ 1 = 0 we have Y¯ 1 = 0  Y¯1 + Y¯ 1 is not full column rank. This contradicts the conditions of local optimality of M(X¯ , Y¯1).
X¯ is rank deficient and Y¯1 is full row rank: The proof of this case if similar to the one above, just substitute X¯ = Y¯ T and Y¯ = X¯ T .
X¯ and Y¯ are both rank deficient: Suppose M(X¯ , Y¯ ) is locally open at (X¯ , Y¯ ), then it directly follows that M(X¯ , Y¯1) is locally open at (X¯ , Y¯1). By Proposition 2, and since there does not exist X¯ such that X¯ + X¯ is full row rank, there exists Y¯ 1 such that X¯ Y¯ 1 = 0 and Y¯1 + Y¯ 1 is full rank.
By letting Y¯ = Y¯ 1 0 , we satisfy the desired condition. Similarly, since M(Y¯ T , X¯ T ) is locally open at (Y¯ T , X¯ T ), the same proof can be used to show that there exists X¯ such that X¯ + X¯ is full column rank, and X¯ Y¯ = 0.
Note that the matrix multiplication is not locally open at (X¯ , Y¯ ) when only one of the matrices is rank deficient.

We now prove the converse direction "  ". Suppose (AX ) and (AY ) hold,
 X such that X Y = 0 and X + X is full column rank. 
and  Y such that XY = 0 and Y + Y is full row rank.
then by Lemma 11, we get that rX¯ = rY¯ r, and the last n - r columns of Y¯ are all zeros. We need to show that for any given > 0, there exists  > 0, such that
IB X¯ Y¯  RuM  M IB X¯ , IB Y¯

Let  = X¯ Y¯ = [ :,1:r 0 ] be a rank r matrix. We show that for any ~  IB X¯ Y¯  RuM, which is a permutation of , ~  IB X¯ Y¯  RMu . Without loss of generality, and by permuting the columns of ~ if necessary, ~ can be expressed as

~ =

:,1:r + R1
m×r

R2
m×(k-r)

(:,1:r + R1)A1 + R2A2
m×(n-k)

or its transpose.

Where A1  Rr×(n-k), A2  R(k-r)×(n-k) assure that the rank condition is satisfied, i.e. rank(~ ) k. Moreover, ~ -    implies that the perturbation matrix
R = R1 R2 (:,1:r + R1)A1 + R2A2 has norm less than or equal , i.e. R  .

Given that there exist Y¯ such that X¯ Y¯ = 0 and Y¯ + Y¯ is full row rank, and knowing that the last n - r columns of Y¯ are all zeros, there exist a perturbation matrix


k×r

k×(k-r) 

Y¯ 1   0 y1 . . . , yk-r 

18

Under review as a conference paper at ICLR 2018

where yi are unit basis of Y¯ such that Y¯1 + Y¯ 1 is a full rank k × k matrix, and X¯ Y¯ 1 = 0. Note that there exist k - r unit basis vectors for Y¯ since the rank(Y¯ )  rank(Y¯ + Y¯ ) - rank(Y¯ ) = k - r.

Let

X¯0 = R1 R2 (Y¯1 + Y¯ 1)-1

Y¯0 = Y¯ 1 Y¯1 :,1:rA1 + Y¯ 1A2

Then

(X¯ + X¯0)(Y¯ + Y¯0) = X¯ Y¯ + R1 R2 (X¯ + X¯0) (Y¯1 :,1:rA1 + Y¯ 1A2

= X¯ Y¯ + R = ~ (25)

To complete the proof, it remains to show that X¯0  ~  M IB X¯ , IB Y¯ .

and Y¯0 

which implies

Perturbation Bound Let r~  r be the rank of ~ , then by lemma 12 and by possibly permuting the columns of ~ , it can be expressed as
~ = ~ 1 ~ 1A¯
where ~ 1  Rm×r~ is full column rank, and A¯ has a bounded norm ( A¯  n2n). Notice that given X¯0 and Y¯0 that satisfy (25), permuting the columns of ~ correspond to permuting the columns of (Y¯ + Y¯0). If we can show that the first r columns are not among the permuted ones, then using the fact that Y¯ has only its first r columns non-zero, it follows that this permutation of the columns of ~ corresponds to the same permutation of the columns of Y¯ + Y¯0 which in turn corresponds to the same permutation of the columns of Y¯0. Moreover, if the first r columns are not among the permuted ones, then without loss of generality we can express

~ =

:,1:r + R1
m×r

R2
m×(k-r)

(:,1:k + R1)A¯1 + R2A¯2
m×(n-k)

where

A¯1 A¯2

= A¯ which has a bounded norm.

We now show that the first r columns of ~ before permutation :,1:r + R1 :,1:r  ~ 1. Assume the contrary, then there exists at least one column call it :,j + R1 j,: that is not in ~ 1, which implies :,j + R1 j,:  ~ 1A¯. Without loss of generality let :,j + R1 j,: = ~ 1A¯:,1. It follows that

j,j + R1 j,j = (~ 1)j,:A¯:,1

But since j,j + R1 j,j is a non-zero perturbed singular value, and since elements of (~ 1)j,: are all of order , then by choosing  sufficiently small, we get |A¯| > n2n which contradicts the statement of lemma 12.

We now get a bound on Y0 . Since the norm of A¯ is bounded, then the norm of A¯2 is bounded by some constant, denote it by K.

  (:,1:r + R1)A¯1 + R2A¯2  (:,1:r + R1)A¯1 - R2A2  (:,1:r + R1)A¯1 - K  min A¯1 - K

where min is the minimum singular value of the full column rank matrix :,1:r + R1 which is bounded away from zero. Then it follows that

A¯1

(1 + K) 
min

19

Under review as a conference paper at ICLR 2018

Then,

Y0 2  Y¯ 1 2 + Y¯1 2 A¯1 2 + Y¯ 1 2 A¯2 2
2
 2 1 + 1 + K + K2 min

(by triangular inequality and Cauchy Shwarz)

For a given > 0, choose   1 + max

(Y¯1 + Y¯ 1)-1 ,

1 + K2 +

1+K min

, then we get
2

Y0 < and

||X¯0||



||R|| ||(Y¯1

+

Y¯ 1)-1||



||(Y

+

Y

)-1||



(||(Y + Y )-1|| ||(Y + Y )-1|| + 1



Note that  is of order . This completes the proof.

We now use Proposition 13, Lemma 10 and Lemma 11 to complete the proof of Theorem 3
Proof. Consider X  Rm×k, and Y  Rk×n with u = k  (m.n). If rX = rY , it follows from Lemma 10 that (AX ) and (AY ) are equivalent. If the two conditions hold, then by Proposition 13, the mapping M is locally open at (X, Y ).
But if rX = rY , then, by Lemma 11, (AX ) and (AY ) cannot both hold which implies, by Theorem 3, that M is not locally open at (X, Y ).This completes the proof.

20

