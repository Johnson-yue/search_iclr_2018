Under review as a conference paper at ICLR 2018
THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES
Anonymous authors Paper under double-blind review
ABSTRACT
It is well known that it is possible to construct "adversarial examples" for neural networks: inputs which are misclassified by the network yet indistinguishable from true data. We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples. We demonstrate this robustness with experiments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that models with thermometer-encoded inputs consistently have higher accuracy on adversarial examples, without decreasing generalization. State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10. We explore the properties of these networks, providing evidence that thermometer encodings help neural networks to find more-non-linear decision boundaries.
1 INTRODUCTION AND RELATED WORK
Adversarial examples are inputs to machine learning models that are intentionally designed to cause the model to produce an incorrect output. The term was introduced by Szegedy et al. (2014) in the context of neural networks for computer vision. In the context of spam and malware detection, such inputs have been studied earlier under the name evasion attacks (Biggio et al., 2013). Adversarial examples are interesting from a scientific perspective, because they demonstrate that even machine learning models that have superhuman performance on I.I.D. test sets fail catastrophically on inputs that are modified even slightly by an adversary. Adversarial examples also raise concerns in the emerging field of machine learning security because malicious attackers could use adversarial examples to cause undesired behavior (Papernot et al., 2016).
Unfortunately, there is not yet any known strong defense against adversarial examples. Adversarial examples that fool one model often fool another model, even if the two models are trained on different training examples (corresponding to the same task) or have different architectures (Szegedy et al., 2014), so an attacker can fool a model without access to it. Attackers can improve their success rate by sending inputs to a model, observing its output, and fitting their own own copy of the model to the observed input-output pairs (Papernot et al., 2016). Attackers can also improve their success rate by searching for adversarial examples that fool multiple different models--such adversarial examples are then much more likely to fool the unknown target model (Liu et al., 2016). Szegedy et al. (2014) proposed to defend the model using adversarial training (training on adversarial examples as well as regular examples) but it was not feasible to generate enough adversarial examples in the inner loop of the training process for the method to be effective at the time. Szegedy et al. (2014) used a large number of iterations of L-BFGS to produce their adversarial examples. Goodfellow et al. (2014) developed the fast gradient sign method (FGSM) of generating adversarial examples and demonstrated that adversarial training is effective for reducing the error rate on adversarial examples. A major difficulty of adversarial training is that it tends to overfit to the method of adversarial example generation used at training time. For example, models trained to resist FGSM adversarial examples usually fail to resist L-BFGS adversarial examples. Kurakin et al. (2016) introduced the basic iterative method (BIM) which lies between FGSM and L-BFGS on a curve trading speed for effectiveness (the BIM consists of running FGSM for a medium number of iterations). Adversarial training using BIM still overfits to the BIM, unfortunately, and different iterative methods can still successfully attack the model. Recently, Madry et al. (2017) showed that adversarial training using adversarial examples created by adding random noise before running BIM results in a model that is
1

Under review as a conference paper at ICLR 2018

highly robust against all known attacks on the MNIST dataset. However, it is less effective on more complex datasets, such as CIFAR. A strategy for training networks which are robust to adversarial attacks across all contexts is still unknown. In this work, we demonstrate that thermometer code discretization and one-hot code discretization of real-valued inputs to a model significantly improves its robustness to adversarial attack, advancing the state of the art in this field.

2 INPUT DISCRETIZATION

We propose to break the linear extrapolation behavior of machine learning models by preprocessing the input with an extremely nonlinear function. This function must still permit the machine learning model to function successfully on naturally occurring inputs. The recent success of the PixelRNN model (Oord et al., 2016) has demonstrated that one-hot discrete codes for 256 possible values of color pixels are effective representations for input data. Other extremely nonlinear functions may also defend against adversarial examples, but we focused attention on vector-valued discrete encoding as our nonlinear function because of the evidence from PixelRNN that it would support successful machine learning.
Images are often encoded as a 3D tensor of integers in the range [0, 255]. The tensor's three dimensions correspond to the image's height, width, and color channels (e.g. three for RGB, one for greyscale). Each value represents an intensity value for a given color at a given horizontal/vertical position. For classification tasks, these values are typically normalized to floating-point approximations in the range (0, 1). Input discretization refers to the process of separating these continuousvalued pixel inputs into a set of non-overlapping buckets, which are each mapped to a fixed binary vector.
Past work, for example depth-color-squeezing (Xu et al., 2017), has explored what we will refer to as quantization of inputs as a potential defense against adversarial examples. In that approach, each pixel value is mapped to the low-bit version of its original value, which is a fixed scalar. The key novel aspect of our approach is that rather than replacing a real number with a number of low bit depth, we replace each real number with a binary vector. Different values of the real number activate different bits of the input vector. Multiplying the input vector by the network's weights thus performs an operation similar to an embedding lookup in a language model, so different input values actually use different parameters of the network. To avoid confusion, we will consistently refer to scalar-toscalar precision reduction as quantization and scalar-to-vector encoding schemes as discretization throughout this work. A comparison of these techniques can be seen in Table 1. Note that, unlike depth-color-squeezing, discretization makes a meaningful change to the model, even when it is configured to use enough discretization levels to avoid losing any information from a traditionally formatted computer vision training set; discretizing each pixel to 256 levels will preserve all of the information contained in the original image. Discretization defends against adversarial examples by changing which parameters of the model are used, and may also discard information if the number of discretization levels is low; quantization can only defend the model by discarding information.

Real-valued 0.13 0.66 0.92

Quantized 0.15 0.65 0.95

Discretized (one-hot) [0100000000] [0000001000] [0000000001]

Discretized (thermometer) [0111111111] [0000001111] [0000000001]

Table 1: Examples mapping from continuous-valued inputs to quantized inputs, one-hot codes, and thermometer codes, with ten evenly-spaced levels.

2.1 DISCRETIZATION AS A DEFENSE
In Goodfellow et al. (2014), the authors provide evidence that several network architectures, including LSTMs (Hochreiter & Schmidhuber, 1997), sigmoid networks (Han & Moraga, 1995), and maxout networks (Goodfellow et al., 2013), are vulnerable to adversarial examples due to the empirical fact that, when trained, the loss function of these networks tends to be highly linear with respect to its inputs.
2

Under review as a conference paper at ICLR 2018

We briefly recall the reasoning of Goodfellow et al. (2014). Assume that we have a logistic regressor with weight matrix w. Consider an image x  Rn which is perturbed into x = x +  by some noise
 such that     for some . The probability that the model assigns to the true class is equal to:

L(x) = (w x) = (w (x + )) = (w x + w )

If the perturbation  is adversarial, such as in the case where i :=  · sign

L(x) xi

, then the input

to the sigmoid is increased by  · n. If n is large, as is typically the case in images and other high-

dimensional spaces of interest, this linearity implies that even imperceptibly small values of  can

have a large effect on the model's prediction, making the model vulnerable to adversarial attacks.

Though neural networks in principle have the capacity to represent highly nonlinear functions, networks trained via stochastic gradient descent on real-world datasets tend to converge to mostly-linear solutions. This is illustrated in the empirical studies conducted by Goodfellow et al. (2014). One hypothesis proposed to explain this phenomenon is that the nonlinearities typically used in networks are either piecewise linear, like ReLUs, or approximately linear in the parts of their domain in which training takes place, like the sigmoid function.

One potential solution to this problem is to use more non-linear activation functions, such as quadratic or RBF units. Indeed, it was shown by Goodfellow et al. (2014) that such units were more resistant to adversarial perturbations of their inputs. However, these units are difficult to train, and the resulting models do not generalize very well (Goodfellow et al., 2014), sacrificing accuracy on clean examples. As an alternative to introducing highly non-linear activation functions in the network, we propose applying a non-differentiable and non-linear transformation (discretization) to the input, before passing it into the model. A comparison of the input to the model under various regimes can be seen in Figure 1, highlighting the strong non-linearity of discretization techniques.

(a) One hot encoding

(b) Thermometer encoding

Figure 1: Comparison of regular inputs, quantized inputs, and discretized inputs (16 levels, projected to one dimension) on MNIST, adversarially trained with  = 0.3. The x-axis represents the true pixel value of the image, and the y-axis represents the value that is passed as input to the network after the input transformation has been applied. For real-valued inputs, the inputs to the network are affected linearly by perturbations to the input. Quantized inputs are also affected approximately linearly by perturbations where  is greater than the bucket width. Discretizing the input, and then using learned weights to project the discretized value back to a single scalar, we see that the model has learned a highly non-linear function to represent the input in a fashion that is effective for resisting the adversarial perturbations it has seen. When starting at the most common pixel-values for MNIST, 0 and 1, any perturbation of the pixels (where   0.3) has barely any effect on the input to the network.

2.2 TYPES OF DISCRETIZATION
In this work we consider two approaches to constructing discretized representations f (x) of the input image x. Assume for the sake of simplicity that the entries of x take values in the continuous domain [0, 1].
3

Under review as a conference paper at ICLR 2018

We first describe a quantization function b. Choose 0 < b1 < b2 < · · · < bk = 1 in some fashion.

(In

this

work,

we

simply

divide

the

domain

evenly,

i.e.

bi

=

i k

.)

For

a

real

number





[0, 1]

define

b() to be the largest index   {1, . . . , k} such that   b.

2.2.1 ONE-HOT ENCODINGS
For an index j  {1, . . . , k} let (j)  Rk be the indicator or one-hot vector of j, i.e.,
1 if l = j (j)l = 0 otherwise.
The discretization function is defined pixel-wise for a pixel i  {1, . . . , n} as:
fonehot(xi) =  (b(xi)) .
One-hot encodings are simple to compute and understand, and are often used when it is necessary to represent a categorical variable in a neural network. However, one-hot encodings are not well suited for representing categorical variables with an interpretation of distance between them. Note that the distance information between two pixels xi and xj is lost by applying the transformation fonehot; for pixels i, j, k whenever b(xi) = b(xj) = b(xk), we see:
 (b(xi)) - (b(xj)) 2 = (b(xk)) - (b(xj)) 2 = 2. In the case of pixel values, this is not a good inductive bias, as there is a clear reason to believe that neighboring buckets are more similar to each other than distant buckets.

2.2.2 THERMOMETER ENCODINGS

In order to discretize the input image x without losing the relative distance information, we propose thermometer encodings. For an index j  {1, . . . , k}, let  (j)  Rk be the thermometer vector
defined as

 (j)l =

1 0

if l  j otherwise.

Then the discretization function f is defined pixel-wise for a pixel i  {1, . . . , n} as:
ftherm(x)i =  (b(xi)) = C(fonehot(xi))
l
where C is the cumulative sum function, C(c)l = cl.
j=0
Note that the thermometer encoding preserves pairwise distance information, i.e., for pixels i, j, k if b(xi) = b(xj) = b(xk) and |xi - xj| < |xk - xj| then
 (b(xi)) -  (b(xj)) 2 <  (b(xk)) -  (b(xj)) 2 .

2.3 WHITE-BOX ATTACKS ON DISCRETIZED INPUTS

Discretizing the input makes it difficult to attack the model with standard white-box attack algorithms, such as FGSM (Goodfellow et al., 2014) and PGD (Madry et al., 2017), since it is impossible to backpropagate through our discretization function to determine how to adversarially modify the model's input. In this section, we describe two novel iterative attacks which allow us to construct adversarial examples for networks trained on discretized inputs.
Constructing white-box attacks on discretized inputs serves two primary purposes. First, it allows us to more completely evaluate whether the model is robust to all adversarial attacks, as white-box attacks are typically more powerful than their black-box counterparts. Secondly, adversarial training is typically performed in a white-box fashion, and so in order to utilize and properly compare against the adversarial training techniques of Madry et al. (2017), it is important to have strong white-box attacks.
For ease of presentation, we will describe the attacks assuming that f : R  Rk discretizes inputs into thermometer encodings; in order to attack one-hot encodings, simply replace all instances of

4

Under review as a conference paper at ICLR 2018

ftherm with fonehot,  with , and C with the identity function I. We represent the adversarial image after t steps of the attack as zt, where the value of the ith pixel is zit.
The first attack, Discrete Gradient Ascent (DGA), follows the direction of the gradient of the loss with respect to f (x), but is constrained at every step to be a discretized vector. If we have discretized the input image into k-dimensional vectors using the one-hot encoding, this corresponds to moving to a vertex of the simplex (k)n at every step. The second attack, Logit-Space Projected Gradient Ascent (LS-PGA), relaxes this assumption, allowing intermediate iterates to be in the interior of the simplex. The final adversarial image is obtained by projecting the final point back to the nearest vertex of the simplex.
Note that if the number of attack steps is 1, then the two attacks are equivalent; however, for larger numbers of attack steps, LS-PGA is a generalization of DGA.

2.3.1 DISCRETE GRADIENT ASCENT (DGA)

Following PGD (Madry et al., 2017), we initialize DGA by placing each pixel into a random bucket that is within  of the pixel's true value. At each step of the attack, we look at all buckets that are within  of the true value, and select the bucket that is likely to do the most `harm', as estimated by the gradient of setting that bucket's indicator variable to 1, with respect to the model's loss at the previous step.

zi0 = ftherm(xi + U (-, ))

harm(zit)l =

(zit -  (l))

·

 L(z t )  zit

0

if (-    ) otherwise.

s.t.

b(xi + ) = l

zit+1 =  arg max harm zit
Because the outcome of this optimization procedure will vary depending on the initial random perturbation, we suggest strengthening the attack by re-running it several times and using the perturbation with the greatest loss. The pseudo-code for the DGA attack is given in Section B of the appendix.

2.3.2 LOGIT-SPACE PROJECTED GRADIENT ASCENT (LS-PGA)

To perform LS-PGA, we soften the discrete encodings into continuous relaxations, and then perform standard Projected Gradient Ascent (PGA) on these relaxed values. We represent the distribution over embeddings as a softmax over logits u, each corresponding to the unnormalized log-weight of a specific bucket's embedding. To improve the attack, we scale the logits with temperature T , allowing us to trade off between how closely our softmax approximates a true one-hot distribution and how much gradient signal the logits receive. At each step of a multi-step attack, we anneal this value via exponential decay with rate .

zit = C



uti Tt

zifinal =  arg max uifinal T t = T t-1 · 

We initialize each of the logits randomly with values sampled from a standard normal distribution. At each step, we ensure that the model does not assign any probability to buckets which are not within  of the true value by fixing the logits to be -. The model's loss is a continuous function of the logits, so we can simply utilize attacks designed for continuous-valued inputs, in this case PGA with step-size .

u0i =

N (0; 1) -

if (-    ) otherwise.

s.t.

b(xi + ) = l

uit+1 l =

(uit)l +  · -

 L(z t ) uit l

if (-    ) otherwise.

s.t.

b(xi + ) = l

5

Under review as a conference paper at ICLR 2018
Because the outcome of this optimization procedure will vary depending on the initial perturbation, we suggest strengthening the attack by re-running it several times and using the perturbation with the greatest loss. The pseudo-code for the LS-PGA attack is given in Section B of the appendix.
3 EXPERIMENTS
We compare models trained with input discretization to state-of-the-art adversarial defenses on a variety of datasets. We match the experimental setup of the prior literature as closely as possible. Rows labeled with "Vanilla (Madry)" give the numbers reported in Madry et al. (2017); other rows contain results of our own experiments, with "Vanilla" containing a direct replication. For our MNIST experiments, we use a convolutional network; for CIFAR-10, CIFAR-100, and SVHN we use a Wide ResNet (Zagoruyko & Komodakis, 2016). We use a network of depth 30 for the CIFAR10 and CIFAR-100 datasets, while for SVHN we use a network of depth 15. The width factor of all the Wide ResNets is set to k = 4. 1 Unless otherwise specified, all quantized and discretized models use 16 levels.
We found that in all cases, LS-PGA was strictly more powerful than DGA, so all attacks on discretized models use LS-PGA with  = 0.01,  = 1.2, and 1 random restart. To be consistent with Madry et al. (2017), we describe attacks in terms of the maximum -norm of the attack, . All MNIST experiments used  = 0.3 and 40 steps for iterative attacks; experiments on CIFAR used  = 0.031 and 7 steps for iterative attacks; experiments on SVHN used  = 0.047 and 10 steps for iterative attacks. These settings were used for adversarial training, white-box attacks, and blackbox attacks. Figure 3 plots the effectiveness of the iterated PGD/LS-PGA attacks on vanilla and discretized models for MNIST and shows that increasing the number of iterations beyond 40 would have no effect on the performance of the model on -bounded adversarial examples for MNIST.
In Madry et al. (2017), adversarially-trained models are trained using exclusively adversarial inputs. This led to a small but noticeable loss in accuracy on clean examples, dropping from 99.2% to 98.8% on MNIST and from 95.2% to 87.3% on CIFAR-10 in return for more robustness towards adversarial examples. Past work has also sometimes performed adversarial training on batches composed of half clean examples and half adversarial examples (Goodfellow et al., 2014; Cisse et al., 2017). To be consistent with Madry et al. (2017), we list experiments on models trained only on adversarial inputs in the main paper; additional experiments on a mix of clean and adversarial inputs can be found in the appendix.
We also run experiments exploring the model's relationship with the number of distinct levels to which we quantize the input before discretizing it, and exploring various settings of hyperparameters for LS-PGA.
4 RESULTS
Our adversarially-trained baseline models were able to approximately replicate the results of Madry et al. (2017). On all datasets, discretizing the inputs of the network dramatically improves resistance to adversarial examples, while barely sacrificing any accuracy on clean examples. Quantized models also beat the baseline, but with lower accuracy on clean examples. Discretization via thermometer encodings outperformed one-hot encodings in most settings. See Tables 2,3,4 and 5 for results on MNIST and CIFAR-10. Additional results on CIFAR-100 and SVHN are included in the appendix.
In Figures 2 and 5 (located in appendix), we plot the test-set accuracy across training timesteps for various adversarially trained models on the SVHN and CIFAR-10 datasets, and observe that the discretized models become robust against adversarial examples more quickly.
5 DISCUSSION
In Goodfellow et al. (2014), the seeming linearity of deep neural networks was shown by visualizing the networks in several different ways. To test our hypothesis that discretization breaks some of this
1A full list of hyperparameters can be found in the appendix. Source code is available at http://anonymized
6

Under review as a conference paper at ICLR 2018

Adv. train Clean

Model
Vanilla (Madry) Vanilla
Quantized One-hot Thermometer
Vanilla (Madry) Vanilla
Quantized One-hot Thermometer

Clean
99.20 99.30 99.19 99.13 99.20
98.80 98.67 98.75 98.61 99.03

FGSM
6.40 0.19 1.10
0 0
95.60 96.17 96.29 96.22 95.84

PGD/LS-PGA
0 0 0 0
93.20 93.30 94.23 94.30 94.02

Table 2: Comparison of adversarial robustness to white-box attacks on MNIST .

Adv. train Clean

XTXarXgeXt XXSoXurXceX Vanilla
Quantized One-hot Thermometer
Vanilla (Madry) Quantized Vanilla One-hot
Thermometer

Vanilla
2.04 39.22 14.57 41.12
97.65 97.62 97.78 98.07

Clean One-hot Thermometer

36.02 32.39 6.91 14.30

24.58 25.63 8.11 10.98

98.16 98.05 98.48 98.75

97.14 97.06 97.87 98.02

Vanilla
3.48 75.02 39.02 61.84
96.0 95.27 95.43 96.87 97.05

Adv. train One-hot Thermometer

80.44 75.92 39.60 59.16

57.69 52.32 18.02 32.93

95.31 95.38 96.60 96.88

96.53 96.23 96.87 97.13

Table 3: Comparison of adversarial robustness to black-box attacks on MNIST .

Adv. train Clean

Model
Vanilla (Madry) Vanilla
Quantized One-hot Thermometer
Vanilla (Madry) Vanilla
Quantized One-hot Thermometer

Clean
95.20 94.29 93.49 93.26 94.22
87.3 87.67 85.75 88.67 89.88

FGSM
25.10 46.15 43.89 52.07 48.50
60.3 59.7 53.53 68.76 80.96

PGD/LS-PGA
4.10 1.66 3.57 53.11 50.50
50.0 41.78 42.09 67.83 79.16

Table 4: Comparison of adversarial robustness to white-box attacks on CIFAR-10 .

linearity, we replicate these visualizations and contrast them to visualizations of discretized models. See Appendix G for an illustration of these properties.
For non-discretized, clean trained models, test-set examples always yield a linear boundary between correct and incorrect classification; in contrast, non-adversarially-trained models have a more interesting parabolic shape (see Figure 9).
When discretizing the input, we introduce Cw · Ch · Co · c · (k - 1) extra parameters, where c is the number of channels in the image, k is the number of levels of discretization, and Cw, Ch, Co are the width, height, and output channels of the first convolutional layer. Discretizing using 16 levels introduced 0.03% extra parameters for MNIST, 0.08% for CIFAR-10 and CIFAR-100, and 2.3% for SVHN. This increase is negligible, so it is likely that the robustness comes from the input discretization, and is not merely a byproduct of having a slightly higher-capacity model.
7

Under review as a conference paper at ICLR 2018

Adv. train Clean

XTXarXgeXt XXSoXurXceX Vanilla (Madry) Vanilla Quantized One-hot Thermometer
Vanilla (Madry) Vanilla
Quantized One-hot Thermometer

Vanilla
0.0 3.38 70.54 83.00 80.33
85.60 85.60 84.56 86.01 88.25

Clean One-hot Thermometer

60.10 62.46 56.25 66.22

52.60 55.38 63.94 53.45

74.99 82.43 77.19 81.59

73.78 82.22 77.70 80.80

Vanilla
79.7 45.48 51.74 54.59 57.04
67.0 67.0 72.52 61.92 67.96

Adv. train One-hot Thermometer

37.21 45.37 49.21 51.03

49.91 55.64 57.28 60.90

50.09 72.29 60.02 67.43

71.03 79.43 72.89 77.68

Table 5: Comparison of adversarial robustness to black-box attacks on CIFAR-10 .

(a) Clean examples

(b) Adversarial examples (iterative white-box)

Figure 2: Comparison of the convergence rate of various adversarially trained models on the SVHN dataset.

(a) Loss over steps of LS-PGA

(b) Loss over steps of PGD

Figure 3: Loss for iterated white-box attacks on various models on a randomly chosen data point from MNIST. By step 40, which is where we evaluate, the loss of the point found by iterative attacks has converged.

6 CONCLUSION
Our findings convincingly demonstrate that the use of thermometer encodings, in combination with adversarial training, can reduce the vulnerability of neural network models to adversarial attacks. Our analysis reveals that the resulting networks are significantly less linear with respect to their inputs, supporting the hypothesis of Goodfellow et al. (2014) that many adversarial examples are caused by over-generalization in networks that are too linear.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim S rndic´, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387­ 402. Springer, 2013.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning, pp. 854­863, 2017.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Jun Han and Claudio Moraga. The influence of the sigmoid function parameters on the speed of backpropagation learning. From Natural to Artificial Neural Computation, pp. 195­201, 1995.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, pp. 646­661. Springer, 2016.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. CoRR, abs/1611.02770, 2016. URL http://arxiv.org/abs/ 1611.02770.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697, 2016.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. ICLR, abs/1312.6199, 2014. URL http://arxiv.org/abs/1312.6199.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
9

Under review as a conference paper at ICLR 2018

A HYPERPARAMETERS
In this section, we describe the hyperparameters used in our experiments. For CIFAR-10 and CIFAR-100 we follow the standard data augmenting scheme as in (Lin et al., 2013; He et al., 2016; Huang et al., 2016; Zagoruyko & Komodakis, 2016): each training image is zero-padded with 4 pixels on each side and randomly cropped to a new 32 × 32 image. The resulting image is randomly flipped with probability 0.5, it's brightness is adjusted with a delta chosen uniformly at random in the interval [-63, 63) and it's contrast is adjusted using a random contrast factor in the interval [0.2, 1.8]. For MNIST we use the Adam optimizer with a fixed learning rate of 1e-4 as in Madry et al. (2017). For CIFAR-10 and CIFAR-100 we use the Momentum optimizer with momentum 0.9, 2 weight decay of  = 0.0005 and an initial learning rate of 0.1 which is annealed by a factor of 0.2 after epochs 60, 120 and 160 respectively as in Zagoruyko & Komodakis (2016). For SVHN we use the same optimizer with initial learning rate of 1e-2 which is annealed by a factor of 0.1 after epochs 80 and 120 respectively. We also use a dropout of 0.3 for CIFAR-10, CIFAR-100 and SVHN.

B PSEUDO-CODE
The DGA attack is described in Algorithm 2 and the LS-PGA attack is described in Algorithm 3. Both these algorithms make use of a getM ask() sub-routine which is described in Algorithm 1.

Input: Image x, parameter 

Output: -discretized mask around x

1 mask  (0)n×k

2 low  max{0, x - }

3 high  min{1, x + }

4

for   0 to 1 by

1 k

do

5 mask  mask + fonehot (  low + (1 - )  high)

6 end

7 return mask

Algorithm 1: Sub-routine for getting an -discretized mask of an image.

Input: Image x, label y, discretization function f , loss L(, f (x), y), l attack steps, parameter 

Output: Adversarial input to the network z

1   U (-, )

2 z0  f (x + ) 3 mask  getM ask(x, )

4 for t  1 to l do /* Loop invariant: zit is discretized for every pixel i
5 grad  zt-1 L(, zt-1, y) 6 if f is one-hot then 7 harml  (zt-1 - (l)) grad
8 else 9 harml  (zt-1 -  (l)) grad
10 end

*/

11 harm  harm  mask - (1 - mask)  

12 zt  f (arg max(harm))

13 end

14 return z  zl

Algorithm 2: Discrete Gradient Ascent (DGA)

10

Under review as a conference paper at ICLR 2018

Input: Image x, label y, discretization function f , loss L(, f (x), y), l attack steps, parameters , 

Output: Adversarial input to the network z

1 mask  getM ask(x, ) 2 u0  N (0k; 1k) n  mask - (1 - mask)  

3 T 1

4 if f is one-hot then

5 FI 6 else

7 FC 8 end

9 z0  F



u0 T

10 for t  1 to l do
11 grad  zt-1 L(, zt-1, y) 12 ut  ut-1 +  · grad

13

zt  

zt-1 T

14 T  T · 

15 end

16 return z  zl

Algorithm 3: Logit-Space Projected Gradient Ascent (LS-PGA)

C ADDITIONAL EXPERIMENTS ON MNIST
In this section we list the additional experiments we performed using discretized models on MNIST. The main hyperparameters of Algorithm 3 are the step size  used to perform the projected gradient ascent, and the annealing rate of . We found that the choice of these hyperparameters was not critical to the robustness of the model. In particular, we performed experiments with  = 1.0 and  = 0.001, and both achieved similar accuracies as in Table 2 and Table 3. Additionally, we found that without annealing, i.e.,  = 1.0, the performance was only slightly worse than with  = 1.2.

Model

  White-box Black-box

One-hot One-hot One-hot

1.0 1.2 0.001 1.2 1.0 1.0

93.08 93.50 93.02

98.39 98.42 98.21

Thermometer 1.0 1.2 Thermometer 0.001 1.2 Thermometer 1.0 1.0

93.74 93.88 93.75

98.45 98.24 98.22

Table 6: Comparison of adversarial robustness to white-box attacks on MNIST using 16 levels and with various choices of the hyperparameters  and  for Algorithm 3. The models are evaluated on white-box attacks and on black-box attacks using a vanilla, clean trained model; both use LS-PGA.

We also experimented with discretizing by using percentile information per color channel instead of using uniformly distributed buckets. This did not result in any significant changes in robustness or accuracy for the MNIST dataset.
Finally, we also trained on a mix of clean and adversarial examples: this resulted in significantly higher accuracy on clean examples, but decreased accuracy on white-box and black-box attacks compared to Tables 2 and 3.
11

Under review as a conference paper at ICLR 2018

Model
Vanilla One-hot Thermometer

Clean
99.03 99.01 99.13

FGSM
95.70 96.14 96.10

PGD/LS-PGA
91.36 93.77 93.70

Table 7: Comparison of adversarial robustness to white-box attacks on MNIST using a mix of clean and adversarial examples.

XTXarXgeXt XXSoXurXceX Vanilla

Vanilla

97.88

One-hot

98.28

Thermometer

98.45

Clean One-hot Thermometer

97.87 98.83 98.70

96.99 98.08 98.08

Vanilla
93.07 95.73 96.35

Adv. train One-hot Thermometer

90.97 95.96 95.72

96.46 97.25 96.97

Table 8: Comparison of adversarial robustness to black-box attacks on MNIST of various models using a mix of clean and adversarial examples.

Levels
One-hot (4) One-hot (8) One-hot (16) One-hot (32) One-hot (64)
Thermometer (4) Thermometer (8) Thermometer (16) Thermometer (32) Thermometer (64)

Clean
99.09 99.10 99.14 99.06 98.89
99.11 99.08 99.07 99.02 99.00

White-box PGD/LS-PGA
92.95 92.65 93.08 93.51 93.63
92.62 93.45 93.88 94.14 94.62

Black-box Vanilla, Clean Vanilla, PGD

98.23 98.49 98.39 98.38 98.35

95.64 96.37 96.26 95.78 95.74

98.23 98.44 98.24 98.24 98.33

95.67 95.93 95.28 95.49 95.71

Table 9: Comparison of adversarial robustness on MNIST as the number of levels of discretization is varied. All models are trained mix of adversarial examples and clean examples.

D ADDITIONAL EXPERIMENTS ON CIFAR-10
In this section we list the additional experiments we performed on CIFAR-10. Firstly, we trained models on a mix of both clean and adversarial examples. The results for mixed training are listed in Tables 10 and 11; as expected it has lower accuracy on adversarial examples, but higher accuracy on clean examples, compared to training on only adversarial examples (Tables 4 and 5).

Model
Vanilla One-hot Thermometer

Clean
92.19 92.32

FGSM
58.87 66.60

PGD/LS-PGA
58.96 65.67

Table 10: Comparison of adversarial robustness to white-box attacks on CIFAR-10 of various models using a mix of regular and adversarial training.

12

Under review as a conference paper at ICLR 2018

XTXarXgeXt XXSoXurXceX Vanilla

Vanilla

83.62

One-hot

88.16

Thermometer

87.50

Clean One-hot Thermometer

75.11 75.91

75.49 74.84

Vanilla
52.83 61.89 61.39

Adv. train One-hot Thermometer

59.17 58.51

69.73 68.22

Table 11: Comparison of adversarial robustness to black-box attacks on CIFAR-10 of various models using a mix of clean and adversarial examples.

In order to explore whether the number of levels of discretization affected the performance of the model, we trained several models which varied this number. As expected, we found that models with fewer levels had worse accuracy on clean examples, likely because there was not enough information to correctly classify the image, but greater robustness to adversarial examples, likely because larger buckets mean a greater chance that a given perturbation will not yield any change in input to the network (Xu et al., 2017). Results can be seen in Tables 12, and are visualized in Figure 4.

Levels
Vanilla (Madry)
One-hot (4) One-hot (8) One-hot (16) One-hot (32) One-hot (64)
Thermometer (4) Thermometer (8) Thermometer (16) Thermometer (32) Thermometer (64)

Clean
87.3
83.67 85.62 88.54 88.56 89.63
84.47 85.17 89.88 90.30 89.95

White-box PGD/LS-PGA
50.00
59.59 61.98 67.83 67.82 65.63
61.88 67.23 79.16 72.91 69.37

Black-box Vanilla, Clean Vanilla, PGD

85.60

67.00

83.03 84.92 86.01 85.69 86.94

72.03 72.10 61.92 66.64 65.77

83.64 83.41 88.25 86.06 83.85

72.96 71.11 67.96 59.32 51.82

Table 12: Comparison of adversarial robustness on CIFAR-10 as the number of levels of discretization is varied. All models are trained only on adversarial examples.

E EXPERIMENTS ON CIFAR-100

We list the experimental results on CIFAR-100 in Table 13. We choose  = 0.01 and  = 1.2 for the LS-PGA attack hyperparameters. For the discretized models, we used 16 levels. All adversarially trained models were trained on a mix of clean and adversarial examples.

XTXarXgeXt XXSoXurXceX Clean

Vanilla

74.32

One-hot 73.25

Thermometer 74.44

Adv. Clean

Vanilla

64.46

One-hot 66.54

Thermometer 68.44

White-box PGD/LS-PGA
0 16.55 16.50
6.02 25.11 28.14

Black-box Vanilla, Clean-trained Vanilla, PGD-trained

0.4 55.72 50.33

9.40 19.33 18.49

7.46 63.09 63.21

11.77 42.46 41.97

Table 13: Comparison of adversarial robustness on CIFAR-100. All adversarially trained models were trained on a mix of clean and adversarial examples.

F EXPERIMENTS ON SVHN
We list the experimental results on SVHN in Table 14. All adversarially trained models were trained only on adversarial examples.
13

Under review as a conference paper at ICLR 2018

XTXarXgeXt XXSoXurXceX Clean

Vanilla

97.90

One-hot 97.59

Thermometer 97.87

Adv. Clean

Vanilla

94.79

One-hot 95.12

Thermometer 97.74

White-box PGD/LS-PGA
6.99 56.02 56.37
59.63 87.34 94.77

Black-box Vanilla, Clean-trained Vanilla, PGD-trained

73.94 75.77 78.04

42.04 41.59 41.69

81.24 83.84 84.97

46.77 43.40 48.67

Table 14: Comparison of adversarial robustness on SVHN.

G SUPPLEMENTARY FIGURES
In Figure 4 we plot the effect of increasing the levels of discretization for the MNIST and CIFAR-10 datasets.

(a) MNIST

(b) CIFAR-10

Figure 4: The effect of increasing the number of distinct discretization levels on the accuracy of the model on MNIST and CIFAR-10. (4a) shows the accuracy on on MNIST for discretized models trained on a mix of legitimate and adversarial examples. (4b) shows the accuracy on CIFAR-10 for discretized models trained only on adversarial examples.

In Figure 5 we plot the convergence rate of clean trained and adversarially trained models on the CIFAR-10 dataset. Note that thermometer encoded inputs converge much faster in accuracy on both clean and adversarial inputs.
Figure 6 plots the norm of the gradient as a function of the number of iterations of the attack on MNIST. Note that the gradient vanishes at around 40 iterations, which coincides with the loss stabilizing in Figure 3.
In Figure 7, we create a linear interpolation between a clean image and an adversarial example, and then continue to extrapolate along this line, evaluating probability of each class at each point. In models trained on unquantized inputs, the class probabilities are all mostly piecewise linear in both the positive and negative directions. In contrast, the discretized model has a much more jagged and irregular shape.
In Figure 8, we plot the error for different models on various values of . The discretized models are extremely robust to all values less-than-or-equal-to the values that they have been exposed to during training. However, beyond this threshold, discretized models collapse immediately, while real-valued models still maintain some semblance of robustness. This exposes a weakness of the discretization approach; the same nonlinearity that helps it learn to become robust to all attacks it sees during training-time causes its behavior is unpredictable beyond that.
In Figures 9, 10 , 11, 12 , 13 and 14 we plot several examples of church-window plots for MNIST (Goodfellow et al., 2014). Each plot is crafted by taking several test-set images, calculating the vector corresponding to an adversarial attack on each image, and then choosing an additional random

14

Under review as a conference paper at ICLR 2018
(a) (b) Figure 5: Comparison of the convergence rate of various adversarially trained models on the CIFAR-10 dataset. The discretized models use 16 levels per color channel. (5a) shows the accuracy on clean examples, while (5b) shows the accuracy on white-box PGD/LS-PGA examples, in wall-clock time.
(a) (b) Figure 6: Gradient norm for iterated white-box attacks on various models on a randomly chosen data point from MNIST. (6a) shows the gradient norm on discretized models as a function of steps of LS-PGA, while (6b) shows the gradient norm on a vanilla model as a function of steps of PGD. orthogonal direction. In each plot, the clean image is at the center and corresponds to the color white, the x-axis corresponds to the magnitude of a perturbation in the adversarial direction, and the y-axis corresponds to the magnitude of a perturbation in the orthogonal direction. Note that we use the same random seed to generate the test set examples and the adversarial directions across different church-window plots.
15

Under review as a conference paper at ICLR 2018
(a) (b) Figure 7: Linear extrapolation plot as in Goodfellow et al. (2014) for MNIST. (7a) shows the behavior of a vanilla model while (7b) for a discretized model using 16 levels and thermometer encoding. The -bound is [-1.5, 1.5].
Figure 8: Plot showing the accuracy of various adversarially trained models ( = 0.3) on MNIST when attacked with increasing values of  using PGD/LS-PGA.
16

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 9: Church-window plots of clean-trained models on MNIST. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0]. Notice the almost-linear decision boundaries on nondiscretized models.

17

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 10: Church-window plots of adversarially-trained models on MNIST, trained on only adversarial examples. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0].

18

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 11: Church-window plots of adversarially-trained models on MNIST, trained using a mix of clean and adversarial examples. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0]. Notice the almost-linear decision boundaries on non-discretized models.

19

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 12: Church-window plots of clean-trained models on CIFAR-10. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0]. Notice the almost-linear decision boundaries on nondiscretized models.

20

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 13: Church-window plots of adversarially-trained models on CIFAR-10, trained on only adversarial examples. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0].

21

Under review as a conference paper at ICLR 2018

(a) Vanilla

(b) Quantized

(c) One-hot

(d) Thermometer

Figure 14: Church-window plots of adversarially-trained models on CIFAR-10, trained using a mix of clean and adversarial examples. The x-axis of each sub-plot represents the adversarial direction, while the y-axis represents a random orthogonal direction. The correct class is represented by white. Every row in the plot contains a training data point chosen uniformly at random, while each column uses a different random orthogonal vector for the y-axis. The  bound for both axes is [-1.0, 1.0]. Notice the almost-linear decision boundaries on non-discretized models.

22

