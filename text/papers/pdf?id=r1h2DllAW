Under review as a conference paper at ICLR 2018
DISCRETE-VALUED NEURAL NETWORKS USING VARIATIONAL INFERENCE
Anonymous authors Paper under double-blind review
ABSTRACT
The increasing demand for neural networks (NNs) being employed on embedded devices has led to plenty of research investigating methods for training low precision NNs. While most methods involve a quantization step, we propose a principled Bayesian approach where we first infer a distribution over a discrete weight space from which we subsequently derive hardware-friendly low precision NNs. To this end, we introduce a probabilistic forward pass to approximate the intractable variational objective that allows us to optimize over discrete-valued weight distributions for NNs with sign activation functions. In our experiments, we show that our model achieves state of the art performance on several real world data sets. In addition, the resulting models exhibit a substantial amount of sparsity that can be utilized to further reduce the computational costs for inference.
1 INTRODUCTION
With the advent of deep neural networks (NNs) impressive performances have been achieved in many applications such as computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), and machine translation (Sutskever et al., 2014), among others. However, the performance improvements are largely attributed to increasing hardware capabilities that enabled the training of ever-increasing network architectures. On the other side, there is also a growing interest in making NNs available for embedded devices with drastic memory and power limitations ­ a field with plenty of interesting applications that barely profit from the tendency towards larger and deeper network structures.
Thus, there is an emerging trend in developing NN architectures that allow fast and energy-efficient inference and require little storage for the parameters. In this paper, we focus on reduced precision methods that restrict the number of bits per weight while keeping the network structures at a decent size. While this reduces the memory footprint for the parameters accordingly, it can also result in drastic improvements in computation speed if appropriate representations for the weight values are used. This direction of research has been pushed towards NNs that require in the extreme case only a single bit per weight. In this case, assuming weights w  {-1, 1} and binary inputs x  {-1, 1}, costly floating point multiplications can be replaced by cheap and hardware-friendly logical XNOR operations. However, training such NNs is inherently different as discrete valued NNs cannot be directly optimized using gradient based methods. Furthermore, NNs with binary weights exhibit their full computational benefits only in case the sign activation function is used whose derivative is zero almost everywhere, and, therefore, is not suitable for backpropagation.
Most methods for training reduced precision NNs either quantize the weights of pre-trained full precision NNs (Courbariaux et al., 2015a) or train reduced precision NNs by maintaining a set of full precision weights that are deterministically or stochastically quantized during forward or backward propagation. Gradient updates computed with the quantized weights are then applied to the full precision weights (Courbariaux et al., 2015b; Rastegari et al., 2016; Hubara et al., 2016). This approach alone fails if the sign activation function is used. A promising approach is based on the straight through gradient estimator (STE) (Bengio et al., 2013) which replaces the zero gradient of hard threshold functions by a non-zero surrogate derivative. This allows information in computation graphs to flow backwards such that parameters can be updated using gradient based optimization methods. Encouraging results are presented in (Hubara et al., 2016) where the STE is applied to the weight binarization and to the sign activation function. These methods, although showing
1

Under review as a conference paper at ICLR 2018

non-Bayesian

real weights
t

1.1 -0.7

0.9

z1 1.3

z2 -0.2
1.2

-0.6

1

x1 1

t

discrete weights
t

1 -1

1

z1 1

-1

z2
1 0

1

x1 1

t

z1 z2

1 z1

z2

1

x1 1

x1

(a) Overview

1

x1 x2
xd0
(b) Probabilistic forward pass

Bayesian

Figure 1: (a) Overview of real-valued vs. discrete-valued NNs and Bayesian vs. non-Bayesian NNs. The aim is to obtain a single discrete-valued NN (top right) with a good performance. We achieve this by training a distribution over discrete-valued NNs (bottom right) and subsequently deriving a single discrete-valued NN from that distribution. (b) Probabilistic forward pass: The idea is to propagate distributions through the network by approximating a sum over random variables by a Gaussian and subsequently propagating that Gaussian through the sign activation function.

convincing empirical performance, have in common that they appear rather heuristic and it is usually not clear whether they optimize any well defined objective.
Therefore, it is desired to develop principled methods that support discrete weights in NNs. In this paper, we propose a Bayesian approach where we first infer a distribution q(W ) over a discrete weight space from which we subsequently derive discrete-valued NNs. The distribution q(W ) can be seen as an exponentially large ensemble of NNs where each NN is weighted by its probability q(W ). Rather than having a single value for each connection of the NN, we now maintain a whole distribution for each connection (see bottom right of Figure 1(a)). To obtain q(W ), we employ variational inference where we approximate the true posterior p(W |D) by minimizing the variational objective KL(q(W )||p(W |D)). Although the variational objective is intractable, this idea has recently received a lot of attention for real-valued NNs due to the reparameterization trick which expresses gradients of intractable expectations as expectations of tractable gradients (Rezende et al., 2014; Kingma & Welling, 2014; Blundell et al., 2015). This allows us to efficiently compute gradient samples of the intractable variational objective that can subsequently be used for stochastic optimization. Unfortunately, the reparameterization trick is only suitable for real-valued distributions which renders it unusable for our case. The recently proposed Gumbel softmax distribution (Jang et al., 2017; Maddison et al., 2017) overcomes this issue by relaxing one-hot encoded discrete distributions with probability vectors. Subsequently, the reparameterization trick can again be applied. However, for the sign activation function one still has to rely on the STE or similar heuristics. The log-derivative trick offers an alternative for discrete distributions to express gradients of expectations with expectations of gradients (Paisley et al., 2012). However, the resulting gradient samples are known to suffer from high variance. Therefore, the log-derivative trick is typically impractical unless suitable variance reduction techniques are used. This lack of practical methods has led to a limited amount of literature investigating Bayesian NNs with discrete weights (Soudry et al., 2014).
In this work, we approximate the intractable variational objective with a probabilistic forward pass (PFP) (Wang & Manning, 2013; Soudry et al., 2014; Hernandez-Lobato & Adams, 2015; Roth & Pernkopf, 2016). The idea is to successively approximate the distribution of activations with a Gaussian using a central limit argument and propagating this Gaussian through the sign activation function (Figure 1(b)). In the last layer, we perform a second-order Taylor expansion to approximate the expectation of the log-softmax with respect to a Gaussian. This way, an approximation of the intractable expected log-likelihood is computed by propagating probabilities through the network. Most importantly, the PFP results in a well-defined objective whose gradient with respect to the variational parameters can be computed analytically. This is true for discrete weight distributions as well as for the sign activation function with zero gradient almost everywhere. The method is very flexible in the sense that different weight distributions can be used in different layers. We utilize this flexibility to represent the weights in the first layer with 3 bits and we use ternary weights w  {-1, 0, 1} in the remaining layers.
2

Under review as a conference paper at ICLR 2018

In our experiments, we evaluate the performance of our model by reporting the error of (i) the most probable model of the approximate posterior q(W ) and (ii) approximated expected predictions using the PFP. We show that averaging over small ensembles of NNs sampled from W  q(W ) can improve the performance while inference using the ensemble is still cheaper than inference using a single full precision NN. Furthermore, our method exhibits a substantial amount of sparsity that further reduces the computational overhead. Compared to (Hubara et al., 2016), our method requires less precision for the first layer, and we do not introduce a computational overhead by using batch normalization which appears to be a crucial component of their method.
The paper is outlined as follows. In Section 2, we introduce the notation and formally define the PFP. Section 3 shows details of our model. Section 4 shows experiments. In Section 5 we discuss important issues concerning our model and Section 6 concludes the paper.

2 VARIATIONAL INFERENCE FOR DISCRETE-VALUED NEURAL NETWORKS

The structure of a feed-forward NN with L layers is determined by the number of neurons {d0, d1, . . . , dL} in each layer. Here, d0 is the dimensionality of the input, dL is the dimensionality of the output, and dl for 0 < l < L is the number of hidden neurons in layer l. A NN defines a function y = xL = f (x0) by iteratively applying a linear transformation al = W lxl-1 to the inputs from the previous layer followed by a non-linear function xl = l(al). For l < L we use the sign activation function l(a) = I(a  0) - I(a < 0) which is applied element-wise to its inputs.
For l = L, we use the softmax activation function smaxi(a) = exp(ai)/ j exp(aj). Note that the expensive softmax function does not need to be computed at test time.

NNs are parameterized by a set of weight matrices W

=

{W l}Ll=1 with W l



Ddl-1 ×dl
l

where Dl is

a finite set.1. For a Bayesian treatment of NNs, we assume a prior distribution p(W ) over the discrete

weights and interpret the output of the NN after the softmax activation as likelihood p(D|W ) for

the data set D. As the induced posterior distribution p(W |D)  p(D|W )p(W ) is intractable for

NNs of any decent size, we employ variational inference to approximate it by a simpler distribution q(W |) by minimizing KL(q(W |)||p(W |D)) with respect to the variational parameters . We

adopt the common mean-field assumption where the approximate posterior factorizes into a product of factors q(w|w) for each weight w  W .2 The variational objective is usually transformed as

KL(q(W )||p(W |D)) = KL(q(W )||p(W )) - Eq(W )[log p(D|W )] + log p(D).

(1)

Minimizing this expression with respect to  does not involve the intractable posterior p(W |D) and the evidence log p(D) is constant with respect to the variational parameters . The KL term can be

seen as a regularizer that pulls the approximate posterior q(W ) towards the prior distribution p(W )

whereas the expected log-likelihood captures the data. While the KL term is tractable if both the

prior and the approximate posterior distribution assume independence of the weights, the expected

log-likelihood is typically intractable due to a sum over exponentially many terms. We propose a

PFP as closed-form approximation to the expected log-likelihood.

2.1 CLOSED-FORM APPROXIMATION OF THE EXPECTED LOG-LIKELIHOOD

The approximation of the expected log-likelihood resembles a PFP. In particular, we have

Eq(W ) log p(t|x0, W ) = · · · q(W ) log p(t|x0, W )

W1

WL

 · · · q(W >1) N a1|µa1 , a1 log p(t|a1, W >1)da1

W2

WL

 ···

q(W >1) Bernoulli(x1|µx1 ) log p(t|x1, W >1)

... W 2

W L x1

 N (aL|µaL , aL ) log smaxt(aL) daL,

(2) (3) (4) (5)

1This notation allows us to have a different set Dl, requiring a different number of bits, for each layer. For
convenience, we include the biases in the weight matrices. 2In the sequel, we omit the dependency of the variational posterior q(W ) on the variational parameters .

3

Under review as a conference paper at ICLR 2018

where we defined W >k = {W l}lL=k+1. The overall aim is to successively get rid of the weights in each layer and consequently reduce the exponential number of terms to sum over. In the first
approximation in (3), we approximate the activation distribution with Gaussians using a central limit
argument. These Gaussian distributions are propagated through the sign activation function resulting
in Bernoulli distributions in (4). These two steps are iterated until a Gaussian approximation of the
output activations in (5) is obtained. This integral is approximated using a second-order Taylor expansion of the log-softmax around µaL . In the following subsections we provide more details of the individual approximations.

2.1.1 APPROXIMATING THE ACTIVATIONS AND THE ACTIVATION FUNCTIONS

The activations of the neurons are computed as weighted sums over the outputs from the previous
layers. Since the inputs and the weights are random variables, the activations are random variables
as well. Given a sufficiently large number of input neurons, we can apply a central limit argument and approximate the activation distributions with Gaussians N (ail|µail , ail ). For computational convenience, we further assume that the activations within a layer are independent. Assuming that the inputs xjl-1 and the weights wilj are independent, we have

dl-1

µali =

µ µ ,wilj xlj-1

j=1

dl-1

ail =

 µ + µwilj (xjl-1)2

2 wilj

j=1

µ - µ(xjl-1)2

2 xjl-1

,

(6)

where µ(xlj-1)2 denotes the raw second moment E[(xlj-1)2]. In case of l = 1, we assume no variance at the inputs and thus the second term of ail in (6) cancels.

In the next step, the Gaussian distributions N (ali|µail , ail ) over the activations are transformed by

the sign activation function. The expectation of the resulting Bernoulli distribution with values

x



{-1, 1}

of

the

sign

activation

function

is

given

by

µxil

=

erf(µail

/(2ali

)

1 2

)

where

erf

denotes

the error function. The raw second moment as needed for (6) is µ(xil)2 = 1.

2.1.2 EXPECTED LOG-LIKELIHOOD AT THE OUTPUTS

After iterating this approximation up to the last layer, it remains to calculate the expectation of the log-softmax with respect to the Gaussian approximation of the output activations in (5). Since this integral does not allow for an analytic solution, we approximate the log-softmax by its second-order Taylor approximation around the mean µaL with a diagonal covariance approximation, resulting in

EN (aL)

log smaxt(aL)



log

smaxt(µaL )

-

1 2

aL smaxt (µaL ) (1 - smaxt (µaL )) . (7) t

t

The maximization of the first term in (7) enforces the softmax output of the true class to be maximized, whereas the second term becomes relevant if there is no output close to one. For softmax outputs substantially different from zero or one, the product inside the sum is substantially larger than zero and the corresponding variance is penalized. In short, the second term penalizes large output variances if their corresponding output activation means are large and close to each other.

3 THE MODEL
In this section we provide details of the finite weight sets Dl and their corresponding prior and approximate posterior distributions, respectively. As reported in several papers (Hubara et al., 2016; Anderson & Berg, 2017), it appears to be crucial to represent the weights in the first layer using a higher precision. Therefore, we use three bits for the weights in the first layer and ternary weights in the remaining layers.
3.1 INPUT LAYER
We use D1 = {-0.75, -0.5, . . . , 0.75} for the first layer which can be represented as fixed point numbers using three bits. Note that |D1| = 7 and we could actually represent one additional value

4

Under review as a conference paper at ICLR 2018

with three bits. However, for the sake of symmetry around zero, we refrain from doing so as we do not want to introduce a bias towards positive or negative values. We empirically observed this range of values to perform well for our problems with inputs x  [-1, 1]. The values in D1 can be scaled with an arbitrary factor at training time without affecting the output of the NN since only the sign of the activations is relevant. We investigated two different variational distributions q(W 1).
(i) General distribution: We store for each weight the seven logits corresponding to the unnormalized log-probabilities for each of the seven values. The normalized probabilities can be recovered using the softmax function. This simple and most general distribution for finite discrete distributions has the advantage that the model can express a tendency towards individual discrete values. Consequently, we expect the maximum of the distribution to be a reasonable value that the model explicitly favors. This is fundamentally different from training full precision networks and quantizing the weights afterwards. The disadvantage of this approach is that the number of variational parameters and the computation time for the means µw and variances w scales with the size of D1.
(ii) Discretized Gaussian: To get rid of the dependency of the number of parameters on |D1|, we also evaluated a discretized Gaussian. The distribution is parameterized by a mean mw and a variance vw and the logits of the resulting discrete distribution are given by -(w - mw)2/(2 vw) for w  D1 (Figure 2(a)). We denote this distribution as ND1 (mw, vw).3 This parameterization has the advantage that only two parameters are sufficient to represent the resulting discrete distribution for an arbitrary size of D1. Furthermore, the resulting distribution is unimodal and neighboring values tend to have similar probabilities which appears natural. Nevertheless, there is no closedform solution for the mean µw and variance w of this distribution and we have to compute a weighted sum involving the |D1| normalized probabilities.
For the prior distribution p(W 1), we use for both aforementioned variational distributions the discretized Gaussian ND1 (0, ) with  being a tunable hyperparameter. Computing the KL-divergence KL(q(W 1)||p(W 1)) also requires computing a weighted sum over |D1| values.
3.2 HIDDEN LAYERS
For the remaining layers we use ternary weights w  Dl = {-1, 0, 1}. We use a shifted binomial distribution, i.e. w  Binomial(2, wp) - 1. This distribution requires only a single parameter wp per weight for the variational distribution. The mean µw is given by 2wp - 1 and the variance w is given by 2wp(1 - wp). This makes the Bernoulli distribution an efficient choice for computing the required moments. It is convenient to select a binomial prior distribution p(w) = Binomial(2, 0.5) as it is centered at zero and we get KL(q(w)||p(w)) = |Dl|(log(2wp)wp + log(2(1 - wp))(1 - wp)).
These favorable properties of the binomial distribution might raise the question why it is not used in the first layer, especially since the required expectations, variances and KL-divergences are available in closed-form independent of the size of Dl. We elaborate more on this in Section 5.

3.3 ACTIVATION NORMALIZATION

We normalize the activations of layer l by dl-1.4 This scales the activation means µa towards zero and keeps the activation variances a independent of the number of incoming neurons. Con-
sequently, the expectation of the Bernoulli distribution after applying the sign activation function

µx

=

erf(µa

/(2a

)

1 2

)

is

less

prone

to

be

in

the

saturated

region

of

the

error

function

and,

thus,

gra-

dients can flow backwards in the computation graph. Note that activation normalization influences

only the PFP and does not change the classification result of individual NNs W  q(W ) since only

the sign of the activation is relevant.

3.4 LIKELIHOOD WEIGHTING
The variational inference objective (1) does not allow to easily trade off between the importance of the expected log-likelihood Eq(W )[log p(D|W )] and the KL term KL(q(W )||p(W )). This is problematic since there are usually many more NN weights than there are data samples and the
3Note that the mean µw and the variance w of ND1 (mw, vw) are in general different from mw and vw. 4The activation variance a is normalized by dl-1.

5

Under review as a conference paper at ICLR 2018

KL term tends to be orders of magnitudes larger than the expected log-likelihood. As a result, the optimization procedure mainly focuses on keeping the approximate posterior q(W ) close to the prior p(W ) whereas the influence of the data is too small. We propose to counteract this issue by
trading off between the expected log-likelihood and the KL term using a convex combination, i.e.,

 Eq(W )[log p(D|W )] + (1 - ) KL(q(W )||p(W )).

(8)

Here,   (0, 1) is a tunable hyperparameter that can be interpreted as creating /(1 - ) copies of the data set D. Another approach to counteract this issue is the KL-reweighting scheme proposed in (Blundell et al., 2015). Due to the exponential weight decay, only a few minibatches are influenced
by the KL term whereas the vast majority is mostly influenced by the expected log-likelihood.

4 EXPERIMENTS
We evaluated the performance of our model (NN VI) on MNIST (LeCun et al., 1998) and variants of the MNIST data set (Larochelle et al., 2007). The data sets contain images of size 28 × 28 showing handwritten digits. The MNIST data set is split into 50000 training samples, 10000 validation samples, and 10000 test samples. The variants of the MNIST data set are split into 10000 training samples, 2000 validation samples and 50000 test samples. We selected a three layer structure with d1 = d2 = 1200 hidden units for all experiments. We evaluated both the general parameterization and the Gaussian parameterization as variational distribution q(W >1) for the first layer (Section 3.1), and a binomial variational distribution q(W >1) for the following layers (Section 3.2). We optimized the variational distribution using ADAM (Kingma & Ba, 2015) without KL reweighting (Blundell et al., 2015), and using rmsprop with KL reweighting, and report the experiment resulting in the better classification performance. For both optimization algorithms, we employ an exponential decay of the learning rate  where we multiply  after every epoch by a factor   1. We use dropout (Srivastava et al., 2014) with dropout rate pin for the input layer and a common dropout rate phid for the remaining hidden layers. We normalize the activation by dl-1 p where p is either pin or phid to consider dropout in the activation normalization. We tuned the hyperparameters   [0, 1],   [10-5, 100],   [10-3, 10-1],   [0.975, 1], pin  [0, 0.5], phid  [0, 0.8] with 50 iterations of Bayesian optimization (Snoek et al., 2012). We report the results for the single most probable model from the approximate posterior W = arg maxW q(W ). This model is indeed a low precision network that can efficiently be implemented in hardware. We also report results by computing predictions using the PFP which can be seen as approximating the expected prediction arg maxt Eq(W )[p(t|x, W )] as it is desired in Bayesian inference.
We compare our model with real-valued NNs (NN real) trained with batch normalization (Ioffe & Szegedy, 2015), dropout, and ReLU activation function. We also evaluated the model from (Hubara et al., 2016) (NN STE) which uses batch normalization, dropout, real-valued weights in the first layer and binary weights w  {-1, 1} in the subsequent layers. The binary weights and the sign activation function are handled using the STE. For NN (real) and NN STE, we tuned the hyperparameters   [10-4, 10-2],   [0.975, 1], pin  [0, 0.5], and phid  [0, 0.8] using 50 iterations of Bayesian optimization.
The results are shown in Table 1. Our model (single) performs on par with the state of the art methods NN (real) and NN STE. Compared to NN STE, our model seems to be more robust on the data sets with different kinds of background artifacts. Interestingly, our model outperforms the other models on MNIST Background and MNIST Background Random by quite a large margin which could be due to the Bayesian nature of our model. The PFP outperforms the single most probable model. This is no surprise since the PFP uses all the available information from the approximate posterior q(W ). Overall, the performance of the general variational distribution seems to be slightly better than the discretized Gaussian at the cost of more computational overhead at training time.
4.1 ENSEMBLES OF DISCRETE NNS
Next, we approximate the expected predictions arg maxt Eq(W )[p(t|x, W )] by sampling several models W  q(W ). We demonstrate this on the MNIST Rotated Background data set. Figure 2(b) shows the classification error of Bayesian averaging over 1000 NNs sampled from the model with the best PFP performance using the discretized Gaussian. We see that the performance approaches

6

Under review as a conference paper at ICLR 2018

Table 1: Classification errors [%] for different NN models. NN (real): Real-valued NNs with batch

normalization, dropout and ReLU activation function. NN STE: NNs with batch normalization,

dropout, sign activation function, real-valued weights in the first layer and binary weights in the

remaining layers. NN VI (our method): 3 bits for the first layer and ternary weights for the remain-

ing layers. We evaluated the single most probable model, the probabilistic forward pass (pfp), the

general 3 bit distribution for the first layer, and the discretized Gaussian for the first layer.

dataset

NN (real) NN STE

NN VI

general/pfp general/single Gauss/pfp Gauss/single

MNIST

1.280

1.240

1.300

1.430

1.290

1.320

MNIST Basic

2.878

2.614

2.736

2.806

2.656

3.064

MNIST Background

18.974

19.600

16.566

16.590

16.656

18.300

MNIST Background Random 12.008

11.808

9.430

9.716

9.574

10.158

MNIST Rotated

8.310

10.220

9.866

10.318

10.212

11.592

MNIST Rotated Background

48.894

52.586

49.030

52.768

49.700

51.780

p(w) CE [%] CE Test [%] frequency

1

0.5

0-0.75 -0.5 -0.25 0 w
(a)

0.25 0.5 0.75

50
49 CE Validate (averaged) CE Validate (pfp) CE Test (averaged) CE Test (pfp)
48
1 200 400 600 800 1,000 # averaged models
(b)

53 ·105

greedy averaging

2.5

Bayesian averaging

52

PFP

2

Dropout No Dropout

1.5 51
1

50 0.5

15

10 15 20

# averaged models

(c)

25

-0150 -100 -50

0 a

(d)

50 100 150

Figure 2: (a) Example discretized Gaussian with mw = 0.3 and vw = 0.3. The red bars show the unnormalized probabilities of the corresponding weight values. (b) Classification errors (CE) of Bayesian averaging over several NN samples W  q(W ). The CEs of the probabilistic forward pass (pfp) are shown as horizontal lines. (c) Test CE of Bayesian averaging using NN samples W  q(W ) and averaging using the greedy forward selection strategy. (d) Histogram over activations of
the same model once trained with dropout and once trained without dropout.

the PFP which indicates that the PFP is a good approximation to the true expected predictions. However, the size of the ensemble needed to approach the PFP is quite large and the computation time of evaluating a large ensemble is much larger than a single PFP. Therefore, we investigated a greedy forward selection strategy, where we sample 100 NNs out of which we include only the NN in the ensemble which leads to the lowest error. This is shown in Figure 2(c). Using this strategy results in a slightly better performance than Bayesian averaging. Most importantly, averaging only a few models results in a decent performance increase while still allowing for faster inference than full precision NNs.
4.2 COMPUTATIONAL COSTS AND SPARSITY
Our NNs obtained by taking the most probable model from q(W ) can be efficiently implemented in hardware. They require only multiplications with 3 bit fixed point values as opposed to multiplications with floating point values in NN (real) and NN STE. In the special case of image data, the inputs are also given as 8 bit fixed point numbers. By scaling the inputs and the weights from the first layer appropriately5, this can be seen as an ordinary integer multiplication. In the following layers we only have to compute multiplications as logical XNOR operations and accumulate -1 and +1 values for the activations.
Table 2 shows the fraction of non-zero weights of the best performing single models from Table 1. Especially in the input layer where we have our most costly 3 bit weights, there are a lot of zero weights on most data sets. This can be utilized to further reduce the computational costs. For example, on the MNIST Background Random data set, evaluating a single NN requires only approximately 23000 integer multiplications and 1434000 XNOR operations instead of approximately 2393000 floating point multiplications.
5We can scale the inputs and the weights with arbitrary constants since only the sign of the activation is important.
7

Under review as a conference paper at ICLR 2018

Table 2: The fraction [%] of non-zero weights of the best performing single model for individual

layers and for all layers combined.

dataset

Layer 1 Layer 2 Layer 3 Overall

MNIST

22.49 66.26 95.92 49.20

MNIST Basic

36.78 96.40 99.84 72.97

MNIST Background

0.43 96.91 99.73 58.99

MNIST Background Random 2.44

98.72 99.99 60.88

MNIST Rotated

81.86 93.12 99.82 88.72

MNIST Rotated Background 4.92 83.98 88.93 52.92

5 DISCUSSION
The presented model has many tunable parameters, especially the type of variational distributions for the individual layers, that heavily influence the behavior in terms of convergence at training time and performance at test time. The binomial distribution appears to be a natural choice for evenly spaced values with many desirable properties. It is fully specified by only a single parameter, and its mean, variance, and KL divergence with another binomial has nice analytic expressions. Furthermore, neighboring values have similar probabilities which rules out odd cases in which, for instance, there is a value with low probability in between of two values with high probability.
Unfortunately, the binomial distribution is not suited for the first layer as it is crucial in the first layer to be able to set weights with high confidence to zero. However, when favoring zero weights by setting wp = 0.5, the variance of the binomial distribution takes on its largest possible value. This might not be a problem in case predictions are computed as the true expectations with respect to q(W ) as in the PFP, but it results in bad classification errors when deriving a single model from q(W ). We also observed that using the binomial distribution in deeper layers favor the weights -1 and 1 over 0 (cf. Table 2). This might indicate that binary weights w  {-1, 1} using a Bernoulli distributions could be sufficient, but in our experiments we observed this to perform worse. We believe this to stem partly from the importance of the zero weight and partly from the larger variance of 4wp(1-wp) of the Bernoulli distribution compared to the variance of 2wp(1-wp) of the binomial distribution.
Furthermore, there is a general issue with the sign activation functions if the activations are close to zero. In this case, a small change to the inputs can cause the corresponding neuron to take on a completely different value which might have a large impact on the following layers of the NN. We found dropout to be a very helpful tool to counteract this issue. Figure 2(d) shows histograms of the activations of the second hidden layer for both a model trained with dropout and the same model trained without dropout. We can see that without dropout the activations are much closer to zero whereas dropout introduces a much larger spread of the activations and even causes the histogram to decrease slightly in the vicinity of zero. Thus, the activations are much more often in regions that are stable with respect to changes of their inputs which makes them more robust. We believe that such regularization techniques are crucial if the sign activation function is used.
6 CONCLUSION
We introduced a method to infer NNs with low precision weights. As opposed to existing methods, our model neither quantizes the weights of existing full precision NNs nor does it rely on heuristics to compute "approximated" gradients of functions whose gradient is zero almost everywhere. We perform variational inference to obtain a distribution over a discrete weight space from which we subsequently derive a single discrete-valued NN or a small ensemble of discrete-valued NNs. Our method propagates probabilities through the network which results in a well defined function that allows us to optimize the discrete distribution even for the sign activation function. The weights in the first layer are modeled using fixed point values with 3 bits precision and the weights in the remaining layers have values w  {-1, 0, 1}. This reduces costly floating point multiplications to cheaper multiplications with fixed point values of 3 bits precision in the first layer, and logical XNOR operations in the following layers. In general, our approach allows flexible bit-widths for each individual layer. We have shown that the performance of our model is on par with state of the art methods that use a higher precision for the weights. Furthermore, our model exhibits a large amount of sparsity that can be utilized to further reduce the computational overhead.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Alexander G. Anderson and Cory P. Berg. The high-dimensional geometry of binary neural networks. CoRR, abs/1705.07199, 2017.
Yoshua Bengio, Nicholas Le´onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In International Conference on Machine Learning (ICML), pp. 1613­1622, 2015.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. In Workshop @ International Conference on Learning Representations (ICLR), pp. CoRR, abs/1412.7024, 2015a.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems (NIPS), pp. 3123­3131, 2015b.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of Bayesian neural networks. In International Conference on Machine Learning (ICML), pp. 1861­1869, 2015.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82­97, 2012.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 4107­4115, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp. 448­456, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In International Conference on Learning Representations (ICLR), 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. arXiv: 1412.6980.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations (ICLR), 2014. arXiv: 1312.6114.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 1106­1114, 2012.
Hugo Larochelle, Dumitru Erhan, Aaron C. Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In International Conference on Machine Learning (ICML), pp. 473­480, 2007.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations (ICLR), 2017.
John William Paisley, David M. Blei, and Michael I. Jordan. Variational Bayesian inference with stochastic search. In International Conference on Machine Learning (ICML), pp. 1367­1374, 2012.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision (ECCV), pp. 525­542, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning (ICML), pp. 1278­1286, 2014.
Wolfgang Roth and Franz Pernkopf. Variational inference in neural networks using an approximate closed-form objective. In Bayesian Deep Learning Workshop @ NIPS, 2016.
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems (NIPS), pp. 2960­2968, 2012.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In Advances in Neural Information Processing Systems (NIPS), pp. 963­971, 2014.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 3104­3112, 2014.
Sida I. Wang and Christopher D. Manning. Fast dropout training. In International Conference on Machine Learning (ICML), pp. 118­126, 2013.
9

