Under review as a conference paper at ICLR 2018

LEARNING SPARSE STRUCTURED ENSEMBLES WITH SG-MCMC AND NETWORK PRUNING
Anonymous authors Paper under double-blind review
ABSTRACT
A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks. In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections. In this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. This is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs. For example, in LSTM based language modeling (LM), we obtain 12% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40% of model parameters and 90% of computations in total, as compared to a state-ofthe-art large LSTM LM.

1 INTRODUCTION

Recently there are increasing interests in using ensembles of Deep Neural Networks (DNNs) (Ju et al. (2017); Huang et al. (2017)), which are known to be more robust and accurate than individual networks. An explanation stems from the fact that learning neural networks is an optimization problem with many local minima (Hansen & Salamon (1990)). Multiple models obtained from applying stochastic optimization, e.g. the widely used Stochastic Gradient Descent (SGD) and its variants, converge to different local minima and tend to make different errors. Due to this diversity, the collective prediction produced by an ensemble is less likely to be in error than individual predictions. The collective prediction is usually performed by averaging the predictions of the multiple neural networks.

On the other hand, the improved prediction accuracy of such model averaging can be understood

from the principled perspective of Bayesian inference with Bayesian neural networks. Specifically,

for each test point x~, we consider the predictive distribution P (y~|x~, D) = P (y~|x~, )P (|D)d,

by integrating the model distribution P (y~|x~, ) with the posterior distribution over the model pa-

rameters P (|D) given training data D. The predictive distribution is then approximated by Monte

Carlo integration P (y~|x~, D)



1 M

M m=1

P

(y~|x~,

(m))

,

where

(m)



P (|D), m

=

1, · · ·

,M,

are posterior samples of model parameters. It is well known that such Bayesian model averaging

is more accurate in prediction and robust to over-fitting than point estimates of model parameters

(Balan et al. (2015), Li et al. (2016)).

Despite the obvious advantages as seen from both perspectives, a practical problem that hinders the use of DNN ensembles in real-world tasks is that an ensemble requires too much computation in both training and testing. Traditionally, multiple neural networks are trained, e.g. with different random initialization of model parameters. Recent studies in (Loshchilov & Hutter (2016); Huang et al. (2017)) propose to learn an ensemble which consists of multiple snapshot models along the optimization path within a single training process, by leveraging a special cyclic learning rate schedule. This reduces the training cost, but the testing cost is still high.

1

Under review as a conference paper at ICLR 2018

Posterior P ( | D)  P(D| )P( )

SG-MC MC sampling

Group sparse prior

Model 1 Model 2
Model N

Sparse Structured Ensemble

Pruning & Retraini ng

SS Model 1 SS Model 2

Inference: Model Averaging

SS Model N

Figure 1: Overview of our two-stage method for learning SSEs.

In this paper we also aim at learning an ensemble within a single training process, but by leveraging the recent progress in Bayesian posterior sampling, namely the Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms. Moreover, we apply group sparse priors in training to enforce group-level sparsity on the network's connections. Subsequently we can further use model pruning to compress the networks so that the testing cost is reduced with no loss of accuracy.
Figure 1 presents a high-level overview of our two-stage method to learn Sparse Structured Ensembles (SSEs) for DNNs. Specifically, in the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections as fine-tuning. In this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. This is empirically verified in our experiments of learning SSE ensembles of both FNNs and LSTMs.
We evaluate the performance of the proposed method on two experiments with different types of neural networks. The first is an image classification experiment, which uses Feed-forward Neural Networks (FNNs) on the well-known MNIST dataset (Deng (2012)). Second, we experiment with the more challenging task of Lont Short-term Memory (LSTM, Hochreiter & Schmidhuber (1997)) based language modeling, which is conducted on the Penn Treebank dataset (Marcus et al. (1993)). It is found that the proposed method works well across both tasks. For example, we obtain 12% relative improvement (from 78.4 to 68.6) in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40% of model parameters and 90% of computations in total, as compared to the large LSTM LM in Zaremba et al. (2014).
2 RELATED WORK
This work draws inspiration from three recent research findings, namely running SG-MCMC for efficient and scalable Bayesian posteriori sampling, applying group sparse priors to enforce network sparsity, and network pruning. To the best of our knowledge, this work represents the first methodology and empirical study of integrating these three techniques and demonstrates its usefulness to learning and using ensembles. In the following, we comment on related studies.
SG-MCMC sampling: SG-MCMC represents a family of MCMC sampling algorithms developed in recent years, e.g. Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh (2011), Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) Chen et al. (2014), mainly for Bayesian learning from large scale datasets. SG-MCMC has the following favorable properties for learning ensembles. (i) SG-MCMC works by adding a scaled gradient noise during training, and thus enhances exploration of the model-parameter space. This is beneficial for finding diverse sample models for ensembles. (ii) Scalable and simple: the basic SG-MCMC algorithm, e.g. SGLD, is just a noisy Stochastic Gradient Descent (SGD), which means the same training cost as SGD on large datasets.
Sparse structure learning: Group Lasso penalty (Yuan & Lin (2006)) is widely used to regularize likelihood function to learn sparse structures. Similar idea has been studied in Bayesian learning, which is known as applying group sparse priors (Marlin et al. (2009); Babacan et al. (2014)); but
2

Under review as a conference paper at ICLR 2018

these previous works use variational method. Applying group sparse priors with SG-MCMC has not been explored.
Model compression: Model pruning and retraining (Han et al. (2015a), Hu et al. (2016)) has been studied to compress CNNs. Recently, Han et al. (2017) and Narang et al. (2017) apply model pruning to LSTM models for automatic speech recognition task. We use similar model pruning and retraining method in the experiments. We find that model averaging can make the ensemble with heavily-pruned networks more robust in prediction.
Learning ensembles: Some efforts have been made to reduce the training and testing cost for ensembles. For reducing the training time cost of ensembles, a special cyclic learning rate schedule is developed in (Loshchilov & Hutter (2016); Huang et al. (2017)), which restarts the learning rate periodically to attempt to visit multiple local minima along its optimization path and saves snapshot models. In contrast to relying on such empirical setting of the learning rate to explore model space, theoretical consistency properties of SG-MCMC methods in posterior sampling have been established (Teh et al. (2016)). For reducing the testing time cost of ensembles, Hinton et al. (2015) and Balan et al. (2015) distill the knowledge of an ensemble into a single model, but still require large training cost.

3 LEARNING ENSEMBLES WITH SG-MCMC AND NETWORK PRUNING

We consider the classification problem under Bayesian inference framework. Given training data

D {(xi, yi)}iN=1 with input feature xi  RD and class label yi  Y, where Y is the set of classes. We view a neural network as a conditional probabilistic model P (yi|xi, ). Denote the

network parameters by , with P () a prior distribution. We compute the posterior distribution

over the model parameters, P (|D)  P ()

N i=1

P

(xi|yi

,

).

For testing, given a test input x~,

the Bayesian predictive distribution for its label y~ is given by P (y~|x~, D) = EP (|D)[P (y~|x~, )] ,

which can be viewed as model averaging across parameters with distribution P (|D). However, the

integration over the posterior is analytically intractable for deep neural networks (DNNs). Thus it is

approximated by Monte Carlo integration as in the following:

P (y~|x~, D)  1

M
P (y~|x~, (m)),

M

(m)  P (|D)

m=1

(1)

where {(m)}mM=1 is a set of posterior samples drawn from P (|D), e.g. by the popular Markov Chain Monte Carlo (MCMC) methods. Traditional MCMC methods either have low-efficiency
for high dimensional sampling or scale poorly with dataset. Fortunately, the recently developed
SG-MCMC methods work on stochastic gradients over small mini-batches, which alleviate these
problems and can be applied for posterior sampling for DNNs.

3.1 SAMPLING VIA STOCHASTIC GRADIENT LANGEVIN DYNAMICS

Specifically, we choose the simplest and most widely used SG-MCMC algorithm - Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh (2011)) as the sampling method in our first stage of learning ensembles. Extension by using other high-order SG-MCMC algorithms is straightforward. SGLD calculates a stochastic gradient of negative log posterior based on St, small mini-batch of training data:

N

gt

 Ut ()

=

- |St|

iSt



log

P

(yi|xi,

)

-



log

P

()

(2)

where U () - log P (D|) - log P () is known as the potential energy in SG-MCMC sampling

and Ut() is its approxmation over the t-th mini-batch. The updating rule of SGLD is as simple as SGD with an additional Gaussian noise   N (0, tI) as following:

t+1

= t -

t
2

gt

+



(3)

where t is the learning rate or step size. By using gradient information and stochastic mini-batch updating, SGLD overcomes the problems in traditional MCMC methods and thus leads to efficient

posterior sampling.

3

Under review as a conference paper at ICLR 2018

In the following, we provide three discussions about applying SGLD to learning ensembles. First, note that SGLD is proposed with the use of annealing learning rates since SGLD does not have a Metropolis-Hastings correction step; discretization error goes to zero only when learning rates annealed to zero. In spite of that, some studies suggest to use constant learning rates in practice (Sato & Nakagawa (2014), Chaudhari et al. (2016)), which is found to give better mixing rate and make more extensive exploration of parameter space. This is also compatible with our aim of learning a good ensemble, since we want to collect diverse models. We test both annealing and constant learning rates in our experiments and find that using constant learning rates performs better, as expected. Hence, we only report the results of using constants learning rate in this work.

Second, we need to consider how to sample  from the parameter updating sequence {t}Tt=1, where T is the total number of iterations. Firstly, a burn-in process is desired. Secondly, a

thinned

collection

of

samples

{ kT M

}Mk=1

performs

better

than

other

strategies

like

backward

col-

lection {t}tT=T -M+1, since there are lower correlations between samples. Our preliminary results

as well as the results from Gan et al. (2016) both hold for that, so we take thinned collection as the

default setting in this work.

Finally, we need to consider how long to run the sampling algorithm and how many models are used for model averaging. The fixed-scale additional noise in SGLD generally reduces overfitting, thus longer running can be allowed in order to better explore the parameter space. As shown by the empirical result in Fig. 3(b), the SGLD learning method indeed can improve performance by averaging more models than other traditional methods of learning ensembles.

3.2 PRUNING AND RETRAINING

After all the models are collected, we come to the second stage of learning DNN ensembles - network pruning and retraining. We use a simple pruning rule, i.e. finding the network connections whose weights are below certain threshold and removing them away, as did in (Han et al. (2015b)). The threshold is determined by the configured overall sparsity or pruning ratio, e.g. 90%, after sorting the weights by their absolute values.

Once the network is pruned, the posterior changes from P (|D) to the reduced posterior Pr(m)(r(m)|D), where m is the index of the pruned network. Retraining is then performed for each pruned network:

^r(m)

=

arg

max
r(m)

log

Pr(m)(r(m)|D),

m = 1, 2, . . . , M

(4)

We thus obtain an ensemble of networks {^r(m)}Mm=1, which are in fact maximum a posterior (MAP) estimates under the reduced posteriors.

The effect of pruning is to reduce the model size as well as the computation cost. Interestingly, it is found in our experiments that retraining of the sampled models, whether being pruned or not, significantly improve the performance of the ensemble, although theoretically, model averaging according to Equ. (1) does not need retraining. Presumably, this is because that the number of samples used is small in our experiments. Futhermore, if we denote by ¯r(m) the network just after pruning but before retraining, it can be seen that the MAP estimate ^r(m) is more likely than ¯r(m) under the reduced posterior. Note that the probability of ¯r(m) under the reduced posterior is close to the probability of ¯(m) under the original posterior, since we only prune small network weights. So retraining increases the posteriori probabilities of the networks in the ensemble and hopefully improves the prediction performance of the networks in the ensemble.

4 SPARSE STRUCTURED ENSEMBLES
The main computation for training or testing a DNN is the large amount of matrix calculations, which are commonly accelerated using a GPU hardware. However, a randomly pruned network is not friendly for GPUs to handle, since the randomly positioned zeros in weight matrix still require floating-point operations (FLOP) without special treatment. In our Sparse Structured Ensembles (SSEs), we take this into consideration and aim at learning structures for reducing FLOP in the sense of matrix calculations.

4

Under review as a conference paper at ICLR 2018

4.1 GROUP SPARSE PRIOR

In optimization, a regularization term is often used as a penalty on the objective function to do tradeoff between minimizing a loss function and choosing a desirable model with certain constraints. The group Lasso regularization proposed by Yuan & Lin (2006) deals with the task to do feature selection in group level, which means keeping or removing all the parameters simultaneously in a group to achieve structured sparsity corresponding to grouping strategy. It can be formulated as:

G
R() = 
g=1

|g| g 2

(5)

where g is a group of partial weights in , G is the number of groups and · 2 denotes the l2 norm.

The term |g| ensures that each group gets regularized uniformly corresponding to its dimension |g|. The coefficient  is a hyperparameter to do trade off between gaining group sparsity and
minimizing the loss function. While in training, the gradient of each component can be calculated

by

 |g| g 2 = g

|g |

g g 2

(6)

A small constant could be added to g 2 in order to avoid the denominator being zero. In our experiments, we find g 2 fluctuate near zero and thus do not add the constant.

In Bayesian inference framework, the regularization term corresponds to the negative log prior term - log P () in the potential energy U (), thus the group Lasso regularization can be converted into
a specific prior as follows:

1 1G

P () = exp(-R()) = exp(- ZZ

|g| g 2)

g=1

(7)

where Z is a normalization constant. The gradient term - log P () in Equ. (2) for SGLD parameter updating can be directly calculated via Equ. (7), without the use of complex hierarchical decomposition form for the prior as the variational methods do (Marlin et al. (2009); Babacan et al. (2014)). We call it a Group Sparse Prior (GSP) as named in Marlin et al. (2009).

4.2 GROUPING STRATEGIES

To learn sparse structured networks for our SSE, it is necessary to specify grouping strategy according to the characteristic of different kinds of neural networks. In this paper, we show how to learn SSE for both FNN and LSTM. Their grouping strategies are described separately in the following.

Feed-forward Neural Network: For FNN, we group all the outgoing connections from a single neuron (input or hidden) together following Scardapane et al. (2017). Since FNN's simple hierarchical structure, if a neuron's outputs are all zeros, it makes no contribution to the next layer and can be removed. This leads to node pruning instead of random connection pruning, which reduces the rows and columns of weight matrices between layers, thus leading to lower matrix-level FLOP as expected. We can also group the incoming connections of a neuron, but the neurons with no incoming weights are still required to shift their biases to the next layer, which is a bit more complex than the above strategy we choose.

Long Short-term Memory: The case is not that simple for LSTM, since the input and hidden units

are used four times when calculating the input gate, forget gate, cell updates and output gate as

follows:

ft = ([xt, ht-1]Wf + bf )

it = ([xt, ht-1]Wi + bi)

ut = tanh([xt, ht-1]Wu + bc) ot = ([xt, ht-1]Wo + bo)

(8)

ct = ft ct-1 + it ut

ht = ot tanh(ct)

where all the vectors are row vectors, (·) is the sigmoid function, [·, ·] denotes concatenating horizontally and is element-wise multiplication. Removing an input or hidden unit is difficult for LSTM since every unit affects all the updating steps. However, note that the weight matrix between units and each gate is still fully-connected, reducing the matrix size by removing a row or column

5

Under review as a conference paper at ICLR 2018

away still makes sense. Specifically, we keep two index lists during pruning to record the remained rows and columns for each weight matrix. When doing computations, just use partial units to update partial dimensions of the gates according to the index lists. This is flexible for different units to provide updating for different gate dimensions.

Thus, our first grouping strategy is to group each row and each column for the four weight matrices

separately in formula 8. Note that the group sparse prior generally selects or removes a certain group,

it's ok to make groups overlapped for reducing matrix size. We choose this as our default strategy

(untied strategy) since the basic LSTM cell implementation in TensorFlow conducts calculation as

in Equ. (8). Alternatively, the LSTM updating formulas can be written as in Gal & Ghahramani

(2016):

 it    

 

ut ft

= 

tanh 

 

ot 

xt ht-1

·W

(9)

where W is a matrix of dimension 2n by 4n (n being the unit number of hidden state), which is the horizontally concatenation of the four weight matrices in Equ. (8). Since acceleration has been reported by concatenating matrix (Appleyard et al. (2016)), we also try to group each row and column of W as a second grouping strategy. This strategy is simpler since only two index lists are kept instead of eight, and we call it tied W strategy. An illustration of these two strategies are shown in Fig. 2.

5 EXPERIMENTS
In our experiments, we implemented the proposed method in TensorFlow, and present results in two parts: (i) learn SSE of FNNs for image classification task on MNIST; (ii) learn SSE of LSTM models for word-level language modeling task on Penn TreeBank corpus. As a specification, the sparsity in this paper means the percentage of pruned weights in total parameters, and FLOP is calculated as dense matrix multiplications without considering any zeros in it. The parameters and FLOP compared in tables are total amount of all the models in ensemble without special note. PR, GSP and SSE denotes Pruning and Retraining, Group Sparse Prior and Sparse Structured Ensemble respectively.
5.1 CLASSIFICATION ON MNIST
We firstly try our method on FNN to do classfication on the well-known MNIST dataset. We choose a commonly used network structure of 2 hidden layers with 300 and 100 hidden neurons respectively and ReLU activation, denoting as FNN-300-100. We run our experiments without any additional

  

 



 





-1

-1

-1

-1

-1

  
(a) Untied (as default) strategy



 



(b) Tied W strategy



Figure 2: Grouping strategy illustration for LSTM. The black lines in each weight matrix denote for non-zero element and the gray areas are rows and columns with all zeros, as a result of group selection by GSP. The horizontal arrows indicate the input and hidden units to get involved in calculation, and the vertical arrows point to the corresponding dimensions of each gate to be updated. Thus it's enough to do calculation by the reduced matrix of intersections in the figure instead of the whole.

6

Under review as a conference paper at ICLR 2018

Table 1: Results of FNN-300-100 ensembles on MNIST.

Method
SGD (baseline) SGD
SGLD SGLD+PR SGLD+PR
SGLD+GSP+PR (Ours) SGLD+GSP+PR (Ours)

Model
1 model 18 models
18 models 18 models, 90% sparsity 18 models, 95% sparsity
18 models, 90% sparsity 18 models, 96% sparsity

Parameters
1 18×
18× 1.8× 0.9×
1.8× 0.7×

FLOP
1 18×
18× 17.8× 12.8×
2.5× 2.2×

Test Error (%)
1.66 1.49
1.53 1.53 1.68
1.26 1.29

PR: Pruning and Retraining GSP: Group Sparse Prior

Table 2: Results of SSE learned on FNN-300-100

Model
Dense (baseline) SSE SSE

Sparsity
90% 96%

Params
266K 27K 11K

Network structure
784-300-100 380-128-24 364-82-22

FLOP
532K 75K(14%) 64K(12%)

SSE: Sparse Structured Ensemble

tricks such as dropout, batch normalization etc, and all the results reported in table 1 are the mean value of 10 independently runs.
We use Stochastic Gradient Descent (SGD) as a baseline, training for 100 epochs with an annealing learning rate of 0.5 which decays by a factor of 2 every 10 epoch. The baseline obtains 1.66% test error rate, and an ensemble of 18 independently trained networks following this setting decrease the error to 1.49%.
Then for SGLD, we train for 100 epochs equally with a fixed learning rate of 0.5. Each network sample is collected every 5 epoch after a 10 epoch burn in process. Ensemble learned by SGLD gives a result of 1.53% test error, which is slightly worse than the independently trained ensemble, since each sample drew by fixed-step-size SGLD sampling is not as precise as training through optimization. After pruning, we retrain each network in the ensemble for 20 epochs with a small learning rate 0.01 which decay by factor 1.15 every epoch. As for sparse ensemble learned by SGLD, the highest sparsity without losing accuracy is 90%, while the FLOP reduction is fairly little due to the unordered pruning. When 95% of the parameters are pruned, performance gets worse obviously. While for our method, SGLD with group sparse prior (GSP) reduces the error rate significantly from 1.53% to 1.26% with the same sparsity of 90%, on the other hand, GSP enables a higher sparity of 96% with an acceptable accuracy loss. Comparing to the result without GSP, the FLOP reduces by almost 6 times since GSP forces the pruned connections to be aligned thus remove more neurons. The detail structure information of an arbitrarily taken model from SSE is shown in table 2. Overall, the SSE of 18 networks learned by our method decreases test error from 1.66% to 1.29% with 70% of parameters and 2.2× computational cost.
5.2 LANGUAGE MODELING
We carry out more in-depth studies with LSTM model for language modeling task, since it is more changing to train RNNs. In word-level language modeling, the input are a sequence of words, and the model is trained to predict the next word. The prediction quality is measured by negative logprobability (perplexity). Penn TreeBank corpus (Marcus et al. (1993)) is a widely used benchmark
7

Under review as a conference paper at ICLR 2018

Perplexity
Perplexity

130 individual model 125 model averaging 120 individual retrained model 115 retrained model averaging
110 105 100 95 90 85 80 75 Burned in 70 65
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85
Epoch
(a) Training curve

82
sample uniformly in 80 epochs 80 sample every 6 epochs
traditional multi-training
78 76 74 72 70
68 5 10 M15odel n2u0 mber25 30 35 (b) Valid perplexity vs model number

Figure 3: Training curve and sampling strategy comparison

for language modeling task, with a vocabulary of size 10K and 929K/73K/10K words in training, validation and test sets respectively. We choose Zaremba et al. (2014) as a baseline and follow their LSTM architectures to make comparable results, while establishing our experiments based on the open source implementation on TensorFlow.
We test our methods on the medium (2 layers with 650 hidden units each) and large (2 layers with 1500 hidden units each) LSTM models specificed in Zaremba et al. (2014). The dimension of word embedding as input is the same as hidden size. All the models are trained together with the dropout technique introduced in Zaremba et al. (2014), and experiments without GSP just follow their dropout keep ratio which are 0.5 and 0.35 for medium and large model respectively. When applying GSP, a higher dropout keep ratio is desired, which are 0.7 for medium and 0.5 for large model. Since GSP and dropout are both some form of regularization and regularizing too much will lead to underfitting. The  for both untied and tied case of GSP are  = 4.2 × 10-5 and  = 2.3 × 10-5 respectively for medium and large model. The learning rates for medium and large model are fixed to 1.5 and 1.0 respectively for SGLD training, while decaying by a factor of 1.25 for retraining by SGD. All the hyperparameter settings above are found empirically via grid search on validation sets.
The results are organized as four parts. Firstly we study the contribution of each component in our algorithm by a series of comparative experiments, then we discuss how the number of model affects ensemble performance. The third part is some analyses of the compression effects of SSE in details. At last several best results obtained by our method are reported and compared.
Table 3 lists the results of SGLD training with different combinations of pruning, retraining and GSP (untied case as default). All the ensembles consist of 10 medium models. As a result, applying PR

Table 3: Different combinations of each component in our method tested on medium model.

Method
SGD (Zaremba, 2014) SGD (Zaremba, 2014)
SGLD SGLD+PR SGLD+GSP SGLD+GSP+R SGLD+GSP+PR(Ours) SGLD+GSP+PR(Ours, tied W)

Model
1 10
10 10 10 10 10 10

Params
1 10×
10× 1× 10× 10× 1× 1×

FLOP
1 10×
10× 10× 10× 10× 4× 4×

Individual model

Valid Test

86.2 82.1 --

87.0 103.8 98.8 80.0 79.8 79.7

83.7 100.2 97.0 76.3 76.6 76.6

Model averaging
Valid Test
-75.2 72.0
80.5 78.9 91.1 89.4 88.0 86.9 70.8 69.1 71.5 69.5 70.9 69.2

8

Under review as a conference paper at ICLR 2018

Table 4: Results of SSE learned on large LSTM model.

Method
Baseline SSE SSE, tied W

Sparsity
90% 90%

Params
66M 6.6M 6.6M

LSTM1
36M 4.3M 4.9M

FLOP

LSTM2 Softmax

36M 6.1M 6.1M

30M 12.2M 12.4M

Total
102M 22.7M (22%) 23.5M (23%)

or GSP alone both lead to worse performance than vanilla SGLD, showing that these two stuff have a positive interaction towards each other. When they are applied together, GSP forces the network to learn group sparse structures which are highly robust to pruning thus lead to a better result. This result beats the traditional multiple-trained ensemble with only 10% parameters and 25% FLOP in total. The result of GSP with only retraining is slightly better than added with pruning, which meets the analysis in section 3.2. Pruning without retraining harms the performance too much hence is not listed. The two group strategies perform similar both in prediction quality and FLOP, which requires additional test in real implementations. The column of individual model denotes the lowest perplexity obtained by a single model in the ensemble, which to show that the prediction quality of an ensemble is strongly depend on its individual components.
Fig.3(a) is the training curve of our method, showing the performance improvements brought by model averaging and PR process apparently. The relationship between ensemble performance and model number are illustrated in Fig.3(b), together with a comparison between sampling strategies. We test the performance of ensemble on valid set consisting of 2 to 35 medium models. The blue curve shows the result of running SGLD for 80 epochs and sampling uniformly after a 10-epoch burn in process, e.g. sampling every 2 epoch to collect 35 models and sampling every 35 epoch to get 3 models. The orange curve alternatively obtained by sampling every 6 epochs, which means the more model collected the longer run of SGLD. The intersection of these two curve shows that 1) it's better to sample with larger interval for relatively small model number; 2) when do averaging with more models, a trade off towards performance and training cost should be considered. Comparing to the curve of traditional method which improves slightly after 15 models, our method shows a greater potential to achieve better performance by increasing cost.
The sparse structured pattern learned by untied and tied GSP are shown in figure 4. The white strips are the removed rows and columns as a result of applying GSP, while the white point in black areas shows the reduced matrices are sparse too to some extent. Table 4 shows FLOP of each layer for a randomly picked large model in SSE. Note that word embedding is not included for FLOP calculation since it is a lookup process instead of matrix calculation in practical, but we still prune it to save more memory.

LSTM layer 1

LSTM layer 1

LSTM layer 2

LSTM layer 2

(a) Untied weights

(b) Tied weights

Figure 4: Sparse structured pattern with LSTM layer 1 up and LSTM layer 2 down

9

Under review as a conference paper at ICLR 2018

Our best results are given in table 5. The lowest perplexity achieved are 66.4 by SSE of 20 large models and 66.3 by SSE of 35 medium models, which should be the best results obtained on standard LSTM model to the best of our knowledge. We also make attempts to use less models to do trade off between cost and performance. The most practical working point is the SSE of 4 large models, which only requires 40% of parameters and 90% of computational amount in total comparing to the individual baseline model, but decrease perplexity from 78.4 to 68.6. This result is consistent with those obtained by Zaremba et al. (2014) (38 independently trained large models) and Gal & Ghahramani (2016) (10 independently trained large models with costly MC dropout for testing), but tremendously reduce the memory and computational cost. Lower perplexities on PTB corpus can be achieved by applying our method on more advanced models such as Recurrent Highway Networks (RHNs, Zilly et al. (2016)) and Pointer Sentinel-LSTM (Merity et al. (2016)).
6 CONCLUSION AND FUTURE WORKS
In this work, we propose a novel method of learning NN ensembles efficiently and cost-friendly by integrating three mutually enhanced techniques: SG-MCMC sampling, group sparse prior and network pruning. The resulting method is easy to implement, yet surprisingly effective. This is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs. The Sparse Structured Ensembles (SSEs) learned by our method gain better prediction performance with reduced training and test cost when compared to traditional methods of learning NN ensembles. Moreover, the method can also be used to produce SSE, which outperforms baseline NN significantly without increasing the model size and computation cost.
Some interesting future works: (1) interleaving model sampling and model pruning; (2) application of this new method, as a new powerful tool of learning ensembles, to more tasks.
ACKNOWLEDGMENTS
REFERENCES
Jeremy Appleyard, Tomas Kocisky, and Phil Blunsom. Optimizing performance of recurrent neural networks on gpus. arXiv preprint arXiv:1604.01946, 2016.
S Derin Babacan, Shinichi Nakajima, and Minh N Do. Bayesian group-sparse modeling and variational inference. IEEE transactions on signal processing, 62(11):2906­2921, 2014.
Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pp. 3438­3446, 2015.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.

Table 5: Top and comparable results of LSTM LMs on PTB dataset.

Method
SGD (Zaremba, 2014) SGD (Zaremba, 2014) SGD (Zaremba, 2014) VD (Gal, 2016) SGLD
SGLD+GSP+PR (Ours) SGLD+GSP+PR (Ours, tied W) SGLD+GSP+PR (Ours) SGLD+GSP+PR (Ours, tied W) SGLD+GSP+PR (Ours) SGLD+GSP+PR (Ours)

Model
1 large 10 large 38 large 10 large 10 large
4 large 4 large 10 large 10 large 20 large 35 medium

Params
1 10× 38× 10× 10×
0.4× 0.4× 1.0× 1.0× 2.0× 1.1×

FLOP
1 10× 38×
10×
0.9× 0.9× 2.3× 2.5× 4.5× 3.3×

Valid
82.2 72.8 71.9
79.0
70.9 70.9 69.2 69.1 68.6 68.4

Test
78.4 69.5 68.7 68.7 77.9
68.7 68.6 67.4 67.2 66.4 66.3

10

Under review as a conference paper at ICLR 2018
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In International Conference on Machine Learning, pp. 1683­1691, 2014.
Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141­142, 2012.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019­1027, 2016.
Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu, Qinliang Su, and Lawrence Carin. Scalable bayesian learning of recurrent neural networks for language modeling. arXiv preprint arXiv:1611.08034, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135­1143, 2015b.
Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao, Yu Wang, et al. Ese: Efficient speech recognition engine with sparse lstm on fpga. In FPGA, pp. 75­84, 2017.
Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10):993­1001, 1990.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017.
Cheng Ju, Aure´lien Bibaut, and Mark J van der Laan. The relative performance of ensemble methods with deep convolutional neural networks for image classification. arXiv preprint arXiv:1704.01664, 2017.
Chunyuan Li, Changyou Chen, David E Carlson, and Lawrence Carin. Preconditioned stochastic gradient langevin dynamics for deep neural networks. In AAAI, volume 2, pp. 4, 2016.
Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. arXiv preprint arXiv:1608.03983, 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313­330, 1993.
Benjamin M Marlin, Mark Schmidt, and Kevin P Murphy. Group sparse priors for covariance estimation. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 383­392. AUAI Press, 2009.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
Sharan Narang, Gregory Diamos, Shubho Sengupta, and Erich Elsen. Exploring sparsity in recurrent neural networks. arXiv preprint arXiv:1704.05119, 2017.
11

Under review as a conference paper at ICLR 2018
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 982­990, 2014.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81­89, 2017.
Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and fluctuations for stochastic gradient langevin dynamics. Journal of Machine Learning Research, 17:1­33, 2016.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681­688, 2011.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49­67, 2006.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ik, and Ju¨rgen Schmidhuber. Recurrent highway networks. arXiv preprint arXiv:1607.03474, 2016.
12

