Under review as a conference paper at ICLR 2018
Transformation Autoregressive Networks
Anonymous authors Paper under double-blind review
Abstract
The fundamental task of general density estimation has been of keen interest to machine learning. Recent advances in density estimation have either: a) proposed a flexible model to estimate the conditional factors of the chain rule, p(xi xi-1, . . .); or b) used flexible, non-linear transformations of variables of a simple base distribution. Instead, this work jointly leverages transformations of variables and autoregressive conditional models, and proposes novel methods for both. We provide a deeper understanding of our methods, showing a considerable improvement through a comprehensive study over both real world and synthetic data. Moreover, we illustrate the use of our models in outlier detection and image modeling tasks.
1 Introduction
Density estimation is at the core of a multitude of machine learning applications. However, this fundamental task, which encapsulates the understanding of data, is difficult in the general setting due to issues like the curse of dimensionality. Furthermore, general data, unlike spatial/temporal data, does not contain a priori known correlations among covariates that may be exploited and engineered with. For example, image data has known correlations among neighboring pixels that may be hard-coded into a model, whereas one must find such correlations in a data-driven fashion with general data.
In order to model high dimensional data, a large number of methods have considered autoregressive models, which model the conditional factors of the chain rule (23; 22; 11; 13). I.e these models estimate the conditionals: p(xi xi-1, . . . , x1), for i  {1, . . . , d}. While some methods directly model the conditionals p(xi xi-1, . . .) using sophisticated semiparametric density estimates, other methods apply sophisticated transformations of variables x  z and take the conditionals over z to be a restricted, often independent base distribution p(zi zi-1, . . .)  f (zi) (8; 9; 12). In this paper we take a step back from these previous approaches that have considered either: a) a flexible autoregressive scheme with simple or no transformations of variables (Figure 1a); or b) a simple autoregressive scheme with flexible transformations of variables (Figure 1b) . We leverage both of these approaches (Figure 1c), develop novel methods for each, and show a considerable improvement with their combination.
Contributions: The following are our contributions. First, we propose two flexible autoreggressive models for modeling conditional distributions: the linear autoregressive model (LAM), and the recurrent autoregressive model (RAM). LAM employs a simple linear form to condition on previously seen covariates in a flexible fashion. RAM uses a recurrent neural network (RNN) to evolve conditioning features as the set of conditioning covariates expands. Furthermore, this paper proposes several novel transformations of variables: 1) we propose an efficient method for learning a linear transfromations on covariates; 2) we develop an invertible RNN-based transformation that directly acts on covariates; 3) we also propose an additive RNN-base transformation. To better capture correlations in general data, we combine our novel autoreggresive models and transformations of variables. That is, we propose transformation autogressive networks (TANs) that use a flexible transformation over the original covariates and a powerful autoregressive model to estimate the conditional probabilities over the transformed space. To assess the efficacy of models, we performed a comprehensive evaluation of autoregressive models and transformations. Extensive experiments on both real and synthetic datasets show the power of TANs for capturing complex
1

Under review as a conference paper at ICLR 2018

dependencies between the covariates. Moreover, we show that the learned model can be used for outlier detection, and image modeling.
The remainder of the paper is structured as follows. First, in Section 2.1 we present two novel methods for modeling condition distributions across covariates. Next, in Section 2.2, we describe several transformations to use in conjunction with our proposed conditional models. After, we discuss related work and contrast our approach to previous methods. We then illustrate the efficacy of our methods with both synthetic and real-world data.

 zi-1
 xi  zi
  zi+1

p(zi) p(zi+1) p(zi+2)

 xi-1  p(xi xi-1, . . .)
 xi  xi  p(xi+1 xi, . . .)
  xi+1  p(xi+2 xi+1, . . .)

(a) Flexible transformation,
restricted conditionals.

(b) Restricted transformation, flexible
conditionals.

 zi-1  p(zi zi-1, . . .)





xi  zi  p(zi+1 zi, . . .)





 zi+1  p(zi+2 zi+1, . . .)

(c) Our approach: flexible
transformation, flexible conditionals.

Figure 1: Illustration from left to right: (a) a flexible transformations of variables with an independent autoregressive scheme; (b) no transformations of variables with a flexible autoregressive scheme; and (c) a transformation autogressive network (TAN) that has a flexible transformation and a flexible autoregressive scheme.

2 Transformation Autoregressive Networks
First, we propose two autoregressive models to estimate the conditional distribution of input covariates x  Rd. After, we shall show how we may use such models over a transformation z = q(x), while renormalizing to obtain density values for x.

2.1 Autoregressive Models

Autoregressive models decompose density estimation of a multivariate variable x  Rd into multiple conditional tasks on a growing set of inputs through the chain rule:

d
p(x1, . . . xd) = p(xi xi-1, . . . , x1).
i=1

(1)

That is, autoregressive models will look to estimate the d conditional distributions
p(xi xi-1, . . .). A particular class of autoregressive models can be defined by approximating conditional distributions through a mixture model, MM((xi-1, . . . , x1)), with parameters depending on xi-1, . . . , x1:

p(xi xi-1, . . . , x1) = p(xi MM((xi-1, . . . , x1)), (xi-1, . . . , x1) = f (hi) hi = gi (xi-1, . . . , x1) ,

(2) (3) (4)

where f () is a fully connected network that may use a element-wise non-linearity on inputs, and gi() is some general mapping that computes a hidden state of features, hi  Rp, which help in modeling the conditional distribution of xi xi-1, . . . , x1. One can control the flexibility of the autoregressive model through gi. It is important to be powerful enough to model our covariates while still generalizing. In order to achieve this we propose two methods for
modeling gi.

First, we propose the linear autoregressive model (LAM), using a straightforward linear map as gi (eq. 4):

gi (xi-1, . . . , x1) = W (i)x<i + b,

(5)

2

Under review as a conference paper at ICLR 2018

where W (i)  Rp×(i-1), b  Rp, and x<i = (xi-1, . . . , x1)T . Notwithstanding the simple form of (eq. 5), the resulting model is quite flexible as it may model consecutive conditional problems
p(xi xi-1, . . . , x1) and p(xi+1 xi, . . . , x1) very differently.

Next we propose the recurrent autoregressive model (RAM), which features a recurrent
relation between gi's. Given the expanding set of covariates progressively fed into gi's, it is natural to consider a hidden state that evolves according to an RNN recurrence relationship:

hi = g (xi-1, g(xi-2, . . . , x1)) = g (xi-1, hi-1) ,

(6)

where g(x, s) is a RNN function for updating one's state based on an input x and prior state s (see Figure 2). In the case of gated-RNNs, the model will be able to scan through previously seen dimensions remembering and forgetting information as needed for conditional densities without making any strong Markovian assumptions.



h1 = b



 x1 h2 = W (2)x1 + b



 x2 h3 = W (3)x<3 + b





 xd-1

hd = W (d)x<d + b



h1 = g(, h0)



x1


h2 = g(x1, h1)



x2


h3 = g(x3, h2)







 xd-1

hd = g(xd-1, hd-1)

(a) LAM

(b) RAM

Figure 2: Illustration of both LAM (left) and RAM (right) models. Hidden states hk's are updated and then used to compute the parameters of the next conditional density for xk. Note that in LAM the hidden states hj's are not tied together, where in RAM the hidden state hj along with xj are used to compute the hidden state hj+1 which determines the parameters of p(xj+1 hj+1).
Both LAM and RAM are flexible and able to adjust the hidden states, hi (eq. 4), to model the distinct conditional tasks p(xi xi-1, . . .). However, there is a trade-off of added flexibility and transferred information between the two models (see Figure 2). LAM treats the conditional tasks for p(xi xi-1, . . .) and p(xi+1 xi, . . .) in a largely independent fashion. This makes for a very flexible model, however the parameter size is also large and there is no sharing of information among the conditional tasks. On the other hand, RAM provides a framework for transfer learning among the conditional tasks by allowing the hidden state hi to evolve through the distinct conditional tasks. This leads to fewer parameters and more sharing of information in respective tasks, but also yields less flexibility since conditional estimates are tied, and may only change in a smooth fashion.

2.2 Transformations

Several methods (8; 9; 12) have shown that the expressive power of very simple conditional
densities (eq. 1) (such as independent Gaussians) can be greatly improved with transfor-
mations of variables. Although the chain rule holds for arbitrary distributions, a limited
amount of data and parameters limits the expressive power of models. Hence, we expect that
combining our conditional models with transformations of variables will also further increase flexibility. When using an invertible transformation of variables z = (q1(x), . . . , qd(x))  Rd, one renormalizes the pdf of x as:

p(x1, . . . xd) =

dq det
dx

d
p (qi(x) qi-1(x), . . . , q1(x)) ,
i=1

(7)

3

Under review as a conference paper at ICLR 2018

where

det

dq dx

is a normalizing factor of the Jacobian of the transformation.

For analytical and computational considerations, we require transformations that are invertible, efficient to compute and invert, and have a structured Jacobian matrix. In order to meet these criteria we consider the following transformations.

Linear Transformation: First, we propose a linear transformation:

z = Ax + b,

(8)

where we take A to be invertable. Note that even though this linear transformation is simple,

it includes permutations, and may also perform a PCA-like transformation, capturing coarse

and highly varied features of the data before moving to more fine grained details. In order

to not incur a high cost for updates, we wish to compute the determinant of the Jacobian

efficiently. We do so by directly working over an LU decomposition A = LU where L is a

lower triangular matrix with unit diagonals and U is a upper triangular matrix with arbitrary

diagonals.

As

a

function

of

L,

U

we

have

that

det

dz dx

= id=1 Uii;

hence

we

may

efficiently

optimize the parameters of the linear map. Furthermore, inverting our mapping is also

efficient through solving two triangular matrix equations.

Recurrent Transformation: Recurrent neural networks are also a natural choice for variable transformations. Due to their dependence on only previously seen dimensions, RNN transformations have triangular Jacobians, leading to simple determinants. Furthermore, with an invertible output unit, their inversion is also straight-forward. We consider the following form to an RNN transformation:

zi = r yxi + wT si-1 + b , si = r uxi + vT si-1 + a ,

(9)

where r is a leaky ReLU unit r(t) = I{t < 0}t + I{t  0}t, r is a standard ReLU unit, s  R is the hidden state y, u, b a are scalars, and w, v  R are vectors. As compared to the linear transformation, the recurrent transformation is able to transform the input with different dynamics depending on its values. Inverting (eq. 9) is a matter of inverting outputs and updating the hidden state (where the initial state s0 is known and constant):

xi = y-1 r-1 zi(r) - wT si-1 - b , si = r uxi + vT si-1 + a .

(10)

Furthermore, the determinant of the Jacobian for (eq. 9) is the product of diagonal terms:

det

dz dx

=

yd

d
r
i=1

yxi + wT si-1 + b

,

where r (t) = I{t > 0} + I{t < 0}.

(11)

Recurrent Shift Transformation: It is worth noting that the rescaling brought on by the recurrent transformation effectively incurs a penalty through the log of the determinant (eq. 11). However, one can still perform a transformation that depends on the values of covariates through a shift operation. In particular, we propose an additive shift based on a recurrent function on prior dimensions:

zi = xi + m(si-1), si = g(xi, si-1),

(12)

where g is some recurrent function for updating states, and m is a fully connected network. Inversion proceeds as before:

xi = zi - m(si-1), si = g(xi, si-1), .

(13)

The Jacobian is again lower triangular, however due to the additive nature of (eq. 12), we

have

a

unit

diagonal.

Thus,

det

dz dx

= 1.

One

interpretation

of

this

transformation

is

that

one

can shift the value of xk based on xk-1, xk-2, . . . for better conditional density estimation

without any penalty coming from the determinant term in (eq. 7).

4

Under review as a conference paper at ICLR 2018

Composing Transformations: Lastly, we considering stacking (i.e. composing) several transformations q = q(1)  . . .  q(T ) and renormalizing:

T
p(x1, . . . xd) =
t=1

dq(t) det dq(t-1)

d
p (qi(x) qi-1(x), . . . , q1(x)) ,
i=1

(14)

where we take q(0) to be x. We note that composing several transformations together allows
one to leverage the respective strengths of each transformation. Moreover, inserting a reversal transformation (x1, . . . , xd  xd, . . . , x1) in between transformations yields biderectional relationships for several transformations.

2.3 Combined Approach

We combine the use of both transformations of variables and rich autoregressive models by: 1) writing the density of inputs, p(x), as a normalized density of a transformation: p(q(x)) (eq. 14). Then we estimate the conditionals of p(q(x)) using an autoregressive model. I.e. to learn our model we minimize the negative log likelihood:

T
- log p(x1, . . . xd) = - log
t=1

dq(t) det dq(t-1)

d
- log p (qi(x)
i=1

hi) ,

(15)

which is obtained by substituting (eq. 2) into (eq. 14) with hi as defined in (eq. 4).

3 Related Work

Nonparametric density estimation has been a well studied problem in statistics and machine learning (24). Unfortunately, nonparametric approaches like kernel density estimation suffer greatly from the curse of dimensionality and do not perform well when data does not have a small number of dimensions (d  3). To alleviate this, several semiparametric approaches have been explored. Such approaches include forest density estimation (16), which assumes that the data has a forest (i.e. a collection of trees) structured graph. This assumption leads to a density which factorizes in a first order Markovian fashion through a tree traversal of the graph. Another common semiparametric approach is to use a nonparanormal type model (15). This approach uses a Gaussian copula with a rank-based transformation and a sparse precision matrix. While both approaches are well-understood theoretically, their strong assumptions often lead to inflexible models.

In order to provide greater flexibility with semiparametric models, recent work has employed deep learning for density estimation. The use of neural networks for density estimation dates back to early work by (6) and has seen success in areas like speech (25; 21), music (7), etc.. Typically such approaches use a network to learn to parameters of a parametric model for data. Recent work has also explored the application of deep learning to build density estimates in image data (19; 9). However, such approaches are heavily reliant on exploiting structure in neighboring pixels, often subsampling, reshaping or re-ordering data, and using convolutions to take advantage of neighboring correlations. Modern approaches for general density estimation in real-valued data include (23; 22; 11; 13; 8).

NADE (23) is an RBM-inspired density estimator with a weight-sharing scheme across conditional densities on covariates. It may be written as a special case of LAM (eq. 5) with:

qi (xi-1, . . . , x1) = W<ix<i + b,

(16)

where W<i  Rp×i-1 is the weight matrix compose of the first i - 1 columns of a shared matrix W = (w1, . . . wd). We note also that LAM and NADE models are both related to fully visible
sigmoid belief networks (10; 17).

Even though the weight-sharing scheme in (eq. 16) reduces the number of parameters, it also greatly limits the types of distributions one can model. Roughly speaking, the NADE weight-sharing scheme makes it difficult to adjust conditional distributions when expanding the conditioning set with a covariate that has a small information gain. We illustrate these kinds of limitations with a simple example. Consider the following 3-dimensional distribution:

x1  N (0, 1), x2  N (sign(x1), ), x3  N (I { x1 < C0.5} , )

(17)

5

Under review as a conference paper at ICLR 2018

where C0.5 is the 50% confidence interval of a standard Gaussian distribution, and > 0 is some small constant. That is, x2, and x3 are marginally distributed as an equi-weighted bimodel mixture of Gaussian with means -1, 1 and 0, 1, respectively. Due to NADE's
weight-sharing linear model, it will be difficult to adjust h2 and h3 jointly to correctly model x2 and x3 respectively. However, given their additional flexibility, both LAM and RAM are able to adjust hidden states to remember and transform features as needed.

NICE models assume that data is drawn from a latent independent Gaussian space and
transformed (8). The transformation uses several "additive coupling" shifting transformations
on the second half of dimensions, using the first half of dimensions. That is, additive coupling proceeds by splitting inputs into halves x = (x<d 2, xd 2), and transforming the second half as an additive function of the first half:

z = x<d 2, xd 2 + m(x<d 2) ,

(18)

where m() is the output of a fully connected network. Inversion is simply a matter of

subtraction x = z<d 2, zd 2 - m(z<d 2) . The full transformation is the result of stacking several of these additive coupling layers together followed by a final rescaling operation.

Furthermore, as with the RNN shift transformation, the additive nature of (eq. 18) yields a

simple

determinant,

det

dz dx

=

1.

We also note that are several methods for obtaining samples from an unknown distribution

that by-pass density estimation. For instance, generative adversarial networks (GANs) apply

a (typically noninvertible) transformation of variables to a base distribution by optimizing a

minimax loss over a discrimator and the transformation (12). Furthermore, one can also

obtain samples with only limited information about the density of interest. For example, if

one has an unnormalized pdf, one may use Markov chain Monte Carlo (MCMC) methods to

obtain samples (18).

4 Experiments
We compare models using several experiments on synthetic and real-world datasets. First, we compute the average log likelihoods on test data. Then, to gain further context of the efficacy of models, we also use their density estimates for anomaly detection, where we take low density instances to be outliers. Moreover, we look at an illustrative MNIST image modeling task.
We study the performance of various combinations of conditional models and transformation. That is, we consider various models for the conditionals p (qi(x) hi) and various transformations q() (eq. 15). In particular the following conditional models were considered: LAM, RAM, Tied, MultiInd, and SingleInd. Here, LAM, RAM, and Tied are as described in equations (eq. 5), (eq. 6), and (eq. 16), respectively. MultiInd takes p (qi(x) hi) to be p (qi(x) MM(i)), that is we shall use d distinct independent mixtures to model the transformed covariates. Similarly, SingleInd takes p (qi(x) hi) to be p (qi(x)), the density of a standard single component. Moreover, we considered the following transformations: None, RNN, 2xRNN, 4xAdd+Re, 4xSRNN+Re, RNN+4xAdd+Re, and RNN+4xSRNN+Re. None indicates that no transformation of variables was performed. RNN and 2xRNN performs a single recurrent transformation (eq. 9), and two recurrent transformations with a reversal permutation in between, respectively. Following (8), 4xAdd+Re performs four additive coupling transformations (eq. 18) with reversal permutations in between followed by a final element-wise rescaling: x  x  exp(s), where s is a learned variable. Similarly, 4xSRNN+Re, performs four recurrent shift transformations (eq. 12) with reversal permutations in between, followed by an element-wise rescaling. RNN+4xAdd+Re, and RNN+4xSRNN+Re are as before, but performing an initial recurrent transformation. Furthermore, we also considered performing an initial linear transformation (eq. 8). We flag this by prepending an L to the transformation; e.g. L RNN denotes a linear transformation followed by a recurrent transformation.
Models were implemented in Tensorflow (4). Both RAM conditional models as well as the RNN shift transformation make use of the standard GRUCell GRU implementation1. We
1Code will be made public upon publication.

6

Under review as a conference paper at ICLR 2018

take the mixture models of conditionals (eq. 2) to be mixtures of 40 Gaussians. We optimize all models using the AdamOptimizer (14) with an initial learning rate of 0.005. Training consisted of 30 000 iterations, with mini-batches of size 256. The learning rate was decreased by a factor of 0.1, or 0.5 (chosen via a validation set) every 5 000 iterations. Gradient clipping with a norm of 1 was used. After training, the best iteration according to the validation set loss was used to produce the test set mean log likelihoods.

4.1 Synthetic
We perform a thorough empirical analysis over synthetic data. By carefully constructing data we will be able to pinpoint strengths and short-comings of conditional models and transformations. We study a dataset with a first-order Markovian structure, and one with a star-shaped structure; they are described below.

4.1.1 Markovian Data
First, we describe experiments performed on a synthetic dataset with a Markovian structure that features several exploitable correlations among covariates. The dataset is sampled as follows: y1, y2, y3  N (0, 1) and yi yi-1, . . . , y1  f (i, y1, y2, y3) + i for i > 3 where i  N ( i-1, ), f (i, y1, y2, x3) = y1 sin(y2gi + y3), and gi's are equi-spaced points on the unit interval. That is, instances are sampled using random draws of amplitude, frequency, and shift covariates y1, y2, y3, which determine the mean of the other covariates, y1 sin(y2gi + y3), stemming from function evaluations on a grid, and random noise i with a Gaussian random walk. The resulting instances are easy to visualize, and contain many correlations among covariates (for instance, y4 is highly informative of y5). To test robustness to correlations from distant (by index) covariates, we observe covariates that are shuffled using a fixed permutation  chosen ahead of time: x = (y1 , . . . , yd ). We take d = 32, and the number of training instances to be 100 000.
We detail the mean log-likelihoods on a test set for TANs using various combinations of conditional models and transformations in Appendix, Table 4. Note that the performance of previous one-prong approaches that considered a complex conditional model with simple or no transformation and vice-versa are illustrated by None & Tied (NADE), 4xAdd+Re & SingleInd (NICE) models, as well as by the entire row corresponding to None transformation and the MultiInd and SingleInd columns. We see that both LAM and RAM conditionals are providing most of the top models. We observe good samples from the best performing model (picked on validation dataset) as shown in Figure 3. Here we also observe relatively good performance stemming from MultiInd conditionals with more complex transformations.

values values values values

1.0 sample

y1sin(y2gi + y3)

0.5

0.0

0.5

1.0 5 10 15 20 25 30 y4, , y32

sample
0.6 0.4 0.2 0.0 0.2

y1sin(y2gi + y3)

5 10 15 20 25 30 y4, , y32

sample

y1sin(y2gi + y3)

0.6

0.4

0.2

0.0

0.2

5 10 15 20 25 30 y4, , y32

sample 1.5

y1sin(y2gi + y3)

1.0

0.5

0.0

0.5

1.0 5 10 15 20 25 30 y4, , y32

Figure 3: RNN+4xSRNN+Re & RAM model samples. Each plot shows a single sample. We plot the sample values of unpermuted dimensions y4, . . . , y32 y1, y2, y3 in blue and the expected value of these dimensions (i.e. without the Markovian noise) in green. One may see that the
model is able to correctly capture both the sinusoidal and random walk behavior of our data.

4.1.2 Star Data
Next, we consider a dataset with a star-structured graphical model where fringe nodes are very uninformative with each-other. We divide the covariates into disjoint center and vertex sets C = {1, . . . , 4}, V = {5, . . . , d} respectively. For center nodes j  C, yj  N (0, 1). Then, for j  V , yj  N (fj(wjT yC ), ) where fj is a fixed step function with 32 intervals, wj  R4 is
7

Under review as a conference paper at ICLR 2018

a fixed vector, and yC = (y1, y2, y3, y4). We note that this dataset poses a difficult density estimation problem since the distribution of each of the fringe vertices will be considerably different from each other, the fringe vertices are largely uninformative from one another, and the distribution of the fringe vertices are difficult to estimate without conditioning on all the center nodes. As before we observe covariates that are shuffled using a fixed permutation  chosen ahead of time: x = (y1 , . . . , yd ), with d = 32.
We detail the mean log-likelihoods on a test set for TANs using various combinations of conditional models and transformations in the Appendix, Table 5. Once more we observe that both LAM and RAM conditionals are providing most of the top models. In this dataset, however, simpler conditional methods are unable to model the data well, suggesting that the complicated dependencies need a two-prong TAN approach. We observe a similar pattern when learning over data with d = 128 (see Appendix, Table 6).

4.2 Test-Data Log Likelihoods
We used multiple datasets from the UCI machine learning repository (2) and Stony Brook outlier detection datasets collection (ODDS) (1) to evaluate log-likelihoods on test data. Broadly, the datasets can be divided into: Particle acceleration: higgs, hepmass, and susy datasets where generated for high-energy physics experiments using Monte Carlo simulations; Music: The music dataset contains timbre features from the million song dataset of mostly commercial western song tracks from the year 1922 to 2011; (5). Word2Vec: wordvecs consists of 3 million words from a Google News corpus. Each word represented as a 300 dimensional vector trained using a word2vec model (3). ODDS datasets: We used several ODDS datasets­forest, pendigits, satimage2. These are multivariate datasets from varied set of sources meant to provide a broad picture of performance across anomaly detection tasks. To not penalize models for low likelihoods on outliers in ODDS, we removed anomalies from test sets when reporting log-likelihoods.
As noted in (8), data degeneracies and other corner-cases may lead to arbitrarily low negative log-likelihoods. In order to avoid such complications, we remove discrete features, standardized all datasets, and add independent Gaussian noise with a standard deviation of 0.01 to training sets.
We report average test log-likelihoods in Table 1. For each dataset and conditional model, we report the test log-likelihood of the top-2 transformations (picked on a validation dataset). We note that the best performing model on each dataset had either LAM or RAM conditionals. The tables detailing test log-likelihoods for all combinations of conditional models and transformations for each dataset may be found in the Appendix (see Tables 8-14). We also observe that L RNN+4xAdd+Re & LAM and L RNN & RAM are consistently among the top-10 picked models.

4.3 Anomaly Detection

Next, we apply density estimates to anomaly detection. Typically anomalies or outliers are

data-points that are unlikely given a dataset. In terms of density estimations, such a task is

framed by identifying which instances in a dataset have a low corresponding density. That

is, we shall label an instance x, as an anomaly if p^(x)  t, where t  0 is some threshold

and p^ is the density estimate based on training data. Note that this approach is trained in

an unsupervised fashion. However, each methods' density estimates were evaluated on test

data with anomaly/non-anomaly labels on instances. We used thresholded log-likelihoods

on the test set to compute precision and recall. We use the average-precision metric:

avg-prec

=

Ntest
k=1

precisionr

(recallr

- recallr-1)

where

precisionr

=

tpr tpr +f

pr

,

recallr

=

tpr tpr +f

nr

,

and tpr, f pr, f nr are true positive anomalies, false positives and false negative respectively

among the bottom r log-likelihood instances in test data. Our results are shown in Table 2.

We see that RAM performs the best on all three datasets. Beyond providing another

interesting use for our density estimates, seeing good performance in these outlier detection

tasks further demonstrates that our models are learning semantically meaningful patterns.

8

Under review as a conference paper at ICLR 2018

Dataset
forrest
d= 10 N=286,048
pendigits
d= 16 N=6,870
susy
d= 18 N=5,000,000
higgs
d= 28 N=11,000,000
hepmass
d= 28 N=10,500,000
satimage2
d= 36 N=5,803
music
d= 90 N=515,345
wordvecs
d= 300 N=3,000,000

LAM

2.389

2.297

L RNN+ 4xAdd+Re
6.923

L RNN+ 4xSRNN+Re
5.854

None
17.673

4xSRNN+ Re
17.474

L 4xAdd+ Re
-3.396

L RNN+ 4xAdd+Re
-3.756

L RNN+ 4xAdd+Re
3.906

L RNN+ 4xSRNN+Re
3.759

L RNN+ 4xAdd+Re
-1.716

L RNN+ 4xSRNN+Re
-7.728

None
-51.572
L RNN+ 4xAdd+Re
-247.440
L 4xAdd+ Re

RNN
-52.617
L RNN+ 4xSRNN+Re
-248.393
L 4xSRNN+ Re

RAM

2.672 2.443

L RNN+ 4xAdd+Re
3.896

L RNN+ 4xSRNN+Re
3.911

2xRNN
18.941
L RNN+ 4xSRNN+Re
-0.340
L RNN
4.935

None
18.389
L RNN
-2.116
RNN+ 4xSRNN+Re
5.047

RNN
-0.550

L RNN
-0.773

L 2xRNN
-55.665
L 4xSRNN+ Re
-272.371
L 4xAdd+ Re

L RNN
-56.190
L RNN+ 4xAdd+Re
-275.508
L 2xRNN

Tied

0.909 0.857

L 4xAdd+ Re
1.437

RNN+ 4xAdd+Re
-2.299

None
15.397
L 4xSRNN+ Re
-8.052
L RNN+ 4xAdd+Re
-0.239
L RNN+ 4xSRNN+Re
-2.137
2xRNN
-58.885
L RNN+ 4xAdd+Re
-273.372
L 4xSRNN+ Re

L RNN
13.765
L RNN+ 4xAdd+Re
-8.006
L 4xAdd+ Re
-0.863
RNN+ 4xSRNN+Re
-2.549
4xSRNN+ Re
-59.093
L 4xAdd+ Re
-273.976
L RNN+ 4xAdd+Re

MultiInd

0.754 0.600

L 4xSRNN+ Re
-5.010

L RNN+ 4xSRNN+Re
-4.742

4xAdd+ Re
12.161

L RNN+ 4xAdd+Re
12.105

L 4xSRNN+ Re
-8.223

L RNN+ 4xSRNN+Re
-9.378

L 4xSRNN+ Re
-5.747

L RNN+ 4xSRNN+Re
-6.091

L 4xSRNN+ Re
-1.570

4xSRNN+ Re
-1.699

L None
-69.484
L RNN+ 4xAdd+Re
-308.148
L RNN+ 4xSRNN+Re

L 2xRNN
-69.887
L 4xAdd+ Re
-308.735
L 4xSRNN+ Re

NADE -0.653 1.437 -5.721 -13.883 -4.948 -9.296 -98.047 -278.789

NICE -0.492 -6.498 4.245 -15.138 -11.387 -17.977 -83.524 -374.563

Table 1: Average test log-likelihood. For each dataset and each conditional model, top-2
transformations are selected using log-likelihoods on a validation set and their mean test log-likelihood are reported.  denotes the best model for each dataset picked by validation. Largest values per dataset are shown in bold.

Dataset forrest O=2,747
pendigits O=156
satimage2 O=71

LAM

0.936 0.902

L RNN+ 4xAdd+Re
0.930

L RNN+ 4xSRNN+Re
0.956

None
0.986

4xSRNN+ Re
0.987

None

RNN

RAM

0.944 0.944

L RNN+ 4xAdd+Re
0.981

L RNN+ 4xSRNN+Re
0.918

2xRNN
0.990

None
0.947

L 2xRNN

L RNN

Tied

0.918 0.923

L 4xAdd+ Re
0.919

RNN+ 4xAdd+Re
0.916

None
0.990
2xRNN

L RNN
0.989
4xSRNN+ Re

MultiInd

0.928 0.882

L 4xSRNN+ Re
0.927

L RNN+ 4xSRNN+Re
0.915

4xAdd+ Re
0.975

L RNN+ 4xAdd+Re
0.969

L None

L 2xRNN

NADE 0.866
0.919
0.990

NICE 0.802
0.933
0.981

Table 2: Average precision score on outlier detection datasets. For each dataset and conditional, the average precision corresponding to the top-2 best transformation model, picked using likelihood on a validation set, is shown. The best score for each dataset is in bold, the number of outliers is O.

4.4 Digit Modeling

For illustrative purposes, we consider modeling MNIST digits. In keeping with our focus of general data modeling, we treat each image as a flattened vector of 784 dimensions. Here we demonstrate that our proposed models can be used to model high dimensional data and produce coherent samples.

First, we model dequantized pixel values rescaled to the unit interval as described in

(8). Moreover, we also model the MNIST digits through a logit transformation of pixel

values. That is, we take the dequantized pixel values in the range [0, 256], y and model:

x = logit



+

(1

-

)

y 256

, with  = 0.05.

This transformation will lessen boundary effects

and keep pixel values inside a valid range.

We ran experiments on MNIST using the two models (L RNN+4xAdd+Re & LAM and L RNN & RAM) that consistently appear in the top-10 in our previous experiments (see Tables 8-14 in Appendix). We observe test set average log-likelihoods and samples reported in bits per pixel in Table 3. Furthermore, we plot samples in Figure 4. We see that our models are able to capture the structure of MNIST digits, with very few artifacts in samples. This is also reflected in the likelihoods, which are comparable or better than state-of-the-art.

NICE (8) Real NVP (9; 20) MADE (20) L RNN+4xAdd+Re & LAM L RNN & RAM L RNN+4xAdd+Re & LAM L RNN & RAM

(Unit)

(Logit)

(Logit)

(Unit)

(Unit)

(Logit)

(Logit)

4.47 ± 0.021

1.93 ± 0.01

1.41 ± 0.01

2.27 ± 0.013

1.60 ± 0.007

2.12 ± 0.01

1.19 ± 0.005

Table 3: Bits per pixel for models (lower is better). "(Unit)" denotes model with unit scale on pixels, and "(Logit)" denotes model with logit transformation on pixels. Standard errors with 2 are shown.

9

Under review as a conference paper at ICLR 2018

(a) LAM (Unit)

(b) RAM (Unit)

(c) LAM (Logit)

(d) RAM (Logit)

Figure 4: Samples from L RNN+4xAdd+Re & LAM, and L RNN & RAM models on unit scaled, and logit transformed pixels.

5 Discussion

We begin by noting the breadth of our proposed methods. As mentioned above, previous approaches considered a complex conditional model with a simple or no transformation and vice-versa. As such, some previous works have proposed a single new type of transformation, or a single new conditional model. Here, we propose multiple methods for transformations (linear, recurrent, and shift recurrent) and multiple autoregressive conditional models (LAM, and RAM). Furthermore, we consider the various combinations of transformations and autoregressive models, most of which constitute a novel TAN.

We draw several conclusions through our comprehensive empirical study. First, we consider our experiments on synthetic data. Methods that only consider complex transformations or condition models are illustrated in the entire row corresponding to the None transformation and the MultiInd and SingleInd columns, respectively. The performance of some of these models, which include None & Tied (NADE), 4xAdd+Re & SingleInd (NICE), was moderate on the Markovian data, however these one-prong approaches fail in the star dataset. Overall LAM and RAM methods provided considerable improvements, especially in the star dataset, where the flexibility of LAM made it possible to learn the widely different conditional probabilities present in the data.

Similarly, we observe that the best performing models in real-world datasets are those that incorporate a flexible transformation and conditional model. In fact, the best model (according to validation dataset) always has LAM or RAM autoregressive components. Hence, validation across models would always select one of these methods. In fact, 95% of top10 models (aggregated across all datasets) have a LAM and RAM conditional model (see Tables 8-14). It is interesting to see that many of these top models also contain a linear transformation. Of course, linear transformations of variables are common to most parametric models, however they have been under-explored in the context of autoregressive density estimation. Our methodology for efficiently learning linear transformations coupled with their strong empirical performance encourages their inclusion in autoregressive models.

Finally, we digest results over the real-world datasets by computing the percentage of the

top likelihood achieved by each transformation t, and conditional model m, in dataset D:

s(t, m, D) = exp(lt,m,D) maxa,b exp(la,b,D), where lt,m,D is the test log-likelihood for t, m

on D.

We then average S

over the datasets:

S(t, m) =

1 T

D

S(t, m, D),

where

T

is the

total number of datasets. We show this score in the Appendix, Table 7. This table gives a

summary of which models performed better (closer to the best performing model per dataset)

over multiple datasets. We see that RAM conditional with L RNN transformation, and LAM

conditional with L RNN+4xAdd+Re were the two best performers.

Conclusion: In conclusion, this work jointly leverages transformations of variables and autoregressive models, and proposes novel methods for both. We show a considerable improvement with our methods through a comprehensive study over both real world and synthetic data. Also, we illustrate the utility of our models in outlier detection and digit modeling tasks.

10

Under review as a conference paper at ICLR 2018
References
[1] Outlier Detection DataSets (ODDS). http://odds.cs.stonybrook.edu. Accessed: 04-01-2017.
[2] UCI Machine Learning Repository. http://archive.ics.uci.edu/ml/. Accessed: 04-01-2017.
[3] word2vec. https://code.google.com/archive/p/word2vec/. Accessed: 04-01-2017.
[4] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
[5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In ISMIR, volume 2, page 10, 2011.
[6] Christopher M Bishop. Mixture density networks. Technical Report, 1994.
[7] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. International Conference on Machine Learning, 2012.
[8] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.
[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.
[10] Brendan J Frey. Graphical models for machine learning and digital communication. MIT press, 1998.
[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: masked autoencoder for distribution estimation. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 881­889, 2015.
[12] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
[13] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. ICML, 2014.
[14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[15] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. Journal of Machine Learning Research, 10(Oct):2295­2328, 2009.
[16] Han Liu, Min Xu, Haijie Gu, Anupam Gupta, John Lafferty, and Larry Wasserman. Forest density estimation. Journal of Machine Learning Research, 12(Mar):907­951, 2011.
[17] Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71­113, 1992.
[18] Radford M Neal. Probabilistic inference using markov chain monte carlo methods. 1993.
[19] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.
[20] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. arXiv preprint arXiv:1705.07057, 2017.
11

Under review as a conference paper at ICLR 2018 [21] B. Uria. Connectionist multivariate density-estimation and its application to speech
synthesis., 2015. [22] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle.
Neural autoregressive distribution estimation. Journal of Machine Learning Research, 17(205):1­37, 2016. [23] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive density-estimator. In Advances in Neural Information Processing Systems, pages 2175­2183, 2013. [24] L Wasserman. All of nonparametric statistics, 2007. [25] Heiga Zen and Andrew Senior. Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 3844­3848. IEEE, 2014.
12

Under review as a conference paper at ICLR 2018

Appendix

Transformation None
L None RNN
L RNN 2xRNN
L 2xRNN 4xAdd+Re
L 4xAdd+Re 4xSRNN+Re
L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re
RNN+4xSRNN+Re
L RNN+4xSRNN+Re

LAM 14.319 15.486(9) 14.777 15.658(5) 14.683 15.474(8) 15.269 15.683(6) 14.829
15.289 15.171 15.078
14.968
15.429

RAM -29.950
14.538 -37.716
10.354 13.698 15.752(3) 12.257
12.594 14.381 16.202(1) 12.991 12.655 16.216(2) 15.566(7)

TIED -0.612
10.906 11.075
10.910 11.493
12.316 12.912
13.845 11.798
12.748 14.455 14.415
12.590
14.179

MultiInd -41.472
5.252 -30.491
5.370 -18.448
5.385 12.446
12.768 11.738 15.415(10) 11.467 12.886 15.589(4)
14.528

SingleInd
---
-9.426 -37.038
3.310 -34.268
3.739 11.625
12.069 12.932
13.908 10.382 12.315
14.231
13.961

Table 4: Held out test log-likelihoods for the Markovian dataset. The superscripts denote rankings of log-likelihoods on the validation dataset.

Transformation
None L None RNN L RNN 2xRNN
L 2xRNN
4xAdd+Re
L 4xAdd+Re 4xSRNN+Re
L 4xSRNN+Re
RNN+4xAdd+Re
L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC
-2.041 5.454 -1.276 7.775 3.705 14.878(3) 13.278(6) 15.728(2) 3.496 16.042(1) 14.071(5) 11.819(8) -0.679 7.433

RAM FC
2.554 8.247 2.762 6.335 8.032
9.946 11.561(9) 12.444(7)
8.429 9.939(10) 14.123(4)
9.253 3.320 7.324

TIED FC
-10.454 -7.858 -6.292 -1.157 -0.565
0.901
7.146
9.031 -1.380
5.598
6.868
2.638 -6.172 3.554

MultiInd
-29.485 -26.988 -25.946 -25.986 -25.100
-23.772
-16.740
-6.091 -15.590
-12.530
-14.773
-7.662 -12.879 -10.427

SingleInd 0.000(0) -38.952 -41.275 -34.408 -38.490
-33.075
-21.332
-11.225 -23.712
-16.889
-20.483
-14.530 -19.204 -15.243

Table 5: Held out test log-likelihoods for star 32d dataset. The superscript denotes ranking of log-likelihood on cross validation dataset

13

Under review as a conference paper at ICLR 2018

Transformation None L None RNN L RNN 2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC
15.671 57.881 18.766 66.070(9) 27.295 85.681(3) 77.195(6) 88.837(1) 33.577 86.375(2) 66.540(8) 80.063(4) 21.719 72.463(7)

RAM FC
15.895 -82.100 48.295
-49.084 45.834
-84.524 61.947(10)
-21.882 -98.796 76.968(5)
-57.861
32.104 -87.335
56.201

TIED FC -83.115 -28.206 -22.485 31.136 -11.930 30.974 16.062 20.234 3.256 33.481 -16.277 21.944 -6.517 26.269

MultiInd -128.238 -123.939 -113.181 -107.083 -113.210 -105.368 -75.206 -65.694 -88.912 -85.590 -75.491 -71.933 -76.459 -71.843

SingleInd 0.000(0) -159.391 -178.641
-155.324 -178.331
-162.635
-111.542
-96.071 -98.936
-93.086
-114.729
-100.384 -85.422
-91.695

Table 6: Held out test log-likelihood for Star 128d dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

Transformation None L None RNN L RNN 2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re
MAX

LAM 0.218 0.154 0.086 0.173 0.151 0.118 0.036 0.153 0.086 0.109 0.121 0.336 0.102 0.211
0.336

RAM 0.118 0.179 0.158 0.540 0.101 0.330 0.047 0.096 0.051 0.143 0.096 0.165 0.151 0.288
0.540

TIED 0.006 0.026 0.014 0.014 0.045 0.015 0.015 0.025 0.031 0.023 0.023 0.024 0.017 0.024
0.045

MultiInd 0.000 0.051 0.001 0.040 0.001 0.045 0.010 0.014 0.010 0.021 0.011 0.016 0.012 0.018
0.051

SingleInd 0.000 0.001 0.000 0.013 0.000 0.025 0.006 0.009 0.008 0.018 0.011 0.013 0.014 0.016
0.025

MAX 0.218 0.179 0.158 0.540 0.151 0.330 0.047 0.153 0.086 0.143 0.121 0.336 0.151 0.288

Table 7: Average performance percentage score for each model across all datasets. Note that this measure is not over a logarithmic space.

Transformation
None L None RNN
L RNN 2xRNN
L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re
L 4xSRNN+Re
RNN+4xAdd+Re
L RNN+4xAdd+Re RNN+4xSRNN+Re
L RNN+4xSRNN+Re

LAM FC
0.751 1.910 1.395 2.189(8) 1.832 2.240(6) 1.106 2.043 1.178 2.089(9)
1.962 2.389(4)
1.599 2.297(5)

RAM FC
-1.383 1.834 0.053
1.747 1.830 2.432(3) 1.430 1.979 1.428 2.061(10) 2.226(7) 2.672(1) 1.545 2.443(2)

TIED FC
-0.653 -0.243 0.221
-0.087 0.448
0.264 0.420 0.909 0.187
0.611
0.857
0.852 0.510
0.804

MultiInd
-12.824 -7.665 -5.130
-4.001 -6.162
-3.956 -0.021 0.365 -0.029
0.754
0.081
0.450 0.182
0.600

SingleInd 0.000(0) -11.062 -15.983
-5.807 -9.095
-5.125 -0.492 -0.088 -0.212
0.593
0.086
0.251 0.369
0.480

Table 8: Held out test log-likelihood for forest dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

14

Under review as a conference paper at ICLR 2018

Transformation
None
L None
RNN
L RNN
2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re
4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re
RNN+4xSRNN+Re
L RNN+4xSRNN+Re

LAM FC 6.923(1) 4.104(9) 5.464(3) 4.072(6) 6.376(5)
2.987
-1.924
-1.796 5.854(2)
3.758
-2.357
-2.687 5.207(4) 3.466(10)

RAM FC 3.911(8)
2.911
3.273
1.398 3.896(7)
0.871 -3.087 -1.438
2.146 -1.020 -2.869 -2.103
2.425
0.496

TIED FC
1.437
-2.872
-1.676
-2.299
-4.002 -3.977 -3.172 -2.288
-2.827 -3.370 -2.187 -2.185
-2.126
-2.761

MultiInd
-14.138
-9.997
-10.144
-10.840
-12.132 -10.890 -5.010 -4.951
-5.970 -5.885 -5.454 -4.742
-5.147
-7.205

SingleInd 0.000(0)
-15.617
-19.719
-13.103
-16.576 -12.711 -6.498 -7.834
-7.084 -12.978 -8.053 -6.941
-8.859
-13.897

Table 9: Held out test log-likelihood for pendigits dataset. The superscript denotes ranking of log-likelihood on crossvalidation dataset

Transformation None L None RNN L RNN 2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC
9.736
15.731 12.784
16.381 11.052
14.523 9.835 17.673(3) 8.798
14.242 15.408 17.474(6)
14.066 16.627(9)

RAM FC
-14.821 16.930(8)
3.347 18.389(2)
14.362 17.373(7)
8.033 16.500(10)
13.235 17.870(5)
12.480
16.376 17.691(4) 18.941(1)

TIED FC -5.721 6.410 6.114 6.772 3.595 10.687 7.238 11.613 1.234 15.397 9.409 13.765 9.136 13.469

MultiInd -21.369 -8.846 -18.575 -5.744 -16.478 -6.884 6.031 10.941 6.936 12.161 7.619 10.951 10.088 12.105

SingleInd 0.000(0)
-17.130 -44.273
-11.489 -33.126
-10.420 4.245
9.034 3.378
13.413 5.446
8.269
7.656
12.349

Table 10: Held out test log-likelihood for susy dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

Transformation None L None RNN L RNN 2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC
-6.220 -3.798(8)
-5.800 -3.975(9)
-6.456
-5.866 -6.502 -5.377 -7.422 -5.999 -4.242(10) -3.396(6)
-5.262 -3.756(7)

RAM FC
-5.848
-10.651 -2.600(3) -0.340(1)
-4.833 -3.222(5)
-10.491 -5.611 -6.863 -9.329
-4.804 -3.049(4) -2.116(2)
-4.773

TIED FC -13.883 -9.084 -10.797 -8.574 -9.192 -8.216 -9.356 -8.006 -11.033 -8.474 -9.187 -8.052 -10.105 -8.097

MultiInd -25.793 -16.025 -25.760 -18.607 -25.398 -16.083 -13.678 -12.106 -11.878 -8.223 -12.321 -12.246 -12.307 -9.378

SingleInd 0.000(0)
-36.051
-66.223
-32.753 -60.040
-30.730 -15.138 -14.129 -12.182 -8.926
-15.261
-13.765
-9.388
-7.721

Table 11: Held out test log-likelihood for higgs dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

15

Under review as a conference paper at ICLR 2018

Transformation
None
L None
RNN
L RNN 2xRNN
L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re
L RNN+4xAdd+Re
RNN+4xSRNN+Re
L RNN+4xSRNN+Re

LAM FC
2.328 3.570(7)
2.088 2.869(10)
1.774
2.053 1.678 1.961 1.443 2.072 2.817 3.906(3)
2.663 3.759(4)

RAM FC 3.710(6)
2.517 4.935(1) 5.047(2)
0.902 3.680(5)
1.873 2.543 2.156 2.730 0.912
-1.869 3.586(8) 3.487(9)

TIED FC
-4.948
-4.052
-1.639
-2.920 -1.909
-2.150 -4.046 -2.259 -2.904 -3.014 -2.514
-3.847
-0.863
-0.239

MultiInd
-19.771
-9.266
-19.851
-16.032 -15.440
-15.457 -9.117 -6.907 -6.091 -5.747 -6.003
-6.339
-7.146
-7.522

SingleInd
nan
-35.042
-47.686
-30.210 -36.754
-24.079 -11.387 -9.275 -7.186 -6.245 -9.284
-9.103
-3.939
-6.102

Table 12: Held out test log-likelihood for hepmass dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

Transformation
None
L None RNN
L RNN
2xRNN
L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC -1.716(9)
-20.164 -7.728
-31.296
-12.283
-20.968 -19.931 -21.128 -7.519 -18.170 -19.278 -20.899 -13.476 -20.179

RAM FC -1.257(3) -1.079(4)
-4.949 -0.773(2) -2.193(7) -0.550(1)
-7.539 -9.944 -11.368 -7.709 -11.789 -12.949 -3.951 -12.128

TIED FC
-9.296
-2.635 -5.466
-3.944
-2.137
-5.140 -11.826 -12.336 -2.549 -5.533 -12.837 -12.867 -6.284 -7.258

MultiInd
-50.507 -1.570(5)
-6.047 -1.824(8)
-5.447 -1.699(6)
-18.901 -21.677 -7.730 -17.085 -21.249 -26.164 -15.025 -18.065

SingleInd
nan
-5.972 -16.521
-2.977
-8.075 -2.276(10)
-17.977 -24.070 -7.232 -15.347 -22.786 -28.302 -16.443 -18.125

Table 13: Held out test log-likelihood for satimage2 dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

Transformation None L None RNN L RNN 2xRNN L 2xRNN 4xAdd+Re L 4xAdd+Re 4xSRNN+Re L 4xSRNN+Re RNN+4xAdd+Re L RNN+4xAdd+Re RNN+4xSRNN+Re L RNN+4xSRNN+Re

LAM FC
-57.873 -52.954(4) -54.933(10) -52.710(3)
-56.958 -53.956(8)
-56.349 -53.169(5)
-57.670 -53.879(7) -53.177(6) -51.572(1) -54.065(9) -52.617(2)

RAM FC -97.925 -74.220 -80.436 -59.815 -85.359 -57.611 -69.302 -59.282 -68.116 -55.665 -67.377 -56.190 -61.204 -68.756

TIED FC -98.047 -72.441 -74.361 -66.536 -77.456 -65.016 -67.064 -59.093 -74.006 -63.894 -63.372 -58.885 -76.437 -65.061

MultiInd -113.099 -82.866 -106.219 -82.731 -104.440 -82.678 -73.886 -69.887 -78.032 -77.564 -73.882 -69.484 -71.814 -83.292

SingleInd nan
-104.287 -144.735 -98.813 -133.898 -96.542 -83.524 -79.330 -121.197 -81.188 -84.032 -79.555 -81.087 -78.997

Table 14: Held out test log-likelihood for music dataset.The superscript denotes ranking of log-likelihood on crossvalidation dataset

16

Under review as a conference paper at ICLR 2018

Transformation L None L RNN L 2xRNN L 4xAdd+Re L 4xSRNN+Re L RNN+4xAdd+Re L RNN+4xSRNN+Re

LAM FC -252.659(6) -252.894(7) -250.285(4) -247.440(1) -248.393(2) -249.980(3) -251.468(5)

RAM FC -279.788 -278.795 -275.508 -272.371(8) -300.666 -280.938 -280.325

TIED FC
-278.789
-278.663
-277.848
-274.205 -273.372(9) -273.976(10)
-274.082

MultiInd -332.474 -332.689 -333.234 -331.148 -308.735 -331.316 -308.148

SingleInd -387.341 -386.700 -386.649 -374.563
0.000 -380.031 -395.084

Table 15: Held out test log-likelihood for wordvecs dataset.The superscript denotes ranking of log-likelihood on validation dataset. Due to time constraints only models with linear transformations were trained.

17

