Under review as a conference paper at ICLR 2018

QUADRATURE-BASED
APPROXIMATION

FEATURES

FOR

KERNEL

Anonymous authors Paper under double-blind review

ABSTRACT
We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods. Our approach allows the use of information about the integrand to enhance approximation and facilitates fast computations. We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.

1 INTRODUCTION

Kernel methods proved to be an efficient technique in numerous real-world problems. The core idea of kernel methods is the kernel trick ­ compute an inner product in a high-dimensional (or even infinite-dimensional) feature space by means of a kernel function k:

k(x, y) = (x), (y) ,

(1)

where  : X  F is a non-linear feature map transporting elements of input space X into a feature
space F. It is a common knowledge that kernel methods incur space and time complexity infeasible to be used with large-scale datasets directly. For example, kernel regression has O(N 3 + N d2) training time, O(N 2) memory, O(N d) prediction time complexity for N data points in original
d-dimensional space X . One of the most successful techniques to handle this problem (Rahimi &
Recht (2008)) introduces a low-dimensional randomized approximation to feature maps:

k(x, y)  ^ (x) ^ (y).

(2)

This is essentially carried out by using Monte-Carlo sampling to approximate scalar product in (1). A randomized D-dimensional mapping ^ (·) applied to the original data input allows employing standard linear methods, i.e. reverting the kernel trick. In doing so one reduces the complexity to that of linear methods, e.g. D-dimensional approximation admits O(N D2) training time, O(N D) memory and O(N ) prediction time.
It is well known that as D  , the inner product in (2) converges to exact kernel k(x, y). Recent research (Yang et al. (2014); Felix et al. (2016); Choromanski & Sindhwani (2016)) aims to improve the convergence of approximation so that a smaller D can be used to obtain the same quality of approximation.

This paper considers kernels that allow the following integral representation

k(x, y) = Eq(w)gxy(w)  Ep(w)fxy(w) = I(fxy),

p(w) =

1

e-

w2
2,

(2)d/2

(3)

where q(w) is a density associated with a kernel, e.g. the popular Gaussian kernel has q(w) = p(w), so the exact equality holds with gxy(w) = fxy(w) = (w x) (w y), where (·) = [cos(·), sin(·)] .

The class of kernels admitting the form in (3) covers shift-invariant kernels (e.g. radial basis function (RBF) kernels) and Pointwise Nonlinear Gaussian (PNG) kernels. They are widely used in practice and have interesting connections with neural networks (Cho & Saul (2009), Williams (1997)).

1

Under review as a conference paper at ICLR 2018

For kernels with q(w) other than Gaussian (e.g. Laplacian kernel is associated with Gamma distribution) asymptotics imply that for large sample sizes one can obtain very accurate results with little effort by using Gaussian approximation of q(w), which is essentially a reparameterization of q(w) and g(w) (Monahan & Genz (1997)).
The main challenge for the construction of low-dimensional feature maps is the approximation of the expectation in (3) which is d-dimensional integral with Gaussian weight. While standard MonteCarlo rule is easy to implement, there are better quadrature rules for such kind of integrals. For example, Yang et al. (2014) apply quasi-Monte Carlo (QMC) rules and obtain better quality kernel matrix approximations compared to random Fourier features of Rahimi & Recht (2008).
Unlike other research studies we refrain from using simple Monte Carlo estimate of the integral, instead, we propose to use specific quadrature rules. We now list our contributions:
1. We propose to use advanced quadrature rules to improve kernel approximation accuracy. We also provide an analytical estimate of the error for the used quadrature rules.
2. We note that for kernels with specific integrand fxy(w) in (3) one can improve on its properties. For example, for kernels with even function fxy(w) we derive the reduced quadrature rule which gives twice smaller embedded dimension D with the same accuracy. This applies, for example, to any RBF kernel.
3. We use structured orthogonal matrices (so-called butterfly matrices) when designing quadrature rule that allow fast matrix by vector multiplications. As a result, we speed up the approximation of the kernel function and reduce memory requirements.
4. We demonstrate our approach on a set of regression and classification problems. Empirical results show that the proposed approach has a better quality of approximation of kernel function as well as better quality of classification and regression when using different kernels.

2 QUADRATURE-BASED RANDOM FEATURES

We start with rewriting the expectation in equation (3) as integral of fxy with respect to p(w):



I (fxy )

=

(2)-

d 2

...

e- w 2 w fxy(w)dw.

-

-

Integration can be performed by means of quadrature rules. The rules usually take a form of interpolating function that is easy to integrate. Given such a rule, one may sample points from the domain of integration and calculate the value of the rule at these points. Then, the sample average of the rule values would yield the approximation of the integral.

We use the average of sampled quadrature rules developed by Genz & Monahan (1998) to yield un-
biased estimates of I(fxy). A change of coordinates is the first step to facilitate stochastic sphericalradial rules. Now, let w = rz, with z z = 1, so that w w = r2 for r  [0, ], leaving us with

I (fxy )

=

(2)-

d 2



e-

r2 2

rd-1fxy

(rz)drdz

=

(2

)-

d 2

2



e-

r2 2

|r|d-1

fxy

(rz)drdz,

Ud 0

Ud -

(4)

where Ud = {z : z z = 1, z  Rd}. As mentioned earlier we are going to use a combination of radial R and spherical S rules. We now describe the logic behind the used quadratures.

Stochastic radial rules. Stochastic radial rule R(h) of degree 2l + 1 has the form of weighted

symmetric sums:

R(h) =

l

wi

h(i

)

+ h(-i 2

)

,

i=0

where h is an integrand in infinite range integral T (h) =



e-

r2 2

|r|d-1h(r)dr.

To

get

an

unbiased

-

estimate for T (h), points i are sampled from specific distributions which depend on the degree of

2

Under review as a conference paper at ICLR 2018

the rule. Weights wi are derived so that R has a polynomial degree 2l + 1, i.e. is exact for integrands h(r) = rp with p = 0, 1, . . . , 2l + 1. For radial rules of degree three R3 the point 0 = 0, while 1  (d + 2) follows Chi-distribution with d + 2 degrees of freedom. Higher degrees require
samples from more complex distributions which are hard to sample from.

Stochastic spherical rules. Spherical rule S(s) approximates an integral of a function s(z) over the surface of unit d-sphere Ud and takes the following form:
p
S(s) = wjs(zj),
j=1

where zj are points on Ud, i.e. z

z

=

1.

If

we

set

weight

wj

=

|Ud | 2(d+1)

and

sum

function

s

values

at original and reflected vertices vj of randomly rotated d-simplex V, we will end up with a degree

three rule:

SQ3 (fxy(Qz)) =

|Ud| d+1 2(d + 1)

fxy(-Qvj ) + fxy(Qvj )

,

j=1

where vj is the j'th vertex of d-simplex V with vertices on Ud and Q is a random d × d orthogonal matrix. We justify the choice of the degree in Appendix A.

Since the value of the integral is approximated as the sample average, the key to unbiased estimate is proper randomization. In this case, randomization is attained through the matrix Q. It is crucial to generate uniformly random orthogonal matrices to achieve an unbiased estimate for spherical surface integrals. We consider various designs of such matrices further in Section 3.

Stochastic spherical-radial rules. Meanwhile, combining foregoing rules results in stochastic spherical-radial rule of degree three:

SRQ3,3,(fxy) =

1

-

d 2

d d+1 fxy(0) + d + 1

fxy(-Qvj) + fxy(Qvj) 22

,

j=1

(5)

which we finally apply to the approximation of (4) by averaging the samples of SRQ3,3,:

I(f )

=

EQ, [S RQ3,3, (fxy )]



I^(fxy)

=

1 n

n

SRQ3,3i,i (fxy),

i=1

(6)

where n is the number of sampled SR rules. Speaking in terms of approximate feature maps, the new feature dimension D in case of quadrature based approximation equals 2n(d + 1) as we sample n rules and evaluate each of them at 2(d + 1) points. Surprisingly, empirical results (see Section 5) show that even a small number of rule samples n provides accurate approximations.

Properties of the integrand. We also note here that for specific functions fxy(w) we can derive better versions of SR rule by taking on advantage of the knowledge about the integrand. For exam-
ple, the Gaussian kernel has fxy(w) = cos(w (x - y)). Note that f is even, so we can discard an excessive term in the summation in (5), since f (w) = f (-w), i.e SR3,3 rule reduces to

SRQ3,3,(fxy) =

1

-

d 2

fxy(0)

+

d

d +

1

d+1

fxy

(Qvj 2

)

.

j=1

(7)

Variance of the error. We contribute the variance estimation for the stochastic spherical-radial rules when applied to kernel function. To the best of our knowledge, it has not been done before. In case of kernel functions the integrand fxy can be represented as

fxy(w) = (w x) (w y) = g(z1, z2), z1 = w x, z2 = w y,

(8)

where z1, z2 are scalar values. Using this representation of kernel function and its Taylor expansion we can obtain the following proposition (see Appendix B for detailed derivation of the result):

3

Under review as a conference paper at ICLR 2018

Proposition 2.1. The quadrature rule (6) is an unbiased estimate of integral of any integrable function f . If function f can be represented in the form (8), i.e. f (w) = g(z1, z2), z1 = w x, z2 = w y for some x, y  Rd, all 4-th order partial derivatives of g are bounded and D = 2n(d+1) is the number of generated features, then

V[I^(f )]



2.66M12L8 nd

+

212M1M2L6 nd3

+

(d + 95)M22L4 4nd2(d - 2)

,

4g 4g

4g

where M1 = max

sup
z

z14

, sup
z

z24

, sup
z

z12z22

and L = max{ x , y }.

, M2 = max
j=0,1,2

2g z1j z22-j (0, 0)

Constants M1, M2 in the proposition are upper bounds on the derivatives of function g and don't depend on the data set, while L plays the role of the scale of inputs. The proposition implies that the error of approximation is proportional to L ­ the less the scale, the better the accuracy (see Figure 1). However, scaling input vectors is equivalent to changing the parameters of the kernel function. For example, decreasing the norm of input variables for RBF kernel is equivalent to increasing the kernel width . Therefore, the wide RBF kernels are approximated better than the narrow ones. This result also gives us the rate of convergence O(1/nd) for the quadrature rule.

|| ^K-K|| ||K||

10-1 10-2 10-3 10-4 10-5 10-6 10-7
0

d = 10 d = 100 d = 500

5 10 15
scaling factor, 

20

Figure 1: Relative error of approximation of kernel matrix with 95% confidence interval depending on the scaling factor, Gaussian kernel was used. In this experiment for each scaling factor  we
construct approximate kernel matrix K and the exact kernel matrix K using scaled input vectors x~ = x/. To plot the confidence interval we run each experiment 10 times each time generating new weights. Experiment was conducted for 3 different input dimensions: d = 10, 100, 500.

The quadrature rule (5) grants us some freedom in the choice of random orthogonal matrix Q.
The next section discusses such matrices and suggests butterfly matrices for fast matrix by vector multiplication as the SR3,3 rule implementation involves multiplication of the matrix QV by the
data vector x.

3 GENERATING UNIFORMLY RANDOM ORTHOGONAL MATRICES
Previously described stochastic spherical-radial rules require a random orthogonal matrix Q (see equation (5)). If Q follows Haar distribution on the set of all matrices in the orthogonal group O(d) in dimension d, then the averages of spherical rules SQ3 i (s) provide unbiased degree three estimates for integrals over unit sphere. Essentially, Haar distribution means that all orthogonal matrices in the group are equiprobable, i.e. uniformly random. Methods for sampling such matrices vary in their complexity of generation and multiplication.
Techniques based on QR decomposition (Mezzadri (2006)) have complexity cubic in d, and the resulting matrix does not allow fast matrix by vector multiplications. Another set of methods is based on a sequence of reflectors (Stewart (1980)) or rotators (Anderson et al. (1987)). The complexity is better (quadratic in d), however the resulting matrix is unstructured and, thus, implicates no fast matrix by vector multiplication. In Choromanski et al. (2017) random orthogonal matrices are considered. They are constructed as a product of random diagonal matrices and Hadamard matrices and

4

Under review as a conference paper at ICLR 2018

therefore enable fast matrix by vector products. Unfortunately, they are not guaranteed to follow the Haar distribution.
To satisfy both our requirements, i.e low computational/space complexity and generation of Haar distributed orthogonal matrices, we propose to use so-called butterfly matrices.

Butterfly matrices. The method from Genz (1998) generates Haar distributed random orthogonal matrix B. As it happens to be a product of butterfly structured factors, a matrix of this type conveniently possesses the property of fast multiplication. For d = 4 an example of butterfly orthogonal

matrix is

c1 -s1 0 0  c2 0 -s2 0  c1c2 -s1c2 -c1s2 s1s2 

.B(4)

=

s1 0

c1 0

0

00 

c2

c3 -s3 s2 0

0 c2

-s2 0

=

s1c2 c3s2

c1c2 -s3s2

-s1s2 c3c2

-c1s2 -s3c2

0 0 s3 c3 0 s2 0 c2

s3s2 c3s2 s3c2 c3c2

Definition 3.1. Let ci = cos i, si = sin i for i = 1, . . . , d - 1 be given. Assume d = 2k with k > 0. Then an orthogonal matrix B(d)  Rd×d is defined recursively as follows

B(2d) =

B(d)cd B^ (d)sd

-B(d)sd B^ (d)cd

,

B(1) = 1,

where B^ (d) is the same as B(d) with indexes i shifted by d, e.g.

B(2) =

c1 s1

-s1 c1

,

B^ (2) =

c3 s3

-s3 c3

.

Matrix B(d) by vector product has computational complexity O(d log d) since B(d) has log d factors and each factor requires O(d) operations. Another advantage is space complexity: B(d) is fully determined by d - 1 angles i, yielding O(d) memory complexity.
One can easily define butterfly matrix B(d) for the cases when d is not a power of two (see Appendix C.1 for details). The randomization is based on the sampling of angles  and we discuss it in Appendix C.2. The method that uses butterfly orthogonal matrices is denoted B in the experiments section.

4 KERNELS

This section gives examples on how quadrature rules can be applied to a number of kernels.

4.1 GAUSSIAN KERNEL

Radial basis function (RBF) kernels are popular kernels widely used in kernel methods. Gaussian kernel is a widely exploited RBF kernel and has the following form:

k(x, y) = exp

- x-y 2 22

.

In this case the integral representation has (w x) = [cos(w x), sin(w x)] . Since fxy(0) = 1, SR3,3 rule for Gaussian kernel has the form ( appears due to scaling):

SRQ3,3,(fxy) =

d 1 - 2

d

d+1 fxy

Qvj 

+ d+1

2

j=1

,

4.2 ARC-COSINE KERNELS
Arc-cosine kernels were originally introduced by Cho & Saul (2009) upon studying the connections between deep learning and kernel methods. The integral representation of the bth-order arc-cosine kernel is
kb(x, y) = 2 b(w x)b(w y)p(w)dw,
Rd

5

Under review as a conference paper at ICLR 2018

Table 1: Space and time complexity for different kernel approximation algorithms.

Method
ORF QMC ROM Quadrature based

Space O(Dd) O(Dd) O(d) O(d)

Time O(Dd) O(Dd) O(d log d) O(d log d)

where b(w x) = (w x)(w x)b, (·) is the Heaviside function and p is the density of the standard Gaussian distribution. Such kernels can be seen as an inner product between the represen-

tation produced by infinitely wide single layer neural network with random Gaussian weights. They

have closed form expression in terms of the angle  = cos-1

xy xy

between x and y.

0th-order

arc-cosine

kernel

is

given

by

k0(x,

y)

=

1

-

 

,

1st-order

kernel

is

given

by

k1(x,

y)

=

xy 

(sin  + ( - ) cos ).

Let 0(w x) = (w x) and 1(w x) = max(0, w x), then we can rewrite the integral rep-

n

resentation as follows: kb(x, y) = 2

Rd b(w

x)b(w

y)p(w)dw



2 n

i=1 SRQ3,3i,i .

For

arc-

cosine kernel of order 0 the value of the function 0(0) = (0) = 0.5 results in

SRQ3,3,(fxy) = 0.25

1

-

d 2

+

d

d +

1

d+1

fxy

(Qvj 2

)

.

j=1

In the case of arc-cosine kernel of order 1, the value of 1(0) is 0 and the SR3,3 rule reduces to

S RQ3,3, (fxy )

=

d

d +

1

d+1

fxy(Qvj )

+ fxy 22

(-Qvj

)

.

j=1

5 EXPERIMENTS
We extensively study the proposed method on several established benchmarking datasets: Powerplant, LETTER, USPS, MNIST, CIFAR100 (Krizhevsky & Hinton (2009)), LEUKEMIA (Golub et al. (1999)). In Section 5.2 we show kernel approximation error across different kernels and number of features. We also report the quality of SVM models with approximate kernels on the same data sets in Section 5.3. The compared methods are described below.

5.1 METHODS
We present a comparison of our method with estimators based on simple Monte Carlo1. The Monte Carlo approach has a variety of ways to generate samples: unstructured Gaussian (Rahimi & Recht (2008)), structured Gaussian (Felix et al. (2016)), random orthogonal matrices (ROM) (Choromanski et al. (2017)).

Monte Carlo integration.

The

kernel

is

estimated

as

k^(x, y)

=

1 D

(Mx)(My),

where

M



RD×d is a random weight matrix. For unstructured Gaussian based approximation M = G, where

G is a random matrix with iid N (0, 1) elements. Structured Gaussian has M = Gort, where Gort =

DQ, Q is obtained from RQ decomposition of G (note that this is not the same Q used in the

quadrature rules), D is a diagonal matrix with diagonal elements sampled from the (d) distribution.

In compliance with the previous work on ROM we use S-Rademacher with three blocks: M = 3
d SDi since three blocks have been shown to yield the best results.

i=1

1We also study quasi-Monte Carlo (Yang et al. (2014)) performance. See Appendix D for details.

6

Under review as a conference paper at ICLR 2018

Arc-cosine 0 K- ^K K

Gaussian K- ^K K

Arc-cosine 1 K- ^K K

LETTER 2.00 ×10-1 1.75 1.50 1.25 1.00 0.75
1234 n
×10-1
3.2
2.4
1.6
0.8
0.0 1234 n
×10-2 1.25 1.00 0.75 0.50 0.25 0.00
1234 n

MNIST ×10-2 3.5 3.0 2.5 2.0 1.5 1.0 5 1234
n
×10-2
6.0
4.5
3.0
1.5
5 1234 n
×10-3
4.0 3.2 2.4 1.6 0.8 0.0 5 1234
n

CIFAR100 ×10-2 1.8 1.5 1.2 0.9 0.6

LEUKEMIA 2.1 ×10-2 1.8 1.5 1.2 0.9 0.6

5

12345

0.5 1.0 1.5 2.0

nn

×10-2

×10-2

3.0 3.0

2.4 2.4

1.8 1.8

1.2 1.2

0.6 0.0

0.6

5

12345

0.5 1.0 1.5 2.0

nn

×10-3
2.5 2.0

6.0 ×10-4 4.5

1.5 3.0

1.0 0.5

1.5

0.0 0.0

5

12345

0.5 1.0 1.5 2.0

nn

Gort ROM G H B

Figure 2: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,

Gaussian) on three datasets: LETTER (d = 16), MNIST (d = 784), CIFAR100 (d = 3072) and

LEUKEMIA(d = 7129). Lower is better. The x-axis represents the factor to which we extend the

original

feature

space,

n

=

D 2(d+1)

,

where

d

is

the

dimensionality

of

the

original

feature

space,

D

is

the dimensionality of the new feature space.

Quadrature rules. Our main method that uses stochastic spherical-radial rules with Q = B2 (butterfly matrix) is denoted by B. As mentioned earlier we also include a variant of our algorithm that uses an orthogonal matrix Q based on a sequence of random reflectors (we denote it as H).

5.2 KERNEL APPROXIMATION

To measure kernel approximation quality we use relative error in Frobenius norm

K-K^ KF

F

,

where

K and K^ denote exact kernel matrix and its approximation. We run experiments for the kernel

approximation on a random subset of a dataset (see Appendix D for details). Approximation was

constructed

for

different

number

of SR

samples

n

=

D 2(d+1)

,

where

d

is an

original feature space

dimensionality

and D

is

the

new

one.

For

the

Gaussian

kernel

we

set hyperparameter



=

1 22

to

the same value for all the approximants, while arc-cosine kernels have no hyperparameters.

We run experiments for each [kernel, dataset, n] tuple and plot 95% confidence interval around the mean value line. Figure 2 show results for kernel approximation error on LETTER, MNIST, CIFAR100 and LEUKEMIA datasets.

We observe that for the most of the datasets and kernels the methods we propose in the paper (B, H) show better results than the baselines. They do coincide almost everywhere, which is expected, as the B method is only different from H in the choice of the matrix Q to facilitate speed up.

5.3 CLASSIFICATION/REGRESSION WITH NEW FEATURES
We report accuracy and R2 scores for the classification and regression tasks on the same data sets (see Figure 3). We examine the performance with the same setting (the number of runs for each [kernel, dataset, n] tuple) as in experiments for kernel approximation error, except now we map the
2B = (BP)1(BP)2 . . . (BP)3, where P is a permutation matrix, for explanation see Appendix C.

7

Under review as a conference paper at ICLR 2018

Powerplant

LETTER

USPS

Arc-cosine 0
accuracy/R2

0.875 0.850 0.825 0.800 0.775
12345 n

0.8 0.957 0.7 0.954 0.6 0.951 0.5 0.948 0.4 0.945 0.3

12345

12345

nn

0.94

0.825

0.9730

0.92

0.800

0.9715

0.90

0.775

0.9700

0.88

0.750

0.9685

0.86 0.84

0.725 0.700

0.9670 0.9655

12345

12345

12345

nnn

exact Gort ROM G B

Arc-cosine 1
accuracy/R2

Gaussian
accuracy/R2

0.936 0.933 0.930 0.927 0.924

0.690 0.675 0.660 0.645

0.9632 0.9624 0.9616 0.9608 0.9600

12345 n

12345 n

12345 n

Figure 3: Accuracy/R2 score using embeddings with three kernels (columns: arc-cosine 0, arc-

cosine 1, Gaussian) on three datasets (rows: Powerplant, LETTER, USPS). Higher is better. The

x-axis represents

the factor

to which we extend the

original

feature

space,

n

=

D 2(d+1)

,

where

d

is

the dimensionality of the original feature space, D is the dimensionality of the new feature space.

We drop one of our methods H here since its kernel approximation almost coincides with B.

whole dataset. We use Support Vector Machines to obtain predictions. We also drop one of our methods H here since its kernel approximation almost coincides with B.
Kernel approximation error does not fully define the final prediction accuracy ­ the best performing kernel matrix approximant not necessarily yields the best accuracy or R2 score. However, the empirical results illustrate that our method delivers comparable and often the best quality on the final tasks. We also note that in many cases our method provides greater performance using less number of features n, e.g. LETTER and Powerplant datasets with arc-cosine kernel of the first order.
6 RELATED WORK
The most popular methods for scaling up kernel methods are based on a low-rank approximation of the kernel using either data-dependent or independent basis functions. The first one includes Nystro¨m method (Drineas & Mahoney (2005)), greedy basis selection techniques (Smola & Scho¨lkopf (2000)), incomplete Cholesky decomposition (Fine & Scheinberg (2001)).
The construction of basis functions in these techniques utilizes the given training set making them more attractive for some problems compared to Random Fourier Features approach. In general, data-dependent approaches perform better than data-independent approaches when there is a gap in the eigen-spectrum of the kernel matrix. The rigorous study of generalization performance of both approaches can be found in (Yang et al. (2012)).
In data-independent techniques, the kernel function is approximated directly. Most of the methods (including the proposed approach) that follow this idea are based on Random Fourier Features (Rahimi & Recht (2008)). They require so-called weight matrix that can be generated in a number of ways. Le et al. (2013) form the weight matrix as a product of structured matrices. It enables fast computation of matrix-vector products and speeds up generation of random features.
Another work (Felix et al. (2016)) orthogonalizes the features by means of orthogonal weight matrix. This leads to less correlated and more informative features increasing the quality of approximation. They support this result both analytically and empirically. The authors also introduce matrices with some special structure for fast computations. Choromanski et al. (2017) propose a generalization of
8

Under review as a conference paper at ICLR 2018
the ideas from (Le et al. (2013)) and (Felix et al. (2016)), delivering an analytical estimate for the mean squared error (MSE) of approximation.
All these works use simple Monte Carlo sampling. However, the convergence can be improved by changing Monte Carlo sampling to Quasi-Monte Carlo sampling. Following this idea Yang et al. (2014) apply quasi-Monte Carlo to Random Fourier Features. In (Yu et al. (2015)) the authors make attempt to improve quality of the approximation of Random Fourier Features by optimizing sequences conditioning on a given dataset.
7 CONCLUSION
In this work we proposed to apply advanced integration rule that allowed us to achieve higher quality of kernel approximation. Our derivation of the variance of the error implies the dependence of the error on the scale of data, which in case of Gaussian kernel can be interpreted as width of the kernel. However, as we have seen earlier, accuracy on the final task has no direct dependence on the approximation quality, so we can only speculate whether better approximated wide kernels deliver better accuracy compared to the poorer approximated narrow ones. It is interesting to explore this connection in the future work.
To speed up the computations we employed butterfly orthogonal matrices yielding the computational complexity O(d log d). Although the procedure we used to generate butterfly matrices claims to produce uniformly random orthogonal matrices, we found that it is not always so. However, the comparison of the method H (uses properly distributed orthogonal matrices) with method B (sometimes fails to do so) did not reveal any differences. We also leave it for the future investigation.
Our experimental study confirms that for many kernels on the most datasets the proposed approach delivers better kernel approximation. Additionally, the empirical results showed that the quality of the final task (classification/regression) is also higher than the state-of-the-art baselines. The connection between the final score and the kernel approximation error is to be explored as well.
REFERENCES
Theodore W Anderson, Ingram Olkin, and Les G Underhill. Generation of random orthogonal matrices. SIAM Journal on Scientific and Statistical Computing, 8(4):625­629, 1987. 4
John A Baker. Integration over spheres and the divergence theorem for balls. The American Mathematical Monthly, 104(1):36­47, 1997. 12
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems, pp. 342­350, 2009. 1, 5
Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear time kernel expansions. arXiv preprint arXiv:1605.09049, 2016. 1
Krzysztof Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness of random orthogonal embeddings. arXiv preprint arXiv:1703.00864, 2017. 4, 6, 8
Petros Drineas and Michael W Mahoney. On the Nystro¨m method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(Dec):2153­2175, 2005. 8
Kai-Tai Fang and Run-Ze Li. Some methods for generating both an NT-net and the uniform distribution on a Stiefel manifold and their applications. Computational Statistics & Data Analysis, 24 (1):29­46, 1997. 14
X Yu Felix, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and Sanjiv Kumar. Orthogonal Random Features. In Advances in Neural Information Processing Systems, pp. 1975­1983, 2016. 1, 6, 8, 9, 15
Shai Fine and Katya Scheinberg. Efficient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2(Dec):243­264, 2001. 8
9

Under review as a conference paper at ICLR 2018
Alan Genz. Methods for generating random orthogonal matrices. Monte Carlo and Quasi-Monte Carlo Methods, pp. 199­213, 1998. 5
Alan Genz and John Monahan. Stochastic integration rules for infinite regions. SIAM journal on scientific computing, 19(2):426­439, 1998. 2
Alan Genz and John Monahan. A stochastic algorithm for high-dimensional integrals over unbounded regions with gaussian weight. Journal of Computational and Applied Mathematics, 112 (1):71­81, 1999. 11
Todd R Golub, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, Mignon L Loh, James R Downing, Mark A Caligiuri, et al. Molecular classification of cancer: class discovery and class prediction by gene expression monitoring. Science, 286(5439):531­537, 1999. 6
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 6
Quoc Le, Tama´s Sarlo´s, and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the International Conference on Machine Learning, 2013. 8, 9
Francesco Mezzadri. How to generate random matrices from the classical compact groups. arXiv preprint math-ph/0609050, 2006. 4
John Monahan and Alan Genz. Spherical-radial integration rules for bayesian computation. Journal of the American Statistical Association, 92(438):664­674, 1997. 2
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, pp. 1177­1184, 2008. 1, 2, 6, 8
Alex J Smola and Bernhard Scho¨lkopf. Sparse greedy matrix approximation for machine learning. 2000. 8
G. W. Stewart. The efficient generation of random orthogonal matrices with an application to condition estimators. SIAM Journal on Numerical Analysis, 17(3):403­409, 1980. ISSN 00361429. URL http://www.jstor.org/stable/2156882. 4
Christopher KI Williams. Computing with infinite networks. In Advances in Neural Information Processing Systems, pp. 295­301, 1997. 1
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. In Proceedings of The 31st International Conference on Machine Learning (ICML-14), pp. 485­493, 2014. 1, 2, 6, 9
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystro¨m Method vs Random Fourier Features: A Theoretical and Empirical Comparison. In Advances in Neural Information Processing Systems, pp. 476­484, 2012. 8
Felix X Yu, Sanjiv Kumar, Henry Rowley, and Shih-Fu Chang. Compact nonlinear maps and circulant extensions. arXiv preprint arXiv:1503.03893, 2015. 9
10

Under review as a conference paper at ICLR 2018

A QUADRATURE RULES DETAILS
The degree of the rules. We now discuss the choice of the degree for the SR rule. Genz & Monahan (1999) show that higher degree rules, being more computationally expensive, often bring in only marginal improvement in performance. For these reasons in our experiments we use the rule of degree three SR3,3, i.e. a combination of radial rule R3 and spherical rule S3.

B VARIANCE OF KERNEL FUNCTION APPROXIMATION USING QUADRATURE
RULE

The function fxy in quadrature rule (7) can be considered as a function of two variables, i.e. fxy = (w x)(w y) = g(z1, z2), where z1 = w x, z2 = w y.
In the quadrature rule   (d + 2) and Q is a random orthogonal matrix. Therefore, random variables wi = Qvi are uniformly distributed on a unit-sphere.
Now, let's write down 4-th order Taylor expansion with Lagrange remainder of function g(wi x, wi y) + g(-wi x, -wi y) around 0 (odd terms cancel out)

g(wi x, wi y) + g(-wi x, -wi y)  2g(0, 0) + 2

2

j

cj !(2 -

j

)!

(wi

x)j (wi

y)2-j +

j=0

+ 2

4

j

dji !(4 -

j)!

(wi

x)j (wi

y)4-j

j=0

where cj

=

2g z1j z22-j

(0,

0),

dij

=

(4g
z1j z24-j

i1,

i2)

+

4g z1j z24-j

(

i 3

,

i 4

),

i 1

is

between

0

and

(wi

x),

i 2

is

between

0

and

(wi

y),

i 3

is

between

0

and

(-wi

x) and

i 4

is

between

0

and

(-wi

y).

Plugging this expression into (5) we obtain

SRQ3,3,(fxy) 

1

-

d 2

g(0, 0)+



d d+1 + 2(d + 1)2 2g(0, 0) + 2

2

j

cj !(2 -

j)!

(wiT

x)j

(wiT

y)2-j

+

i=1 j=0



2

4

dj j!(4 -

j)!

(wiT

x)j (wiT

y)4-j



=

j=0

1 d+1

= g(0, 0) + 2

Si,

i=1

where Bi =

Si

=

Ai + Bi, Ai

=

2 2

2 j=0

j

cj !(2-j)!

(wi

x)j (wi

y)2-j ,

4 j=0

j

dij !(4-j)!

(wi

x)j (wi

y)4-j ,

wj

=

Qvj, matrix Q is a random orthogonal ma-

trix uniformly distributed on a set of orthogonal matrices O(n). From uniformity of orthogonal

matrix Q it follows that vector wj is uniform on a unit n-sphere. Also note that Ai and Bj are

independent if i = j, Bi and Bj are independent if i = j, however, Ai and Aj are dependent as they have common random variable . Therefore, Cov(Si, Sj) = Cov(Ai, Aj) = E(AiAj) - (E(Ai))2
if i = j.

11

Under review as a conference paper at ICLR 2018

Let us calculate the variance of the estimate.



V

S RQ3,3, (fxy )

1 d+1

1 d+1

1 d+1

= 4 V  Sj = 4 V (Si) + 4 Cov(Si, Sj) 

j=1

i=1

i=j



1 4

d+1
V (Si)

+

1 4

d+1
E(Ai Aj )

=

1 4

d+1
V (Si) +

i=1 i=j

i=1

1 d+1

+ E 44

E

2

ck k!(2 -

k)!

(wi

x)k (wi

y)2-k

i=j k=0

E

2

ck k!(2 -

k)!

(wjT

x)k

(wjT

y)2-k

k=0

× .

(9)

Distribution of random variable  is (d + 2), therefore

11

1

1

E

2

=, d

E

4

=

d(d

-

, 2)

d

>

2.

1. Now, let us calculate the variance VSi from the first term of equation (9) VSi = V(Ai + Bi) = V(Ai) + V(Bi) + Cov(Ai, Bi).
Let x  y . Then

   2

4
V(Bi) = V 

dij

(wi x)j (wi y)4-j j!(4 - j)!





E



4

dij (wi x)j (wi y)4-j

j!(4 - j)!



=

j=0

j=0

=E

(d0i )2(w

y)8 + (di4)2(w

x)8 + (4(di1)2 + 3d0i di2)(w

x)2(w

y)6 +

596 144

(4(d3i )2 + 3d4i di2)(w

x)6(w

y)2 +

144

(1296(di2)2 + 72d0i d4i + 1152di1di3)(w

20736

x)4(w

y)4





M12 298

E

wT x 8 + 7M12 72

x

2E

wT x 6 + 0.122M12

x

4E

wT x

4,

(10)

4g 4g

4g

where M1 = max

sup
z

z14 (z1, z2)

, sup
z

z24 (z1, z2)

,

sup
z

z12z22 (z1, z2)

.

To calculate expectations of the form E(w x)k we will use expression for integral of

monomial over unit sphere Baker (1997)

J (k1, k2, . . . , kd) =

Sd-1

xk11

xk22

·

·

·

xdkd

dx

=

d

(k1

- 1)!!(k2 - d(d + 2) · · ·

1)!! · (d +

· · (kd |k| -

- 2)

1)!!

,

(11)

where ki = 2si, si  Z+, |k| = i ki, d is a volume of an d-dimensional unit sphere.

For example, let us show how to calculate E(w x)4:

E(w x)4 = E

wiwj wkwmxixj xkxm =

i,j,k,m

= thanks to symmetry all terms, for which at least one index doesn't coincide with



other indices, are equal to 0. = E  wi4xi4 + 3 wi2wj2x2i x2j  =
i i=j

3 =
d(d + 2)

xi4

+

3 d(d +

2)

xi2x2j

=

3 d(d +

2)

x

4.

i i=j

12

Under review as a conference paper at ICLR 2018

Using the same technique for (10) we obtain

V(Bi)  M12 x 8

8.46

10.21

0.37

++

d(d + 2)(d + 4)(d + 6) d(d + 2)(d + 4) d(d + 2)

 2.66M12 x 8 d(d + 2)

 (12)

For the variance of Ai we have

   2

2 V(Ai) = V  2

2

cj

(wiT x)j (wiT y)2-j j!(2 - j)!





E



2 2

2

cj (wiT x)j (wiT y)2-j j!(2 - j)! 

=

j=0

j=0

 2

42 = d(d - 2) E 

cj (wiT x)j (wiT y)2-j j!(2 - j)! 

=

j=0

4 = d(d - 2)

3c20 x 4 + 3c22 y 4 + (4c12 + 2c0c2)( x 2 y 2 + 2(xT y)2) 4d(d + 2)



 we assume that x  y



24 d2(d2 -

4)

M22

x

4,

(13)

where M2 = max
j=0,1,2

2g z1j z22-j (0, 0)

Let's estimate covariance Cov(Ai, Bi):

.

Cov(Ai, Bi)  E(AiBi) + |EAiEBi|.

The first term of the right hand side:



2 E  2

2

cj (wiT x)j (wiT y)2-j j!(2 - j)!

4

dij (wiT x)j (wiT y)4-j j!(4 - j)!





j=0

j=0

we assume that x  y



210M1M2 d2(d + 2)(d

x +

6
4)

.

The second term EAiEBi (again for x  y )

|EAiEBi|



2M1M2 x d3(d + 2)

6
.

Combining the derived inequalities we obtain

VSi



24 d2(d2 -

4)

M22

x

4+

2.66M12 x d(d + 2)

8

+

210M1M2 x 6 d2(d + 2)(d + 4)

+

2M1M2 x 6 d3(d + 2)





2.66M12 x d(d + 2)

8

+

212M1M2 x d3(d + 2)

6

+

24M22 d2(d2

x4 - 4)

.

(14)

2. Now, let's examine the expectation of the second term in (9):

E

2

ck k!(2 -

k)!

(wi

x)k (wi

y)2-k

=

k=0

c0 2

E(wi

y)2 + c1E(wi

x)(wi

y) +

c2 2

E(wi

x)2



 we assume that x  y



M2

x

2
.

d

(15)

13

Under review as a conference paper at ICLR 2018

(a) (b)
Figure 4: (a) Butterfly orthogonal matrix factors for d = 16. (b) Sparsity pattern for BPBPBP (left) and B (right), d = 15.

Substituting (14) and (15) into (9) we obtain

V SRQ3,3,(fxy)  (d + 1)

2.66M12 x d(d + 2)

8

+

212M1M2 x d3(d + 2)

6

+

24M22 d2(d2

x4 - 4)

+

1 4d(d -

2)

d(d

-

1)

M2 x 2 d

2




2.66M12 d

x

8

+

212M1M2 d3

x

6

+

(d

+ 95)M22 x 4d2(d - 2)

4
.

+

And finally

V

1 n

n
SRQ3,3i,i (fxy)



2.66M12 nd

x

8

+

212M1M2 nd3

x

6

+

(d + 95)M22 4nd2(d -

x 2)

4
.

i=1

C BUTTERFLY MATRICES GENERATION DETAILS

C.1 NOT A POWER OF TWO
We discuss here the procedure to generate butterfly matrices of size d × d when d is not a power of 2.
Let the number of butterfly factors k = log d . Then B(d) is constructed as a product of k factor matrices of size d × d obtained from k matrices used for generating B(2k). For each matrix in the product for B(2k), we delete the last 2k - d rows and columns. We then replace with 1 every ci in the remaining d × d matrix that is in the same column as deleted si.
For the cases when d is not a power of two, the resulting B has deficient columns with zeros (Figure 4b, right), which introduces a bias to the integral estimate. To correct for this bias one may apply additional randomization by using a product BP, where P  {0, 1}d×d is a permutation matrix. Even better, use a product of several BP's: B = (BP)1(BP)2 . . . (BP)t. We set t = 3 in the experiments.

C.2 BUTTERFLY RANDOMIZATION

The key to uniformly random orthogonal butterfly matrix B is the sequence of d-1 angles i. To get B(d) Haar distributed, we follow Fang & Li (1997) algorithm that first computes a uniform random

point u from Ud. It then calculates the angles by taking the ratios of the appropriate u coordinates

i

=

,ui
ui+1

followed

by

computing

cosines

and

sines

of

the

's.

D KERNEL APPROXIMATION ERRORS ON DIFFERENT DATA SETS

Table 2 displays the settings for the experiments across datasets.

14

Under review as a conference paper at ICLR 2018

Arc-cosine 0 K- ^K K

Arc-cosine 1 K- ^K K

Gaussian K- ^K K

Table 2: Experimental settings for the datasets. N is the total number of objects, d is dimensionality of the original feature space.

Dataset
Powerplant LETTER
USPS MNIST CIFAR100 LEUKEMIA

N
9568 20000 9298 70000 60000
72

d
4 16 256 784 784 7129

Number of samples
550 550 550 50 50 10

Number of runs
500 500 500 50 50 10

Powerplant

LETTER

USPS

MNIST

×10-1 5.6 4.8 4.0 3.2 2.4 1.6

2.00 ×10-1 1.75 1.50 1.25 1.00 0.75

×10-2
5.6 4.8 4.0 3.2 2.4 1.6

×10-2
3.5 3.0 2.5 2.0 1.5 1.0

12345

12345

12345

12345

nnnn

×10-1 7.5 6.0 4.5 3.0 1.5
12345 n

×10-1

×10-1

×10-2

4 3 2

1.0 0.8 0.6 0.4

6.0 4.5 3.0

1 0.2 1.5

0 0.0

12345

12345

12345

nnn

×10-1

×10-2

×10-2

1.25 6.0 1.00 4.5 0.75 3.0 0.50
0.25 1.5 0.00

3.0 2.5 2.0 1.5 1.0 0.5

12345

12345

12345

nnn

×10-3
4 3 2 1 0
12345 n

Halton Gort ROM G H B

Figure 5: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,

Gaussian) on three datasets: Powerplant (d = 4), LETTER (d = 16), USPS (d = 256), MNIST

(d = 784). Lower is better. The x-axis represents the factor to which we extend the original

feature space, n

=

D 2(d+1)

,

where

d

is

the

dimensionality

of

the

original

feature

space,

D

is

the

dimensionality of the new feature space.

Here we discuss the datasets that did not appear in the main body of the paper. Figure 5 shows the results for the kernel approximation error on Powerplant, LETTER, USPS and MNIST datasets. For these datasets we include QMC with Halton sequences into comparison.
Quasi-Monte Carlo integration boasts improved rate of convergence 1/D compared to 1/D of Monte Carlo, however, empirical results illustrate its performance is poorer than that of orthogonal random features (Felix et al. (2016)). For QMC the weight matrix M is generated as a transformation of quasi-random sequences. We run our experiments with Halton sequences in compliance with the previous work (see Figure 5 with QMC method included).
Although, for the arc-cosine kernels, our methods are the best performing estimators, for the Gaussian kernel the error is not always the lowest one and depends on the dataset, e.g. on the USPS dataset the lowest is Monte Carlo with ROM. However, for the most of the datasets we demonstrate superiority of our approach with this kernel.
We also notice that the dataset with a small amount of features, Powerplant, enjoys Halton and Orthogonal Random Features best, while ROM's convergence stagnates at some point. This could be due the small input feature space with d = 4 and we leave it for the future investigation.

15

