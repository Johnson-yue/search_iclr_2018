Under review as a conference paper at ICLR 2018

DISSECTING ADAM: THE SIGN, MAGNITUDE AND VARIANCE OF STOCHASTIC GRADIENTS
Anonymous authors Paper under double-blind review

ABSTRACT
The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn't. Why? We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance. We disentangle these two aspects and analyze them in isolation, shedding light on ADAM's inner workings. Transferring the "variance adaptation" to momentum-SGD gives rise to a novel method, completing the practitioner's toolbox for problems where ADAM fails.

1 INTRODUCTION

Many prominent machine learning models pose empirical risk minimization problems of the form

min L()
Rd

=

1 M

M k=1

(; xk),

with gradient

L()

=

1 M

M

 (; xk),

k=1

(1)

where   Rd is a vector of parameters, {x1, . . . , xM } a training set, and (; x) is a loss quantifying the performance of parameter vector  on example x. Computing the exact gradient in each step of an iterative optimization algorithm becomes inefficient for large M . Instead, we construct a minibatch B  {1, . . . , M } of |B| M data points sampled uniformly and independently from the training set and compute an approximate stochastic gradient

g()

=

1 |B|

 (; xk),

kB

(2)

which is an unbiased estimate, E[g()] element-wise variances.1

=

L().

We will denote by ()i2

:=

var[g()i] its

The basic stochastic optimizer is stochastic gradient descent (SGD, Robbins & Monro, 1951) and its momentum variants (Polyak, 1964; Nesterov, 1983). A number of methods, widely-used in the deep learning community, choose per-element update magnitudes based on the history of stochastic gradient observations. Among these are ADAGRAD (Duchi et al., 2011), RMSPROP (Tieleman & Hinton, 2012), ADADELTA (Zeiler, 2012) and ADAM (Kingma & Ba, 2015).

1.1 A CLOSER LOOK AT ADAM We start out from a reinterpretation of the widely-used ADAM optimizer. Some of the considerations naturally extend to ADAM's close relatives RMSPROP and ADADELTA, but we restrict our attention to ADAM to keep the presentation concise. ADAM maintains moving averages of the observed stochas-

1 With k  U ({1, . . . , M }), the gradient  k() :=  (, xk) is a random variable with mean

E[ k()] = L() and variances var[ k()i] = M -1

M k =1

(

k

()i

- L()i)2.

If

the elements

of B are drawn iid with replacement, the variance of g() scales inversely with |B|: ()i2 := var[g()i] =

|B|-1var[ k()i]. This holds approximately for sampling without replacement if |B| M .

1

Under review as a conference paper at ICLR 2018

2 2

3 2 2

True gradient Stochastic gradient (SG)

1
2 0

-1 1 1

-1 0 1 2 1

3

3 Adapted true gradient Adapted SG
2

1

0

-1 -1 0 1 2 1

3

Figure 1: Conceptual sketch of variance adaptation, ignoring the sign aspect of ADAM. The left panel

shows the true gradient L = (2, 1) and stochastic gradients scattered around it with (1, 2) =

(1, 1.5). In the right panel, we employ a variance adaptation (to be derived in §3.2) that scales the

t(ih-te22hu=cpodo2ar.td2ei5nd)aitrtheeacbntiyoth(n1ea+t 1th-cei2o)eo-xr1pd.einInnasteetho(ifs12beix=aasmi0np.g2le5it,)atahwneadyi2sf-rctohomuosrdthsihneoatrtretueehnaegsdra.mdTuiehcnihst

higher relative variance reduces the variance of in expectation.

tic gradients and their element-wise square2,

m~ t = 1m~ t-1 + (1 - 1)gt, v~t = 2v~t-1 + (1 - 2)gt2,

mt = (1 - 1t )-1m~ t, vt = (1 - 2t )-1v~t.

(3) (4)

Here, mt and vt are "bias-corrected" versions of the exponential moving averages to obtain convex combinations of past observed (squared) gradients. ADAM then updates

t+1 = t -  vmt +t 

(5)

with a small constant  > 0 guaranteeing numerical stability of this division. Ignoring  and assuming |mt,i| > 0 for the moment, we can rewrite the update direction as3

mvtt

=

sign(mvtt)|mt|

=

sign(mt)
vt m2t

=

sign(mt) . 1 + vt-m2t
mt2

(6)

Since mt and vt approximate the first and second moment of the stochastic gradient gt, respectively, tvhte-nomn-2tcceanntrablesseeceonndasmaonmeestnitmeaffteecotifveelleymreemnto-wveissethsetomchaagsntiitcugdreaodfiemntt;viatroiannlyceasp. pTeharesdinivtihsieornatbiyo (vt - m2t )/mt2. Hence, ADAM can be interpreted as a combination of the two following aspects:

· The update direction (±) for the i-th weight is given by the sign of mt,i. · The update magnitude for the i-th weight is uniquely determined by the global step size 
and an estimate of the relative variance,

^t2,i

:=

vt,i - mt2,i mt2,i



t2,i L2t,i

=: t2,i.

(7)

iSnphecigifihc-raelllayt,ivthee-vuaprdiaanteceincothoerdii-nthatecso.oFrdigin.a1teshisowscsalaesdkbeytch1o+f th^it2s,iva-r1i/a2n,csehaodrtaepntiantgiosnte. ps

2 Notation: Divisions, squares, etc. on vectors are to be understood element-wise. denotes element-wise multiplication. We occasionally drop , writing g instead of g(), etc. We use the shorthands Lt, gt, t2, etc. for sequences t and double-indices, e.g. gt,i = g(t)i, to denote vector elements.
3 For convenience, we define sign(x) = 1 for x  0 and sign(x) = -1, x < 0, for our theoretical considerations, but use sign(0) = 0 in practice. Application to vectors is to be understood element-wise.

2

Under review as a conference paper at ICLR 2018

Table 1: The methods under consideration in this paper.

Sign + Magnitude

Sign

Not Variance-Adapted

SGD

SSD

"Stochastic Sign Descent"

Variance-Adapted

SVAG "Stochastic Variance-Adapted Gradient"

ADAM

1.2 OVERVIEW

Both aspects of ADAM--taking the sign and variance adaptation--are briefly mentioned in Kingma

a&ndBare(f2e0r1t5o),mwth/onvotteasthaat""si[gt]nhael-etfof-encotiivsee

stepsize [...] ratio". The

is also invariant to the scale of the gradients" purpose of this paper is to disentangle these

two intertwined aspects in order to discuss and analyze them in isolation.

This perspective naturally suggests two alternative methods by incorporating one of the aspects but not the other (see Table 1). Taking the sign of the stochastic gradient (or momentum term) without any further modification gives rise to "Stochastic Sign Descent" (SSD). On the other hand, "Stochastic Variance-Adapted Gradient" (SVAG) applies element-wise variance adaptation factors directly on the stochastic gradient (or momentum term) instead of on its sign. We proceed as follows: In Section 2, we investigate the sign aspect. In the simplified setting of stochastic quadratic problems, we derive conditions under which the element-wise sign of a stochastic gradient can be a better update direction than the stochastic gradient itself. Section 3 discusses the variance adaptation. We present a principled derivation of "optimal" element-wise variance adaptation factors for a stochastic gradient as well as its sign. Subsequently, we incorporate momentum and briefly discuss the practical estimation of stochastic gradient variance. Section 4 presents some experimental results.

1.3 RELATED WORK The idea of using the sign of the gradient as the principal source of the optimizer update has already received some attention in the literature. The RPROP algorithm (Riedmiller & Braun, 1993) ignores the magnitude of the gradient and dynamically adapts the per-element magnitude of the update based on observed sign changes. With the goal of reducing communication cost in distributed training of neural networks, Seide et al. (2014) empirically investigate the use of the sign of stochastic gradients. Regarding the variance adaptation, Schaul et al. (2013) derive element-wise step sizes for stochastic gradient descent that have (among other factors) a dependency on the stochastic gradient variance.

1.4 THE SIGN OF A STOCHASTIC GRADIENT

We briefly establish a fact that will be used throughout the paper. The sign of a stochastic gradient s() = sign(g()) estimates the sign of the true gradient. Its distribution (and thus the quality of this estimate) is fully characterized by the success probabilities i := P [s()i = sign(L()i)]. These depend on the distribution of the stochastic gradient. If we assume g() to be Gaussian--which is strongly supported by a Central Limit Theorem argument on Eq. (2)--we have

i

:=

P [s()i

=

sign(L()i)]

=

1 2

+

1 2

erf

|L()i| 2()i

,

see §B.2 in the supplements. Furthermore, it is E[s()i] = (2i - 1) sign(L()i).

(8)

2 WHY THE SIGN?
Can it make sense to ignore the gradient magnitude? We provide some intuition under which circumstances the element-wise sign of a stochastic gradient is a better update direction than the stochastic gradient itself. This question is difficult to tackle in general, which is why we restrict the problem

3

Under review as a conference paper at ICLR 2018

class to the simple, yet insightful, case of stochastic quadratic problems, where we can investigate the effects of curvature properties and its interaction with stochastic noise. Model Problem (Stochastic Quadratic Problem (QP)). Consider the loss function (, x) = 0.5 ( - x)T Q( - x) with a symmetric positive definite matrix Q  Rd and "data" coming from the distribution x  N (x, 2I). It is

L()

:=

Ex[

(, x)]

=

1 2

(

-

x)T Q(

-

x)

+

2 2

tr(Q),

(9)

with L() = Q( - x). Stochastic gradients are given by g() = Q( - x)  N (x, 2I).

2.1 THEORETICAL COMPARISON

We want to compare update directions on stochastic QPs in terms of their expected decrease in function value from a single update step. If we update from  to  + z, we have

E[L(

+

z)]

=

L()

+

L()T

E[z]

+

2 2

E[z

T

Qz].

(10)

For this comparison of update directions, we allow for the optimal step size that minimizes Eq. (10), which is easily found to be  = -L()T E[z]/E[zT Qz] and yields an expected improvement of

I (z )

:=

|E[L(

+ z)] - L()|

=

(L()T 2E[zT

E[z])2 Qz]

.

(11)

We find the following expressions/bounds for the improvement of SGD and SSD:

I (g)

=

1 2

(L()T L())2 L()T QL() + 2

d i=1

i3

,

I (s)



1 2

di=1(2i - 1)|L()i| 2

d i,j

=1

|qij

|

(12)

where the i  R+ are the eigenvalues of Q with orthonormal eigenvectors vi  Rd. Derivations can be found in §B.1 of the supplements. Comparing these expressions, we make two observations.

Firstly, I(s) has a dependency on i,j |qij|. This quantity relates to the eigenvalues, as well as the orientation of the eigenbasis of Q. By writing Q in its eigendecomposition one finds that
i,j |qij|  i i vi 21. If the eigenvectors are perfectly axis-aligned (diagonal Q), their 1-norms are vi 1 = vi 2 = 1. It is intuitive that this is the best case for the intrinsically axis-aligned sign update. In general, the 1-norm is only bounded by vi 1  d vi 2 = d, suggesting that the sign update will have difficulties with arbitrarily oriented eigenbases. We can alternatively express this matter in terms of "diagonal dominance". Assuming Q has a percentage c  [0, 1] of its "mass" on the diagonal, i.e., i |qii|  c i,j |qij|, we can write

I (s)



1 2

di=1(2i - 1)|L()i|

c-1

d i=1

|qii

|

2

=

1 2

id=1(2i - 1)|L()i| 2

c-1

d i=1

i

.

(13)

Becker & LeCun (1988) empirically investigated the diagonal dominance of Hessians in optimization problems arising from neural networks and found relatively high percentages of mass on the diagonals of c = 0.1 up to c = 0.6 for the problems they investigated.

Secondly, I(g) contains the constant offset 2 hugely obstructive for ill-conditioned and noisy

proid=b1lemi3s.inInthIe(sd)e,noonmtihneatoort,hewr hhiacnhdc, athnebreeciosmnoe

such interaction between the magnitude of the noise and the eigenspectrum; the noise only manifests

in the element-wise success probabilities i, its effect in the denominator is bounded. A recent paper (Chaudhari et al., 2016) investigated the eigenspectrum in deep learning problems and found it to be

very ill-conditioned with the majority of eigenvalues close to zero and a few very large ones.

In summary, we can expect the sign update to be beneficial for noisy, ill-conditioned problems with "diagonally dominant" Hessians. There is some (weak) empirical evidence that these conditions might be fulfilled in deep learning problems.

4

Under review as a conference paper at ICLR 2018

Rotated Axis-aligned Rotated Axis-aligned

Eigenvalues
10

Noise-free
103 100

Low noise

High noise

0 10-3 0.5 1.0 103

100

10-3 103

50 102

00

25

50

101 103

SGD

101

SSD 10-1 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100

Steps

Steps

Steps

Figure 2: Performance of SGD and SSD on 100-dimensional stochastic quadratic problems. Rows correspond to different QPs: the eigenspectrum is shown and each is used with a randomly rotated and an axis-aligned eigenbasis. Columns correspond to different noise levels. Horizontal axis is number of steps; vertical axis is log function value and is shared per row for comparability.

2.2 EXPERIMENTAL EVALUATION We verify the above findings on artificially generated stochastic QPs, where all relevant quantities are known analytically and controllable. We control the eigenspectrum by specifying a diagonal matrix  of eigenvalues: (1) a mildly-conditioned problem with eigenvalues drawn uniformly from [0.1, 1.1] and (2) an ill-conditioned problem with a structured eigenspectrum similar to the one reported for neural networks by Chaudhari et al. (2016) by uniformly drawing 90% of the eigenvalues from [0, 1] and 10% from [30, 60]. Q is then generated by (1) Q =  to produce an axis-aligned problem and (2) Q = RRT with a rotation matrix R drawn uniformly at random (see Diaconis & Shahshahani, 1987). This makes four different matrices, which we consider at noise levels   {0, 0.1, 4.0}. We compare SGD and SSD, both with the optimal step size as derived from Eq. (10), which can be computed exactly in this setting. Figure 2 shows the results, which confirm the theoretical findings. On the well-conditioned, noisefree problem, gradient descent vastly outperforms the sign-based method. Surprisingly, adding even a little noise almost evens out the difference in performance. The orientation of the eigenbasis had little effect on the performance of SSD in the well-conditioned case. On the ill-conditioned problem, the methods work roughly equally well when the eigenbasis is randomly rotated. As predicted, SSD benefits drastically from an axis-aligned eigenbasis (last row), where it clearly outperforms SGD.
3 VARIANCE-BASED ELEMENT-WISE STEP SIZE ADAPTATION
Besides the sign direction, the other defining property of ADAM are variance-based element-wise step sizes. Considering the variance adaptation in isolation from the sign aspect naturally suggests to employ it directly on the stochastic gradient, without taking the sign. In both cases, a motivation arises from the following consideration: Assume we want to update in a direction p  Rd (or sign(p)), but only have access to an unbiased estimate p^  Rd with E[p^] = p. We allow for element-wise factors   Rd, i.e., we update  p^ or  sign(p^). One way to make "optimal" use of these factors is to choose them such as to minimize the expected distance to the desired update direction. Using the squared Euclidean norm as a distance measure, we find the following result.
5

Under review as a conference paper at ICLR 2018

Lemma 1. Let p^  Rd be a random variable with E[p^] = p and var[pi] = i2. Then

min E[ 
Rd

p^ - p 22]

is solved by

i

=

pi2 pi2 + i2

=

1 1 + i2/pi2

and

min E[ 
Rd

sign(p^) - sign(p) 22] is solved by i = (2i - 1),

where i = P[sign(p^i) = sign(pi)].

(14) (15)

In the sign case, i is proportional to the success probability with i = 1 if we are certain about the sign (i = 1) and i = 0 if we have no information about the sign at all (i = .5).

3.1 VARIANCE ADAPTATION FOR THE SIGN OF A STOCHASTIC GRADIENT

Applying Eq. (15) to p^ = g, the optimal variance adaptation factors for the sign of a stochastic

gradient are found to be i = 2i - 1, where i = P[sign(gi) = sign(Li)]. Recall from Eq. (8)

otahrueatt2,touibn-edea1r ct=lhoeseerGfaa[p(upsrs2oiaxnii)m-aas1ts]iu.omnApDotAfioeMnr,fu[ts(hees2tshuiec)cv-ea1sr]si,apansrcoesbhaaodbwailpnittaiientisoFonigffuathrceeto5srisign(n1tho+ef

a stochastic gradient i2)-1/2, which turns supplements. Hence,

ADAM can be regarded as an approximate realization of this optimal variance adaptation scheme.

We experimented with both variants and found them to have identical effects. The small difference

between them can be regarded as insignificant when  itself is subject to approximation error. We

thus stick to (1 + i2)-1/2 for accordance with ADAM and to avoid the (more costly) error function.

3.2 STOCHASTIC VARIANCE-ADAPTED GRADIENT (SVAG)

Applying Eq. (14) to p^ = g, the optimal variance adaptation factors for SGD are found to be

i

=

1 1 + i2/Li2

=

1

1 +

i2

.

(16)

This term is known from Schaul et al. (2013), where it appears together with diagonal curvature estimates in element-wise step sizes for SGD. We refer to this method (without curvature estimates) as "Stochastic Variance-Adapted Gradient" (SVAG). A momentum variant will be derived below.

Intriguingly, variance adaptation of this form guarantees convergence without manually decreasing

the global step size. We recover the O(1/t) rate of SGD for smooth, strongly convex functions. We

emphasize that for this form of

this result considers variance adaptation,

an "idealized" version of not a statement about the

pSeVrAfoGrmwaitnhceexwaictthei2s.tiImt iasteadmvaortiivanatcieosn.

Theorem 1. Let f be µ-strongly convex and L-smooth. Assume we update t+1 = t - (t gt), where gt is a stochastic gradient with E[gt|t] = f (t), var[gt,i|t] = t2,i, variance adaptation factors t,i = (1 + t2,i/ft2,i)-1, and  = 1/L. Assume E[ gt 2]  G2. Then

E[f (t) - f]  O

1 t

,

where f is the minimum value of f .

(17) (Proof in §B.4)

3.3 ESTIMATING GRADIENT VARIANCE

In practice, the relative variance is of course not known and must be estimated. As noted in the

introduction, ADAM obtains an estimate of the stochastic gradient variance from moving averages,

ovt2,eirthse^t",ief=fevctti,vi e-tmim2te,i.hTorhiezounn"deorflythinegmaosvsuinmgpatvioenraigset,hsaut cthhethfuant cthtieonredcoeenst

not change drastically gradients can approx-

imately be considered to be iid draws from the stochastic gradient distribution. An estimate of the

relative variance can then be obtained by (vt - m2t )/(mt2), as in ADAM.

Unlike ADAM we do not use different moving average constants for mt and vt. The constant for the moving average should define a time horizon over which the gradients can approximately be

6

Under review as a conference paper at ICLR 2018

considered to come from the same distribution. From this perspective, it is hardly justifiable to use different horizons for the gradient and its square. Furthermore, we found individual moving average constants for mt and vt to have only minor effect on the performance of our methods. An alternative variance estimate can be computed locally "within" a single mini-batch. A more detailed discussion of both estimators can be found in §C of the supplements. We have experimented with both estimators and found them to work equally well for our purpose of variance adaptation. We thus stick to moving average-based estimates for the main paper. Appendix D provides details and experimental results for the mini-batch variant.

3.4 INCORPORATING MOMENTUM

When we add momentum--i.e., we want to update in the direction rt or sign(rt) with a momentum

term rt = µrt-1 + the relative variance

gt of

= rt,

accts=or0dµinsggtt-osL--emthme ava1r.iaInt cise

adaptation

factors

should

be

determined

by

t
E[rt] = µsLt-s,
s=0

tt

var[rt,i] = (µs)2var[gt-s,i] = µ2st2-s,i.

s=0

s=0

(18)

Replacing E[gt-s]  mt-s However, this would require

and two

avdadrit[igotn-asl] movvint-g sav-ermag2te-ssanwde

could compute these can thus be discarded

quantities. as imprac-

tical. Fortunately, we can motivate an approximation that does not require any additional memory

requirements (see §C):

var[rt] E[rt]2



(µ,

t)

vt

- m2t mt2

with

(µ, t)

:=

(1 (1

- -

µ2t)(1 - µ)2 µ2)(1 - µt)2

.

(19)

Note that the correction factor (µ, t) does not appear in ADAM, which updates in the direction

sign(mt) = sign(rt) but contain experiments with

performs a variant

variance of ADAM

adaptation based that includes this

ocnor(rvect t-ionmft2a)c/tomr.2t .

The

supplements

4 EXPERIMENTS
We compare momentum-SGD (M-SGD) and ADAM to two new methods: First, we consider M-SSD: stochastic sign descent using a momentum term. The second method is M-SVAG, i.e., SGD with momentum and variance adaptation of the form (1 + 2)-1, where the relative variance of the momentum term is estimated from moving averages according to Eq. (19). These four methods are the four possible recombinations of the sign aspect and the variance adaptation aspect of ADAM, as laid out in Table 1. Algorithms 1 and 2 provide pseudo-code for M-SSD and M-SVAG. For all experiments, we use µ = 0.9 for M-SGD, M-SSD and M-SVAG and default parameters (1 = 0.9, 2 = 0.999,  = 10-8) for ADAM. Note that M-SVAG does not use an -parameter, see Alg. 2.
Algorithm 1 M-SSD (Stochastic Sign Descent with Momentum) Require: initial value 0, step size , momentum parameter µ  [0, 1], number of steps T 1: Initialize m = 0, v = 0 2: for t = 1, . . . , T do 3: Compute stochastic gradient g = g() 4: Update moving average m  µm + g 5: Update    -  sign(m) 6: end for
7

Under review as a conference paper at ICLR 2018

Algorithm 2 M-SVAG (Stochastic Variance-Adapted Gradient with Momentum)

Require: initial value 0, step size , momentum parameter µ  [0, 1], number of steps T 1: Initialize m~ = 0, v~ = 0 2: for t = 1, . . . , T do 3: Compute stochastic gradient g = g() 4: Update moving averages m~  µm~ + (1 - µ)g, v~  µv~ + (1 - µ)g2 5: Bias-correct m = (1 - µt)-1m~ , v = (1 - µt)-1v~

6:

Compute

relative

variance

estimate

2

=

(µ,

t)

v-m2 m2

7: Compute variance adaptation factors  = (1 + 2)-1

8: Update    - ( m)

9: end for

Eq. (19)

We do not use an -parameter as in ADAM. In the (rare) case that mi = 0 for coordinate i, the division by zero in line 6 is caught and the update magnitude will be set to zero in line 8.

4.1 EXPERIMENTAL SET-UP We tested all methods on three problems: a simple fully-connected neural network on the MNIST data set (LeCun et al., 1998), as well as convolutional neural networks (CNNs) on the CIFAR-10 and CIFAR-100 data sets (Krizhevsky, 2009). On CIFAR-10, we used a simple CNN with three convolutional layers, interspersed with max-pooling, and three fully-connected layers. On CIFAR100 we used the AllCNN architecture of Springenberg et al. (2014) with a total of nine convolutional layers. A complete description of all network architectures has been moved to §A. While MNIST and CIFAR-10 are trained with a constant global step size (), we used a fixed decreasing schedule for CIFAR-100, dividing by 10 after 40k and 50k steps (adopted from Springenberg et al., 2014). We used a batch size of 128 on MNIST and 256 on the two CIFAR data sets. Step sizes (initial step sizes in the case of CIFAR-100) were tuned for each method individually by first finding the maximal stable step size by trial and error, then searching downwards over two orders of magnitude (details in §A). We selected the one that yielded maximal overall test accuracy within the fixed number of training steps. Experiments with the best step size have been replicated ten times with different random seeds and all performance indicators are reported as mean plus/minus one standard deviation.
4.2 RESULTS Results are shown in Figure 3. On MNIST, ADAM clearly outperforms M-SGD. Interestingly, there is only a very small difference in performance between the two sign-based methods, M-SSD and ADAM. Apparently, the advantage of ADAM over M-SGD on this problem is primarily due to the sign aspect. Going from M-SGD to M-SVAG, gives a considerable boost in performance, but MSVAG is still outperformed by the two sign-based methods. On CIFAR-10, the sign-based methods again have superior performance. Neither M-SSD nor M-SGD can benefit significantly from adding variance adaptation. Finally, the situation is reversed on CIFAR-100, where M-SGD outperforms ADAM. It attains lower minimal loss values (both training and test) and converges faster. This is also reflected in the test accuracies, where M-SGD beats ADAM by almost 10 percentage points. Furthermore, ADAM is much less stable with significantly larger variance in performance. On this problem, variance adaptation has a small but significant positive effect for the sign-based methods as well as for M-SGD. When going from M-SGD to M-SVAG we gain some speed in the initial phase. The difference is later evened out by the manual learning rate decrease (which was necessary, for all methods, to train this architecture to satisfying performance).
5 DISCUSSION AND CONCLUSION
We have argued that ADAM combines two aspects: taking signs and variance adaptation. Our separate analysis of both aspects provides some insight into the inner workings of this method.
8

Under review as a conference paper at ICLR 2018

Training loss

MNIST

1.4

M-SGD

1.4

Training loss

1.2 1.0

ADAM
M-SSD M-SVAG

1.2 1.0

0.8

0.6 0.8

0.4 0.6

1.4 1.2 1.0 0.8 0.6 0.4
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Steps (·10-3)

Test loss

1.4 1.2 1.0 0.8 0.6
0

CIFAR-10
5 10 15 20 25 Steps (·10-3)

Test loss

Training loss

4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0
0

CIFAR-100
10 20 30 40 50 60 Steps (·10-3)

Test loss

M-SGD
ADAM
M-SSD M-SVAG

92.0 ± 0.2% 91.8 ± 0.3% 92.0 ± 0.2% 91.6 ± 0.4%

Test accuracies

M-SGD

84.5 ± 0.6%

ADAM
M-SSD M-SVAG

84.8 ± 0.6% 85.0 ± 0.4% 84.5 ± 0.8%

M-SGD
ADAM
M-SSD M-SVAG

67.8 ± 0.3% 58.3 ± 1.3% 51.4 ± 4.9% 67.7 ± 0.2%

Figure 3: Experimental results on the three test problems. Plots display training and test loss over the number of steps. Curves for the different optimization methods are color-coded. The shaded area spans plus/minus one standard deviation, obtained from ten replications. The table below contains test accuracies evaluated after the last iteration.

Taking the sign can be beneficial, but does not need to be. Our theoretical analysis suggests that it depends on the interplay of stochasticity, the conditioning of the problem, and its "axis-alignment". Our experiments confirm that sign-based methods work well on some, but not all problems. Variance adaptation can be applied to any stochastic update direction. In our experiments it was beneficial in all cases, but its effect can sometimes be minuscule. M-SVAG, a variance-adapted variant of momentum-SGD, is a useful addition to the practitioner's toolbox for problems where sign-based methods like ADAM fail. Its memory and computation cost are identical to ADAM and it has two hyper-parameters, the momentum constant µ and the global step size . Our TensorFlow (Abadi et al., 2015) implementation of this method will be made available upon publication.
ACKNOWLEDGMENTS We want to thank [names removed] for many helpful discussions.
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
9

Under review as a conference paper at ICLR 2018
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org. Lukas Balles, Maren Mahsereci, and Philipp Hennig. Automizing stochastic optimization with gradient variance estimates. In Automatic Machine Learning Workshop at ICML 2017, 2017a. Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence (UAI), pp. 410­419, 2017b. Sue Becker and Yann LeCun. Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 Connectionist Models Summer School, pp. 29­37, 1988. Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-SGD: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016. Persi Diaconis and Mehrdad Shahshahani. The subgroup algorithm for generating uniform random variables. Probability in the Engineering and Informational Sciences, 1(01):15­32, 1987. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011. Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. The International Conference on Learning Representations (ICLR), 2015. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. In Advances in Neural Information Processing Systems 28, pp. 181­189, 2015. Maren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without a validation set. arXiv preprint arXiv:1703.09580, 2017. Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372­376, 1983. Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964. Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on, pp. 586­591. IEEE, 1993. Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pp. 400­407, 1951. Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In Proceedings of the 30th International Conference on Machine Learning (ICML), pp. 343­351, 2013. Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. Tijmen Tieleman and Geoffrey Hinton. RMSPROP: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, Lecture 6.5, 2012. Matthew D Zeiler. ADADELTA: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
10

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY MATERIAL A DESCRIPTION OF EXPERIMENTS
A.1 NETWORK ARCHITECTURES MNIST We train a simple fully-connected neural network with three hidden layers of 1000, 500 and 100 units with ReLU activation. The output layer has 10 units with softmax activation. We use the cross-entropy loss function and apply L2-regularization on all weights, but not the biases. We use a batch size of 128. The global learning rate  stays constant. CIFAR-10 The CIFAR-10 data set consists of 32×32px RGB images with one of ten categorical labels. We train a convolutional neural network (CNN) with three convolutional layers (64 filters of size 5×5, 96 filters of size 3×3, and 128 filters of size 3×3) interspersed with max-pooling over 3×3 areas with stride 2. Two fully-connected layers with 512 and 256 units follow. We use ReLU activation function for all layers. The output layer has 10 units for the 10 classes of CIFAR-10 with softmax activation. We use the cross-entropy loss function and apply L2-regularization on all weights, but not the biases. During training we perform some standard data augmentation operations (random cropping of sub-images, left-right mirroring, color distortion) on the input images. We use a batch size of 256. The global learning rate  stays constant. CIFAR-100 We use the AllCNN architecture of Springenberg et al. (2014). It consists of seven convolutional layers, some of them with stride, and no pooling layers. The fully-connected layers are replaced with two layers of 1×1 convolutions with global spatial averaging in the end. ReLU activation function is used in all layers. Details can be found in the original paper. We use the cross-entropy loss function and apply L2-regularization on all weights, but not the biases. We used the same data augmentation operations as for CIFAR-10 and a batch size of 256. The global learning rate  is decreased by a factor of 10 after 40k and 50k steps.
A.2 LEARNING RATE TUNING Learning rates for each optimizer have been tuned by first finding the maximal stable learning rate by trial and error and then searching downwards over two orders of magnitude with learning rates 6 · 10m, 3 · 10m, and 1 · 10m for order of magnitude m. We evaluated loss and accuracy on the full test set at a constant interval and selected the best-performing learning rate for each method in terms of maximally reached test accuracy. Using the best learning rate, we replicated the experiment ten times with different random seeds.

B MATHEMATICAL DETAILS

B.1 DETAILS OF THE ANALYSIS ON STOCHASTIC QPS

We derive the expressions for I(s) and I(g) in Eq. (12). We drop the fixed  from the notation for

readability. For SGD, we have E[g] = L and E[gT Qg] = LT QL + tr(Qcov[g]), which is a

general fact for quadratic forms of random variables. For the stochastic QP the gradient covariance is

cov[g] = 2QQ, thus tr(Qcov[g]) = 2 tr(QQQ) = 2 yields

i i3. Plugging everything into Eq. (11)

I (g)

=

(LT L)2 LT QL + 2

d i=1

. i3

(20)

For stochastic sign descent, we have E[si] = (2i - 1) sign(Li) and thus LT E[s] =

d i=1

LiE[si]

=

i(2i - 1)|Li|. Regarding the denominator, it is

dd

d

0  sT Hs = |sT Qs| =

qij sisj  |qij ||si||sj | = |qij |.

i=1 i=1

i=1

(21)

11

Under review as a conference paper at ICLR 2018

pdf(x)

0.6 0.6 0.6 0.5 0.5 0.5 0.4 0.4 0.4 0.3 0.3 0.3 0.2 0.2 0.2 0.1 0.1 0.1 0.0 0.0 0.0
-4 -3 -2 -1 0 1 2 3 4 -4 -3 -2 -1 0 1 2 3 4 -4 -3 -2 -1 0 1 2 3 4 xxx
Figure 4: Probability density functions (pdf) of three Gaussian distributions, all with µ = 1, but different variances 2 = 0.5 (left), 2 = 1.0 (middle), 2 = 4.0 (right). The shaded area under the curve corresponds to the probability that a sample from the distribution has the opposite sign than its mean. For the Gaussian distribution, this probability is uniquely determined by the fraction /|µ|, as shown in Lemma 2.

Plugging everything into Eq. (11) yields I(s) 

id=1(2i - 1)|Li| 2

d i=1

|qij

|

.

(22)

B.2 SUCCESS PROBABILITIES OF THE SIGN OF A STOCHASTIC GRADIENT

We have stated in the main text that the sign of a stochastic gradient, s() = sign(g()), has success probabilities

i

=

P[s()i

=

sign(L()i)]

=

1 2

+

1 2

erf

|L()i| 2()i

(23)

under the assumption that g  N (L, ). The following Lemma formally proves this statement and Figure 4 provides a pictorial illustration.

Lemma 2. If X  N (µ, 2) then



=

P[sign(X )

=

sign(µ)]

=

1 2

1 + erf

|µ| 2

.

(24)

Proof. where

The (z)

cumulative = 0.5(1 +

edrefn(zsi/tyf2u)n)citsiothne(ccddff)oof fthXe

 N (µ, 2) is standard normal

P[X  x] = distribution. If

((x - µ)/), µ < 0, then

If µ > 0, then

 = P[X < 0] = 

0-µ 

=

1 2

1 + erf

-µ 2

.

(25)

 = P[X > 0] = 1 - P[X  0] = 1 - 

0-µ 

=

1

-

1 2

1 + erf

-µ 2

=

1 2

1 + erf

µ 2

where the last step used the anti-symmetry of the error function.

,

(26)

12

Under review as a conference paper at ICLR 2018

Variance adaptation factor

1.00 (1 + 2)-1 0.75 erf[(2)-1]
(1 +  2)-1/2 0.50
0.25
0.00 0123456 

Figure 5: Variance adaptation factors as

functions of the relative standard deviation

. (1 + 2)-1 is the optimal variance adap-

tation factor for SGD (Eq. 16). The optimal

fearcf t(o(rf2or )t-he1 )siugnndoefr

a stochastic gradient is the Gaussian assump-

tion (Eq. 15). It is closely approximated by

(1 + 2)-1/2, which is the factor implicitly

employed by ADAM (Eq. 6).

B.3 DETAILS ON VARIANCE ADAPTATION FACTORS

Proof of Lemma 1. Using E[p^i] = pi and E[p^i2] = pi2 + i2, we get

E[ 

dd

p^ - p

2 2

]

=

E[(ip^i - pi)2] =

i2E[p^2i ] - 2ipiE[p^i] + p2i

i=1 i=1

d
= i2(pi2 + i2) - 2ipi2 + p2i .
i=1

Setting the derivative w.r.t. i to zero, we find the optimal choice

i

=

p2i

pi2 +

i2

.

Using E[sign(p^i)] = (2i - 1) sign(pi) and sign(·)2 = 1, we get

(27) (28)

E[ 

d
sign(p^) - sign(p) 22] = i2E[sign(p^i)2] - 2i sign(pi)E[sign(p^i)] + sign(pi)2 (29)
i=1
= i2 - 2i(2i - 1) + 1

and easily find the optimal choice

i = 2i - 1.

(30)

by setting the derivative to zero.

See Figure 5 for a plot of the variance adaptation factors considered in this paper.

B.4 CONVERGENCE OF IDEALIZED STOCHASTIC VARIANCE-ADAPTED GRADIENT

We proof the convergence results for idealized variance-adapted stochastic gradient descent. We

have to clarify an aspect that we have glossed over in the main text. A stochastic optimizer gen-

eexrapteecstaatidoinscgreivteenstaocrehaalsitziactipornocoefstsh{attp}rtocNe0s.s

We denote as Et[·] = E[·|0, . . . , t] up to time step t. Recall that E[Et[·]]

the conditional = E[·].

Proof of Theorem 1. Using the Lipschitz continuity of f , we can bound f ( + )  f () +

f ()T



+

L 2



2. Hence,

Et[ft+1]  ft - Et[ftT (t

gt)] +

L2 2

Et[

t

gt 2]

=

ft

-

1 L

d

t,ift,iE[gt,i]

+

1 2L

d

t2,iEt[gt2,i]

i=1 i=1

=

ft

-

1 L

d

t,ift2,i

+

1 2L

d

t2,i(ft2,i + t2,i).

i=1 i=1

(31)

13

Under review as a conference paper at ICLR 2018

Plugging in the definition and simplifying, we get

t,i

=

ft2,i ft2,i + t2,i

(32)

Et[ft+1]



ft

-

1 2L

d i=1

ft2,i ft2,i + t2,i

ft2,i

.

Using Jensen's inequality4

(33)

d i=1

ft2,i ft2,i + t2,i

ft2,i

=



=

d
ft 2
i=1

ft2,i ft 2

ft2,i + t2,i ft2,i

ft 2

d ft2,i ft2,i + t2,i i=1 ft 2 ft2,i

ft 4 di=1(ft2,i + t2,i)



ft G2

4
.

-1 -1

Due to strong convexity, we have ft 2  2µ(ft - f) and can further bound

(34)

d i=1

ft2,i ft2,i + t2,i

ft2,i



4µ2(ft - G2

f)2

.

Inserting this in (33) and subtracting f, we get

Et[ft+1]

-

f



ft

-

f

-

2µ2 LG2

(ft

-

f)2,

and, consequently, by total expectation

(35) (36)

E[ft+1

-

f]

=

E [Et[ft+1]

-

f]



E[ft

-

f]

-

2µ2 LG2

E[(ft

-

f)2]



E[ft

-

f]

-

2µ2 LG2

E[ft

-

f]2,

which we rewrite, using the shorthand et := E[ft - f], as

(37)

0  et+1  et(1 - cet),

c

=

2µ2 LG2

.

(38)

To conclude assume et+1

the proof, we > 0 and get

will

show

that

this

implies

et



O(

1 t

).

Without

loss

of

generality,

we

e-t+11  et-1(1 - cet)-1  et-1(1 + cet) = e-t 1 + c,

(39)

where the second step is due to the simple fact that (1-x)-1  (1+x) for any x  [0, 1). Summing this inequality over t = 0, . . . , T - 1 yields e-T 1  e0-1 + T c and, thus,

T eT 

1 T e0

+

c

-1

T-

1 c

<

,

(40)

which

shows

that

et



O(

1 t

).

4 Jensen's inequality cients ci  0, i ci =

says that 1. Here,

we iacpipl(yxiit)tothe(conivcexi xfiu)nfcotiroan

convex function (x) = 1/x, x

 >

and convex coeffi0, and coefficients

ft2,i/ ft 2.

14

Under review as a conference paper at ICLR 2018

C MORE ON GRADIENT VARIANCE ESTIMATION

C.1 ESTIMATES FROM MOVING AVERAGES

Iterating the recursive formula for m~ t backwards, we get

mt

=

m~ t 1 - 1t

=

1 1 - 1t

(1mt-1

+ (1 - 1)gt)

=

...

=

1 - 1 1 - 1t

t-1
1sgt-s.
s=0

(41)

Hence, mt is a weighted average of past observed gradients with coefficients c(1, t, s) := 1s(1 -

T1h)e/a(1na-log1to)u, swshtiacthemsuemnt

to one, since holds for vt.

Ttsh-=e10ba1ssic=ra(1tio-nal1te)/th(1at-fac1i)libtayteths eagveaormiaentcreicessutimmfaotermfruolma.

past gradient observation is to assume that the true gradient does not change drastically over the

effective time horizon of the exponential moving average. For mathematical simplicity, we can

translate this assumption to mean that, at the t-th step, we treat all {gt-s,i | s = 0, . . . , t - 1} as iid

with mean Lt,i and variance t2,i. This will of course be utterly wrong for gradient observations

that are far in the past, but since c(µ, t, s) is very small for large t - s, these won't contribute

significantly to the moving average. The moving average constant defines the effective time horizon,

for which we implicitly make this assumption.

Under this peculiar assumption, mt and vt are unbiased estimates of the first and second moment of gt, respectively:

t-1 t-1

E[mt,i] = c(µ, t, s)E[gt-s,i] = Lt,i c(µ, t, s) = Lt,i,

s=0

s=0

t-1 t-1
E[vt,i] = c(µ, t, s)E[gt2-s,i] = (Lt2,i + t2,i) c(µ, t, s) = Lt2,i + t2,i,

s=0

s=0

(42) (43)

motivating vt estimate due

- to

tmhe2t

should generally be

dfaaoscmtaimgnrat2atdeiidsennbtoytvoaatrhnieaurnnceberrieaossrtesidmouearstcetie.msHaaotnewdoewfveilrl,Ltvh2ttu.-s Tbmeheit2geinsrornoreordta.arnisiunngbifarosemd

variance this bias

C.2 MINI-BATCH ESTIMATES

An alternative gradient variance estimate can be obtained locally, within a single mini-batch. The individual gradients  (, xk) in a mini-batch are iid random variables and, as noted in the introduction, var[g()] = |B|-1var[ (, xk)]. We can thus estimate g()'s variances by computing the sample variance of the { (, xk)}kB, then scaling by |B|-1,

s^()

=

1 |B|

1 |B| - 1

 (, xk)2 - g()2 .

kB

(44)

Several recent papers (Mahsereci & Hennig, 2015; Balles et al., 2017b; Mahsereci et al., 2017) have

used this unbiased

variance estimate

estimate for of the local

ogtrhaedrieanspt evcatrsiaonfcset.ocThhaest(incoonp-ttirmiviizael)rsi.mIpnlceomnetnratsattitoonvot-f tmhis2t ,etshtiismiastaonr

for neural networks is described in Balles et al. (2017a).

C.3 RELATIVE VARIANCE OF A MOMENTUM TERM (DERIVATION OF EQ. 19)

When estimating the variance with moving averages, we assume that E[gt] = mt and var[gt] =

vt - term

bmy2t .

Plugging

this

into

Eq.

(18)

we

can

approximate

the

mean

and

variance

of

the

momentum

t2

t

E[rt]2 

µsmt-s , var[rt]  µ2s(vt-s - m2t-s).

(45)

s=0

s=0

Computing these two expressions would require two more moving averages in addition to mt and vt. However, mt and vt will change slowly over time and, by using vt - mt2 as the variance estimate for

15

Under review as a conference paper at ICLR 2018

gt, we anyways make the assumption that all gradients in the effective time horizon of the moving average have the same mean and variance. We thus further approximate by replacing mt-s with mt and get

E[rt]2  mt2

t
µs
s=0

2
= m2t

1 - µt 1-µ

2
,

var[rt]  (vt - m2t )

t

µ2s

=

(vt

-

m2t )

1 1

- -

µ2t µ2

.

s=0

(46) (47)

The two scalar factors lead to the correction term (µ, t) in Eq. (19). When estimating the gradient variance from the mini-batch (Eq. 44), we can obtain an unbiased estimate of var[rt] in Eq. (18) via

s¯t = µ2s¯t-1 + s^t,

(48)

where s^t is given by Eq. (44).

D VARIATIONS OF VARIANCE-ADAPTED METHODS
Based on the considerations in Section 3, we examined three more variance-adapted methods. The first is a variation of M-SVAG which estimates stochastic gradient variances locally within the minibatch, as explained in §C.2. Pseudo-code can be found in Alg. 5. Furthermore, we tested a variant of ADAM that applies the correction factor from Eq. (19) to the estimate of the relative variance of the momentum term. We refer to this method as ADAM*. Two variants of ADAM* with the two variance estimates can be found in Algorithms 4 and 5.

Algorithm 3 M-SVAG-mb (with mini-batch variance estimates)

Require: initial value 0, step size , momentum parameter µ  [0, 1], number of steps T

1: Initialize m = 0, s¯ = 0

2: for t = 1, . . . , T do

3: Compute stochastic gradient g() and variance estimate s^()

Eq. (44)

4: Update aggregators m  µm + g(), s¯  µ2s¯ + s^()

5: Compute relative variance estimate 2 = s¯/m2

6: Compute variance adaptation factors  = (1 + 2)-1

7: Update    - ( m)

8: end for

Algorithm 4 ADAM* (with exp. moving average variance estimates)

Require: initial value 0, step size , momentum/averaging constant µ  [0, 1], number of steps T 1: Initialize m = 0, v = 0 2: for t = 1, . . . , T do 3: Compute stochastic gradient g = g() 4: Update moving averages m  µm + (1 - µ)g, v  µv + (1 - µ)g2 5: Bias-correct m = (1 - µt)-1m~ , v = (1 - µt)-1v~

6:

Compute

relative

variance

estimate

2

=

(µ,

t)

v-m2 m2

7: Compute variance adaptation factors  = (1 + 2)-1/2

8: Update    - ( sign(m))

9: end for

Eq. (19)

This is ADAM (1 = 2 = µ,  = 0), expect for the correction factor (µ, t) for the relative variance.

16

Under review as a conference paper at ICLR 2018

Training loss

1.4 1.2 1.0 0.8 0.6
1.4 1.2 1.0 0.8 0.6
0

CIFAR-10
ADAM
ADAM* ADAM*-mb

5 10 15 20 Steps (·10-3)

25

Test loss

Training loss

4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0
0

CIFAR-100
10 20 30 40 50 60 Steps (·10-3)

Test loss

Figure 6: Comparison of the original ADAM algorithm to the variants in Algs. 4 and 5. Set-up of the plots as in Fig. 3. All three algorithms exhibit very similar performance on both problems.

Algorithm 5 ADAM*-mb (with mini-batch variance estimates)

Require: initial value 0, step size , momentum/averaging constant µ  [0, 1], number of steps T

1: Initialize m = 0, s¯ = 0

2: for t = 1, . . . , T do

3: Compute stochastic gradient g() and variance estimate s^()

Eq. (44)

4: Update aggregators m  µm + g(), s¯  µ2s¯ + s^()

5: Compute relative variance estimate 2 = s¯/m2

6: Compute variance adaptation factors  = (1 + 2)-1/2 7: Update    - ( sign(m)) 8: end for

D.1 EXPERIMENTAL RESULTS We evaluated the variants on the two CIFAR test problems. Figure 6 shows a comparison of the two ADAM* variants with the original ADAM. Figure 7 compares the mini-batch variant of M-SVAG to the one with exponential moving averages.

17

Under review as a conference paper at ICLR 2018

Training loss

1.4 1.2 1.0 0.8 0.6
1.4 1.2 1.0 0.8 0.6
0

CIFAR-10
M-SVAG M-SVAG-mb

5 10 15 20 Steps (·10-3)

25

Test loss

Training loss

4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0
0

CIFAR-100
10 20 30 40 50 60 Steps (·10-3)

Test loss

Figure 7: Comparison of the two variants of the M-SVAG algorithm. Set-up of the plots as in Fig. 3. Both variants exhibit very similar performance on both problems.

18

