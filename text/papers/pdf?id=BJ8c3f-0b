Under review as a conference paper at ICLR 2018
Auto-Encoding Sequential Monte Carlo
Anonymous authors Paper under double-blind review
Abstract
We build on auto-encoding sequential Monte Carlo (aesmc):1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (smc) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.
1 Introduction
We build upon aesmc (Anon, 2017), a method for model learning that itself builds on variational auto-encoders (vaes) (Kingma & Welling, 2014; Rezende et al., 2014) and importance weighted auto-encoders (iwaes) (Burda et al., 2016). Aesmc is similarly based on maximizing a lower bound to the log marginal likelihood, but uses smc (Doucet & Johansen, 2009) as the underlying marginal likelihood estimator instead of importance sampling. For a very wide array of models, particularly those with sequential structure, smc forms a substantially more powerful inference method than importance sampling, typically returning lower variance estimates for the marginal likelihood. Consequently, by using smc for its marginal likelihood estimation, aesmc often leads to improvements in model learning compared with vaes and iwaes. We provide experiments on structured time-series data that show aesmc based learning was able to learn useful representations of the latent space for both reconstruction and prediction more effectively that the iwae counterpart.
Aesmc was introduced in an earlier preprint (Anon, 2017) concurrently with the closely related methods of Maddison et al. (2017); Naesseth et al. (2017). In this work we take these ideas further by providing new theoretical insights for the resulting evidence lower bounds (elbos), extending these to explore the relative efficiency of different approaches to proposal learning, and using our results to develop a new and improved training procedure. In particular, we introduce a method for expressing the gap between an elbo and the log marginal likelihood as a Kullback-Leibler (kl) divergence between two distributions on an extended sampling space. Doing so allows us to investigate the behavior of this family of algorithms when the objective is maximized perfectly, which occurs only if the kl divergence becomes zero. In the iwae case, this implies that the proposal distributions are equal to the posterior distributions under the learned model. In the aesmc case, it has implications for both the proposal distributions and the intermediate set of targets that are learned. We demonstrate that, somewhat counter-intuitively, using lower variance estimates for the marginal likelihood can actually be harmful to proposal learning. In other words, we show that tighter is not necessarily better, and often substantially worse, in the context of variational bounds. Using these insights, we propose an adaptation to the aesmc algorithm, which we call alternating elbos, that uses different lower bounds for updating the model parameters and proposal parameters. We further show that this adaptation empirically leads to both improved model learning and proposal adaptation performance.
1This work builds upon an earlier preprint of the same name (Anon, 2017) along with the independent, simultaneously developed, closely related, work of Maddison et al. (2017) and Naesseth et al. (2017).
1

Under review as a conference paper at ICLR 2018

2 Background

2.1 State-Space Models

State-space models (ssms) are probabilistic models over a set of latent variables x1:T and ob-

served variables y1:T . Given parameters , a ssm is characterized by an initial density µ(x1),

a series of transition densities ft,(xt|x1:t-1), and a series of emission densities gt,(yt|x1:t)

with the joint density being p(x1:T , y1:T ) = µ(x1)

T t=2

ft,

(xt

|x1:t-1)

T t=1

gt,

(yt|x1:t).

We are usually interested in approximating the posterior p(x1:T |y1:T ) or the expectation of some test function  under this posterior I() := (x1:T )p(x1:T |y1:T ) dx1:T . We refer to
these two tasks as inference. Inference in models which are non-linear, non-discrete, and

non-Gaussian is difficult and one must resort to approximate methods, for which smc has been shown to be one of the most powerful approaches (Doucet & Johansen, 2009).

We will consider model learning as a problem of maximizing the marginal likelihood p(y1:T ) = p(x1:T , y1:T ) dx1:T in the family of models parameterized by .

2.2 Sequential Monte Carlo

smc performs approximate inference on a sequence of target distributions (t(x1:t))Tt=1. In the context of ssms, the target distributions are often taken to be (p(x1:t|y1:t))Tt=1. Given a parameter  and proposal distributions q1,(x1|y1) and (qt,(xt|y1:t, x1:t-1))tT=2 from which we can sample and whose densities we can evaluate, smc is described in Algorithm 1.

Using the set of weighted particles (x~k1:T , wTk )Kk=1 at the last time step, we can approximate

the posterior as

K k=1

w¯Tk

x~1k:T

(x1:T

)

and

the

integral

I

as

K k=1

w¯Tk

(x~k1:T

),

where

w¯Tk

:=

wTk / j wTj is the normalized weight and z is a Dirac measure centred on z. Furthermore, one

can obtain an unbiased estimator of the marginal likelihood p(y1:T ) using the intermediate

particle weights:

T
Z^SMC :=

1 K

K
wtk

.

t=1 k=1

(1)

Algorithm 1: Sequential Monte Carlo

Data: observed values y1:T , model parameters , proposal parameters  Result: particles (x~1k:T )Kk=1, weights (wTk )kK=1, marginal likelihood estimate Z^SMC begin
Sample initial particle values xk1  q1,(·|y1). Compute and normalize weights:

w1k

=

µ

(xk1 )g1,(y1|xk1 q1,(xk1 |y1)

)

,

w¯1k =

Initialize particle set: x~k1  x1k for t = 2, 3, . . . , T do
Sample ancestor index akt-1  Discrete(·|w¯t1-1, . . . , w¯tK-1). Sample particle value xkt  qt,(·|y1:t, x~a1:tkt--11). Update particle set x~k1:t  (x~a1:ktt--11, xtk). Compute and normalize weights:

w1k
K =1

w1

.

wtk

=

ft,

(xkt |x~a1:ktt--11)gt,(yt|x~k1:t qt,(xtk|y1:t, x~1a:tkt--11)

)

,

w¯tk =

wtk
K =1

wt

.

Compute marginal likelihood: Z^SMC =

T1 t=1 K

K k=1

wtk

.

return particles (x~1k:T )kK=1, weights (wTk )Kk=1, marginal likelihood estimate Z^SMC

2

Under review as a conference paper at ICLR 2018

The sequential nature of smc and the resampling step are crucial in making smc scalable to large T . The former makes it easier to design efficient proposal distributions as each step need only target the next set of variables xt. The resampling step allows the algorithm to focus on promising particles in light of new observations, avoiding the exponential divergence between the weights of different samples that occurs for importance sampling as T increases. This can be demonstrated both empirically and theoretically (Del Moral, 2004, Chapter 9). We refer the reader to (Doucet & Johansen, 2009) for an in-depth treatment of smc.

2.3 Importance Weighted Auto-Encoders

Given a dataset of observations (y(n))Nn=1, a generative model p(x, y) and an inference

network

q(x|y),

iwaes

(Burda

et

al.,

2016)

maximize

1 N

N n=1

elboIS(,

,

y(n))

where,

for

a given observation y, the elboIS (with K particles) is a lower bound on log p(y) by Jensen's

inequality:

elboIS(, , y) = QIS(x1:K ) log Z^IS(x1:K ) dx1:K  log p(y), where

(2)

K
QIS(x1:K ) = q(xk|y),
k=1

Z^IS(x1:K )

=

K k=1

p(xk, y) q (xk |y )

.

(3)

Note that for K = 1 particle, this objective reduces to a vae (Kingma & Welling, 2014; Rezende et al., 2014) objective to which we will refer to as

elboVAE(, , y) = q(x|y)(log p(x, y) - log q(x|y)) dx.

(4)

The iwae optimization is performed using stochastic gradient ascent (sga) where a sam-

ple from

K k=1

q(xk

|y(n))

is obtained using the reparameterization trick (Kingma &

Welling,

2014)

and

the

gradient

1 N

N n=1

,

log

optimization step.

K p (xk,y(n)) k=1 q(xk|y(n))

is used to perform an

3 Auto-Encoding Sequential Monte Carlo
aesmc implements model learning, proposal adaptation, and inference amortization in a similar manner to the vae and the iwae: it uses sga on an empirical average of the elbo over observations. However, it varies in the form of this elbo. In this section, we will introduce the aesmc elbo, explain how gradients of it can be estimated, and discuss the implications of these changes.

3.1 Objective Function

Consider a family of ssms {p(x1:T , y1:T ) :   } and a family of proposal distributions

{q(x1:T |y1:T ) = q1,(x1|y1)

T t=2

qt,

(xt|x1:t-1

,

y1:t)

:





}.

Aesmc

uses

an

elbo

objec-

tive based on the smc marginal likelihood estimator (1). In particular, for a given y1:T , the

objective is defined as

elboSMC(, , y1:T ) := QSMC(x11::KT , a11::TK-1) log Z^SMC(x11::TK , a11::TK-1) dx11::TK da11::TK-1, (5)

where Z^SMC(x11::KT , a11::TK-1) is defined in (1) and QSMC is the sampling distribution of smc,

QSMC(x11::KT , a11::TK-1) =

K
q1,(xk1 )

TK
qt,(xkt |x~1a:ktt--11) · Discrete(akt-1|wt1-:K1 ) . (6)

k=1

t=2 k=1

elboSMC forms a lower bound to the log marginal likelihood log p(y1:T ) due to Jensen's

inequality. Hence, given a dataset (y1(n:T) )Nn=1, we can perform model parameter learning

based

on

maximizing

the

lower

bound

of

1 N

N n=1

log

p

(y1(n:T)

):

J (, ) := 1 N

N

elboSMC(, , y1(n:T) ).

n=1

(7)

3

Under review as a conference paper at ICLR 2018

For notational convenience, we will talk about optimizing elbos in the rest of this section. However, we note that the main intended use of aesmc is to amortize over datasets, for which we replace the elbo gradient with an empirical sum as per (7).

3.2 Gradient Estimation

We describe a gradient estimator used for optimizing elboSMC(, , y1:T ) using sga. The

smc sampler in Algorithm 1 proceeds by sampling x11:K , a11:K , x21:K , . . . sequentially from

their respective distributions

K k=1

q1(x1k

),

K k=1

Discrete(a1k

|w11:K

),

K k=1

q2 (x2k |x1a1k

),

.

.

.

until the whole particle-weight trajectory (x11::KT , a11::TK-1) is sampled using which the marginal

likelihood estimator in (1) is formed.

Assuming that the sampling of latent variables x11::KT is reparameterizable, we can make

their sampling independent of (, ). In particular, assume that there exists a set of

auxiliary random variables

1:K 1:T

where

k t



st

and

a

set

of

reparameterization

functions

rt

using which we can simulate the smc sampler as follows: sample

1:K 1



K k=1

s1

and

set

xk1

=

r1(

k 1

),

then

sample

a11:K

from

K k=1

Discrete(ak1 |w11:K ),

then

sample

1:K 2



K k=1

s2

and

set

x2k

=

r2(

k 2

,

x1ak1

),

until

we

obtain

(x11::KT , a11::TK-1).

We

use

this

reparameterized

sample

of (x11::TK , a11::KT -1) to evaluate the gradient estimator , log Z^SMC(x11::TK , a11::KT -1).

To account for the discrete choices of ancestor indices akt we can additionally use the reinforce (Williams, 1992) trick, however in practice, we find that the additional term in the estimator has problematically high variance. We explore various other possible gradient
estimators and empirical assessments of their variances in Appendix A.

3.3 Bias & Implications on the Proposals

In this section, we express the gap between elbos and the log marginal likelihood as a kl divergence and study implications on the proposal distributions. We present a set of claims and propositions whose full proofs are in Appendix B. These give insight into the
behavior of aesmc, compared with alternatives and show the advantages, and disadvantages, of using our different elbo. This insight motivates Section 4 which proposes an algorithm for improving proposal learning.

Definition 1. Given an unnormalized target density P~ : X  [0, ) with normalizing

constant ZP > 0, P := P~/ZP , and a proposal density Q : X  [0, ), the elbo

P~(x)

elbo =

Q(x) log

dx,

Q(x)

(8)

is a lower bound on log ZP and satisfies

elbo = log ZP - kl (Q||P ) .

(9)

This is a standard identity used in variational inference (Wainwright et al., 2008) and vaes. In the case of vaes, applying Definition 1 with P being p(x|y), P~ being p(x, y), ZP being p(y), and Q being q(x|y), we directly can rewrite (4) as elboVAE(, , y) = log p(y) - kl (q(x|y)||p(x|y)).
The key observation for expressing such a bound for general elbos such as elboIS and elboSMC is that the target density P and the proposal density Q need not directly correspond to p(x|y) and q(x|y). This allows us to view the underlying sampling distributions of the marginal likelihood Monte Carlo estimators such as QIS in (3) and QSMC in (6) as proposal distributions on an extended space X . The following claim uses this observation to express
the bound between a general elbo and the log marginal likelihood as kl divergence from the extended space sampling distribution to a corresponding target distribution.

4

Under review as a conference paper at ICLR 2018

Claim 1. Given a non-negative unbiased estimator Z^P (x)  0 of the normalizing constant ZP where x is distributed from the proposal distribution Q(x), the following holds:

elbo = Q(x) log Z^P (x) dx = log ZP - kl (Q||P ) ,

(10)

where P (x) = Q(x)Z^P (x) ZP
is a normalized target density.

(11)

In the case of iwaes, we can apply Claim 1 with Q and Z^P being QIS and Z^IS defined in (3) and ZP being p(y). This yields

elboIS(, , y) = log p(y) - kl (QIS||PIS) , where

(12)

PIS(x1:K )

=

1 K

K

q(x1|y) · · · q(xk-1|y)p(xk|y)q(xk+1|y) · · · q(xK |y) .

k=1

(13)

Similarly, in the case of aesmc, we obtain

elboSMC(, , y1:T ) = log p(y1:T ) - kl (QSMC||PSMC) , where

(14)

PSMC(x11::KT , a11::TK-1) = QSMC(x11::TK , a11::KT -1)Z^SMC(x11::KT , a11::KT -1)/p(y1:T ).

(15)

Having expressions for the target distribution P and the sampling distribution Q for a given
elbo allows us to investigate what happens when we maximize that elbo, remembering that the kl term is strictly non-negative and zero if and only if P = Q. For the vae and iwae cases then, provided the proposal is sufficiently flexible, one can always perfectly maximize the elbo by setting p(x|y) = q(x|y) for all x. The reverse implication also holds: if elboVAE = log ZP then it must be the case that p(x|y) = q(x|y). However, for aesmc, achieving elbo = log ZP is only possible when one also has sufficient flexibility to learn a particular series intermediate target distributions, namely the marginals of the final
target distribution. In other words, it is necessary to learn a particular factorization of
the generative model, not just the correct individual proposals, to achieve P = Q. These
observations are formalized in Propositions 1 and 2 below.
Proposition 1. QIS(x1:K ) = PIS(x1:K ) for all x1:K if and only if q(x|y) = p(x|y) for all x.
Proposition 2. If K > 1, then PSMC(x11::KT , a11::KT -1) = QSMC(x11::KT , a11::KT -1) for all (x11::TK , a11::KT -1) if and only if

1. t(x1:t) = p(x1:T |y1:T ) dxt+1:T = p(x1:t|y1:T ) for all x1:t and t = 1, . . . , T , and

2. q1(x1|y1) = p(x1|y1:T ) for all x1 and qt(xt|x1:t-1, y1:t) = p(x1:t|y1:T )/p(x1:t-1|y1:T ) for t = 2, . . . , T for all x1:t,

where t(x1:t) are the intermediate targets used by smc.

Proposition 2 has the consequence that if the family of generative models is such that condition
1 does not hold, we will not be able to make the bound tight elboSMC = ZP . This means that, except for a very small class of models, then, for most convenient parameterizations,
it will be impossible to learn a perfect proposal that gives a tight bound, i.e. there will be
no  and  such that the above conditions can be satisfied. However, it also means that
elboSMC encodes important additional information about the implications the factorization of the generative model has on the inference--the model depends only on the final target T (x1:T ) = p(x1:T |y1:T ), but some choices of the intermediate targets t(x1:t) will lead to much more efficient inference than others. Perhaps more importantly, smc is usually a far more powerful inference algorithm than importance sampling and so the aesmc setup allows for more ambitious model learning problems to be effectively tackled than the vae or iwae. After all, even though it is well known in the smc literature that, unlike for importance sampling, most problems have no perfect set of smc proposals which will generate exact samples from the posterior (Doucet & Johansen, 2009), smc is still gives superior performance on most problems with more than a few dimensions. These intuitions are backed up by
our experiments that show that using elboSMC regularly learns better models than using elboIS.

5

Under review as a conference paper at ICLR 2018

4 Improving Proposal Learning

Given the implications from the previous section, we now ask whether optimizing elboIS and

elboSMC actually improves the proposal distribution? In other words, does the optimization procedure make q(x1:T |y1:T ) closer to p(x1:T |y1:T ) and how does the number of particles K affect this? In the vae case, we are directly optimizing kl (q(x|y)||p(x|y)) for fixed model

parameters and so it is straight forward to see that we will induce proposal learning. In the

iwae and aesmc cases, such optimization only minimizes kl (QIS||PIS) and kl (QSMC||PSMC)

respectively, which does not directly imply that Counter-intuitively, it transpires that the tighter

kl (q(x|y)||p(x|y)) bounds implied by

is small.
KDE of

rµqdELBO

103 102 10 1

using a larger K is often harmful to proposal learning for both iwae

and aesmc. At a high-level, this is because an accurate estimate for Z^P can be achieved for a wide range of proposal parameters  and so the magnitude of  elbo reduces as K increases. Typically, this

shrinkage happens faster than increasing K reduces the standard

deviation of the estimate and so the signal-to-noise ratio (snr) for

the gradient estimate actually decreases, even though it is a lower

variance estimate. This effect is demonstrated in Figure 1 which

shows a kernel density estimator for the distribution of the elbo

0 10

gradient estimate for different K and the model given in Section 5.2. Figure 1: Density es-

Here we see that as we increase K, both the expected gradient estimate (i.e. true gradient) and standard deviation of the estimate

timate of  elbo for different K

decrease. However, the former decreases faster and so the snr

decreases. This is perhaps easiest to appreciate by noting that for K  10, there is a roughly

equal probability of the estimate being positive or negative, such that we are equally likely

to increase or decrease the parameter value at the next sga iteration, inevitably leading to

poor performance. On the other hand, when K = 1, it is far more likely that the gradient

estimate is positive than negative, and so there is clear drift to the gradient steps. We

add to the empirical evidence for this behavior is Section 5. Note the critical difference for

model learning is that  elbo does not, in general, decrease in magnitude as K increases.

Note also that using a larger K should always give better performance at test time--the

implication of our result is that it may be better to learn  using a smaller K.

We can further demonstrate this result using an informal theoretical argument for the case of the iwae. Our gradient estimate for the K particle iwae is

IK =  log

1 K p(xk, y) K k=1 q(xk|y)

,

(16)

where I = lim IK = 0 because with infinite samples, the estimate is exact and thus
K 
independent of the proposal parameters. Now adapting the iwae result of Rainforth et al.
(2017) shows that

E IK2

=E

(IK - I)2



C0214 4K 2

+

C0214 4K 2

+

2012 K

+

C0013 K 3/2

+O

1 K3

,

(17)

where C0, 0 (K0 in Rainforth et al. (2017)), and 1 are constants and we have set N = 1

in

their

formulation.

Here

the

first

term,

,C02 14
4K 2

is

a

"bias

term",

in

our

context

(E [IK ])2

=

( elbo)2, and the rest are variance terms. We can now define our snr as follows

snr =  elbo  Var[IK ]

C02 14 4K 2

+ + + OC0214
4K 2

02 12 K

C0 0 13 K 3/2

1 K3



C0212 402K

=O

1 , (18)
K

where we have substituted in the bounds for the bias and variance from (17) and the

approximations will, in general, become increasingly exact as K increases. We thus see that

increasing K reduces the snr and so a lower K is preferable for proposal learning.

4.1 Alternating elbos
To address these issues, we propose the alternating elbos (alt) algorithm which updates (, ) in a coordinate descent fashion using different elbos, and thus gradient esti-

6

Under review as a conference paper at ICLR 2018

mates, for each. We pick a -optimizing pair and a -optimizing pair (A, K), (A, K)  {importancesampling(is), smc} × {1, 2, . . . }, corresponding to a inference type number of particles. In an optimization step, we obtain an estimator for  elboA with K particles and an estimator for  elboA with K particles which we call g and g respectively. We use g to update the current  and g to update the current . The results from the previous sections suggest that using A = smc and A = is with a large K and a small K should perform better model and proposal learning than just fixing (A, K) = (A, K) to (smc, large) since using A = is with small K helps learning  and using A = smc with large K helps learning . We experimentally find that this procedure in fact improves both model and proposal learning, leading to a new algorithm that improves the aesmc approach
even further.

5 Experiments
We now present a series of experiments designed to answer the following questions: 1) Does reducing the gap by using more particles or a better inference procedure lead to an adverse effect on proposal learning? 2) Can aesmc, despite this effect, outperform iwae? 3) Can we further improve the learned model and proposal by using our newly proposed algorithm alt?
First we investigate a linear Gaussian state space model (lgssm) for model learning and a latent variable model for proposal adaptation. This allows us to compare the learned parameters to the optimal ones. Doing so, we confirm our conclusions for this simple problem.
We then extend those results to more complex, high dimensional observation spaces that require models and proposals parameterized by neural networks. We do so by investigating the Moving Agents dataset, a set of partially occluded video sequences.

5.1 Linear Gaussian State Space Model

Given the following lgssm

p(x1) = Normal x1; 0, 12 ,

p(xt|xt-1) = Normal xt; 1xt-1, 12 ,

p(yt|xt) = Normal

2 yt; 2xt, 0.1

,

t = 2, . . . T, t = 1, . . . , T,

(19) (20) (21)

we find that optimizing elboSMC(, , y1:T ) w.r.t  leads to better generative models than optimizing elboIS(, , y1:T ). The same is true for using more particles.

To do so, we generate on sequence y1:T for T = 200 by sampling from the model with  = (1, 2) = (0.9, 1.0). We then optimize the different elbos w.r.t  using the bootstrap proposal q1(x1|y1) = µ(x1) and qt(xt|x1:t-1, y1:t) = ft,(xt|x1:t-1). Any  appearing in q terms is detached from the computational graph to not influence the gradient updates
through q.

We use a fixed learning rate of 0.01 and optimize for 500 steps. Figure 2 shows that the
convergence of both log p(y1:T ) to max log p(y1:T ) and  to argmax log p(y1:T ) is faster when elboSMC and more particles are used.

5.2 Proposal Learning
We now investigate how learning , i.e. the proposal, is affected by the the choice of elbo and the number of particles.
Consider a simple, fixed generative model p(µ)p(x|µ) = Normal(µ; 0, 12)Normal(x; µ, 12) where µ and x are the latent and observed variables respectively and a family of proposal distributions q(µ) = Normal(µ; µq, q2) parameterized by  = (µq, log q2). For a fixed observation x = 2.3, we initialize  = (0.01, 0.01) and optimize elboIS with respect to . We investigate the quality of the learned parameter  as we increase the number of particles K during training. Note that for K = 1, this reduces to stochastic variational inference (Hoffman et al., 2013). Figure 3 (left) clearly demonstrates that the quality of  compared to the analytic posterior decreases as we increase K.
Similar behavior is observed in Figure 3 (middle, right) where we optimize elboSMC with respect to both  and  for the lgssm described in Section 5.1. We see that using more

7

Under review as a conference paper at ICLR 2018

Log Marginal Likelihood 1
2

-1000 -2000 -3000 -4000 -5000
0

is 10 is 10000

smc 10 smc 10000

100 200 300 400 Optimization Step

500

0.5
0.0
2 1 0
0

100 200 300 400 Optimization Step

500

Figure 2: (Left) Log marginal likelihood analytically evaluated at every  during optimization;
the black line indicates max log p(y1:T ) obtained by the expectation maximization (em) algorithm. (Right) learning of model parameters; the black line indicates argmax log p(y1:T ) obtained by the em algorithm.

particles helps model learning but makes proposal learning worse. Using our alt algorithm alleviates this problem and at the same time makes model learning faster as it profits from a more accurate proposal distribution.

q2 µq Log Marginal Likelihood L2 between approx and true marginal posterior means

1.0 0.5 0.0

1.5

1.0

0.5

0 500 1000

Optimization step

true 1

10 100

1000 10000

-250

-300

-350

-400

-450

-500

0 200 400 Optimization step
smc 10 smc 1000

20
15
10
5 0 200 400 Optimization Step
alternate EM/bootstrap

Figure 3: (Left) Optimizing elboIS for the Gaussian unknown mean model with respect to  results in worse  as we increase number of particles K. (Middle, right) Optimizing

elboSMC with respect to (, ) for lgssm and using the alt algorithm for updating (, ) with (A, K) = (smc, 1000) and (A, K) = (is, 10). Right measures the quality of  by

showing

Tt=1(µkt alman - µtapprox)2 where µkt alman is the marginal mean obtained from the

Kalman smoothing algorithm under the model with em-optimized parameters and µatpprox is

an marginal mean obtained from the set of 10 smc particles with learned/bootstrap proposal.

5.3 Moving Agents To show that our results are applicable to complex, high dimensional data we compare alt, aesmc and iwae on stochastic, partially observable video sequences. Figure 7 in the appendix shows an example of such a sequence.
The dataset consists of 5000 sequences of images (y0(i:3)9)15000 of which 1000 are randomly held out as test set. Each sequence contains 40 images represented as a 3 dimensional array of size 1 × 32 × 32. In each sequence there is one agent, represented as circle, whose starting position is sampled randomly along the top and bottom of the image. The dataset is inspired by (Ondruska & Posner, 2016), however with the crucial difference that the movement of the agent is stochastic. The agent performs a directed random walk through the image. At
8

Under review as a conference paper at ICLR 2018

each timestep, it moves according to

yt+1  N (yt+1|yt + 0.15, 0.022) xt+1  N (xt+1|0, 0.022)

(22)

where (xt, yt) are the coordinates in frame t in a unit square that is then projected onto 32 × 32 pixels. In addition to the stochasticity of the movement, half of the image is occluded,

preventing the agent to be observed.

As generative model and proposal distribution we use a Variational Recurrent Neural Network
(vrnn) (Chung et al., 2015). It extends recurrent neural networks (rnns) by introducing a stochastic latent state xt at each timestep t. Together with the observation yt, this state conditions the deterministic transition of the rnn. By introducing this unobserved stochastic state, the vrnn is able to better model complex long range variability in stochastic sequences. Architecture and hyperparameter details are given in the appendix.

One can see in Figure 4 that models trained using the alternating training schedule alt outperform aesmc which in turn outperform iwae. Using more particles improves the elbo, but more so for aesmc and alt as the differences in estimators and training methods become more pronounced with higher particle numbers. In the appendix we inspect different learned
generative models by using them for prediction, confirming the results presented here.

max(ELBOIS , ELBOSMC )

-363 -364 -365 -366 -367 -368
0

20 10 5 AESMC IWAE ALT
20 40 60 80 Epoch

Particles 5 10 20

Method
iwae aesmc alt iwae aesmc alt iwae aesmc alt

Moving Agents -365.1 -364.0 -363.9 -364.6 -363.3 -363.1 -364.4 -363.06 -362.6

Figure 4: (Left) Rolling mean over 5 epochs of max(elboSMC, elboIS) on the test set. The color indicates the number of particles, the line style the used algorithm. (Right) The table
shows the final max(elboSMC, elboIS) for each learned model.

6 Conclusions
We have developed aesmc--a method for performing model learning using a new elbo objective which is based on the smc marginal likelihood estimator. This elbo objective is optimized using sga and the reparameterization trick. Our approach utilizes the efficiency of smc in models with intermediate observations and hence is suitable for a highly structured models. We experimentally demonstrated that this objective has a tighter gap to the log marginal likelihood than the iwae objective and that it works well on structured problems such as learning the generative model of moving balls which can be used for tasks such as reconstruction or prediction.
Additionally, in Claim 1, we provide a simple way to express the bias of objectives induced by log of marginal likelihood estimators as a kl divergence. In Propositions 1 and 2, we investigate the implications of these kls being zero in the case of iwae and aesmc. In the latter case, we find that we can achieve zero kl only if we are able to learn smc intermediate target distributions corresponding to marginals of the target distribution. We then built on these results to show that tighter in not necessarily better when it comes to variational bounds and used this insight to develop a new method, alternating elbos, that address some of the issues of aesmc to improve proposal learning.

9

Under review as a conference paper at ICLR 2018
References
Anon. Auto-encoding sequential Monte Carlo. arXiv preprint arXiv:1705.*****v1, 2017. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
In ICLR, 2016. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and
Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015. P Del Moral. Feynman-Kac formulae: genealogical and interacting particle systems with applications. Probability and its applications, 2004. Arnaud Doucet and Adam M Johansen. A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering, 12(656-704):3, 2009. Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303­1347, 2013. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014. Chris J Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Whye Teh. Filtering variational objectives. arXiv preprint arXiv:1705.09279, 2017. Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational sequential Monte Carlo. arXiv preprint arXiv:1705.11140, 2017. Peter Ondruska and Ingmar Posner. Deep tracking: Seeing beyond seeing using recurrent neural networks. In Thirtieth AAAI Conference on Artificial Intelligence, 2016. Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, and Frank Wood. On the opportunities and pitfalls of nesting monte carlo estimators. arXiv preprint arXiv:1709.06181, 2017. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014. Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1­2):1­305, 2008. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
10

Under review as a conference paper at ICLR 2018

A Gradients
The goal is to obtain an unbiased estimator for the gradient , QSMC(x11::KT , a11::KT -1) log Z^SMC(x11::KT , a11::KT -1) dx11::KT da11::TK-1.

(23)

A.1 Full Reinforce We express the required quantity as
, QSMC(x11::TK , a11::KT -1) log Z^SMC(x11::KT , a11::TK-1) dx11::KT da11::KT -1

(24)

= ,QSMC(x11::KT , a11::TK-1) log Z^SMC(x11::KT , a11::TK-1)+ QSMC(x11::KT , a11::TK-1), log Z^SMC(x11::KT , a11::KT -1) dx11::TK da11::TK-1
= QSMC(x11::KT , a11::TK-1) , log QSMC(x11::KT , a11::KT -1) log Z^SMC(x11::TK , a11::KT -1)+

(25) (26) (27)

, log Z^SMC(x11::TK , a11::KT -1) dx11::KT da11::TK-1,

(28)

which we can estimate by sampling (x11::TK , a11::KT -1) directly from QSMC and evaluating , log QSMC(x11::TK , a11::KT -1) log Z^SMC(x11::TK , a11::TK-1) + , log Z^SMC(x11::KT , a11::TK-1) .

A.2 Reinforce & Reparameterization We express the required quantity as

, QSMC(x11::TK , a11::KT -1) log Z^SMC(x11::KT , a11::KT -1) dx11::KT da11::KT -1

(29)

= ,

K
q1 (x1k )
k=1

TK
qt(xkt |xta-kt-11 ) · Discrete(atk-1|wt1-:K1 )
t=2 k=1

log Z^SMC(x11::KT , a11::KT -1) dx11::TK da11::TK-1

(30)

= ,

K

s1(

k 1

)

k=1

TK

st(

k t

)

·

Discrete(akt-1|wt1-:K1

)

t=2 k=1

log Z^SMC(r(

1:K 1:T

),

a11::TK-1)

d

1:K 1:T

da11::KT -1

(31)

TK

TK

=

st(

k t

)

,

Discrete(atk-1|wt1-:K1 ) log Z^SMC(r(

1:K 1:T

),

a11::TK-1)+

t=1 k=1

t=2 k=1

TK

Discrete(atk-1|wt1-:K1 )

, log Z^SMC(r(

1:K 1:T

),

a11::TK-1

)

d

1:K 1:T

da11::TK-1

t=2 k=1

(32)

TK

TK

=

st(

k t

)

Discrete(atk-1|wt1-:K1 ) ·

t=1 k=1

t=2 k=1

TK

, log

Discrete(atk-1|wt1-:K1 )

log Z^SMC(r(

1:K 1:T

),

a11::KT -1)+

t=2 k=1

, log Z^SMC(r(

1:K 1:T

),

a11::KT -1

)

d

1:K 1:T

da11::KT -1,

(33)

where

r(

1:K 1:T

)

denotes

a

sample

with

identical

distribution

as

x11::KT

obtained

by

passing

the auxiliary samples

1:K 1:T

through

the

reparameterization

function.

We

can

thus

estimate

the gradient by sampling

1:K 1:T

from

the

auxiliary

distribution,

reparameterizing

and

evaluating

, log

T t=2

K k=1

Discrete(akt-1

|wt1-:K1

)

log Z^SMC(r(

1:K 1:T

),

a11::KT -1)

+

,

log

Z^SMC(r(

1:K 1:T

),

a11::KT -1

)

.

11

Under review as a conference paper at ICLR 2018

In Figure 5, we demonstrate that the estimator in (33) has much higher variance if we include the first term.

ignore reinforce

0.00003 0.02
0.00002
0.01 0.00001

0.00 0.00000

250 300 350

-50000

0

50000

Figure 5: T = 200 model described in Section 5.1. Kernel density estimation (kde) of 1 elboSMC evaluated at 1 = 0.1 with K = 16 using 100 samples.

B Proofs for Bias & Implications on the Proposals

Derivation of (9).

elbo =

Q(x) log ZP P (x) dx Q(x)

=

Q(x) log ZP dx -

Q(x)

Q(x) log

dx

P (x)

= log ZP - kl (Q||P ) .

(34)
(35) (36)

Proof of Claim 1. Since Z^P (x)  0, Q(x)  0 and Q(x)Z^P (x) dx = ZP , we can let the unnormalized target density in Definition 1 be P~(x) = Q(x)Z^P (x). Hence, the normalized target density is P (x) = Q(x)Z^P (x)/ZP . Substituting these quantities into (8) and (9) yields
the two equalities in (10).

Proof of Proposition 1. ( = ) Substituting for QIS(x1:K ) = PIS(x1:K ), we obtain

K q(xk|y) = 1 K

K

k=1

k=1

K =1

q(x

|y)

q(xk|y)

p(xk

|y)

=

1 K

K

q(x1|y) · · · q(xk-1|y)p(xk|y)q(xk+1|y) · · · q(xK |y) .

k=1

(37)

Integrating both sides with respect to (x2, . . . , xK) over the whole support (i.e. marginalize out everything except x1), we obtain:

q(x1|y) = 1

K
p(x1|y) + q(x1|y) .

K

k=2

Rearranging gives us q(x1|y) = p(x1|y) for all x1.

(38)

( = ) Substituting p(xk|y) = q(xk|y), we obtain

PIS(x1:K )

=

1 K

K

QIS(x1:K q(xk|y)

)

p(xk

|y)

k=1

1 =
K

K

QIS(x1:K )

k=1

= QIS(x1:K ).

(39)
(40) (41)

Proof of Proposition 2. We consider the general sequence of target distributions t(x1:t) (p(x1:t|y1:t) in the case of ssms), their unnormalized versions t(x1:t) (p(x1:t, y1:t) in the case of ssms), their normalizing constants Zt = t(x1:t) dx1:t (p(y1:t) in the case of ssms), where Z = ZT = p(y1:T ).
12

Under review as a conference paper at ICLR 2018

( = ) It suffices to show that Z^SMC(x11::KT , a11::KT -1) = Z for all (x11::TK , a11::KT -1) implies 1 and 2 in Proposal 2 due to equation (11).

We first prove that Z^SMC(x11::KT , a11::KT -1) = Z for all (x11::TK , a11::TK-1) implies that the weights

w1(x1)

:=

1(x1) q1(x1)

(42)

wt(x1:t)

:=

t(x1:t) t-1(x1:t-1)qt(xt|x1:t-1)

for t = 2, . . . , T

(43)

are constant with respect to x1:t.

Pick t  {1, . . . , T } and distinct k,  {1, . . . , K}. Also, pick x1:t and x 1:t. Now, consider two sets of particle sets (x¯11::TK , a¯11::KT -1) and (x~11::TK , a~11::KT -1), illustrated in Figure 6, such that

x 



x¯

=

x  x



x

if  = and  < t if (,  ) = (k, t) if  = k and  < t otherwise

for  = 1, . . . , T,  = 1, . . . , K,

(44)

a¯ = 

if (,  ) = (k, t - 1) or (k, t) otherwise

for  = 1, . . . , T - 1,  = 1, . . . , K, (45)

x 



x~

=

x x



x

if  = and  < t if (,  ) = (k, t) if  = k and  < t otherwise

a~ = 

if (,  ) = (k, t) otherwise

for  = 1, . . . , T,  = 1, . . . , K,

(46)

for  = 1, . . . , T - 1,  = 1, . . . , K. (47)

... ···

x1

x2

... ···

xt-1

...

k x1

x2

···

...

xt-1

xt

···

...

··· ···

... ···

x1

x2

... ···

xt-1

...

k x1

x2

··· ... xt-1

xt

···

...

··· ···

Figure 6: (Left) particle set (x¯11::KT , a¯11::KT -1) and (right) particle set (x~11::KT , a~11::KT -1). Lines indicate ancestor indices.

The weights w¯ and w~ for the respective particle sets are identical except when (, ) = (t, k) where

w¯tk = wt(x 1:t),

(48)

w~tk = wt(x1:t).

(49)

Since Z^(x¯11::TK , a¯11::TK-1) = Z^(x~11::KT , a~11::TK-1), we have wt(x 1:t) = wt(x1:t). We have proven that wt(x1:t) is constant with respect to x1:t for all t = 1, . . . , T .

Now, for x1:t, consider the proposal by rearranging (42) and (43)

q1(x1)

=

1(x1) w1

qt(xt|x1:t-1)

=

t(x1:t) t-1(x1:t-1)wt

for t = 2, . . . , T,

(50) (51)

13

Under review as a conference paper at ICLR 2018

where wt := wt(x1:t) is constant. For this to be a normalized density with respect to xt, we must have

w1 = 1(x1) dx1 = Z1,

(52)

and for t = 2, . . . , T :

wt =

t(x1:t) t-1(x1:t-1)

dxt

(53)

= t(x1:t) dxt t-1(x1:t-1)

(54)

= Zt · t(x1:t) dxt . Zt-1 t-1(x1:t-1)

(55)

Since t+1(x1:t+1) dxt+1 and t(x1:t) are both normalized densities, we must have t(x1:t) = t+1(x1:t+1) dxt+1 for all t = 1, . . . , T - 1 for all x1:t. For a given t  {1, . . . , T - 1} and
x1:t, applying this repeatedly yields

t(x1:t) = t+1(x1:t+1) dxt+1 =

t+2(x1:t+2) dxt+2 dxt+1 = · · · = T (x1:T ) dxt+1:T .

(56)

We also have

w1(x1) = Z1,

wt(x1:t)

=

Zt , Zt-1

q1(x1) = 1(x1) = T (x1),

qt(xt|x1:t-1)

=

t(x1:t) t-1(x1:t-1)

=

T (x1:t) , T (x1:t-1)

t = 2, . . . , T, t = 2, . . . , T.

(57) (58) (59) (60)

( = ) We substitute identities in 1 and 2 of Proposal 2 back to the expression of Z^(x11::KT , a11::TK-1) to obtain Z^(x11::KT , a11::KT -1) = Z.

C VRNN
In the following we give the details of our vrnn architecture. The generative model is given by:

p(x1:T , h0:T , y1:T ) = p(h0) p(xt|ht-1)p(yt|ht-1, xt)p(ht|ht-1, xt, yt)
t

where

p(h0) = N (h0; 0, 1)
p(xt|ht-1) = N (xt; µx(ht-1), x(ht-1)2) p(yt|ht-1, xt) = Bernoulli(yt; µy (x (xt), ht-1)) p(ht|ht-1, xt, yt) = f (ht-1,x(xt),y(yt))(ht)

and the proposal distribution is given by

(61) (62)

p(xt|yt, ht-1) = N (xt; µp(y (yt), ht-1), p2(y (yt), ht-1))

(63)

The 128

wfuhnocsteiofinrsstµlxayaenrdissxhaarreedc.omxpuisteodnebyfunlleytwcoonrknsecwteitdhlatwyeor

fully connected of size 128.

layers

of

size

For visual input, the encoding y is a convolutional network with conv-4x4-2-1-32, conv4x4-2-1-64, conv-4x4-2-1-128 where conv-wxh-s-p-n denotes a convolutional network with n

filters of size w × h, stride s, padding p. Between convolutions we use leaky ReLUs with

14

Under review as a conference paper at ICLR 2018
slope 0.2 as nonlinearity and batch norms. The decoding µy uses transposed convolutions of the same dimensions but in reversed order, however with stride s = 1 and padding p = 0 for the first layer. A Gated Recurrent Unit (GRU) is used as RNN and if not stated otherwise ReLUs are used in between fully connected layers. For the proposal distribution, the functions µp and p are neural networks with three fully connected layers of size 128 that are sharing the first two layers. Sigmoid and softplus functions are used where values in (0, 1) or R+ are required. We use a minibatch size of 40. For the moving agents dataset we use sga with a learning rate of 1e - 5. A specific feature of the vrnn architecture is that the proposal and the generative model share the component y,. Consequently, we set  =  for the parameters belonging to this module and train it using gradients for both  and .
D Experiments
D.1 Moving Agents In figure 7 we investigate the quality of the generative model by comparing visual predictions. We do so for models learned by iwae (top) and aesmc (bottom). The models were learned using ten particles but for easier visualization we only predict using five particles. The first row in each graphic shows the ground truth. The second row shows the averaged predictions of all five particles. The next five rows show the predictions made by each particle individually. The observations (i.e. the top row) up to t = 19 are shown to the model. Up to this timestep the latent values x0:19 are drawn from the proposal distribution q(xt|yt, ht-1). From t = 20 onwards the latent values x20:37 are drawn from the generative model p(xt|xt-1). Consequently, the model predicts the partially occluded, stochastic movement over 17 timesteps into the future. We note that most particles predict a viable future trajectory. However, the model learned by iwae is not as consistent in the quality of its predictions, often 'forgetting' the particle. This does not happen in every predicted sequence but the behavior shown here is very typical. Models learned by aesmc are much more consistent in the quality of their predictions.
15

Under review as a conference paper at ICLR 2018
Figure 7: Visualisation of the learned model. Ground truth observations (top row in each sub figure) are only revealed to the algorithm up until t=19 inclusive. The second row shows the prediction averaged over all particles, all following rows show the prediction made by a single particle. (Top) iwae. (Bottom) aesmc.
16

