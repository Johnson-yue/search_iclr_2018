Under review as a conference paper at ICLR 2018

DEBIASING EVIDENCE APPROXIMATIONS: ON IMPORTANCE-WEIGHTED AUTOENCODERS AND JACKKNIFE VARIATIONAL INFERENCE
Anonymous authors Paper under double-blind review

ABSTRACT
The importance-weighted autoencoder (IWAE) approach of Burda et al. (2015) defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. (2017) reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on K samples we show the bias to be of order O(K-1). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI), a family of bias-reduced estimators reducing the bias to O(K-(m+1)) for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.

1 INTRODUCTION

Variational autoencoders (VAE) are a class of expressive probabilistic deep learning models useful for generative modeling, representation learning, and probabilistic regression. Originally proposed in Kingma & Welling (2013) and Rezende et al. (2014), VAEs consist of a probabilistic model as well as an approximate method for maximum likelihood estimation. In the generative case, the model is defined as

p(x) = p(x|z) p(z) dz,

(1)

where z is a latent variable, typically a high dimensional vector; the corresponding prior distribution p(z) is fixed and typically defined as a standard multivariate Normal distribution N (0, I). To achieve an expressive marginal distribution p(x), we define p(x|z) through a neural network, making the model (1) a deep probabilistic model.

Maximum likelihood estimation of the parameters  in (1) is intractable, but Kingma & Welling (2013) and Rezende et al. (2014) propose to instead maximize the evidence lower-bound (ELBO),

log p(x)



Ezq (z|x)

log

p(x|z) p(z) q (z |x)

=: LE.

(2) (3)

Here, q(z|x) is an auxiliary inference network, parametrized by . Simultaneous optimization of (2) over both  and  performs approximate maximum likelihood estimation in the model p(x) of (1)
and forms the standard VAE estimation method.

In practice LE is estimated using Monte Carlo: we draw K samples zi  q(z|x), then use the unbiased estimator L^E of LE,

L^E

=

1 K

K i=1

log

p(x|zi) p(zi) . q (zi |x)

(4)

1

Under review as a conference paper at ICLR 2018

The VAE approach is empirically very successful but are there fundamental limitations? One limitation is the quality of the model p(x|z): this model needs to be expressive enough to model the true distribution over x. Another limitation is that LE is only a lower-bound to the true likelihood. Is this bound strong? It can be shown, Kingma & Welling (2013), that when q(z|x) = p(z|x) we have LE = log p(x), hence (2) becomes exact. Therefore, we should attempt to choose an expressive class of distributions q(z|x) and indeed recent work has extensively investigated richer variational families.
We discuss these methods in Section 7 but now review the importance weighted autoencoder (IWAE)
method we build upon.

2 BURDA'S IMPORTANCE-WEIGHTED AUTOENCODER (IWAE) BOUND

The importance weighted autoencoder (IWAE) method Burda et al. (2015) seemingly deviates from (2) in that they propose the IWAE objective, defined for an integer K  1,

log p(x)



Ez1,...,zK q(z|x)

1 log
K

K i=1

p(x|zi) p(z) q (zi |x)

=: LK .

(5) (6)

We denote with L^K the empirical version which takes one sample z1, . . . , zK  q(z|x) and evaluates the inner expression in (6). We can see that L1 = LE, and indeed Burda et al. (2015)
further show that

LE = L1  L2  · · ·  log p(x),

(7)

and limK LK = log p(x). These results are a strong motivation for the use of LK to estimate  and the IWAE method can often significantly improve over LE. The bounds LK seem quite different from LE, but recently Cremer et al. (2017) showed that an exact correspondence exists: any LK can be converted into the standard form LE by defining a modified distribution qIW(z|x) through an
importance sampling construction.

We now analyze the IWAE bound L^K in more detail.

3 ANALYSIS OF THE IWAE BOUND

We now analyze the statistical properties of the IWAE estimator of the log-marginal likelihood. Basic consistency results have been shown in Burda et al. (2015); here we provide more precise results and add novel asymptotic results regarding the bias and variance of the IWAE method. Our results are given as expansions in the order K of the IWAE estimator but do involve moments µi which are unknown to us. The jackknife method in the following sections will effectively circumvent the problem of not knowing these moments.
Proposition 1 (Expectation of L^K). Let P be a distribution supported on the positive real line and let P have finite moments of all order. Let K  1 be an integer. Let w1, w2, . . . , wK  P independently. Then we have asymptotically, for K  ,

E[L^K ] = E

1 log
K

K
wi

i=1

=

log E[w] -

1 K

µ2 2µ2

+

1 K2

µ3 3µ3

-

3µ22 4µ4

-1 K3

µ4 - 3µ22 - 10µ3µ2

4µ4 4µ4

5µ5

+ o(K-3),

(8)

where µi := EP [(w - EP [w])i] is the i'th central moment of P and µ := EP [w] is the mean.

Proof. See Appendix A, page 11.

The above result directly gives the bias of the IWAE method as follows.

2

Under review as a conference paper at ICLR 2018

Corollary 1 (Bias of L^K). If we see L^K as an estimator of log p(x), then for K   the bias of L^K is

B[L^K ] = E[L^K ] - log E[w]

(9)

=

-1 K

µ2 2µ2

+

1 K2

µ3 3µ3

-

3µ22 4µ4

-

1 K3

µ4 4µ4

-

3µ22 4µ4

-

10µ3µ2 5µ5

+ o(K-3).

(10)

Proof. The bias (10) follows directly by subtracting the true value log p(x) = log E[w] from the right hand side of (8).

The above result shows that the bias is reduced at a rate of O(1/K). This is not surprising because the
IWAE estimator is a smooth function applied to a sample mean. The coefficient of the leading O(1/K) bias term uses the ratio µ2/µ2, the variance divided by the squared mean of the P distribution. The
quantity µ2/µ2 is known as the coefficient of variation and is a common measure of dispersion of a distribution. Hence, for large K the bias of L^K is small when the coefficient of variation is small; this makes sense because in case the dispersion is small the logarithm function behaves like a linear
function and few bias results. The second-order and higher-order terms takes into account higher
order properties of P .

The bias is the key quantity we aim to reduce, but every estimator is also measured on its variance.

We now quantify the variance of the IWAE estimator.

Proposition 2 (Variance of L^K). For K  , the variance of L^K is given as follows.

V[L^K ] =

1 K

µ2 µ2

-

1 K2

µ3 µ3

-

5µ22 2µ4

+ o(K-2).

(11)

Proof. See Appendix A, page 12.

Both the bias B[L^K] and the variance V[L^K] vanish for K   at a rate of O(1/K) with similar coefficients. This leads to the following result which was already proven in Burda et al. (2015).

Corollary 2 (Consistency of L^K). For K   the estimator L^K is consistent, that is, for all > 0

lim P (|L^K - log p(x)|  ) = 0.
K 

(12)

Proof. See Appendix A, page 13.

How good are the asymptotic results? This is hard to say in general because it depends on the particular distribution P (w) of the weights. In Figure 1 we show both a simple and challenging case to demonstrate the accuracy of the asymptotics.
The above results are reassuring evidence for the IWAE method, however, they cannot be directly applied in practice because we do not know the moments µi. One approach is to estimate the moments from data, and this is in fact what the delta method variational inference (DVI) method does (see Appendix C, page 13); however, estimating moments accurately is difficult. We avoid the difficulty of estimating moments by use of the jackknife, a classic debiasing method. We now review this method.

4 A BRIEF REVIEW OF THE JACKKNIFE
We now provide a brief review of the jackknife and generalized jackknife methodology. Our presentation deviates from standard textbook introductions, Miller (1974), in that we also review higher-order variants.
The jackknife methodology is a classic resampling technique originating with Quenouille (1949; 1956) in the 1950s. It is a generally applicable technique for estimating the bias B[T^] = E[T^] - T and the variance V[T^] of an estimator T^. Our focus is on estimating and correcting for bias.

3

Under review as a conference paper at ICLR 2018

Bias

0.1 Bias of log p(x) evidence approximations, P =Gamma(1;1)

0.0

-0.1 -0.2 -0.3 -0.4 -0.5 -0.6
0

IWAE, ^[L^K ], empirical IWAE, [L^K ], asymptotic JVI, ^[L^KJ ], empirical JVI, [L^KJ ], asymptotic
5 10 15 20 K

Variance

10V1 ariance of log p(x) evidence approximations, P =Gamma(1;1) IWAE, ^[L^K ], empirical
100 IWAE, [L^K ], asymptotic JVI, ^[L^JK ], empirical
10-1

10-2 0

5 10 15 20 K

Bias

(a) Asymptotic bias for a simple case.
5 Bias of log p(x) evidence approximations, P =Gamma(0:1;1)

0

-5 -10 -150

IWAE, ^[L^K ], empirical IWAE, [L^K ], asymptotic JVI, ^[L^KJ ], empirical JVI, [L^JK ], asymptotic
5 10 15 20 K

Variance

(b) Asymptotic variance for a simple case.

1V0a3 riance of log p(x) evidence approximations, P =Gamma(0:1;1)

IWAE, ^[L^K ], empirical

102

IWAE, [L^K ], asymptotic JVI, ^[L^JK ], empirical

101

100

10-1 0

5 10 15 20 K

(c) Asymptotic bias for a challenging case.

(d) Asymptotic variance for a challenging case.

Figure 1: Comparing asymptotics with empirical values of bias and variance on P = Gamma(1, 1) using 100,000 independent evaluations: (a)-(b) shows a simple case, P = Gamma(1, 1), and (c)-(d) shows a challenging case. Observation: (a) the IWAE is negatively biased, underestimating log p(x), with asymptotic expression (10) agreeing very well with empirical bias; (b) empirical and asymptotic variance (11) in good agreement. (c) for a challenging case, the bias asymptotics match empirical estimates for K  10; (d) in the challenging case, the variance asymptotics match empirical estimates for K  10.

The basic intuition is as follows: in many cases it is possible to write the expectation of a consistent

estimator T^n evaluated on n samples as an asymptotic expansion in the sample size n, that is, for

large n   we have

E[T^n]

=

T

+

a1 n

+

a2 n2

+....

(13)

In particular, this is possible in case the estimator is consistent and a smooth function of linear

statistics. If an expansion (13) is possible, then we can take a linear combination of two estimators T^n and T^n-1 to cancel the first order term,

E[nT^n - (n - 1)T^n-1]

=

n

T

+

a1 n

+

a2 n2

- (n - 1)

T

+

a1 n-

1

+

(n

a2 - 1)2

+ O(n-2)

=

T

+

a2 n

-

a2 n-

1

+ O(n-2)

=

T

-

a2 n(n -

1)

+

O(n-2)

(14) (15)

= T + O(n-2).

(16)

Therefore, the jackknife bias-corrected estimator T^J := nT^n - (n - 1)T^n-1 achieves a reduced bias of O(n-2). For T^n-1 any estimator which preserves the expectation (13) can be used. In practice we use the original sample of size n to create n subsets of size n - 1 by removing each individual sample once. Then, the empirical average of n estimates T^n\-i 1, i = 1, . . . , n is used in place of T^n-1. In Sharot (1976) this construction was proved optimal in terms of maximally reducing the variance of T^J for any given sample size n.
In principle, the above bias reduction (16) can be repeated to further reduce the bias to O(n-3) and beyond. The possibility of this was already hinted at in Quenouille (1956) by means of an example.1
A fully general and satisfactory solution to higher-order bias removal was only achieved by the

1Which was subtly wrong and did not reduce the bias to O(n-2) as claimed, see Schucany et al. (1971).

4

Under review as a conference paper at ICLR 2018

generalized jackknife of Schucany et al. (1971), considering estimators T^G of order m, each having

the form,

m

T^G(m) = c(n, m, j) T^n-j .

(17)

j=0

The form of the coefficients c(n, m, j) in (17) are defined by the ratio of determinants of certain Vandermonde matrices, see Schucany et al. (1971). In a little known result, an analytic solution for c(n, m, j) is given by Sharot (1976). We call this form the Sharot coefficients, (Sharot, 1976, Equation (2.5) with r = 1), defined for m < n and 0  j  m,

c(n, m, j) = (-1)j (n - j)m . (m - j)! j!

(18)

The generalized jackknife estimator T^G(m) achieves a bias of order O(m-(j+1)), see Schucany et al. (1971). For example, the classic jackknife is recovered because c(n, 1, 0) = n and c(n, 1, 1) = -(n - 1). As an example of the second-order generalized jackknife we have

n2 c(n, 2, 0) = ,
2

c(n, 2, 1) = -(n - 1)2,

(n - 2)2

c(n, 2, 2) =

.

2

(19)

The variance of generalized jackknife estimators is more difficult to characterize and may in general decrease or increase compared to T^n. Typically we have V[T^G(m+1)] > V[T^G(m)] with asymptotic rates being the same.

The generalized jackknife is not the only method for debiasing estimators systematically. One classic method is the delta method for bias correction Small (2010). Two general methods for polynomial debiasing are the iterated bootstrap for bias correction (Hall, 2016, page 29) and the debiasing lemma McLeish (2010); Strathmann et al. (2015); Rhee & Glynn (2015). Remarkably, the debiasing lemma exactly debiases a large class of estimators.

The delta method bias correction has been applied to variational inference by Teh et al. (2007); we provide novel theoretical results for the method in Appendix C, page 13.

5 JACKKNIFE VARIATIONAL INFERENCE (JVI)

We now propose to apply the generalized jackknife for bias correction to variational inference by debiasing the IWAE estimator.
Definition 1 (Jackknife Variational Inference (JVI)). Let K  1 and m < K. The jackknife variational inference estimator of the evidence of order m with K samples is

m

L^JK,m :=

c(K, m, j) L¯K-j,

j=0

(20)

where L¯K-j is the empirical average of one or more IWAE estimates obtained from a subsample

of size K - j, and c(K, m, j) are the Sharot coefficients defined in (18). In this paper we use all

possible

K K -j

subsets, that is,

L¯K-j :=

1
K K -j

( )K K-j L^K-j (Zi(K-j)),
i=1

(21)

where Zi(K-j) is the i'th subset of size K - j among all

K K -j

subsets from the original samples

Z = (z1, z2, . . . , zK ). We further define LJK,m = EZ [L^JK,m].

From the above definition we can see that JVI strictly generalizes the IWAE bound and therefore also includes the standard ELBO objective: we have the IWAE case for L^KJ,0 = L^K , and the ELBO case for L^J1,0 = L^E.

5

Under review as a conference paper at ICLR 2018

5.1 ANALYSIS OF L^JK,m

The proposed family of JVI estimators has less bias than the IWAE estimator. The following result is a consequence of the existing theory on the generalized jackknife bias correction.

Proposition 3 (Bias of L^JK,m). For any K  1 and m < K we have that the bias of the JVI estimate satisfies

B[L^JK,m] = E[L^JK,m - log p(x)] = LKJ,m - log p(x) = O(K-(m+1)).

(22)

Proof. The JVI estimator L^KJ,m is the application of the higher-order jackknife to the IWAE estimator which has an asymptotic expansion of the bias (10) in terms of orders of 1/K. The stated result is then a special case of (Schucany et al., 1971, Theorem 4.2).

We show an illustration of higher-order bias removal in Appendix C, page 14. It is more difficult to characterize the variance of L^KJ,m. Empirically we observe that V[L^JK,m] < V[L^JK,m ] for m < m , but we have been unable to derive a formal result to this end. Note that the variance is over the sampling distribution of q(z|x), so we can always reduce the variance by averaging multiple estimates L^KJ,m, whereas we cannot reduce bias this way. Therefore, reducing bias while increasing variance is a
sensible tradeoff in our application.

5.2 EFFICIENT COMPUTATION OF L^KJ,m
We now discuss how to efficiently compute (20). For typical applications, for example in variational autoencoders, we will use small values of K, say K < 100. However, even with K = 50 and m = 2 there are already 1276 IWAE estimates to compute in (20­21). Therefore efficient computation is important to consider. One property that helps us is that all these IWAE estimates are related because they are based on subsets of the same weights. The other property that is helpful is that computation of the K weights is typically orders of magnitude more expensive than elementary summation operations required for computation of (21).
We now give a general algorithm for computing the JVI estimator L^JK,m, then give details for efficient implementation on modern GPUs and state complexity results.
Algorithm 1 computes log-weights and implements equations (20­21) in a numerically robust manner.2

Algorithm 1 Computing L^KJ,m, the jackknife variational inference estimator

1: function COMPUTEJVI(m, K, p, q, x)

2: for i = 1, . . . , K do

3: Sample zi  q(z|x) 4: vi  log p(x|zi) + log p(zi) - log q(zi|x)

5: end for

6: L  0

7: for j = 0, . . . , m do 8: L¯  0

9: for S  EnumerateSubsets({1, . . . , K}, K - j) do

10: L¯  L¯ + log sS exp vs - log(K - j)

11: end for

12:

L



L

+

c(K,m,j)
(KK-j)

L¯

13: end for

14: return L

15: end function

L¯K-j list all subsets of size K - j IWAE estimate for subset S
Using equation (18)
JVI estimate L^KJ,m

Proposition 4 (Complexity of Algorithm 1). Given K  1 and m  K/2 the complexity of

Algorithm 1 is

O Kem K m . m

(23)

2As usual, the log-sum-exp operation needs to be numerically robustly implemented.

6

Under review as a conference paper at ICLR 2018

Runtime for the full MNIST test set (GPU)
102

Runtime (s)

101 100

L^KJ;0 , IWAE (JVI-0) L^KJ;1 , JVI-1 L^KJ;2 , JVI-2 L^KJ;3 , JVI-3 L^KJ;4 , JVI-4 L^KJ;5 , JVI-5

101 102 103

K
Figure 2: Runtime evaluation of the L^KJ,m estimators.

Proof. See Appendix C, page 14.

The above algorithm is suitable for CPU implementation; to utilize modern GPU hardware efficiently we can instead represent the second part of the algorithm using matrix operations. We provide further details in Appendix C, page 15. Figure 2 demonstrates experimental runtime evaluation on the MNIST test set for different JVI estimators. We show all JVI estimators with less than 5,000 total summation terms. The result demonstrates that runtime is largely independent of the order of the JVI correction and only depends linearly on K.

5.3 VARIATIONS OF THE JVI ESTIMATOR

Variations of the JVI estimator with improved runtime exist. Such reduction in runtime are possible if we consider evaluating only a fraction of all possible subsets in (21). When tractable, our choice of evaluating all subsets is generally preferable in terms of variance of the resulting estimator. However, to show that we can even reduce bias to order O(K-K) at cost O(K) we consider the estimator

K-1

L^KX :=

c(K, K - 1, j) L^K-j (Z1:(K-j))

j=0

= c(K, K - 1, K - 1) log(exp(vK ))

+ c(K, K - 1, K - 2) log

1 2 (exp(vK-1) + exp(vK ))

(24) (25) (26)

+ · · · + c(K, K - 1, 0) log

1K K exp(vi)

.

i=1

(27)

The sum (25­27) can be computed in time O(K) by keeping a running partial sum

k i=1

exp(vi)

for k  K and by incrementally updating this sum3, meaning that (24) can be computed in O(K)

overall. As a generalized jackknife estimate L^KX has bias O(K-K ). We do not recommend its use in practice because its variance is large, however, developing estimators between the two extremes of

taking one set and taking all sets of subsets of a certain size seems a good way to achieve high-order

bias reduction while controlling variance.

6 EXPERIMENTS
We now empirically validate our key claims regarding the JVI method: 1. JVI produces better estimates of the marginal likelihood by reducing bias, even for small K; and 2. Higher-order bias reduction is more effective than lower-order bias reduction;
To this end we will use variational autoencoders trained on MNIST. Our setup is purposely identical to the setup of Tomczak & Welling (2016), where we use the dynamically binarized MNIST data set
3To do this in a numerically stable manner, we need to use streaming log-sumexp computations, see for example http://www.nowozin.net/sebastian/blog/ streaming-log-sum-exp-computation.html

7

Under review as a conference paper at ICLR 2018

Evidence: estimate of log p(x) Evidence: estimate of log p(x)
Evidence: estimate of log p(x)

-88 -89 -90 -91 -92 -93

Evidence Estimates on ELBO-trained VAE (MNIST test set)
L^1J;0 , ELBO L^KJ;0 , IWAE (JVI-0) L^KJ;1 , JVI-1 L^JK;2 , JVI-2 L^JK;3 , JVI-3 L^KJ;4 , JVI-4 L^JK;5 , JVI-5

-84 -86 -88 -90 -92 -94

Evidence Estimates on IWAE-trained VAE (MNIST test set)
L^ 1J;0 , ELBO L^ KJ;0 , IWAE (JVI-0) L^ KJ;1 , JVI-1 L^ KJ;2 , JVI-2 L^ JK;3 , JVI-3 L^ KJ;4 , JVI-4 L^ JK;5 , JVI-5

100 101 102 103

-91600 101 102 103

latent sample count K

latent sample count K

(a) Evidence estimates on VAE-trained MNIST model. (b) Evidence estimates on IWAE-trained MNIST model.

-84 Evidence Estimates on JVI-1-trained VAE (MNIST test set)

-86 -88 -90 -92 -94 -96 -98

L^ J1;0 , ELBO L^ JK;0 , IWAE (JVI-0) L^ KJ;1 , JVI-1 L^ KJ;2 , JVI-2 L^ KJ;3 , JVI-3 L^ JK;4 , JVI-4 L^ KJ;5 , JVI-5

-101000 101 102 103 latent sample count K
(c) Evidence estimates on JVI-1-trained MNIST model.

Figure 3: Comparing evidence approximations on MNIST variational autoencoders: (a) VAE trained
using the ELBO objective L^E; (b) VAE trained using the IWAE objective L^K with K = 32; (c) VAE trained using the JVI-1 objective L^KJ,1 with K = 32.

of Salakhutdinov & Murray (2008). Our numbers are therefore directly comparable to the numbers reported in the above works.
We first evaluate the accuracy of evidence estimates given a fixed model. This setting is useful for assessing model performance and for model comparison.

6.1 JVI AS EVALUATION METHOD

We train a regular VAE on the dynamically binarized MNIST dataset using either the ELBO, IWAE, or JVI-1 objective functions. We use the same two-layer neural network architecture with 300 hidden units per layer as in (Tomczak & Welling, 2016). We train on the first 50,000 training images, using 10,000 images for validation. We train with SGD for 5,000 epochs and take as the final model the model with the maximum validation objective, evaluated after every training epoch. Hyperparameters are the batch size in {1024, 4096} and the SGD step size in {0.1, 0.05, 0.01, 0.005, 0.001}. The final model achieving the best validation score is evaluated once on the MNIST test set. All our models are implemented using Chainer (Tokui et al., 2015) and run on a NVidia Titan X.

For three separate models, trained using the ordinary ELBO, IWAE, and JVI-1 objectives, we then

estimate the marginal log-likelihood (evidence) on the MNIST test set. For evaluation we use JVI

estimators up to order five in order to demonstrate higher-order bias reduction. Among all possible

JVI estimators up to order five we evaluate only those JVI estimators whose total sum of IWAE

estimates has less than 5,000 terms. For example, we do not evaluate L^J32,3 because it contains

32 0

+

32 1

+

32 2

+

32 3

= 5489 terms.4

Figure 3 shows the evidence estimates for three models. We make the following observations, applying to all plots: 1. Noting the logarithmic x-axis we can see that higher-order JVI estimates are more than one order of magnitude more accurate than IWAE estimates. 2. The quality of the evidence estimates empirically improves monotonically with the order of the JVI estimator; 3. In absolute terms the improvements in evidence estimates is larges for small values of K, which is what

4We do this because we discovered numerical issues for large sums of varying size and found all summations of less than a few thousand terms not to have this problem but we are looking into a way to compute more summation terms in a fast and robust manner.

8

Under review as a conference paper at ICLR 2018

Training objective (K = 32) ELBO IWAE JVI-1

Evaluation objective (nats), K = 32

ELBO

IWAE

JVI-1

JVI-2

-93.38 ± 0.03 -89.22 ± 0.02 -88.66 ± 0.02 -88.40 ± 0.02

-95.30 ± 0.05 -86.05 ± 0.01 -85.28 ± 0.03 -85.01 ± 0.02

-99.19 ± 0.06 -86.56 ± 0.02 -85.43 ± 0.02 -85.14 ± 0.01

Table 1: Evaluating models trained using ELBO, IWAE, and JVI-1 learning objectives.

is typically used in practice; 4. The higher-order JVI estimators remove low-order bias but significant higher-order bias remains even for K = 64, showing that on real VAE log-weights the contribution of higher-order bias to the evidence error is large; 5. The standard error of each test set marginal likelihood (shown as error bars, best visible in a zoomed version of the plot) is comparable across all JVI estimates; this empirically shows that higher-order bias reduction does not lead to high variance.
6.2 JVI AS A TRAINING OBJECTIVE
We now report preliminary results on learning models using the JVI objectives. The setting is the same as in Section 6.1 and we report the average performance of five independent runs.
Table 1 reports the results. We make the following observations: 1. When training on the IWAE and JVI-1 objectives, the respective score by the ELBO objective is impoverished and this effect makes sense in light of the work of Cremer et al. (2017). Interestingly the effect is stronger for JVI-1. 2. The model trained using the JVI-1 objective falls slightly behind the IWAE model, which is surprising because the evidence is clearly better approximated as demonstrated in Section 6.1. We are not sure what causes this issue.
7 RELATED WORK
Delta-method variational inference (DVI) proposed by Teh et al. (2007) is the closest method we are aware of and we discuss it in detail as well as provide novel results in Appendix C, page 13. Another exciting recent work is perturbative variational inference (Bamler et al., 2017) which considers different objective functions for variational inference; we are not sure whether there exists a deeper relationship to debiasing schemes.
There also exists a large body of work that uses the ELBO objective but considers ways to enlarge the variational family. This is useful because the larger the variational family, the smaller the bias.
NICE (Dinh et al., 2014), Hamiltonian Variational Inference (Salimans et al., 2015) Framework of normalizing flows (Rezende & Mohamed, 2015), which includes the special flows inverse autoregressive flow (Kingma et al., 2016) Householder flow (Tomczak & Welling, 2016)
Another way to improve the flexibility of the variational family has been to use implicit models (Mohamed & Lakshminarayanan, 2016) for variational inference; this line of work includes adversarial variational Bayes (Mescheder et al., 2017), wild variational inference (Li & Liu, 2016), deep implicit models (Tran et al., 2017), implicit variational models (Husza´r, 2017), and adversarial message passing approximations (Karaletsos, 2016).
8 CONCLUSION
In summary we proposed to leverage classic higher-order bias removal schemes for evidence estimation. Our approach is simple to implement, computationally efficient, and clearly improves over existing evidence approximations based on variational inference. More generally our jackknife variational inference debiasing formula can also be used to debias log-evidence estimates coming from annealed importance sampling.
However, one surprising finding from our work is that using our debiased estimates for training VAE models did not improve over the IWAE training objective and this is surprising because apriori a better evidence estimate should allow for improved model learning.
9

Under review as a conference paper at ICLR 2018

One possible extension to our work is to study the use of other resampling methods for bias reduction; promising candidates are the iterated bootstrap, the Bayesian bootstrap, and the debiasing lemma. These methods could offer further improvements on bias reduction or reduced variance, however, the key challenge is to overcome computational requirements of these methods or, alternatively, to derive key quantities analytically.5 Application of the debiasing lemma in particular requires the careful construction of a truncation distribution and often produces estimators of high variance.
While variance reduction plays a key role in certain areas of machine learning, our hope is that our work shows that bias reduction techniques are also widely applicable.

REFERENCES

Jordanka A Angelova. On moments of sample mean and variance. Int. J. Pure Appl. Math, 79:67­85, 2012.

Robert Bamler, Cheng Zhang, Manfred Opper, and Stephan Mandt. Perturbative black box variational inference. arXiv preprint arXiv:1709.07433, 2017.

Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.

Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-weighted autoencoders. arXiv preprint arXiv:1704.02916, 2017.

Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014.

Peter Hall. Methodology and theory for the bootstrap, 2016. URL http://anson.ucdavis. edu/~peterh/sta251/bootstrap-lectures-to-may-16.pdf.

Ferenc Husza´r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.

Theofanis Karaletsos. Adversarial message passing for graphical models. arXiv preprint arXiv:1612.05048, 2016.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.

Yingzhen Li and Qiang Liu. Wild variational approximations. In NIPS workshop on advances in approximate Bayesian inference, 2016.

Don McLeish. A general method for debiasing a monte carlo estimator. arXiv preprint arXiv:1005.2228, 2010.

Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.

Rupert G Miller. The jackknife-a review. Biometrika, 61(1):1­15, 1974.

Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.

Maurice H Quenouille. Approximate tests of correlation in time-series. Journal of the Royal Statistical Society. Series B (Methodological), 11(1):68­84, 1949.

5For example, if we could analytically compute EuU [log

K i=1

uiwi]

for

the

sampling

distributions

U

appearing in the jackknife and bootstrap methods, we could develop improved closed-form estimators of the log

marginal likelihood.

10

Under review as a conference paper at ICLR 2018

Maurice H Quenouille. Notes on bias in estimation. Biometrika, 43(3/4):353­360, 1956.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Chang-han Rhee and Peter W Glynn. Unbiased estimation with square root convergence for sde models. Operations Research, 63(5):1026­1043, 2015.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning, pp. 872­879. ACM, 2008.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1218­1226, 2015.
WR Schucany, HL Gray, and DB Owen. On bias reduction in estimation. Journal of the American Statistical Association, 66(335):524­533, 1971.
Trevor Sharot. The generalized jackknife: finite samples and subsample sizes. Journal of the American Statistical Association, 71(354):451­454, 1976.
Christopher G. Small. Expansions and Asymptotics for Statistics. CRC Press, 2010.
Heiko Strathmann, Dino Sejdinovic, and Mark Girolami. Unbiased bayes for big data: Paths of partial posteriors. arXiv preprint arXiv:1501.03326, 2015.
Yee W Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In Advances in neural information processing systems, pp. 1353­ 1360, 2007.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on Neural Information Processing Systems (NIPS), volume 5, 2015.
Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow. arXiv preprint arXiv:1611.09630, 2016.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv preprint arXiv:1702.08896, 2017.
Lingyun Zhang. Sample mean and sample variance: Their covariance and their (in) dependence. The American Statistician, 61(2):159­160, 2007.

APPENDIX A: PROOFS FOR THE IWAE ANALYSIS

EXPECTATION AND BIAS OF L^K

Proof. (Of Proposition 1, page 2) To show (8) we apply the delta method for moments (Small, 2010,

Section

4.3).

First,

we

define

the

random

variable

YK

:=

1 K

K i=1

wi

corresponding

to

the

sample

mean of w1, . . . , wK . Because of linearity of expectation we have E[YK ] = E[w]. We expand the

logarithm function log YK = log(E[w] + (YK - E[w])) around E[w] to obtain

log(E[w] + (YK - E[w]))

=

log E[w]

-

 j=1

(-1)j j E[w]j

(YK

-

E[w])j .

(28)

11

Under review as a conference paper at ICLR 2018

Note that only YK is random in (28), all other quantities are constant. Therefore, by taking the expectation on the left and right side of (28) we obtain

E[log YK ]

=

log

E[w]

-

 j=1

(-1)j j E[w]j

E[(YK

-

E[w])j ].

(29)

The right hand side of (29) is expressed in terms of the central moments for i  2, i := E[(YK - E[YK ])i] of YK , whereas we are interested in an expression using the central moments i  2, µi := E[(w - E[w])i] of P . With  = µ = E[w] we denote the shared first non-central moment. Because YK is a sample mean we can use existing results that relate i to µi. In particular (Angelova, 2012, Theorem 1) gives the relations

=µ

2

=

µ2 K

3

=

µ3 K2

4

=

3 K2

µ22

+

1 K3

µ4 - 3µ22

10 1 5 = K3 µ3µ2 + K4 (µ5 - 10µ3µ2) .

Expanding (29) to order five and using the relations (30) to (34) gives

(30) (31) (32) (33) (34)

E[log YK ]

=

log E[w] -

1 2µ2

µ2 K

+

1 3µ3

µ3 K2

-

1 4µ4

3 K2

µ22

+

1 K3

µ4 - 3µ22

1 + 5µ5

10 K3

µ3µ2

+

1 K4

(µ5

-

10µ3µ2)

+ o(K-3).

(35)

Regrouping the terms by order of K produces the result (8).

VARIANCE OF L^K

Proof. (Of Proposition 2, page 3) We use the definition of the variance and the series expansion of the logarithm function, obtaining

V[log YK ] = E[(log YK - E[log YK ])2]

(36)

=

 E


log µ -

(-1)i iµi

(YK

-

µ)i

-

log

µ

+



(-1)i iµi

E[(YK

-

µ)i]

2 

i=1 i=1



=

E

 (-1)i iµi

E[(YK - µ)i] - (YK - µ)i

i=1

2 .

(37)

By expanding (37) to third order and expanding all products we obtain a moment expansion of YK as follows.

V[log YK ]



2 µ2

-

1 µ3 (3

-

12)

+

2 3µ4 (4

-

13)

+

1 4µ4 (4

-

22)

-

1 3µ5

(5

-

23)

+

1 9µ6

(6

-

32).

(38) (39)

By substituting the sample moments i of YK with the central moments µi of the original distribution P and simplifying we obtain

V[log YK ]

=

1 K

µ2 µ2

-

1 K2

µ3 µ3

-

5µ22 2µ4

+ o(K-2).

(40)

12

Under review as a conference paper at ICLR 2018

CONSISTENCY OF L^K

Proof. We have

P (|L^K - log p(x)|  ) = P (|L^K - E[L^K ] + E[L^K ] - log p(x)|  )  P (|L^K - E[L^K ]| + |E[L^K ] - log p(x)|  )

(41) (42)



P (|L^K

-

E[L^K ]|



2 ) + P (|E[L^K ] - log p(x)|



). 2

(43)

The second term in (43) does not involve a random variable therefore is either zero or one. For large enough K it will always be zero due to (10).

For the first term in (43) we apply Chebyshev's inequality. We set  = 

and have

2 V[L^K ]

P (|L^K

-

E[L^K ]|



) 2

=

P (|L^K - E[L^K ]|  

V[L^K ])

1 2

=

4

V[L^K
2

]

= O(1/K).

(44) (45)
(46) (47)

Thus, for K   and any > 0 we have that (43) has a limit of zero. This establishes convergence in probability and hence consistency.

APPENDIX B: ANALYSIS OF DELTA-METHOD VARIATIONAL INFERENCE (DVI)

Definition 2 (Delta method Variational Inference (DVI) Teh et al. (2007)).

LDK

:=

Ez1,...,zK q(z|x)

1 log
K

K

wi

+

w^2 2K w^

,

i=1

where

wi

=

p(x|zi) p(zi q (zi |w)

)

,

i = 1, . . . , K,

w^2

:=

1 K -1

K
(wi - w^)2,

i=1

1K

w^ := K

wi,

i=1

so that w^2 corresponds to the sample variance and w^ corresponds to the sample mean.

The practical Monte Carlo estimator of (48) is defined as follows.

zi  q(z|x), i = 1, . . . , K,

L^KD

:=

1 log
K

K

wi

+

w^2 . 2K w^

i=1

(48)
(49) (50) (51)
(52) (53)

ANALYSIS OF DELTA METHOD VARIATIONAL INFERENCE

Proposition 5 (Bias of L^KD ). We evaluate the bias of L^DK in (53) as follows.

B[L^DK ]

=

-

1 K2

µ3 µ3

-

3µ22 2µ4

+ o(K-2).

(54)

13

Under review as a conference paper at ICLR 2018

Proof.

Consider the function f (x, y) =

x y2

and its second order Taylor expansion around (x, y) =

(µ2, µ),

f (µ2 + (µ^2 - µ2), µ + (µ^ - µ))



µ2 µ2

+

1 µ2 (µ^2

-

µ2)

-

2µ2 µ3

(µ^

-

µ)

(55)

-

2 µ3

(µ^2

-

µ2)(µ^

-

µ)

+

6µ2 2µ4

(µ^

-

µ)2.

(56)

Taking expectations on both sides cancels all linear terms and yields

µ^2 E µ^2



µ2 µ2

-

2 µ3 E[(µ^2

- µ2)(µ^ - µ)] +

3µ2 µ4

E[(µ^

-

µ)2].

(57)

By classic results we have that the expected variance of the sample mean around the true mean is related to the variance by E[(µ^ - µ)2] = µ2/K. Furthermore, Zhang (2007) showed a beautiful result
about the covariance of sample mean and sample variance for arbitrary random variables, namely that

Cov[µ^2, µ^] = E[(µ^2 - µ2)(µ^ - µ)] = µ3/K.

(58)

Using both results in (57) produces

µ^2 E µ^2

=

µ2 µ2

-

1 K

2µ3 µ3

-

3µ22 µ4

+ o(K-1).

(59)

We can now decompose the expectation of L^DK as follows.

E[L^KD ]

=

1K

E

log K

wi

+E

µ^2 2K µ^2

i=1

=

log E[w]

-

µ2 2K µ2

+

1 2K

µ2 µ2

-

1 K

2µ3 µ3

-

3µ22 µ4

+ o(K-1)

(60) (61)

1 = log E[w] - K2

µ3 µ3

-

3µ22 2µ4

+ o(K-2).

(62)

Notably, in (62) the 1/K term is cancelled exactly by the delta method correction, even though we used an empirical ratio estimator µ^2/µ^2. Subtracting the true mean log p(x) = log E[w] from (62)
yields the bias (54) and completes the proof.

EXPERIMENTAL COMPARISON OF DVI AND JVI
We perform the experiment shown in Figure 1 including the DVI estimator. The result is shown in Figure 4 and confirms that DVI reduces bias but that for the challenging case JVI is superior in terms of bias reduction.

APPENDIX C: MORE JVI DETAILS

COMPLEXITY PROOF

Proof. The first for loop of the algorithm has complexity O(K). The second part of the algorithm

considers all subsets of size K, K - 1, . . . , K - m. In total these are S(K, m) =

m j=0

K K -j

=

m j=0

K j

sets. Justin Melvin derived a bound on this partial binomial sum6, as

S(K, m)  em

K

m
.

m

(63)

For each of the S(K, m) sets we have to perform at most K operations to compute the log-sum-exp operation, which yields the stated complexity bound.

HIGHER-ORDER BIAS REMOVAL DEMONSTRATION
We illustrate the behaviour of the higher-order JVI estimators on the same P = Gamma(0.1, 1) example we used previously. Figure 5 demonstrates the increasing order of bias removal, O(K-(m+1)) for the L^KJ,m estimators.
6See https://mathoverflow.net/questions/17202/sum-of-the-first-k-binomial-coefficients-for-fi

14

Under review as a conference paper at ICLR 2018

Bias

0.1 Bias of log p(x) evidence approximations, P =Gamma(1;1)

0.0

-0.1 -0.2 -0.3 -0.4 -0.5 -0.6
0

IWAE, ^[L^K ], empirical IWAE, [L^K ], asymptotic DVI, ^[L^DK ], empirical DVI, [L^KD ], asymptotic JVI, ^[L^KJ ], empirical JVI, [L^JK ], asymptotic
5 10 15 20 K

Variance

10V1 ariance of log p(x) evidence approximations, P =Gamma(1;1)

IWAE, ^[L^K ], empirical

IWAE, [L^K ], asymptotic

100

DVI, ^[L^KD ], empirical

JVI, ^[L^KJ ], empirical

10-1

10-2 0

5 10 15 20 K

Bias

(a) Asymptotic bias for a simple case.
5 Bias of log p(x) evidence approximations, P =Gamma(0:1;1)

0 -5 -10 -150

IWAE, ^[L^K ], empirical IWAE, [L^K ], asymptotic DVI, ^[L^DK ], empirical DVI, [L^DK ], asymptotic JVI, ^[L^JK ], empirical JVI, [L^KJ ], asymptotic
5 10 15 20 K

Variance

(b) Asymptotic variance for a simple case.

1V0a3 riance of log p(x) evidence approximations, P =Gamma(0:1;1)

IWAE, ^[L^K ], empirical

102

IWAE, [L^K ], asymptotic DVI, ^[L^DK ], empirical

JVI, ^[L^JK ], empirical

101

100

10-1 0

5 10 15 20 K

(c) Asymptotic bias for a challenging case.

(d) Asymptotic variance for a challenging case.

Figure 4: Comparing asymptotics with empirical values of bias and variance on P = Gamma(1, 1) using 100,000 independent evaluations: (a)-(b) shows a simple case, P = Gamma(1, 1), and (c)-(d) shows a challenging case. Observation: (a) both DVI and JVI correct for bias efficiently; (b) DVI and JVI variance closely match. (c) for a challenging case, the JVI bias is considerably smaller than the DVI bias; (d) in the challenging case, JVI has a higher variance than both DVI and IWAE.

101Bias of log p(x) evidence approximations, P = Gamma(0.1, 1)

100

Bias

10 1
IWAE, [ K] DVI, [ KD] JVI-1, [ KJ,1] JVI-2, [ KJ,2] 10 2 JVI-3, [ KJ,3]
100 101 K
Figure 5: Absolute bias as a function of K.

GPU IMPLEMENTATION OF JVI

To this end let K  1 and m < K be fixed and assume the log-weights vi are concatenated in one column vector of K elements. We then construct a matrix B of size (|S|, K), where S is the set of
all subsets that will be considered,

m
S = EnumerateSubsets({1, . . . , K}, K - j).
j=0

(64)

15

Under review as a conference paper at ICLR 2018

There are |S| rows in B and each row in B corresponds to a subset S  S of samples so that we can use S to index the rows in B. We set

1 BS,i = |S| IiS ,

(65)

where Ipred is one if the predicate is true and zero otherwise. We furthermore construct a vector A with |S| elements. We set

AS = c(K, m, K - |S|)/

K K - |S|

= (-1)K-|S|

|S|! |S|m

.

K! (m - K + |S|)!

(66)

Using these definitions we can express the estimator as A log(B exp(v)), with the log and exp
operations being elementwise. However, this is not numerically robust. Instead we can compute the estimator in the log domain as logsumexp2(IS×1v + log B) A, where logsumexp2 denotes a log-sum-exp operation along the second axis. This can be easily implemented in modern neural
network frameworks and we plan to make our implementation available.

16

