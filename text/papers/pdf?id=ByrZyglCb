Under review as a conference paper at ICLR 2018
ROBUSTNESS OF CLASSIFIERS TO UNIVERSAL PERTURBATIONS: A GEOMETRIC PERSPECTIVE
Anonymous authors Paper under double-blind review
ABSTRACT
Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.
1 INTRODUCTION
Despite the success of deep neural networks in solving complex visual tasks He et al. (2015); Krizhevsky et al. (2012), these classifiers have recently been shown to be highly vulnerable to perturbations in the input space. In Moosavi-Dezfooli et al. (2017), state-of-the-art classifiers are empirically shown to be vulnerable to universal perturbations: there exist very small imageagnostic perturbations that cause most natural images to be misclassified. The existence of universal perturbation is further shown in Hendrik Metzen et al. (2017) to extend to other visual tasks, such as semantic segmentation. Universal perturbations fundamentally differ from the random noise regime, and exploit essential properties of deep networks to misclassify most natural images with perturbations of very small magnitude. Why are state-of-the-art classifiers highly vulnerable to these specific directions in the input space? What do these directions represent? To answer these questions, we follow a theoretical approach and find the causes of this vulnerability in the geometry of the decision boundaries induced by deep neural networks. For deep networks, we show that the key to answering these questions lies in the existence of shared directions (across different datapoints) along which the decision boundary is highly curved. This establishes fundamental connections between geometry and robustness to universal perturbations, and thereby reveals new properties of the decision boundaries induced by deep networks.
Our aim here is to derive an analysis of the vulnerability to universal perturbations in terms of the geometric properties of the boundary. To this end, we introduce two decision boundary models: 1) the locally flat model assumes that the first order linear approximation of the decision boundary holds locally in the vicinity of the natural images, and 2) the locally curved model provides a second order local description of the decision boundary, and takes into account the curvature information. We summarize our contributions as follows:
· Under the locally flat decision boundary model, we show that classifiers are vulnerable to universal directions as long as the normals to the decision boundaries in the vicinity of natural images are correlated (i.e., they approximately span a low dimensional space). This result formalizes and proves some of the empirical observations made in Moosavi-Dezfooli et al. (2017).
· Under the locally curved decision boundary model, the robustness to universal perturbations is instead driven by the curvature of the decision boundary; we show that the existence
1

Under review as a conference paper at ICLR 2018

of shared directions along which the decision boundary is positively1 curved implies the existence of very small universal perturbations.
· We show that state-of-the-art deep nets remarkably satisfy the assumption of our theorem derived for the locally curved model: there actually exist shared directions along which the decision boundary of deep neural networks are positively curved. Our theoretical result consequently captures the large vulnerability of state-of-the-art deep networks to universal perturbations.
· We finally show that the developed theoretical framework provides a novel (geometric) method for computing universal perturbations, and further explains some of the properties observed in Moosavi-Dezfooli et al. (2017) (e.g., diversity, transferability) regarding the robustness to universal perturbations.

2 DEFINITIONS AND NOTATIONS

Consider an L-class classifier f : Rd  RL. Given a datapoint x  Rd, we define the estimated

label k^(x) = argmaxk fk(x), where class. We define by µ a distribution

fk(x) is the over natural

kth component images in Rd.

of f The

(x) that corresponds to the kth main focus of this paper is to

analyze the robustness of classifiers to universal (image-agnostic) noise. Specifically, we define v

to be a universal noise vector if k^(x + v) = k^(x) for "most" x  µ. Formally, a perturbation v is

(, )-universal, if the following two constraints are satisfied:

v 2  ,

P k^(x + v) = k^(x)  1 - .

This perturbation image v is coined "universal", as it represents a fixed image-agnostic perturbation that causes label change for a large fraction of images sampled from the data distribution µ. In Moosavi-Dezfooli et al. (2017), state-of-the-art classifiers have been shown to be surprisingly
vulnerable to this simple perturbation regime.

It should be noted that universal perturbations are different from adversarial perturbations Szegedy et al. (2014); Biggio et al. (2013), which are datapoint-specific perturbations that are sought to fool a specific image. An adversarial perturbation is a solution to the following optimization problem

r(x) = arg min r 2 subject to k^(x + r) = k^(x),
rRd

(1)

which corresponds to the smallest additive perturbation that is necessary to change the label of the classifier k^ for x. From a geometric perspective, r(x) quantifies the distance from x to the decision
boundary (see Fig. 1a). In addition, due to the optimality conditions of Eq. (1), r(x) is orthogonal to
the decision boundary at x + r(x), as illustrated in Fig. 1a.

In the remainder of the paper, we analyze the robustness of classifiers to universal noise, with respect
to the geometry of the decision boundary of the classifier f . Formally, the pairwise decision boundary, when restricting the classifier to class i and j is defined by B = {z  Rd : fi(z) - fj(z) = 0} (we omit the dependence of B on i, j for simplicity). The decision boundary of the classifier hence corresponds to points in the input space that are equally likely to be classified as i or j.

In the following sections, we introduce two models on the decision boundary, and quantify in each case the robustness of such classifiers to universal perturbations. We then show that the locally curved model better explains the vulnerability of deep networks to such perturbations.

3 ROBUSTNESS OF CLASSIFIERS WITH FLAT DECISION BOUNDARIES

We start here our analysis by assuming a locally flat decision boundary model, and analyze the
robustness of classifiers to universal perturbations under this decision boundary model. We specifically study the existence of a universal direction v, such that

k^(x + v) = k^(x) or k^(x - v) = k^(x),

(2)

1Throughout the paper, the sign of the curvature is chosen according to the normal vector, and the data point x, as illustrated in Fig. 3

2

Under review as a conference paper at ICLR 2018

(a) Local geometry of the decision boundary.

Class 2
v -v

v -v v
-v
Class 1

(b) Universal direction v of a linear binary classifier.

Figure 1

where v is a vector of sufficiently small norm. It should be noted that a universal direction (as opposed to a universal vector) is sought in Eq. (2), as this definition is more adapted to the analysis of classifiers with locally flat decision boundaries. For example, while a binary linear classifier has a universal direction that fools all the data points, only half of the data points can be fooled with a universal vector (provided the classes are balanced) (see Fig. 1b). We therefore consider this slightly modified definition in the remainder of this section.

We start our analysis by introducing our local decision boundary model. For x  Rd, note that

x + r(x) belongs to the decision boundary and r(x) is normal to the decision boundary at x + r(x)

(see Fig. 1a). A linear approximation of the decision boundary of the classifier at x + r(x) is

therefore given by x + {v : r(x)T v =

r(x)

2 2

}.

Under this approximation, the vector r(x) hence

captures the local geometry of the decision boundary in the vicinity of datapoint x. We assume a

local decision boundary model in the vicinity of datapoints x  µ, where the local classification

region of x occurs in the halfspace r(x)T v 

r(x)

2 2

.

Equivalently, we assume that outside of this

half-space, the classifier outputs a different label than k^(x). However, since we are analyzing the

robustness to universal directions (and not vectors), we consider the following condition, given by

Ls(x, ) : v  B(), |r(x)T v| 

r(x)

2 2

= k^(x + v) = k^(x) or k^(x - v) = k^(x).

(3)

where B() is a ball of radius  centered at 0. An illustration of this decision boundary model is

provided in Fig. 2a. It should be noted that linear classifiers satisfy this decision boundary model,

as their decision boundaries are globally flat. This local decision boundary model is however more

general, as we do not assume that the decision boundary is linear, but rather that the classification region in the vicinity of x is included in x + {v : |r(x)T v|  r(x) 22}. Moreover, it should be noted that the model being assumed here is on the decision boundary of the classifier, and not an assumption on the classification function f .2 Fig. 2a provides an example of nonlinear decision

boundary that satisfies this model.

In all the theoretical results of this paper, we assume that r(x) 2 = 1, for all x  µ, for simplicity of the exposition. The results can be extended in a straightforward way to the case where r(x) 2 takes different values for points sampled from µ. The following result shows that classifiers following
the locally flat decision boundary model are not robust to small universal perturbations, provided
the normals to the decision boundary (in the vicinity of datapoints) approximately belong to a low dimensional subspace of dimension m d.

Theorem 1. Let   0,   0. Let S be an m dimensional subspace such that PS r(x) 2 

1 -  for almost all x  µ,, where PS is the projection operator on the subspace. Assume moreover

that Ls (x, ) holds

for almost all

x



µ,

with 

=

em (1-)

.

Then,

there exists

a

universal noise

vector v, such that

v

2





and

P
xµ

k^(x + v) = k^(x) or k^(x - v) = k^(x)

 1 - .

The proof can be found in supplementary material, and relies on the construction of a universal perturbation through randomly sampling from S. The vulnerability of classifiers to universal perturba-
tions can be attributed to the shared geometric properties of the classifier's decision boundary in the

2The decision boundary B is the zero level set of the functions fi - fj. f can be a highly nonlinear function of the inputs, even when the zero-level set B is locally flat in the vicinity of datapoints.

3

Under review as a conference paper at ICLR 2018
vicinity of different data points. In the above theorem, this shared geometric property across different data points is expressed in terms of the normal vectors r(x). The main assumption of the above theorem is specifically that normal vectors r(x) to the decision boundary in the neighborhood of data points approximately live in a subspace S of low dimension m < d. Under this assumption, the above result shows the existence of universal perturbations of 2 norm of order m.When m d, Theorem 1 hence shows that very small (compared to random noise, which scales as d Fawzi et al. (2016)) universal perturbations misclassifying most data points can be found.
Remark 1. Theorem 1 can be readily applied to assess the robustness of multiclass linear classifiers to universal perturbations. In fact, when f (x) = W T x, with W = [w1, . . . , wL], the normal vectors are equal to wi - wj, for 1  i, j  L, i = j. These normal vectors exactly span a subspace of dimension L - 1. Hence, by applying the result with  = 0, and m = L - 1, we obtain that linear classifiers are vulnerable to universal noise, with magnitude proportional to L - 1. In typical problems, we have L d, which leads to very small universal directions.
Remark 2. Theorem 1 provides a partial expalanation to the vulnerability of deep networks, provided a locally flat decision boundary is assumed. Evidence in favor of this assumption was given through visualization of randomly chosen cross-sections in Warde-Farley et al. (2016); Fawzi et al. (2016). In addition, normal vectors to the decision boundary of deep nets (near data points) have been observed to approximately span a subspace S of sufficiently small dimension in Moosavi-Dezfooli et al. (2017). However, unlike linear classifiers, the dimensionality of this subspace m is typically larger than the the number of classes L, leading to large upper bounds on the norm of the universal noise, under the flat decision boundary model. This simplified model of the decision boundary hence fails to exhaustively explain the large vulnerability of state-of-the-art deep neural networks to universal perturbations.
We show in the next section that the second order information of the decision boundary contains crucial information (curvature) that captures the high vulnerability to universal perturbations.

r x

r x

(a) Flat decision boundary model Ls(x, ).

(b) Curved decision boundary model Q(x, ).

Figure 2: Illustration of the decision boundary models considered in this paper. (a): For the flat decision boundary model, the set {v : |r(x)T v|  r(x) 22} is illustrated (stripe). Note that for v taken outside the stripe (i.e., in the grayed area), we have k^(x + v) = k^(x) or k^(x - v) = k^(x) in
the  neighborhood. (b): For the curved decision boundary model, the any vector v chosen in the grayed area is classified differently from k^(x).

4 ROBUSTNESS OF CLASSIFIERS WITH CURVED DECISION BOUNDARIES
We now consider a model of the decision boundary in the vicinity of the data points that allows to leverage the curvature of nonlinear classifiers. Under this decision boundary model, we study the existence of universal perturbations satisfying k^(x + v) = k^(x) for most x  µ.3
We start by establishing an informal link between curvature of the decision boundary and robustness to universal perturbations, that will be made clear later in this section. As illustrated in Fig. 3, the
3Unlike for classifiers with locally flat decision boundaries, we now consider the problem of finding a universal vector (as opposed to universal direction) that fools most of the data points. This corresponds to the notion of universal perturbations first highlighted in Moosavi-Dezfooli et al. (2017).

4

Under review as a conference paper at ICLR 2018

r(x) v
x

r(x) x

v

r(x) x

v

Figure 3: Link between robustness and curvature of the decision boundary. When the decision boundary is positively curved (left), small universal perturbations are more likely to fool the classifier.

norm of the required perturbation to change the label of the classifier along a specific direction v is smaller if the decision boundary is positively curved, than if the decision boundary is flat (or with negative curvature). It therefore appears from Fig. 3 that the existence of universal perturbations (when the decision boundary is curved) can be attributed to the existence of common directions where the decision boundary is positively curved for many data points. In the remaining of this section, we formally prove the existence of universal perturbations, when there exists common positively curved directions of the decision boundary.

Recalling the definitions of Sec. 2, a quadratic approximation of the decision boundary at z =

x + r(x) gives x + {v : (v - r(x))T Hz(v - r(x)) + xr(x)T (v - r(x)) = 0}, where Hz denotes

the Hessian of F at z, and x =

F (z) r(x) 2

2

,

with

F

=

fi - fj .

In this model, the second order

information (encoded in the Hessian matrix Hz) captures the curvature of the decision boundary.

We assume a local decision boundary model in the vicinity of datapoints x  µ, where the local

classification region of x is bounded by a quadratic form. Formally, we assume that there exists

 > 0 where the following condition holds for almost all x  µ:

Q(x, ) : v  B(), (v - r(x))T Hz(v - r(x)) + xr(x)T (v - r(x))  0 = k^(x + v) = k^(x).

An illustration of this quadratic decision boundary model is shown in Fig. 2b. The following result shows the existence of universal perturbations, provided a subspace S exists where the decision boundary has positive curvature along most directions of S:

Theorem 2. Let  > 0,  > 0 and m  N. Assume that the quadratic decision boundary model

Q (x, ) holds for almost all x  µ, with  =

2

log(2/) m

-1

+

-1/2.

Let

S

be

a

m

dimensional

subspace such that

P
vS

u  R2, x-1uT Hzr(x),vu  

u

2 2

 1 -  for almost all x  µ,

where Hzr(x),v = T Hz with  an orthonormal basis of span(r(x), v), and S denotes the unit sphere in S. Then, there is a universal perturbation vector v such that v 2   and
P k^(x + v) = k^(x)  1 -  - .
xµ

The above theorem quantifies the robustness of classifiers to universal perturbations in terms of the

curvature  of the decision boundary, along normal sections spanned by r(x), and vectors v  S (see

Fig. 4 (left) for an illustration of a normal section). Fig. 4 (right) provides a geometric illustration

of the assumption of Theorem 2. Provided a subspace S exists where the curvature of the decision

boundary in the vicinity of datapoints x is positive (along directions in S), Theorem 2 shows that

universal

perturbations

can

be

found

with

a

norm

of

approximately

-1 m

+

-1/2.

Hence,

when

the

curvature  is sufficiently large, the existence of small universal perturbations is guaranteed with

Theorem 2.4

Remark 1. We stress that Theorem 2 does not assume that the decision boundary is curved in the direction of all vectors in Rd, but we rather assume the existence of a subspace S where the decision boundary is positively curved (in the vicinity of natural images x) along most directions in S. Moreover, it should be noted that, unlike Theorem 1, where the normals to the decision boundary are
assumed to belong to a low dimensional subspace, no assumption is imposed on the normal vectors.

4Theorem 2 should not be seen as a generalization of Theorem 1, as the models are distinct. In fact, while the latter shows the existence of universal directions, the former bounds the existence of universal perturbations.

5

Under review as a conference paper at ICLR 2018

r v
x

Tx B

U

v r
x
1/

Figure 4: Left: Normal section U of the decision boundary, along the plane spanned by the normal vector r(x) and v. Right: Geometric interpretation of the assumption in Theorem 2. Theorem 2 assumes that the decision boundary along normal sections (r(x), v) is locally (in a  neighborhood) located inside a disk of radius 1/. Note the difference with respect to traditional notions of curvature, which express the curvature in terms of the osculating circle at x + r(x). The assumption we use
here is more "global".

Instead, we assume the existence of a subspace S leading to positive curvature, for points on the decision boundary in the vicinity of natural images.
Remark 2. Theorem 2 does not only predict the vulnerability of classifiers, but it also provides a constructive way to find such universal perturbations. In fact, random vectors sampled from the subspace S are predicted to be universal perturbations (see supp. material for more details). In Section 5, we will show that this new construction works remarkably well for deep networks, as predicted by our analysis.
5 EXPERIMENTAL RESULTS: UNIVERSAL PERTURBATIONS FOR DEEP NETS

We first evaluate the validity of the assumption of Theorem 2 for deep neural networks, that is the

existence of a low dimensional subspace where the decision boundary is positively curved along

most directions sampled from the subspace. To construct the subspace, we find the directions that

lead to large positive curvature in the vicinity of a given set of training points {x1, . . . , xn}. We recall that principal directions v1, . . . , vd-1 at a point z on the decision boundary correspond to the eigenvectors (with nonzero eigenvalue) of the matrix Hzt , given by Hzt = P HzP , where P denotes the projection operator on the tangent to the decision boundary at z, and Hz denotes the Hessian of
the decision boundary function evaluated at z Lee (2009). Common directions with large average

curvature at zi = xi + r(xi) (where r(xi) is the minimal perturbation defined in Eq. (1)) hence

correspond to the eigenvectors of the average Hessian matrix H = n-1

n i=1

Hzt i .

We

therefore

set our subspace, Sc, to be the span of the first m eigenvectors of H, and show that the subspace

constructed in this way satisfies the assumption of Theorem 2, for a deep net (LeNet) trained on

CIFAR-10 with n = 100. To determine whether the decision boundary is positively curved in most

directions of Sc (for unseen datapoints from the validation set), we compute the average curvature across random directions in Sc for points on the decision boundary, i.e. z = x + r(x); the average

curvature is formally given by

S (x) = E
vS

(P v)T Hz(P v)

Pv

2 2

,

where S denotes the unit sphere in Sc. In Fig. 6 (a), the average of S (x) across points sampled from the validation set is shown (as well as the standard deviation) in function of the subspace dimension m. Observe that when the dimension of the subspace is sufficiently small, the average curvature is
strongly oriented towards positive curvature, which empirically shows the existence of this subspace Sc where the decision boundary is positively curved for most data points in the validation set. This empirical evidence hence suggests that the assumption of Theorem 2 is satisfied, and that universal perturbations hence represent random vectors sampled from this subspace Sc.

To show this strong link between the vulnerability of universal perturbations and the positive curvature of the decision boundary, we now visualize normal sections of the decision boundary of deep networks

6

Under review as a conference paper at ICLR 2018

Figure 5: Visualization of normal cross-sections of the decision boundary, for ImageNet (CaffeNet, left) and CIFAR-10 (LeNet, right). Top: Normal cross-sections along (r(x), v), where v is the universal perturbation computed using the algorithm in Moosavi-Dezfooli et al. (2017). Bottom: Normal cross-sections along (r(x), v), where v is a random vector uniformly sampled from the unit sphere in Rd.

7 × 10-4

0.9

0.85 6
0.8
5 0.75

0.7 4
0.65

3 0.6

0.55
2 0.5

1 0

100 200 300 400 500 0.45 0

(a)

Sc Sf
100 200 300 400 500
(b)

Figure 6: (a) Average curvature S , averaged over 1000 validation datapoints, as a function of
the subspace dimension. (b) Fooling rate of universal perturbations (on an unseen validation set)
computed using random perturbations in 1) Sc: the subspace of positively curved directions, and 2) Sf : the subspace collecting normal vectors r(x). The dotted line corresponds to the fooling rate using the algorithm in Moosavi-Dezfooli et al. (2017). Sf corresponds to the largest singular vectors corresponding to the matrix gathering the normal vectors r(x) in the training set (similar to the
approach in Moosavi-Dezfooli et al. (2017)).

trained on ImageNet (CaffeNet) and CIFAR-10 (LeNet) in the direction of their respective universal perturbations. Specifically, we visualize normal sections of the decision boundary in the plane (r(x), v), where v is a universal perturbation computed using the universal perturbations algorithm of Moosavi-Dezfooli et al. (2017). The visualizations are shown in Fig. 5. Interestingly, the universal perturbations belong to highly positively curved directions of the decision boundary, despite the absence of any geometric constraint in the algorithm to compute universal perturbations. To fool most data points, universal perturbations hence naturally seek common directions of the embedding space, where the decision boundary is positively curved. These directions lead to very small universal perturbations, as highlighted by our analysis in Theorem 2. It should be noted that such highly curved directions of the decision boundary are rare, as random normal sections are comparatively very flat (see Fig. 5, second row). This is due to the fact that most principal curvatures are approximately zero, for points sampled on the decision boundary in the vicinity of data points.
Recall that Theorem 2 suggests a novel procedure to generate universal perturbations; in fact, random perturbations from Sc are predicted to be universal perturbations. To assess the validity of this result, Fig. 6 (b) illustrates the fooling rate of the universal perturbations (for the LeNet network on CIFAR-10) sampled uniformly at random from the unit sphere in subspace Sc, and scaled to have a fixed norm (1/5th of the norm of the random noise required to fool most data points). We assess the quality of such perturbation by further indicating in Fig. 6 (b) the fooling rate of the universal perturbation computed using the original algorithm in Moosavi-Dezfooli et al. (2017). Observe that
7

Under review as a conference paper at ICLR 2018

Figure 7: Diversity of universal perturbations randomly sampled from the subspace Sc. The normalized inner product between two perturbations is less than 0.1.

random perturbations sampled from Sc (with m small) provide very poweful universal perturbations, fooling nearly 85% of data points from the validation set. This rate is comparable to that of the algorithm in Moosavi-Dezfooli et al. (2017), while using much less training points (only n = 100, while at least 2, 000 training points are required by Moosavi-Dezfooli et al. (2017)). The very large fooling rates achieved with such a simple procedure (random generation in Sc) confirms that the curvature is the governing factor that controls the robustness of classifiers to universal perturbations,
as analyzed in Section 4. In fact, such high fooling rates cannot be achieved by only using the model
of Section 3 (neglecting the curvature information), as illustrated in Fig. 6 (b). Specifically, by generating random perturbations from the subspace Sf collecting normal vectors r(x) (which is the procedure that is suggested by Theorem 1 to compute universal perturbations, without taking into account second order information), the best universal perturbation achieves a fooling rate of 65%,
which is significantly worse than if the curvature is used to craft the perturbation.

The existence of this subspace Sc (and that universal perturbations are random vectors in Sc) further explains the high diversity of universal perturbations. Fig. 7 illustrates different universal perturbations for CIFAR-10 computed by sampling random directions from Sc. The diversity of such perturbations justifies why re-training with perturbed images
(as in Moosavi-Dezfooli et al. (2017)) does not significantly improve the robustness of such networks, as other directions in Sc can still lead to universal perturbations, even if the network becomes robust to some directions. Finally, it is interesting to note that this subspace Sc is likely to be shared not only across datapoints, but also different networks (to some
extent). To support this claim, Fig. 8 shows the cosine of the principal angles between subspaces ScLeNet and ScNiN, computed for LeNet and NiN Lin et al. (2014) models. Note that the first principal angles between
the two subspaces are very small, leading to shared directions between
the two subspaces. A similar observation is made for networks trained on ImageNet in the supp. material. The sharing of Sc across different networks explains the transferability of universal perturbations observed
in Moosavi-Dezfooli et al. (2017).

Cosine of Principal Angles

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0

ScLeNet vs ScNiN Two random subspaces 100 200 300 400 500

Figure 8: Cosine of principal angles between ScLeNet and ScNiN. For comparison, cosine of angles between two random
subspaces is also shown.

6 DISCUSSION AND RELATED WORK

In this paper, we analyzed the robustness of classifiers to universal perturbations, under two decision boundary models: Locally flat and curved. We showed that the first are not robust to universal directions, provided the normal vectors in the vicinity of natural images are correlated. While this model explains the vulnerability for e.g., linear classifiers, this model discards the curvature information, which is essential to fully analyze the robustness of deep nets to universal perturbations. The second, classifiers with curved decision boundaries, are instead not robust to universal perturbations, provided the existence of a shared subspace along which the decision boundary is positively curved (for most directions). We empirically verify this assumption for deep nets. Our analysis hence explains the existence of universal perturbations, and further provides a purely geometric approach for computing such perturbations, in addition to explaining properties of perturbations, such as their diversity.
Other authors have focused on the analysis of the robustness properties of SVM classifiers (e.g., Xu et al. (2009)) and new approaches for constructing robust classifiers (based on robust optimization) Caramanis et al. (2012); Lanckriet et al. (2003). More recently, some have assessed the robustness of deep neural networks to different regimes such as adversarial perturbations Szegedy et al. (2014); Biggio et al. (2013), random noise Fawzi et al. (2016), and occlusions Sharif et al. (2016); Evtimov et al. (2017). The robustness of classifiers to adversarial perturbations has been specifically studied in Szegedy et al. (2014); Goodfellow et al. (2015); Moosavi-Dezfooli et al. (2016); Carlini & Wagner

8

Under review as a conference paper at ICLR 2018

(2017); Baluja & Fischer (2017), followed by works to improve the robustness Madry et al. (2017); Gu & Rigazio (2014); Papernot et al. (2015); Cisse et al. (2017), and attempts at explaining the phenomenon in Goodfellow et al. (2015); Fawzi et al. (2015); Tabacof & Valle (2016); Tanay & Griffin (2016). This paper however differs from these previous works as we study universal (imageagnostic) perturbations that can fool every image in a dataset, as opposed to image-specific adversarial perturbations that are not universal across datapoints (as shown in Moosavi-Dezfooli et al. (2017)). Moreover, explanations that hinge on the output of a deep network being well approximated by a linear function of the inputs f (x) = W x + b are inconclusive, as the assumption is violated even for relatively small networks. We show here that it is precisely the large curvature of the decision boundary that causes vulnerability to universal perturbations. Our bounds indeed show an increasing vulnerability with respect to the curvature of the decision boundary, and represent up to our knowledge the first quantitative result showing tight links between robustness and curvature. In addition, we show empirically that the first-order approximation of the decision boundary is not sufficient to explain the high vulnerability to universal perturbations (Fig. 6 (b)). Recent works have further proposed new methods for computing universal perturbations Mopuri et al. (2017); Khrulkov & Oseledets (2017); instead, we focus here on an analysis of the phenomenon of vulnerability to universal perturbations, while also providing a constructive approach to compute universal perturbations leveraging our curvature analysis. Finally, it should be noted that recent works have studied properties of deep networks from a geometric perspective (such as their expressivity Poole et al. (2016); Montufar et al. (2014)); our focus is different in this paper as we analyze the robustness with the geometry of the decision boundary.
Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it is key to suppress this subspace of shared positive directions, which can possibly be done through regularization of the objective function. This will be the subject of future works.

A PROOF OF THEOREM 1

We first start by recalling a result from Fawzi et al. (2016), which is based on Dasgupta & Gupta (2003).

Lemma 1. Let v be a random vector uniformly drawn from the unit sphere Sd-1, and Pm be the projection matrix onto the first m coordinates. Then,

P

1

(,

m)

m d



Pmv

2 2



2(,

m)

m d

 1 - 2,

(4)

with 1(, m) = max((1/e)2/m, 1 -

2(1 - 2/m), and 2(, m) = 1 + 2

ln(1/) m

+

2

ln(1/) m

.

We use the above lemma to prove our result, which we recall as follows:

Theorem 1. Let   0,   0. Let S be an m dimensional subspace such that PS r(x) 2 

1 -  for almost all x  µ,, where PS is the projection operator on the subspace. Assume moreover

that Ls (x, ) holds

for almost all

x



µ,

with 

=

em (1-)

.

Then,

there exists

a

universal noise

vector v, such that

v

2





and

P
xµ

k^(x + v) = k^(x) or k^(x - v) = k^(x)

 1 - .



Proof.

Define S to be the unit sphere centered at 0 in the subspace S.

Let  =

em (1-)

,

and

denote

by

S the sphere scaled by . We have

E P k^(x + v) = k^(x) or k^(x - v) = k^(x)
vS xµ

= E P k^(x + v) = k^(x) or k^(x - v) = k^(x)
xµ vS

E
xµ

P |r(x)T v| -
vS

r(x)

2 2

0

=E
xµ

P
vS

|(PS r(x) + PSorth r(x))T v| -

r(x)

2 2

0

,

9

Under review as a conference paper at ICLR 2018

where PSorth denotes the projection operator on the orthogonal of S. Observe that (PSorth r(x))T v = 0.

Note moreover that

r(x)

2 2

= 1 by assumption.

Hence, the above expression simplifies to

E
xµ

P |(PS r(x))T v| - 1  0
vS

=E
xµ
E
xµ

P |(PS r(x))T v|  -1
vS

P
vS

(PS r(x))T PS r(x) 2

v



 em

,

where we have used the assumption of the projection of r(x) on the subspace S. Hence, it follows from Lemma 1 that

E P k^(x + v) = k^(x) or k^(x - v) = k^(x)  1 - .
vS xµ

Hence, there exists a universal vector v of P k^(x + v) = k^(x) or k^(x - v) = k^(x)  1 - .
xµ

2 norm  such that

B PROOF OF THEOREM 2

Theorem 2. Let  > 0,  > 0 and m  N. Assume that the quadratic decision boundary model

Q (x, ) holds for almost all x  µ, with  =

2

log(2/) m

-1

+

-1/2.

Let

S

be

a

m

dimensional

subspace such that

P
vS

u  R2, x-1uT Hzr(x),vu  

u

2 2

 1 -  for almost all x  µ,

where Hzr(x),v = T Hz with  an orthonormal basis of span(r(x), v), and S denotes the unit sphere in S. Then, there is a universal perturbation vector v such that v 2   and
P k^(x + v) = k^(x)  1 -  - .
xµ

Proof. Let x  µ. We have

E P k^(x + v) = k^(x)
vS xµ
= E P k^(x + v) = k^(x)
xµ vS

E
xµ

P
vS

x-1(v - r)T Hz(v - r) + rT (v - r)  0

=E
xµ

P
vS

x-1(v - r)T Hz(v - r) + rT (v - r)  0

Using the assumptions of the theorem, we have

P
vS

x-1(v - r)T Hz(v - r) + rT (v - r)  0

P
vS



v - r

2 2

+

rT

(v

-

r)



0

+

 P (1 - 2)vT r + 2 + ( - 1)  0 + 
vS

 P (1 - 2)vT r  - + P 2 + ( - 1) 

vS

vS

+ ,

for > 0. The goal is therefore to find  such that 2 + ( - 1)  , together with

P (1 - 2)vT r  -
vS

 . Let 2 =

+1 

.

Using

the

concentration

of

measure

on

the

sphere

Matousek (2002), we have

P
vS

vT r



- (1 -

2)

 2 exp

-

m2 22(1 - 2)2

.

10

Under review as a conference paper at ICLR 2018

To bound the above probability by , we set

=

C

 m

,

where

C

=

choose  such that

2 = -1 Cm-1/2 + 1

2 log(2/). We therefore

The solution of this second order equation gives



 = C-1m-1/2 +

-2C2m-1 + 4-1 2

 C-1m-1/2 + -1/2.

Hence, for this choice of , we have by construction

P
vS

x-1(v - r)T Hz(v - r) + rT (v - r)  0

  + .

We therefore conclude that E P k^(x + v) = k^(x)  1 -  - . This shows the existence
vS xµ
of a universal noise vector v  S such that k^(x + v) = k^(x) with probability larger than 1 -  - .

C TRANSFERABILITY OF UNIVERSAL PERTURBATIONS

Figure 9: Transferability of the subspace Sc across different networks. The first row shows normal cross sections along a fixed direction in Sc for VGG-16, with a subspace Sc computed with CaffeNet. Note the positive curvature in most cases. To provide a baseline for comparison, the second row illustrates normal sections along random directions.
Fig. 9 shows examples of normal cross-sections of the decision boundary across a fixed direction in Sc, for the VGG-16 architecture (but where Sc is computed for CaffeNet). Note that the decision boundary across this fixed direction is positively curved for both networks, albeit computing this subspace for a distinct network. The sharing of Sc across different nets explains the transferability of universal perturbations observed in Moosavi-Dezfooli et al. (2017).
REFERENCES
Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387, 2017.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387­402, 2013.
Constantine Caramanis, Shie Mannor, and Huan Xu. Robust optimization in machine learning. In Suvrit Sra, Sebastian Nowozin, and Stephen J Wright (eds.), Optimization for machine learning, chapter 14. Mit Press, 2012.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39­57. IEEE, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International Conference on Machine Learning (ICML), 2017.
11

Under review as a conference paper at ICLR 2018
Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22(1):60­65, 2003.
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers' robustness to adversarial perturbations. arXiv preprint arXiv:1502.02590, 2015.
Alhussein Fawzi, Seyed Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Neural Information Processing Systems (NIPS), 2016.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.
Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.
Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker Fischer. Universal adversarial perturbations against semantic image segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2755­2764, 2017.
Valentin Khrulkov and Ivan Oseledets. Art of singular vectors and universal adversarial perturbations. arXiv preprint arXiv:1709.03582, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 1106­1114, 2012.
Gert Lanckriet, Laurent Ghaoui, Chiranjib Bhattacharyya, and Michael Jordan. A robust minimax approach to classification. The Journal of Machine Learning Research, 3:555­582, 2003.
Jeffrey M Lee. Manifolds and differential geometry, volume 107. American Mathematical Society Providence, 2009.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In International Conference on Learning Representations (ICLR), 2014.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
Jiri Matousek. Lectures on discrete geometry, volume 108. Springer New York, 2002.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances In Neural Information Processing Systems, pp. 2924­2932, 2014.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Konda Reddy Mopuri, Utsav Garg, and R Venkatesh Babu. Fast feature fool: A data independent approach to universal adversarial perturbations. In British Machine Vision Conference (BMVC), 2017.
12

Under review as a conference paper at ICLR 2018
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1511.04508, 2015.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems, pp. 3360­3368, 2016.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 1528­1540. ACM, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.
Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. IEEE International Joint Conference on Neural Networks, 2016.
Thomas Tanay and Lewis Griffin. A boundary tilting persepective on the phenomenon of adversarial examples. arXiv preprint arXiv:1608.07690, 2016.
David Warde-Farley, Ian Goodfellow, T Hazan, G Papandreou, and D Tarlow. Adversarial perturbations of deep neural networks. Perturbations, Optimization, and Statistics, 2016.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. The Journal of Machine Learning Research, 10:1485­1510, 2009.
13

