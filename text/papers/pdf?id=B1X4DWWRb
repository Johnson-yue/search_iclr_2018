Under review as a conference paper at ICLR 2018
LEARNING WEIGHTED REPRESENTATIONS FOR GENERALIZATION ACROSS DESIGNS
Anonymous authors Paper under double-blind review
ABSTRACT
Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to machine learning applications. One example is the estimation of treatment effects from observational data, where a subtask is to predict the effect of a treatment on subjects that are systematically different from those who received the treatment in the data. A related kind of distributional shift appears in unsupervised domain adaptation, where we are tasked with generalizing to a distribution of inputs that is different from the one in which we observe labels. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift are often heuristic or rely on assumptions that are rarely true in practice, such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. In this work, we devise a family of algorithms to address these issues, by jointly learning a representation and a re-weighting of observed data in the induced representation. We show that our algorithms minimize an upper bound on the generalization error under design shift, and verify the effectiveness of this approach in causal effect estimation.
1 INTRODUCTION
A long-term goal in artificial intelligence is for agents to learn how to act. This endeavor relies on accurately predicting and optimizing for the outcomes of actions, and fundamentally involves estimating counterfactuals--what would have happened if the agent acted differently? In many applications, such as the treatment of patients in hospitals, experimentation is infeasible or impractical, and we are forced to learn from biased, observational data. Doing so requires adjusting for the distributional shift between groups of patients that received different treatments. A related kind of distributional shift arises in unsupervised domain adaptation, the goal of which is to learn predictive models for a target domain, observing ground truth only in a source domain.
In this work, we pose both domain adaptation and treatment effect estimation as special cases of prediction across shifting designs, referring to changes in both action policy and feature domain. We separate policy from domain as we wish to make causal statements about the policy, but not about the domain. Learning from observational data to predict the counterfactual outcome under treatment B for a patient who received A, one must adjust for the fact that treatment A was systematically given to patients of different characteristics from those who received treatment B. We call this predicting under a shift in policy. Furthermore, if all of our observational data comes from hospital P , but we wish to predict counterfactuals for patients in hospital Q, with a population that differs from P , an additional source of distributional shift is at play. We call this a shift in domain. Together, we refer to the combination of domain and policy as the design. The design for which we observe ground truth is called the source, and the design of interest the target.
The two most common approaches for addressing distributional shift are to learn shift-invariant representations of the data (Ajakan et al., 2014) or to perform sample re-weighting or matching (Shimodaira, 2000; Kallus, 2016). Representation learning approaches attempt to extract only information from the input that is invariant to a change in design and predictive of the variable of interest. Such representations are typically learned by fitting deep neural networks in which activations of deeper layers are regularized to be distributionally similar across designs (Ajakan et al., 2014; Long
1

Under review as a conference paper at ICLR 2018
et al., 2015). Although representation learning can be shown to reduce the error associated to distributional shift (Long et al., 2015), standard approaches are biased, even in the limit of infinite data. Re-weighting methods correct for distributional shift by assigning higher weight to samples from the source design that are representative of the target design, often using importance sampling. This idea has been well studied in, for example, the causal inference (Rosenbaum & Rubin, 1983), domain adaptation (Shimodaira, 2000) and reinforcement learning (Precup et al., 2001) literature. For example, in causal effect estimation, importance sampling is equivalent to re-weighting units by the probability of observed treatments. Re-weighting, with knowledge of importance sampling weights, often leads to asymptotically unbiased estimators of the target outcome, but may suffer from high variance in finite samples (Swaminathan & Joachims, 2015).
A significant hurdle is that optimal weights are rarely known in practice. There are a variety of methods to learn these weights. Weights can be estimated as the inverse of estimated propensities (Rosenbaum & Rubin, 1983; Freedman & Berk, 2008) but this plug-in approach can lead to highly unstable estimates. More stable methods learn weights by minimizing distributional distance metrics (Gretton et al., 2009; Kallus, 2016; 2017; Zubizarreta, 2015). Closely related, matching (Stuart, 2010) produces weights by finding units in the source design that are close in some metric to units in the target design. Specifying a distributional or unit-wise metric is challenging, especially if the input space is high-dimensional where no metric incorporating all features can ever be made small. This has inspired heuristics such as first performing variable selection and then finding a matching in the selected covariates.
Our key algorithmic contribution is to show how to combine the intuition behind shift-invariant representation learning and re-weighting methods by jointly learning a representation  of the input space and a weighting function w() to minimize a) the re-weighted empirical risk and b) a reweighted measure of distributional shift between designs. By letting w depend on  we alleviate the problem of choosing a metric by which to optimize unit weights, as  extracts information predictive of the outcome. At the same time, our theory still guarantees a uniform bound on true risk in the target design. This leads to a general algorithmic framework, and a natural bound on the generalization error under design shift.
Main contributions We bring together two techniques used to overcome distributional shift between designs--re-weighting and representation learning, with complementary robustness properties, generalizing existing methods based on either technique. We give finite-sample generalization bounds for prediction under design shift, assuming that we have access to neither importance sampling weights nor a well-specified model, and develop an algorithmic framework to minimize these bounds. We propose a neural network architecture that jointly learns a representation of the input and a weighting function to improve balance across changing settings. We apply our proposed algorithm to the task of predicting causal effects from observational data, achieving state-of-the art results on a widely used benchmark.
2 PREDICTING OUTCOMES UNDER DESIGN SHIFT
The goal of this work is to accurately predict outcomes of interventions T  T in contexts X  X drawn from a target design p(X, T ). The outcome of intervening with t  T is the potential outcome Y (t)  Y (Imbens & Rubin, 2015, Ch. 1­2), which has a stationary distribution pt(Y | X) given context X. For example, in the classical binary setting Y (1) represents the outcome under treatment and Y (0) the outcome under control. The target design consists of two components: the target policy p(T | X), which describes how one intends to map observations of contexts (such as patient prognostics) to interventions (such as pharmacological treatments) and the target domain p(X), which describes the population of contexts to which the policy will be applied. The target design is known to us only through m unlabeled samples (x1, t1), . . . , (xm, tm) from p(X, T ). Outcomes are only available to us in labeled samples from a source domain: (x1, t1, y1), . . . , (xn, tn, yn), where (xi, ti) are draws from a source design pµ(X, T ) and yi = yi(ti) is a draw from pT (Y | X), corresponding only to the factual outcome Y (T ) of the treatment administered. Like the target design, the source design consists of a domain of contexts for which we have data and a policy, which describes the (unknown) historical administration of treatment in the data. Only the factual outcomes of the treatments administered are observed, while the counterfactual outcomes yi(t) for t = ti are, naturally, unobserved. Our focus
2

Under review as a conference paper at ICLR 2018

is the observational or off-policy setting, in which interventions in the source design are performed non-randomly as a function of X, pµ(T | X) = pµ(T ). This encapsulates both the covariate shift often observed between treated and control populations in observational studies and the covariate shift between the domain of the study and the domain of an eventual wider intervention. Examples of this problem are plentiful: in addition to the example given in the introduction, consider predicting the return of an advertising policy based on the historical results of a different policy, applied to a different population of customers. We stress that we are interested in the causal effect of an intervention T on Y , conditioned on X. As such, we cannot think of X and T as a single variable.
Without additional assumptions, it is impossible to deduce the effect of an intervention based on observational data alone (Pearl, 2009), as it amounts disentangling correlation and causation. Crucially, for any unit i, we can observe the potential outcome yi(t) of at most one intervention t. To make prediction of outcomes and causal effects possible, we make the following standard assumptions.
Assumption 1 (Consistency, ignorability and overlap). For any unit i, assigned to intervention ti, we observe Yi = Y (ti). Further, {Y (t)}tT and the data-generating process pµ(X, T, Y ) satisfy strong ignorability: {Y (t)}tT  T | X and overlap: Prp (pµ(T | X) > 0) = 1.
Assumption 1 is a sufficient condition for causal identifiability (Rosenbaum & Rubin, 1983). Ignorability is also known as the no hidden confounders assumption, indicating that all variables that cause both T and Y are assumed to be measured. Under ignorability therefore, any domain shift in p(X) cannot be due to variables that causally influence T and Y , other than through X. Under Assumption 1, potential outcomes equal conditional expectations: E[Y (t) | X = x] = E[Y | X = x, T = t], and we may predict Y (t) by regression. We further assume common domain support, x  X : p(X = x) > 0  pµ(X = x) > 0. Finally, we adopt the notation p(x) := p(X = x).

2.1 RE-WEIGHTED RISK MINIMIZATION

We attempt to learn predictors f : X ×T  Y such that f (x, t) approximates E[Y | X = x, T = t].

Recall that under Assumption 1, this conditional expectation is equal to the (possibly counterfactual)

potential outcome Y (t), conditioned on X. Our goal is to ensure that these hypotheses are accurate

under a design p that deviates from the data-generating process, pµ. This is unlike the supervised learning setting in that the distribution p to which we wish to generalize is different from that which generated the training data, pµ. We measure the (in)ability of f to predict outcomes under , using the expected risk,

R(f ) := Ex,t,yp [ f (x, t, y)]

(1)

based on a sample from µ, Dµn = {(xi, ti, yi)  pµ; i = 1, ..., n}. Here, f (x, t, y) := L(f (x, t), y) is an appropriate loss function, such as the squared loss, L(y, y ) := (y - y )2 or the log-loss,

depending on application. As outcomes under the target design p are not observed, even through a finite sample, we cannot directly estimate (1) using the empirical risk under p. A common way to resolve this is to use importance sampling (Shimodaira, 2000)--the observation that if pµ and p have common support, with w(x, t) = p(x, t)/pµ(x, t),

Rµw (f ) := Ex,t,ypµ [w(x, t) f (x, t, y)] = R(f ) .

(2)

Hence, with access to w, an unbiased estimator of R(f ) may be obtained by re-weighting the (factual) empirical risk under µ,

R^µw (f )

:=

1 n

n

w(xi, ti) f (xi, ti, yi) .

i=1

(3)

Unfortunately, importance sampling weights can be very large when p is large and pµ small, re-

rsaurletilnygkinnolwarngienvparraiacnticcee,inanRd^µwn(efit)he(Sr wisawmi.nIanthparnin&cipJolea,chhoimwes,v2er0,1a5n)y.

More importantly, p(x, re-weighting function w

t) is with

the following property yields a valid risk under the re-weighted distribution pwµ .

Definition 1. A function w : X × T  R+ is a valid re-weighting of pµ if

Ex,tpµ [w(x, t)] = 1 and pµ(x, t) > 0  w(x, t) > 0. We denote the re-weighted density pµw(x, t) := w(x, t)pµ(x, t).

3

Under review as a conference paper at ICLR 2018

A natural candidate in place of w is an estimate w^ of the importance sampling weights. In this work, we adopt a different strategy, learning re-weighting functions w from observational data, that minimize an upper bound on the risk under .
Prediction under shifting design is of interest in several tasks, perhaps the most common of which are domain adaptation and counterfactual estimation. In this work, we focus on the latter, observing that the estimation of causal effects is an important special case.

2.2 CONDITIONAL TREATMENT EFFECT ESTIMATION

An important special case of our setting is when treatments are binary, T  {0, 1}, often interpreted
as treating (T = 1) or not treating (T = 0) a unit, and the domain is fixed across designs, pµ(x) = p(x). This is the classical setting for estimating treatment effects--the effect of choosing one intervention over another (Morgan & Winship, 2014).1 The effect of an intervention T in context X, is measured by the conditional average treatment effect (CATE),  (x) = E [Y (1) - Y (0) | X = x].

Predicting  for unobserved units involves prediction of both potential outcomes. In a clinical set-
ting, this is necessary to assess which medication should be administered to a certain individual. Historically, the (population) average treatment effect, ATE = Exp[ (x)], has received comparatively much more attention (Rosenbaum & Rubin, 1983), but is inadequate for personalized decision making. Using predictors f (x, t) of potential outcomes Y (t) in contexts X = x, we can estimate the CATE by ^(x) = f (x, 1)-f (x, 0) and measure the quality using the mean squared error (MSE),

MSE(^) = Ep (^(x) -  (x))2

(4)

In Section 4, we argue that estimating CATE from observational data requires overcoming distributional shift with respect to the treat-all and treat-none policies, in predicting each respective potential outcomes, and show how this can be used to give generalization bounds for CATE.

3 RELATED WORK
A large body of work has shown that under assumptions of ignorability and having a wellspecified model, various regression methods for counterfactual estimation are asymptotically consistent (Chernozhukov et al., 2017; Athey & Imbens, 2016; Belloni et al., 2014; Van Der Laan & Rubin, 2006). However, consistency results like these provide no insight in the case of model misspecification. Under model misspecification, ordinary regression may suffer from (unnecessarily) large bias when generalizing across designs. A common way to alleviate this is importance sampling, see Section 2. This idea is used in propensity-score methods (Austin, 2011), that use treatment assignment probabilities (propensities) to re-weight samples for causal effect estimation, and more generally in re-weighted regression, see e.g. (Swaminathan & Joachims, 2015). A major drawback of these methods is the assumption that the design density is known. To address this, others (Gretton et al., 2009; Kallus, 2016), have proposed learning weights to minimize the distributional distance between samples under p and pµ, but rely on specifying the data representation a priori.
Johansson et al. (2016); Shalit et al. (2017) proposed learning representations for counterfactual inference, inspired by work in unsupervised domain adaptation (Mansour et al., 2009). However, the generalization bounds of Johansson et al. (2016) do not apply to general hypotheses and the bounds of Shalit et al. (2017) and Long et al. (2015) are loose--even if infinite samples are available, they are not guaranteed to converge to the lowest possible error. Moreover, these approaches do not make use of important information that is can be estimated from data: the treatment assignment probabilities.

4 GENERALIZATION UNDER DESIGN SHIFT
We give a bound on the risk in predicting outcomes under a target design p(T, X) based on unlabeled samples from p and labeled samples from a source design pµ(T, X). Our result combines distribution matching and re-weighting, resulting in a tighter bound than the closest related work (Shalit et al., 2017). The predictors we consider are compositions f (x, t) = h((x), t) where
1Notions of causal effects exist also for the non-binary case, but these are not considered here.

4

Under review as a conference paper at ICLR 2018

 is a representation of x and h an hypothesis. We first state a result for the general design shift setting, then show how this result can be used to bound the error in prediction of treatment effects. In Section 5 we give a result about the asymptotic properties of the minimizers of the upper bound.
Our bounds build on the intuition that if either a)  is close to a re-weighting of µ, or b) the true outcome is a simple function of x and t, the gap between the target error and the re-weighted source error is small. This can be formalized using integral probability metrics (IPM) (Sriperumbudur et al., 2009) that measure distance between distributions w.r.t. a normed vector space of functions H.
Definition 2. The integral probability metric (IPM) distance, associated with a normed vector space of functions H, between distributions p and q is, IPMH(p, q) := suphH |Ep[h] - Eq[h]|.

Important examples of IPMs include the Wasserstein distance, for which H is the family of functions with Lipschitz constant at most 1, and the Maximum Mean Discrepancy for which H are functions
in the norm-1 ball in a reproducing kernel Hilbert space. We can now state the following result.

Lemma 1. For hypotheses f with loss f such that f / f H  H, and pµ, p with common support, there exists a valid re-weighting w of pµ, see Definition 1, such that,

R(f )  Rµw(f ) + f HIPMH(p, pµw)  Rµ(f ) + f HIPMH(p, pµ) .

(5)

The first inequality is tight for importance sampling weights, w(x, t) = p(x, t)/pµ(x, t). The second inequality is not tight for general f , even if f / f H  H, unless p = pµ.

Lemma 1 implies that there exist weighting functions w(x, t) that achieve a tighter bound than uniform weights w(x, t) = 1. While importance sampling weights result in a tight bound in expectation, they are not known in general, and can result in large variance in finite samples (Cortes et al., 2010). Instead, we will search for a weighting function w, that minimizes a finite-sample version of Lemma 1, but first we introduce the notion of representation learning to combat distributional shift.

The idea of learning representations to minimize distributional shift in representation space, and thus
the source-target error gap, has been applied to domain adaptation (Ajakan et al., 2014), algorithmic
fairness (Zemel et al., 2013) and counterfactual prediction (Shalit et al., 2017), to name a few.
We follow the setup of Shalit et al. (2017), and consider learning twice-differentiable, invertible representations  : X  Z, where Z is the representation space, and  : Z  X is the inverse representation, such that ((x)) = x for all x. Let C denote space of such representation functions. For a design , we let p,(z, t) be the distribution induced by  over Z × T , with pw,(z, t) := p,(z, t)w((z), t) its re-weighted form and p^w, its re-weighted empirical form, following our previous notation. Let G  {h : Z × T  Y} denote a set of hypotheses h(, t) operating on the representation  and let F the space of all compositions, F = {f = h((x), t) : h  G,   C}. We can now relate the expected target risk R(f ) to the re-weighted empirical source risk R^µw(f ).
Theorem 1. Given is a labeled sample (x1, t1, y1), ..., (xn, tn, yn) from pµ, and an unlabeled sample (x1, t1), ..., (xm, tm) from p, with corresponding empirical measures p^µ and p^. Suppose that  is a twice-differentiable, invertible representation, that h(, t) is an hypothesis, and f = h((x), t)  F . Define mt(x) = EY [Y | X = x, T = t], let h,((z), t) := L(h(z, t), mt((z))) where L is the squared loss, L(y, y ) = (y - y )2, and assume that there exists a constant B > 0 such that h,/B  H  {h : Z × T  Y}, where H is a reproducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) < . Finally, let w be a valid re-weighting of pµ,. Then with probability at least 1 - 2,

R(f )  R^µw(f ) + BIPMH(p^,, p^µw,) + Vµ(w,

f

)

CnF, n3/8

+

D,H

1 + 1 mn

+ Y2

(6)

where CnF, measures the capacity of F and has only logarithmic dependence on n, DmH,n, measures the capacity of H, Y2 is the expected variance in potential outcomes, and

Vµ(w, f ) = max

Epµ [w2(x, t)

2 f

(x,

t)],

Ep^µ [w2(x, t)

2 f

(x,

t)]

.

A similar bound exists where H is the family of functions Lipschitz constant at most 1, but with worse sample complexity.

5

Under review as a conference paper at ICLR 2018

See Appendix A.2 for a proof that involves applying finite-sample generalization bounds to the first inequality in Lemma 1, as well as moving to the space induced by the representation .
Using uniform weights w(x, t) = 1 in (6), results in a bound similar to that of Shalit et al. (2017) and Long et al. (2015). For  = µ, minimizing the resulting bound results biased hypotheses, even in the asymptotical limit, as the IPM term does not vanish when the sample size increases. This is a rather undesirable property, as even k-nearest-neighbor classifiers are consistent in the limit of infinite samples. Instead, we consider minimizing (6) with respect to w, improving the tightness of the bound. Recall from Lemma 1 that there exist weights w for which R(f ) = Rµw(f ).
Theorem 1 indicates, as noted by for example Cortes et al. (2010), that even though importance sampling weights w yield estimators with small bias, they can suffer from high variance due to the factor Vµ(w, f ). The factor B is not known in general as it depends on the true outcome, and is determined by f H as well as the determinant of the Jacobian of , see the appendix. Qualitatively, B measures the joint complexity of  and (r, t). In practice, Shalit et al. (2017) substituted a hyperparameter  for B, but discussed the difficulties of selecting its value without access to counterfactual labels. In our experiments, we explore a heuristic for adaptively choosing , based on measures of complexity of the observed held-out loss as a function of the input.
Theorem 1 is immediately applicable to the case of unsupervised domain adaptation in which there is only a single potential outcome of interest, T = {0}. In this case, pµ(T | X) = p(T | X).

Conditional Average Treatment Effects A simple argument shows that the error in predicting the conditional average treatment effect, MSE(^) can be bounded by the sum of risks under the constant treat-all and treat-none policies. As in Section 2.2, we consider the case of a fixed domain p(X) = pµ(X) and binary treatment T = {0, 1}. Let Rt (f ) denote the risk under the constant policy t such that x  X : pt (T = t | X = x) = 1.
Proposition 1. We have with MSE(^) as in (4) and Rt(f ) the risk under the constant policy t,

MSE(^)  2(R1 (f ) + R0 (f )) - 42

(7)

where  is such that t  T , x  X , Y (x, t)   and Y (x, t) is standard deviation of Y (t) conditioned on X = x.

The proof involves the relaxed triangle inequality and the law of total probability. By Proposition 1,
we can apply Theorem 1 to R1 and R0 separately, to obtain a bound on MSE( ). For brevity, we refrain from stating the full result, but emphasize that it follows from Theorem 1. In Section 6, we
evaluate our framework in treatment effect estimation, minimizing this bound.

5 JOINT LEARNING OF REPRESENTATIONS AND SAMPLE WEIGHTS

Motivated by the theoretical insights of Section 4, we propose to jointly learn a representation (x), a re-weighting w(x, t) and an hypothesis h(, t) by minimizing a bound on the risk under the target design, see (6). This approach improves on previous work in two ways: it alleviates the bias of Shalit et al. (2017) when sample sizes are large, see Section 4, and it increases the flexibility of the balancing method of Gretton et al. (2009) by learning the representation to balance.

For notational brevity, we let wi = w(xi, ti). Recall that p^w, is the re-weighted empirical distribution of representations  under p. The training objective of our algorithm is the RHS of (6), with hyperparameters  = (, h, w) substituted for model (and representation) complexity terms,

L(h, , w; )

=

1 n

n

wi

h((xi), ti) +

h n

R(h)

+



IPMG(p^,,

p^wµ,

)

+

w

i=1

Lh (h,,w;D,,h)

Lw (,w;D,,w )

w2 n

(8)

where R(h) is a regularizer of h, such as 2-regularization. We can show the following result.

Theorem 2. Suppose H is a reproducing kernel Hilbert space given by a bounded kernel. Suppose

weak overlap holds in that E[(p(x, t)/pµ(x, t))2] < . Then,



min L(h, , w; )]  min R(f ) + Op(1/ n + 1/ m) .

h,,w

f F

6

Under review as a conference paper at ICLR 2018

Context

Repres. Hypothesis Weightedrisk Treatment

        

DNN

  

IPM(+,-, ./,-)

Weighting

Imbalance

Figure 1: Architecture for predicting outcomes under design shift. A re-weighting function w is fit jointly with a representation  and hypothesis h of the potential outcomes, to minimize a bound on the target risk. Dashed lines are not back-propagated through. Regularization penalties not shown.

Table 1: Causal effect estimation on IHDP. CATE error, target prediction error and std errors. Lower is better.

OLS OLS-IPW Random For. Causal For. BART IPM-WNN CFRW RCFR Oracle , w = 1 RCFR Oracle  RCFR Adapt. 

RMSE(^)
2.3 ± .11 2.4 ± .11 6.6 ± .30 3.8 ± .18 2.3 ± .10 1.2 ± .12 .76 ± .02
.81 ± .07 .65 ± .04 .67 ± .05

R^(f ) 1.1 ± .05 1.2 ± .05 4.1 ± .18 1.8 ± .08 1.7 ± .07 .65 ± .02 .46 ± .01
.47 ± .03 .38 ± .01 .37 ± .01

Consequently, under the assumptions of Thm. 1, for sufficiently large  and w,

R (f^n )



min

R (f

)

+

Op(1/n3/8

+

 1/ m).

f F

In words, the minimizers of (8) converge to the representation and hypothesis that minimize the counterfactual risk, in the limit of infinite samples.

While minimizing L(h, , w; ) directly over h,  and w is justified by Theorem 2, we note that adjusting w to minimize the empirical risk term serves little purpose, as it may result in overemphasizing "easy" training examples, especially if  is small. Instead, as a heuristic, we split the objective in two, see (8), and use only the IPM term and regularizer to learn w. In short, we solve the following alternating minimization problem.
hk, k = arg min Lh(h, , w; D, , h), wk+1 = arg min Lw(k, w; D, , w) (9)
h, w

In principle, the weighting function w(x, t) could be represented as one free parameter per training point, as it is only used to learn the model, not for prediction. However, we propose to let w be a parametric function of (x). Doing so enables us to both control the smoothness of the weights more flexibly, and lets us compute weights on a hold-out set, for example to perform early stopping. An example architecture for this framework is presented in Figure 1. As noted previously, estimating treatment effects involves predicting under the two constant policies--treat-everyone and treat-noone. In Section 6, we evaluate our framework in this task.
As noted by Shalit et al. (2017), choosing hyperparameters for prediction outcomes is fundamentally difficult, as we cannot observe ground truth for counterfactuals. In this work, we explore setting the balance parameter  adaptively.  is used in (8) in place of a factor measuring the complexity of the loss and representation function as functions of the input, a quantity that evolves during training. As a heuristic, we use an approximation of the Lipschitz constant of f based on observed examples, with f = h((x), t), h, = maxi,j[n] | f (xi, ti) - f (xj, tj)|/ xi - xj 2. We use a moving average over batches to encourage stability.

6 EXPERIMENTS
We evaluate our framework in the CATE estimation setting, see Section 2.2--our task is to predict the expected difference between potential outcomes conditioned on pre-treatment variables, for a held-out sample of the population. We compare our results to ordinary least squares (OLS) (with one regressor per outcome), OLS-IPW (re-weighted OLS according to a logistic regression estimate of propensities), Random Forests, Causal Forests (Wager & Athey, 2017), BART (Chipman et al., 2010), and CFRW (Shalit et al., 2017) (with Wasserstein penalty). Finally, we use as baseline (IPM-WNN): first weights are found by IPM minimization in the input space (Gretton et al., 2009; Kallus, 2016), then used in a re-weighted neural net regression, with the same architecture as our

7

Under review as a conference paper at ICLR 2018

CATE Error, RMSE(^) Counterfactual gap, RMSE(^)/ ^Rµ

 =0.0001

1.6

 =0.01  =1

 =10

1.4  =100

 =1000

1.2

1.0

0.8

10-4

10-2

10-0

102



Re-weighting regularization w (uniformity)

1.35 1.30 1.25 1.20 1.15 1.10 1.05 1.00
0.6

Increasing  , with  re-weighting Increasing  ,  without  re-weighting

0.7 0.8 0.9
Factual RMSE, R^µ

1.0

1.1

Figure 2: Error in CATE estimation on IHDP as a function of re-weighting regularization strength
w (left) and source prediction error (right). We see in the left-hand plot that a) for small imbalance penalties , re-weighting (low w) has no effect, b) for moderate , less uniform re-weighting (smaller w) improves the error, c) for large , weighting helps, but overall error increases. In the right-hand plot, we compare the ratio of CATE error to source error. Color represents  (see left)
and size w. We see that for large , the source error is more representative of CATE error, but does not improve in absolute value without weighting. Here,  was fixed. Best viewed in color.

method. We use IHDP as benchmark, a semi-synthetic binary-treatment dataset (Hill, 2011), split into training and test sets by Shalit et al. (2017). IHDP has synthesized continuous outcomes that can be used to compute the ground-truth CATE error.
Our implementation, dubbed RCFR for re-weighted Counterfactual Regression, parameterizes representations (x), weighting functions w(, t) and hypotheses h(, t) using neural networks, trained by minimizing (8). We use the RBF-kernel maximum mean discrepancy as the IPM (Gretton et al., 2012). For a description of the architecture, training procedure and hyperparameters, see Appendix B. We compare results using uniform w = 1 and learned weights, setting the balance parameter  either fixed, by an oracle (test-set error), or adaptively using the heuristic described in Section 5. To pick other hyperparameters, we split training sets into one part used for function fitting and one used for early stopping and hyperparameter selection. Hyperparameters for regularization are chosen based on the empirical risk on a held-out source (factual) sample.
We present the results of our evaluation on IHDP in Table 1. We see that our proposed method achieves state-of-the-art results, and that adaptively choosing  does not hurt performance much. Furthermore, we see a substantial improvement from using non-uniform sample weights. In Figure 2 we take a closer look at the behavior of our model as we vary its hyperparameters on the IHDP dataset. Between the two plots we can draw the following conclusions: a) For moderate to large   [10, 100], we observe a marginal gain from using the IPM penalty. This is consistent with the observations of Shalit et al. (2017). b) For large   [10, 1000], we see a large gain from using a non-uniform re-weighting (small w). c) While large  makes the factual error more representative of the counterfactual error, using it without re-weighting results in higher absolute error.
7 DISCUSSION
We have proposed a theory and an algorithmic framework for learning to predict outcomes of interventions under shifts in design--changes in both intervention policy and feature domain. The framework combines representation learning and sample re-weighting to balance source and target designs, emphasizing information from the source sample relevant to the target. Existing reweighting methods either use pre-defined weights or learn weights based on a measure of distributional distance in the input space. These approaches are highly sensitive to the choice of metric used to measure balance, as the input may be high-dimensional and contain information that is not predictive of the outcome. In contrast, by learning weights to achieve balance in representation space, we base our re-weighting only on information that is predictive of the outcome. In this work, we apply this framework to causal effect estimation, but emphasize that joint representation learning and re-weighting is a general idea that could be applied in many applications with design shift.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, and Mario Marchand. Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.
Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27):7353­7360, 2016.
Peter C Austin. An introduction to propensity score methods for reducing the effects of confounding in observational studies. Multivariate behavioral research, 46(3):399­424, 2011.
Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2):608­650, 2014.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins, et al. Double/debiased machine learning for treatment and causal parameters. Technical report, 2017.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1):266­298, 2010.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In Advances in neural information processing systems, pp. 442­450, 2010.
David A Freedman and Richard A Berk. Weighting regressions by propensity scores. Evaluation Review, 32(4):392­409, 2008.
Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and Bernhard Scho¨lkopf. Covariate shift by kernel mean matching. 2009.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723­773, 2012.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 2011.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In International Conference on Machine Learning, pp. 3020­3029, 2016.
Nathan Kallus. Generalized optimal matching methods for causal inference. arXiv preprint arXiv:1612.08321, 2016.
Nathan Kallus. Optimal a priori balance in the design of controlled experiments. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2017. doi: 10.1111/rssb.12240.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pp. 97­105, 2015.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Stephen L Morgan and Christopher Winship. Counterfactuals and causal inference. Cambridge University Press, 2014.
Judea Pearl. Causality. Cambridge university press, 2009.
Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with function approximation. In ICML, pp. 417­424, 2001.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41­55, 1983.
9

Under review as a conference paper at ICLR 2018
Uri Shalit, Fredrik Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In International Conference on Machine Learning, 2017.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scho¨lkopf, and Gert RG Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.
Elizabeth A Stuart. Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1):1, 2010.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In International Conference on Machine Learning, pp. 814­823, 2015.
Mark J Van Der Laan and Daniel Rubin. Targeted maximum likelihood learning. The International Journal of Biostatistics, 2(1), 2006.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, (just-accepted), 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 325­ 333, 2013.
Jose´ R Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511):910­922, 2015.
10

Under review as a conference paper at ICLR 2018

APPENDIX

A PROOFS

A.1 DEFINITIONS

Distribution re-weighting Definition 1 (Restated). A function w : X × T  R+ is a valid re-weighting of pµ if
Ex,tpµ [w(x, t)] = 1 and pµ(x, t) > 0  w(x, t) > 0. We denote the re-weighted density pwµ (x, t) := w(x, t)pµ(x, t).

Expected & empirical risk We let the (expected) risk of f measured by h under pµ be denoted

Rµ(h) = Epµ [lh(x, t)]

where lh is an appropriate loss function, and the empirical risk over a sample Dµ = {(x1, t1, y1)..., (xn, tn, yn) from pµ

R^µ(f )

=

1 n

n

lf (xi, ti, yi) .

i=1

We use the superscript w to denote the re-weighted risks

Rµw(f ) = E[w(x, t)lf (x, t)]

R^µw(f )

=

1 n

n

w(xi, ti)lh(xi, ti, yi)

i=1

Definition A1 (Importance sampling). For two distributions p, q on Z, of common support, z  Z : p(z) > 0  q(z) > 0, we call

q(z) wIS(z) := p(z)

the importance sampling weights of p and q.
Definition 2 (Restated). The integral probability metric (IPM) distance, associated with the function family H, between distributions p and q is defined by

IPMH(p, q) := sup |Ep[h] - Eq[h]|
h: h H=1

A.2 LEARNING BOUNDS

We begin by bounding the expected risk under a distribution p in terms of the expected risk under pµ and a measure of the discrepancy between p and pµ. Using definition 2 we can show the following result.

Lemma 1 (Restated). For hypotheses f with loss f such that f / f H  H, and pµ, p with common support, there exists a valid re-weighting w of pµ, see Definition 1, such that,

R(f )  Rµw(f ) + f HIPMH(p, pµw)  Rµ(f ) + f HIPMH(p, pµ) .

(10)

The first inequality is tight for importance sampling weights, w(x, t) = p(x, t)/pµ(x, t). The second inequality is not tight for general f , even if f  H, unless p = pµ.

Proof. The results follows immediately from the definition of IPM.
R(f ) - Rµw(f ) = E[ f (x, t)] - Eµ[w(x, t) f (x, t)]  sup |E[h(x, t)] - Eµ[w(x, t)h(x, t)]|
hH
= IPMH (p, pwµ )

11

Under review as a conference paper at ICLR 2018

Further,

for

importance

sampling

weights

wIS(x, t)

=

(t;x) µ(t;x)

,

for

any

h



H,

E [h(x,

t)]

-

Eµ[wIS

(x,

t)h(x,

t)]

=

E [h(x,

t)]

-

Eµ[

(t; µ(t;

x) x)

h(x,

t)]

=0

and the LHS is tight.

We could apply Lemma 1 to bound the loss under a distribution q based on the weighted loss under p. Unfortunately, bounding the expected risk in terms of another expectation is not enough to reason about generalization from an empirical sample. To do that we use Corollary 2 of Cortes et al. (2010), restated as a Theorem below.
Theorem A1 (Generalization error of re-weighted loss (Cortes et al., 2010)). For a loss function h of any hypothesis h  H  {h : X  R}, such that d = Pdim({ h : h  H}) where Pdim is the pseudo-dimension, and a weighting function w(x) such that Ep[w] = 1, with probability 1 -  over a sample (x1, ..., xn), with empirical distribution p^,

Rpw(h)  R^pw(h) + 25/4Vp,p^[w(x)l(x)]

d log

2ne d

+

log

4 

n

3/8

with Vp,p^[w(x)l(x)] = max(

Ep[w2(x)

2 h

(x)],

Ep^[w2(x) h2 (x)]). With

CnH = 25/4

2ne 4 d log + log
d

3/8

we get the simpler form

Rpw (h)



R^ pw (h)

+

Vp,p^[w(x)l(x)]

CnH n3/8

.

We will also need the following result about estimating IPMs from finite samples from Sriperumbudur et al. (2009).

Theorem A2 (Estimation of IPMs from empirical samples (Sriperumbudur et al., 2009)). Let M be
a measurable space. Suppose k is measurable kernel such that supxM k(x, x)  C   and H the reproducing kernel Hilbert space induced by k, with  := supxM,fH f (x) < . Then, with p^, q^ the empirical distributions of p, q from m and n samples respectively, and with probability at
least 1 - ,

|IPMH(p, q) - IPMH(p^, q^)| 

182

log

4 C



1 + 1 mn

We consider learning twice-differentiable, invertible representations  : X  Z, where Z is the representation space, and  : Z  X is the inverse representation, such that ((x)) = x for all x. Let C denote space of such representation functions. For a design , we let p,(z, t) be the distribution induced by  over Z × T , with pw,(z, t) := p,(z, t)w((z), t) its re-weighted form and p^w, its re-weighted empirical form, following our previous notation. Note that we do not include t in the representation itself, although this could be done in principle. Let G  {h : Z × T  Y} denote a set of hypotheses h(, t) operating on the representation  and let F denote the space of all compositions, F = {f = h((x), t) : h  G,   C}. We now restate and prove Theorem 1.
Theorem 1 (Restated). Given is a labeled sample Dµ = {(x1, t1, y1), ..., (xn, tn, yn)} from pµ, and an unlabeled sample D = {(x1, t1), ..., (xm, tm)} from p, with corresponding empirical measures p^µ and p^. Suppose that  is a twice-differentiable, invertible representation, that h(, t) is an hypothesis, and f = h((x), t)  F . Define mt(x) = EY [Y | X = x, T = t], let h,((z), t) := L(h(z, t), mt((z))) where L is the squared loss, L(y, y ) = (y - y )2, and assume that there exists a constant B > 0 such that h,/B  H  {h : Z × T  Y}, where
12

Under review as a conference paper at ICLR 2018

H is a reproducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) < . Finally, let w be a valid re-weighting of pµ,. Then with probability at least 1 - 2,

R(f )  R^µw(f ) + BIPMH(p^,, p^wµ,) + Vµ(w,

f

)

CnF, n3/8

+ D,H

1 + 1 mn

+ Y2

(11)

where CnF, measures the capacity of F and has only logarithmic dependence on n, DmH,n, measures the capacity of H, Y2 is the expected variance in potential outcomes, and

Vµ(w, f ) = max(

Epµ [w2(x, t)

2 f

(x,

t)],

Ep^µ [w2(x, t)

2 f

(x,

t)])

.

A similar bound exists where H is the family of functions Lipschitz constant at most 1, but with worse sample complexity.

Proof. We have by definition R(f ) - Rµw(f ) = E[ f (x, t, y)] - Eµ[w(x, t) f (x, t, y)]
= f (x, t, y)p(y | t, x)(p(x, t) - pµw(x, t))dxdtdy
x,t,y
Define h,(x, t) = L(h((x), t), mt(x)) where mt(x) := E[Y | T = t, X = x]). Then, with L, the squared loss, L(y, y ) = (y - y )2, we have,
E[ h,(x, t, y)] = E[ h,(x, t)] + 2 where 2 = Ep [(Y - mt(x))2], and analogously for µ. We get that

R(f ) - Rµw(f ) =

h,(x, t)(p(x, t) - pwµ (x, t))dxdt + 2 + µ2

xX ,tT

= h,((z), t)(p,(z, t) - pµw,(z, t))|J(z)|dzdt + 
z Z ,tT

 A

h,((z), t)(p(z, t) - pµw(z, t))dzdt + 2 + µ2

z Z ,tT

 2 + µ2 + A h, H sup

h((z), t) p,(rz, t) - pµw,(z, t) dzdt

hH zZ,tT

= B · IPMH(p,, pµw,) + 2 + µ2

where J(z) is the Jacobian matrix of  evaluated at z and A  |J(z)| for all z  Z, where |J|

is the absolute determinant of J. By application of Theorem A1 we have with probability at least

1 - ,

Rµw(f )  R^µw(f ) + Vµ(w,

) CnH, n3/8

.

and by applying Theorem A2, we have with probability at least 1 - ,

IPMH(p,, pµw,) - IPMH(p^,, p^µw,) 

182

log

4 C

1

+ 1

 mn

We let Y2 = 2 + µ2 and

D,H := B

182 log 4 C 

Combining these results, observing that (1 - )2  1 - 2, we obtain the desired result.

A.3 ASYMPTOTICS

Theorem 2 (Restated). Suppose H is a reproducing kernel Hilbert space given by a bounded kernel.

Suppose weak overlap holds in that E[(p(x, t)/pµ(x, t))2] < . Then,



min L(h, , w; )]  min R(f ) + O(1/ n + 1/ m) .

h,,w

f F

13

Under review as a conference paper at ICLR 2018

Proof. Let f  =   h  arg minfF R(f ) and let w(x, t) = p,((x), t)/pµ,((x), t).

Since R (f

m) i+nhO,(,1w/Ln(h+,

,w; ) 1/ m).

 L(h, , w; ), We will work term by

it suffices term:

to

show

that

L (h ,

,

w;

)

=

L(h, , w; )

=

1 n

n

wi h((xi), ti) +h

R(h) n

+

IPMG(q^, p^wk ) +w

i=1

C

AB

w 2. n D

For term D , letting wi = w(xi, ti), we have that by weak overlap

D2= 1×1 nn

n
(wi)2 = Op(1/n),

i=1

 so that D = Op(1/ n). For term A , under ignorability, each term in the sum in the first term

has we

heaxvpeecAtati=onRequ(falt)o+RO (pf(1/) annd).soF,osrotebrymwBeak,

overlap since h

and bounded second moments of loss, is fixed we have deterministically that

B = O(1/ n).

Finally, we address term C , which when expanded can be written as

1 sup ( h 1 m

m i=1

h((xi),

ti)

-

1 n

n i=1

wih((xi),

ti)).

Let xi , ti for i = 1, . . . , m and xi , ti for i = 1, . . . , n be new iid replicates of x1, t1, i.e., new ghost samples drawn from the target design. By Jensen's inequality,

E[

C

2]

=

E[

1 sup ( h 1 m

m i=1

h((xi), ti)

-

1 n

n i=1

wih((xi), ti))2]

=

E[

1 sup ( h 1 m

m
(h((xi), ti) -
i=1

E[h((xi

), ti

)])

-1 n

n
(wih((xi), ti) - E[h((xi ), ti )]))2]

i=1



E[

1 sup ( h 1 m

m
(h((xi), ti) -
i=1

h((xi

), ti

))

-1 n

n
(wih((xi), ti) - h((xi ), ti )))2]

i=1



2E[

1 sup ( h 1 m

m
(h((xi), ti)
i=1

-

h((xi

), ti

)))2]

+ 2E[

1 sup ( h 1 n

n
(wih((xi), ti) - h((xi
i=1

), ti

)))2]

Let that M.

fSioi(rmhe)ivla=errlyyh,h(E, [E([xiii()2h,]t)i])=-2EEh[[((iw(hi())X2] ]=Miq)0a+.nMd2Molerteovi(ehMr,)E=[<wiih2](beca(4xuEsie)[,Kotif()w-e(ahxk(i)o,vte(ix,rliap).(,xtLiie)),t.tiNi)]ofoter

i = 1, . . . , n be iid replicates of i (ghost sample) and let i be iid Rademacher random variables.

Because H is a Hilbert space, we have that sup h 1(A(h))2 = A 2 = A, A . Therefore, by

14

Under review as a conference paper at ICLR 2018

Jensen's inequality,

E[

1 sup ( h 1 n

n
(wih((xi), ti) - h((xi
i=1

), ti

)))2]

=

E[

1 sup ( h 1 n

n i=1

i(h))2]

=

E[

1 sup ( h 1 n

n
(i(h)
i=1

-

E[i (h)]))2 ]



E[

1 sup ( h 1 n

n
(i(h)
i=1

-

i (h)))2 ]

1n

=

E[

sup ( h 1 n

i=1

i(i(h) - i(h)))2]



4 n2 E[

n
sup (
h 1 i=1

ii(h))2]

4 = n2 E[

n

ii 2]

i=1

4n

= n2 E[

i j i, j ]

i,j=1

4n = n2 E[

i 2]

i=1

4 = n2

n

E[ i 2]

i=1

 4M n

An analogous argument can be made of i's, showing that E[ C 2] = O(1/n) and hence C = O(1/ n) by Markov's inequality.

B IMPLEMENTATION
We implemented all neural network models (IPM-WNN, RCFR) in TensorFlow as feed-forward fully-connected networks with ELU activations. All architectures have a representation with two hidden layers of 32 and 16 hidden units, and hypotheses (one for each outcome) of 1 layer of 16 hidden units. The networks were trained using stochastic gradient descent with the ADAM optimizer with a learning rate of 10-3. The batch size was 128. Representations were normalized by dividing by the norm. Weight functions were implemented as 2 hidden layers of 32 units each, as functions of the representation .  in the RBF kernel was set to 1.0. w was set to 0.1 and h to 10-4.

15

