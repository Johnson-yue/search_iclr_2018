Under review as a conference paper at ICLR 2018
LEARNING NON-LINEAR TRANSFORM WITH DISCRIM-
INATIVE AND MINIMUM INFORMATION LOSS PRIORS
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes learning a non-linear transform with two priors. The first is a discriminative prior defined using a measures on a support intersection and the second is a minimum information loss prior expressed as a constraint on the conditioning and the coherence. An approximation of the measures for the discriminative prior is addressed, connecting it to a similarity concentrations. Along quantifying the discriminative properties of the transform representation a sensitivity analysis of the similarity concentration w.r.t. the parameters of the nonlinear transform is given. Furthermore, a measure, related to the similarity concentration, reflecting the discriminative properties, named as discrimination power is introduced and its bounds are presented.
To support and validate the theoretical analysis a learning algorithm with the proposed prior is presented. The advantages and the potential of the proposed algorithm are evaluated by a computer simulation.
1 INTRODUCTION
Learning a transform that provides a sparse and discriminative representation is an active domain of research in many areas, including data processing, pattern recognition, image processing, language modeling, text analysis, gene separation, etc. A class of algorithms proposed by Kreutz-Delgado et al. (2003), Mairal et al. (2009), Bengio et al. (2012), Gangeh et al. (2015), Mairal et al. (2008), Jiang et al. (2011), Guo et al. (2012), Cai et al. (2014) and Liu et al. (2016) for learning discriminative sparse representation have been shown to perform well across various learning tasks. A subclass of them known as discriminative dictionary learning (DDL) Guo et al. (2012), Jiang et al. (2013), Cai et al. (2014), Shekhar et al. (2014), Xu et al. (2015), Liu et al. (2016), Bengio et al. (2012), Gangeh et al. (2015), con, Jiang et al. (2016), Vu & Monga (2016b) address the estimate of the dictionary in a supervised meaner such that the representation w.r.t. words (vectors) from the resulting dictionary (vector set) is discriminative. Most of the DDL methods synthesize the data sample xc,k  N as an approximation by a linear combination yc,k  M (refereed to as a sparse data representation) of a few words (vectors) yc,k 0 << M , from a dictionary (vector set) D  N×M , i.e., xc,k = Dyc,k + vc,k, vc,k  N , with vc,k denoting the approximation error. It is important to highlight that with the synthesis model approach the data reconstruction is addressed. The differences between the DDL methods Guo et al. (2012), Jiang et al. (2013), Cai et al. (2014), Shekhar et al. (2014), Gangeh et al. (2015), Xu et al. (2015), Liu et al. (2016), Bengio et al. (2012), Jiang et al. (2016), Vu & Monga (2016b) are determined by the prior defined on the sparse representation and the prior defined for the relations between the sparse representations for the data samples from same/different classes. The discrimination is enforced by approximating the prior with a structural constraints on the dictionary or imposing a discriminative term on the sparse representation. Additionally, in a number of works such as Mairal et al. (2008), Guo et al. (2012), Taalimi et al. (2015) is considered even a joint estimation/learning of dictionary, sparse representation and classifier by using iterative alternating minimization strategy. A comprehensive overview covering different approaches is given in Bengio et al. (2012), Cai et al. (2014) and Gangeh et al. (2015).
1

Under review as a conference paper at ICLR 2018
Figure 1: a) Data samples Xc, c  {1, 2, 3, 4} from four different classes, b) Given a k-th data sample xc,k from class c, the non-linear transform is represented as two step operation: linear mapping Axc,k (step 1) followed by an element wise thresholding function yc,k = H (Axc,k) (step 2). c) The transform data samples Yc, c  {1, 2, 3, 4}.
1.1 OPEN ISSUES
The general open issue for the DDL methods is the computational complexity w.r.t. the optimal dictionary/transform learning and the discriminative encoding, since the sparse representation in the synthesis model is a solution to an inverse problem. An additional open issue with most of the proposed approaches Guo et al. (2012), Jiang et al. (2013), Cai et al. (2014), Gangeh et al. (2015), Liu et al. (2016), Bengio et al. (2012), Jiang et al. (2016) and Vu & Monga (2016b) is that there is no formal notion to measure the discriminative properties. Therefore, there are no means that provide a quantitative evaluation about the quality of the representation, other then the performance of a classifier used on top of the representation. In therms of the specifics about the discriminative constraints, Yang et al. (2011b) proposed a synthesis model with a discriminative fidelity term and Fisher discriminant constraints, where the withinclass scatter and the between-class scatter of the representation is minimized and maximized, respectively. Vu & Monga (2016c) proposed extension considering a low rank constraints on the dictionary. The authors in Guo et al. (2013) used a synthesis model with a pairwise constraints on the sparse representation. They have modeled the pair-wise constraints using a 2 distance metric. The methods of Yang et al. (2011b), Vu & Monga (2016c) and Guo et al. (2013) take into account assumption on the metric by defining the scatter and the pair-wise relation. Therefore, they constrain the space of the representation, that essentially is determined by the dictionary. However, these works do not consider whether that the used metric is optimal for the sparse representation. The authors in Liu et al. (2016) have proposed a method that finds a dictionary under which the coefficients of a data sample from the same class c have a common sparse structure while the size of the overlapped signal support of different classes, denoted as c1 and c2 is minimized. Furthermore, assuming yc1,k1  M and yc2,k2  M are two sparse representations for data samples xc1,k1  N and xc2,k2  N , then the proposed similarity measure in Liu et al. (2016) is defined as expectation of yc1,k1 yc2,k2 0, where represents the Hadamard product. Note that two transform data samples yc1,k2 and yc2,k2 that have small support overlap yc1,k1 yc2,k2 0 = s, s << M , might not necessarily be similar or dissimilar, i.e., yc1,k1 = yc2,k2 and yc1,k1 = -yc2,k2 with yc1,k1 0 = yc2,k2 0 = s and s small.
1.2 MOTIVATION AND APPROACH
This paper addresses a non-linear transform model, where the data reconstruction is not modeled, and at least it is not targeted explicitly. Moreover, the model does not restrict the transform representation to be in the column space of the dictionary, allowing a rich class of representations to be modeled. In this line, we have to mention that a special case of the non-linear transform model is the sparsifying transform model, first introduced in Ravishankar & Bresler (2012). This model assumes that the data sample x is approximately sparsifiable under a linear transform A  M×N , i.e., Axc,k = yc,k + zc,k, zc,k  M , where yc,k is sparse yc,k 0 << M . Moreover, we note that
2

Under review as a conference paper at ICLR 2018

a) b) c)
Figure 2: a) Two transform representations yc1,k1 and yc1,k1, b) the resulting Hadamard products yc+,k yc+2,k2 and yc-1,k1 yc-2,k2 on the support intersection for the similarity contribution and c) the resulting Hadamard products yc+1,k1 yc-2,k2 and yc-1,k1 yc+2,k2 on the support intersection for the dissimilarity contribution between yc1, k1 and yc2,k2.

the sparsifiying transform model represents a generalization of the analysis model Rubinstein et al. (2010), Rubinstein et al. (2013), Ravishankar & Bresler (2014) and Rubinstein & Elad (2014).

Together with the non-linear transform model, a novel prior is proposed, which is defined by a para-

metric measures on the support intersection. The first motivation behind using a measure defined

on the support intersection is that it allows more freedom in imposing a regularization on the dis-

criminative properties for the transform representation without taking into account any additional

assumptions, contrary to Yang et al. (2011b), Vu & Monga (2016c), Guo et al. (2013) and Liu et al.

(2016). Second, by a simple approximation on the parametric measure the focus of the regulariza-

tion can be directly put on the contributing components for the similarity, i.e., consider the measure (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k21 between two transform representations yc1,k1 and yc2,k2. This measure captures the main contributing components for similarity on the support intersection, whereas (yc+1,k1)T yc-2,k2 + (yc-1,k1)T yc+2,k2 captures the main contribution for dissimilarity between yc1,k1 and yc2,k2. Third, the expectation of this similarity measure across a the data set captures the

concentration of similarity for that set. Therefore, it provides the possibility to define a formal notion to quantify the discriminative properties. Forth, the measure (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 is not ambiguous w.r.t. a notion for similarity/dissimilarity between two sparse representations yc1,k1

and yc2,k2. This is because the support intersections for the positive and the negative components

yc+1,k1 yc+2,k2 1 and yc-1,k1 yc-2,k2 1, respectively, are considered separately. In addition a

measure for the strength on the support intersection, defined as yc1,k1

yc2,k2

2 2

is

also

taken

into

account. Note that the expectation of the measure yc1,k1

yc2,k2

2 2

across

a

the

data

set

captures

the expected strength on the support intersection for that set.

A schematic diagram of the transform and the main idea behind the proposed concept are shown in Figure 1. In Figure 2 a), b), c) is given an illustrative example for the support intersection between the positive and negative component of two vectors yc1,k1 and yc2,k2 in the transform domain.

We propose a learning algorithm for the non-linear transform model with the discriminative prior. In a direction of quantifying the discriminative properties we give a sensitivity analysis w.r.t. the parameters of the transform. Along the same line, we present an information preservation rela-

1yc1,k1 = yc+1,k1 - yc-1,k1 where yc+1,k1 = max(yc1,k1, 0) and yc-1,k1 = max(-yc1,k1, 0). Note that (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 = yc+1,k1 yc+2,k2 1 + yc-1,k1 yc-2,k2 1 captures the only contribution for the similarity (whereas yc+1,k1 yc-2,k2 1 + yc-1,k1 yc+2,k2 1 captures the only contribution for the dissimilarity) between the vectors yc1,k1 and yc2,k2.
Moreover, it can be shown that for certain models under some conditions ycT1,k1yc2,k2 = yc1,k1 yc2,k2 1,
if and only if the dissimilarity contribution on the support intersection between the vectors yc1,k1 and yc2,k2 is 0. That is ycT1,k1yc2,k2 = (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 - (yc-1,k1)T yc+2,k2 - (yc+1,k1)T yc-2,k2  (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2, now if the dissimilarity contribution -(yc-1,k1)T yc+2,k2 - (yc+1,k1)T yc-2,k2 is zero then yc1,k1 yc2,k2 1 = (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 = ycT1,k1yc2,k2.

3

Under review as a conference paper at ICLR 2018

x(n), A(n, m)
xc,k, yc,k X, Xc, X\{kc}, Y, A

.p Pc={A  M×N , c  T P (.)

D DP1P1((xxcc11,,kk11 ;;xxc2c,2k,2k)2 )

A
DP (X)

D1P (X)

1

A


I

DBN (X)
1
o,IAto

|Ao

=B

M}

scalar vector matrix
Hadamard product p-norm, set of parameters non-linear parametric function with parameters P similarity measure between T P (xc1,k1) and T P (xc1,k2)

first

order

derivative

of

DP 1

(xc1,k1

;

xc2,k2)

w.r.t.

A

concentration of similarity

change of concentration w.r.t. change of A

change of concentration w.r.t. change of Ao evaluated at B discrimination power in original and transform domain

Table 1: a) Most important notations used thought the paper

tions for the change of a similarity concentrations w.r.t. change of the parameters of the transform, connecting all together the proposed prior, the model and the used non-linear transform. Furthermore, w.r.t. the similarity concentrations a notion defined as discrimination power, reflecting the discriminate properties of the representation for a data set is introduced. Based on the sensitivity analysis we present lower and upper bound on the discrimination power in the transform domain. On the practical side, the advantages and the potential of the proposed algorithm are demonstrated by a numerical experiments using the Extended YALE B Georghiades et al. (2001), AR Mart´inez & Benavente (1998), Norb LeCun et al. (2004), Coil-20 Nene et al. (1996), Clatech101 LeCun et al. (2008), UKB Niste´r & Stewe´nius (2006) and MNIST Lecun & Cortes data sets.
1.3 NOTATIONS
A scalar variable is denoted using the usual symbols, i.e., x, a vector is denoted by a bold, low caps symbols, i.e., x, a matrix by bold, upper cap symbol, i.e., A. A single element from a vector (or matrix) is denoted as x(n) (or A(m, n)). A set is denoted by a calligraphic symbol, i.e, S. The p-norm is denoted as . p and the nuclear norm as . . The symbol represents the Hadamard product. Throughout the paper it is assumed that a set of data samples X = [X1, X2, ..., XC ]  N×L, L = CK from C classes is given and that every class c  C = {1, 2, 3, ..., C} has K samples, Xc = [xc,1, xc,2, ..., xc,K ]  N×K , xc,k  N , c  C, k  K = {1, 2, ..., K}. We denote a the transform data as Y = [Y1, Y2, ..., YC ]  M×L, where Yc = [yc,1, yc,2, ..., yc,K ]  M×K and yc,k  M . We denote X\{kc} = X1, X2, ..Xc,\k, ...XC  N×(L-1) as the matrix that has all the columns of X, except the column xc,k  N , where Xc,\k = [xc,1, xc,2, ..., xc,k-1, xc,k+1, ..., xc,K ]  N×(K-1) is a matrix that has all the columns of block Xc, except the column xc,k, c  C and k  K. We let N = {1, 2, ..., N } and M = {1, 2, ..., M }. We denote DM as the set of all M × M diagonal matrices with non-negative diagonal elements.

2 LEARNING NON-LINEAR TRANSFORM WITH DISCRIMINATIVE AND
MINIMUM INFORMATION LOSS PRIORS

Assume that training data X = [X1, X2, ..., XC ]  N×CK are given, consisting of C classes. Per every class c  C there are K training samples, i.e., Xc = [xc,1, xc,2, ..., xc,K ]. Furthermore, we assume that per class c  C there exist an unknown non-linear functions defined by set of parameters:

Pc = {A  M×N , c  M }, c  C,

(1)

that separates the data samples from different classes in the transform domain. The linear map A is
shared for all of them, but, the parameters c per different class c from the corresponding non-linear transforms Pc are different. Additionally, given any xc,k from class c where k  K it is assumed that

4

Under review as a conference paper at ICLR 2018

the non-linear transform is expressible in two steps, consisting of a linear mapping (step 1) followed by an element-wise non-linearity (step 2), as follows:

xc,k --A- Axc,k -H--c-,(.) yc,k,

step 1

step 2

(2)

where Hc,(.) : M  M is a non-linear thresholding function with parameters c. It is important to mention that the thresholding is done with a different thresholding parameter per different
transform dimension, i.e., c(m), m  M.

The goal of learning a non-linear transform (2) is to estimate only one model defined by a set of

parameters

P = {A 

M×N ,  =  1 

M +

},

(3)

that is common for all data samples that come form all classes by taking into account a set of priors.

That is, we try to find as accurate as possible approximation (3) to the models represented by a set of parameters Pc = {A  M×N , c  M } with as small as possible loss in the discriminative

properties of the representation in the transform domain.

2.1 THE NON-LINEAR TRANSFORM MODEL WITH A DISCRIMINATIVE PRIOR

The compact description of the non-linear transform (2) by a non-linear model is expressed as

follows:

Axc,k = yc,k + zc,k where yc,k = T Pc (xc,k),

(4)

where the function T Pc (.) : N  M is a parametric non-linear function that gives yc,k, by using the set of parameters Pc. The term zc,k = Axc,k - yc,k is the non-linear transform error vector

that represents the deviation of the transform data Axc,k from the targeted transform representation yc,k = T Pc (xc,k) in the transform domain.

Assuming Gaussian distributed error vector, the prior on zc,k is modeled as p(xc,k|yc,k) 

exp(-

Axc,k -yc,k 0

2 ),

additionally,

assuming

that the non-linear function T Pc (xc,k) gives sparse

yc,k, then we have the improper prior on yc,k, defined as p(yc,k)  exp(-

yc,k 1

1 ).

This

paper

models

the

discriminative

prior

as

p(c,

|yc,k )



exp(-

D(yc,k ;c ) 2

)

by

using

an

paramet-

ric measure D(yc,k; c) with parameters c. Assuming that the measure D(yc,k; c) is defined on

the support intersection between yc,k and c we propose the following formulation:

D(yc,k; c) = yc+,k

c+ 1 + yc-,k

c- 1 + yc,k

c

2 2

(5)

where yc+,k c+ 1 + yc-,k c- 1 measures the similarity contribution on the support intersection using the positive and the negative components yc+,k, c+ and yc-,k, c- of yc,k and c, respec-

tively and yc,k

c

2 2

measures

the

strength

of

the

support

intersection

between

yc,k

and

c.

We highlight that the true p(c), c, are not known and the only prior about D(yc,k; c) and p(c) is that they should be informative enough w.r.t. the discriminative properties of the transform representation yc,k. Furthermore, instead of estimating them explicitly, an approximation to D(yc,k; c) is considered, based only on the concentrations of the similarity on the support intersection and the
expected strength of the support intersection for the transform data in the transform domain.

The approximation We propose an approximation that captures two expectations. The first one is

the expected similarity on the support intersection for the positive and negative component between

yc,k and the set of transform representations Y\c that come from all classes c1 different from c, i.e., c = c1. The second is the expected strength on the support intersection between yc,k and the set of transform representations Y\c that come from all classes c1 different from c, i.e., c = c1. The approximation is defined as:

D(yc,k ;

c)



DP 1

(X)

+

SP 2

(X),

DP (X) = 1 c1 k1 c1=c

yc+,k yc+1,k1 1 + yc-,k yc-1,k1 1 ,

(6)

SP (X) = 2 c1 k1 c1=c

yc,k

yc1,k1

2 2

5

Under review as a conference paper at ICLR 2018

where the m-th element yc+,k(m) of yc+,k is defined as yc+,k(m) = max(yc,k(m), 0) and similarly,

yc-,k(m) = max(-yc,k(m), 0), m  M. We also define the expected similarity using the posi-

tive and negative components of yc,k for the set of transform representations Yc that come from

the

same

classes

c

as

DP 1

,c

(X)

=

k1 yc+,k

yc+,k1 1 + k1 yc-,k

yc-,k1 1. The terms

2 ((C -1)K )(C K )

c1 c1=c

k1 yc-1,k1

and

2 ((C -1)K )(C K )

c1 c1=c

k1 yc+1,k1 might also be seen finite

sample estimates variable c2. The

otefrmthe((pCo-si1t)ivK2e)(aCnKd )negact2ic=v2ec1comkp2oynec2n,tk2 c+

and c-, respectively, for yc2,k2 might also be seen

the unknown as finite sam-

ple estimates of the Hadamard square c

c

for

the

unknown

variable

c3.

If

the

measure

DP (X) 1

is not used then the approximation (6) is most similar to the one proposed in Liu et al. (2016).

Note that the Fisher discriminate constraint Yang et al. (2011b), the pairwise constraint Guo et al. (2013) and the support intersection constraint Liu et al. (2016) are all approximations of a discriminative prior. However, all of them are with specific assumptions on the distribution of the data representation in the transform domain. The advantage of using (6) is that the aproximation is without any prior on the probability distributions p(c) and without any explicit assumption about the distance metric/measure, or the space/manifold in the transform domain.

2.2 THE LEARNING ALGORITHM

The summary of the used priors is as follows:

p(xc,k|yc,k)  exp(-

Axc,k - yc,k 0

2)

p(yc,k)  exp(-

yc,k 1

1)

p(c,

|yc,k )



exp(-

D(yc,k ; 2

c)

).

(7)

Additionally, we have a prior on A to regulirize the information loss in order to avoid trivially unwanted matrices A, i.e., matrices that have repeated or zero rows. The corresponding prior is defined as:

p(A)  exp

-

1 3

A

2 F

+

1 4

AAT

-I

2 F

-

1 5

log | det AT A|

,

(8)

where the

A F penalty helps regularize the scale ambiguity, the log | det (AT A)| and

A

2 F

are

functions of the singular values of A and together help regularize the conditioning of A. Assuming

that the expected coherence µ2 (A) between the rows am of A (i.e., AT = [a1, a2, ..., aM ]) is

defined

as

µ2

(A)

=

2 M (M -1)

m1=m2 |am1 amT 2 |2, m1, m2  {1, 2, .., M }. Then

AAT

-I

2 F

measures the expected coherence µ2 (A) and the 2 norm for the rows of A.

Note that the joint probability can be expressed as:

p(xc,k, yc,k, c, A) = p(xc,k, yc,k, c) =

p(xc,k, yc,k, c, |A)p(A) = p(A|xc,k, yc,k, c)p(xc,k, yc,k, c, )
p(xc,k|yc,k)p(c, |yc,k)p(yc,k),

(9)

since p(xc,k|yc,k, c, ) = p(xc,k|yc,k).

2Note that since DP (X) = 1

c1 c1=c

k1 yc+,k

yc+1,k1 1 +

c1 c1=c

k1 yc-,k

yc-1,k1 1 =
2 ((C-1)K)(CK)
3Note that

c1 c1=c

k1 yc+1,k1

yc+,k 1+

c1 c1=c
since

k1 yc-1,k1

and c+



2 ((C-1)K)(CK)

SP (X) 2

=

c2 c2=c1

k2

c1 c1=c
c1 c1=c
yc1,k1

k1 yc-1,k1

k1 yc+1,k1.

yc2,k2

2 2

yc-,k 1, c- = (yc1,k1

yc1,k1 )T

c2 c2=c1

k2 yc2,k2

yc2,k2 , c

c



2 ((C-1)K)(CK)

c2 c2=c1

k2 yc2,k2 yc2,k2.



6

Under review as a conference paper at ICLR 2018

Algorithm 1 Non-linear transform learning algorithm

Input X, 0, 1, 2, 3, 4 A  inicialize
repeat
DISCRIMINATIVE ENCODING closed form solution per data sample

Y  AX

repeat

for c  C do

d-c 

c1 c1=c

sc  c1

c1=c

end for

k1 yc-1,k1, dc+  k1 yc1,k1 yc1,k1

c1 c1=c

k1 yc+1,k1 and

for c  C and k  K do g  sign(max(Axc,k, 0)) d+c + sign(max(-Axc,k, 0)) dc- yc,k  sign(AX) max (|Axc,k| - 0g - 11, 0) (1 + 20sc) yc,k  yc,k/ yc,k 2

end for

until convergence

TRANSFORM UPDATE -close closed form solution

UX 2X UTX  XXT + 2I and UUX XY UX XY VUTX XY  UTX XYT

minA(n)

3 X 4

A4 (n)

+

X2 (n)-23 X2 (n)

A2 (n)

-

 (n) X (n)

A

(n)

-

24

log

A (n) X (n)

where (n)  T (n, n), T  UUX XY UX XY UTUX XY , n  N A VUX XY UTUX XY A-X1UXT

until convergence

Output A, Y

Given the avlable tranining data set X  N×CK maximizing p(xc,k, yc,k, c, A) over Y and A is same as minimizing the following problem:

1 min Y,A 2

AX - Y

2 F

+

0D(yc,k; c) + 1 yc,k 1+

c,k

2 2

A

2 F

+

3 2

AAT - I

2 F

-

4

log

|

det

AT

A|,

(10)

where {0, 1, 3, 4} are inversely proportional to the scaling parameters {1, 2, 3, 4, 5}. Note that the solution to (10) is not equivalent to the maximum a priory (MAP) solution, which
is difficult to compute, as it involves integrating over the vectors yc,k. In therms of optimization the problem is not convex in the variables (Y, A). The solution is obtained by iteratively, marginally
maximizing the probability p(xc,k, yc,k, c, A) over Y and A which is equivalent to maximizing the conditional densities p(yc,k|xc,k, c, A) and p(A|yc,k, xc,k), respectively. Meaning that at one iterating step one of the variables Y or A is fixed and w.r.t. the other the problem (10) is minimized.
The following text describes the iterating steps that consist of linear map estimation (maximizing p(A|yc,k, xc,k)) and discriminative encoding (maximizing p(yc,k|xc,k, c, A)).

Linear map estimation: Given the available data samples X and the corresponding transform representations Y the linear map A estimation problem reduces to:

1 min
A2

AX - Y

2 F

+

2 2

A

2 F

+

3 2

AAT - I

2 F

-

4

log | det AT A|,

(11)

and we use the -close closed form solution estimated as follows:

Proposition 1 ( -close solution ): Given Y  M×L, X  N×L and M  N , 2  0,3  0, 4  0, let the eigen value decomposition UX X2 UTX of XXT + 2I and the singular value decomposition UUXXY UXXY VUTXXY of UTX XYT exist, then if and only if X (n) > 0, n  N = {1, 2, 3, ..., N }, (11) has -close approximative solution as:

A^ = VUX XY UUTX XY AX-1UTX ,

(12)

7

Under review as a conference paper at ICLR 2018

where A is diagonal matrix, A(n, n) = A(n)  0, and A(n) are solutions to

min
A (n)

3 X 4

A4 (n)

+

X2 (n) - 23 X2 (n)

A2 (n)

-

(n) X (n)

A(n)

-

24

log

A(n) , X (n)

(13)

with (n) = T (n, n), T = UUXXY UXXY UUTXXY , n  N (we give the proof in Appendix A).

Discriminative encoding: Given the available data samples X and the current estimate of the transform A, define for simplicity Q = AX  M×L. The discriminative representation estimation problem is formulated as:

min
Y

Q-Y

2 F

+

0D(yc,k; c) + 1 yc,k .

c,k

(14)

Note that (10) and consequently (14) is a well defined problem only if the model variables c , or the
probability distributions p(c) are know in advance. Nevertheless, we show that the approximation (6) leads to an efficient solution of (14). Assuming that Y\c is given, then for any sample k  K from any class c  C by using the approximation (6), problem (14) reduces to a constrained projection:

min
yc,k

Axc,k - yc,k

2 2

+

0 D(yc,k ;

c)

+

1

yc,k

1,

(15)

and has a closed form solution as:

yc,k = (max (Axc,k - 0g + 11, 0) - max (-Axc,k - 0g + 11, 0)) (1 + 20sc) , (16)

where denotes Hadamard (element-wise) division, g = sign(max(Axc,k, 0)) dc+ +

sign(max(-Axc,k, 0)) dc-, d-c =

c1 c1=c

k1 yc-1,k1 and d+c =

c1 c1=c

k1 yc+1,k1, sc =

c1 c1=c

k1 yc1,k1

yc1,k1 (the proof is given in Appendix B).

We note that at convergence (which we do not prove here) we can only claim that a joint local maximum in (Y, A) of p(xc,k, yc,k, c, A) has been reached, even if, as in this case, each optimization step achieves the (marginal) -close and global optimal solution, respectively.

The exact steps of the proposed non-linear transform learning are described by Algorithm 1.

3 SENSITIVITY ANALYSIS AND INTERPRETATIONS

The similarity concentration measure provides possibility to measure the discriminative properties, their deviation, increase (or decrease) and the corresponding relations between different non-linear transform models across one domain or different domains, thereby quantifying their quality w.r.t. the discriminative properties.

3.1 SENSITIVITY ANALYSIS W.R.T. THE SIMILARITY CONCENTRATION MEASURES

To measure the ability for an increase in discriminative properties by a non-linear transform4 we first have to define a notion for the discriminative properties on a data set under different non-linear transform models. Therefore, first we introduce the "special" base models and then analyze the properties of the similarity concentration measures under the change in model parameter and the relation between the base model and the proposed non-linear transform model (3).

Any data set X in the original domain might have a transform model with parameters BN = {Ao 

N×N ,  = 0 

N +

},

if

Ao

=

I



D+N

we

refer

to

it

as

a

base

original

model.

Similarly

as

in

the original domain, any data set Y in the transform domain might have a transform model with

parameters BM = {At 

M×M ,  = 0 

M +

},

if

At

=

I



D+M

we

refer

to

it

as

a

base

transform model. Any base model, defined ether in the original domain BN or in the transform

domain BM , has domain equal to the co-domain, since xc,k = T BN (xc,k) and yc,k = T BM (yc,k)

holds trivially, for the respective sets of parameters BN = {Ao = I  D+N ,  = 0 

N +

}

and

BM = {At = I  D+M ,  = 0 

M +

}.

This

is

illustrated

with

a

diagram

shown

in

Figure

3.

4instead of using the term non-liner transform model as defined by (2) or (4) for short we just use the term model

8

Under review as a conference paper at ICLR 2018

original domain
X
T BN

-T-P

transform domain
Y
T BM

X transform domain

Y transform domain

Figure 3: The original and the transform domains under a non-linear transforms with a set of parameters BN , P and BM , note that for T BN and T BM the original and the transform domains are the
same.

A base original model provides a possibility to compare it with any other non-linear transform model

with parameters P = {A 

M×N ,  

M +

}.

Additionally, note that for BM

=

{At

=

I



DM ,  = 0 

M}

we

have

that

DB1M,c(Y)

=

DP 1

,c

(X)

and

that

DBM (Y) 1

=

DP (X). 1

It

implies

that the similarity concentrations can be analyzed as a function in the original domain under model

P or in the transform domain under model BM . The main relations considering the preservation

of change in the similarity concentration between two models, defined not necessary in the same

domain are stated by Proposition 1.

Lemma 1: The non-linear transform model (4) totally preserves the information in the change for
the similarity concentration for a data set X w.r.t. a small change in the parameters of the models BN , BM and P if o  = 0 and t  = 0 as

(R1) : A



DB1N,c(X) Ao

|Ao

=I

+

DBN (X)

1
Ao

|Ao=I

=



DP 1

,c

(X)

A

+

DP (X) 1 A

+ o

(R2) : A



DP 1

,c

(X)

+

DP (X) 1

A A

T

=

DB1M,c(Y) At

|At=I

+

DBM (Y)

1
At

|At=I + t

(17)

(R3)

:

AoT

=



DB1M,c(Z) At

|At

=I

+

DBM (Z)

1
At

|At=I

+ tT ,

where o =

c,c1

k,k1

zc1,k1xcT,k + zc,kxcT1,k1,

+DP 1

,c

(X)

A

DP (X)
1
A

=

k=k1

c,c1 k,k1 k=k1

yc1,k1xTc,k + yc,kxcT1,k1, t =

c,c1

k,k1 zc1,k1ycT,k + zc,kycT1,k1,
k=k1

| +DB1M,c(Y)

At

At =I

|DBM (Y)

1
At

At =I

=

c,c1

k,k1 yc1,k1ycT,k + yc,kycT1,k1,
k=k1

| + |DB1M,c(Z)
At

At =I

DBM (Z)

1
At

At =I

=

c,c1

k,k1

zc1,k1zTc,k + zc,kzcT1,k1,

| DB1N,c (X)
Ao

Ao =I

=

k=k1

c k,k1 xc,kxcT,k1 + xc,k1xcT,k and

|DBN (X)

1
Ao

Ao =I

=

c,c1,c=c1

k,k1 xc,kxTc1,k1 + xc1,k1xcT,k. The proof is given in Appendix C.

The terms zc,k represent the non-linear transform error vectors that appear in the model Axc,k = yc,k + zc,k as a result of applying an element-wise non-liner operation H to Axc,k, i.e., yc,k = H (Axc,k). As an example in the sparsifing transform model zc,k is the "loss of information", that is the information about the values of the elements in Axc,k that are discarded. The terms o and t correlate the errors zc,k with the original data xc1,k1 and transform data yc1,k1, respectively. Note that if there is no loss of information (in the earlier example it means that there is no thresholding
and just a simple linear transform model is used) then o = 0 and t = 0. Moreover, o and t bear important information about the discriminative properties in the transform domain.

The

terms

DP
1

,c

(X)

A

and

DP (X)
1
A

represent

the

change

of

the

similarity

concentrations

under

in-

finitesimally

small

change

of

the parameter

A from

the

model

P.

The

terms

| DB1N,c (X)
Ao

Ao =I

and

|DBN (X)

1
Ao

Ao =I

have

dual

interpretation.

Assuming

metric

Ao

=

I,

then

the

first

one

is

considered

as a change of the similarity concentrations under infinitesimally small change of the space metric,

9

Under review as a conference paper at ICLR 2018

or equivalently under small metric perturbation. Conversely, assuming the data samples are dis-

tributed under a Gaussian distribution with parameters identity covariance matrix and zero mean,

i.e.

xc,k



N (µ

=

0, 

=

I),

then

| DB1N,c (X)

Ao

Ao =I

and

|DBN (X) 1 Ao Ao=I

represent

the

change

of

the

similarity concentrations under small change in the assumption away from a Gaussian distribution.

Equation (R1) relates the base transform for the original domain BN with any arbitrary transform defined in the original domain P. The relation (R2) is a result about the preservation of change in the similarity concentration between two models BM and P defined on two different domains.
Whereas (R3) gives the preservation of change in the similarity concentration between the error in
the transform domain.

The next result highlights the relation between: the linear projection (by the linear map A that appears in the model P) of the change in the similarity concentration under the model BN in the original domain and the change of the similarity concentration under the model BM in the transform
domain.

This

relation

exists

independently

for

| DB1N,c (X)
Ao

Ao =I

and



DBN (X)
1
Ao

|Ao

=I

,

nevertheless,

we

will

define the summarized and the independent versions.

Therefore, first we define

J 

1 (X) Ao

|Ao

=I

=

| +DB1N,c(X)
Ao

Ao =I

|DBN (X)

1
Ao

Ao =I

and



J 

1 (Y) At

|At

=I

=

| DB1N,c (Y)
At

At =I

+

|DBN (Y)

1
At

At =I

and

rewrite

(R1)

as

A

J (X) Ao

|Ao

=I

-

o

=

DP
1

,c

(X)

A

DP (X)

+

1
A

, then replace



DP
1

,c

(X)

A

+

DP (X)
1
A

in (R2) by the same term in (R1), use (R3), reorder and we have the following result.

Lemma 2: For fixed  any non-linear transform model (4) preservers the information in the change of similarity concentrations w.r.t. a small change in A by

(R4)

:A



J 

1 (X) Ao

|Ao

=I

AT

=



J 

1 (V) At

|At

=I

=



J 

1 (Y) At

|At

=I

+ c

+ ,

(R5)

:A



D 1,c(X) Ao

|Ao

=I

AT

=



D 1,c(V) At

|At

=I

=

D 1,c(Y) At

|At=I

+

c,

(18)

(R6)

:A



D 

1 (X) Ao

|Ao

=I

AT

=



D 

1 (V) At

|At

=I

=

D 1 (Y) At

|At=I

+

,

where V

=

AX, c + 

=

J 1 (Z) At

|At

=I

+



J

1 (Y;Z) At

|At

=I

,

J 

1 (Z) At

|At

=I

=

| +DB1N,c(Z)
At

At =I

|DBN (Z)

1
At

At =I

and



J

1 (Y;Z) At

|At

=I

= t + tT

.

Note

that

|nJ 1 (X)
 n Ao

Ao =I

=

4 |n-1J 1 (X)
 n-1 Ao

Ao =I

and

|nJ 1 (Y)
 n At

At =I

=

4 |n-1J 1 (Y)
 n-1 At

At =I

therefore,



J 

1 (X) Ao

|Ao

=I

=

|1 2J 1 (X)
4 2Ao

Ao =I

and

J 1 (Y) At

|At

=I

=

1 4

|2J 1 (Y)
 2 At

At =I

might

be

interpreted

as

Fisher information matrices evaluated at Ao = I  DN and At = I  DM . The expressions by

(R4) actually relate the metric in the original domain under the model BN to the induced metric in

the transform domain for the model BM , with induction done by the model P with parameter set

{A 

M×N ,  

M +

}.

Moreover,

the

model

P

might

describe

a

transform

domain

with

a

non-

smooth manifold. Since the manifolds of the original and the transform domain under the models

BN and BM are smooth the analysis of their relations reveals insights about the relation between the

manifolds

under

the

models

BN

and

P.

The

terms



J 

1 (Z) At

|At

=I

and



J

1 (Y;Z) At

|At

=I

carry

out

the

information about the breaks and the discontinuities of the regularity and smoothness in the manifold

induced by the model P. Furthermore, if



J 

1 (Z) At

|At

=I

 = 0 and



J

1 (Y;Z) At

|At

=I

 = 0, then

(R4) in information geometry Amari (2013) is seen as change of coordinates on a manifold, where

the intrinsic properties of curvature remain unchanged under different parametrization.

3.2 A MEASURE FOR THE DISCRIMINATIVE PROPERTIES AND ITS BOUNDS

This paper proposes a notion for the discriminative properties of a data set under a non-linear trans-

form named as discrimination power, based on a measure for the relations between the concentra-

tions

DP 1

,c

(X)

and

DP 1

(X).

10

Under review as a conference paper at ICLR 2018

X



log

DB1N,c (X) DBN (X)+

1



Io

-T-P

log

DP
1

,c

(X)

DP (X)+

1

Y



=

log

DB1M,c (Y) DBM (Y)+

1



It

Figure 4: The relation for the definition of the discrimination power in the original and the transform domain under the base models BN and BM .

Cn(A) µ(A) te[min] D1 2.21 0.03 5.10 D2 1.80 0.02 5.45 D3 2.12 0.02 6.55 D4 0.08 0.02 8.92 D5 6.01 0.01 12.8 D6 33.1 0.02 30.1 D7 1.60 0.02 5.00

IO

I RT

I ST 

INT

0.03 0.18 0.68 1.98

0.02 0.10 1.30 1.79

0.00 0.01 0.71 1.61

0.08 0.61 0.89 1.89

0.01 0.16 1.02 2.12

0.06 0.53 1.36 3.36

0.13 0.63 1.06 1.96

Table 2: The conditioning number Cn(A) and the expected mutual coherence µ(A) for the learned transform A , The execution time te[min] in minutes of the proposed algorithm for 28 iterations at the transform domain dimensionality M = 19000.

Proposition 1: The discrimination power for any dataset X  M×CK under any transform with parameter set P is defined as:

It

=

log(DP 1

,c

(X))

-

log(DP (X) 1

+

)

=

log(DB1M,c(Y))

-

log(DBM 1

(Y)

+

).

(19)

Remark 1: The advantage of this measure is that it logarithmically signifies the difference between

DP 1

,c

(X)

and

DP (X)5 1

.

The definition about the discrimination power of the data set X, but, now under a model with a parameter set BN is equivalent to the one defined for It, we denote it as Io. An illustration is given
by a diagram shown in Figure 4.

The bound on the discrimination power is given by the following result.

Theorem 1: The discrimination power for any data set X  N×CK under any transform with parameter set P is bounded as:

log(min(AT

A))

+

log(

T

r{



DB1N,c (X) Ao

|Ao

=I

}

DB1M,c(AX) +

)

I t



log(DB1M,c(AX)) - log(

)

The proof is given in Appendix D.

(20)

At first the resulting bounds might look counterintuitive since the loss of information seems to increase the discrimination power. This fact is true, however, up to a certain limit. Therefore, it is important to distinguish two main conclusions.

First, for any model with a set of parameters P for which there is no loss of information c +  = 0, the only condition for the increase in the discrimination power is DBN (X)  DBM (AX) and
11
DB1N,c(X)  DB1M,c(AX).

5Assuming we have a discriminative prior p(c|yc,k) = exp(- c+ yc+,k 1 - c- yc-,k 1) and

p(c|yc,k) = exp(- c+ yc+,k 1 - c- yc-,k 1), where c  M is an unknown parameter. Then the

difference

DP 1

,c

(X)

-

DP 1

(X)

between

DP 1

,c

(X)

and

DP 1

(X)

actually

represents

a

finite

sample

approxima-

tion

to

a

discrimination

density,

since

it

approximates

the

density

log(

p(c|yc,k ) p(c|yc,k )

),

i.e.,

DP 1

,c

(X)

-

DP 1

(X)



-

log(

p(c,yc,k ) p(c,yc,k )

)

=

c +

yc+,k 1 + c-

yc-,k 1 - ( c+

yc+,k 1 + c-

yc-,k 1).

11

Under review as a conference paper at ICLR 2018

log(C1/C2) C2

C1/C2 C1

0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
0 0

5

MNIST 250 NORB
CalTech101 UKB 200 E-YALE-B AR Coil20 150
100

10 15 20 Number of iterations

MNIST NORB CalTech101 UKB E-YALE-B AR Coil20
25 30

0.08 0.07 0.06 0.05

MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

0.04

0.03

0.02

0.01

0 0 5 10 15 20 25 30 Number of iterations
2.5 MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

2

1.5

1

50 0.5

0 0

5 10 15 20 25 30

0 0

5 10 15 20 25 30

Iteration number

Number of iterations

Figure

5:

The

evolution

of

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM 1

(Y),

their

ratio C1/C2 and the discrimination power log(C1/C2) = It during the learning of the non-linear

transform with transform dimension M = 19000.

log(C1\C2)

12

MNIST

4 MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

NORB

10

CalTech101

2

UKB

E-YALE-B

0

8 AR

Coil20

-2

6

-4

log(C1/C2)

4 -6

2 -8

0 -10

0 10 20 30  40 50 60 70

0 10 20 30  40 50 60 70

Figure

6:

The

ratio

C 1/C 2

of

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM (Y) 1

and the discrimination power log(C1/C2) = It for randomly chosen subsets from all of the used

databases under a non-linear transform with transform dimension M = 19000 and varying thresh-

olding parameter  = 1.

Second,

in

the

rest

of

the

cases

for

which

DBN 1

(X)



DBM (Y) 1

and

DB1N,c(X)



DB1M,c(Y)

holds

true it will be possible to increase the discrimination power. Moreover, there is a trade-off between

the increase in discrimination power as a result of loss of information c +   > 0.

4 NUMERICAL EXPERIMENTS
The numerical experiments are summarized in two different parts. In the first series of the experiments the properties of the learned map A for the proposed algorithm are investigated. We evaluate the computational efficiency, as run time te[min], the conditioning number Cn(A) 6, the expected
6the conditioning number is computed as the ratio between the largest and the smallest singular value of the matrix A

12

Under review as a conference paper at ICLR 2018

C2 log(C1/C2) µ(A)

Cn (A)

40 5 ×10 -3

MNIST

MNIST

35 NORB

NORB

CalTech101

4

CalTech101

30 UKB

UKB

25

E-YALE-B AR

3

E-YALE-B AR

Coil20

Coil20

20 2

15 1
10 0
5

0 -1 0 5 10 15 20 0 5 10 15 20

Transform dimension (M)

Transform dimension (M)

Figure 7: The conditioning number Cn(A) and the expected mutual coherence µ(A) for the learned linear transform A at different dimensionality M  {100, 1150, 2200, 3250, 4300, 5350, 6400, 7450, 8500, 9550, 10600, 11650, 12700, 13750,
14800, 15850, 16900, 17950, 19000}.

C1/C2 C1

0.12 0.1
0.08 0.06

MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

0.04

0.02

0 0 5 10 15 20 Transform dimention (M)

MNIST 250 NORB
CalTech101 UKB 200 E-YALE-B AR 150 Coil20

0.04 0.035
0.03 0.025

MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

0.02

0.015

0.01

0.005

0 0 5 10 15 20 Transform dimension (M)
2.5 MNIST NORB CalTech101 UKB E-YALE-B AR Coil20

2

1.5

100 1

50 0.5

00 0 5 10 15 20 0 5 10 15 20

Transform dimension (M)

Transform dimension (M)

Figure 8:

The similarity concentrations C1

=

DB1M,c(Y) and C2

=

DBM (Y), 1

their ratio C1/C2 and the discrimination power log(C1/C2) = It on a subset

of the transform data using learned non-linear transform at different dimensionality

M  {100, 1150, 2200, 3250, 4300, 5350, 6400, 7450, 8500, 9550, 10600, 11650, 12700, 13750,

14800, 15850, 16900, 17950, 19000}.

mutual coherence µ(A) and the discrimination power across several databases for a learned nonlinear transforms having different dimensionality. Additionally, a comparison between the resulting discrimination power in the original domain, after transform by a random matrix (having Gaussian random samples as entries and transform dimension of M = 19000) and after a learned non-linear transform having transform dimension M = 19000 without and with support dissimilarity prior, denoted as I0, IRT , IST  and INT , respectively, is estimated and presented .
The second part evaluates a comparison of the discrimination power between the proposed algorithm and different supervised dictionary learning methods (SDL) Ramirez et al. (2010), Yang et al. (2011a), Vu et al. (2015) and Vu & Monga (2016a). This comparison considers a setup where the used data sets are spited into a training and test set, moreover, the learning is performed on the
13

Under review as a conference paper at ICLR 2018

Accuracy

I DLS I IF DDL ICOP AR I LRS DL INT

D1
0.71 0.87 0.57 0.42 0.98 a)

D7 0.67 0.63 0.54 0.40 0.81

DLSI F DDL COP AR LRSDL NT

D1 Acc. [%] 96.5 97.5 98.3 98.7 99.7
b)

DLSI F DDL COP AR LRSDL NT

D7 Acc. [%] 98.74 96.31 96.41 - 99.02
c)

Table 3: a) The discrimination power for the methods DLSIRamirez et al. (2010), F DDL Yang et al. (2011a), COP AR Vu et al. (2015) and LRSDL Vu & Monga (2016a) and the proposed non-liner transform N T , b) and c) The recognition results on the Extended Yale B and MNIST database.

100 E-YALE-B 98
96

100 MNIST 98

Accuracy

94 96

92 94
90

88 92 0.2 0.4 0.6 0.8 1 1.2 1.2 1.3 1.4 1.5

Discriminative power

Discriminative power

M 100 500 1.5K 4K

M 1K 4K 6K 12K

Acc. [%] 89.1 94.3 97.4 99.7 It 0.21 0.73 0.93 1.23

Acc. [%] 92.49 94.24 96.01 99.02 It 1.18 1.35 1.42 1.55

a) b)

Figure 9: a) and b) The recognition results and the discrimination power on the Extended Yale B

and MNIST databases, respectively, using a non-linear transform with different dimensionality M

and linear SVM classifier on top of the transform representation.

3 ×10 -4

E-YALE-B

×10 -5 3

MNIST

22

11

Expected loss per dimension

Expected loss per dimension

0 0.2 0.4 0.6 0.8

1

Discriminative power

M 100 500 1.5K

E[

zc,k M
I

2
t

]10-4

3 0.3 0.21 0.73

0.14 0.93

a)

1.2
4K 0.08 1.23

0 1.2 1.3 1.4

Discriminative power

M 1K 4K

E[

zc,k M
I

2
t

]10-5

3.1 1.18

0.82 1.35

b)

1.5
6K 0.6 1.42

12K
0.28 1.55

Figure 10: a) and b) The expected loss per transform dimension E[

zc,k M

2 ] = E[

Axc,k -yc,k

2/M ]

and the discrimination power on the Extended Yale B and MNIST databases, respectively, on the

transform representation Y, obtained by using a non-linear transform T P at different dimensionality

M.

training set and the evaluation is performed on the test set. In the same series of experiments the recognition accuracy for two data sets is also computed and compared.
14

Under review as a conference paper at ICLR 2018

C1\C2

5 10 TD-100

TD-500

4

E-YALE-B TD-1500

8

TD-4000

36

MNIST

TD-1000 TD-4000 TD-6000 TD-12000

C1\C2

24

12

0 0 20 40 60 

0 0 20  40 60

2

TD-100

5

TD-500

E-YALE-B TD-1500

0

TD-4000

0

MNIST

TD-1000 TD-4000 TD-6000 TD-12000

-2 -5

log(C1\C2)

log(C1\C2)

-4 -10

-6 -15

0

20  40

60

0

20  40

60

Figure 11: The ratio of the similarity concentrations similarity concentrations C1 = DB1M,c(Y) and C2 = DBM (Y) and the discrimination power log(C1/C2) = It for the Extended Yale B and
1
MNIST databases under non-linear transforms having different transform dimension M and varying
thresholding parameter  = 1.

Data sets and algorithms set up The used data sets are Extended YALE B (D1)Georghiades et al. (2001), AR (D2) Mart´inez & Benavente (1998), Norb (D3) LeCun et al. (2004), Coil-20 (D4) Nene et al. (1996), Clatech101 (D5) LeCun et al. (2008), UKB (D6) Niste´r & Stewe´nius (2006) and MNIST (D7) Lecun & Cortes. All the images from the respective datasets were downscaled to resolutions 21 × 21, 32 × 28, 24 × 24, 20 × 25, 21 × 21, 20 × 25, 28 × 28, respectively, and are normalized to unit variance. Considering the used implementation of the algorithm note that the singular value decomposition for large matrix has high computational complexity. However, A - A^ , where A^ is estimated as a solution in the transform update step, can be considered as an proximal operator Parikh & Boyd (2014) for the gradient of the objective (11). Additionally, instead of using all of the available data samples X a subset of them might be used. Therefore, one simple on-line variant for the update of A w.r.t. a subset of the available training set has the form At+1 = At-(At-A^ t) with  predefined step size. In the numerical experiments we use the on-line variant of the algorithm (the convergence analysis for this variant of the algorithm is left for future work). the parameters 0 and 1 are set such that the resulting non-linear transform representation has very small number of non-zeros w.r.t. the transform dimension, in the experiments here this number is set to be 15. The rest of the parameters are set as {2, 3, 4} = {1000000, 1000000, 1000000}. The algorithm is initialized with a random matrix having i.i.d. Gaussian (zero mean, variance one) entries and is terminated after the 28th iteration. The results are obtained as average of 3 runs. An implementation presented in Vu & Monga (2016a) was used to learn the dictionaries and estimate the sparse codes for the respective supervised dictionary learning methods (SDL) Ramirez et al. (2010), Yang et al. (2011a), Vu et al. (2015) and Vu & Monga (2016a).
Linear map properties, the change in the similarity concentrations and the discrimination power The conditioning number and the expected coherence for the learned transforms are shown on Table 2. The learned transforms for all the databases have good conditioning numbers and low expected coherence. The running time te, measured in minutes and the number of used dimensions, denoted as M are also shown in Table 2. The learned transforms for all the data sets have relatively low execution time, regardless of a transform dimensions 19000. The discrimination power is sig-
15

Under review as a conference paper at ICLR 2018

nificantly increased in the transform domain INT compared to the one in the original domain IO and is higher than IST  and IRT .

The

evolution

of

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM (Y), 1

their

ratio

C1/C2 and the discrimination power log(C1/C2) = It for a subsets of the used databases after

applying a non-linear transform with transform dimension M = 19000 is shown on Figure 5. It

is

important

to

note

that

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM (Y) 1

are

decreasing, meaning that there is a loss in information. However, how this loss effects the resulting

similarity concentration is crucial for the discriminative properties. As shown on Figure 5 the slope

of decrease for C2 is stronger then the slope of decrease for C1, therefore, the discrimination power

increases per iteration. For the Coil-20 (D4) database there is a fluctuation. This is explained by the

fact that during learning we used a small number of data samples from the same database and that

in the data there is high variability.

The conditioning number and the expected coherence for the learned transforms for all the databases at different transform dimensions are shown on Figure 7. We see that the value of both the conditioning number and the coherence is reducing and converging to a common values, respectively, implying that the conditioning and the coherence constraints are effective.

The

ratio

C 1/C 2

between

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM 1

(Y)

and

the discrimination power log(C1/C2) = It on a subsets of the used databases after applying a

non-linear transform with transform dimension M = 19000 and varying the thresholding parameter

 = 1 is shown on Figure 6. We used 70 different values for the parameter , sampled uni-

formly from the interval 0, (maxc,k maxm |aTmxc,k|) . The result were obtained using a non-linear transforms learned with one value for the parameter  for all the databases. Since all the databases

have different variabilities and the amount of available data is different, this result suggest that per

different database there should be different optimal value of the parameter .

The similarity concentrations C1

=

DB1M,c(Y) and C2

=

DBM (Y), their ra1

tio C1/C2 and the discrimination power log(C1/C2) = It for a subsets of the

used databases after applying a non-linear transform having transform dimensions

M  {100, 1150, 2200, 3250, 4300, 5350, 6400, 7450, 8500, 9550, 10600, 11650, 12700, 13750,

14800, 15850, 16900, 17950, 19000} is shown on Figure 8. We can see a similar behavior as

previous,

that

is,

the

similarity

concentrations

C1

=

DB1M,c(Y)

and

C2

=

DBM 1

(Y)

are

decreasing,

but, the slope of decrease for C2 = DBM (Y) is stronger. Therefore, the discrimination power

1

increases as the transform dimension increases.

SNTL vs SDL discrimination power and recognition performance The proposed method is compared with DLSI Ramirez et al. (2010), F DDL Yang et al. (2011a), COP AR Vu et al. (2015) and LRSDL Vu & Monga (2016a). Half of the data samples from the data set Extended-YALEB, sampled at random are used for learning and the remaining other half are used for evaluation. Considering the MNIST database the training set is used for learning and the test set is used for evaluation. We compute both the discrimination power and the recognition accuracy on the test sets.

The dictionary size (transform dimension M ) is set to be equal to 150, 75, 1515, 3825, 570, 150, 300 for the corresponding databases, respectively, in all of the comparing algorithms. The discrimination power of the comparing methods is denoted as IDLSI , IF DDL, ICOP AR and ILRSDL, respectively. The recognition results for the methods DLSI, F DDL, COP AR and LRSDL on the data sets Extended YALE B and MNIST were not computed here, rather than that we use the best reported result form the respective papers Ramirez et al. (2010), Yang et al. (2011a), Vu et al. (2015) and Vu & Monga (2016a). Considering the proposed algorithm the transform was learned for the transform dimensions M equal to [100, 500, 1500, 4000] and [1000, 4000, 6000, 12000], respectively, for the used data sets. After the transform was learned the transform data samples were computed for the respective training and test sets. Then, the transform training data samples were used as features to learn a linear SVM classifier in one-against-all regime. Finally, the evaluation was performed on the respective test sets.

The results are shown on Table 3 a), b) and c). The discrimination power of the proposed nonlinear transform is higher that the discrimination power of the comparing methods. Whereas the

16

Under review as a conference paper at ICLR 2018

recognition accuracy is higher for high dimensionality of the proposed method and outperforms the SDL methods at dimensionality 4000 and 12000.

The

results

about

the

accuracy

of

recognition

and

the

expected

loss

measured

as

E[

zc,k M

2]

=

E[ Axc,k - yc,k 2/M ] per the discrimination power at different transform dimension are shown on

Figure 9 and Figure 10, respectively. It is interesting to highlight that as the discrimination power at

different transform dimension increases it also increases the accuracy of recognition, moreover, the

results on these two data bases show that this increase is approximately linear. On the other hand the

expected loss per transform dimension decreases as the discrimination power at different transform

dimension increases.

The results about the ratio C1/C2 between the similarity concentrations C1 = DB1M,c(Y) and C2 = DBM (Y) and the discrimination power log(C1/C2) = It on Extended-Yale-B
1
and MNIST databases after applying a non-linear transform with transform dimensions M =

{100, 500, 1500, 4000} and M = {1000, 4000, 6000, 12000}, respectively, and varying the thresh-

olding parameter  = 1 are shown on Figure 11. We again used 70 different values for the

parameter , sampled uniformly from the interval 0, (maxc,k maxm |amT xc,k|) , that were different for the used databases. The result were obtained using a non-linear transforms learned

with optimally choose values (by using cross-validation) of the parameter  for the two different

databases. As expected we can see the extreme points of the ratio between the similarity concen-

trations

C1

=

DB1M,c(Y)

and

C2

=

DBM (Y) 1

and

the

discrimination

power

log(C 1/C 2)

=

It

is

around the optimal values of the parameter  = 1.

5 CONCLUSION
This paper presented an analysis on the discriminative properties for non-linear transform models expressible as two step transform, linear mapping (step 1) followed by an element wise non-linearity (step 2). A novel discriminative prior was proposed and the properties around the model and the proposed prior were investigated. A low complexity learning algorithm was presented with the proposed priors.
The preliminary results w.r.t. the introduced measures and the recognition accuracy on the used databases showed promising performance. We showed that it is possible to increase the discrimination power with information loss. Moreover, we highlight that when expanding to high dimensional space with non-linear transform how the loss of information reflects the similarity concentrations is crucial for the discriminative properties.
A study on the recognition capabilities for other databases are our next future extensions. An analysis for the synthesis model and transform based auto-encoder towards generalization and fair comparison between different transforms and encoding methods is other future direction. In the line of the encoding one might consider to minimize directly the difference or the ratio between the introduced similarity concentrations or the discrimination power.
In a different direction the extensions covering the sufficient conditions for increase in discrimination power in the transform domain, together with an analysis for a deep architecture where per single layer we have one non-linear transform are left for our future work.

17

Under review as a conference paper at ICLR 2018

A.

THE GLOBAL OPTIMAL SOLUTION Given the current estimate of Y the estimate of the transform A is solution to the following problem:

{A^ } = argmin

AX - Y

2 F

+

2

AT A

F + 3

AAT - I

F - 4(log | det AT A|).

(21)

A

Theorem 1 (global optimal solution): Given Y  M×L and X  N×L, if and only if the joint

decomposition:

XXT = UX X2 UXT XYT = UX XY VXT Y ,

(22)

exists, where UX  N×N is orthonormal, VXY  M×N is per columns orthonormal and X , XY  N×N are diagonal matrices with positive diagonal elements, then (21) has a global

minimum as:

A^ = VXY AX-1UTX ,

(23)

and A(n, n) = A(n) > 0, A(n), n are solutions to

3 X4 (i)

A4 (i)

+

X2 (i) - 23 X2 (i)

A2 (i) -

XY (i) X (i)

A

(i)

-

24 log

A(i) X (i)

=

0.

(24)

Proof of Theorem 1:

Consider the equivalent trace form of (21) we have:

minT r{(AX - Y)T (AX - Y)} + 2T r{AT A} + 3T r{(AAT - I)T (AAT - I)}- A (25) 4(log | det AT A|).

Note that 2  0, XXT + 2I is symmetric positive definite matrix with all eigenvalues nonnegative, therefore it decomposes as:

UX X UXT UX X UTX = XXT + 2I.

(26)

Let Define

A =BD, D = UX X-1UTX . g1 = BDXYT , g2 = BBT , g3 = BDDT B BDDT B T

(27)

M

g4 = g5 = (BD)(BD)T

Pm,M (BD)(BD)T Pm,M

m=1

(28)

M
g6 = Pm,M (BD)(BD)T Pm,M , g7 = log | det BDDT BT |.
m=1
where Pm,M  DM , Pm,M (m1, m1) = 1 if m1 = m and Pm,M (m1, m1) = 0 if m1 = m. Then (21) equivalently is:

min -T r{g1} + T r{g2} + 3(T r{g3} - T r{g4}) + 3 (T r{g5} - T r{g6}) - 4g7.
B

(29)

Assume that B decomposes as:

UBBVBT ,

(30)

where B is diagonal with positive diagonal elements, UB is column orthogonal and VB is orthogonal square matrix. Moreover, let the following decomposition of XYT exists

XYT = UX XY VXT Y ,

(31)

substitute as

UB =VXY , VB = UX ,

(32)

18

Under review as a conference paper at ICLR 2018

then T r{g1} =T r{UBBVBUX -X1UXT XYT } = T r{BX-1XY }.

The term

T r{g2} =T r{(UBBVB)(UBBVB)T } = T r{B2 }.

Define  = BX-2B then

T r{g3} =T r{ UTBUB UBT UB T } = T r{} = T r{B4 X -4}.

Note that

T r{g4} =

M

T r{BDDT BT

Pm,M BDDT BT Pm,M } = T r{

m=1

M
Pm,M UX UXT Pm,M
m=1

2
}.

Therefore, 3T r{g4} = 3T r{g5}. The term T r{g6} = T r{ B-X1 2}. The term g7 is

(33) (34) (35)
(36)

log | det AT A| = log | det UX Y-1B2 -Y 1UX | = | det -Y 1B2 Y-1|.

(37)

Finally (21) is reduced to:

min
B

N n=1

3 X4 (n)

B4 (n)

+

X2 (n) - 23 X2 (i)

B2 (n)

-

XY (n) X (n)

B

(n)

-

24

log

B(n) . X (n)

(38)

Equalling to zero the first order derivative w.r.t B(n) of the objective in (38) and multiplying by B(n) gives:

4

3 X4 (i)

B4

(i)

+

2

X2 (i) - 23 X2 (i)

B2 (i)

-

XY (i) X (i)

B

(i)

-

24

=

0.

(39)

A closed form solution to (39) exists and depends on the discriminant  of the quartic polynomial.

Moreover,

since

4 4
X 4

is

positive

a

global

minimum

exists

as

A = VXY B X-1UXT .

(40)

THE -CLOSE CLOSED FORM APPROXIMATION

Proof of Proposition 1:

Consider the equivalent trace form of (21). Note that 2  0, XXT + 2I is symmetric positive definite matrix with all eigenvalues non-negative, therefore it decomposes as

UX X UXT UX X UXT = XXT + 2I.

(41)

Let

D = UX X-1UTX , and define A = BD, UUX XY UX XY VUTX XY = UX XYT .

(42)

Assume that B decomposes as UBBVBT , where B is diagonal with positive diagonal elements, UB is column orthogonal and VB is orthogonal square matrix. Let

UB = (UUX XY VUTX XY )T , VB = UX ,

(43)

then

T r{AXYT } = T r{BUX X-1UTX XYT } = T r{VUX XY UUTX XY B X-1UUX XY UX XY VUTX XY }

(44)

Moreover, since UB = (UUXXY VUTXXY )T , VB = UX , using Mirsky (1959) and Neumann (1937), note that

min
B

max T
UB ,VB

r{UBBVBT UX X-1UX XYT

}



min
B

T

r{ B -X1 }

(45)

19

Under review as a conference paper at ICLR 2018

where  is diagonal matrix having diagonal elements (n) = T (n, n), n  N and T = UUX XY UX XY UUTX XY . The term T r{BBT } = T r{(UBBVB)(UBBVB)T } = T r{B2 } and the term T r{(AAT )2} = T r{(UTBUB)(UBT UB)T } = T r{4BX -4}. The term T r{AAT } = T r{ BX-1 2}. The term log | det AT A| is

log | det AT A| = log | det UX -Y 1B2 -Y 1UX | = | det Y-1B2 -Y 1|.

(46)

Finally, using the bound (45), the approximation of (21) is reduced to

min
B

N i=1

3 X4 (n)

B4

(n)

+

X2 (n) - 23 X2 (n)

B2 (n)

-

(n) X (n)

B

(n)

-

24

log

B(n) . X (n)

(47)

Equaling to zero the first order derivative w.r.t B(n) of the objective for (47) and multiplying by B(n) gives:

4

3 X4 (n)

B4

(n)

+

2

X2 (n) - 23 X2 (n)

B2 (n)

-

(n) X (n)

B

(n)

-

24

=

0.

(48)

A closed form solution to (47) exists and it depends on the discriminant  of the quartic poly-

nomial.

Moreover,

since

4 4
X 4

is positive a global minimum to (47) exists.

Therefore, having

UB = (UUXXY VUTXXY )T and VB = UX with the solution for B by (47) gives the -close

closed form approximative solution to problem (21) as

A = (UUX XY VUTX XY )T B X-1UTX ,

(49)

where by (45) it implies that the -close closed form solution is a lower bound on the solution to

(21).

APPENDIX B.

Let yc,k = yc+,k + yc-,k, yc+,k 

M +

and

yc-,k



M -

.

Note

that

the

term

DP (X) 1

is

defined

as:

DP (X) = 1

yc+1,k1

c1,c2 k1,k2

c1=c2

yc+2,k2 1 +

yc-1,k1

c1,c2 k1,k2

c1=c2

yc-2,k2 1 =

|yc+1,k1|T |yc+2,k2| +

|yc-1,k1|T |yc-2,k2| =

c1,c2 k1,k2

c1,c2 k1,k2

c1=c2

c1=c2

(50)

|yc+1,k1|T

|yc+2,k2| + |yc-1,k1|T

|yc-2,k2| =

c1,c2 k1,k2

c1,c2 k1,k2

c1=c2

c1=c2

|yc+1,k1|T g+ + |yc-1,k1|T g-,

where g+ = c1,c2 k1,k2 |yc+2,k2|, g- = c1,c2 k1,k2 |yc-2,k2| and we abuse notation by

c1=c2

c1=c2

denoting |yc1,k1| as the vector whose elements are the absolute values of the elements in yc1,k1.

Note that the term SP (X) is defined as: 2

SP (X) = 2 c2 k2 c2=c1

yc1,k1

yc2,k2

2 2

=

(yc1,k1
c2 k2 c2=c1

yc2,k2)T (yc1,k1

yc2,k2) =

(yc1,k1
c2 k2 c2=c1

yc1,k1)T (yc2,k2 

yc2,k2) = 

(51)

(yc1,k1

yc1,k1)T

 

yc2,k2

c2 k2 c2=c1

yc2,k2 = (yc1,k1

yc1,k1)T sc

20

Under review as a conference paper at ICLR 2018

where sc =

c2 c2=c1

k2 yc2,k2

Consider the related problem

yc2,k2 .

minyc1,k1

yc1,k1 - qc1,k1

2 2

+

0((yc+1,k1)T g+ + (yc-1,k1)T g- + (yc1,k1

yc1,k1)T sc) + 1 yc1,k1 1,

by taking the first order derivative w.r.t. yc1,k1 we have that

(yc1,k1 - qc1,k1) + 1sign(yc1,k1)+ 0(sign(yc+1,k1) g+ + sign(yc-1,k1)

g- + yc1,k1

sc) = 0,

take the sign magnitude decomposition of yc1,k1 = sign(yc1,k1) |yc1,k1| then we have

sign(yc1,k1) |yc1,k1| (1 + 20sc) - sign(qc1,k1) |qc1,k1|+ 1sign(yc1,k1) + 0(sign(yc+1,k1) g+ + sign(yc-1,k1) g-) = 0.

(52) (53) (54)

Let the sign of yc1,k1, i.e. sign(yc1,k1) be equal to the sign of sign(qc1,k1), and Hadamard multiply from the left side by sign(qc1,k1) then we have

(1 + 20sc) sign(qc1,k1)

|yc1,k1| - |qc1,k1| + 11 + 0(sign(qc1,k1) sign(q-c1,k1) g-) = 0,

sign(qc+1,k1)

g++

(55)

note that sign(qc1,k1) sign(qc+1,k1) = sign(q+c1,k1) and that sign(qc1,k1) sign(-qc-1,k1), therefore we have

sign(qc-1,k1) =

|yc1,k1| = |qc1,k1| - 11 - 0(sign(q+c1,k1)

g+ + sign(-q-c1,k1)

g-)

(1 + 20sc).

(56)

Define Gc1,k1 as diagonal matrix with positive diagonal elements Gc1,k1(m, m) = z(m), m  M

where:

z = sign(max(qc1,k1, 0)) g+ + sign(max(-qc1,k1, 0)) g-,

(57)

and note that Gc1,k11 = z then

|yc1,k1| = |qc1,k1| - 11 - 0(Gc1,k11),

(58)

since the magnitude might be only positive |yc1,k1| = max(|qc1,k1| - 11 - 0(Gc1,k11), 0). Therefore, the closed form solution to (52) is:

yc1,k1 =sign(qc1,k1) max(|qc1,k1| - 11 - 0Gc1,k11, 0) (1 + 20sc).

(59)

which completes the proof

APPENDIX C.

Note that for the model Pt we have that y =T (Ax) = max(Ax -  , 0) - max(-Ax -  , 0), q =T (Ax) = max(Ag -  , 0) - max(-Ag -  , 0),

since

sign(a) max(|a| - b, 0) = max(a - b, 0) - max(-a - b, 0),

The first order derivative of the divergence DPt (x; g) w.r.t. the parameter A is: 1

DPt (x; g) 1= A
(max(Ax -  , 0)T max(Ag -  , 0)) (max(-Ax -  , 0)T max(-Ag -  , 0)) +
A A

(60) (61)
(62)

21

Under review as a conference paper at ICLR 2018

we assume that the threshold parameter  is chosen such that the vector |Ax| -  (or for any other q, the vector |Aq| -  ) has least one non-zero element, then

(max(Ax -  , 0)T max(Ag -  , 0)) = max(Ag -  , 0)xT + max(Ax -  , 0)gT , A
and (max(-Ax -  , 0)T max(-Ag -  , 0)) = A - max(-Ag -  , 0)xT - max(-Ax -  , 0)gT ,

(63) (64)

combining (63) and (64) we have that

DPt (x; g) 1= A
max(Ag -  , 0)xT + max(Ax -  , 0)gT -
(max(-Ag +  , 0)xT + max(-Ax -  , 0)gT ) =
qxT + ygT

where

y(m) = sign(amT x) max(|amT x| -  (m), 0) q(m) = sign(amT g) max(|aTmg| -  (m), 0),

m  M. Similarity, note that for the model P0o = {Ao,  = 0} we have that

xo =T P0o (Aox) = sign(Aox) max(|Aox| - 0, 0) = max(Aox - 0, 0) - max(-Aox - 0, 0),
go =T P0o (Aog) = sign(Aog) max(|Aog| - g, 0) = max(Aog - 0, 0) - max(-Aog - 0, 0).

The

first

order

derivative

of

the

divergence

DP0o (x; 1

g)

w.r.t

Ao

is:

DP0o (x; g) 1 Ao

=

xogT

+ goxT ,

note that at Ao = I, P0t = BN and we have that

DBN (x; g)

1
Ao

|Ao=I

=

xgT

+

gxT

Also for the model P0t = {At,  = 0} we have that

yt =T P0t (Aty) = sign(Aty) max(|Aty| - 0, 0) = max(Aty - 0, 0) - max(-Aty - 0, 0),
qt =T P0t (Atq) = sign(Atq) max(|Atq| - q, 0) = max(Atq - 0, 0) - max(-Atq - 0, 0).

(65) (66)
(67) (68) (69) (70)

22

Under review as a conference paper at ICLR 2018

The

first

order

derivative

of

the

divergence

DP0t (y; 1

q)

w.r.t

At

is:

DP0t (y; q) 1 At

=

ytqT

+ qtyT ,

note that at At = I, P0t = BM and we have that

DBM (y; q)

1
At

|Ao=I

=

yqT

+

qyT

Consider the following

where

Ag = q + z1/xT Ax = y + z2/gT



AxgT = ygT + z1gT AgxT = qxT + z2xT

A(xgT + gxT ) = ygT + qxT + z1gT + z2xT

z1 = Ax - sign(Ax) max(|Ax| -  1, 0) z2 = Ag - sign(Ag) max(|Ag| -  1, 0)

A closer look at (73) reveals us that

where

DBN (x; g)

A

1
Ao

|Ao=I

=

DP (x; g) 1 A

+ oz1,z2

oz1,z2 = z1gT + z2xT

By similar construction applied to the rest of the pairs of data samples we have the result:

A



DB1N,c(X) Ao

|Ao

=I

+

DBN (X)

1
Ao

|Ao=I

=



DP 1

,c

(X)

A

+

DP (X) 1 A

+

o

Note that where

Ax = y + z1/qT Ag = q + z2/yT



AxqT = yqT + z1qT AgyT = qyT + z2yT

A(xyT + qgT ) = yqT + qyT + z1qT + z2yT ,

z1 = Ax - sign(Ax) max(|Ax| -  1, 0) z2 = Ag - sign(Ag) max(|Ag| -  1, 0)

A closer look at (78) reveals us

A

DP (x; g) 1 A

T

=

DBM (y; q)

1
At

|At=I

+ tz1,z2.

where

tz1,z2 = z1qT + z2yT

By similar construction applied to the rest of the pairs of data samples we have the result:

(71) (72)
(73) (74)
(75) (76) (77) (78) (79) (80) (81)

A

DP1t,c(X)

+

DPt (X) 1

T
=

A A



DB1M,c(Y At

)

|At

=I

+

DBM (Y)

1
At

|At=I

+ t

23

(82)

Under review as a conference paper at ICLR 2018

Note that where

Ax = y + z1/zT2 Ag = q + z2/z1T



Axz2T = yz2T + z1zT2 Agz1T = qzT1 + z2zT1

A(xzT2 + qzT1 ) = yz2T + qz1T + z1zT2 + z2zT1 ,

z1 = Ax - sign(Ax) max(|Ax| -  1, 0) z2 = Ag - sign(Ag) max(|Ag| -  1, 0)

A closer look at (83) reveals us

where

A(oz1,z2 )T

=



DBM 1

(z1

;

At

z2)

|At=I

+

(tz1,z2)T

.

tz1,z2 = z1qT + z2yT

By similar construction applied to the rest of the pairs of data samples we have the result:

AoT =



DB1M,c(Z) At

|At

=I

+

DBM (Z)

1
At

|At=I

+ tT

(83) (84) (85) (86)
(87)

APPENDIX D.

The result in (18) decomposes on the contributing components for similarity and the contribut-

ing components for dissimilarity,

i.e.,



J

1 (AX) At

|At

=I

|s

-



J

1 (AX) At

|At

=I

|d

=

J 1 (Y) At

|At

=I

|s

-



J 

1 (Y) At

|At

=I

|d

+

c,d

-

c,d

+

s

-

d.

Moreover,

w.r.t.

the

similarity

concentrations

we

have

the

following

splitting

T

r{

 DB1M,c (AX) At

|At

=I

|s

}

=

DB1M,c(Y)

+

T r{c,s}

=

DP 1

,c

(X)

+

T r{c,s}

and

T

r{



DBM (AX)
1
At

|At

=I

|s

}

=

DBM 1

(Y)

+

T

r{s}

=

DP 1

,c

(X)

+

T

r{s}.

Therefore, we have the

following bounds

a

:

T

r{A

DB1N,c(X) Ao

|Ao=IAT

}

DP 1

,c

(X)



T

r{

DB1M,c(AX) At

|At=I|s}

=

DB1M,c(AX)

b

:

T

r{A



DBN (X) 1 Ao

|Ao

=I

AT

}

DP (X) 1



DBM (AX)

T r{

1
At

|At=I|s}

=

DBM 1

(AX)

(88)

Note

that

c

:

min

(AT

A)T

r{

 DB1N,c (X) Ao

|Ao

=I

}



T

r{A

 DB1N,c (X) Ao

|Ao

=I

AT

}

where

min(AT A)

is

the

minimum

singular

value

to

the

matrix

AT A.

Taking

the

logarithm

of

the

ratio

DP
1

,c

(X)

DP (X)+

and

1

using the bounds a, b and c we arrive at the desired result

REFERENCES

Shun-ichi Amari. Information geometry and its applications: Survey. In GSI, volume 8085 of Lecture Notes in Computer Science, pp. 3. Springer, 2013.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Unsupervised feature learning and deep learning: A review and new perspectives. CoRR, abs/1206.5538, 2012. URL http://arxiv. org/abs/1206.5538.
Sijia Cai, Wangmeng Zuo, Lei Zhang, Xiangchu Feng, and Ping Wang. Support vector guided dictionary learning. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV, pp. 624­639, 2014.
24

Under review as a conference paper at ICLR 2018
Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, and Mohamed S. Kamel. Supervised dictionary learning and sparse representation-a review. CoRR, abs/1502.05928, 2015. URL http://arxiv.org/abs/1502.05928.
A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23:643­660, 2001.
Huimin Guo, Zhuolin Jiang, and Larry S. Davis. Discriminative dictionary learning with pairwise constraints. In Computer Vision - ACCV 2012 - 11th Asian Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part I, pp. 328­342, 2012.
Huimin Guo, Zhuolin Jiang, and Larry S. Davis. Discriminative Dictionary Learning with Pairwise Constraints, pp. 328­342. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3642-37331-2. doi: 10.1007/978-3-642-37331-2 25. URL https://doi.org/10.1007/ 978-3-642-37331-2_25.
Rui Jiang, Hong Qiao, and Bo Zhang. Efficient fisher discrimination dictionary learning. Signal Process., 128(C):28­39, November 2016. ISSN 0165-1684. doi: 10.1016/j.sigpro.2016.03.013. URL http://dx.doi.org/10.1016/j.sigpro.2016.03.013.
Zhuolin Jiang, Zhe Lin, and L. S. Davis. Learning a discriminative dictionary for sparse coding via label consistent k-svd. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR '11, pp. 1697­1704, Washington, DC, USA, 2011. IEEE Computer Society. ISBN 978-1-4577-0394-2. doi: 10.1109/CVPR.2011.5995354. URL http://dx. doi.org/10.1109/CVPR.2011.5995354.
Zhuolin Jiang, Zhe Lin, and Larry S. Davis. Label consistent K-SVD: learning a discriminative dictionary for recognition. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2651­2664, 2013. doi: 10.1109/TPAMI.2013.88. URL http://dx.doi.org/10.1109/TPAMI.2013.88.
Kenneth Kreutz-Delgado, Joseph F. Murray, Bhaskar D. Rao, Kjersti Engan, Te-Won Lee, and Terrence J. Sejnowski. Dictionary learning algorithms for sparse representation. Neural Comput., 15(2):349­396, February 2003. ISSN 0899-7667. doi: 10.1162/089976603762552951. URL http://dx.doi.org/10.1162/089976603762552951.
Yann Lecun and Corinna Cortes. The MNIST database of handwritten digits. URL http:// yann.lecun.com/exdb/mnist/.
Yann LeCun, Fu Jie Huang, and Le´on Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR'04, pp. 97­104, Washington, DC, USA, 2004. IEEE Computer Society.
Yann LeCun, David G. Lowe, Jitendra Malik, Jim Mutch, Pietro Perona, and Tomaso Poggio. Object Recognition, Computer Vision, and the Caltech 101: A Response to Pinto et al. Technical report, March 2008.
Yang Liu, Wei Chen, Qingchao Chen, and Ian J. Wassell. Support discrimination dictionary learning for image classification. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II, pp. 375­390, 2016.
Julien Mairal, Francis R. Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman. Supervised dictionary learning. In Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pp. 1033­1040, 2008.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pp. 689­696, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1. doi: 10.1145/ 1553374.1553463. URL http://doi.acm.org/10.1145/1553374.1553463.
25

Under review as a conference paper at ICLR 2018
A. Mart´inez and R. Benavente. The ar face database. Technical Report 24, Computer Vision Center, Jun 1998. URL "http://www.cat.uab.cat/Public/Publications/1998/ MaB1998".
L. Mirsky. On the trace of matrix products. 20(3-6):171­174, 1959. ISSN 2167-3888.
Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20. Technical report, 1996.
J. Von Neumann. Some matrix-inequalities and metrization of matrix-space. Tomskii Univ. Rev., 1: 286­300, 1937.
D. Niste´r and H. Stewe´nius. Scalable recognition with a vocabulary tree. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 2161­2168, June 2006. oral presentation.
Neal Parikh and Stephen Boyd. Proximal algorithms. Found. Trends Optim., 1(3):127­239, January 2014. ISSN 2167-3888.
I. Ramirez, P. Sprechmann, and G. Sapiro. Classification and clustering via dictionary learning with structured incoherence and shared features. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3501­3508, June 2010. doi: 10.1109/CVPR.2010. 5539964.
Saiprasad Ravishankar and Yoram Bresler. Learning sparsifying transforms for image processing. In 19th IEEE International Conference on Image Processing, ICIP 2012, Lake Buena Vista, Orlando, FL, USA, September 30 - October 3, 2012, pp. 681­684. IEEE, 2012. ISBN 978-1-46732534-9. doi: 10.1109/ICIP.2012.6466951. URL http://dx.doi.org/10.1109/ICIP. 2012.6466951.
Saiprasad Ravishankar and Yoram Bresler. Doubly sparse transform learning with convergence guarantees. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, pp. 5262­5266. IEEE, 2014. doi: 10.1109/ICASSP.2014. 6854607. URL http://dx.doi.org/10.1109/ICASSP.2014.6854607.
Ron Rubinstein and Michael Elad. Dictionary learning for analysis-synthesis thresholding. IEEE Trans. Signal Processing, 62(22):5962­5972, 2014.
Ron Rubinstein, Alfred M. Bruckstein, and Michael Elad. Dictionaries for sparse representation modeling. Proceedings of the IEEE, 98(6):1045­1057, 2010.
Ron Rubinstein, Tomer Peleg, and Michael Elad. Analysis K-SVD: A dictionary-learning algorithm for the analysis sparse model. IEEE Trans. Signal Processing, 61(3):661­677, 2013.
Sumit Shekhar, Vishal M. Patel, and Rama Chellappa. Analysis sparse coding models for imagebased classification. In 2014 IEEE International Conference on Image Processing, ICIP 2014, Paris, France, October 27-30, 2014, pp. 5207­5211. IEEE, 2014. ISBN 978-1-4799-5751-4. doi: 10.1109/ICIP.2014.7026054. URL http://dx.doi.org/10.1109/ICIP.2014. 7026054.
Ali Taalimi, Shahab Ensafi, Hairong Qi, Shijian Lu, Ashraf A. Kassim, and Chew Lim Tan. Multimodal dictionary learning and joint sparse representation for hep-2 cell classification. pp. 308­ 315, 2015.
T. H. Vu and V. Monga. Learning a low-rank shared dictionary for object classification. In 2016 IEEE International Conference on Image Processing (ICIP), pp. 4428­4432, Sept 2016a. doi: 10.1109/ICIP.2016.7533197.
T. H. Vu, H. S. Mousavi, V. Monga, U. K. A. Rao, and G. Rao. Dfdl: Discriminative feature-oriented dictionary learning for histopathological image classification. In 2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI), pp. 990­994, April 2015. doi: 10.1109/ISBI.2015. 7164037.
26

Under review as a conference paper at ICLR 2018
Tiep Huu Vu and Vishal Monga. Fast low-rank shared dictionary learning for image classification. CoRR, abs/1610.08606, 2016b. URL http://arxiv.org/abs/1610.08606.
Tiep Huu Vu and Vishal Monga. Fast low-rank shared dictionary learning for image classification. CoRR, abs/1610.08606, 2016c. URL http://arxiv.org/abs/1610.08606.
Yong Xu, Yuping Sun, Yuhui Quan, and Bo Zheng. Discriminative structured dictionary learning with hierarchical group sparsity. Comput. Vis. Image Underst., 136(C):59­68, July 2015. ISSN 1077-3142. doi: 10.1016/j.cviu.2015.01.006. URL http://dx.doi.org/10.1016/j. cviu.2015.01.006.
M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination dictionary learning for sparse representation. In 2011 International Conference on Computer Vision, pp. 543­550, Nov 2011a. doi: 10.1109/ICCV.2011.6126286.
Meng Yang, Lei Zhang, Xiangchu Feng, and David Zhang. Fisher discrimination dictionary learning for sparse representation. In Proceedings of the 2011 International Conference on Computer Vision, ICCV '11, pp. 543­550, Washington, DC, USA, 2011b. IEEE Computer Society. ISBN 978-1-4577-1101-5. doi: 10.1109/ICCV.2011.6126286. URL http://dx.doi.org/10. 1109/ICCV.2011.6126286.
27

