Under review as a conference paper at ICLR 2018
GRADIENT ESTIMATORS FOR IMPLICIT MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.
1 INTRODUCTION
Modelling is fundamental to the success of technological innovations for artificial intelligence. A powerful model learns a useful representation of the observations for a specified prediction task, and generalises to unknown instances that follow similar generative mechanics. A well established area of machine learning research focuses on developing prescribed probabilistic models (Diggle & Gratton, 1984), where learning is based on evaluating the probability of observations under the model. Implicit probabilistic models, on the other hand, are defined by a stochastic procedure that allows for direct generation of samples, but not for the evaluation of model probabilities. These are omnipresent in scientific and engineering research involving data analysis, for instance ecology, climate science and geography, where simulators are used to fit real-world observations to produce forecasting results. Within the machine learning community there is a recent interest in a specific type of implicit models, generative adversarial networks (GANs) (Goodfellow et al., 2014), which has been shown to be one of the most successful approaches to image and text generation (Radford et al., 2016; Yu et al., 2017; Arjovsky et al., 2017; Berthelot et al., 2017). Very recently, implicit distributions have also been considered as approximate posterior distributions for Bayesian inference, e.g. see (Liu & Feng, 2016; Wang & Liu, 2016; Li & Liu, 2016; Karaletsos, 2016; Mescheder et al., 2017; Husza´r, 2017; Li et al., 2017; Tran et al., 2017). These examples demonstrate the superior flexibility of implicit models, which provide highly expressive means of modelling complex data structures.
Whilst prescribed probabilistic models can be learned by standard (approximate) maximum likelihood or Bayesian inference, implicit probabilistic models require substantially more severe approximations due to the intractability of the model distribution. Many existing approaches first approximate the model distribution or optimisation objective function and then use those approximations to learn the associated parameters. However, for any finite number of data points there exists an infinite number of functions, with arbitrarily diverse gradients, that can approximate perfectly the objective function, and optimising such approximations can lead to unstable training and poor results. Recent research on GANs, where the issue is highly prevalent, suggest that restricting the representational power of the discriminator is effective in stabilising training (e.g. see (Arjovsky et al., 2017)). However, such restrictions often introduce undesirable biases, responsible for problems such as mode collapse in the context of GANs, and the underestimation of uncertainty in variational inference methods (Turner & Sahani, 2011).
1

Under review as a conference paper at ICLR 2018

In this paper we explore approximating the score function as an alternative method for training implicit models. An accurate approximation of the score function then allows the application of many well-studied algorithms, such as maximum likelihood, maximum entropy estimation, variational inference and gradient-based MCMC, to implicit models. Concretely, our contributions include:
· the Stein gradient estimator, a novel generalisation of the score matching estimator (Hyva¨rinen, 2005), that includes both parametric and non-parametric forms;
· a comparison of the proposed estimator with the score matching and the KDE plug-in estimators on performing gradient-free MCMC, meta-learning of approximate posterior samplers for Bayesian neural networks, and entropy based regularisation of GANs.

2 LEARNING IMPLICIT PROBABILISTIC MODELS

Given a dataset D containing i.i.d. samples we would like to learn a probabilistic model p(x) for the underlying data distribution pD(x). In the case of implicit models, p(x) is defined by a generative process. For example, to generate images, one might define a generative model p(x) that consists of sampling randomly a latent variable z  p0(z) and then defining x = f(z). Here f is a function parametrised by , usually a deep neural network or a simulator. We assume f to be differentiable
w.r.t. . An extension to this scenario is presented by conditional implicit models, where the addition of a supervision signal y, such as an image label, allows us to define a conditional distribution p(x|y) implicitly by the transformation x = f(z, y). A related methodology, wild variational inference (Liu & Feng, 2016; Li & Liu, 2016) assumes a tractable joint density p(x, z), but uses implicit proposal distributions to approximate an intractable exact posterior p(z|x). Here the approximate posterior q(z|x) can likewise be represented by a deep neural network, but also by a truncated
Markov chain, such as that given by Langevin dynamics with learnable step-size.

Though providing extreme flexibility and expressive power, the intractability of density evaluation also brings serious optimisation issues for implicit models. This is because many learning algorithms, e.g. maximum likelihood estimation (MLE), rely on minimising a distance/divergence/discrepancy measure D[p||pD], which often requires evaluating the model density (c.f. Ranganath et al., 2016; Liu & Feng, 2016). Thus good approximations to the optimisation procedure are the key to learning implicit models that can describe complex data structures. In the context of GANs, the Jensen-Shannon divergence is approximated by a variational lower-bound represented by a discriminator (Barber & Agakov, 2003; Goodfellow et al., 2014). Related work for wild variational inference (Li & Liu, 2016; Mescheder et al., 2017; Husza´r, 2017; Tran et al., 2017) uses a GAN-based technique to construct a density ratio estimator for q/p0 (Sugiyama et al., 2009; 2012; Uehara et al., 2016; Mohamed & Lakshminarayanan, 2016) and then approximate the KL-divergence term in the variational lower-bound:

LVI(q) = Eq [log p(x|z)] - KL[q(z|x)||p0(z)].

(1)

In addition, Li & Liu (2016) and Mescheder et al. (2017) exploit the additive structure of the KLdivergence and suggest discriminating between q and an auxiliary distribution that is close to q, making the density ratio estimation more accurate. Nevertheless all these algorithms involve a minimax optimisation, and the current practice of gradient-based optimisation is notoriously unstable.

The stabilisation of GAN training is itself a recent trend of related research (e.g. see Salimans et al., 2016; Arjovsky et al., 2017). However, as the gradient-based optimisation only interacts with gradients, there is no need to use a discriminator if an accurate approximation to the intractable gradients could be obtained. For example, notice that the variational lower-bound can be rewritten as

LVI(q) = Eq [log p(x, z)] + H[q(z|x)],

(2)

the gradient of the variational parameters  can be computed by a sum of the path gradient of the first term (i.e. E f log p(x, f ( , x))Tf ( , x) ) and the gradient of the entropy term
H[q(z|x)]. Expanding the latter, we have

-H[q(z|x)] = E f log q(f ( , x)|x)Tf ( , x)] + Eq [ log q(z|x)] , (3)

in which the second term on the RHS is zero (Roeder et al., 2017). As we typically assume the tractability of f , an accurate approximation to z log q(z|x) would remove the requirement of

2

Under review as a conference paper at ICLR 2018

discriminators, speed-up the learning and obtain potentially a better model. Many gradient approximation techniques exist (Stone, 1985; Fan & Gijbels, 1996; Zhou & Wolfe, 2000; De Brabanter et al., 2013), and in particular, we will review score matching methods (Hyva¨rinen, 2005) in more detail in the next section, and motivate the main contribution of the paper.

3 GRADIENT APPROXIMATION WITH THE STEIN GRADIENT ESTIMATOR

We propose the Stein gradient estimator as a novel generalisation of the score matching gradient es-
timator. Before presenting it we first set-up the notation. Column vectors and matrices are boldfaced. The random variable under consideration is x  X with X = Rd×1 if not specifically mentioned. To avoid misleading notation we use the distribution q(x) to derive the gradient approximations for gen-
eral cases. As Monte Carlo methods are heavily used for implicit models, in the rest of the paper we mainly consider approximating the gradient g(xk) := xk log q(xk) for xk  q(x), k = 1, ..., K. We use xij to denote the jth element of the ith sample xi. We also denote the matrix form of the collected gradients as G := x1 log q(x1), · · · , xK log q(xK ) T  RK×d, and its approximation G^ := g^(x1), · · · , g^(xK ) T with g^(xk) = xk log q^(xk) for some q^(x).

3.1 KDE GRADIENT ESTIMATOR: PLUG-IN ESTIMATOR WITH KERNEL DENSITY ESTIMATION

A naive approach for gradient approximation would first estimate the intractable density q^(x) 

q(x) (up to a constant), then approximate the exact gradient by x log q^(x)  x log q(x). One

may favour complex density approximation schemes for better accuracy, but here we focus on "quick

and dirty" approximations as the gradient approximation is repeatedly required for the optimisation

procedure. Specifically we consider kernel density estimation (KDE) with the reproducing kernel

K(x, ·) of an RKHS H (Berlinet & Thomas-Agnan, 2011), i.e.

q^(x)

=

1 K

K k=1

K(x,

xk

)

×

C.

Simple derivations (Singh, 1977) reveal the KDE gradient estimator as

KK

G^ iKjDE =

xij K(xi, xk)/

K(xi, xk).

k=1

k=1

(4)

3.2 SCORE MATCHING GRADIENT ESTIMATOR: MINIMISING MSE

The KDE estimator performs indirect approximation of the gradient via density estimation, which
can be inaccurate. An alternative approach directly approximates the gradient x log q(x) by minimising the expected l2 error w.r.t. the approximation g^(x) = (g^1(x), · · · , g^d(x))T:

F (g^) := Eq ||g^(x) - x log q(x)||22 .

(5)

It has been shown in Hyva¨rinen (2005) that this objective can be reformulated as

F (g^) = Eq ||g^(x)||22 + 2 , g^(x) + C,

d
, g^(x) = xj g^j(x).
j=1

(6)

The key insight here is the usage of integration by parts: after expanding the l2 loss objective, the cross term can be rewritten as Eq g^(x)Tx log q(x) = -Eq [ , g^(x) ] , if assuming the

boundary condition: q(x)g^(x)|X = 0, or limx q(x)g^(x) = 0 if X = Rd. This condition

holds for many distributions, e.g. those with Gaussian-like tails. The optimum of (6) is referred as

the score matching gradient estimator. Sasaki et al. (2014) and Strathmann et al. (2015) derived

a parametric solution by first approximating the log density up to a constant with the RBF kernel

K(x, x ) to obtain

:= the

ceoxepffi-cie2n1t2s||a^xksc-orexan|d|22

, i.e. log q^(x) :=

K k=1

constructing the gradient

akK(x, xk) estimator as

+

C,

then

minimising

(6)

K

G^ isc·ore =

a^kscorexi K(xi, xk).

(7)

k=1

A slightly different approach Sasaki et al. (2015) proposes an estimator for xq(x), by minimising the l2 error (g^j(x) - xj q(x))2dx and removing explicit calculations of xq(x) in an analogous way as to derive (6).

3

Under review as a conference paper at ICLR 2018

3.3 STEIN GRADIENT ESTIMATOR: INVERTING STEIN'S IDENTITY

Recall the derivation of the score matching objective (6) that uses integration by parts and the bound-
ary condition. Stein's identity (Stein, 1972) can also be derived using the same idea . Can we directly
exploit Stein's identity to obtain an estimator of the gradients? In this section we investigate the mul-
tivariate version of Stein's identity (Stein, 1981) to derive a new gradient estimator that we call the Stein gradient estimator. Let h : Rd×1  Rd ×1 be a differentiable multivariate test function satisfying the boundary condition, which maps x to a column vector h(x) = [h1(x), h2(x), ..., hd (x)]T. Now we introduce the multivariate Stein's identity (Stein, 1981)

Eq[h(x)x log q(x)T + xh(x)] = 0,

(8)

in which the gradient matrix term xh(x) = (xh1(x), · · · , xhd (x))T  Rd ×d. We further approximate the above with Monte Carlo (MC):

1 K

K

-h(xk )xk

log q(xk)T

+

err

=

1 K

K

xk h(xk),

k=1

k=1

xk  q(xk),

(9)

with err  Rd ×d the random error due to MC approximation, which has mean 0 and vanishes

as K  +. Now by temporarily denoting H = h(x1), · · · , h(xK )  Rd ×K , xh =

1 K

K k=1

xk

h(xk

)



Rd

×d,

equation

(9)

can

be

rewritten

as

-

1 K

HG

+

err

=

xh.

Thus

we

consider a ridge regression method (i.e. adding an l2 regulariser) to estimate G:

G^ VStein

:=

arg min ||xh
G^ RK×d

+

1 K

HG^ ||F2

+

 2K

2

||G^ ||F2

,

(10)

with || · ||F the Frobenius norm of a matrix and   0. Simple calculation shows that

G^ VStein = -(K + I)-1 , K ,

(11)

where K := HTH, Kij = K(xi, xj) := h(xi)Th(xj), , K := KHTxh, , K ij =

K k=1

xkj

K(xi,

xk ).

One

can

show

that

the

RBF

kernel

satisfies

Stein's

identity

(Liu

et

al.,

2016).

In this case h(x) = K(x, ·), d = + and by the reproducing kernel property, h(x)Th(x ) =

K(x, ·), K(x , ·) H = K(x, x ).

Interestingly for translation invariant kernels K(x, x ) = K(x - x ) the KDE gradient estimator (4) can be rewritten as G^ KDE = -diag (K1)-1 , K . Inspecting and comparing it with the Stein
gradient estimator (11), one might notice that the Stein method uses the full kernel matrix as the
pre-conditioner, while the KDE method computes an averaged "kernel similarity" for the denom-
inator. We conjecture that this difference is key to the superior performance of the Stein gradient
estimator when compared to the KDE gradient estimator (see later experiments). The KDE method only collects the similarity information between xk and other samples xj to form an estimate of xk log q(xk), whereas for the Stein gradient estimator, the kernel similarity between xi and xj for all i, j = k are also incorporated. Thus it is reasonable to conjecture that the Stein method can
be more sample efficient, which also implies higher accuracy when the same number of samples are
collected.

3.4 STEIN GRADIENT ESTIMATOR MINIMISES THE KERNELISED STEIN DISCREPANCY

So far we have presented the Stein gradient estimator and compared it to the KDE estimator. In this

section we derive the Stein gradient estimator again but from a divergence/discrepancy minimisation

perspective. Stein's method also provides a tool for checking if two distributions q(x) and q^(x) are

identical. If the test function set H is sufficiently rich, then one can define a Stein discrepancy

measure by

S(q, q^) := sup Eq x log q^(x)Th(x) + , h ,
hH

(12)

see (Gorham & Mackey, 2015) for an example derivation. When H is defined as a unit ball in an

RKHS induced by a kernel K(x, ·), Liu et al. (2016) and Chwialkowski et al. (2016) showed that

the supremum in (12) can be analytically obtained as (with Kxx shorthand for K(x, x )):

S2(q, q^) = Ex,x q (g^(x) - g(x))TKxx (g^(x ) - g(x )) ,

(13)

4

Under review as a conference paper at ICLR 2018

which is also named the kernelised Stein discrepancy (KSD). Chwialkowski et al. (2016) showed that for C0-universal kernels satisfying the boundary condition, KSD is indeed a discrepancy measure: S2(q, q^) = 0  q = q^. Gorham & Mackey (2017) further characterised the power of KSD on detecting non-convergence cases. Furthermore, if the kernel is twice differentiable, then using the same technique as to derive (6) one can compute KSD by

S2(q, q^) = Ex,x q g^(x)TKxx g^(x ) + g^(x)Tx Kxx + xKxTx g^(x ) + Tr(x,x Kxx ) .

In practice KSD is estimated with samples {xk}kK=1  q, and simple derivations show that the V-

statistic

of KSD

can

be reformulated as

SV2 (q, q^)

=

1 K2

Tr(G^ T

KG^

+

2G^ T

, K

) + C.

Thus the l2

error in (10) is equivalent to the V-statistic of KSD if h(x) = K(x, ·), and we have the following:

Theorem 1. G^ VStein is the solution of the following KSD V-statistic minimisation problem

G^ VStein

=

arg min
G^ RK×d

SV2

(q, q^)

+

 2K

2

||G^ ||2F

.

(14)

One can also minimise the U-statistic of KSD to obtain gradient approximations, in which we provide a derivation of corresponding optimal solutions in the appendix. In experiments we use Vstatistic solutions and leave comparisons between these methods to future work.

The score matching objective (5) is also called Fisher divergence (Johnson, 2004) which is a special case of KSD by selecting K(x, x ) = x=x . Thus the Stein gradient estimator can be viewed as a generalisation of the score matching estimator. The comparison between the two estimators is
more complicated. Certainly by the Cauchy-Schwarz inequality the Fisher divergence is stronger
than KSD in terms of detecting convergence (Liu et al., 2016). However it is difficult to perform di-
rect gradient estimation by minimising the Fisher divergence as it involves computing second-order
gradient terms. Therefore one needs to propose a parametric approximation (e.g. in the form of (7)) to G and then optimise the associated parameters accordingly, which can remove the advantage
of using a stronger divergence potentially. Conversely, the proposed Stein gradient estimator (11) is non-parametric in that it directly optimises over functions evaluated at locations {xk}kK=1. This brings in two key advantages over the score matching gradient estimator: (i) it removes the ap-
proximation error due to the use of restricted family of parametric approximations and thus can be
potentially more accurate; (ii) it has a much simpler and ubiquitous form that applies to any kernel
satisfying the boundary condition, whereas the score matching estimator requires tedious deriva-
tions for different kernels repeatedly (see appendix). In terms of computation speed, since in most
of the cases the computation of the score matching gradient estimator also involves kernel matrix inversions, both estimators are of the same order of complexity, which is O(K3 + K2d) (kernel matrix computation plus inversion). Low-rank approximations such as the Nystro¨m method (Smola
& Scho¨kopf, 2000; Williams & Seeger, 2001) can enable speed-up, but this is not investigated in the
paper.

3.5 ADDING PREDICTIVE POWER
Though providing potentially more accurate approximations, the non-parametric estimator (11) has no predictive power as described so far. Crucially, many tasks in machine learning require predicting gradient functions at samples sampled from distributions other than q, for example, in MLE q(x) corresponds to the model distribution which is learned using samples from the data distribution instead. To address this issue, hereby we derive two predictive estimators, one generalised from the non-parametric estimator and the other minimises KSD using parametric approximations.

Predictions using the non-parametric estimator. Let us consider an unseen datum y. If y is sam-
pled from q, then one can also apply the non-parametric estimator (11) for gradient approximation, given the observed data X = {x1, ..., xK }  q. Concretely, if writing g^(y)  y log q(y)  Rd×1 then the non-parametric Stein gradient estimator computed on X  {y} is

g^(y)T G^

= -(K + I)-1

yK(y, y) +

K k=1

xk

K(y

,

xk

)

, K + yK(·, y)

,

K =

Kyy KXy

KyX K

,

with yK(·, y) denoting a K × d matrix with rows yK(xk, y), and yK(y, y) only differentiates through the second argument. Then we demonstrate in the appendix that, by simple matrix

5

Under review as a conference paper at ICLR 2018

calculations and assuming a translation invariant kernel, we have (with column vector 1  RK×1):

y log q(y)T  - Kyy +  - KyX(K + I)-1KXy -1 KyXG^ SVtein - KyX(K + I)-1 + 1T yK(·, y) .

(15)

In practice one would store the computed gradient G^ VStein, the kernel matrix inverse (K + I)-1 and  as the "parameters" of the predictive estimator. For a new observation y  p in general, one can "pretend" y is a sample from q and apply the above estimator as well. The approximation quality depends on the similarity between q and p, and we conjecture here that this similarity measure, if can be described, is closely related to the KSD.

Fitting a parametric estimator using KSD. The non-parametric predictive estimator could be
computationally demanding. Setting aside the cost of fitting the "parameters", in prediction the time complexity for the non-parametric estimator is O(K2 + Kd). Also storing the "parameters" needs O(Kd) memory for G^ VStein. These makes the non-parametric estimator undesirable for highdimensional data, since in order to obtain accurate predictions it often requires K scaling (more
than linearly) with d as well. To address this, one can also minimise the KSD using parametric
approximations, in a similar way as to derive the score matching estimator in Section 3.2. More
precisely, we define a parametric approximation in a similar fashion as (7), and in the appendix we
show that if the RBF kernel is used for both the KSD and the parametric approximation, then the linear coefficients a = (a1, ..., aK )T can be calculated analytically: a^SVtein = ( + I)-1b, with

 =X (KKK) + K(K X)K - ((KK) X)K - K((KK) X), b =(Kdiag(X)K + (KK) X - K(K X) - (K X)K)1.

(16)

Then for an unseen observation y  p the gradient approximation returns y log q(y)  (a^SVtein)TyK(·, y). In this case one only maintains the linear coefficients a^VStein and compute a linear combination in prediction, which takes O(K) memory and O(Kd) time and therefore it is
computationally cheaper than the non-parametric prediction model (24).

4 APPLICATIONS
We present some case studies that apply the gradient estimators for implicit models. Detailed settings (architecture, learning rate, etc.) are presented in the appendix. Implementation will be released at https://github.com/FirstAuthor/SteinGrad upon acceptance.
4.1 SYNTHETIC EXAMPLE: HAMILTONIAN FLOW WITH APPROXIMATE GRADIENTS
We first consider a simple synthetic example to demonstrate the accuracy of the proposed gradient estimator. More precisely we consider the kernel induced Hamiltonian flow (not an exact sampler) (Strathmann et al., 2015) on a 2-dimensional banana-shaped object: x  B(x; b = 0.03, v = 100)  x1  N (x1; 0, v), x2 = + b(x21 - v),  N ( ; 0, 1). The approximate Hamiltonian flow is constructed using the same operator as in Hamiltonian Monte Carlo (HMC) (Neal et al., 2011), except that the exact score function x log B(x) is replaced by the approximate gradients. We still use the exact target density to compute the rejection step as we mainly focus on testing the accuracy of the gradient estimators. We test both versions of the predictive Stein gradient estimator. We fit the gradient estimators on K = 200 training datapoints from the target density. The bandwidth of the RBF kernel is computed by the median heuristic and scaled up by a scalar between [1, 5]. All three methods are simulated for T = 2, 000 iterations, share the same initial locations that are constructed by target distribution samples plus Gaussian noises of standard deviation 2.0, and the results are averaged over 200 parallel chains.
We visualise the samples and some MCMC statistics in Figure 1. In general all the resulting Hamiltonian flows are HMC-like, which give us the confidence that the gradient estimators extrapolate reasonably well at unseen locations. However all of these methods have trouble exploring the extremes, because at those locations there are very few or even no training data-points. Indeed we found it necessary to use large (but not too large) bandwidths, in order to both allow exploration of those extremes, and ensure that the corresponding test function is not too smooth. In terms of

6

Under review as a conference paper at ICLR 2018

Figure 1: Kernel induced Hamiltonian flow compared with HMC. Top: samples generated from the dynamics, training data (in cyan), an the trajectory of a particle for T = 1 to 200 starting at the star location (in yellow). Bottom: statistics computed during simulations. See main text for details.
quantitative metrics, the acceptance rates are reasonably high for all the gradient estimators, and the KSD estimates (across chains) as a measure of sample quality are also close to that computed on HMC samples. The returned estimates of E[x1] are close to zero which is the ground true value. We found that the non-parametric Stein gradient estimator is more sensitive to hyper-parameters of the dynamics, e.g. the stepsize of each HMC step. We believe a careful selection of the kernel (e.g. those with long tails) and a better search for the hyper-parameters (for both the kernel and the dynamics) can further improve the sample quality and the chain mixing time, but this is not investigated here.

4.2 META-LEARNING OF APPROXIMATE POSTERIOR SAMPLERS FOR BAYESIAN NNS

One of the recent focuses on meta-learning has been on learning optimisers for training deep neural

networks, e.g. see (Andrychowicz et al., 2016). Could analogous goals be achieved for approximate

inference? In this section we attempt to learn an approximate posterior sampler for Bayesian neural

networks (Bayesian NNs, BNNs) that generalises to unseen datasets and architectures. Concretely,

we consider a binary classification task: p(y = 1|x, ) = sigmoid(NN(x)), p0() = N (; 0, I).

After observing the training data D = {(xn, yn)}nN=1, the predictive distribution for a new obser-

vation x

is approximated by p(y

=

1|x, D)



1 K

K k=1

p(y

=

1|x, k), k



q().

Here

the approximate posterior q() is implicitly defined by the following stochastic normalising flow

(Rezende & Mohamed, 2015) t+1 = f (t, t, t): given the current location t and the mini-

batch data {(xm, ym)}mM=1, the update for the next step is

t+1 = t + (t, t) + (t, t) t, t  N ( ; 0, I),

NM

t = t M

log p(ym|xm, t) + log p0(t) .

m=1

(17)

The coordinates of the noise standard deviation (t, t) and the moving direction (t, t) are parametrised by a coordinate-wise neural network. If properly trained, this neural network will
learn the best combination of the current location and gradient information, and produce approxi-
mate posterior samples efficiently on different probabilistic modelling tasks. Here we propose using the variational inference objective (2) computed on the samples {tk} to learn the variational parameters . Since in this case the gradient of the log joint distribution can be computed analytically, we only approximate the gradient of the entropy term H[q] as in (3), with the exact score function replaced by the presented gradient estimators. We report the results using the non-parametric
Stein gradient estimator as we found it works better than the parametric version. The RBF kernel is
applied for gradient estimation, with the bandwidth computed using the median heuristic.

We briefly describe the test protocol. We take from the UCI repository (Lichman, 2013) six binary classification datasets (australian, breast, crabs, ionosphere, pima, sonar), train an approximate sampler on crabs with a small neural network that has one 20-unit hidden layer with ReLU activation, and generalise to the remaining datasets with a bigger network that has 50 hidden units and uses sigmoid activation. We use ionosphere as the validation set to tune . The remaining 4 datasets are further split into 40% training subset for simulating samples from the approximate sampler, and 60% test subsets for evaluating the sampler's performance.

7

Under review as a conference paper at ICLR 2018

Figure 2: Generalisation performances for trained approximate posterior samplers.

Figure 2 presents the (negative) test log-likelihood (LL), classification error, and an estimate of the KSD U-statistic SU2 (p(|D), q()) (with data sub-sampling) over 5 splits of each test datasets. Besides the gradient estimators we also compare with two baselines: an approximate posterior sampler trained by maximum a posteriori (MAP), and stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) evaluated on the test datasets directly. Surprisingly the score matching estimator method failed to produce good results on all metrics for all test datasets, even after carefully tuning the bandwidth and the regularisation parameter . For clarity we do not include the results for the score matching estimator in the main text and refer to the appendix for details. By contrast, the median heuristic seems sufficient for the other two methods to produce overall good results, although careful hyper-parameter tuning can further improve their performance. In summary, the Stein approach performs a little better than SGLD, whereas the KDE method is slightly worse. The MAP model under-performs for test log-likelihood and KSD metrics, presumably because the noise variance  has been turned off during learning thus having discovered an optimiser instead. Future work should investigate the usage of advanced recurrent neural networks such as an LSTM (Hochreiter & Schmidhuber, 1997), which is expected to return better performance.

4.3 TOWARDS ADDRESSING MODE COLLAPSE IN GANS USING ENTROPY REGULARISATION

GANs are notoriously difficult to train in practice. Besides the instability of gradient-based minimax optimisation which has been partially addressed by many recent proposals (Salimans et al., 2016; Arjovsky et al., 2017; Berthelot et al., 2017), they also suffer from mode collapse. We propose adding an entropy regulariser to the GAN generator loss. Concretely, assume the generative model p(x) is implicitly defined by x = f(z), z  p0(z), then the generator's loss is defined by

J~gen() = Jgen() - H[p(x)],

(18)

where Jgen() is the original loss function for the generator from any GAN algorithm and  is a hyper-parameter. In practice (the gradient of) (18) is estimated using Monte Carlo.

We empirically investigate the entropy regularisation idea on the very recently proposed boundary

equilibrium GAN (BEGAN) (Berthelot et al., 2017) method using (continuous) MNIST, and we

refer to the appendix for the detailed mathematical set-up. In this case the non-parametric V-statistic

Stein gradient estimator is used. We use a convolutional generative network and a convolutional

auto-encoder and select the hyper-parameters of BEGAN   {0.3, 0.5, 0.7},   [0, 1] and  =

0.001.

The

Epanechnikov

kernel

K(x, x

)

:=

1 d

jd=1(1 - (xj - xj)2) is used as the pixel values lie

in a unit interval (see appendix for the expression of the score matching estimator), and to ensure the

boundary condition we clip the pixel values into range [1e-8, 1 - 1e-8]. The generated images are

visualised in Figure 3. BEGAN without the entropy regularisation fails to generate diverse samples

even when trained with learning rate decay. The other three images clearly demonstrate the benefit of

the entropy regularisation technique, with the Stein approach obtaining the highest diversity without

compromising visual quality.

8

Under review as a conference paper at ICLR 2018
Figure 3: Visualisation of generated images from trained BEGAN models.
Figure 4: Quantitative evaluation on entropy regularised BEGAN. The higher the better for the LHS panels and the other way around for the RHS ones. See main text for details.
We further consider four metrics to assess the trained models quantitatively. First 500 samples are generated for each trained model, then we compute their nearest neighbours in the training set using l1 distance, and obtain a probability vector p by averaging over these neighbour images' label vectors. In Figure 4 we depict the entropy of p (top left), averaged l1 distances to the nearest neighbour (top right), and the difference between the largest and smallest elements in p (bottom right). The error bars are obtained by 5 independent runs. These results demonstrate that the Stein approach performs significantly better than the other two, in that it learns a better generative model not only faster but also in a more stable way. Interestingly the KDE approach achieves the lowest average l1 distance to nearest neighbours, possibly because it tends to memorise training examples. We next train a fully connected network (y|x) on MNIST that achieves 98.16% text accuracy, and compute on the generated images an empirical estimate of the inception score (Salimans et al., 2016) Ep(x)[KL[(y|x)||(y)]] with (y) = Ep(x)[(y|x)] (bottom left panel). High inception score indicates that the generate images tend to be both realistic looking and diverse, and again the Stein approach out-performs the others on this metric by a large margin.
5 CONCLUSIONS AND FUTURE WORK
We have presented the Stein gradient estimator as a novel generalisation to the score matching gradient estimator. With a focus on learning implicit models, we have empirically demonstrated the efficacy of the proposed estimator by showing how it opens the door to a range of novel learning tasks: approximating gradient-free MCMC, meta-learning for approximate inference, and unsupervised learning for image generation. Future work will expand the understanding of gradient estimators in both theoretical and practical aspects. Theoretical development will compare both the V-statistic and U-statistic Stein gradient estimators and formalise consistency proofs. Practical work will improve the sample efficiency of kernel estimators in high dimensions and develop fast yet accurate approximations to matrix inversion. It is also interesting to investigate applications of gradient approximation methods to training implicit generative models without the help of discriminators. Finally it remains an open question that how to generalise the Stein gradient estimator to non-kernel settings and discrete distributions.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1):3563­3593, 2014.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981­3989, 2016.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
David Barber and Felix V Agakov. The im algorithm: A variational approach to information maximization. In NIPS, pp. 201­208, 2003.
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In Proceedings of The 33rd International Conference on Machine Learning, pp. 2606­2615, 2016.
Kris De Brabanter, Jos De Brabanter, Bart De Moor, and Ire`ne Gijbels. Derivative estimation with local polynomial fitting. The Journal of Machine Learning Research, 14(1):281­301, 2013.
Peter J Diggle and Richard J Gratton. Monte carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society. Series B (Methodological), pp. 193­227, 1984.
Jianqing Fan and Irne Gijbels. Local polynomial modelling and its applications. Chapman & Hall, 1996.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Jackson Gorham and Lester Mackey. Measuring sample quality with stein's method. In NIPS, 2015.
Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In ICML, 2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771­1800, 2002.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Ferenc Husza´r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.
Aapo Hyva¨rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(Apr):695­709, 2005.
Aapo Hyva¨rinen. Consistency of pseudolikelihood estimation of fully visible boltzmann machines. Neural Computation, 18(10):2283­2292, 2006.
Oliver Thomas Johnson. Information theory and the central limit theorem. World Scientific, 2004.
Theofanis Karaletsos. Adversarial message passing for graphical models. arXiv preprint arXiv:1612.05048, 2016.
Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
Yingzhen Li and Qiang Liu. Wild variational approximations. NIPS workshop on advances in approximate Bayesian inference, 2016.
10

Under review as a conference paper at ICLR 2018
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised mcmc. arXiv preprint arXiv:1702.08343, 2017.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ ml.
Qiang Liu and Yihao Feng. Two methods for wild variational inference. arXiv preprint arXiv:1612.00081, 2016.
Qiang Liu, Jason D Lee, and Michael I Jordan. A kernelized stein discrepancy for goodness-of-fit tests and model evaluation. In ICML, 2016.
Siwei Lyu. Interpretation and generalization of score matching. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 359­366. AUAI Press, 2009.
Benjamin Marlin, Kevin Swersky, Bo Chen, and Nando Freitas. Inductive principles for restricted boltzmann machine learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 509­516, 2010.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:113­162, 2011.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
Rajesh Ranganath, Jaan Altosaar, Dustin Tran, and David M. Blei. Operator variational inference. In NIPS, 2016.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In ICML, 2015.
Geoffrey Roeder, Yuhuai Wu, and David Duvenaud. Sticking the landing: An asymptotically zerovariance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016.
Jaakko Sa¨rela¨ and Harri Valpola. Denoising source separation. Journal of machine learning research, 6(Mar):233­272, 2005.
Hiroaki Sasaki, Aapo Hyva¨rinen, and Masashi Sugiyama. Clustering via mode seeking by direct estimation of the gradient of a log-density. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 19­34. Springer, 2014.
Hiroaki Sasaki, Yung-Kyun Noh, and Masashi Sugiyama. Direct density-derivative estimation and its application in kl-divergence approximation. In AISTATS, 2015.
Radhey S Singh. Improvement on some known nonparametric uniformly consistent estimators of derivatives of a density. The Annals of Statistics, pp. 394­399, 1977.
Alex J Smola and Bernhard Scho¨kopf. Sparse greedy matrix approximation for machine learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pp. 911­918. Morgan Kaufmann Publishers Inc., 2000.
Casper Kaae Sonderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Husza´r. Amortised map inference for image super-resolution. In ICLR, 2017.
11

Under review as a conference paper at ICLR 2018
Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, pp. 583­602, 1972.
Charles M Stein. Estimation of the mean of a multivariate normal distribution. The annals of Statistics, pp. 1135­1151, 1981.
Charles J Stone. Additive regression and other nonparametric models. The annals of Statistics, pp. 689­705, 1985.
Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, and Arthur Gretton. Gradient-free hamiltonian monte carlo with efficient kernel exponential families. In Advances in Neural Information Processing Systems, pp. 955­963, 2015.
Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Shohei Hido, Jun Sese, Ichiro Takeuchi, and Liwei Wang. A density-ratio framework for statistical data processing. Information and Media Technologies, 4(4):962­987, 2009.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density-ratio matching under the bregman divergence: a unified framework of density-ratio estimation. Annals of the Institute of Statistical Mathematics, 64(5):1009­1044, 2012.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv preprint arXiv:1702.08896, 2017.
R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. In D. Barber, T. Cemgil, and S. Chiappa (eds.), Bayesian Time series models, chapter 5, pp. 109­130. Cambridge University Press, 2011.
Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Generative adversarial nets from a density ratio estimation perspective. arXiv preprint arXiv:1610.02920, 2016.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661­1674, 2011.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096­1103. ACM, 2008.
Dilin Wang and Qiang Liu. Learning to draw samples: With application to amortized mle for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681­688, 2011.
Christopher KI Williams and Matthias Seeger. Using the nystro¨m method to speed up kernel machines. In Advances in neural information processing systems, pp. 682­688, 2001.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: sequence generative adversarial nets with policy gradient. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Shanggang Zhou and Douglas A Wolfe. On derivative estimation in spline regression. Statistica Sinica, pp. 93­108, 2000.
12

Under review as a conference paper at ICLR 2018

A SCORE MATCHING ESTIMATOR: REMARKS AND DERIVATIONS

In this section we provide more discussions and analytical solutions for the score matching estimator. More specifically, we will derive the linear coefficient a = (a1, ..., aK) for the case of the Epanechnikov kernel.

A.1 SOME REMARKS ON SCORE MATCHING

Remark. It has been shown in Sa¨rela¨ & Valpola (2005); Alain & Bengio (2014) that de-noising auto-

encoders (DAEs) (Vincent et al., 2008), once trained, can be used to compute the score function

approximately. Briefly speaking, a DAE learns to reconstruct a datum x from a corrupted input

x~ = x+ ,  N (0, I) by minimising the mean square error. Then the optimal DAE can be used to

approximate the

score function as

x log p(x)



1 2

(DAE

(x)

-

x).

Sonderby

et al. (2017) applied

this idea to train an implicit model for image super-resolution, providing some promising results

in some metrics. However applying similar ideas to variational inference can be computationally

expensive, because the estimation of z log q(z|x) is a sub-routine for VI which is repeatedly

required. Therefore in the paper we deploy kernel machines that allow analytical solutions to the

score matching estimator in order to avoid double loop optimisation.

Remark. As a side note, score matching can also be used to learn the parameters of an unnormalised density. In this case the target distribution q would be the data distribution and q^ is often a Boltzmann distribution with intractable partition function. As a parameter estimation technique, score matching is also related to contrastive divergence (Hinton, 2002), pseudo likelihood estimation (Hyva¨rinen, 2006), and DAEs (Vincent, 2011; Alain & Bengio, 2014). Generalisations of score matching methods are also presented in e.g. Lyu (2009); Marlin et al. (2010).

A.2 THE RBF KERNEL CASE

The derivations for the RBF kernel case is referred to (Strathmann et al., 2015), and for com-

pleteness we include the final solutions here. Assume the parametric approximation is defined as

log q^(x) =

K k=1

ak

K(x,

xk

)

+

C

,

where

the

RBF

kernel

uses

bandwidth

parameter

.

then

the

optimal solution of the coefficients a^score = ( + I)-1v, with

d
v = 2K1 - K(xi
i=1

xi) + diag(xi)K1 - 2diag(xi)Kxi

,

d
 = [diag(xi)K - Kdiag(xi)] [Kdiag(xi) - diag(xi)K] ,
i=1
xi = (xi1, x2i , ..., xiK )T  RK×1.

A.3 THE EPANECHNIKOV KERNEL CASE

The Epanechnikov kernel is

defined as K(x, x )

=

1 d

d i=1

(1

-

(xi

-

xi)2),

where

the

first

and

second order gradients w.r.t. xi is

xi K(x, x

)

=

2 d

(xi

- xi),

xi xi K(x,

x

)

=

-2 d

.

(19)

Thus the score matching objective with log q^(x) =

K k=1

ak K(x,

xk )

+

C

is

reduced

to

F (a) = 1 K K

||

K

ak

2 (xk d

-

xj )||22

-

2

K

2 ak d d

j=1 k=1

k=1

=4 K K

1K d2

K
akak (xk - xj )T(xk - xj ) - aT1

j=1

k=1 k =1

:= 4(aTa - aT1),

13

Under review as a conference paper at ICLR 2018

with the matrix elements



kk

=

1 d2

(xk )T xk

+1 K

K

||xj ||22 - (xk + xk )Txj  .

j=1

Define the "gram matrix" Xij = (xi)Txj, we write the matrix form of  as



=

1 d2

X

+

1 K

Tr(X) - 2X11T

.

Thus with an l2 regulariser, the fitted coefficients are

a^ score

=

d2 2

X

+

1 K

Tr(X) - 2X11T

-1
+ I 1.

B STEIN GRADIENT ESTIMATOR: DERIVATIONS

B.1 DIRECT MINIMISATION OF KSD V-STATISTIC AND U-STATISTIC

The V-statistic of KSD is the following: given samples xk  q, k = 1, ..., K and recall Kjl = K(xj, xl)

SV2

(q, q^)

=

1 K2

K

K

g^(xj )TKjlg^(xl) + g^(xj )Txl Kjl + xj KjTlg^(xl) + Tr(xj,xl Kjl) .

j=1 l=1

(20)

The last term xj,xl Kjl will be ignored as it does not depend on the approximation g^. Using matrix

notations defined in the main text, readers can verify that the V-statistic can be computed as

SV2

(q, q^)

=

1 K2

Tr(KG^ G^ T

+

2

, K

G^ T)

+

C.

(21)

Using the cyclic invariance of matrix trace leads to the desired result in the main text. The U-statistic of KSD removes terms indexed by j = l in (20), in which the matrix form is

SU2 (q, q^)

=

1 K(K -

1) Tr((K

-

diag(K))G^ G^ T

+

2(

, K

- diag(K))G^ T) + C.

(22)

with the jth row of diag(K) defined as xj K(xj, xj). For most translation invariant kernels this extra term diag(K) = 0, thus the optimal solution of G^ by minimising KSD U-statistic is

G^ UStein = -(K - diag(K) + I)-1 , K .

(23)

B.2 DERIVING THE NON-PARAMETRIC PREDICTIVE ESTIMATOR

Let us consider an unseen datum y. If y is sampled from the q distribution, then one can also apply the non-parametric estimator (11) for gradient approximations, given the observed data X = {x1, ..., xK }  q. Concretely, if writing g^(y)  y log q(y)  Rd×1 then the non-parametric Stein gradient estimator (using V-statistic) is

g^(y)T G^

= -(K + I)-1

yK(y, y) +

K k=1

xk

K(y

,

xk

)

, K + yK(·, y)

,

K =

Kyy KXy

KyX K

,

with yK(·, y) denoting a K ×d matrix with rows yK(xk, y), and yK(y, y) only differentiates through the second argument. Thus by simple matrix calculations, we have:

y log q(y)T  - Kyy +  - KyX(K + I)-1KXy -1
K
yK(y, y) + xk K(y, xk) + KyXG^ VStein - KyX(K + I)-1yK(·, y) .
k=1
(24)

14

Under review as a conference paper at ICLR 2018

For translation invariant kernels, typically yK(y, y) = 0, and more conveniently, xk K(y, xk) = xk (xk - y)(xk-y)K(xk - y) = -yK(xk, y).
Thus equation (24) can be further simplified to (with column vector 1  RK×1) y log q(y)T  - Kyy +  - KyX(K + I)-1KXy -1 KyXG^ VStein - KyX(K + I)-1 + 1T yK(·, y) .
The solution for the U-statistic case can be derived accordingly which we omit here.

(25)

B.3 PARAMETRIC STEIN GRADIENT ESTIMATOR WITH THE RBF KERNEL

We define a parametric approximation in a similar way as for the score matching estimator:

K
log q^(x) := akK(x, xk) + C,

K(x, x ) = exp

-

1 22

||x

-

x

||22

.

k=1

(26)

Now we show the optimal solution of a = (a1, ..., aK )T by minimising (20). To simplify derivations

we assume the approximation and KSD use the same kernel. First note that the gradient of the RBF

kernel is

xK(x, x ) =

1 2

K(x,

x

)(x

- x).

(27)

Substituting (27) into (20):

SV2 (q, q^) = C +  + 2,



=

1 K2

K

K

K

K

akak Kkj KjlKlk

1 4

(xk

-

xj )T(xk

- xl),

k=1 k =1 j=1 l=1

(28)



=

1 K2

K

K

K

akKkj Kjl

1 4

(xk

-

xj )T(xj

-

xl).

k=1 j=1 l=1

(29)

We first consider summing the j, l indices in . Recall the "gram matrix" Xij = (xi)Txj, the inner product term in  can be expressed as Xkk + Xjl - Xkl - Xjk . Thus the summation over j, l can
be re-written as

KK

 :=

Kkj KjlKlk (Xkk + Xjl - Xkl - Xjk )

j=1 l=1

= X (KKK) + K(K X)K - ((KK) X)K - K((KK)

X).

And

thus



=

1 4

aT

a.

Similarly

the

summation

over

j,

l

in



can

be

simplified

into

KK

-b :=

Kkj Kjl(Xkj + Xjl - Xkl - Xjj )

j=1 l=1

= - (Kdiag(X)K + (KK) X - K(K X) - (K

X)K)1,

which leads to 

=

-

1 4

aT

b.

Thus minimising SV2 (q, q^) plus an l2

regulariser returns the Stein

estimator a^VStein in the main text.

Similarly we can derive the solution for KSD U-statistic minimisation. The U statistic can also be represented in quadratic form SU2 (q, q^) = C + ~ + 2~ , with ~ =  and

~

=



-

1 K2

K

K

K

akak Kkj Kjj Kjk

1 4

(Xkk

+ Xjj - Xkj - Xjk ).

k=1 k =1 j=1

15

Under review as a conference paper at ICLR 2018

neg. LL

test error

K S D/dim()

0.660 0.655 0.650 0.645 0.640 0.3330 0 0.3325 0.3320 0.3315 0.3310 0.3305 0.3300 0.3295 0.3290
90 0 80 70 60 50 40 30 20 10 0
0

2 = 1.0,  = 0.5

australian

0.7

0.6

0.5

0.4

0.3

500 1000 1500
500 1000 1500
500 1000 1500 iteration

20000.35 0 0.30 0.25 0.20 0.15 0.10 0.05 0.00
2000 350 30 25 20 15 10 5 0
2000 0

2 = 10.0,  = 1.0 breast

0.80

0.78

0.76

0.74

500 1000 1500
500 1000 1500
500 1000 1500 iteration

0.72 20000.653 0
0.652 0.651 0.650 0.649 0.648 0.647 0.646 0.645 2000300 0
250 200 150 100 50
0 2000 0

median trick,  = 2.0 pima
500 1000 1500 2000
500 1000 1500 2000
500 1000 1500 2000 iteration

Figure 5: Results of the score matching approach with some of the hyper-parameter set-ups.

Summing over the j indices for the second term, we have

K
Kkj Kjj Kjk (Xkk + Xjj - Xkj - Xjk )
j=1
= X (Kdiag(K)K) + Kdiag(K X)K - ((Kdiag(K))

X)K - K((diag(K)K)

X).

Working through the analogous derivations reveals that a^UStein = (~ + I)-1b, with
~ =X (K(K - diag(K))K) + K((K X) - diag(K X))K - ((K(K - diag(K))) X)K - K(((K - diag(K))K) X).

C MORE DETAILS ON THE EXPERIMENTS
We describe the detailed experimental set-up in this section. All experiments use Adam optimiser (Kingma & Ba, 2015) with standard parameter settings.

C.1 APPROXIMATE POSTERIOR SAMPLER EXPERIMENTS
We use a one hidden layer neural network with 20 hidden units to compute the noise variance and the moving direction of the next update. In a nutshell it takes the ith coordinate of the current position and the gradient t(i), t(i) as the inputs, and output the corresponding coordinate of the moving direction (t, t)(i) and the noise variance (t, t)(i). Softplus non-linearity is used for the hidden layer and to compute the noise variance we apply ReLU activation to ensure non-negativity. The step-size  is selected as 1e-5 which is tuned on the KDE approach. For SGLD step-size 1e-5 also returns overall good results.
The training process is the following. We simulate the approximate sampler for 10 transitions and sum over the variational lower-bounds computed on the samples of every step. Concretely, the maximisation objective is
T
L() = LVI(qt),
t=1
where T = 100 and qt() is implicitly defined by the marginal distribution of t that is dependent on . In practice the variational lower-bound LVI(qt) is further approximated by Monte Carlo and

16

Under review as a conference paper at ICLR 2018

data sub-sampling:

LVI(qt)



N M

M

log p(ym|xm, t) + log p0(t).

m=1

Truncated back-propagation is applied for every 10 steps in order to avoid vanishing/exploding gradients. The simulated samples at time T are stored to initialise the Markov chain for the next iteration, and for every 50 iterations we restart the simulation by randomly sampling the locations from the prior. Early stopping is applied using the validation dataset, and the learning rate is set to 0.001.

We found the median heuristic is sufficient for the KDE and Sten approach. However, we failed
to obtain desirable results using the score matching estimator, even after spending reasonable amount of time doing grid search on the bandwidth 2  {0.25, 1.0, 4.0, 10.0, median trick} and   {0.1, 0.5, 1.0, 2.0}. In Figure 5 we show the results of three set-ups that we have tried with the
score matching estimator.

C.2 BEGAN EXPERIMENTS

In this section we describe the experimental details of the BEGAN experiment, but first we introduce the mathematical idea and discuss how the entropy regulariser is applied.

Assume the generator is implicitly defined: x  p(x)  x = f(z), z  p0(z). In BEGAN the discriminator is defined as an auto-encoder D(x) that reconstructs the input x. After selecting a ratio parameter  > 0, a control rate 0 initialised at 0, and a "learning rate"  > 0 for the control rate, the loss functions for the generator x = f(z), z  p0(z) and the discriminator are:

J (x) = ||D(x) - x||, || · || = || · ||22 or || · ||1, Jgen(; ) = J (f(z)), z  p0(z) Jdis(; ) = J (x) - tJgen(; ), x  D t+1 = t + (J (x) - J (f(z))).

(30)

The main idea behind BEGAN is that, as the reconstruction loss J (·) is approximately Gaussian distributed, with  = 1 the discriminator loss Jdis is (approximately) proportional to the Wasserstein distance between loss distributions induced by the data distribution pD(x) and the generator p(x). In practice it is beneficial to maintain the equilibrium EpD [J (x)] = Ep [J (x)] through the optimisation procedure described in (30) that is motivated by proportional control theory. This
approach effectively stabilises training, however it suffers from catastrophic mode collapsing prob-
lem (see the left most panel in Figure 3). To address this issue, we simply subtract an entropy term
from the generator's loss function, i.e.

J~gen(; ) = Jgen(; ) + H[p],

(31)

where the rest of the optimisation objectives remains as in (30). This procedure would maintain

the equilibrium EpD [J (x)] = Ep [J (x)] - H[p]. We approximate the gradient H[p] using the estimators presented in the main text. For the purpose of updating the control rate t two

strategies are considered to approximate the contribution of the entropy term. Given K samples

x1, ..., xk  p(x), The first proposal considers a plug-in estimate of the entropy term with a KDE

estimate of p(x), which is consistent with the KDE estimator but not necessary with the other

two (as they use kernels when representing log p(x) or x log p(x)). The second one uses a

proxy

of

the

entropy

loss

-H~ [p]



1 K

K k=1

xk

log

p (xk )T xk

with

generated

samples

{xk }

and

xk log p(xk) approximated by the gradient estimator in use.

In the experiment, we construct a deconvolutional net for the generator and a convolutional auto-
encoder for the discriminator. The convolutional encoder consists of 3 convolutional layers with
filter width 3, stride 2, and number of feature maps [32, 64, 64]. These convolutional layers are
followed by two fully connected layers with [512, 64] units. The decoder and the generative net have
a symmetric architecture but with stride convolutions replaced by deconvolutions. ReLU activation
function is used for all layers except the last layer of the generator, which uses sigmoid non-linearity. The reconstruction loss in use is the squared l2 norm || · ||22. The randomness p0(z) is selected as uniform distribution in [-1, 1] as suggested in the original paper (Berthelot et al., 2017). Learning

17

Under review as a conference paper at ICLR 2018 rate is initialised at 0.0002 and decayed by 0.9 every 10 epochs, which is tuned on the KDE model. The selected  and  values are: for KDE estimator approach  = 0.3,  = 0.05, for score matching estimator approach  = 0.3,  = 0.1, and for Stein approach  = 0.5 and  = 0.3. The presented results use the KDE plug-in estimator for the entropy estimates (used to tune ) for the KDE and score matching approaches. Initial experiments found that for the Stein approach, using the KDE entropy estimator works slightly worse than the proxy loss, thus we report results using the proxy loss. An advantage of using the proxy loss is that it directly relates to the approximate gradient. Furthermore we empirically observe that the performance of the Stein approach is much more robust to the selection of  and  when compared to the other two methods.
18

