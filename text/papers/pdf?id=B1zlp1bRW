Under review as a conference paper at ICLR 2018
LARGE SCALE OPTIMAL TRANSPORT AND MAPPING ESTIMATION
Anonymous authors Paper under double-blind review
ABSTRACT
This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.
1 INTRODUCTION
Mapping one distribution to another Given two random variables X and Y taking values in X and Y respectively, the problem of finding a map f such that f (X) and Y have the same distribution, denoted f (X)  Y henceforth, finds applications in many areas. For instance, in domain adaptation, given a source dataset and a target dataset with different distributions, theory has shown that generalization depends on the similarity between the two distributions (Ben-David et al., 2010). The use of a mapping to align the source and target distribution is then a natural formulation for domain adaptation (Gopalan et al., 2011). Current state-of-the-art methods for computing generative models such as Generative Adversarial Networks (Goodfellow et al., 2014), Generative Moments Matching Networks (Li et al., 2015) or Variational Auto Encoders (Kingma & Welling, 2013) also rely on finding f such that f (X)  Y . In this setting, the latent variable X is often chosen as a continuous random variable, such as a Gaussian distribution, and Y is a discrete distribution of real data, e.g. the ImageNet dataset. By learning a map f , sampling from the generative model boils down to simply drawing a sample from X and then applying f to that sample.
Mapping with optimality Among the potentially many maps f verifying f (X)  Y , it may be of interest to find a map which satifies some optimality criterion. Given the cost of moving mass from one point to another, one would naturally look for a map which minimizes the total cost of transporting the mass from X to Y . This is the original formulation of Monge (Monge, 1781), which initiated the development of Optimal Transport (OT) theory. Such optimal maps can be practical for applications such as color transfer (Ferradans et al., 2014), shape matching (Su et al., 2015) or Bayesian inference (Reich, 2013). Unfortunately, the computation of Monge maps remains an open problem, except in specific cases. The modern approach to OT was introduced by Kanthorovich, who relaxed the Monge problem by optimizing over plans, i.e. distributions over the product space X × Y, rather than maps, casting the OT problem as a linear program. However, even with specialized algorithms such as the network simplex, solving that linear program takes O(n3 log n) time, where n is the size of the distribution (measure) support.
Large-scale OT with continuous measures Recently, Cuturi (2013) showed that introducing entropic regularization into the OT problem turns its dual into an easier optimization problem with two blocks of variables (one for each distribution), which can be solved using the Sinkhorn algorithm. However, the Sinkhorn algorithm does not scale well to measures supported on a large number of samples, since each of its iterations has an O(n2) complexity. In addition, the Sinkhorn algorithm cannot handle continuous probability measures. To address these issues, two recent works pro-
1

Under review as a conference paper at ICLR 2018

Target and Source distributions

Generated Samples
Target samples Generated samples

Displacement field

Generated density

Source dist. Target dist.
Figure 1: Example of estimated optimal map between a continuous Gaussian distribution (colored level sets) and a multi-modal discrete measure (red +). (left) continuous source and discrete target distributions. (center left) generated samples obtained by sampling from the source distribution and applying our estimated Monge map f . (center right) displacement field of the estimated optimal map: each arrow is proportional to f (xi) where (xi) is a uniform discrete grid. (right) level sets of the resulting density (approximated as a 2D histogram over 106 samples).
posed to optimize variations of the dual OT problem through stochastic gradient methods. Genevay et al. (2016) proposed to leverage the entropy-smoothed "semi-dual", which eliminates the block of variables corresponding to the continuous measure. However, their approach still requires O(n) operations per iteration and hence only scales moderately w.r.t. the size of the input measures. Arjovsky et al. (2017) proposed a formulation that is specific to the so-called 1-Wassertein distance (unregularized OT using the Euclidean distance as a cost function). This formulation has a simpler dual form with a single block of variables, which can be parameterized as a neural network. This approach scales better to very large datasets and handles continuous measures, enabling the use of OT as a loss between the mapping of a Gaussian distribution and a target empirical distribution. However, a drawback of that formulation is that the dual variable has to satisfy the non-trivial constraint of being a Lipshitz function. As a workaround, Arjovsky et al. (2017) proposed to use heuristic weight clipping between updates of the neural network parameters. However, this makes it unclear whether the learned generative model is truly optimized in an OT sense.
Contributions We present a novel two-step approach for learning a map f that satisfies f (X)  Y . First, we learn an optimal transport plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a new simple dual stochastic gradient algorithm for solving regularized OT which scales well with the size of the input measures. We provide numerical evidence that our approach converges faster than semi-dual approaches considered in (Genevay et al., 2016). Second, we estimate an optimal map (also referred as a Monge map) as a neural network learned by approximating the barycentric projection of the OT plan obtained in the first step. Parameterization of this map with a neural network allows efficient learning and generalizes even outside the support of the input measure. Fig. 1 provides a 2D example showing the computed map between a Gaussian measure and a discrete measure and the resulting density estimation. On the theoretical side, we prove the convergence of regularized optimal plans (resp. barycentric projections of regularized OT plans) to the optimal plan (resp. Monge map) between the underlying continuous measures from which data are sampled. We demonstrate our approach on domain adaptation and generative modeling.
Notations: We denote X and Y some complete metric spaces. In most applications, these are Euclidean spaces. We denote random variables such as X or Y as capital letters. We use X  Y to say that X and Y have the same distribution, and also X  µ to say that X is distributed according to the probability measure µ. Supp(µ) refers to the support of µ, a subset of X , which is also the set of values which X  µ can take. Given X  µ and a mapping f defined on Supp(µ), f #µ is the probability distribution of f (X). We say that a measure is continuous when it admits a density w.r.t. the Lebesgues measure. We denote id the identity map.
2 BACKGROUND ON OPTIMAL TRANSPORT
The Monge Problem Consider a cost function c : (x, y)  X × Y  c(x, y)  R+, and two random variables X  µ and Y   taking values in X and Y respectively. The Monge problem
2

Under review as a conference paper at ICLR 2018

(Monge, 1781) consists in finding a map f : X  Y which transports the mass from µ to  while minimizing the mass transfer cost

inf EXµ [c(X, f (X)] subject to f (X)  Y.
f

(1)

When µ is a discrete measure, a map f satisfying the constraint may not exist: if µ is supported on
a single point, no such mapping exists as soon as  is not supported on a single point. In that case, the Monge problem is not feasible. However, when X = Y = Rd, µ admits a density and c is the squared Euclidean distance, an important result by Brenier (1991) states that the Monge problem
is feasible and that the infinum of Problem (1) is attained. The existence and uniqueness of Monge
maps was later generalized to more general costs (e.g. strictly convex and super-linear) by several
authors. With the notable exception of the Gaussian to Gaussian case which has a close form affine solution, computation of Monge maps remains an open problem for measures supported on Rd with d > 3 as space-discretization approaches become intractable.

Kantorovich Relaxation In order to make Problem (1) always feasible, Kantorovich (1942) relaxed the problem by casting Problem (1) into a minimization over couplings (X, Y )   rather than over the set of mappings, where  should have marginals equals to µ and ,

inf


E(X,Y ) [c(X, Y )]

subject to X  µ, Y

 .

(2)

Concretely, this relaxation allows mass at a given point x  Supp(µ) to be transported to several locations y  Supp(), while the Monge problem would send the whole mass at x to a unique location f (x). This relaxed formulation is a linear program, which can be solved by specialized algorithms such as the network simplex when considering discrete measures. However, current implementations of this algorithm have a super-cubic complexity in the size of the support of µ and , preventing wider use of OT in large-scale settings.

Regularized OT OT regularization was introduced by Cuturi (2013) in order to speed up the computation of OT. Regularization is achieved by adding a negative-entropy penalty R to the primal variable  of Problem (2),

inf


E(X,Y ) [c(X, Y )] + R()

subject to X  µ,

Y

 .

(3)

Besides efficient computation through the block-coordinate Sinkhorn algorithm, regularization also makes the OT distance differentiable everywhere w.r.t. the input measures, whereas OT is differentiable only almost everywhere. We also consider the L2 regularization introduced by (Dessein et al., 2016), whose computation is found to be more stable since there is no exponential term causing overflow. As highlighted by Blondel et al. (2017), adding an entropy or squared L2 norm regularization term to the primal problem (3) makes the dual problem an unconstrained maximization problem. We use this dual formulation in the next section to propose an efficient stochastic gradient algorithm.

3 LARGE-SCALE OPTIMAL TRANSPORT

By considering the dual of the regularized OT problem, we first show that stochastic gradient ascent can be used to maximize the resulting concave objective. A close form for the primal solution  of Problem (3) can then be obtained by using first-order conditions.

3.1 DUAL STOCHASTIC APPROACH

OT dual Let X  µ and Y  . We denote the densities (w.r.t. the Lebesgues measure) of µ and  as a and b. When µ and  are discretes, a and b are vectors of non-negative weights summing to one. The Kantorovich duality provides the following dual of OT problem (2),

sup EXµ,Y  [u(X) + v(Y )]
uC(X ),vC(Y)

(4)

where the dual variables u and v are continuous functions such that u(x) + v(y) c(x, y) for all (x, y). This dual formulation suggests that stochastic gradient methods can be used to maximize the objective of Problem (4) by sampling batches from the independant coupling µ × . However

3

Under review as a conference paper at ICLR 2018

Algorithm 1 Stochastic OT computation
1: Inputs: input measures µ, ; cost function c; batch size n; learning rate . 2: Discrete case: µ = i aixi and u is a finite vectors: u(xi) d=ef. ui (and similarly for  and v) 3: Continuous case: µ is continuous measure and u is a neural network (and similarly for  and v)
and  indicates the gradient wrt the parameters 4: while not converged do 5: sample a batch (x1, · · · , xn) from µ 6: sample a batch (y1, · · · , yn) from  7: update u  u +  ij u(xi) + uF(u(xi), v(yj))u(xi) 8: update v  v +  ij v(yj) + vF(u(xi), v(yj))v(yj) 9: end while

there is no easy way to fulfill the constraint on u and v along gradient iterations. This motivates considering regularized OT.

Regularized OT dual In the present paper, we consider both the entropy regularization Re considered in (Genevay et al., 2016) and L2 regularization RL2,

Re() d=ef.
X ×Y

ln

(x, y) a(x)b(y)

-1

(x, y)dxdy,

RL2() d=ef.

(x, y)2 dxdy.
X ×Y a(x)b(y)

(5)

When µ and  are discrete, and so is , the integrals are replaced by sums. The dual of the regularized OT problems is the following,

sup E(X,Y )µ× [u(X) + v(Y ) + F(u, v)] ,
u,v

(6)

-e

1 

(u(X

)+v(Y

)-c(X,Y

))

(entropy reg.)

where F(u, v) =

-

1 4

(u(X

)

+

v(Y

)

-

c(X,

Y

))2+

(L2 reg.)

.

Compared to Problem (4), the constraint u(x)+v(y) c(x, y) has been relaxed and is now enforced

smoothly through a penalty term F(u, v) which is concave w.r.t. (u, v).

Primal-Dual relationship The previous section shows how to compute the regularized OT objective as well as the dual variables u and v. In order to recover the solution  of the regularized primal
problem (3), we can use the first-order optimality conditions of the regularized OT problem to obtain
as in (Blondel et al., 2017),

(x, y) = a(x)b(y)H(x, y) where H(x, y) =

e e eu(x) 

-

c(x,y) 

v(y) 

(entropy reg.)

1 2

(u(x) + v(y) - c(x, y))+

(L2 reg.)

. (7)

Algorithm The relaxed dual (6) is an unconstrained concave problem which can be maximized

through stochastic gradient methods by sampling batches according to µ×. When µ is discrete, µ =

n i=1

aixi ,

the

dual

variables

u

is

a

n-dimensional

vector

over

which

we

carry

the

optimization,

where u(xi) d=ef. ui. When µ has a density, u is a function on X which has to be parameterized in

order to carry optimization. We thus consider deep neural networks for their ability to approximate

general functions. The same discussion also stands for the second dual variable v. Our stochastic

gradient algorithm is detailed in Alg. 1.

The proposed algorithm is capable of computing regularized OT objective and optimal plans between empirical measures supported on arbitrary large numbers of samples. In statistical machine learning, one aims at estimating the underlying continuous distribution from which empirical observations have been sampled. In the context of optimal transport, one would like to approximate the true (non-regularized) optimal plan between the underlying measures measures. The next section states theoretical guarantees regarding this problem.

3.2 CONVERGENCE OF REGULARIZED OT PLANS

Consider discrete probability measures µn =

n i=1

aixi



P (X )

and

n

=

n j=1

bj

yj



P (Y).

Analysis of entropy-regularized linear programs (Cominetti & San Mart´in, 1994) show that the

4

Under review as a conference paper at ICLR 2018

solution n converges exponentially fast to a solution n of the non-regularized OT problem. Also, a result about stability of the optimal transport (Villani, 2008)[Theorem 5.20] states that, if µn  µ and n   weakly, then a sequence (n) of optimal transport plans between µn and n converges
weakly to a solution  of the OT problem between µ and . We can thus write,

lim
n

lim
0

n

=

.

(8)

A more refined result consists in establishing the weak convergence of n to  when (n, ) jointly converge to (, 0). This is the result of the following theorem which states a stability property of entropy-regularized plans (proof in the Appendix).

Theorem 1. Let µ  P (X ) and   P (Y) where X and Y are complete metric spaces. Let

µn =

n i=1

aixi

and

n

=

n j=1

bj

yj

be

discrete

probability

measures

which

converge

weakly

to µ and  respectively, and let (n) a sequence of non-negative real numbers converging to 0 sufficiently fast. Assume the cost c is continuous on X × Y and finite. Let nn the solution of the

entropy-regularized OT problem (3) between µn and n. Then, up to extraction of a subsequence, (nn ) converges weakly to the solution  of the OT problem (2) between µ and ,

nn   weakly.

(9)

Keeping the analogy with statistical machine learning, this result is an analog to the consistency property of some estimator. In most applications, we consider empirical measures, so that regularization, besides making the dual stochastic approach feasible, may also help learn the optimal plan between the underlying continuous measures.
So far, we have derived an algorithm for computing regularized OT objective and optimal plans regardless of µ and  being discrete or continuous. The OT objective has been used as a loss successfully in machine learning (Rolet et al., 2016; Arjovsky et al., 2017), whereas the use of optimal plans has obvious applications in logistics, as well as economy (Kantorovich, 1942; Carlier, 2012) or computer graphics (Bonneel et al., 2011). In numerous applications however, we often need mappings rather that joint distributions. This is all the more motivated since Brenier (1991) proved that when the source measure is continuous, the optimal transport plan is actually induced by a map. Hence in the next section, we investigate how to recover an optimal map, i.e. find an approximate solution of the Monge Problem (1), from regularized optimal plans.

4 OPTIMAL MAPPING ESTIMATIONS

A map can be obtained form a solution to the OT problem (2) or regularized OT problem (3) through
the computation of its barycentric projection. Indeed, a solution  of Problem (2) or (3) between a source measure µ and a target measure  is a plan  : (x, y)  X × Y  R+ which can be seen as a weighted one-to-many map, i.e.  send x to each location y  Supp() where (x, y) > 0. A
map can then be obtained by simply averaging over these y according to the weights (x, y).

Definition 1. (Barycentric projection) Let  be a solution of the OT problem (2) or regularized OT problem (3). The barycentric projection ¯ w.r.t. a cost d : Y × Y  R+ is defined as,

¯(x)

=

inf
z

EY

(·|x)(d(z,

Y

))

(10)

In the special case d(x, y) = ||x - y||22, Eq. (10) has the close form solution ¯(x) = EY (·|x) [Y ],

which is equal to ¯

=

yt a

in a discrete setting with y

=

(y1, · · · , yn) and a the weights of µ.

Formula (10) provides a pointwise value of the barycentric projection. When µ is discrete, this

means that we only have mapping estimations for a finite number of points. In order to define a map

which is defined everywhere, we parameterize the barycentric projection as a deep neural network.

We show in the next paragraph how to efficiently learn its parameters.

Optimal map estimation An estimation f of the barycentric projection which generalizes outside the support of µ can be obtained by learning a deep neural network which minimizes the following objective w.r.t. the parameters ,

EXµ EY (·|x) [d(Y, f(X))] = E(X,Y ) [d(Y, f(X))] = E(X,Y )µ× [d(Y, f(X))H(X, Y )]

(11)

5

Under review as a conference paper at ICLR 2018

Algorithm 2 Optimal Map estimation with SGD
Inputs: input measures µ, ; cost function c; dual optimal variables u and v; map f parameterized as a deep NN; batch size n; learning rate . while not converged do
sample a batch (x1, · · · , xn) from µ sample a batch (y1, · · · , yn) from  update    -  ij d(yj, f(xi))H(xi, yj) end while

When d(x, y) = ||x - y||2, the last term in Eq. (11) is simply a weighted sum of squared errors, with possibly an infinite number of terms whenever µ or  are continuous. We propose to minimize the objective (11) by stochastic gradient descent, which provides the simple algorithm Alg. 2. The OT problem being symmetric, we can also compute the opposite barycentric projection g w.r.t. to a cost d : X × X  R+ by minimizing EXµ,Y  [d(g(Y ), X)H(X, Y )].
When considering the special case c = d = ||x - y||22, the barycentric projection holds the property that ¯ is an optimal map (Ambrosio et al., 2006)[Theorem 12.4.4]. However, unless the plan  is induced by a map, the averaging process results in having the image of the source measure by ¯ only approximately equal to the target measure . Still, when the size of discrete measure is large and the regularization is small, we show in the next paragraph that 1) the barycentric projection of a regularized transport plan is close to the Monge map between the underlying continuous measures (Theorem 2) and 2) the image of the source measure by the barycentric projection should be close to the target measure  (Corollary 1).
Theoretical guarantees As stated earlier, when X = Y and c(x, y) = ||x - y||22, Brenier (1991) proved that when the source measure µ is continuous, there exists a solution to the Monge problem (1). This result was generalized to more general cost functions, see (Villani, 2008)[Corollary 9.3] for details. In that case, the plan  between µ and  is written as (id, f )#µ where f is the Monge map. Now considering discrete measures µn and n which converge to µ (continuous) and  respectively, we have proved in 1 that n converges weakly to  = (id, f )#µ when (n, )  (, 0). The next theorem, proved in the Appendix, shows that the barycentric projection n also converges weakly the true Monge map between µ and , justifying our approach.

Theorem 2. Let µ be a continuous probability measure on Rd, and  an arbitrary probability

measure

on

Rd

and

c

a

cost

function

satisfying

(Villani,

2008)[Corollary

9.3].

Let

µn

=

1 n

n i=1

xi

and n

=

1 n

n j=1

yj

converging weakly to µ and  respectively.

Let (n) a sequence of non-

negative real numbers converging sufficiently fast to 0 and ¯nn the barycentric projection w.r.t.

d = c of the solution nn of the entropy-regularized OT problem (3). Then, up to extraction of

a subsequence, the plan (id, ¯nn )#µn converges weakly to the plan (id, f )#µ , where f is the

solution of the Monge problem (1) between µ and .

This theorem shows that our estimated barycentric projection is close to an optimal map between the underlying continuous measures for n big and  small. The following corollary confirms the intuition that the image of the source measure by this map converges to the underlying target measure.
Corollary 1. With the same assumptions as above, ¯nn #µn   weakly.
In terms of random variables, the last equation states that if Xn  µn and Y  , then ¯nn (Xn) converges in distribution to Y . These theoretical results show that our estimated Monge mapping can thus be used to perform domain adaptation by mapping a source dataset to a target dataset nearly optimally, as well as perform generative modeling by mapping a continuous measure to a target discrete set. We demontrate this in the following section.
6

Under review as a conference paper at ICLR 2018

Training objective value

0.14 0.12 0.10 0.08 0.06 0.04 0.02 0.00
0

= 2.5 10 2
dual semi-dual 500 1t0im00e1(5s0e0co2n0d0s0)2500 3000

0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02 0.00 0

= 10 1
dual semi-dual 500 1t0im00e1(5s0e0co2n0d0s0)2500 3000

0.425 0.450 0.475 0.500 0.525 0.550 0.575 0.600 0

=1
dual semi-dual 500 1t0im00e1(5s0e0co2n0d0s0)2500 3000

Figure 2: Convergence plots of the the Stochastic Dual Algorithm 1 against a Stochastic Semi-Dual implementation (adapted from (Genevay et al., 2016): we use SGD instead of SAG), for several entropy-regularization values. Learning rates are {5., 20., 20.} and batch sizes {1024, 500, 100} respectively and are taken the same for the dual and semi-dual methods.

5 NUMERICAL EXPERIMENTS
5.1 DUAL VS SEMI-DUAL SPEED COMPARISONS
We start by evaluating the training time of our dual stochastic algorithm 1 against a stochastic semidual approach similar to (Genevay et al., 2016). In the semi-dual approach, one of the dual variable is eliminated and is computed in close form. However, this computation has O(N ) complexity where N is the size of the target measure . We compute OT with both methods on a spectral transfer problem, which is related to the color transfer problem (Bonneel et al., 2016), but where images are multispectral, i.e. they share a finer sampling of the light wavelength. We take two 500 × 500 images from the CAVE dataset (Yasuma et al., 2010) that have 31 spectral bands. As such, the optimal transport problem is computed on two empirical distributions of 250000 samples in R31. The timing evolution of train losses are reported in Figure 2 for three different regularization values  = {.1, 1, 5}. In the three cases, one can observe that convergence is much faster.

5.2 LARGE SCALE DOMAIN ADAPTATION

We apply here our computation framework on an unsupervised domain adaptation (DA) task, for which optimal transport has shown to perform well on small scale datasets (Courty et al., 2017; Perrot et al., 2016; Courty et al., 2014). This restriction is mainly due to the fact those works only consider the primal formulation of the OT problem. Our goal here is not to compete with the stateof-the-art methods in domain adaptation but to assess that our formulation allows to scale optimal transport based domain adaptation (OTDA) to large datasets. OTDA is illustrated in Fig. 3 and follows two steps: 1) learn an optimal transport mapping between the source and target distribution, 2) map the source samples and train a classifier on them in the target domain. Our formulation also allows to any differentiable ground cost c while (Courty et al., 2017) was limited to squared Euclidean ground cost.

Datasets

OT Domain Adaptation

Class 1 Class 2
Samples
Samples Classifier on

Samples Samples Classifier on

Figure 3: Illustration of the OT Domain Adaptation method adapted from (Courty et al., 2017). Source samples are mapped to the target set through the barycentric projection ¯. A classifier is
then learned on the mapped source samples.

7

Under review as a conference paper at ICLR 2018

Table 1: Experimental results (accuracy in %) on domain adaption among MNIST, USPS and SVHN datasets with entropy (Re) and L2 (RL2) regularization. Source only indicates the 1-NN classification between source and target samples before adaptation.

Method
Source only Bary. Proj. EMD Bary. Proj. Sinkhorn Bary. Proj. Alg. 1 with Re Bary. Proj. Alg. 1 with RL2 Est. Monge map Alg. 1+2 with Re Est. Monge map Alg. 1+2 with RL2

MNIST USPS 73.47 57.75 68.75 72.87 67.8 77.92 72.61

USPS MNIST 36.97 52.46 57.35 57.55 57.47 60.02 60.50

SVHN  MNIST 54.33
intractable intractable
58.87 60.56 61.11 62.88

Datasets We consider the three cross-domain digit image datasets MNIST (Lecun et al., 1998), USPS, and SVHN (Netzer et al., 2011), which consist of 10 classes of digits. For the adaptation between MNIST and USPS, we use 60000 samples in the MNIST domain and 9298 samples in USPS domain. MNIST images are resized to the same resolution as USPS (16 × 16). For the adaptation between SVHN and MNIST, we use 73212 samples in the SVHN domain and 60000 samples in the MNIST domain. MNIST images are zero-padded to reach the same resolution as SVHN (32 × 32) and extended to three channels to match SVHN image sizes. The labels in the target domain are withheld during the adaptation. In the experiment, we consider the adaptation in three directions: MNIST  USPS, USPS  MNIST, and SVHN  MNIST.
Methods and experimental setup Our goal is to demonstrate the potential of the proposed method in large scale settings. Adaptation performance is evaluated using a 1-Nearest Neighbor (1-NN) classifier, since it has the advantage of being parameter free and permits to measure more directly the quality of the adapted representation as discussed in (Courty et al., 2017). In all experiments, we consider as the baseline 1-NN classification, where labeled neighbors are searched in the source domain and the accuracy is computed on target data. We compare our approach with exact EMD and Sinkhorn (Cuturi, 2013) algorithms whenever their computation is tractable. Note that these methods do not provide out of sample mapping and require the whole dataset to perform the barycentric projection. We compute the estimated Monge map of our proposed approach with either entropy or L2 regularization. For the Monge map estimation in Alg. 2, we use two fully-connected hidden layers (d  200  500  d). For the adaptation between SVHN and MNIST, as a preprocessing step, we extract deep features by learning a modified LeNet architecture on the source data and extracting the 100 dimensional features output by the top hidden layer. Adaptation is performed on those features. We report for all the methods the best accuracy over the hyperparameters on the target dataset. While this setting is unrealistic in a practical DA application, it is widely used in the DA community (Long et al., 2013) and our goal is here to investigate the relative performances of large scale OTDA in a fair setting.
Results Experimental results are reported in Table 1. In all the cases, our proposed approach outperforms the existing EMD and Sinkhorn algorithms. On MNISTUSPS, the EMD and Sinkhorn algorithms are worse than the source only accuracy whereas our method leads to successful adaptation results with 20% and 10% accuracy points over EMD and Sinkhorn based methods respectively. On USPSMNIST, all three algorithms lead to successful adaptation results, but our method achieves the highest adaptation results. Finally, on the challenging large scale adaptation task SVHNMNIST, only our method is able to handle the whole datasets, and clearly outperforms the source only results.
Comparing the results between the barycentric projection and estimated Monge map illustrates that learning a parametric mapping provides some kind of regularization, and improves the performance. Finally we can see that the RL2 regularization that has never been investigated in DA works very well in practice, possibly because the optimization problem is more numerically stable.
5.3 GENERATIVE OPTIMAL TRANSPORT (GOT)
Approach Corollary 1 shows that when the support of the discrete measures µ and  is large and the regularization  is small, then we have approximately ¯#µ = . This observation motivates
8

Under review as a conference paper at ICLR 2018
Figure 4: Samples generated by our optimal generator learned through Algorithms 1 and 2.
the use of our Monge map estimation as a generator between an arbitrary continuous measure µ and a discrete measure  representing the discrete distribution of some dataset. We can thus obtain a generative model by first computing OT through Alg. 1 between a Gaussian measure µ and a discrete dataset  and then compute our generator with Alg. 2. This requires to have a cost function between the latent variable X  µ and the discrete variable Y  . The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost. Permutation-invariant MNIST We preprocess MNIST data by rescaling grayscale values in [-1, 1] and centering the rescaled dataset. We run Alg. 1 and Alg. 2 where µ is a Gaussian whose covariance is taken equal to the empirical covariance of the preprocessed MNIST dataset; we have observed that this makes the learning easier. The target discrete measure  is the preprocessed MNIST dataset. Permutation invariance means that we consider each grayscale 28 × 28 images as a 784d vector and do not rely on convolutional architectures. In Alg. 1 the dual potential u is parameterized as a (d  1024  1024  1) fully-connected NN with ReLU activation for each hidden layer, and the L2 regularization is considered as it produced experimentally less blurring. The barycentric projection f of Alg. 2 is parameterized as a (d  1024  1024  d) fullyconnected NN with ReLU activation for each hidden layer and a tanh activation on the output layer. We display some generated samples in Fig. 4.
6 CONCLUSION
We proposed two original algorithms that allow for i) large-scale computation of regularized optimal transport problems ii) finding a mapping in the input space that moves nearly optimally one probability distribution onto another (the so-called Monge map), with theoretical guarantees. We believe that these two contributions enable a large deployment of optimal transport strategies in deep learning frameworks. Notably, we have shown how it can be used in an unsupervised domain adaptation setting, or in generative modeling, where a Monge map acts directly as a generator. Although we have focused on machine learning applications, we believe the proposed algorithms may be useful in the many other areas where optimal maps are of interest.
REFERENCES
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare´. Gradient flows: in metric spaces and in the space of probability measures. Springer, 2006.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151­175, 2010.
Garrett Birkhoff. Three observations on linear algebra. Univ. Nac. Tucuma´n. Revista A, 5:147­151, 1946.
9

Under review as a conference paper at ICLR 2018
Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport. arXiv preprint arXiv:1710.06276, 2017.
Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. In ACM Transactions on Graphics (TOG), volume 30, pp. 158. ACM, 2011.
Nicolas Bonneel, Gabriel Peyre´, and Marco Cuturi. Wasserstein barycentric coordinates: Histogram regression using optimal transport. ACM Transactions on Graphics, 35(4), 2016.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics, 44(4):375­417, 1991.
Guillaume Carlier. Optimal transportation and economic applications. Lecture Notes.(Cited on page 2.), 2012.
Roberto Cominetti and Jaime San Mart´in. Asymptotic analysis of the exponential penalty trajectory in linear programming. Mathematical Programming, 67(1-3):169­187, 1994.
Nicolas Courty, Re´mi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 274­289. Springer, 2014.
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal Transport for Domain Adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9): 1853­1865, sep 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pp. 2292­2300, 2013.
Arnaud Dessein, Nicolas Papadakis, and Jean-Luc Rouas. Regularized optimal transport and the rot mover's distance. arXiv preprint arXiv:1610.06447, 2016.
Sira Ferradans, Nicolas Papadakis, Gabriel Peyre´, and Jean-Franc¸ois Aujol. Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3):1853­1882, 2014.
Aude Genevay, Marco Cuturi, Gabriel Peyre´, and Francis Bach. Stochastic optimization for largescale optimal transport. In Advances in Neural Information Processing Systems, pp. 3432­3440, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014.
Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 999­1006. IEEE, 2011.
Leonid Vitalievich Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk SSSR, volume 37, pp. 199­201, 1942.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. ISSN 00189219. doi: 10.1109/5.726791.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718­1727, 2015.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature learning with joint distribution adaptation. In Proceedings of the IEEE international conference on computer vision, pp. 2200­2207, 2013.
10

Under review as a conference paper at ICLR 2018
G. Monge. Memoire sur la theorie des deblais et des remblais. Histoire de l'Academie Royale des Sciences de Paris, 1781.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading Digits in Natural Images with Unsupervised Feature Learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.
Michae¨l Perrot, Nicolas Courty, Re´mi Flamary, and Amaury Habrard. Mapping Estimation for Discrete Optimal Transport. In Neural Information Processing System, Barcelone, Spain, December 2016.
Sebastian Reich. A nonparametric ensemble transform method for bayesian inference. SIAM Journal on Scientific Computing, 35(4):A2013­A2024, 2013.
Antoine Rolet, Marco Cuturi, and Gabriel Peyre´. Fast dictionary learning with a smoothed wasserstein loss. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pp. 630­638, 2016.
Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass transport for shape matching and comparison. IEEE transactions on pattern analysis and machine intelligence, 37(11):2246­2259, 2015.
Ce´dric Villani. Optimal transport: old and new, volume 338. Springer, 2008. Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, and Shree K Nayar. Generalized assorted pixel
camera: postcapture control of resolution, dynamic range, and spectrum. IEEE Transactions on Image Processing, 19(9):2241­2253, 2010.
11

Under review as a conference paper at ICLR 2018

Appendix

A DOMAIN ADAPTATION EXPERIMENT
Hyper-parameters and learning rate The value for the regularization parameter is set in {5, 2, 0.9, 0.5, 0.1, 0.05, 0.01}. Adam optimizer with batch size 1000 is used to optimize the network. The learning rate is varied in {2, 0.9, 0.1, 0.01, 0.001, 0.0001}. For the Monge map estimation, we used two (d  200  500  d) fully-connected layers and the network is optimized using Adam optimizer with the learning rate and batch size set to 0.0001, and 1000 respectively. For the Sinkhorn algorithm regularization value is chosen from {0.01, 0.1, 0.5, 0.9, 2.0, 5.0, 10.0}.

B PROOFS

Proof of Theorem 1.

Proof. Let n be the solution of the OT problem (2) between µn and n which has maximum entropy. A result about stability of optimal transport (Villani, 2008)[Theorem 5.20] states that, up
to extraction of a subsequence, n converges weakly to a solution  of the OT problem between µ and  (regardless of n being the solution with maximum entropy or not). We still write (n) this subsequence, as well as (nn ) the corresponding subsequence. Let g  Cb(X × Y) a bounded continuous function on X × Y. We have,

gdnn -

gd =

gdnn -

gdn +

gdn -

gd (12)

X ×Y

X ×Y

X ×Y

X ×Y

X ×Y

X ×Y

The second term in the right-hand side converges to 0 as a result of the already mentioned about stability of optimal transport. We now show the convergence of the first term to 0 when n  0 sufficiently fast. We have,

gdnn -

gdn =

g(xi, yj )nn (xi, yj ) -

g(xi, yj)n(xi, yj)

X ×Y

X ×Y

i=1,n j=1,n

i=1,n j=1,n

Mg |nn (xi, yj ) - n(xi, yj )|
ij
= Mg||nn - n||Rn×n,1

(13)

where Mg is an upper-bound of g. A convergence result by Cominetti & San Mart´in (1994) shows that there exists positive constants (w.r.t. n) Mcn,µn,n and cn,µn,n such that,

||nn - n||Rn×n,1

M ecn,µn,n

-

cn

,µn n

,n

(14)

where cn = (c(x1, y1), · · · , c(xn, yn)). The subscript indices indicate the dependences of each constant. Hence, we see that choosing any (n) such that the right-hand side of Eq. (14) tends to 0 provides the results. In particular, we can take

n

=

cn ,µn ,n ln(nMcn,µn,n )

(15)

which suffices to have the convergence of (13) to 0 for any bounded continuous function g  Cb(X × Y). This proves the weak convergence of nn to .

Proof of Theorem 2.

Proof. First, note that the existence of a Monge map between µ and  follows from the absolute continuity of µ and the assumptions on the cost functions c (Villani, 2008)[Corollary 9.3].

12

Under review as a conference paper at ICLR 2018

Let g  Cl(Rd × Rd) a Lipschitz function on Rd × Rd. Let n be the unique (by assumption) solution of the OT problem between µn and n. We have,

gd(id, ¯nn )#µn -

gd(id, f )#µ =

gd(id, ¯nn )#µn -

gd(id, ¯n)#µn

Rd ×Rd

Rd ×Rd

Rd ×Rd

Rd ×Rd

+

gd(id, ¯n)#µn -

gd(id, f )#µ

Rd ×Rd

Rd ×Rd

(16)

Since µn and n are uniform discrete probability measures supported on the same number of points, we know by (Birkhoff, 1946) that the optimal transport n is actually an optimal assignment Tn, so that we have n = (id, Tn)#µn. This also implies ¯n = Tn so that (id, ¯n)#µn = (id, Tn)#µn. Hence, the second term in the right-hand side of (16) converges to 0 as a result of the stability of
optimal transport (Villani, 2008)[Theorem 5.20]. Now, we show that the first term also converges to
0 for n converging sufficiently fast to 0. By definition of the pushforward operator,

gd(id, ¯nn )#µn-

gd(id, ¯n)#µn =

Rd ×Rd

Rd ×Rd

and we can bound,

g(x, ¯nn (x)dµn(x)- g(x, Tn(x))dµn(x)
X Rd
(17)

g(x, ¯nn (x))dµn(x) -
X

g(x, Tn(x))dµn(x) =
X

1 n

n

g(xi, ¯nn (xi))

-

1 n

n

g(xi, Tn(xi))

i=1 i=1

Kg||¯nn (xi) - Tn(xi)||Rd,2
i
= nKg||nn Yn - nYn||Rn×n,2
nKg||nn - n||R1/n2×n,2||Yn||R1/n2×d,2

(18)

where Yn = (y1, · · · , yn)t and Kg is the Lipschitz constant of g. The first inequality follows from g being Lipschitz. The next equality follows from the discrete close form of the barycentric projection. The last inequality is obtained through Cauchy-Schwartz. We can now use the same arguments as in the previous proof. A convergence result by Cominetti & San Mart´in (1994) shows that there exists positive constants (w.r.t. n) Mcn,µn,n and cn,µn,n such that,

||nn - n||R1/n2×n,2

M ecn,µn,n

-

cn

,µn n

,n

(19)

where cn = (c(x1, y1), · · · , c(xn, yn)). The subscript indices indicate the dependences of each

constant. Hence, we see that choosing any (n) such that (19) tends to 0 provides the results. In

particular, we can take

n

=

cn ,µn ,n ln(n2||Yn||1R/n2×d,2Mcn,µn,n )

(20)

which suffices to have the convergence of (13) to 0 for Lipschitz function g  Cl(Rd × Rd). This proves the weak convergence of (id, ¯nn )#µn to (id, f )#µ.

13

