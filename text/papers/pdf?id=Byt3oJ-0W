Under review as a conference paper at ICLR 2018
LEARNING LATENT PERMUTATIONS WITH GUMBELSINKHORN NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator. Sinkhorn operator is attractive because it functions as a simple, easy-to-implement analog of the softmax operator. With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al., 2016; Maddison et al., 2016) to distributions over latent matchings. We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.
1 INTRODUCTION
In principle, deep networks can learn arbitrarily sophisticated mappings from inputs to outputs. However, in practice we must encode specific inductive biases in order to learn accurate models from limit data. In a variety of recent research efforts, practitioners have provided models with the ability to explicitly manipulate latent combinatorial objects such as stacks (Dyer et al., 2015; Joulin & Mikolov, 2015), memory slots (Graves et al., 2014; Sukhbaatar et al., 2015), mathematical expressions (Neelakantan et al., 2015), program traces (Gaunt et al., 2016; Bosnjak et al., 2017), and first order logic (Rockta¨schel & Riedel, 2017). Operations on these discrete objects can be approximated using differentiable operations on continuous relaxations of the objects. As such, these operations can be included as modules in neural network models that can be trained end-toend by gradient descent.
Matchings and permutations are a fundamental building block in a variety of applications, as they can be used to align, canonicalize, and sort data. Prior work has developed learning algorithms for supervised learning where the training data includes annotated matchings (Caetano et al., 2009; Petterson et al., 2009; Tang et al., 2016). However, we would like to learn models with latent matchings, where the matching is not provided to us as supervision. This is a common and relevant setting. For example, Linderman et al. (2017) showed a problem from neuroscience involving the identification of neurons from the worm C. elegans can be cast as the inference of latent permutation on a larger hierarchical structure.
Unfortunately, maximizing the marginal likelihood for problems with latent matchings is very challenging. Unlike for problems with categorical latent variables, we cannot obtain unbiased stochastic gradients of the marginal likelihood using the score function estimator (Williams, 1992), as computing the probability of a given matching requires computing an intractable partition function for a structured distribution. Instead, we draw on recent work that obtains biased stochastic gradients by relaxing the discrete latent variables into continuous random variables that support the reparametrization trick (Jang et al., 2016; Maddison et al., 2016).
Our contributions are the following: first, in Section 2 we present a theoretical result showing that the non-differentiable parameterization of a permutation can be approximated in terms of a differentiable relaxation, the so-called Sinkhorn operator. Based on this result, in Section 3 we introduce Sinkhorn networks, which generalize the work of method of Adams & Zemel (2011) for predict-
1

Under review as a conference paper at ICLR 2018

ing rankings, and complements the concurrent work by Cruz et al. (2017), by focusing on more fundamental aspects. Further, in Section 4 we introduce the Gumbel-Sinkhorn, an analog of the Gumbel Softmax distribution (Jang et al., 2016; Maddison et al., 2016) for permutations. This enables optimization of the marginal likelihood by the reparametrization trick. Finally, in Section 5 we demonstrate that our methods outperform strong neural network baselines on the tasks of sorting numbers, solving jigsaw puzzles, and identifying neural signals from C. elegans worms.

2 THE SINKHORN OPERATOR: AN ANALOG OF THE SOFTMAX FOR
PERMUTATIONS

One sensible way to approximate a discrete category by continuous values is by us-
ing a temperature-dependent softmax function, component-wise defined as softmax (x)i = exp(xi/ )/ j=1 exp(xj/ ). For positive values of  , softmax (x)i is a point in the probability simplex. Also, in the limit   0, softmax (x)i converges to a vertex of the simplex, a one-hot vector corresponding to the largest xi 1. This approximation is a key ingredient in the successful implementations by Jang et al. (2016); Maddison et al. (2016), and here we extend it to permutations.

To do so, we first state an analog of the normalization implemented by the softmax. This is achieved through the Sinkhorn operator (or Sinkhorn normalization, or Sinkhorn balancing), which iteratively normalizes rows and columns of a matrix. Following Adams & Zemel (2011), we define the Sinkhorn operator S(X) over an N dimensional square matrix X as:

S0(X) = exp(X),

(1)

Sl(X) = Tc Tr(Sl-1(X)) ,

S(X) = lim Sl(X).
l

where Tr(X) = X (X1N 1N ), and Tc(X) = X (1N 1N X) as the row and column-wise normalization operators of a matrix, with denoting the element-wise division and 1N a column vector of ones. Then, we define the Sinkhorn operator S(·) as follows:

Sinkhorn (1964) proved that S(X) must belong to the Birkhoff polytope, the set of doubly stochastic matrices, that we denote BN 2.

Building on our analogy with categories, notice that choosing a category can always be cast as a

maximization problem: the choice arg maxi xi v being a one-hot vector), i.e. the maximizing

is v

the one that maximizes the is the one that indexes the

function x, v (with largest xi. Similarly,

one may parameterize the choice of a permutation P through a square matrix X, as the solution to

the linear assignment problem (Kuhn, 1955), with PN denoting the set of permutation matrices and A, B F = trace(A B) the (Frobenius) inner product of matrices:

M (X) = arg max P, X F .
P PN

(2)

We call M (·) the matching operator, which allow us to parameterize the hard choice of a permutation (see Figure 1a for an example). Our theoretical contribution is to show that M (X) can be obtained as the limit of S(X/ ), meaning that one can approximate M (X)  S(X/ ) with a small  . Theorem

1 summarizes our finding. We provide a rigorous proof in appendix A; briefly, it is based on showing that S(X/ ) solves a certain entropy-regularized problem in Bn, which in the limit converges to the matching problem in equation 2.

Theorem 1. For a doubly-stochastic matrix P , define its entropy as h(P ) = - i,j Pi,j log (Pi,j).

Then, one has,

S(X/ ) = arg max P, X F +  h(P ).
P BN

(3)

Now, assume also the entries of X are drawn independently from a distribution that is absolutely

continuous with respect to the Lebesgue measure in R. Then, almost surely, the following conver-

gence holds:

M (X) = lim S(X/ ).
 0+

(4)

1With the exception of the degenerate case of ties.

2This theorem requires certain technical conditions which are trivially satisfied if X has positive entries,

motivating the use of the component-wise exponential exp(·) in the first line of equation 1.

2

Under review as a conference paper at ICLR 2018

Finally, we note that Theorem 1 cannot be realized in practice, as it involves a limit on the Sinkhorn iterations l. Instead, we'll always consider the incomplete version of the Sinkhorn operator (Adams & Zemel, 2011), where we truncate l in (1) to L. Figure 1b illustrates the dependence of the approximation in  and L.

3 SINKHORN NETWORKS

Here we show how to apply the approximation stated in Theorem 1 in the context of artificial neural networks. We construct a layer that encodes the representation of a permutation, and show how to train networks containing such layers as intermediate representations.

We define the components of this network through a minimal example: consider the supervised task
of learning a mapping from scrambled objects X~ to actual, non-scrambled X. Data, then, are M
pairs (Xi, X~i) where X~i can be constructed by randomly permuting pieces of Xi. We state this problem as a permutation-valued regression Xi = P-,X1~i X~i + i, where i is a noise term, and P,X~i is the permutation matrix that maps Xi to X~i, and that depends on X~i and some parameters . We are concerned with minimization of the reconstruction error 3:

M

f (, X, X~ ) =

||Xi - P-,X1~i X~i||2.

i=1

(5)

One way to express a complex parameterization of this kind is through a neural network: this network receives X~i as input, which is then passed through some intermediate, feed-forward computations of the type gl(Wlxl + bl), where gl are nonlinear activation functions, xl is the output of a previous layer, and  = {(Wl, bl)} are the network parameters. To make the final network output be a permutation, we appeal to constructions developed in section 2: by assuming that the
final network output P,X~ can be parameterized as the solution of the assignment problem; i.e., P,X~ = M (g(X~ , )), where g(X~ , ) is the output of the computations involving gl.
Unfortunately, the above construction involves a non-differentiable f (in ). We use Theorem 1 as a justification for replacing M (g(X~ , )) by the differentiable S(g(X~ , )/ ) in the computational
graph. The value of  must be chosen with caution: if  is too small, gradients vanishes almost everywhere, as S(g(X~ , )/ ) approaches the non-differentiable M (g(X~ , )). Conversely, if  is too large, S(X/ ) may be far from the vertices of the Birkhoff polytope, and reconstructions P-,X1~ X~ may be nonsensical (see Figure 2a). Importantly, we will always add noise to the output layer g(X~ , ) as a regularization device: by doing so we ensure uniqueness of M (g(X~ , )), which is
required for convergence in Theorem 1.

3.1 PERMUTATION EQUIVARIANCE
Among all possible architectures that respect the aforementioned parameterization, we will only consider networks that are permutation equivariant, the natural kind of symmetry arising in this context. Specifically, we require networks to satisfy:
P,P (X~ ) P X~ = P P,X~ X~
where P is an arbitrary permutation. The underlying intuition is simple: reconstructions of objects should not depend on how pieces were scrambled, but only on the pieces themselves. We achieve permutation equivariance by using the same network to process each piece of X~ , which we require to have an N dimensional output. Then, these N outputs (each with N components) are used to create the rows of the matrix g(X~ , ), to which we finally apply the (differentiable) Sinkhorn operator. One can interpret each row as representing a vector of local likelihoods of assignment, but they might be inconsistent. The Sinkhorn operator, then, mixes those separate representations, and ensures that consistent (approximate) assignment are produced.
3This error arises from gaussian i. Other choices may be possible, but here we stick to the most straightforward formulation

3

Under review as a conference paper at ICLR 2018
(a) (b) (c) Sample 1 Sample 2 Sample 3 Sample 4 Sample 5

Figure 1: Illustrating the Matching and Sinkhorn operators, and the Gumbel-Matching and GumbelSinkhorn distributions. Each 5x5 grid represent a matrix, with the shading indicating cell values (a) Matching operator M (X) applied to a parameter matrix X. (b) Sinkhorn Operator S(X/ ) approximating M (X) for different temperature  and number of Sinkhorn iterations, L. (c). First row: samples from the Matching Sinkhorn distribution. Second and third rows: samples from the Gumbel-Sinkhorn distribution at two temperatures. At low temperature, both distributions are indistinguishable.

With permutation equivariance, the only consideration left to the practitioner is the choice of the particular architecture, which will depend on the particular kind of data. In Section 5 we illustrate the uses of Sinkhorn networks with three examples, each of them using a different architecture.

3.2 SUMMARY
Sinkhorn network is a supervised method for learning to reconstruct a scrambled object X~ (input) given several training examples (Xi, X~i). By applying some non-linear transformations, a Sinkhorn network richly parameterizes the mapping between X~ and the permutation P that once applied to X~ , will allow to reconstruct the original object as Xrec = P X~ (the output). We note that Sinkhorn networks may be similarly used not only to learn permutations, but also to learn matchings between objects in two sets of the same size.

4 PROBABILISTIC ASPECTS: THE GUMBEL-SINKHORN AND GUMBEL-MATCHING DISTRIBUTIONS

Recently, in Jang et al. (2016) and Maddison et al. (2016), the Gumbel-Softmax or Concrete distributions were defined for computational graphs with stochastic nodes; i.e, latent probabilistic representations. Their choice is guided by the following i) they seek re-parameterizable distributions to enable the re-parameterization trick (Kingma & Welling, 2013), ii) they note that via the Gumbel trick (see below) any categorical distribution is re-parameterizable, with the Gumbel distribution as the noise distribution, iii) since the re-parameterization in ii) is not differentiable, they consider instead sampling under the softmax approximation. This gives rise to the well-studied GumbelSoftmax distribution.

Here we parallel these choices to enable learning of a probabilistic latent representation of permutations. To this aim, we start by considering a generic distribution on the discrete set Y, with potential function X : Y  R:

p(y|X)  exp (X(y)) 1yY .

(6)

Regarding ii), the Gumbel trick arises in the context of Perturb and MAP methods (Papandreou & Yuille, 2011) for sampling in discrete graphical models. This has recently received renewed interest (Balog et al., 2017), as it recasts the a difficult sampling problem as an easier optimization problem. In detail, sampling from (6), can be achieved by the maximization of random perturbations of each potential X(y), with Gumbel i.i.d. noise (y); i.e., arg maxyY {X(y) + (y)}  p(·|X). Therefore, one can re-parameterize any categorical distribution (corresponding to (6) with X(y) = X, y ) by the choice of a category, after injecting noise.

4

Under review as a conference paper at ICLR 2018

Test distribution
U (0, 1) U (0, 1) (Vinyals et al., 2015)
U (0, 10) U (0, 1000) U (1, 2) U (10, 11) U (100, 101) U (1000, 1001)

N =5
.0 .06
.0 .0 .0 .0 .0 .0

N = 10
.0 0.43
.0 .0 .0 .0 .0 .0

N = 15
.0 0.9
.0 .0 .0 .0 .01 .07

N = 80
.0 -
.0 .01 .01 .08 .02 1.

N = 100
.0 -
.02 .02 .04 .08 .99 1.

N = 120
.01 -
.03 .04 .08 .6 1. 1.

Table 1: Results on the number sorting task. In the top two rows we compare to Vinyals et al. (2015), showing that our approach can sort far more inputs at significantly higher accuracy. In the bottom rows we evaluate generalization to different intervals on the real line.

However, the above scheme is unfeasible in our context, as |Y| = N !. Nonetheless, we appeal to

an interesting result: in cases where Y factorizes, Y =

N i=1

Yi

4,

the

use

of

rank-one

perturbations

(y) =

N i=1

i(yi)

is

proposed

as

a

more

tractable

alternative.

Although

ultimately

heuristic,

they

lead to bounds in the partition function (Hazan & Jaakkola, 2012; Balog et al., 2017), and can also

be understood as providing approximate or unbiased samples from the true density (Hazan et al.,

2013; Tomczak, 2016).

Guided by this, we say the random permutation P follows the Gumbel-Matching distribution with parameter X, denoted P  G.M(X), if it has the distribution arising by the rank-one perturbation of (6) on permutations, with the linear potential X(P ) = X, P F (replacing y with P ). One can verify, in a similar line as in Li et al. (2013), that M (X + )  G.M(X), if  is a matrix of standard i.i.d. Gumbel noise.

Regarding iii) with the categorical case, Gumbel-Matching distribution samples are not differentiable in X, but by appealing to Theorem 1, we define its relaxation for doubly stochastic matrices as follows: we say P follows the Gumbel-Sinkhorn distribution with parameter X and temperature  , denoted P  G.S(X,  ), if it has the distribution of S((X + )/ ). Samples of G.S(X,  ) converge almost surely to samples of the Gumbel-Matching distribution, as illustrated in Figure 1c.

Unfortunately, unlike for the categorical case, neither the Gumbel-Matching nor Gumbel-Sinkhorn distributions have tractable densities, which is a traditional requirement for computing the relevant quantities arising in stochastic nodes. However, this does not preclude inference: likelihoodfree methods have recently been developed to enable learning in such implicitly defined distributions (Ranganath et al., 2016; Tran et al., 2017). These methods avoid evaluating the likelihood based on the observation that in many cases inference can be cast as the estimation of a likelihood ratio, which can be obtained from samples (Husza´r, 2017). Regardless of these useful advances, here we develop a solution based on using the likelihoods of random variables whose densities are available, but we defer the details to Section 5.4.

5 EXPERIMENTS

The first three experiments of this section explore different Sinkhorn network architectures of increasing complexity, i.e., they mostly implements section 3. The fourth experiment relates to the probabilistic constructions described in section 4, and addresses a problem involving marginal inferences over a latent, unobserved permutation. All experimental details not stated here are in appendix B.

5.1 SORTING NUMBERS
To illustrate the capabilities of Sinkhorn Networks in a simple scenario, we consider the task of sorting numbers using artificial neural networks as in Vinyals et al. (2015). Specifically, we sample uniform random numbers X~ in the [0, 1] interval and we train our network with pairs (X~ , X) where
4It suffices that Y is a subset of the product space, which here is true as Y = Pn  {1, . . . , N }N .

5

Under review as a conference paper at ICLR 2018

MNIST

Celeba

Imagenet

2x2 3x3 4x4 5x5 6x6 2x2 3x3 4x4 5x5 2x2 3x3

Kendall tau

1.

Kendall tau

(Cruz et al., 2017) -

.83 .43 .39 .27 1.0 .96 .88 .78 .85 .73 - - - - - - - - - .72

Prop. wrong

.0 .09 .45 .45 .59 .0 .03 .1 .21 .12 .26

Prop. any wrong .0 .28 .97 1. 1. .0 .09 .36 .73 .19 .53

l1 .0 .0 .04 .02 .03 .0 .01 .04 .08 .05 .12

l2 .0 .0 .26 .18 .19 .0 .11 .18 .24 .22 .31

Table 2: Jigsaw puzzle results. We compare to the available result on the Kendall Tau metric from Cruz et al. (2017) and provide additional results from our experiments. Note that our model has at least 20x fewer parameters.

X are the same X~ but in sorted order. The network has a first fully connected layer that links a number with an intermediate representation (with 32 units), and a second (also fully connected) layer that turns that representation into a row of the matrix g(X~ , ).
Table 1 shows our network learns to sort up to N = 120 numbers. As an evaluation measure, we report the proportion of sequences where there was at least one error. Surprisingly, the network learns to sort numbers even when test examples are not sampled from U (0, 1), but on a considerably different interval. This indicates the network is not overfitting. These results can be compared with those from Vinyals et al. (2015), where a much more complex (recurrent) network was used, but performance guarantees were obtained only with at most N = 15 numbers. In that case, the reported error rate is 0.9, whereas ours starts to degrade only after  N = 100 numbers for most test intervals.
5.2 JIGSAW PUZZLES
A more complex scenario for learning permutations arises in the reconstruction of an image X from a collection of scrambled "jigsaw" pieces X~ (Noroozi & Favaro, 2016; Cruz et al., 2017). In this example, our network differs from the one in 5.1 in that we add a simple CNN (convolution + max pooling) layer, which maps the puzzle pieces to an intermediate representation.
For evaluation on test data, we report l1 and l2 (train) losses and the Kendall tau, a "correlation coefficient" for ranked data. In Table 2, we benchmark results for the MNIST, Celeba and Imagenet datasets, with puzzles between 2x2 and 6x6 pieces. In MNIST we achieve very low l1 and l2 errors on up to 6x6 puzzles, although a high proportion of errors. This is a consequence of our loss being agnostic to particular permutations, but only caring about reconstruction errors: as the number of black pieces increases with the number of puzzle pieces, most of them become unidentifiable under this loss.
In Celeba, we are able to solve puzzles of up to 5x5 pieces with only 21% of pieces of faces being incorrectly ordered (see Figure 2a for examples of reconstructions). For this dataset, we provide additional baselines in Table 4 of appendix C.1: there, we show that performance substantially decreases if the temperature is too small or large, but only slightly decreases if only one Sinkhorn iterations is made. We observe that temperature does play a relevant role, consistent with the findings of Maddison et al. (2016); Jang et al. (2016). This might not be obvious a-priori, as one could reason that temperature over-parameterizes the network. However, results confirm this is not the case. We hypothesize that different temperatures result in parameter convergence in different phases or regions. Also, the minor difference for a single iteration suggest that only a few might be necessary, implying potential savings in the memory needed to unroll computations in the graph, during training.
Learning in the Imagenet dataset is much more challenging, as there isn't a sequential structure that generalizes among images, unlike Celeba and MNIST. In this dataset, our network ties with the .72 Kendall tau score reported in (Cruz et al., 2017). Their network, named DeepPermNet, is based on the stacking of up to the sixth fully connected layer fc6 of AlexNet (Krizhevsky et al., 2012), which
6

Under review as a conference paper at ICLR 2018
Figure 2: (a) Sinkhorn networks can be trained to solve Jigsaw Puzzles. Given a trained model, `soft' reconstructions are shown at different  using S(X/ ). We also show hard reconstructions, made by computing M (X) with the Hungarian algorithm (Munkres, 1957). (b) Sinkhorn networks can also be used to learn to transform any MNIST digit into another. We show hard and soft reconstructions, with  = 1.
finally (fully) connects to a Sinkhorn layer through intermediate fc7 and fc8. We note, however, our network is much simpler, with only two layers and far fewer parameters. Specifically, the network that produced our best results had around 1,050,000 parameters (see appendix B for a derivation), while in DeepPermNet, the layer connecting fc6 with fc7 has 512 × 4096 × 9  19, 000, 000 parameters, let alone the AlexNet parameters (also to be learned). Indeed, we believe there is no reason to consider a complex stacking of convolutions: as the number of pieces increases, each piece is smaller and the convolutional layer eventually becomes fully connected. In the following experiment we explore this phenomenon in more detail.
5.3 ASSEMBLY OF ARBITRARY MNIST DIGITS FROM PIECES
We also consider an original application, motivated by the observation that the Jigsaw Puzzle task becomes ill-posed if a puzzle contains too many pieces. Indeed, consider the binarized MNIST dataset: there, reconstructions are not unique if pieces are sufficiently atomic, and in the limit case of pieces of size 1x1 squared pixels, for a given scrambled MNIST digit there are as many valid reconstructions as there are MNIST digits with the same number of white pixels. In other words, reconstructions stop being probabilistic and become a multimodal distribution over permutations. We exploit this intuition to ask whether a neural network can be trained to achieve arbitrary digit reconstructions, given their loose atomic pieces. To address this question, we slightly changed the network in 5.2, this time stacking several second layers linking an intermediate representation to the output. We trained the network to reconstruct a particular digit with each layer, by using digit identity to indicate which layer should activate with a particular training example. Our results demonstrate a positive answer: Figure 2b shows reconstructions of arbitrary digits given 10x10 scrambled pieces. In general, they can be unambiguously identified by the naked eye. Moreover, this judgement is supported by the assessment of a neural network. Specifically, we trained a two-layer CNN 5 on MNIST (achieving a 99.2% accuracy on test set) and evaluated its performance on the test set generated by arbitrary transformations of each digit of the original test set into any other digit. We found the CNN made an appropriate judgement in 85.1% of the time. More specific results, regarding specific transformations are presented in Table 5 of appendix C.2. Finally, we note that meaningful assemblies are possible regardless of the original digit (Figure 3 of appendix C.2).
5.4 POSTERIOR INFERENCE OVER PERMUTATIONS USING THE GUMBEL-SINKHORN
ESTIMATOR
We illustrate how the Gumbel-Sinkhorn distribution can be used as a continuous relaxation for stochastic nodes in a computational graph. To this end, we revisit the " C. elegans neural identification problem", originally introduced in Linderman et al. (2017). We refer the reader to (Linderman et al., 2017) for an in-depth introduction, but briefly, C. elegans is a nematode (worm) whose bio-
5Specifically, we used the one described in the Deep MNIST for experts tutorial.
7

Under review as a conference paper at ICLR 2018

Prop. known neurons
Difficulty
MCMC (Linderman et al., 2017) Gumbel-Sinkhorn Gumbel-Sinkhorn, no regularization

40.%
Easy Hard
.85 .82 .97 .95 .97 .96 .96 .93

30.%
Easy Hard
.51 .44 .90 .85 .92 .84 .89 .78

20.%
Easy Hard
.29 .27 .77 .59 .76 .59 .71 .52

10.%
Easy Hard
.16 .12 .39 .21 .44 .26 .4 .23

Table 3: Results for the C. elegans neural inference problem.

logical neural configuration ­ the connectome ­ is stereotypical; i.e. specimens always posses the same number of somatic neurons (282) (Varshney et al., 2011), and the ways those neurons connect and interact changes little from worm to worm. Therefore, its brain can be thought of as a canonical object, and its neurons can unequivocally be identified with names.
The task, then, consists of matching traces from the observed neural dynamics Y to identities (neuron names) in the canonical brain. This problem is stated in terms of a Bayesian hierarchical model, in order to profit from prior information that may constrain the possibilities. Specifically, one states a linear dynamical system Yt = P W P TYt-1 + t, where t is a noise term and W and P are latent variables with respective prior distributions. W encodes the dynamics, with a prior p(W ) to represent the sparseness of the connectome, etc., and P is a permutation matrix representing the matching between indexes of observed neurons and their canonical counterparts, where we place a flat prior p(P ) over permutations. Notably, within the framework it is possible to model the simultaneous problem with many worms sharing the same dynamical system, but here we avoid explicit references to individuals for notational ease.
Given this model, we seek the posterior distribution p({P, W }|Y ), a problem that we address with variational inference (Blei et al., 2017). We consider the mean-field variational family q(P, W ) = q(P )q(W ), where q(W ) is Gaussian, and for q(P ) we use a G.S(X,  ) distribution as a relaxation for the G.M.(X) distribution. Likewise, we relax our flat permutation prior over the set of permutations Pn using the isotropic G.S.(X = 0, prior) distribution.
Our goal, then, reduces to maximizing the evidence lower bound (ELBO) of log p(Y ). This ELBO contains an intractable KL (q(P )||p(P )). We solve this problem by treating as stochastic node the variable (X + )/ instead of P  G.M.(X). This approximation comes at the cost of a theoretically less tight lower bound, but in practice this loss of tightness is minimal, as commented in detail in appendix C.3 (with a focus on the categorical case).
In Table 3 (and also in Table 7 of appendix C.4) we show results for the C. elegans task, using accuracy in matching as the performance measure. These are broken down by relevant experimental covariates (Linderman et al., 2017): different proportion of neurons known beforehand, and by task difficulty. As baselines, we include i) a simple MCMC sampler that proposes local swipes on permutations ii) the rounding method presented in Linderman et al. (2017), iii) our method, where we also consider the absence of regularization. Results show our method outperforms the alternatives in most cases. MCMC fails because mixing is poor, but differences are much subtler with the other baselines. With them, we see that clear differences with the no-regularization case confirm the stochastic nature of this problem, i.e., that it is truly necessary to represent a latent probabilistic permutation. We believe our method outperforms the one in Linderman et al. (2017) because theirs, although it provides a explicit density, is a less tight relaxation, in the sense that points can be anywhere in the space, and not only on the Birkhoff polytope. Therefore, their prior also needs to be defined on the entire space and may not property act as an efficient regularizer.
6 RELATED WORK
Learning with matchings has been extensively been studied in the machine learning community; but current applications mostly relate to structured prediction (Petterson et al., 2009; Tang et al., 2016). However, our probabilistic treatment focuses on marginal inference in a model with a latent matching. This is a more challenging scenario, as standard learning techniques, i.e. the score func-
8

Under review as a conference paper at ICLR 2018
tion estimator or REINFORCE (Williams, 1992), are not applicable due to the partition function for non-trivial distributions over matchings.
In the case of latent categories, a recent technique that combines a relaxation and the reparameterization trick (Kingma & Welling, 2013) was proposed as a competitive alternative to REINFORCE for the marginal inference scenario. Specifically, Maddison et al. (2016); Jang et al. (2016) use the Gumbel-trick to re-parameterize a discrete density, and then replace it with a relaxed surrogate, the Gumbel Softmax distribution, to enable gradient-descent. Our work, like the simultaneous work of Linderman et al. (2017), aims to extends the scope of this technique to latent permutations. We deem our Gumbel Sinkhorn distributions as the most natural tractable extension of the Gumbel Softmax distribution (Maddison et al., 2016; Jang et al., 2016) to permutations, as we clearly parallel each of the steps leading to its construction. A parallel is also presented in Linderman et al. (2017); and notably, unlike ours, their framework produces tractable densities. However, it is less clear how their constructions extend each of the features of the Gumbel Softmax: for example, their rounding-based relaxation also utilizes the Sinkhorn operator, but the limit they consider does not make use of the non-trivial statement of Theorem 1, which naturally extends the categorical case (see appendix A.2 for details). In practice, we see our results favor the Gumbel Sinkhorn distribution, since it is a tighter relaxation.
Connections between permutations and the Sinkhorn operator have been known for at least twenty years. Indeed, the limit in Theorem 1 was first presented in Kosowsky & Yuille (1994), but their interpretation and motivation were more linked to statistical physics and economics. We note, however, that a connection between the approach by Kosowsky & Yuille (1994) and ours exists in the light of recent theory on Optimal Transport (OT): the work of Genevay et al. (2016) introduces several related versions of an entropy regularized optimal transport problem, and their so-called `semidual' essentially corresponds to the one introduced in Kosowsky & Yuille (1994). Indeed, Theorem 1 draws on the entropy-regularization for OT technique developed in Cuturi (2013). In Appendix D we comment on the relations between our work and recent developments in OT, and how this technique may be applied to other combinatorial problems as an alternative to current applications, mostly concerned with learning of generative models (Arjovsky et al., 2017).
We understand our work as extending Adams & Zemel (2011), which developed neural networks to learn a permutation-like structure; a ranking. However, there, as in Helmbold & Warmuth (2009), the objective function was linear and the Sinkhorn operator was instead used as an approximation of a matrix of the marginals, i.e., S(P )  E(P ). In consequence, there was no need to introduce a temperature parameter and consider a limit argument, which is critical to our case. Sinkhorn networks were also very recently introduced in Cruz et al. (2017), although their work substantially differs from ours. While their interest lies in the representational aspects of CNN's, we are more concerned with the more fundamental properties. On the Jigsaw puzzle task, we showed that we achieve equivalent performance with a much simpler network having several times fewer parameters and layers. Nonetheless, we recognize the need for more complex architectures for the tasks considered in Cruz et al. (2017), and we hope our more general theory; particularly, Theorem 1 and the notion of equivariance, may aid further developments in that direction.
7 DISCUSSION
We have demonstrated Sinkhorn networks are able to learn to find the right permutation in the most elementary cases; where all training samples obey the same sequential structure; e.g., in sorted number and in pieces of faces, as we expect parts of faces occupy similar positions from sample to sample. This is already non-trivial, as indicates one can train a neural network to solve the linear assignment problem.
However, the fact that Imagenet represented a much more challenging scenario indicates there are clear limits to our formulation. As the most obvious extension we propose to introduce a sequential stage, in which current solutions are kept on a memory buffer, and improved. That sequential improvement would help solve the "Order Matters" problem (Vinyals et al., 2015), and we deem our elementary work as a significant step in that direction.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, and Adrian Weller. Lost relatives of the gumbel trick. arXiv preprint arXiv:1706.04161, 2017.
Garrett Birkhoff. Tres observaciones sobre el algebra lineal. Univ. Nac. Tucuma´n. Revista A, 5: 147­151, 1946.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, (just-accepted), 2017.
Matko Bosnjak, Tim Rockta¨schel, Jason Naradowsky, and Sebastian Riedel. Programming with a differentiable forth interpreter. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 547­556, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/bosnjak17a.html.
Tibe´rio S Caetano, Julian J McAuley, Li Cheng, Quoc V Le, and Alex J Smola. Learning graph matching. IEEE transactions on pattern analysis and machine intelligence, 31(6):1048­1058, 2009.
Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual permutation learning. arXiv preprint arXiv:1704.02729, 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pp. 2292­2300, 2013.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. Transition-based dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075, 2015.
Alexander L Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. arXiv preprint arXiv:1608.04428, 2016.
Aude Genevay, Marco Cuturi, Gabriel Peyre´, and Francis Bach. Stochastic optimization for largescale optimal transport. In Advances in Neural Information Processing Systems, pp. 3440­3448, 2016.
Aude Genevay, Gabriel Peyre´, and Marco Cuturi. Sinkhorn-autodiff: Tractable wasserstein learning of generative models. arXiv preprint arXiv:1706.00292, 2017.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori perturbations. arXiv preprint arXiv:1206.6410, 2012.
Tamir Hazan, Subhransu Maji, and Tommi Jaakkola. On sampling from the gibbs distribution with random maximum a-posteriori perturbations. In Advances in Neural Information Processing Systems, pp. 1268­1276, 2013.
David P Helmbold and Manfred K Warmuth. Learning permutations with exponential weights. Journal of Machine Learning Research, 10(Jul):1705­1736, 2009.
10

Under review as a conference paper at ICLR 2018
Ferenc Husza´r. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 190­ 198. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets. pdf.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Philip A Knight. The sinkhorn­knopp algorithm: convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261­275, 2008.
JJ Kosowsky and Alan L Yuille. The invisible hand algorithm: Solving the assignment problem with statistical physics. Neural networks, 7(3):477­490, 1994.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics (NRL), 2(1-2):83­97, 1955.
Ke Li, Kevin Swersly, Ryan, and Richard S Zemel. Efficient feature learning using perturb-and-map. NIPS Workshop on Perturbations, Optimization, and Statistics, 2013.
Scott W. Linderman, Gonzalo E. Mena, Hal Cooper, Liam Paninski, and John P. Cunningham. Reparameterizing the Birkhoff polytope for variational permutation inference. arXiv preprint arXiv:1710.09508, 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
James Munkres. Algorithms for the assignment and transportation problems. Journal of the society for industrial and applied mathematics, 5(1):32­38, 1957.
Arvind Neelakantan, Quoc V Le, and Ilya Sutskever. Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834, 2015.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69­84. Springer, 2016.
George Papandreou and Alan L Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 193­200. IEEE, 2011.
James Petterson, Jin Yu, Julian J McAuley, and Tibe´rio S Caetano. Exponential family graph matching and ranking. In Advances in Neural Information Processing Systems, pp. 1455­1463, 2009.
Rajesh Ranganath, Dustin Tran, Jaan Altosaar, and David Blei. Operator variational inference. In Advances in Neural Information Processing Systems, pp. 496­504, 2016.
C Radhakrishna Rao. Convexity properties of entropy functions and analysis of diversity. Lecture Notes-Monograph Series, pp. 68­77, 1984.
Tim Rockta¨schel and Sebastian Riedel. End-to-end differentiable proving. arXiv preprint arXiv:1705.11040, 2017.
11

Under review as a conference paper at ICLR 2018
Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876­879, 1964.
Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343­348, 1967.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440­2448, 2015.
Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S Zemel, Ruslan R Salakhutdinov, and Ryan P Adams. Cardinality restricted boltzmann machines. In Advances in neural information processing systems, pp. 3293­3301, 2012.
Kui Tang, Nicholas Ruozzi, David Belanger, and Tony Jebara. Bethe learning of conditional random fields via map decoding. AISTATS, 2016.
Daniel Tarlow, Kevin Swersky, Richard S Zemel, Ryan P Adams, and Brendan J Frey. Fast exact inference for recursive cardinality models. arXiv preprint arXiv:1210.4899, 2012.
Jakub M Tomczak. On some properties of the low-dimensional gumbel perturbations in the perturband-map model. Statistics & Probability Letters, 115:8­15, 2016.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv preprint arXiv:1702.08896, 2017.
Lav R Varshney, Beth L Chen, Eric Paniagua, David H Hall, and Dmitri B Chklovskii. Structural properties of the caenorhabditis elegans neuronal network. PLoS computational biology, 7(2): e1001066, 2011.
Ce´dric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003. Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008. Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
arXiv preprint arXiv:1511.06391, 2015. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229­256, 1992.
12

Under review as a conference paper at ICLR 2018

A PROOF OF THEOREM 1
In this section we give a rigorous proof of Theorem 1. Also, in A.2 we briefly comment on how Theorem 1 extend a perhaps more intuitive results, in the probability simplex.
Before stating Theorem 1 we need some preliminary definitions. We start by recalling a well-known result in matrix theory, the Sinkhorn theorem.
Theorem (Sinkhorn). Let A be an N dimensional square matrix with positive entries. Then, there exists two diagonal matrices D1, D2, with positive diagonals, so that P = D1AD2 is a doubly stochastic matrix. These D1, D2 are unique up to a scalar factor. Also, P can be obtained through the iterative process of alternatively normalizing the rows and columns of A.

Proof. See Sinkhorn (1964); Sinkhorn & Knopp (1967); Knight (2008).

For our purposes, it is useful to define the Sinkhorn operator S(·) as follows: Definition 1. Let A be an arbitrary matrix with dimension N . Denote Tr(X) = X (X1N 1N ), Tc(X) = X (1N 1N A) (with representing the element-wise division and 1n the n dimensional vector of ones) the row and column-wise normalization operators, respectively. Then, we define the Sinkhorn operator applied to A; S(X), as follows:
S0(X) = exp(X), Sl(X) = Tc Tr(Sl-1(X)) , S(X) = lim Sl(X).
n
Here, the exp(·) operator is interpreted as the component-wise exponential. Notice that by Birkhoff's theorem, S(X) is a doubly stochastic matrix.
Finally, we review some key properties related to the space of doubly stochastic matrices. First, we need to define a relevant geometric object. Definition 2. We denote by BN the N -Birkhoff polytope, i.e., the set of doubly stochastic matrices of dimension N . Likewise, we denote Pn be the set of permutation matrices of size N . Alternatively,
BN = {P  [0, 1]  RN,N P 1N = 1N , P 1N = 1N },

PN = {P  {0, 1}  RN,N P 1N = 1N , P 1N = 1N }.
Theorem (Birkhoff). PN is the set of extremal points of BN . In other words, the convex hull of BN equals PN .

Proof. See Birkhoff (1946).

A.1 AN APPROXIMATION THEOREM FOR THE MATCHING PROBLEM

Let's now focus on the standard combinatorial assignment (or matching) problem, for an arbitrary N dimensional matrix X. We aim to maximize a linear functional (in the sense of the Frobenius norm) in the space of permutation matrices. In this context, let's define the matching operator M (·) as the one that returns the solution of the assignment problem:

M (X)  arg max P, X F .
P PN

(7)

Likewise, we define M~ (·) as a related operator, but changing the feasible space by the Birkhoff

polytope:

M~ (X)  arg max P, X F .
P BN

(8)

Notice that in general M~ (X), M (X) might not be unique matrices, but a face of the Birkhoff poly-

tope, or a set of permutations, respectively (see Lemma 2 for details). In any case, the relation

13

Under review as a conference paper at ICLR 2018

M (X)  M~ (X) holds by virtue of Birkhoff's theorem, and the fundamental theorem of linear programming.

Now we state the main theorem of this work:

Theorem 1. For a doubly stochastic matrix P define its entropy as h(P ) = - i,j Pi,j log (Pi,j). Then, one has,

S(X/ ) = arg max P, X F +  h(P ).
P BN

(9)

Now, assume also the entries of X are drawn independently from a distribution that is absolutely continuous with respect to the Lebesgue measure in R. Then, almost surely the following conver-
gence holds:

M (X) = lim S(X/ ).
 0+

(10)

We divide the proof of Theorem 1 in three steps. First, in Lemma 1 we state a relation between S(X/ ) and the entropy regularized problem in equation (9). Then, in Lemma 2 we show that under our stochastic regime, uniqueness of solutions holds. Finally, in Lemma 3 we show that in this well-behaved regime, convergence of solutions holds. states that and Lemma 2b endows us with the tools to make a limit argument.

A.1.1 INTERMEDIATE RESULTS FOR THEOREM 1

Lemma 1.

S(X/ ) = arg max P, X F +  h(P ).
P BN

Proof. We first notice that the solution P of the above problem exists, and it is unique. This is a simple consequence of the strict concavity of the objective (recall the entropy is strictly concave Rao (1984)).
Now, let's state the Lagrangian of this constrained problem
L(, , P ) = P, X F +  h(P ) +  (P 1N - 1N ) +  (P 1N - 1N ),
It is easy to see, by stating the equality L/P = 0 that one must have for each i, j,
pi,j = exp(i/ - 1/2) exp(Xi,j/ ) exp(j/ - 1/2),
in other words, P = D1 exp(Xi,j/ )D2 for certain diagonal matrices D1, D2, with positive diagonals. By Sinkhorn's theorem, and our definition of the Sinkhorn operator, we must have that S(X/ ) = P .
Lemma 2. Suppose the entries of X are drawn independently from a distribution that is absolutely continuous with respect to the Lebesgue measure in R. Then, almost surely, M~ (X) = M (X) is a unique permutation matrix.

Proof. This is a known result from sensibility analysis on linear programming which we prove for completeness. Notice first that the problem in (2) is a linear program on a polytope. As such, by the fundamental theorem of linear program, the optimal solution set must correspond to a face of the polytope. Let F be a face of BN of dimension  1, and take P1, P2  F , P1 = P2. If F is an optimal face for a certain XF , then XF  {X : P1, X F = P2, X F }. Nonetheless, the latter set does not have full dimension, and consequently has measure zero, given our distributional assumption on X. Repeating the argument for every face of dimension  1 and taking a union bound we conclude that, almost surely, the optimal solution lies on a face of dimension 0, i.e, a vertex. From here uniqueness follows.
Lemma 3. Call P the solution to the problem in equation 9, i.e. P = P (X) = S(X/ ). Under the assumptions of Lemma 2, P  P0 when if   0+.

14

Under review as a conference paper at ICLR 2018

Proof. Proof Notice that by Lemmas 1 and 2, P is well defined and unique for each   0. Moreover, at  = 0, P0 = M (X) is the unique solution of a linear program. Now, let's define f (·) = ·, X F +  h(·). We observe that f0(P )  f0(P0). Indeed, one has:
f0(P0) - f0(P ) = P0, X F - P , X F = P0, X F - f (P ) +  h(P ) < P0, X F - f (P0) +  h(P ) <  (h(P ) - h(P0))
<  max h(P ).
P BN

From which convergence follows trivially. Moreover, in this case convergence of the values implies
the converge of P : suppose P does not converge to P0. Then, there would exist a certain  and sequence n  0 such that Pn - P0 > . On the other hand, since P0 is the unique maximizer of an LP, there exists  > 0 such that f0(P0) - f0(P ) >  whenever P - P0 > , P  BN . This
contradicts the convergence of f0(Pn ).

A.1.2 PROOF OF THEOREM 1

The first statement is Lemma 1. Convergence (equation 10) is a direct consequence of Lemma 3, after noticing P = S(X/ ) and P0 = M (X).

A.2 RELATION TO SOFTMAX

Finally, we notice that all of the above results can be understood as a generalization of the well-
known approximation result arg maxi xi = lim0+ sof tmax(x/ ). To see this, treat a category as a one-hot vector. Then, one has

arg max xi = arg max e, x ,
i eSN

(11)

where Sn is the probability simplex, the convex hull of the one-hot vectors (denoted Hn). Again, by the fundamental theorem of linear algebra, the following holds:

arg max xi = arg max e, x .
i eHN

(12)

On the other hand, by a similar (but simpler) argument than of the proof of theorem 4 one can easily

show that

exp(x/ )

sof tmax(x/ ) 

= arg max e, x +  h(e),

i=1 exp(xi/ )

eSn

(13)

where the entropy h(·) is not defined as h(e) = -

n i=1

ei

log(ei)

B SUPPLEMENTAL METHODS

B.1 EXPERIMENTAL PROTOCOLS
All experiments were run on a cluster using Tensorflow Abadi et al. (2016), using several GPU (Tesla K20, K40, K80 and P100) in parallel to enable an efficient exploration of the hyperparameter space: temperature, learning rate, and neural network parameters (dimensions).
In all cases, we used L = 20 Sinkhorn Operator Iterations, and a 10x10 batch size: for each sample in the batch we used Gumbel perturbations to generate 10 different reconstructions. Finally, we note that in practice we don't apply the Sinkhorn operator as in (1), but work on more stable logarithmic scale.
For evaluation, we used the Hungarian Algorithm Munkres (1957) to compute M (X) required to infer the predicted matching.
Finally, experiments of section 5.4 were done consistent with model specifications stated in Linderman et al. (2017)

15

Under review as a conference paper at ICLR 2018

B.2 NUMBER OF PARAMETERS ON SINKHORN NETWORKS

In the simplest network, the one that sorts number, the number of parameters is given by N +N ×nu: Indeed, each number is connected with the hidden layer with nu (here, 32) units. This layer connects with another layer with N units, representing a row of g(X~ , ).
For images, the first layer is a convolution, composed by nf convolutional filters of receptive field size Ks with nc channels (one or three) followed by a ReLU + max-pooling (with stride s) operations. Then, the number of parameters in the first layer is given by Ks2 × nc × nf + nf . The second layers connects the output of a convolution, i.e., the stacked convolved l × l images by each of the filters (after max-pooling) and p2 units, where p is the number of pieces each side was divided by. Therefore, the number of parameters is given by l2/(p2s2) × nf × p2 = l2/s2 × nf , up to rounding and padding subtleties. Then, the total number of parameters is l2/s2 × nf + Ks2 × nc × nf + nf . For the 3x3 puzzle on Imagenet, l = 256, p = 3, nc = 3 and the optimal network was such that nf = 64, s = 2, Ks = 5. Then, it had 1,053,440 parameters.
Finally, for arbitrary assembly experiments, as one includes additional fully connected second layers, the total number of parameters is nl × l2/s2 × nf + Ks2 × nc × nf + nf , where nl is the number of labels (here, nl = 10).

B.3 INFERENCE WITH THE IMPLICIT GUMBEL-SINKHORN DISTRIBUTION

For learning of a latent permutation on a probabilistic model (in our case, representing the posterior distribution of a matching P and dynamics matrix W given neural recordings and prior information), we adhere to the Variational Inference framework Blei et al. (2017): we aim to learn the approximate distribution p(z|y) of a latent code z through an approximate family q(z|y), the one that maximizes the evidence lower bound (ELBO):

log p(y)  Eq(z|y) (log p(y|z)) - KL(q(z|y) p(z)).

(14)

In our case, z = {W, P }, but hereafter we focus only on the discrete component; i.e, z = P . As suggested in Maddison et al. (2016), we change P by its relaxation and replace the actual ELBO, the r.h.s. of equation (14), by the surrogate that results from using a continuous distribution instead.

In our context, both the posterior and prior come from the re-parameterizable Gumbel-Sinkhorn distributions P = S((X + )/ )  G.S(X,  ) and S(/prior)  G.S.(X = 0, prior), respectively, and we need to compute KL(G.S(X,  ) G.S.(X = 0, prior)). As this term is intractable, we propose to use as our `code' or stochastic node z, the variable (X + )/ instead. Then, the KL term
substantially simplifies to KL((X + )/ /prior). To compute this term, we recall that i,j are i.i.d. standard Gumbel variables. Also, one can see that the density of the variable g = (a + g)/b,
where g has a Gumbel distribution and a, b are constants is given by:

log pg (z) = log b - (bz - a + exp (a - bz)) .

(15)

Therefore, the log density ratio LR(z) between each component of g1 = (xi,j + i,j)/ and g2 = i,j/prior is given by (supresing indexing for simplicity)

LR(z) = log pg1 (z)/ log pg2 (z) = log  - ( z - x + exp (x - z )) - log prior + (priorz + exp (-zprior)).

We need to take expectations with respect to the distribution of g1. By the law of the unconscious statistician, one can express the above in terms of the standard Gumbel  instead. Then, one has

LR() = log( /prior) - ( + exp (-) - ( + x)prior/ - exp (-( + x)prior/ )))
log( /prior) - ((1 - prior/ ) + 1 - xprior/ - exp (-xprior/ ) (1 + prior/ )).
Where the last line follow from the identities E() =   0.5772 (the Euler-Mascheroni constant) and E(exp(t)) = (1 - t) (the moment generating function), implying E(exp(-)) = 1 and E(exp (-prior/ )) = (1 + prior/ ).

From this, it easily follows (adding all the N 2 components) that
KL((X+)/ /prior) = N 2+N 2 (log( /prior) - (1 - prior/ ))-S1-(1+prior/ )S2,
where S1 = prior/ i,j xi,j and S2 = i,j exp (-xi,j prior/ ) We note that key to to our treatment of the problem is the fact that both the prior and posterior were the same function (§(·)) of a simpler distribution. This may not be the case in more general models.

16

Under review as a conference paper at ICLR 2018

C SUPPLEMENTAL RESULTS

C.1 PUZZLES
In table 4 we provide further performance measures for the Jigsaw puzzle task on Celeba, for extreme hyper-parameter values: small temperature, large temperature, and a single Sinkhorn iteration These are worse than the ones in table 2, although surprisingly, one Sinkhorn iteration already provides reasonable performance, as long temperature is chosen in an appropriate range.
Table 4: Jigsaw puzzle results for different extreme hyper-parameter values

 = 0.01

 = 100

L=1

2x2 3x3 4x4 5x5 2x2 3x3 4x4 5x5 .2x2 3x3 4x4 5x5

Prop. wrong

.06 .08 .23 .36 .03 .1 .28 .5 .0

Prop. any wrong .1 .22 .36 .9 .04 .23 .67 .97 .0

Kendall tau

.9 .89 .74 .62 .97 .88 .7 .47 1.0

l1 .03 .04 .1 .14 .01 .04 .11 .19 .0

l2 .16 .18 .28 .34 .11 .19 .3 .38 .0

.03 .13 .28 .08 .42 .82 .96 .86 .72 .01 .05 .11 .11 .21 .3

C.2 TRANSFORMATIONS INTO ARBITRARY DIGITS
In table 5 we show performance of a 2-layer CNN in detecting transformed digits as the ones they are intended to be. From this we see the most troublesome transformation was to one, as this network most of the times categorized it as a different number. Also, in figure 3 we show transformations,
Becomes
0 1 2 3 456 7 8 9
0 1. .0 1. 1. 1. 1. 1. 1. 1. 1. 1 .91 1. .97 .99 .99 1. 1. .56 .75 .2 2 1. .0 1. 1. 1. 1. 1. .70 1. 1. 3 .04 .0 1. 1. 1. 1. .96 1. 1. .96 4 1. .46 1. 1. 1. 1. 1. 1. .68 .36 5 1. .0 1. 1. .63 1. 1. 1. 1. 1. 6 .3 .01 1. 1. 1. 1. .65 1. .65 1. 7 .0 .73 .27 .46 1. 1. 1. 1. 1. .72 8 1. .07 1. 1. 1. 1. 1. .07 1. 1. 9 1. .33 1. 1. 1. 1. 1. 1. 1. .66

Actual digit

Table 5: Accuracies of two-layer convolutional neural network in identifying transformed digits

showing that to reconstruct to arbitrary digits it is not required that the original ones have an actual digit-like structure, but they can be only pieces of `strokes' or `dust'. In detail, these "digits" were crafted by sampling, without replacement, from a bag containing all the small pieces from all original digits. These reconstructions suggest the possibility of an alternative to generative modeling, based on the (random) assembly of small pieces of noise, instead of the processing of noise through a neural network. However, this would require training the network without supervision, which is beyond the scope of this work.

C.3 RESULTS ON CATEGORIAL VAE IN MNIST

In general, for arbitrary random variables Z1, Z2 and a function g, one has

KL(Z1 Z2)  KL(g(Z1) g(Z2)).

(16)

We show this is true in the discrete case, for simplicity: call q(z) and p(z) the densities of Z1, Z2, and call y = g(z). This induces two joint distributions, p(z, y) and q(z, y). Now, define

KL (q(z|y) p(z|y)) = (q(z, y) log q(z|y) - log p(z|y)).
y,z

17

Under review as a conference paper at ICLR 2018

Mixed Scrambled

Hard reconstructions

Soft Reconstructions

Figure 3: First column: samples from dataset created by mixing all pieces of digits, and then reassembling them into `digits'. Second column: random permutations of first column. Third column: hard reconstructions using M (X). Fourth column: soft reconstructions using S(X/ ) and  = 1. Metaphorically, one is able to reconstruct pieces out of `dust'.

Under this definition, one can verify that
KL(q(z, y) p(z, y)) =KL(q(z) p(z)) + KL(q(y|z) p(y|z)) =KL(q(y) p(y)) + KL(q(z|y) p(z|y)).
But KL(q(y|z) p(y|z)) = 0, as y is a deterministic function of z. Therefore, KL(q(z) p(z)) = KL(q(y) p(y)) + KL(q(z|y)|p(z|y)), and since the second term is positive (a KL divergence) we conclude KL(q(z) p(z))  KL(q(y) p(y)).

This implies a lower (or less tight) ELBO if using Z1, Z2 instead of g(Z1), g(Z2). However, we note that in the categorical case this has a minimal impact in performance. Indeed, we replicated the den-
sity estimation on MNIST task described in Jang et al. (2016); Maddison et al. (2016), and as alternative method we considered the concrete distribution, but using as stochastic node ( + x)/ (with prior /prior instead of two concrete distributions. In other words, for us g(x) = softmax (x) and Z1 = ( + x)/, Z2 = (2)/prior. Results are shown in Table 6. We first see that Concrete distribution does worse than Gumbel-Softmax, which we attribute to a sub-optimal parameter search. However, we see that working in the Gumbel space has little impact on log p(x): the difference was smaller than .5 nats.

Method Gumbel-Softmax Concrete Concrete (Gumbel space)

- log p(x) 106.7 111.5 111.9

Table 6: Summary of results in VAE

C.4 SUPPLEMENTARY RESULTS ON C.ELEGANS
Finally, in Table 7 we show additional results for the C.elegans experiment. The setting is the same as in Figure 4(a) in Linderman et al. (2017). Likewise, Table 3 correspond to the setting of Figure 4(b) in Linderman et al. (2017).
D CONNECTIONS WITH OPTIMAL TRANSPORT
Our theoretical treatment of the problem draws on recent developments in Optimal Transport (OT) Villani (2003): specifically, the first part of result from Cuturi (2013), linking the entropy-regularized transportation problem and the computation of a `Sinkhorn distance'. Our work extends the argument to Birkhoff polytope from the original statement on the transportation polytope, the optimiza-

18

Under review as a conference paper at ICLR 2018

Mean number of candidates
Difficulty
MCMC (Linderman et al., 2017) Gumbel-Sinkhorn Gumbel-Sinkhorn (no regularization)

10
1 worm 4 worms
.34 .65 .77 .93 .79 .94
.77 .92

30
1 Worm 4 worms
.18 .28 .33 .7 .4 .69
.4 .64

45
1 worm 4 worms
.14 .17 .18 .48 .25 .51
.25 .44

60
1 worms 4 worms
.13 .16 .17 .37 .21 .44
.21 .39

Table 7: Accuracy in the C.elegans neural identification problem, for varying mean number of candidate neurons (10, 30, 45, 60) and number of worms (1 and 4).

tion set that arises in the mostly adopted Kantorovich formulation of OT (Genevay et al., 2016). There are clear connections between optimal transport and our problem, explaining the appositeness of our extension: first, the Birkhoff polytope corresponds exactly to transportation polytope (see Cuturi (2013)) between two uniform histograms, multiplied by N . Also, the set of permutations is the one that naturally appears when stating the discrete OT problem with the slightly stricter Monge formulation (Villani, 2008), that explicitly requires a one-to-one assignment.
We hope our work may catalyze new developments at the intersection of OT and machine learning: currently, the most prominent application relates to generative model learning (Genevay et al., 2017), as OT provides provides a richer perspective to the question on how to compare two distributions (Cuturi, 2013); now as the minimum amount of `total mass' moved to transform one distribution into another (Arjovsky et al., 2017). Our work highlights there are combinatorial aspects that also emerge naturally from OT, but have remained understudied in the community. We hypothesize other extensions of the entropy regularization method may be possible to address more general discrete structures: for example, the ones that put cardinality constrains on objects (Swersky et al., 2012; Tarlow et al., 2012).

19

