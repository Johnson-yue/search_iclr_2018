Under review as a conference paper at ICLR 2018
TRAINING GANS WITH OPTIMISM
Anonymous authors Paper under double-blind review
ABSTRACT
We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.
1 INTRODUCTION
Generative Adversarial Networks (GANs) Goodfellow et al. (2014) have proven a very successful approach for fitting generative models in complex structured spaces, such as distributions over images. GANs frame the question of fitting a generative model from a data set of samples from some distribution as a zero-sum game between a Generator (G) and a discriminator (D). The Generator is represented as a deep neural network which takes as input random noise and outputs a sample in the same space of the sampled data set, trying to approximate a sample from the underlying distribution of data. The discriminator, also modeled as a deep neural network is attempting to discriminate between a true sample and a sample generated by the generator. The hope is that at the equilibrium of this zero-sum game the generator will learn to generate samples in a manner that is indistinguishable from the true samples and hence has essentially learned the underlying data distribution.
Despite their success at generating visually appealing samples when applied to image generation tasks, GANs are very finicky to train. One particular problem, raised for instance in a recent survey as a major issue (Goodfellow, 2017) is the instability of the training process. Typically training of GANs is achieved by solving the zero-sum game via running simultaneously a variant of a Stochastic Gradient Descent algorithm for both players (potentially training the discriminator more frequently than the generator).
The latter amounts essentially to solving the zero-sum game via running no-regret dynamics for each player. However, it is known from results in game theory, that no-regret dynamics in zero-sum games can very often lead to limit cycles, rather than converge to an equilibrium. Even in convexconcave zero-sum games it is only the average of the weights of the two players that constitutes an equilibrium and not the last-iterate. In fact recent theoretical results of Mertikopoulos et al. (2017) show the strong result that no variant of GD that falls in the large class of Follow-the-RegularizedLeader (FTRL) algorithms can converge to an equilibrium in terms of the last-iterate and are bound to converge to limit cycles around the equilibrium.
1

Under review as a conference paper at ICLR 2018
Averaging the weights of neural nets is a prohibitive approach in particular because the zero-sum game that is defined by training one deep net against another is not a convex-concave zero-sum game. Thus it seems essential to identify training algorithms that make the last iterate of the training be very close to the equilibrium, rather than only the average.
Contributions. In this paper we propose training GANs, and in particular Wasserstein GANs Arjovsky et al. (2017), via a variant of gradient descent known as Optimistic Mirror Descent. Optimistic Mirror Descent (OMD) takes advantage of the fact that the opponent in a zero-sum game is also training via a similar algorithm and uses the predictability of the strategy of the opponent to achieve faster regret rates. It has been shown in the recent literature that Optimistic Mirror Descent and its generalization of Optimistic Follow-the-Regularized-Leader (OFTRL), achieve faster convergence rates than gradient descent in convex-concave zero-sum games (Rakhlin & Sridharan, 2013a;b) and even in general normal form games (Syrgkanis et al., 2015). Hence, even from the perspective of faster training, OMD should be preferred over GD due to its better worst-case guarantees and since it is a very small change over GD.
Moreover, we prove the surprising theoretical result that for a large class of zero-sum games (namely bi-linear games), OMD actually converges to an equilibrium in terms of the last iterate. Hence, we give strong theoretical evidence that OMD can help in achieving the long sought-after stability and last-iterate convergence required for GAN training. The latter theoretical result is of independent interest, since solving zero-sum games via no-regret dynamics has found applications in many areas of machine learning, such as boosting (Freund & Schapire, 1996). Avoiding limit cycles in such approaches could help improve the performance of the resulting solutions.
We complement our theoretical result with toy simulations that portray exactly the large qualitative difference between OMD as opposed to GD (and its many variants, including gradient penalty, momentum, adaptive step size etc.). We show that even in a simple distribution learning setting where the generator simply needs to learn the mean of a multi-variate distribution, GD leads to limit cycles, while OMD converges pointwise.
Moreover, we give a more complex application to the problem of learning to generate distributions of DNA sequences of the same cellular function. DNA sequences that carry out the same function in the genome, such as binding to a specific transcription factor, follow the same nucleotide distribution. Characterizing the DNA distribution of different cellular functions is essential for understanding the functional landscape of the human genome and predicting the clinical consequence of DNA mutations (Zeng et al., 2015; 2016; Zeng & Gifford, 2017). We perform a simulation study where we generate samples of DNA sequences from a known distribution. Subsequently we train a GAN to attempt to learn this underlying distribution. We show that OMD achieves consistently better performance than GD variants in terms of the Kullback-Leibler (KL) divergence between the distribution learned by the Generator and the true distribution.
Finally, we apply optimism to training GANs for images and introduce the Optimistic Adam algorithm. We show that it achieves better performance than Adam, in terms of inception score, when trained on CIFAR10.
2 PRELIMINARIES: WGANS AND OPTIMISTIC MIRROR DESCENT
We consider the problem of learning a generative model of a distribution of data points Q  (X). Our goal is given a set of samples from D, to learn an approximation to the distribution Q in the form of a deep neural network G(·), with weight parameters , that takes as input random noise z  F (from some simple distribution F ) and outputs a sample G(z)  X. We will focus on addressing this problem via a Generative Adversarial Network (GAN) training strategy.
The GAN training strategy defines as a zero-sum game between a generator deep neural network G(·) and a discriminator neural network Dw(·). The generator takes as input random noise z  F , and outputs a sample G(z)  X. A discriminator takes as input a sample x (either drawn from the true distribution Q or from the generator) and attempts to classify it as real or fake. The goal of the generator is to fool the discriminator.
In the original GAN training strategy Goodfellow et al. (2014), the discriminator of the zero sum game was formulated as a classifier, i.e. Dw(x)  [0, 1] with a multinomial logistic loss. The latter
2

Under review as a conference paper at ICLR 2018

boils down to the following expected zero-sum game (ignoring sampling noise).

inf sup ExQ [log(Dw(x))] + EzF [log(1 - Dw(G(z)))]
w

(1)

If the discriminator is very powerful and learns to accurately classify all samples, then the problem of the generator amounts to solving the Jensen-Shannon divergence between the true distribution and the generators distribution. However, if the discriminator is very powerful, then in practice the latter leads to vainishing gradients for the generator and inability to train in a stable manner.

The latter problem lead to the formulation of Wasserstein GANs (WGANs) Arjovsky et al. (2017),
where the discriminator rather than being treated as a classifier is instead trying to simulate the Wasserstein-1 or earth-mover metric between the true distribution and the distribution of the generator. In this case, the function Dw(x) is not constrained to being a probability in [0, 1] but rather is an arbitrary 1-Lipschitz function of x. This reasoning leads to the following zero-sum game:

inf sup ExQ [Dw(x)] - EzF [Dw(G(z))]
w

(2)

If the function space of the discriminator covers all 1-Lipschitz functions of x, then the quantity supw ExD [Dw(x)] - EzF [Dw(G(z))] that the generator is trying to minimize corresponds to the earth-mover distance between the true distribution Q and the distribution of the generator. Given
the success of WGANs we will focus on WGANs in this paper.

2.1 GRADIENT DESCENT VS OPTIMISTIC MIRROR DESCENT

The standard approach to training WGANs is to train simultaneously the parameters of both networks via stochastic gradient descent. We begin by presenting the most basic version of adversarial training via stochastic gradient descent and then comment on the multiple variants that have been proposed in the literature in the following section, where we compare their performance with our proposed algorithm for a simple example.

Let us start how training a GAN with gradient descent would look like in the absence of sampling error, i.e. if we had access to the true distribution Q. For simplicity of notation, let:

L(, w) = ExQ [Dw(x)] - EzF [Dw(G(z))]

(3)

denote the loss in the expected zero-sum game of WGAN, as defined in Equation (2), i.e.

inf supw L(, w). The classic WGAN approach is to solve this game by running gradient descent (GD) for each player, i.e. for t  {1, . . . , T - 1}: with w,t = wL(t, wt) and

,t = L(t, wt)

wt+1 = wt +  · w,t t+1 = t -  · ,t

(4)

If the loss function L(, w) was convex in  and concave w and the step size  is chosen of the

order 1 , then standard results in game theory and no-regret learning imply that the pair (¯, w¯) of

T

average parameters, i.e.

w¯

=

1 T

T t=1

wt

and ¯ =

1 T

T t=1

t

is

an

-equilibrium of the zero-sum

game, for = O 1 . However, no guarantees are known beyond the convex-concave setting
T

and, more importantly for the paper, even in convex-concave games, no guarantees are known for

the last-iterate pair (T , wT ).

Rakhlin and Sridharan (Rakhlin & Sridharan, 2013a) proposed an alternative algorithm for solving

zero-sum games in a decentralized manner, namely Optimistic Mirror Descent (OMD), that achieves

faster convergence rates to equilibrium of

=O

1 T

for the average of parameters. The algorithm

essentially uses the last iterations gradient as a predictor for the next iteration's gradient. This

follows from the intuition that if the opponent in the game is using a stable (or regularized) algorithm,

then the gradients between the two iterations will not change much. Later Syrgkanis et al. (2015)

showed that this intuition extends to show faster convergence of each individual player's regret in

general normal form games.

Given these favorable properties of OMD when learning in games, we propose replacing GD with OMD when training WGANs. The update rule of a OMD is a small adaptation to GD. OMD is parameterized by a predictor of the next iteration's gradient which could either be simply last iteration's gradient or an average of a window of last gradient or a discounted average of past gradients.

3

Under review as a conference paper at ICLR 2018

In the case where the predictor is simply the last iteration gradient, then the update rule for OMD boils down to the following simple form:

wt+1 = wt + 2 · w,t -  · w,t-1 t+1 = t - 2 · .t +  · ,t-1

(5)

The simple modification in the GD update rule, is inherently different than any of the existing adaptations used in GAN training, such as Nesterov's momentum, or gradient penalty.

General OMD and intuition. The intuition behind OMD can be more easily understood when

GD is viewed through the lens of the Follow-the-Regularized-Leader formulation. In particular,

it is well known that GD is equivalent to the Follow-the-Regularized-Leader algorithm with an 2

regularizer, i.e.:

t

wt+1

=

arg

max 
w

w, w,

-

w

2 2

 =1

t

(6)

wt+1 = arg min 


, ,

+



2 2

 =1

It is known that if the learner knew in advance the gradient at the next iteration, then by adding that

to the above optimization would lead to constant regret that comes solely from the regularization

term. OMD essentially augments FTRL by adding a predictor Mt+1 of the next iterations gradient,

i.e.:
t

wt+1 = arg max 

w, w, + w, Mw,t+1

w

 =1

t

-

w

2 2

(7)

t+1 = arg min 


, , + , M,t+1

+



2 2

 =1

For an arbitrary set of predictors, the latter boils down to the following set of update rules:

wt+1 = wt +  · (w,t + Mw,t+1 - Mw,t) t+1 = t -  · (,t + M,t+1 - M,t)

(8)

In the theoretical part of the paper we will focus on the case where the predictor is simply the last iteration gradient, leading to update rules in Equation (5). In the experimental section we will also explore performance of other alternatives for predictors.

2.2 STOCHASTIC GRADIENT DESCENT AND STOCHASTIC OPTIMISTIC MIRROR DESCENT

In practice we don't really have access to the true distribution Q and hence we replace Q with an empirical distribution Qn over samples {x1, . . . , xn} and Fn of random noise samples {z1, . . . , zn}, leading to empirical loss for the zero-sum game of:

Ln(, w) = ExQn [Dw(x)] - EzFn [Dw(G(z))]

(9)

Even in this setting it might be impractical to compute the gradient of the expected loss with respect to Qn or Fn, e.g. ExQn [wDw(x)].

However, GD and OMD still leads to small loss if we replace gradients with unbiased estimators of them. Hence, we can replace expectation with respect to Qn or Fn, by simply evaluating the gradients at a single sample or on a small batch of B samples. Hence, we can replace the gradients at each iteration with the variants:

^ w,t =

1 |B|

(wDwt (xi) - wDwt (Gt (zi)))

iB

^ ,t =

-

1 |B|

(Dwt (Gt (zi)))

iB

(10)

Replacing w,t and ,t with the above estimates in Equation (4) and (5), leads to Stochastic Gradient Descent (SGD) and Stochastic Optimistic Mirror Decent (SOMD) correspondingly.

4

Under review as a conference paper at ICLR 2018

3 AN ILLUSTRATIVE EXAMPLE: LEARNING THE MEAN OF A DISTRIBUTION

We consider the following very simple WGAN example: The data are generated by a multivariate normal distribution, i.e. Q N (v, I) for some v  Rd. The goal is for the generator to learn the unknown parameter v. In Appendix C we also consider a more complex example where the generator is trying to learn a co-variance matrix.

We consider a WGAN, where the discriminator is a linear function and the generator is a simple additive displacement of the input noise z, which is drawn from F N (0, I), i.e:

Dw(x) = w, x G(z) = z + 

(11)

The goal of the generator is to figure out the true distribution, i.e. to converge to  = v. The WGAN loss then takes the simple form:

L(, w) = ExN(v,I) [ w, x ] - EzN(0,I) [ w, z +  ]

(12)

We first consider the case where we optimize the true expectations above rather than assuming that

we only get samples of x and samples of z. Due to linearity of expectation, the expected zero-sum

game takes the form:

inf sup w, v - 
w

(13)

We see here that the unique equilibrium of the above game is for the generator to choose  = v and

for the discriminator to choose w = 0. For this simple zero sum game, we have w,t = v - t and ,t = -wt. Hence, the GD dynamics take the form:

wt+1 =wt + (v - t) t+1 =t + wt

(GD Dynamics for Learning Means)

while the OMD dynamics take the form:

wt+1 =wt + 2 · (v - t) -  · (v - t-1) t+1 =t + 2 · wt -  · wt-1

(OMD Dynamics for Learning Means)

We simulated simultaneous training in this zero-sum game under the GD and under OMD dynamics and we find that GD dynamics always lead to a limit cycle irrespective of the step size or other modifications. In Figure 1 we present the behavior of the GD vs OMD dynamics in this game for v = (3, 4). We see that even though GD dynamics leads to a limit cycle (whose average does indeed equal to the true vector), the OMD dynamics converge to v in terms of the last iterate. In Figure 2 we see that the stability of OMD even carries over to the case of Stochastic Gradients, as long as the batch size is of decent size.

(a) GD dynamics.

(b) OMD dynamics.

Figure 1: Training GAN with GD converges to a limit cycle that oscilates around the equilibrium (we applied weight-clipping at 10 for the discriminator). On the contrary training with OMD converges to equilibrium in terms of last-iterate convergence.

In the appendix we also portray the behavior of the GD dynamics even when we add gradient penalty (Gulrajani et al., 2017) to the game loss (instead of weight clipping), adding Nesterov momentum to the GD update rule (Nesterov, 1983) or when we train the discriminator multiple times in between a train iteration of the generator. We see that even though these modifications do improve the stability of the GD dynamics, in the sense that they narrow the band of the limit cycle, they still lead to a non-vanishing limit cycle, unlike OMD.
In the next section, we will in fact prove formally that for a large class of zero-sum games including the one presented in this section, OMD dynamics converge to equilibrium in the sense of last-iterate convergence, as opposed to average-iterate convergence.

5

Under review as a conference paper at ICLR 2018

(a) Stochastic OMD dynamics with mini-batch of 50. (b) Stochastic OMD dynamics with mini-batch of 200. Figure 2: Robustness of last-iterate convergence of OMD to stochastic gradients.

4 LAST-ITERATE CONVERGENCE OF OPTIMISTIC ADVERSARIAL TRAINING

In this section, we show that Optimistic Mirror Descent exhibits final-iterate, rather than only

average-iterate convergence to min-max solutions for bilinear functions. More precisely, we con-

sider the problem minx maxy xT Ay, for some matrix A, where x and y are unconstrained. In

Appendix D, we also show that our convergence result appropriately extends to the general case,

where the bi-linear game also contains terms that are linear in the players' individual strategies,

i.e. games of the form:

inf sup xT Ay + bT x + cT y + d .
xy

(14)

In the simpler minx maxy xT Ay problem, Optimistic Mirror Descent takes the following form, for all t  1:

xt = xt-1 - 2Ayt-1 + Ayt-2 yt = yt-1 + 2AT xt-1 - AT xt-2

(15) (16)

Initialization: For the above iteration to be meaningful we need to specify x0, x-1, y0, y-1. We choose any x0  R(A), and y0  R(AT ), and set x-1 = 2x0 and y-1 = 2y-1, where R(·) represents column space. In particular, our initialization means that the first step taken by the dynamics gives x1 = x0 and y1 = y0.

We will analyze Optimistic Mirror Descent under the assumption   1, where  = max{||A||, ||AT ||} and || · || denotes spectral norm of matrices. We can always enforce that   1 by appropriately scaling A. Scaling A by some positive factor clearly does not change the min-max solutions (x, y), only scales the optimal value xT Ay by the same factor.

We remark that the set of equilibrium solutions of this minimax problem are pairs (x, y) such that x is in the null space of AT and y is in the null space of A. In this section we rigorously show that Optimistic Mirror Descent converges to the set of such min-max solutions. This is interesting in light of the fact that Gradient Descent actually diverges, even in the special case where A is the identity matrix, as per the following proposition whose proof is provided in Appendix D.3.
Proposition 1. Gradient descent applied to the problem minx maxy xT y diverges starting from any initialization x0, y0 such that x0, y0 = 0.

Next, we state our main result of this section, whose proof can be found in Appendix D, where we also state its appropriate generalization to the general case (14).

Theorem 1 (Last Iterate Convergence of OMD). Consider the dynamics of Eq. (15) and (16) and

any

initialization

1 2

x-1

=

x0



R(A),

and

1 2

y-1

=

y0



R(AT ).

Let

also

 = max AAT + , AT A + ,

where for a matrix X we denote by X+ its generalized inverse and by ||X|| its spectral norm.

Suppose that max{||A||, ||AT ||}    1 and  < , and let t = 1 = 0 and for all t  2:

AT xt

2 2

+

||Ayt||22.

Then

t  (1 - ) t-1 + O(30).

In particular, t  O(20) as t  +. Hence, for large enough t, the last iterate of OMD is within O( 0) distance from the space of equilibrium points of the game, where 0 is the distance of the initial point (x0, y0) from the equilibrium space, and where both distances are taken
with respect to the norm xT AAT x + yT AT Ay.

6

Under review as a conference paper at ICLR 2018

5 EXPERIMENTAL RESULTS FOR GENERATING DNA SEQUENCES

We take our theoretical intuition to practice, applying OMD to the problem of generating DNA sequences from an observed distribution of sequences. DNA sequences that carry out the same function can be viewed as samples from some distribution. For many important cellular functions, this distribution can be well modeled by a position-weight matrix (PWM) that specifies the probability of different nucleotides occuring at each position (Stormo, 2000). Thus, training GANs from DNA sequences sampled from a PWM distribution serves as a practically motivated problem where we know the ground truth and can thus quantify the performance of different training methods in terms of the KL divergence between the trained generator distribution and the true distribution.

In our experiments, we generated 40,000 DNA sequences of six nucleotides according to a given position weight matrix. A random 10% of the sequences were held out as the validation set. Each sequence was then embedded into a 4 × 6 matrix by encoding each of the four nucleotides with an one-hot vector. On this dataset, we trained WGANs with different variants of OMD and SGD and evaluated their performance in terms of the KL divergence between the empirical distribution of the WGAN-generated samples and the true distribution described by the position weight matrix. Both the discriminator and generator of the WGAN used in this analysis were chosen to be convolutional neural networks (CNN), given the recent success of CNNs in modeling DNA-protein binding (Zeng et al., 2016; Alipanahi et al., 2015). The detailed structure of the chosen CNNs can be found in Appendix E. The discriminator is trained 5 times before training the generator once as suggested in Arjovsky et al. (2017).

To account for the impact of learning rate and training epochs, we explored two different ways of model selection when comparing different optimization strategies: (1) using the iteration and learning rate that yields the lowest discriminator loss on the held out test set. This is inspired by the observation in Arjovsky et al. (2017) that the discriminator loss negatively correlates with the quality of the generated samples. (2) using the model obtained after the last epoch of the training. To account for the stochastic nature of the initialization and optimizers, we trained 50 independent models for each learning rate and optimizer, and compared the optimizer strategies by the resulting distribution of KL divergences across 50 runs.

For GD, we used variants of Equation (4) to examine the effect of using momentum and an adaptive step size. Specifically, we considered momentum, Nesterov momentum and Adagrad. The specific form of all these modifications is given for reference in Appendix A.

For OMD we used the general predictor version of Equation (10) with a fixed step size and with the

following variants of the next iteration predictor Mt+1: (v1) Last iteration gradient: Mt+1 = ft,

(v2)

Running

average

of

past

gradients:

Mt+1

=

1 t

t i=1

fi

,

(v3)

Hyperbolic

discounted

average

of past gradients: Mt+1 = Mt + (1 - )ft,   (0, 1).

Method Method

SOMD_ver1 SOMD_ver2 SOMD_ver3 SGD_vanilla SGD_adagrad SGD_nesterov SGD_momentum
0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 KL Divergence
(a) WGAN with the lowest validation discriminator loss

SSSGGGDDDSSSSSSGGG___GGGSSSmmmSSSSSSSSSDDDDDDGGGOOOOOOOOO___ooo___DDDnnnMMMMMMMMMmmmaaa___eeedddDDDDDDDDDvvveeesssaaa_________aaannntttgggvvvvvvvvveeennnttteeeeeeeeerrrrrruuuiiiaaaooolllrrrrrrrrrmmmlll123321213dddaaavvv_____________________555555555555555555555eeeeeeeeeeeeeeeeeeeee---------------------000000000000000000000453345435443455533534 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 KL Divergence
(b) WGAN at the last epoch

Figure 3: KL divergence of WGAN trained with different optimization strategies. Methods in (a) are named by a underscore-delimited string that denotes the category and version of the method. For (b) where we don't combine the models trained with different learning rates, the learning rate is appended at the end of the method name. For momentum and Nesterov momentum, we used  = 0.9. For Adagrad, we used the default = 1e-8.

For all afore-described algorithms, we experimented with their stochastic variants. Figure 3 shows the KL divergence between the WGAN-generated samples and the true distribution. When evaluated by the epoch and learning rate that yields the lowest discriminator loss on the validation set,

7

Under review as a conference paper at ICLR 2018

WGAN trained with Stochastic OMD (SOMD) achieved significantly lower KL divergence than the competing SGD variants. Evaluated by the last epoch, SOMD is much less sensitive to the choice of learning rate than the SGD variants. And considering the best performance across different learning rates, SOMD outperformed vanilla SGD and SGD with Adagrad, and performed comparably to SGD with momentum or Nesterov momentum.
6 GENERATING IMAGES FROM CIFAR10 WITH OPTIMISTIC ADAM
In this section we applying optimistic WGAN training to generating images, after training on CIFAR10. Given the success of Adam on training image WGANs we will use an optimistic version of the Adam algorithm, rather than vanilla OMD. We denote the latter by Optimistic Adam. Optimistic Adam could be of independent interest even beyond training WGANs. We present Optimistic Adam for (G) but the analog is also used for training (D). We trained on CIFAR10 images with

Algorithm 1 Optimistic ADAM, proposed algorithm for training WGANs on images.

Parameters: stepsize , exponential decay rates for moment estimates 1, 2  [0, 1), stochastic

loss as a function of weights t(), initial parameters 0

for each iteration t  {1, . . . , T } do

Compute stochastic gradient: ,t =  t()

Update biased estimate of first moment: mt = 1mt-1 + (1 - 1) · ,t

Update biased estimate of second moment: vt = 2vt-1 + (1 - 2) · 2,t

Compute bias corrected first moment: m^ t = mt/(1 - 1t)

Compute bias corrected second moment: v^t = vt/(1 - 2t)

Perform

optimistic

gradient

step:

t

=

t-1

-

2

·

m^ t v^t +

+  m^ t-1
v^t-1 +

Return T

Optimistic Adam with the hyper-parameters matched to Gulrajani et al. (2017), and we observe that it outperforms Adam in terms of inception score (see Figure 10), a standard metric of quality of WGANs (Gulrajani et al., 2017; Salimans et al., 2016). In particular we see that optimistic Adam achieves high numbers of inception scores after very few epochs of training. Importantly, we note that when training Adam we only trained the discriminator once after one iteration of the generator training. The latter is inline with the intuition behind the use of optimism: optimism hinges on the fact that the gradient at the next iteration is very predictable since it is coming from another regularized algorithm. However, if we train the other algorithm multiple times, then the gradient is not that predictable and the benefits of optimism are lost. We see that vanilla Adam performs poorly when the discriminator is trained only once in between iterations of the generator training. Moreover, even if we use vanilla Adam and train 5 times (D) in between a training of (G), as proposed by Arjovsky et al. (2017), then performance is again worse than Optimistic Adam with a 1:1 ratio of training. The same learning rate 0.0001 and betas (1 = 0.5, 2 = 0.9) as in Appendix B of Gulrajani et al. (2017) were used for all the methods compared. We also matched other hyper-parameters such as gradient penalty coefficient  and batch size. For a larger sample of images see Appendix G.

8 adam

7

adam_ratio1 optimAdam

optimAdam_ratio1 6

Inception Score

5

4

3

2

1 0 20 40 60 80 100 Epoch

(a) Inception score on CIFAR10, when training with Adam and Optimistic (b) Sample of images from Gen-

Adam. Ratio1, means we performed 1 iteration of training of (D) in be- erator of Epoch 94, which had

tween 1 iteration of (G). Otherwise we performed 5 iterations.

the highest inception score.

Figure 4: Comparison of Adam and Optimistic Adam on CIFAR10.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology, 33 (8):831­838, 2015.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, COLT '96, pp. 325­332, New York, NY, USA, 1996. ACM. ISBN 0-89791-811-8. doi: 10.1145/238061.238163. URL http://doi.acm.org/10.1145/238061.238163.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014. URL http://papers. nips.cc/paper/5423-generative-adversarial-nets.pdf.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160, 2017. URL http://arxiv.org/abs/1701.00160.
Ishaan Gulrajani, Faruk Ahmed, Mart´in Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017. URL http://arxiv. org/abs/1704.00028.
Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. arXiv preprint arXiv:1709.02738, 2017.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372­376, 1983.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT, pp. 993­1019, 2013a.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, NIPS'13, pp. 3066­3074, USA, 2013b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999792.2999954.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Gary D Stormo. Dna binding sites: representation and discovery. Bioinformatics, 16(1):16­23, 2000.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. In Advances in Neural Information Processing Systems, pp. 2989­2997, 2015.
Haoyang Zeng and David K Gifford. Predicting the impact of non-coding variants on dna methylation. Nucleic Acids Research, 2017.
Haoyang Zeng, Tatsunori Hashimoto, Daniel D Kang, and David K Gifford. Gerv: a statistical method for generative evaluation of regulatory variants for transcription factor binding. Bioinformatics, 32(4):490­496, 2015.
Haoyang Zeng, Matthew D Edwards, Ge Liu, and David K Gifford. Convolutional neural network architectures for predicting dna­protein binding. Bioinformatics, 32(12):i121­i127, 2016.
9

Under review as a conference paper at ICLR 2018

A VARIANTS OF GD TRAINING

For ease of reference we briefly describe the exact form of update rules for several modifications of GD training that we have used in our experimental results.

Adagrad:

w,t =



t i=1

w2 ,i

+

wt+1 = wt + w,t · w,t

,t =



t i=1

2,i

+

t+1 = t - ,t · ,t

(17)

Momentum:

vw,t+1 =  · vw,t +  · w,t wt+1 = wt + vw,t+1

v,t+1 =  · v,t +  · ,t t+1 = t - v,t+1

(18)

Nesterov momentum:
wahead = wt +  · vw,t vw,t+1 =  · vw,t +  · wL(t, wahead)
wt+1 = wt + vw,t+1

ahead = t -  · v,t v,t+1 =  · v,t +  · L(ahead, wt)
t+1 = t - v,t+1

(19)

B PERSISTENCE OF LIMIT CYCLES IN GD TRAINING

In Figure 5 we portray example Gradient Descent dynamics in the illustrative example described in Section 3 under multiple adaptations proposed in the literature. We observe that oscillations persist in all such modified GD dynamics, though alleviated by some. We briefly describe the modifications in detail first.

Gradient penalty. The Wasserstein GAN is based on the idea that the discriminator is approximating all 1-Lipschitz functions of the data. Hence, when training the discriminator we need to make sure that the function Dw(x) has a bounded gradient with respect to x. One approach to achieving this is weight-clipping, i.e. clipping the weights to lie in some interval. However, the latter might introduce extra instability during training. Gulrajani et al. (2017) introduce an alternative approach by adding a penalty to the loss function of the zero-sum game that is essentially the 2 norm of the gradient of Dw(x) with respect to x. In particular they propose the following regularized WGAN loss:
L(, w) = ExQ [Dw(x)] - EzF [Dw(G(z))] - Ex^Q ( xDw(x^) - 1)2

where Q is the distribution of the random vector x + (1 - )G(z) when x  Q and z  F . The expectations in the latter can also be replaced with sample estimates in stochastic variants of the training algorithms.

For our simple example, xDw(x) = w. Hence, we get the gradient penalty modified WGAN:

L(, w) = w, v -  -  ( w - 1)2

(20)

Hence, the gradient of the modified loss function with respect to  remains unchanged, but the gradient with respect to w becomes:

wt = v - t - 2wt

wt 2 - 1 wt 2

(21)

Momentum. GD with momentum was defined in Equation (18). For the case of the simple illustrative example, these dynamics boil down to:

mw,t+1 = · mw,t +  · (v - t) wt+1 =wt + mw,t+1

m,t+1 = · m,t -  · wt t+1 =t - m,t+1

(22)

10

Under review as a conference paper at ICLR 2018

Nesterov momentum. GD with Nesterov's momentum was defined in Equation (19). For the illustrative example, we see that Nesterov's momentum is identical to momentum in the absence of gradient penalty. The reason being that the function is bi-linear. However, with a gradient penalty, Nesterov's momentum boils down to the following update rule.

w^t =wt +  · mw,t

mw,t+1 = · mw,t +  · (v - t) - 2 · w^t

w^t 2 - 1 w^t 2

wt+1 =wt + mw,t+1

m,t+1 = · mw,t -  · wt (23) t+1 =t - m,t+1

Asymmetric training. Another approach to reducing cycling is to train the discriminator more frequently than the generator. Observe that if we could exactly solve the supremum problem of the discriminator after every iteration of the generator, then the generator would be simply solving a convex minimization problem and GD should converge point-wise. The latter approach could lead to slow convergence given the finiteness of samples in the case of stochastic training. Hence, we cannot really afford completely solving the discriminators problem. However, training the discriminator for multiple iterations, brings the problem faced by the generator closer to convex minimization rather than solving an equilibrium problem. Hence, asymmetric training could help with cycling. We observe below that asymmetric training is the most effective modification in reducing the range of the cycles and hence making the last-iterate be close to the equilibrium. However, it does not really eliminate the cycles, rather it simply makes their range smaller.

11

Under review as a conference paper at ICLR 2018
(a) GD dynamics with a gradient penalty added to the loss.  = 0.1 and  = 0.1.
(b) GD dynamics with momentum.  = 0.1 and  = 0.5.
(c) GD dynamics with momentum and gradient penalty.  = .1,  = 0.2 and  = 0.1.
(d) GD dynamics with momentum and gradient penalty, training generator every 15 training iterations of the discriminator.  = .1,  = 0.2 and  = 0.1.
(e) GD dynamics with Nesterov momentum and gradient penalty, training generator every 15 training iterations of the discriminator.  = .1,  = 0.2 and  = 0.1.
Figure 5: Persistence of limit cycles in multiple variants of GD training. 12

Under review as a conference paper at ICLR 2018

C ANOTHER EXAMPLE: LEARNING A CO-VARIANCE MATRIX

We demonstrate the benefits of using OMD over GD in another simple illustrative example. In this case, the example is does not boil down to a bi-linear game and therefore, the simulation results portray that the theoretical results we provided for bi-linear games, carry over qualitatively beyond the linear case.

Consider the case where the data distribution is a mean zero multi-variate normal with an unknown

co-variance matrix, i.e., x  N (0, ). We will consider the case where the discriminator is the set

of all quadratic functions:

DW (x) = Wij xixj = xT W x

(24)

ij

The generator is a linear function of the random input noise z  N (0, I), of the form:

GV (z) = V z

(25)

The parameters W and V are both d × d matrices. The WGAN game loss associated with these functions is then:

L(V, W ) = ExN(0,) xT W x - EzN(0,I) zT V T W V z

(26)

Expanding the latter we get:







L(V, W ) =ExN(0,)  Wij xixj  - EzN(0,I)  Wij Vikzk Vjmzm

ij

ij k

m







=ExN(0,)  Wij xixj  - EzN(0,I) 

Wij VikVjmzkzm

ij ijkm

= Wij ExN(0,) [xixj ] -

Wij VikVjmEzN(0,I) [zkzm]

ij ijkm

= Wij ij -

Wij VikVjm1{k = m}

ij ijkm

= Wij ij - Wij VikVjk
ij ijk

= Wij ij - VikVjk
ij k
Given that the covariance matrix is symmetric positive definite, we can write it as  = U U T . Then the loss simplifies to:

L(V, W ) = Wij ij - VikVjk = Wij (UikUjk - VikVjk)
ij k ijk

(27)

The equilibrium of this game is for the generator to choose Vik = Uik for all i, k, and for the discriminator to pick Wij = 0. For instance, in the case of a single dimension we have L(V, W ) = W · (2 - V 2), where 2 is the variance of the Gaussian. Hence, the equilibrium is for the generator
to pick V =  and the discriminator to pick W = 0.

Dynamics without sampling noise. For the mean GD dynamics the update rules are as follows:

Witj =Witj-1 +  ij -

Vitk-1Vjtk-1

k

Vitj =Vitj-1 + 

Witk-1 + Wkt-i 1 Vktj-1

k

(28)

13

Under review as a conference paper at ICLR 2018

We can write the latter updates in a simpler matrix form:

Wt =Wt-1 +   - Vt-1VtT-1 Vt =Vt-1 + (Wt-1 + WtT-1)Vt-1

(GD for Covariance)

Similarly the OMD dynamics are:

Wt =Wt-1 + 2  - Vt-1VtT-1 -   - Vt-2VtT-2 Vt =Vt-1 + 2(Wt-1 + WtT-1)Vt-1 - (Wt-2 + WtT-2)Vt-2

(OMD for Covariance)

Due to the non-convexity of the generators problem and because there might be multiple optimal
solutions (e.g. if  is not strictly positive definite), it is helpful in this setting to also help dynamics
by adding 2 regularization to the loss of the game. The latter simply adds an extra 2Wt at each gradient term W L(Vt, Wt) for the discriminator and a 2Vt at each gradient term V L(Vt, Wt) for the generator. In Figures 7 and 6 we give the weights and the implied covariance matrix G = V V T of the generator's distribution for each of the dynamics for an example setting of the step-size
and regularization parameters and for two and three dimensional gaussians respectively. We again
see how OMD can stabilize the dynamics to converge pointwise.

Stochastic dynamics. In Figure 8 and 9 we also portray the instability of GD and the robustness of the stability of OMD under stochastic dynamics. In the case of stochastic dynamics the gradients are replaced with unbiased estimates or with averages of unbiased estimates over a small minibatch. In the case of a mini-batch of one, the unbiased estimates of the gradients in this setting take the following form:

^ W,t = xtxtT - VtztztT VtT ^ V,t = -(Wt + WtT )VtztztT

(Stochastic Gradients)

where xt, zt are samples drawn from the true distribution and from the random noise distribution respectively. Hence, the stochastic dynamics simply follow by replacing gradients with unbiased
estimates:

Wt =Wt-1 + ^ W,t-1 Vt =Vt-1 - ^ V,t-1

(SGD for Covariance)

Wt =Wt-1 + 2^ W,t-1 - ^ W,t-2 Vt =Vt-1 - 2^ V,t-1 + ^ V,t-2
14

(SOMD for Covariance)

Under review as a conference paper at ICLR 2018
(a) GD dynamics.  = 0.1, T = 500,  = 0.3.
(b) OMD dynamics.  = 0.1, T = 500,  = 0.3. Figure 6: Stability of OMD vs GD in the co-variance learning problem for a two-dimensional gaussian (d = 2). Weight clipping in [-1, 1] was applied in both dynamics.
(a) GD dynamics.  = 0.1, T = 500,  = 0.3.
(b) OMD dynamics.  = 0.1, T = 500,  = 0.3. Figure 7: Stability of OMD vs GD in the co-variance learning problem for a three-dimensional gaussian (d = 3). Weight clipping in [-1, 1] was applied in both dynamics.
15

Under review as a conference paper at ICLR 2018

(a) Stochastic GD dynamics with mini-batch size 50.  = 0.02, T = 1000,  = 0.1.

(b) True Distribution (c) Iterate T - 50 (d) Iterate T - 35 (e) Iterate T - 20

(f) Iterate T

(g) Comparison of true distribution and distribution of generator at various points closer to the end of training.

Figure 8: Stochastic GD dynamics for covariance learning of a two-dimensional gaussian (d = 2). Weight clipping in [-1, 1] was applied to the discriminator weights.

(a) Stochastic OMD dynamics with mini-batch size 50.  = 0.02, T = 1000,  = 0.1.

(b) True Distribution (c) Iterate T - 50 (d) Iterate T - 35 (e) Iterate T - 20

(f) Iterate T

(g) Comparison of true distribution and distribution of generator at various points closer to the end of training.

Figure 9: Stability of OMD with stochastic gradients for covariance learning of a two-dimensional gaussian (d = 2). Weight clipping in [-1, 1] was applied to the discriminator weights.

16

Under review as a conference paper at ICLR 2018

D LAST ITERATE CONVERGENCE OF OMD IN BILINEAR CASE

The goal of this section is to show that Optimistic Mirror Descent exhibits last iterate convergence to min-max solutions for bilinear functions. In Section D.1, we provide the proof of Theorem 1, that OMD exhibits last iterate convergence to min-max solutions of the following min-max problem

min max xT Ay,
xy

(29)

where A is an abitrary matrix and x and y are unconstrained. In Section D.2, we state the appropriate extension of our theorem to the general case:

inf sup xT Ay + bT x + cT y + d .
xy

(30)

D.1 PROOF OF THEOREM 1

As stated in Section 4, for the min-max problem (29) Optimistic Mirror Descent takes the following form, for all t  1:

xt = xt-1 - 2Ayt-1 + Ayt-2

(31)

yt = yt-1 + 2AT xt-1 - AT xt-2

(32)

where for the above iterations to be meaningful we need to specify x0, x-1, y0, y-1.

As stated in Section 4 we allow any initialization x0  R(A), and y0  R(AT ), and set x-1 = 2x0 and y-1 = 2y-1, where R(·) represents column space. In particular, our initialization means that
the first step taken by the dynamics gives x1 = x0 and y1 = y0.

Before giving our proof of Theorem 1, we need some further notation. For all i  N, we set:

Mi = Aj (AT A)k, Ni = AT j AAT k

ti = ||NiAyt||22 +

MiAT xt

2 2

where k  Z and j  {0, 1} are such that: i = 2k + j.

With this notation, t0 =

AT xt

2 2

+

||Ayt||22,

t1

=

AT AAT xt

2 2

+

AAT Ayt 22, etc.

AAT xt

2 2

+

AT Ayt 22, 2t =

We also use the notation u, v X = uT XXT v, for vectors u, v  Rd and square d × d matrices X. We similarly define the norm notation ||u||X = u, u X . Given our notation, we have the following claim, shown in Appendix D.3.
Claim 1. Au, Av AMiT = u, v AT NiT+1 and u, Av AMiT = v, AT u .AT NiT

With our notation in place, we show (through iterated expansion of the update rule), the following lemma, proved in Appendix D.3:

Lemma 2.

For the dynamics of Eq. (31) and (32) and any initialization

1 2

x-1

= x0

 R(A), and

1 2

y-1

=

y0



R(AT

)

we

have

the

following

for

all

i,

t



N

such

that

i



0

and

t



2:

ti - ti-1 = 42it+-11 - 52it+-12 - 23 xt-2, Ayt-4 AMiT+1 - yt-2, AT xt-4 AT NiT+1 .

We are ready to prove Theorem 1. Its proof is implied by the following stronger theorem, and

Corollary 6.

Theorem 3.

Consider the dynamics of Eq. (31) and

(32) and any initialization

1 2

x-1

=

x0



R(A),

and

1 2

y-1

=

y0



R(AT ).

Let  = max

AAT + , AT A + , where for a ma-

trix X we denote by X+ its generalized inverse and by ||X|| its spectral norm. Suppose that max{||A||, ||AT ||}    1 and  < . Then, for all i  N:

i1 = 0i ,

(33)

and, for all i, t  N such that t  2, the following condition holds:

H(i, t) : ti  (1 - ) ti-1 + O(300).

(34)

17

Under review as a conference paper at ICLR 2018

Proof. Eq. (33) holds trivially as under our initialization x1 = x0 and y1 = y0. We use induction on t to prove (34). We start our proof by showing the inductive step, and postpone establishing the basis of our induction to the end of this proof. For the inductive step, we assume that H(i,  ) holds for all i  0 and 1   < t, for some t > 2. Assuming this, we show next that H(i, t) holds for all i. To do this, we make use of a couple of lemmas, whose proofs are given in Appendix D.3.

Lemma 4. Under the conditions of the theorem, for all i  0, t  2:

ti - ti-1  42it+-11 - 52it+-12 + 3 (1 + )it+-12 + 42 0i+1 . Lemma 5. Under the conditions of the theorem, for all i  0, t  0:

it+1



1 

it

.

Given these lemmas, we show our inductive step. So for t  3:

ti - it-1  42it+-11 - 52it+-12 + 3 (1 + )ti+-12 + 42 i0+1

 2((1 + ) - 1)ti+-12 + 442i0+1 + O(500)

 2(2 - 1)ti+-12 + 4400 + O(500)

= -2ti+-12 + 3(2it+-12 + 400) + O(500)



-

1 

2it-2

+

6300

+

O(500)

(35) (36) (37) (38)
(39)



-

1 

2ti-1

+

6300

+

O(400)

(40)

where for the first inequality we used Lemma 4, for the second inequality we used that ti+-11  it+-12 + O(300) (which is implied by the induction hypothesis), for the third inequality we used that   1 and that 0i+1  00 (which also easily follows from the fact that   1), for the fourth inequality we used Lemma 5 and that ti+-12  00 + O(200), which follows by first noting that it+-12  t0-2 (which easily follows from the fact that   1) and then noting that 0t-2  00 + O(200) (which follows by iteratively applying the inductive hypothesis and using (33)), and for the last inequality we used that ti-1  it-2 + O(300), which follows from the inductive hypothesis, and that  < . Hence:

it  (1 - ) it-1 + O(300).

This completes the proof of our inductive step.

It remains to show the basis of the induction, namely that H(i, 2) holds for all i  N. To do this, we

follow the derivation in lines (35)-(40) above, noticing that what we needed for this derivation to go

through holds for t = 2. In particular, the first inequality uses Lemma 4, which holds for t = 2. The

second inequality goes through because i1+1 = i0+1, for all i, given (33). The third inequality goes through for the same reasons that were used in the induction step. The fourth inequality goes

through since i0+1



1 

0i ,

which

holds

from

Lemma

5,

and

i0+1



00, which holds since

  1. The last inequality follows from the fact that 1i = i0.

Corollary 6. Under the conditions of Theorem 3, 0t 

AT xt

2 2

+ ||Ayt||22



O(200)

as

t  +. In particular, for large enough t, the last iterate of OMD is within O( 00) distance

from the space of equilibrium points of the game, where 00 is the distance of the initial point (x0, y0) from the equilibrium space, and where both distances are taken with respect to the norm

xT AAT x + yT AT Ay.

Proof of Corollary 6: It follows from (33) and (34) that:

t0  (1 - )t-110 + O


(1 - )t300
t=0

= (1 - )t-100 + O 200 ,

which shows the first part of our claim. For the second part of our claim recall that the solutions to (29) are all pairs (x, y) such that x is in the null space of AT and y is in the null space of A.

18

Under review as a conference paper at ICLR 2018

D.2 GENERAL BILINEAR CASE

Theorem 7. Consider OMD for the min-max problem (30):
inf sup xT Ay + bT x + cT y + d .
xy
Under the same conditions as Corollary 6 and whenever (30) is finite, OMD exhibits last iterate convergence in the same sense as in Corollary 6. In particular, for large enough t, the last iterate of OMD is within O( 0) distance from the space of equilibrium points of the game, where 0 is the distance of the initial point (x0, y0) from the equilibrium space, and where both distances are taken with respect to the norm xT AAT x + yT AT Ay. Whenever (30) is infinite or undefined, the OMD dynamics travels to infinity and we characterize its motion.

Proof of Theorem 7: Trivially, we need only consider functions of the form xT Ay + bT x + cT y. We consider the following decompositions of b and c:

b = b1 + b2 c = c1 + c2

where b1  R(A), b2  N (AT ) where c1  R(AT ), c2  N (A)

Given the above we can also define b3 and c3 as follows:

Ac3 = b1 AT b3 = c1

feasible since b1  R(A) feasible since c1  R(AT )

Then, we can make the following variable substition:
t = xt + tb2 + b3 t = yt - tc2 + c3 so that: AT t = AT xt + tAT b2 + AT b3
= AT xt + c1 since b2  N (AT ) At = Ayt - tAc2 + Ac3
= Ayt + b1 since c2  N (A)

We also state the OMD dynamics for xt and yt for problem (30):
xt = xt-1 - 2(Ayt-1 + b) + (Ayt-2 + b) = xt-1 - 2Ayt-1 + Ayt-2 - b
yt = yt-1 + 2(AT xt-1 + c) - (AT xt-2 + c) = yt-1 + 2AT xt-1 - AT xt-2 + c
Note that given this update step:
xt+1 = xt - 2Ayt + Ayt-1 - b xt+1 = xt - b2 - 2Ayt + Ayt-1 - Ac3 xt+1 = xt - b2 - 2A(yt + c3) + A(yt-1 + c3) xt+1 = xt - b2 - 2A(yt - c2t + c3) + A(yt-1 - c2(t - 1) + c3) xt+1 + b2(t + 1) = xt + b2t - 2A(yt - c2t + c3) + A(yt-1 - c2(t - 1) + c3) xt+1 + b2(t + 1) + b3 = xt + b2t + b3 - 2A(yt - c2t + c3) + A(yt-1 - c2(t - 1) + c3) t+1 = t - 2At + At-1 Analogously: t+1 = t + 2AT t - AT t-1
19

Under review as a conference paper at ICLR 2018

Note that these are precisely the dynamics for which we proved convergence in Theorem 1. Thus,
by invoking Theorem 3 and Corollary 6 on the sequence (t, t) and then substituting back (xt, yt), we have that for all large enough t:

xt = -b2t - b3 + x(t) yt = c2t - c3 + y(t)
such that ||AT x(t)||2, ||A y(t)||2  O(

0),

where 0 = ||AT (x0 + b3)||22 + ||A(y0 + c3)||22.
In particular, this shows that, whenever (30) is finite (i.e. b2 = c2 = 0), OMD exhibits last iterate convergence. For large enough t, the last iterateof OMD is within O( 0) distance from the space of equilibrium points of the game, where 0 is the distance of (x0 + b3, y0 + c3) from the equilibrium space in the norm xT AAT x + yT AT Ay. Whenever (30) is infinite or undefined, the OMD dynamics travels to infinity linearly, with fluctuations around the divergence specified as above.

D.3 OMITTED PROOFS
Proof of Proposition 1: To show this, we consider the 2 distance of the solution at time t. First, recall the GD update step in the special case of f (x, y) = xT y:
xt = xt-1 - yt-1 yt = yt-1 + xt-1

Then, note that the squared 2 distance of the running iterate (xt, yt) to the unique equilibrium solution (0, 0) is given by d(t) := ||xt||22 + ||yt||22, which we can calculate:
||xt||22 = ||xt-1||22 - 2xTt-1yt-1 + 2||yt-1||22 ||yt-1||22 = ||yt-1||22 + 2ytT-1xt-1 + 2||xt-1||22
 d(t) = d(t - 1) + 2d(t - 1) = (1 + 2)d(t - 1)
This indicates that for any value of  > 0, the running iterate of GD diverges from the equilibrium.

Proof of Claim 1: For our first claim, observe that:
Au, Av AMiT = uT AT AMiT MiAT Av = uT AT A(AT A)k(Aj )T Aj (AT A)kAT Av = uT AT (AAT )kA(Aj )T Aj AT (AAT )kAv = uT AT [(AAT )kA(Aj)T ][AjAT (AAT )k]Av = uT AT NiT+1Ni+1Av = u, v AT NiT+1
20

Under review as a conference paper at ICLR 2018
For our second claim: u, Av AMiT = uT AMiT MiAT Av = uT A(AT A)k(AT A)j(AT A)kAT Av if j = 0: = uT A(AT A)k(AT A)kAT Av = uT A[AT (AAT )k][(AAT )kA]v = uT A[AT NiT ][NiA]v = v, AT u AT NiT otherwise: = uT A(AT A)kAT A(AT A)kAT Av = uT A[(AT A)kAT A][(AT A)kAT A]v = uT A[AT (AAT )kA][AT (AAT )kA]v = uT A[AT NiT ][NiA]v = v, AT u AT NiT
Proof of Lemma 2: First, we note the following scaled update rule: MiAT xt = Mi AT xt-1 - 2AT Ayt-1 + AT Ayt-2 NiAyt = Ni Ayt-1 + 2AAT xt-1 - AAT xt-2
Then, taking the norm of both sides, and using the statements of Claim 1: ||xt||A2 MiT = ||xt-1||2AMiT + 42 ||yt-1||A2 T NiT+1 + 2 ||yt-2||A2 T NiT+1 - 4 xt-1, Ayt-1 AMiT + 2 xt-1, Ayt-2 AMiT - 42 yt-1, yt-2 AT NiT+1
||yt||A2 T NiT = ||yt-1||2AT NiT + 42 ||xt-1||2AMiT+1 + 2 ||xt-2||2AMiT+1 + 4 yt-1, AT xt-1 AT NiT + 2 yt-1, AT xt-2 AT NiT - 42 xt-1, xt-2 AMiT+1
 ti = ||xt||2AMiT + ||yt||2AT NiT = ti-1 + 42ti+-11 + 2it+-12 + 2( xt-1, Ayt-2 AMiT - yt-1, Axt-2 )AT NiT - 42( xt-1, xt-2 AMiT+1 + yt-1, yt-2 )AT NiT+1
Expanding the first pair of inner products above (and using Claim 1 again): xt-1, Ayt-2 AMiT - yt-1, AT xt-2 AT NiT = xt-2 - 2Ayt-2 + Ayt-3, Ayt-2 AMiT
- yt-2 + 2AT xt-2 - AT xt-3, AT xt-2 AT NiT
= -2(||yt-2||A2 T NiT+1 + ||xt-2||2AMiT+1 ) + ( xt-2, xt-3 AMiT+1 + yt-2, yt-3 )AT NiT+1
Then, multiplying by 2 and substituting into the previous derivation yields: ti - ti-1 = 42it+-11 - 32it+-12 + 22( xt-2, xt-3 AMiT+1 + yt-2, yt-3 )AT NiT+1 - 42( xt-1, xt-2 AMiT+1 + yt-1, yt-2 )AT NiT+1
21

Under review as a conference paper at ICLR 2018

Now, consider the following inner product:
xt-2, xt-1 AMiT+1 + yt-2, yt-1 AT NiT+1 = xt-2, xt-2 - 2Ayt-2 + Ayt-3 AMiT+1 + yt-2, yt-2 + 2AT xt-2 - AT xt-3 AT NiT+1

= it+-12 +  xt-2, Ayt-3 AMiT+1 - yt-2, AT xt-3 AT NiT+1

Once again, we multiply by -42 and substitute:
it - ti-1 = 42ti+-11 - 72ti+-12 + 22( xt-2, xt-3 AMiT+1 + yt-2, yt-3 )AT NiT+1 + 43( yt-2, AT xt-3 -AT NiT+1 xt-2, Ayt-3 )AMiT+1

Now, we use the update step for time t - 2; For all t  1, this is well-formed, since x-1 and y-1 are defined. To ensure that this step is sound for t = 0 requires we define the following, where X+ denotes the generalized inverse:

x-2

=

4x0

+

1 (AT 

)+y0

y-2

=

4y0

-

1 

A+x0

We

define these

such

that:

AT x-2

=

4AT x0

+

y0 

and

Ay-2

=

4Ay0

-

x0 

(since

x0



R(A) and

y0  R(AT ), and thus the following equalities hold:

x0 = x-1 - 2Ay-1 + Ay-2 y0 = y-1 + 2AT x-1 - AT x-2

This allows us to use the following expansion freely for all t  0:

xt-2 = xt-3 - 2Ayt-3 + Ayt-4 yt-2 = yt-3 + 2AT xt-3 - AT xt-4

= xt-3 - 2Ayt-3 = xt-2 - Ayt-4 = yt-3 + 2AT xt-3 = yy-2 + AT xt-4

We can gather the inner product terms and use this update rule to get our final desired result: it - ti-1 = 42ti+-11 - 72ti+-12 + 22( xt-2, xt-3 - 2Ayt-3 AMiT+1 + yt-2, yt-3 + 2AT xt-3 )AT NiT+1
= 42it+-11 - 52ti+-12 - 23( xt-2, Ayt-4 AMiT+1 - yt-2, AT xt-4 )AT NiT+1

Proof of Lemma 4: To prove this, first consider the following trivial inequality:

yt-2 - AT xt-4

2 AT NiT+1

+

||xt-2

+

Ayt-4||A2 MiT+1

= ||yt-2||A2 T NiT+1 - 2 yt-2, AT xt-4 AT NiT+1 +

AT xt-4

2 AT NiT+1

+ ||xt-2||2AMiT+1 + 2 xt-2, Ayt-4 AMiT+1 + ||Ayt-4||A2 MiT+1

0

Then, rearranging:

2 yt-2, AT xt-4 AT NiT+1 - 2 xt-2, Ayt-4 AMiT+1  it+-12 +  ti+-12 +

AT xt-4 2it+-14

2 AT NiT+1

+

||Ayt-4||2AMiT+1

22

Under review as a conference paper at ICLR 2018

Now, using the update rule we know that:
||xt-2||A2 MiT = ||xt + 2Ayt-1 + Ayt-2 - Ayt-3||A2 MiT  ||xt||2AMiT +  ||2Ayt-1 + Ayt-2 - Ayt-3||2AMiT  ||xt||2AMiT + 4 ||Ayt-3||A2 MiT
it-2  it + 4it+-13  ti + 4(1 - 2)t-3i0+1  ti + 40i
Applying this for t - 4 (using the same construction of x-2, y-2 found in the Proof of Lemma 2 to ensure well-formedness) yields:
2 yt-2, AT xt-4 AT NiT+1 - 2 xt-2, Ayt-4 AMiT+1  it+-12 + 2 ti+-12 + 40i+1  (1 + 2 )it+-12 + 420i+1
Now, we can apply this bound to the result of Lemma 2:
it - it-1 = 42it+-11 - 52it+-12 - 23( xt-2, Ayt-4 AMiT+1 - yt-2, AT xt-4 )AT NiT+1  42it+-11 - 52it+-12 - 23(2it+-12 + 4i0+1)

Which is what we sought out to prove.

Proof of Lemma 5: Now, note that choosing x0  R(A) = xt  R(A) = R(AAT )  t, due to the update step. Similarly, yt  R(AT ). Thus, x = AAT (AAT )+x, and similarly y = AT A(AT A)+. Letting Q = (AAT )+ and P = (AT A)+, and recalling key properties of the generalized inverse:

ti =

MiAT xt

2 2

+

||NiAyt||2i+1

=

MiAT AAT Qxt

2 2

+

NiAAT AP yt

2 2

=

Mi+2AT Qxt

2 2

+

||Ni+2AP

yt||22

=

(AT )j Q(AAT )k+1xt

2 2

+

Aj P (AT A)k+1yt

2 2

=

QMi+1AT x

2 2

+

||P

Ni+1Ayt||22

P Mi+1AT x

2 2

+

||QNi+1Ayt||22

if j = 0 if j = 1

 max (||Q||, ||P ||) · it+1

23

Under review as a conference paper at ICLR 2018
E DNA-GENERATION WGAN ARCHITECTURE

Operation

Kernel Output Shape BatchNorm? Nonlinearity

Length of DNA sequence: L = 6 Gradient penalty:  = 1e-4 Batch size: 512 G(z) : z Fully connected
Fully connected
Reshape Upsampling by 2 Convolution D(x) : x Convolution Fully connected Fully connected

[1 × 3] × 4
[1 × 3] × 16 -

50

128

16

×

L 2

16

×

1

×

L 2

16 × 1 × L

4×1×L

4×1×L 16 × 1 × L 32 1

no yes no
no no no

tanh tanh tanh
tanh tanh linear

F CIFAR10 WGAN ARCHITECTURE

Operation

Kernel Output Shape BatchNorm? Nonlinearity

Gradient penalty:  = 10 Batch size: 64 G(z) : z Fully connected Fully connected Reshape TransposedConv Convolution TransposedConv Convolution D(x) : x Convolution Convolution Convolution Fully connected Fully connected

[5 × 5] × 128 [5 × 5] × 64 [5 × 5] × 64 [5 × 5] × 3
[5 × 5] × 64 [5 × 5] × 128 [5 × 5] × 128 -

100 1024 8192 128 × 8 × 8 128 × 16 × 16 64 × 16 × 16 64 × 32 × 32 3 × 32 × 32
3 × 32 × 32 64 × 32 × 32 128 × 14 × 14 128 × 7 × 7 1024 1

no yes yes yes yes no
no no no no no

LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU tanh
LeakyReLU LeakyReLU LeakyReLU LeakyReLU linear

24

Under review as a conference paper at ICLR 2018
G CIFAR10 GENERATOR IMAGE SAMPLES
(a) Sample of images from Generator of Epoch 94, which had the highest inception score. Figure 10: Samples of images from Generator trained via Optimistic Adam on CIFAR10. G.1 COMPARISON OF EARLY EPOCH IMAGES OF OPTIMISTIC ADAM VS ADAM Below we give samples of images from an early epoch 19 Generator trained via Optimistic Adam with 1:1 training ratio, Adam with 1:1 and Adam with 5:1 ratio on CIFAR10. We see that Optimistic Adam has already achieved visually appealing results unlike the latter two vanilla Adam based versions.
25

Under review as a conference paper at ICLR 2018
Figure 11: Sample of images from Generator of Epoch 19 trained via Optimistic Adam and 1:1 training ratio.
Figure 12: Sample of images from Generator of Epoch 19 trained via Adam and 1:1 training ratio. 26

Under review as a conference paper at ICLR 2018 Figure 13: Sample of images from Generator of Epoch 19 trained via Adam and 5:1 training ratio. 27

