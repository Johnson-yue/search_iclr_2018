Under review as a conference paper at ICLR 2018

OMIE: ON-LINE MUTUAL INFORMATION ESTIMATOR
Anonymous authors Paper under double-blind review

ABSTRACT
This paper presents OMIE, an Online Mutual Information Estimator(OMIE) that is linearly scalable both on the dimension of the data as well as sample size. OMIE is back-propable and we prove that it is strongly consistent. We apply the estimation principle underlying the mutual information estimator to general dependency f-divergence as well as integral probability metrics dependency measures. We illustrate a handful of applications in which OMIE is succesfully applied to enhance the property of generative models in both unsupervised and supervised settings. We apply our framework to estimate the information bottleneck and apply it in tasks related to supervised classification problem and show that there is substantial added flexibility and improvement in these settings.

1 INTRODUCTION

Mutual information is an important quantity for expressing and understanding the relationship between random variables. As a fundamental tool of data science, it has found application in a range of domains and tasks, including genetics, blind source separation (BSS, e.g., independent component analysis, Hyva¨rinen et al., 2004), information bottleneck (IB, Tishby et al., 2000), feature selection (Kwak & Choi, 2002; Peng et al., 2005), and causality (Butte & Kohane, 2000).

In contrast to correlation, mutual information captures the absolute statistical dependency between
two variables, and thus can act as a measure of true dependence. Put simply, mutual information is
the shared information of two random variables, X and Z, defined on the same probability space, (X × Z, F ), where X × Z is the domain over both variables (such as Rm × Rn) and F is the set of all possible outcomes over both variables. The mutual information has the form 1:

I(X; Z) =

X ×Z

log

dPXZ dPX  PZ

dPXZ

=

DKL(PXZ

||

PX



PZ )

(1)

such that PXZ : F  [0, 1] is a measure (commonly known as a joint probability distribution in this context), and PX = Z dPXZ and PZ = X dPXZ are the marginals.
The mutual information is notoriously difficult to compute. Exact computation is only tractable with discrete variables (as the sum can be computed exactly) or with a limited class of problems where the probability distributions are known and for low dimensions. For more general problems, common approaches include binning (Fraser & Swinney, 1986; Darbellay & Vajda, 1999), kernel density estimation (Moon et al., 1995; Kwak & Choi, 2002), edgeworth expansion based estimators Van Hulle (2005) and likelihood-ratio estimators based on support vector machines (SVMs, e.g., Suzuki et al., 2008). While it can be estimated from empirical samples, the mutual information estimates still make critical assumptions about the underlying distribution of samples, and estimate errors can reflect this. In addition, these estimators typically do not scale well with sample size or dimension.

More recently, there has been great progress in the estimation of -divergences (Nguyen et al., 2010) (a.k.a. f -divergences) and integral probability metrics (IPMs, Sriperumbudur et al., 2009) using deep neural networks (e.g., in the context of -divergences and the Wasserstein distance or Fisher IPMs, Nowozin et al., 2016; Arjovsky et al., 2017; Mroueh & Sercu, 2017). These methods are at the center of generative adversarial networks (GANs Goodfellow et al., 2014), which train a generative model without any explicit assumptions about the underlying distribution of the data. One

1We assume the convention that log is the natural log, so that our units of information are in nats.

1

Under review as a conference paper at ICLR 2018

perspective on these works is that, given the correct constraints on a neural network, the network can be used to compute a variational lower-bound on the distance or divergence of interest.
In this paper we look to extend this estimation strategy to mutual information as given in equation 1, which we note corresponds to the Kullback-Leibler (KL-) divergence between the joint, PXZ and the product of the marginal distributions, PX  PZ , i.e., DKL(PXZ || PX  PZ ). This observation can be used to help formulate variational Bayes in terms of implicit distributions (Mescheder et al., 2017) or INFOMAX (Brakel & Bengio, 2017). However, as we will show below, this estimate for the mutual information is not very tight and does not generally work well in practice.
We introduce a better estimator for the mutual information based on the Donsker-Varadhan representation of the KL-divergence (Ruderman et al., 2012). As with those introduced by Nowozin et al. (2016), our estimator is scalable, flexible, and is completely trainable via back-propagation. The contributions of this paper are as follows.
· We introduce the on-line mutual information estimator (OMIE), providing its theoretical bases and generalizability to other information metrics.
· We illustrate that our estimator can be used to train a model with less mode-dropping and richer learned representation for training adversarial models (such as adversarially-learned inferences, ALI, Dumoulin et al., 2016).
· We demonstrate how to use OMIE to improve reconstructions and inference in Adversarially Learned Inference Dumoulin et al. (2016) on large scale Datasets.
· We show that our estimator allows the application of Information Bottleneck method Tishby et al. (2000) in a continuous setting with very general encoder's and that it outperforms variational bottleneck methods.

2 BACKGROUND

2.1 MUTUAL INFORMATION

Mutual information is a Shannon entropy-based measure of dependence between random variables. Following the definition in Equation 1, the mutual information can be understood as the decrease in the uncertainty of X given Z

I(X; Z) := H(X) - H(X | Z) = H(Z) - H(Z | X),

(2)

where H is the Shannon entropy and H(Z | X) is the conditional entropy of Z given X (the amount
of information in Z not given from X). Using simple manipulation, we write the mutual information
as the Kullback-Leibler (KL-) divergence between the joint PXZ and the product of the marginals PX  PZ :

I(X; Z) = H(X) + H(Z) - H(X, Z) = DKL(PXZ || PX  PZ ),

(3)

where H(X, Z) is the joint entropy of X and Z.

The intuitive meaning is immediately clear: the larger the divergence between the joint the product of the marginal, the stronger the dependence between X and Z, and the mutual information is zero exactly when the KL-divergence is zero.

3 THE ON-LINE MUTUAL INFORMATION ESTIMATOR
Estimation of the mutual information is challenging due to the intractability of the joints and the marginals. In this section, we introduce an on-line, consistent mutual information estimator (OMIE) by exploiting the Donsker-Varadhan representation of the KL-divergence as well as the representational power of deep neural networks.
3.1 THE DONSKER-VARADHAN BOUND
OMIE relies on the Donsker-Varadhan representation of the Kullback-Leibler (KL-) divergence, which provides a tight lower-bound on the mutual information that works for a number of important applications.

2

Under review as a conference paper at ICLR 2018

Theorem 1 (Donsker-Vardhan representation of the KL-divergence). Let  be a measure space and P and Q any two distributions on  such that P is absolutely continuous with respect to Q. Consider the class L(Q) of functions T : X  R that are essentially bounded with respect to Q on X . Then we have (Donsker & Varadhan, 1983) (Ruderman et al., 2012)

DKL(P || Q) = sup EP[T ] - log(EQ[eT ])
T L(Q)

(4)

Hence for a given choice of family of functions F  L(Q), the following bound holds:

DKL(P || Q)  sup EP[T ] - log(EQ[eT ])
T F

(5)

The bound (5) is known as the compression lemma in the PAC-Bayes community (Banerjee, 2006). It can be viewed as a consequence of the information inequality, which gives a lower bound on the description length of coding P by means of any distribution densit :

EP[- log ]  EP[- log dP] = H(P)

In

fact,

choosing

for



the

Gibbs

density



=

1 Z

exp(T )dQ

for

a

given

T



F,

we

get

(6)

EP[T ] - log Z  EP[log dP] - EP[log dQ] = DKL(P || Q) and plugging normalization condition Z = EQ[eT ] yields (5).

(7)

3.2 MUTUAL INFORMATION ESTIMATOR

We are interested in the case of a joint random variable (X, Z) on a joint probability space  = X × Z, and P = PXZ is the joint distribution, Q = PX  PZ is the product distribution. P is then absolutely continuous with respect to Q, and we obtain the following variational lower-bound to the mutual information:

I(X; Z) = DKL(PXZ || PX  PZ )  EPXZ [T (x, z)] - log(EPX PZ [eT (x,z)]).

(8)

The inequality in equation 8 is intuitive in terms of deep learning optimizations. The idea is to parametrize the function T by a deep neural network with parameters   , turning the infinite dimensional problem above into a much easier parametric optimization problem in R||.

In the following we call T the statistic network. The expectations in the above lower-bound can then be estimated using a Monte-Carlo (MC) estimation using empirical samples (x, z)  PXZ and
the objective is maximized by gradient ascent. If we denote

^ = arg sup EPXn Z [T(x, z)] - log(EPXn PZn [eT(x,z¯)])


(9)

we obtain the On-line Mutual Information Estimator (OMIE):

Definition 3.1. On-line mutual information estimator (OMIE)

I(X; Z)n = EPXn Z [T^(x, z)] - log(EPnX PZn [eT^(x,z¯)]).

(10)

The Algorithm 1 details the implementation of OMIE. We show in the Appendix that OMIE has the desirable strong consistency and convergence properties.

3.3 GENERALIZATION TO -INFORMATION MEASURE

We close this section by pointing out OMIE can be generalized to -divergence (often called f divergences) based dependency measures

D(P || Q) :=




dP dQ

dQ,

(11)

indexed by a convex function  : (0, )  R such that (1) = 0. The convexity of  in order to leverage Fenchel-Legendre duality as well as powerful associated variational representations in terms of their convex dual. The analogue of the representation (1) is (Ruderman et al., 2012)

D(P || Q) = sup EP[T ] - (EQ )(T )
T L(Q)

(12)

3

Under review as a conference paper at ICLR 2018

Algorithm 1 . Mutual Information Estimation

  initialize network parameters

repeat

(x(1), z(1)), . . . , (x(N), z(N))  PXZ

z¯(1), . . . , z¯(N)  PZ

V ()



1 n

n i=1

T (x(i) ,

z(i))

-

log(

1 n

   + V() until convergence

Draw N samples from the joint distribution

Draw N from the Z marginal distribution

n i=1

exp(T

(x(i)

,

z¯(i)

))

evaluate the statistic network

Update the statistic network parameters

where (EQ ) is the Fenchel-dual of the operator µ  EQf (µ) acting on the space of integrable

functions µ  L1(Q). The case (u) = u log(u) corresponds to the KL divergence. For the 2-

divergence

(u)

=

(u

-

1)2,

and

we

can

compute

(EQ)(T

)

=

EQ[

1 2

T

2

+

T

].

Just as the mutual information can be understood as the Kullback-leibler (KL-) divergence between

the joint and product of marginals distributions, we can define a family of -information measures

as -divergences:

I(X; Z) := D(PXZ || PX  PZ )

(13)

and construct an online estimator just like in the previous section.

4 APPLICATIONS, RELATED WORKS, AND EXPERIMENTS
In this section, we present applications of mutual information through the on-line mutual information estimator (OMIE), as well as competing methods that are designed to achieve the same goals. In the following section, we present experimental results touching on each of these applications.
4.1 MUTUAL INFORMATION ESTIMATION
Mutual information is an important quantity for analyzing and understanding the statistical dependencies between random variables. The most straightforward application for OMIE then is estimation of the mutual information.
Related works on estimating mutual information There are a number of methods that can also be used to estimate mutual information given only empirical samples of the joint distribution of variables of interest. The fundamental difficulty is the intractability of joint and product of marginals. Kraskov et al. (2004) proposes a K-NN estimator based on estimating the entropy terms of the mutual information. This estimator comes with the usual limitations of non-parametric methods. Van Hulle (2005) presents an estimator built around Edgeworth expansions. The entropy of the distribution is approximated by that of a gaussian with additional correction brought by higher order cumulants. This method is only tractable in very low dimensional data and is breaks down when departure from gaussianity is too severe. Suzuki et al. (2008) exploits a likelihood ratio estimator using kernel methods Barber & Agakov (2003).
OMIE, on the other hand, inherits all the benefits of neural networks in scalability and can, in principle, calculate the mutual information using a large number of high-dimensional samples. We posit then that, given empirical samples of two random variables, X and Z, and a high-enough capacity neural network, OMIE will provide good estimates for the mutual information.
Experiment: estimating mutual information between two Gaussians We begin by comparing OMIE to the k-means-based non-parametric estimator found in Kraskov et al. (2004). In our experiment, we consider two bivariate Gaussian random variables Xa and Xb with correlation, corr(Xa, Xb) =   [-0.99, -0.9, -0.7, -0.5, -0.3, -0.1, 0., 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]. As the mutual information is invariant to continuous bijective transformation of the considered variables, it is enough to consider standardized Gaussians marginals.
Next, we compare the estimation of the mutual information in OMIE, which relies on the DonskerVaradhan representation of the variational lower bound against the one given by Nowozin et al.

4

Under review as a conference paper at ICLR 2018

Figure 1: Mutual information between two bivariate gaussians with component-wise correlation of corr(Xa, Xb) =   [-0.99, -0.9, -0.7, -0.5, -0.3, -0.1, 0., 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]. Note that the OMIE is nearly perfect in estimating the mutual information in this setting.
(2016) and used in Mescheder et al. (2017). As we show in Figure 2 that OMIE provides an estimate that much closer to the ground truth than that provided by Nowozin et al. (2016).

(a) Mutual Information estimation on bivari- (b) Mutual Information estimation error on

ate gaussians of dimension 2

bivariate gaussians of dimension 50

Figure 2: Comparison between the Donsker-Varadhan and the f-divergence variational representation of the mutual information. As can be clearly seen, the Donsker-Varadhan representation of the KL-divergence used by OMIE is much tighter than that given by the KL-divergence estimator in Nowozin et al. (2016).

4.2 ENTROPY MAXIMIZED GANS TO IMPROVE GENERATIVE SUPPORT
Mode-dropping (Che et al., 2016) is a common pathology of generative adversarial networks (GANs, Goodfellow et al., 2014) where the generator does not generate all of the modes in the target dataset (such as not generating images that correspond to specific labels). We identify at least two source of mode dropping in GANs:
· Discriminator liability: In this case, the discriminator classifies only a fraction of the real data as real. As a consequence of this, there is no gradient for the generator to learn to generate modes that have poor representation under the discriminator.
· Generator liability: The generator is greedy and concentrates its probability mass on the smallest subset most likely to fool the discriminator. Here, the generator simply focuses on a subset of modes which maximize the discriminator's Bayes risk.
We focus here on the second type of mode-dropping. In order to alleviate the greedy behavior of the generator, we encourage the generator to maximize the entropy of the generated data. This can be achieved by modifying the GAN objective for the generator with a negative-entropy term.
5

Under review as a conference paper at ICLR 2018

Our treatment involves the typical GAN setting in Goodfellow et al. (2014). First, let the generated probability distribution, Q, be induced by a function, G : Z  X , let z  p(z) be a sample drawn form a (relatively simple) prior density (such as a spherical Gaussian), and let x  P be a sample drawn from the empirical target distribution. In this setting, the discriminator, D : X  R, which is
modeled by a deep neural with a sigmoid nonlinearity, is optimized to maximize the value function:

arg max V (D, G) = arg max EP[log D(x)] + Ep(z)[log (1 - D(G(z)))].
DD

(14)

As was observed in Nowozin et al. (2016), maximizing the value function, V , corresponds to

maximizing the variational lower-bound of 2DJS(P||Q)-2 log 2. The generator is then optimized to minimize V alternatively as the discriminator maximizes it. However, in practice the generator

maximizes a proxy, Ep(z)[log(D(G(z)))], which can palliate vanishing gradients.

In order to palliate mode-dropping, we introduce a term to the generator objective which maximizes

the estimated mutual information provided by OMIE. Maximizing the mutual information in the

joint distribution induced by the prior and generator function, QXZ, corresponds to maximizing the marginal entropy, HQ(X):

IQ(X; Z) = HQ(X) - HQ(X | Z).

(15)

Thus, training the generator to maximize the mutual information can indirectly increase the entropy

of the generated samples and reduce mode-dropping. The generator objective then becomes:

arg max Ep(z)[log(D(G(z)))] + I(G(Z); Z).
G

(16)

As the samples G(z) are differentiable w.r.t. the parameters of G and OMIE is a completely differentiable function, we can maximize the mutual information using back-propagation and gradient descent by only specifying this additional loss term.

Related works on mode-dropping Mode regularized GANs Che et al. (2016) proposes to learn a reconstruction distribution and teaches the generator to sample from it. The intuition behind this is that the reconstruction distribution is a de-noised or smoothed version of the data distribution, and thus easier to learn. However, the connection to reducing mode dropping is only indirect.
InfoGAN (Chen et al., 2016) is a method which attempts to improve mode coverage by leveraging the Agokov and Baber conditional entropy variational lower-bound (Barber & Agakov, 2003). This bound involves approximating the intractable conditional distribution PZ|X by using a tractable recognition network, F : X  Z. In this setting, the variational approach only bounds the conditional entropy, H(X | Z), so the model is not maximizing the mutual information between Z and X and does not benefit of maximizing the marginal entropy, H(X).
VEEGAN Srivastava et al. (2017), like InfoGAN, makes use of a recognition network to maximize the Agokov and Baber variational lower-bound, but is trained like adversarially learned inference (ALI, Dumoulin et al., 2016, , see the following section for details). Since, at convergence the joint distributions of the generative and recognition networks are matched, this has the effect of minimizing the conditional entropy, H(X|Z).
Our approach is closest to that of Dai et al. (2017) in that they also formulated a GAN with entropy regularization of the generator. Interestingly, they show that, in the context of Energy-based GANs, such a regularization strategy yields a discriminator score function that at equilibrium is proportional to the log density of the empirical distribution. The main difference between their work and our regularized GAN formulation is that we use OMIE to estimate entropy while they used a nonparametric estimate that does not scale particularly well with dimensionality of the data domain.

Experiment: swiss-roll dataset Next, we apply OMIE to improve mode coverage when training a generative adversarial network (GAN, Goodfellow et al., 2014). Folowing Equation 16, we estimate the mutual information using OMIE and use this estimate to maximize the entropy of the generator. We demonstrate this effect on a Swiss-roll dataset, comparing two models, one with  = 0 (which corresponds to the orthodox GAN as in Goodfellow et al. (2014)) and one with  = 1.0, which corresponds to entropy-maximization.
Our results show improved mode coverage over the baseline with no mutual information objective. This confirms our hypothesis that maximizing mutual information helps against mode-dropping in this simple setting.

6

Under review as a conference paper at ICLR 2018

(a) GAN. 1000 iterations

(b) GAN. 3000 iterations

(c) GAN. 5000 iterations

(d) OMIEGAN 1000 iterations (e) OMIEGAN. 3000 iterations (f) OMIEGAN. 5000 iterations
Figure 3: The generator of the GAN model without mutual information maximization suffers from mode collapse (has poor coverage of the target dataset). In addition to the the GAN objective, OMIEGAN maximizes the mutual information I(G(Z); Z). The OMIEGAN generator learns a distribution with a high amount of structured noise. In addition, OMIEGAN converges faster, shows better coverage of the ground truth distribution, as well as less mode dropping.

(a) Original data

(b) GAN

(c) OMIEGAN

Figure 4: KDE plots for OMIEGAN samples and GAN samples on 25 Gaussians dataset. It is evident from the plot that again OMIE does a decent job of capturing all the modes of the distribution while the standard GAN drops quite a few.

4.3 IMPROVING THE REPRESENTATION OF BI-DIRECTIONAL ADVERSARIAL MODELS

Adversarial bi-directional models are an extension of GANs which incorporate a reverse model, F : X  Z. These were introduced in adversarially-learned inference (ALI, Dumoulin et al., 2016), closely related BiGAN (Donahue et al., 2016), and variants that minimize the conditional entropy (ALICE, Li et al., 2017). These models train a discriminator to maximize the value function of Equation 14 over the two joint distributions, QXZ and PXZ induced by the forward and reverse models, respectively.

In principle, ALI should be able to learn a feature representation as well as palliate mode dropping. However, in practice ALI guarantees neither due to identifiability issues (Li et al., 2017). This is further evident as the generated samples from the forward model can be poor reconstructions of the data given the inferred latent representations from the reverse model. In order to address these issues, ALICE introduces an additional term to minimize the conditional entropy by minimizing the reconstruction error.

To demonstrate the connection to mutual information, it can be shown that the reconstruction error is bounded (see Appendix for proof):

R  DKL(PXZ || QXZ ) - IP (X; Z) + HP (Z)

(17)

7

Under review as a conference paper at ICLR 2018

If HP (Z) is fixed (which can be accomplished in how we define the reverse model), then matching the joint distributions training in addition to maximizing the mutual information between X and Z
will lower the reconstruction error.

In order to ensure HP (Z) is fixed, we introduce a conditional density, p(z | x), modeled with a deep neural network that outputs the means, µ = F (x), of a spherical Gaussian with fixed  = 1. Let
us assume that the generating distribution is the same with GANs in the previous section. Let QXZ and PXZ be the generated and inferred joint distributions, respectively. The objectives for training a bi-directional adversarial model then becomes:

arg min V (D, F, G) = arg min EPXZ [log D(x, z)] + EQXZ [log (1 - D(x, z))]
DD
arg max EPXZ [log (1 - D(x, z))] + EQXZ [log D(x, z)] + IP (G(Z); Z).
F,G

(18)

We will show that a bi-directional model trained in this way has the benefits of higher mutual information, including better mode coverage and reconstructions.

4.3.1 ADAPTIVE CLIPPING

Notice that the generator is updated by two gradients. The first gradient is that of the generator's

loss, Lgen with respect to the generator's parameters gen, gunsup

:=

.Lgen
gen

The second flows

the

the

mutual

information

estimate

to

the

generator,

gM I

:=

-

 I (X ;Z  gen

)

.

If

left

unchecked,

because

mutual information is unbounded, the latter can overwhelm the former, leading to a failure mode

of the algorithm where the generator puts all of its attention on maximizing the mutual information

and ignores the adversarial game with the discriminator. We propose to adaptive clip the gradient

from the mutual information so that its Frobenius norm is at most that of the gradient from the

discriminator. More formally, we have

gadapted := min( gunsup

,

gMI )

gM I gM I

.

(19)

Please note that adaptive clipping can be considered in any situation where OMIE is to be maximized.

Experiment: bi-directional adversarial model with mutual information maximization In this section we compare OMIE to existing bi-directional adversarial models in terms of euclidean reconstructions, reconstruction accuracy and MS-SSIM metric (Wang & Simoncelli, 2004). One of the potential features of a good generative model is how close the reconstructions are to the original in pixel space. Adding OMIE to a bi-directional adversarial model gets us closer to this objective. We train OMIE on datasets of increasing order of complexity: a toy dataset composed of 25-Gaussians, MNIST, and the CelebA dataset. Figure 6 shows the reconstruction ability of OMIE compared to ALI. Although ALICE does perfect reconstruction (which is in its explicit formulation), we observe significant mode-dropping in the sample space. OMIE does a balanced job of reconstructing along with capturing all the modes of the underlying data distribution.
We use MS-SSIM (Wang & Simoncelli, 2004) scores to measure the likelihood of generated samples within the class. Table 1 compares OMIE to the existing baselines in terms of euclidean reconstruction errors, reconstruction accuracy and MS-SSIM metric. OMIE does a better job than ALI in terms of reconstruction errors by a margin and is competitive to ALICE with respect to reconstruction accuracy and MS-SSIM. Table2 shows that OMIE's effect on reconstructions is even more dramatic when compared to ALI and ALICE. Thus showing that OMIE can efficiently operate in a truly large scale setting.

4.4 INFORMATION BOTTLENECK
The Information Bottleneck (IB, Tishby et al., 2000), is an information theoretic method for extracting relevant information, or yielding a representation, that an input X  X contains about an output Y  Y. An optimal representation of X would capture the relevant factors and compress X by diminishing the irrelevant parts which do not contribute to the prediction of Y . As such, IB can

8

Under review as a conference paper at ICLR 2018

(a) ALI

(b) ALICE (L2)

(c) ALICE (A)

(d) OMIE

Figure 5: Reconstructions, samples, and embeddings from adversarially learned inference (ALI) and variations intended to increase the mutual information. Shown left to right are the baseline (ALI), ALICE with the L2 loss to minimize the reconstruction error, ALI with an additional adversarial loss, and OMIE. Top to bottom are the reconstructions, samples from the prior, and the embeddings. ALICE with the adversarial loss has the best reconstruction, though at the expense of sample quality. Overall, OMIE provides both very good reconstructions and the best mode representation in its samples.

ALI ALICE OMIE

MNIST Reconstruction Reconstruction Error(Euclidean) Accuracy(%)

14.2403 5.1652 9.7347

45.95 98.17 96.10

MS-SSIM
0.9683 0.9789 0.9997

Table 1: Comparison of OMIE with other bi-directional adversarial models in terms of euclidean reconstruction error, reconstruction accuracy, and ms-ssim on MNIST dataset. We used MLP both in the generator and discriminator identical to the setting described in Salimans et al. (2016) and MLP Statistics network for this task. OMIE does a decent job comapred to ALI in terms of reconstructions. Though the explicit reconstruction based baselines do better than OMIE in terms of rtasks related to reconstructions, they lag behind in MS-SSIM scores.

be seen as process to construct approximate minimally sufficient statistics of the data. IB seeks a feature map, or encoder, q(Z | X), that would induce the Markovian structure  X  Z  Y . This is done by minimizing the IB Lagrangian Markovian structure,  X  X^ .

L[q(Z | X)] = I(X; Z) - I(Z; Y ),

or equivalently: I(X; Y ) - I(X; Z)

(20)

Related works In the discrete setting, Tishby et al. (2000) uses the Blahut-Arimoto Algorithm Arimoto (1972), which can be understood as cyclical coordinate ascent in function spaces. While the information bottleneck is successful and popular in a discrete setting, it's application to continuous settings was stifled by the intractability of the continuous mutual information. Nonetheless, the Information Bottleneck was applied in the case of jointly Gaussian random variables in ?. In order to overcome the intractability of I(X; Z) is the continuous setting, Alemi et al. (2016); Kolchinsky et al. (2017); Chalk et al. (2016) exploit the variational bound of (Barber & Agakov, 2003) to approximate the conditional entropy part of I(X; Z). The approaches of the aforementioned works differ

9

Under review as a conference paper at ICLR 2018

ALI ALICE OMIE

CelebA Reconstruction Reconstruction Error(Euclidean) Accuracy(%)

53.752480 92.589607 36.108347

57.49 48.95 76.08

MS-SSIM
0.8147 0.5157 0.9907

Table 2: Comparison of OMIE with other bi-directional adversarial models in terms of euclidean reconstruction error, reconstruction accuracy, inception score, encoding accuracy and MS-SSIM on CelebA faces dataset. We can see that the trend remains same from MNIST results. OMIE achieves a substantial decrease in reconstruction errors without compromising much on inception and yielding better MS-SSIM score.

only on their treatment of the marginal distribution of the bottleneck variable. Alemi et al. (2016) assumes a standard multivariate normal marginal distribution while Chalk et al. (2016) a Student-t. Kolchinsky et al. (2017) uses a non-parametric estimators. Due to their reliance on a variational approximation, all the method above require a tractable density for the approximate posterior.

Information Bottleneck with OMIE OMIE estimate the mutual information directly. As such, it allows for general posterior as it does not require densities. Thus OMIE allows the use of general encoders/posteriors.

Permutation Invariant MNIST We use estimate by optimizing the Information Bottleneck objective on a permutation invariant MNIST. We use a similar setup to Alemi et al. (2016), except that we do not use Polyak averaging on the weights. The architecture of the encoder is an MLP with two hidden layers and an output of 256 dimensions. The decoder is a simple softmax. As Alemi et al. (2016) is using a variational bound on the conditional entropy, their approach requires a tractable density. They opt for a conditional Gaussian encoder z = µ(x) +  , where  N (0, I). OMIE not being bound by the requirement of a tractable density, we consider three type of encoders; A Gaussian encoder as in Alemi et al. (2016) in order to assess if OMIE estimate the MI information better than the Variational approach approximates it, an additive noise encoder, z = enc(x +  ) as well as a propagated noise encoder, z = enc([x... ]). 3 exhibits the results.

Variational Bottleneck
Variational Bottleneck OMIE (Gaussian) OMIE(Propagated) OMIE(Additive)

Misclassification rate(%)
1.37% 1.26% 1.24% 1.19%

Table 3: Permutation Invariant MNIST misclassification rate using information bottleneck methods.

5 CONCLUSION
We propose a scalable in dimension and sample-size mutual information estimator. We apply the estimator to alleviate mode dropping issue in generative models. We use our estimator to improve inference and reconstructions in Adversarially Learned Inference. Finally, we show that our estimator allows for tractable application of Information bottleneck methods in a continuous setting.
6 APPENDIX
6.1 BOUND ON THE RECONSTRUCTION ERROR Here we will make the connection between reconstruction error and mutual information clear.
10

Under review as a conference paper at ICLR 2018

Definition 6.1 (Reconstruction Error). Let x  P be samples an empirical distribution, P, with density, q(x). Next, let p(x, z) = p(x | z)p(z) and q(x, z) = q(z | x)q(x) be joint densities. The
reconstruction error is:

R = EP[Ezq(z|x)[- log (p(x | z))]]

(21)

Through elementary manipulations, we can rewrite the reconstruction error:

R = DKL(p(x, z) || q(x, z)) - DKL(p(z) || q(z)) + Hq(Z | X),

(22)

Hq(Z | X) corresponds to the conditional entropy under the encoder's joint distribution, q(x, z). Rewriting this conditional entropy as the difference between the marginal entropy and the mutual
information (using 15):

R = DKL(p(x, z) || q(x, z)) - DKL(p(z) || q(z)) - Iq(X; Z) + Hq(Z)

(23)

As the Kullback-Leibler divergence is positive, we have:

R  DKL(p(x, z) || q(x, z)) - Iq(x; z) + Hq(z)

(24)

REFERENCES
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. IEEE Transactions on Information Theory, 18(1):14­20, 1972.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. In Proceedings of the 34th International Conference on Machine Learning, pp. 214­223, 2017.
A Banerjee. On baysian bounds. ICML, pp. 81­88, 2006.
David Barber and Felix Agakov. The im algorithm: a variational approach to information maximization. In Proceedings of the 16th International Conference on Neural Information Processing Systems, pp. 201­208. MIT Press, 2003.
Philemon Brakel and Yoshua Bengio. Learning independent features with adversarial nets for nonlinear ica. arXiv preprint arXiv:1710.05050, 2017.
Atul J Butte and Isaac S Kohane. Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements. In Pac Symp Biocomput, volume 5, pp. 26, 2000.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational information bottleneck. In Advances in Neural Information Processing Systems, pp. 1957­1965, 2016.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016.
Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville. Calibrating energy-based generative adversarial networks. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.
Georges A Darbellay and Igor Vajda. Estimation of the information by an adaptive partitioning of the observation space. IEEE Transactions on Information Theory, 45(4):1315­1321, 1999.
Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.

11

Under review as a conference paper at ICLR 2018
MD Donsker and SRS Varadhan. Asymptotic evaluation of certain markov process expectations for large time, iv. Communications on Pure and Applied Mathematics, 36(2):183212, 1983.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual information. Physical review A, 33(2):1134, 1986.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Aapo Hyva¨rinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John Wiley & Sons, 2004.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. arXiv preprint arXiv:1705.02436, 2017.
Alexander Kraskov, Harald Sto¨gbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004.
Nojun Kwak and Chong-Ho Choi. Input feature selection by mutual information based on parzen window. IEEE transactions on pattern analysis and machine intelligence, 24(12):1667­1671, 2002.
Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, and Lawrence Carin. Towards understanding adversarial learning for joint distribution matching. arXiv preprint arXiv:1709.01215, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using kernel density estimators. Physical Review E, 52(3):2318, 1995.
Youssef Mroueh and Tom Sercu. Fisher gan. arXiv preprint arXiv:1705.09675, 2017.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847­5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226­1238, 2005.
Avraham Ruderman, Mark Reid, Dar´io Garc´ia-Garc´ia, and James Petterson. Tighter variational representations of f-divergences via restriction to probability measures. arXiv preprint arXiv:1206.4664, 2012.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scho¨lkopf, and Gert RG Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.
12

Under review as a conference paper at ICLR 2018 Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, and Charles Sutton. Vee-
gan: Reducing mode collapse in gans using implicit variational learning. arXiv preprint arXiv:1705.07761, 2017. Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori. Approximating mutual information by maximum likelihood density ratio estimation. In New challenges for feature selection in data mining and knowledge discovery, pp. 5­20, 2008. Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. Marc M Van Hulle. Edgeworth approximation of multivariate differential entropy. Neural computation, 17(9):1903­1910, 2005. Bovik Alan C Sheikh Hamid R Wang, Zhou and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13:600­612, 2004.
13

