Under review as a conference paper at ICLR 2018
CHEAP DNN PRUNING WITH PERFORMANCE GUARANTEES. / CONFERENCE SUBMISSIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalization Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.
1 INTRODUCTION
Recently, deep neural networks have achieved state-of-the art results in a number of machine learning tasks LeCun et al. (2015). Training such networks is computationally intensive and often requires dedicated and expensive hardware. Furthermore the resulting networks often require a considerable amount of memory to be stored. Using a Pascal Titan X GPU the popular AlexNet and VGG-16 models require 13 hours and 7 days respectively to train, while requiring 200MB and 600MB respectively to store. The large memory requirements limit the use of DNNs in embedded systems and portable devices such as smartphones, which are now obiquitous.
A number of approaches have been proposed to reduce the DNN size during training time, often with little or no degradation to classification performance. Approaches include introducing bayesian sparsity inducing priors Louizos et al. (2017) Blundell et al. (2015) Molchanov et al. (2017) and binarization Hou et al. (2016) Courbariaux et al. (2016).Other methods include the hashing trick has been used in Chen et al. (2015), tensorisation Novikov et al. (2015) and efficient matrix factorisations Yang et al. (2015).
However trained DNN models are used by researchers and developers that do not have dedicated hardware to train them, often as general feature extractors for transfer learning. In such settings it is important to introduce a cheap compression method, i.e., one that can be implemented as a postprocessing step with little or no retraining. Some first work in this direction has been Kim et al. (2015) Han et al. (2015a) Han et al. (2015b) although these still require a lengthy retraining procedure. Closer to our approach recently in Aghasi et al. (2016) the authors propose a convexified layerwise pruning algorithm termed Net-Trim. Building upon Net-Trim the authors in Dong et al. (2017) propose LOBS an algorithm for layerwise pruning by loss function approximation.
Pruning a neural network layer introduces a pertubation to the latent signal representations generated by that layer. As the pertubated signal passes through layers of non-linear projections the pertubation could become arbitrary large. In Aghasi et al. (2016) Dong et al. (2017) the authors conduct a theoretical analysis using the Lipschitz properties of DNNs showing the stability of the latent representations, over the training set, after pruning. The methods employed have connections to recent work Sokolic et al. (2017) Bartlett et al. (2017) Neyshabur et al. (2017) that have used the
1

Under review as a conference paper at ICLR 2018

Lipschitz properties to analyze the Generalization Error (GE) of DNNs, a more useful performance measure.
1.1 CONTRIBUTIONS
In this work we introduce an cheap pruning algorithm for dense layers of DNNs. We also conduct a theoretical analysis of how pruning affects the Generalization Error of the trained classifier.
· We show that the sparsity inducing objective proposed in Aghasi et al. (2016) can be cast as a difference of convex problem, that has an efficient solution. For a fully connected layer with input dimension d1 and output dimension d2 while Net-Trim and LOBS scale like O(d31N ) and O(d21N ) respectively our algorithm scales like O(d1d2N ) with d2 d1. Emprirically our algorithm is orders of magnitude faster than competing approaches. We then extend our formulation to allow retraining a layer with any convex regulariser.
· We build upon the work of Sokolic et al. (2017) to bound the GE of a DNN after pruning. Our theoretical analysis provides a principled way of pruning while managing the GE.
Experiments on common feedforward architectures show that our method is orders of magnitude faster than competing pruning methods, while allowing for a controlled degradation in GE.
2 OUR FORMULATION
2.1 DC DECOMPOSITION
We consider a classification problem, where we observe a vector x  X  RN that has a corresponding class label y  Y. The set X is called the input space, Y = {1, 2, ..., NY } is called the label space and NY denotes the number of classes. The samples space is denoted by S = X × Y and an element of S is denoted s = (x, y). We assume that samples from S are drawn according to a probability distribution P defined on S. A training set of m samples drawn from P is denoted by Sm = {si}mi=1 = {(xi, yi)}im=1. We start from the Net-Trim formulation and show that it can be cast as a difference of convex problem. For each training signal x  RN we assume also that we have access to the inputs a  Rd1 and the outputs b  Rd2 of the fully connected layer, with a rectifier non-linearity (x) = max(0, x). The optimisation problem that we want to solve is then:

1 min Um

||(U T aj) - bj||22 + (U )

sj Sm

(1)

where  is the sparsity parameter. The term ||(U T aj) - bj||22 ensures that the nonlinear projection remains the same for training signals. The term (U ) is the convex regulariser which imposes the desired structure on the weight matrix U .
2

Under review as a conference paper at ICLR 2018

The objective in Equation 1 is non-convex. We show that the optimisation of this objective can be
cast as a difference of convex (DC) problem. We assume just one training sample for simplicity x  RN with latent representations a  Rd and b  Rz.

||(U T a) - b||22 + (U )

= [(uiT a) - bi]2 + (U )

i

= [2(uiT a) - 2(uiT a)bi + bi2] + (U )

i

= [2(uiT a) + bi2] + (U ) + [-2bi(uiT a)]

ii

= [2(uiT a) + bi2] + (U ) + [-2bi(uiT a)] + [-2bi(uiT a)]

i ii

bi <0

bi 0

(2)

Notice that after the split the first term (bi < 0) is convex while the second (bi  0) is concave. We note that bi  0 by definition of the ReLu and set:

g(U ; x) = [2(uiT a) + bi2]
i
h(U ; x) = [2bi(uiT a)]
i bi >0
Then by summing over all the samples we get:

(3) (4)

f (U ) = g(U ; xj) + (U ) - h(U ; xj)
jj
= g(U ) + (U ) - h(U )

(5)

which is difference of convex. The rectifier nonlinearity is non-smooth we can alleviate that by

assuming

a

smooth

approximation.

A

common

choice

for

this

task

is

(x)

=

1 

log(1

+

exp(x))

with  a positive constant.

2.2 OPTIMISATION
It is well known that DC programs have efficient optimisation algorithms. We propose to use a variant of the DCA algorithm Tao & An (1997). DCA is an iterative algorithm that consists in solving, at each iteration, the convex optimisation problem obtained by linearizing h(·) (the nonconvex part of f = g - h) around the current solution. Although DCA is only guaranteed to reach local minima the authors of Tao & An (1997) state that DCA often converges to the global minimum, and has been used succefully to optimise a fully connected DNN layer Fawzi et al. (2015). At iteration k of DCA, the linearized optimisation problem is given by:

arg min{g(U ) + (U ) - T r(U T h(U k))}
U

(6)

where U k is the solution estimate at iteration k. The detailed procedure is then given in algorithms 1 and 2. We assume that the regulariser is convex but possibly non-smooth in which case the optimisation can be performed using proximal methods.

In more details for algorithm 2 at each iteration a minibatch A and B is drawn. The gradient for the smooth part is calculated and the algorithm takes a step in that direction with step size t. Then the proximal operator for the non-smooth regulariser (·) is applied to the result. We find that for the outer iterations K the values 15 to 50 are usually sufficient, while for the inner iterations T = 150
is usually sufficient. We name our algorithm FeTa, Fast and Efficient Trimming Algorithm.

3

Under review as a conference paper at ICLR 2018

Algorithm 1 FeTa (Fast and Efficient Trimming Algorithm)
1: Choose initial point: U 0 2: for k = 1,...,K do 3: Compute C  h(U k). 4: Solve with Algorithm 2 the convex optimisation problem:
U k+1  arg min{g(U ) + (U ) - T r(U T C)}
U
5: end for 6: If U k+1  U k return U k+1.

Algorithm 2 Solve linearized problem

1: Initialization: U  U k

2: for t = 1,...,T do

3: Choose (A, B) randomly chosen minibatch.

4:

Choose

stepsize

t



min(,



t0 t

)

5: Update U by projected subgradient step:

6: end for 7: Return U k+1  U

U  prox(·)(U - t[g(U ) - T r(U T C)])

(7) (8)

3 GENERALIZATION ERROR
3.1 GENERALIZATION ERROR OF PRUNED LAYER
Having optimized our pruned layer for the training set we want to see if it is stable for the test set. We denote f 1(·, W 1) the original representation and f 2(·, W 2) the pruned representation. We assume that after training si  Sm ||f 1(ai, W 1) - f 2(ai, W 2)||22  C1. Second, we assume that s  S si  Sm  ||a - ai||22  . Third the linear operators in W 1 , W 2 are frames with upper frame bounds B1 , B2 respectively. Theorem 3.1. For any testing point s  S the distance between the original representation f 1(a, W 1) and the pruned representation f 2(a, W 2) is bounded by ||f 1(a, W 1) - f 2(a, W 2)||22  C2 where C2 = C1 + (B1 + B2) .
the detailed proof can be found in Appendix A.
3.2 GENERALIZATION ERROR OF CLASSIFIER
In this section we use tools from the robustness framework Xu & Mannor (2012) to bound the generalization error introduced by our approximation. We consider DNN classifiers defined as

g(x) = max (f (x))i,
i[Ny ]

(9)

where (f (x))i is the i-th element of Ny dimensional output of a DNN f : RN  RNy . We assume that f (x) is composed of L layers:

f (x) = fL(fL-1(...f1(x, W 1), ...W L-1), W L),

(10)

where fl(·, W l) represents the l-th layer with parameters W l, l = 1, ..., L. The output of the l-th layer is denoted zl, i.e. zl = fl(zl-1, W l). The input layer corresponds to z0 = x and the output of the last layer is denoted by z = f (x). We then need the following two definitions of the classification margin and the score that we take from Sokolic et al. (2017). These will be useful later for measuring the generalization error.

4

Under review as a conference paper at ICLR 2018

Definition 3.1. (Classification Margin). The classification margin of a training sample si = (xi, yi) measured by a metric d is defined as

d(si) = sup{a : d(xi, x)  a  g(x) = yi x}

(11)

The classification margin of a training sample si is the radius of the largest metric ball (induced by d) in X centered at xi that is contained in the decision region associated with class label yi. We then
restate a useful result from Sokolic et al. (2017).

Corollary 3.1.1. Assume that X is a (subset of) CM regular k-dimensional manifold, where

N

(X

;

d;

)



(

CM 

)k .

Assume

also

that

DNN

classifier

g(x)

achieves

a

lower

bound

to

the

classifi-

cation margin d(si) > , si  Sm and take l(g(xi, yi)) to be the 0 - 1 loss. Then for any  > 0,

with probability at least 1 - ,

where A =

GE(g)



A

·

(

)-

k 2

+

B

(12)

log (2)·Ny ·2k+1·(CM )k m

and B

=

2 log 1/ m

are

constants

related

to

the

data

manifold.

We are now ready to state our main result.

Theorem 3.2. Assume that X is a (subset of) CM regular k-dimensional manifold, where

N

(X

;

d;

)



(

CM 

)k .

Assume

also

that

DNN

classifier

g1(x)

achieves

a

lower

bound

to

the

classi-

fication margin d(si) > , si  Sm and take l(g(xi, yi)) to be the 0 - 1 loss. Assume also a new

classifier g2(x) which we have on layer i using Algorithm 1. Then for any  > 0, with probability

at least 1 - ,

where A =

GE(g2)  A · ( -

log (2)·Ny ·2k+1·(CM )k m

and B

=

C2 ·

i>i

||Wi||F

)-

k 2

+B

i ||Wi||F

(13)

2 log 1/ m

are

constants

related

to

the

data

manifold.

The detailed proof can be found in Appendix B. We must note here that k is the intrinsic dimensionality of the data manifold which is much smaller than the ambient dimesion. Our result is quite pessimistic, and assumes that the GE grows exponentially in respect to the remaining layer depth of the pertubated layer. This is in line with previous work Raghu et al. (2016) Han et al. (2015b) that demonstrates that layers closer to the input are much less robust compared to layers close to the output. Our algorithm is applied to the fully connected layers of a DNN which are much closer to the output compared to convolutional layers.

4 EXPERIMENTS
4.1 TIME COMPLEXITY
We make experiments to compare the execution time of FeTa with that of LOBS and NetTrimADMM. We set (U ) = ||U ||1 and aim for 95% sparsity. We see that given that d1 are the input dimensions, d2 are the output dimensions and N is the number of training samples FeTa scales like O(d1d2N ). Conversely LOBS scales like O(d21N ) while NetTrim-ADMM scales like O(d13N ) due to the required Cholesky factorisation. This gives a computational advantage to our algorithm in settings where the input dimension is large. We validate this by constructing a synthetic experiment with d2 = 10 , d1 = {2000 : 100 : 3000} and N = 1000. The samples a  Rd1 and b  Rd2 are generated with i.i.d Gaussian entries. We plot in Figure 1 the results, which are in line with the theoretical predictions.
4.2 CLASSIFICATION ACCURACY
4.2.1 SPARSE REGULARISATION
In this section we perform experiments on the proposed compression scheme with feedforward neural networks. We compare the original full-precision network (without compression) with the

5

Under review as a conference paper at ICLR 2018

Figure 1: Time Complexity

following compressed networks: (i) FeTa with (U ) = ||U ||1 (ii) Net-Trim (iii) LOBS (vi) Hard Thresholding. We refer to the respective papers for Net-Trim and LOBS, Hard Thresholding is defined as F (x) = x I(|x| > t) where I is the elementwise indicator function, is the Hadamard product and t is a positive constant.
Experiments were performed on two commonly used datasets:
1. MNIST: This contains 28 × 28 gray images from ten digit classes. We use 55000 images for training, another 5000 for validation, and the remaining 10000 for testing. We use the LeNet-5 model:

(1 × 6C5) - M P 2 - (6 × 16C5) - M P 2 - 120F C - 84F C - 10SM

(14)

where C5 is a 5 × 5 ReLU convolution layer, M P 2 is a 2 × 2 max-pooling layer, F C is a fully connected layer and SM is a linear softmax layer.

2. CIFAR-10:This contains 60000 32 × 32 color images for ten object classes. We use 50000 images for training and the remaining 10000 for testing. The training data is augmented by random cropping to 24 × 24 pixels, random flips from left to right, contrast and brightness distortions to 200000 images. We use a smaller variant of the AlexNet model:

(3 × 64C5) - M P 2 - (64 × 64C5) - M P 2 - 384F C - 192F C - 10SM (15)

We prune only the first fully connected layer (the one furthest from the output) for clarity. Figure 2 shows the classification accuracy vs compression ratio for FeTa and Hard Thresholding. We see that Hard Thresholding works adequately up to 85% sparsity. From this level of sparsity and above the performance of Hard Thresholding degrades rapidly and FeTa has 10% higher accuracy on average.

Table 1: Test accuracy rates (%) for feedforward neural network models.

Method

Networks Original Accuracy CR Pruned Accuracy Computation Time

Net-Trim LOBS Thresholding FeTa

LeNet-5 LeNet-5 LeNet-5 LeNet-5

99.2% 99.2% 99.2% 99.2%

95% 95.2% 95% 93.5% 95% 83.1% 95% 91%

455s 75s
32s

Net-Trim

CifarNet

86%

-

-

LOBS

CifarNet

86%

97.4%

80.5%

Thresholding CifarNet

86%

97.4%

65.2%

FeTa

CifarNet

86%

97.4%

74.6%

2h 47min
9min

6

Under review as a conference paper at ICLR 2018

(a) LeNet-5

Figure 2: Accuracy vs Sparsity

(b) CifarNet

For the LeNet-5 model when compared to Net-Trim and LOBS we see that FeTa achieves lower classification accuracy but manages to prune the dense layer 2× to 15× faster. For the CifarNet model Net-Trim is not feasible on the machine used for the experiments as it requires over 16GB of RAM. Compared to LOBS FeTa again achieves lower accuracy but is 18× faster. Note that as mentioned in Dong et al. (2017) and Wolfe et al. (2017) retraining can recover classification accuracy that was lost during pruning. Starting from a good pruning which doesn't allow for much degradation significantly reduces retraining time.
4.2.2 LOW RANK REGULARISATION
As a proof of concept for the generality of our approach we apply our method while imposing lowrank regularisation on the learned matrix U . For low rank k we compare two methods (i) FeTa with (U ) = ||U || and (ii) Hard Thresholding of singular values using the truncated SVD defined as U = N V ,  = diag({i}1ik). We plot the results in Figure 3.

(a) LeNet-5

Figure 3: Accuracy vs Sparsity

(b) CifarNet

In the above given U  Rd1×d2 the Commpression Ratio (CR) is defined as CR = (k  d1 + k + k  d2)/(d1  d2). The results are in line with the l1 regularisation, with significant degredation in classification accuracy for Hard Thresholding above 85% CR.
4.3 GENERALIZATION ERROR
According to our theoretical analysis the GE drops exponentially as the pruning moves away from the output layer. To corroborate this we train a Lenet-5 to high accuracy, then we pick a single layer and gradually increase it's sparsity using Hard Thresholding. We find that the layers closer to the input are exponentially less robust to pruning, in line with our theoretical analysis. We plot the results in Figure 4.

7

Under review as a conference paper at ICLR 2018

Figure 4: Layer Robustness

For some layers there is a sudden increase in accuracy around 90% sparsity which could be due to the small size of the DNN. We point out that in empirical results Raghu et al. (2016) Han et al. (2015b) for much larger networks the degradation is entirely smooth.
5 CONCLUSION
In this paper we have presented an efficient pruning algorithm for fully connected layers of DNNs, based on difference of convex optimisation. Our algorithm is orders of magnitude faster than competing approaches while allowing for a controlled degradation in the Generalization Error. We provided a theoretical analysis of the degradation in GE resulting from our pruning algorithm. This analysis validates the previously observed phenomenon that network layers closer to the input are exponentially less robust to pruning compared to layers close to the output. Experiments on common feedfoward architectures validate our results.
6 APPENDIX
A. PROOF OF THEOREM 3.1. We denote f 1(·, W 1) the original representation and f 2(·, W 2) the pruned representation. We assume that after training s  Sm ||f 1(a, W 1) - f 2(a, W 2)||22  C1. Second, we assume that s  S si  Sm  ||a - ai||22  . Third the linear operators in W 1 , W 2 are frames with upper frame bounds B1 , B2 respectively. The following two lemmas will be useful: Lemma 6.1. The operator f 1(·, W 1) is Lipschitz continuous with upper Lipschitz constant B1.

Proof. See Bruna et al. (2013) for details, the derivation is not entirely trivial due to the nonsmoothness of the rectifier non-linearity.

Lemma 6.2. The operator f 2(·, W 2) is Lipschitz continuous with upper Lipschitz constant B2.

Proof.

We see that:

d dx

(x)

=

d dx

1 

log(1

+

exp(x))

=

exp(x) 1+exp(x)



1.

Therefore the smooth

approximation to the rectifier non-linarity is Lipschitz smooth with Lipschitz constant k = 1. Then

||f 2(x, W 2) - f 2(y, W 2)||22  k||W 2x - W 2y||22  ||W 2(x - y)||22  B2||x - y||22.

We drop the W i from the layer notation for clarity. Using the triangle inequality:

8

Under review as a conference paper at ICLR 2018

||f 1(a) - f 2(a)||22 = ||f 1(a) + f 1(ai) - f 1(ai) - f 2(a)||22  ||f 1(a) - f 1(ai)||22 + ||f 1(ai) - f 2(a)||22 = ||f 1(a) - f 1(ai)||22 + ||f 1(ai) + f 2(ai) - f 2(ai) - f 2(a)||22  ||f 1(a) - f 1(ai)||22 + ||f 1(ai) - f 2(ai)||22 + ||f 2(ai) - f 2(a)||22  B1||ai - a||22 + C + B2||ai - a||22 = C1 + (B1 + B2)||ai - a||22  C1 + (B1 + B2)
where we used Lemma 6.1 and Lemma 6.2 in line 5.

(16)

B. PROOF OF THEOREM 3.2.

We first introduce the following definition which will prove useful:

Definition 6.1. (Score). Take the score of a training sample si = (xi, yi)



o(si) = min
j=yi

2(yi - j )T f (xi),

(17)

where i  RNy is the Kronecker delta vector with (i)i = 1.



Throughout the proof we will use the notation o1(si) = o1(xi, yi) and vij = 2(i - j).

Assume that the classification margin d(si) of a training sample (xi, yi) is given and take

j = arg minj=yi min vyTijf (xi). We then take a point x that lies on the decision boundary be-

tween yi and j such that o2(x , yi) = 0. The subscripts 1 and 2 indicate the original and the pruned

architecture. We get

o1(xi, yi) = o1(xi, yi) - o2(x , yi) = vTyij (f 1(xi) - f 2(x ))



||v

T yi

j

||2||f 1(xi) - f 2(x

)||2

= ||fL1 (xi) - fL2 (x

)||2

 ||W i||F ||fi1 (xi) - fi2 (x )||2

i>i

 ||W i||F {||fi1 (xi) - fi1 (x )||2 + ||fi1 (x ) - fi2 (x )||2}
i>i
 ||W i||F {||fi1 (xi) - fi1 (x )||2 + C2}
i>i

(18)

 ||W i||F ||xi - x ||2 + ||W i||F C2
i i>i

 ||W i||F 2d(si) + ||W i||F C2
i i>i

where we used Theorem 3.1 in line 5. From the above we can therefore write



o1(x1, yi) - C2 i>i i ||W i||F

||W i||F

 2d(si)

we know that a lower bound for o1(x1, yi)( i ||W i||F )-1 exists o1(x1, yi)( Therefore we can finally write



-

C2

i>i ||W i||F i ||W i||F

 2d(si)

The theorem follows from direct application of Corollary 3.1.1 .

(19) i ||W i||F )-1 = .
(20)

REFERENCES
Alireza Aghasi, Nam Nguyen, and Justin Romberg. Net-trim: A layer-wise convex pruning of deep neural networks. arXiv preprint arXiv:1611.05162, 2016.

9

Under review as a conference paper at ICLR 2018
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1706.08498, 2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.
Joan Bruna, Arthur Szlam, and Yann LeCun. Signal recovery from pooling representations. arXiv preprint arXiv:1311.4025, 2013.
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In International Conference on Machine Learning, pp. 2285­2294, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.
Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layerwise optimal brain surgeon. arXiv preprint arXiv:1705.07565, 2017.
Alhussein Fawzi, Mike Davies, and Pascal Frossard. Dictionary learning for fast classification based on soft-thresholding. International Journal of Computer Vision, 114(2-3):306­321, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135­1143, 2015b.
Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. arXiv preprint arXiv:1611.01600, 2016.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530, 2015.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. arXiv preprint arXiv:1705.08665, 2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. arXiv preprint arXiv:1701.05369, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442­450, 2015.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 2017.
Pham Dinh Tao and Le Thi Hoai An. Convex analysis approach to dc programming: Theory, algorithms and applications. Acta Mathematica Vietnamica, 22(1):289­355, 1997.
Nikolas Wolfe, Aditya Sharma, Lukas Drude, and Bhiksha Raj. The incredible shrinking neural network: New perspectives on learning representations through the lens of pruning. arXiv preprint arXiv:1701.04465, 2017.
10

Under review as a conference paper at ICLR 2018 Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391­423,
2012. Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1476­1483, 2015.
11

