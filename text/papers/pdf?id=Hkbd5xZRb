Under review as a conference paper at ICLR 2018

SPHERICAL CNNS
Anonymous authors Paper under double-blind review

ABSTRACT
Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.
In this paper we introduce the building blocks for contructing spherical CNNs. We propose a definition for the spherical convolution that is both expressive and rotation-equivariant. We show that this spherical convolution satisfies a generalized convolution theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.

1 INTRODUCTION

Convolutional networks are able to detect local patterns regardless of their position in the image. Like patterns in a planar image, patterns on the sphere can move around, but in this case the "move" is a 3D rotation instead of a translation. In analogy to the planar CNN, we would like to build a network that can detect patterns regardless of how they are rotated over the sphere.

As shown in Figure 1, there is no good way to use translational

convolutions to analyze spherical signals. The most obvious ap-

proach, then, is to change the definition of convolution by replacing

filter translations by rotations. Doing so, we run into a subtle but

important difference between the plane and the sphere: whereas the

space of moves for the plane (2D translations) is itself isomorphic

to the plane, the space of moves for the sphere (3D rotations) is Figure 1: Any planar projeca different, three-dimensional manifold called SO(3)1. It follows tion of a spherical signal will

that the result of a spherical convolution (the output feature map) is result in distortions. A rotato be considered a signal on SO(3), not a signal on the sphere, S2. tion of a spherical signal can-

For this reason, we deploy SO(3) group convolutions in the higher not be emulated by translation

layers of a spherical CNN (Cohen and Welling, 2016).

of its planar projection.

The implementation of a spherical CNN involves two major challenges. Whereas a square grid of pixels has discrete translation symmetries, no perfectly symmetrical grids for the sphere exist. This means that there is no simple way to define the rotation of a spherical filter by one pixel. Instead, in order to rotate a filter we would

0A preliminary version of this paper was published as a non-archival workshop paper (to be de-anonimized after acceptance)
1To be more precise: although the symmetry group of the plane contains more than just translations, the translations form a subgroup that acts on the plane. In the case of the sphere there is no coherent way to define a composition for points on the sphere, and so the sphere cannot act on itself (it is not a group). For this reason, we must consider the whole of SO(3).

1

Under review as a conference paper at ICLR 2018
need to perform some kind of interpolation. The other challenge is computational efficiency; SO(3) is a three-dimensional manifold, so a naive implementation of SO(3) convolution is O(n6).
We address both of these problems using techniques from non-commutative harmonic analysis (Chirikjian and Kyatkin, 2001; Folland, 1995). This field presents us with a far-reaching generalization of the Fourier transform, which is applicable to signals on the sphere as well as the rotation group. It is known that the SO(3) convolution satisfies a convolution theorem with respect to the SO(3) Fourier transform, and we show that the same is true for our definition of S2 convolution. Hence, the S2 and SO(3) convolutions can be implemented efficiently using generalized FFT algorithms.
Because our work represents the first instance of a group equivariant network (G-CNN) for a continuous group, we rigorously evaluate the degree to which the mathematical properties predicted by the continuous theory hold in practice for our discretized implementation.
Furthermore, we demonstrate the utility of spherical CNNs for rotation invariant classification and regression problems by experiments on three datasets. First, we evaluate the stability of our convolution under random rotations on a variant of MNIST projected on the sphere. Second, we use the CNN for classifying 3D shapes. In a third experiment we use the model for molecular energy regression.
CONTRIBUTIONS
The main contributions of this work are the following:
1. The theory of spherical convolutional networks. We prove equivariance and a new spectral convolution theorem. This is the first group equivariant network for a continuous group.
2. The first automatically differentiable implementation of the generalized Fourier transform for S2 and SO(3). Our PyTorch implementation is easy to use, fast, and memory efficient.
3. The first empirical support for the utility of spherical CNNs for rotation-invariant learning problems.
2 RELATED WORK
It is well understood that the power of convolutional networks stems in large part from their ability to exploit (translational) symmetries at every layer through a stack of equivariant layers. It thus becomes natural to consider generalizations that exploit larger groups of symmetries, and indeed this has been the subject of several recent papers by Gens and Domingos (2014); Olah (2014); Dieleman et al. (2015; 2016); Cohen and Welling (2016); Ravanbakhsh et al. (2016); Guttenberg et al. (2016); Cohen and Welling (2017).
To efficiently perform convolutions on the sphere and rotation group, we use generalized FFT algorithms. Generalized Fourier Analysis, sometimes called abstract- or noncommutative harmonic analysis, has a long history in mathematics and many books have been written on the subject (Sugiura, 1990; Taylor, 1986; Folland, 1995). For a good engineering-oriented treatment which covers generalized FFT algorithms, see (Chirikjian and Kyatkin, 2001). Other important works include (Driscoll and Healy, 1994; Healy et al., 2003; Potts et al., 1998; Kunis and Potts, 2003; Drake et al., 2008; Maslen, 1998; Rockmore, 2004; Kostelec and Rockmore, 2007; 2008; Potts et al., 2009).
3 CONVOLUTION ON THE SPHERE AND ROTATION GROUP
We will explain the S2 and SO(3) convolution by analogy to the classical planar Z2 convolution. The planar convolution can be understood as follows:
The value of the output feature map at translation x  Z2 is computed as an inner product between the input feature map and a filter, shifted by x.
Similarly, the spherical convolution can be understood as follows:
The value of the output feature map evaluated at rotation R  SO(3) is computed as an inner product between the input feature map and a filter, rotated by R.
2

Under review as a conference paper at ICLR 2018

Because the output feature map is indexed by a rotation, it is modelled as a function on SO(3). We will discuss this issue in more detail shortly.

The above definition refers to various concepts that we have not yet defined mathematically. In what follows, we will go through the required concepts one by one and provide a precise definition. Our goal for this section is only to present a mathematical model of spherical CNNs. Generalized Fourier theory and implementation details will be treated later.
The Unit Sphere S2 can be defined as the set of points x  R3 with norm 1. It is a two-dimensional manifold, which can be parameterized by spherical coordinates   [0, 2] and   [0, ].
Spherical Signals We model spherical images and filters as continuous functions f : S2  RK, where K is the number of channels.

Rotations The set of rotations in three dimensions is called SO(3), the "special orthogonal group". Rotations can be represented by 3 × 3 matrices that preserve distance (i.e. ||Rx|| = ||x||) and orientation (det(R) = +1). If we represent points on the sphere as 3D unit vectors x, we can perform a rotation using the matrix-vector product Rx. The rotation group SO(3) is a three-dimensional manifold, and can be parameterized by ZYZ-Euler angles   [0, 2],   [0, ], and   [0, 2].

Rotation of Spherical Signals In order to define the spherical convolution, we need to know not only how to rotate points x  S2 but also how to rotate filters (i.e. functions) on the sphere. To this
end, we introduce the rotation operator LR that takes a function f and produces a rotated function LRf by composing f with the rotation R-1:

[LRf ](x) = f (R-1x).

(1)

Due to the inverse on R, we have LRR = LRLR . Inner products The inner product on the vector space of spherical signals is defined as:

K

f,  =

fk (x)k (x)dx,

S2 k=1

(2)

The integration measure dx denotes the standard rotation invariant integration measure on the sphere, which can be expressed as d sin()d/4 in spherical coordinates (see Appendix A). The invariance of the measure ensures that S2 f (Rx)dx = S2 f (x)dx, for any rotation R  SO(3). That is, the volume under a spherical heightmap does not change when rotated. Using this fact, we can show that LR-1 is adjoint to LR, which implies that LR is unitary:

K

LRf,  =

fk (R-1 x)k (x)dx

S2 k=1

K

= fk(x)k(Rx)dx
S2 k=1

= f, LR-1  .

(3)

Spherical Convolution With these ingredients in place, we are now ready to state mathematically what was stated in words before. For spherical signals f and , we define convolution as:

K

[f  ](R) = f, LR =

fk (x)k (R-1 x)dx

S2 k=1

(4)

As mentioned before, the output of the spherical convolution is a function on SO(3). This is perhaps somewhat counterintuitive, and indeed the conventional definition of spherical convolution gives as output a function on the sphere. However, as shown in Appendix B, the conventional definition effectively restricts the filter to be circularly symmetric about the Z axis, which would greatly limit the expressive capacity of the network.

Rotation of SO(3) Signals We defined the rotation operator LR for spherical signals (eq. 1), and used it to define spherical convolution (eq. 4). To define the SO(3) convolution, we need to generalize

3

Under review as a conference paper at ICLR 2018

the rotation operator so that it can act on signals defined on SO(3). As we will show, naively reusing eq. 1 is the way to go. That is, for f : SO(3)  RK, and R, Q  SO(3):

[LRf ](Q) = f (R-1Q)

(5)

Note that while the argument R-1x in eq. 1 refers to the action of a rotation R-1  SO(3) on a point x  S2, the argument R-1Q in eq. 5 refers to the action of SO(3) on itself, i.e. the composition of two rotations.

Rotation Group Convolution Using the same analogy as before, we can define the convolution of two signals on the rotation group, f,  : SO(3)  RK, as follows:

K

[f  ](R) = f, LR =

fk (Q)k (R-1 Q)dQ

SO(3) k=1

(6)

The integration measure dQ is the invariant measure on SO(3), which may be expressed in ZYZ-Euler angles as d sin()dd/(8)2 (see Appendix A).

Equivariance As we have seen, convolution is defined in terms the rotation operator LR. This operator acts naturally on the input space of the network, but what justification do we have for using
it in the second layer and beyond?

The justification is provided by an important property, shared by all kinds of convolution, called equivariance. A function (layer)  is equivariant if   LR = TR  , for some operator TR. Using the definition of convolution and the unitarity of LR, showing equivariance is a one liner:
[[LQf ]  ](R) = LQf, LR = f, LQ-1R = [f  ](Q-1R) = [LQ[f  ]](R). (7)

The derivation is valid for spherical convolution as well as rotation group convolution.

4 FAST SPHERICAL CONVOLUTION WITH G-FFT

It is well known that convolutions can be computed efficiently using the Fast Fourier Transform (FFT). This is a result of the Fourier theorem, which states that f   = f^ · ^ Since the FFT can be computed in O(n log n) time and the element-wise product · has linear complexity, implementing the convolution using FFTs is asymptotically faster than the naive O(n2) spatial implementation.
For functions on the sphere and rotation group, there is an analogous transform, which we will refer to as the generalized Fourier transform (GFT) and a corresponding fast algorithm (GFFT). This transform finds it roots in the representation theory of groups, but due to space contraints we will not go into details here and instead refer the interested reader to Sugiura (1990) and Folland (1995).
Conceptually, the GFT is nothing more than the linear projection of a function onto a set of orthogonal basis functions called "matrix element of irreducible unitary representations". For the circle (S1) or line (R), these are the familiar complex exponentials exp(in). For SO(3), we have the Wigner Dfunctions Dml n(R) indexed by l  0 and -l  m, n  l. For S2, these are the spherical harmonics2 Yml (x) indexed by l  0 and -l  m  l.
Denoting the manifold (S2 or SO(3)) by X and the corresponding basis functions by U l (which is either vector-valued (Y l) or matrix-valued (Dl)), we can write the GFT of a function f : X  R as

f^l = f (x)U l(x)dx
X
This integral can be computed efficiently using a GFFT algorithm (see Section 4.1).
The inverse SO(3) Fourier transform is defined as:

(8)

b ll

f (R) = (2l + 1)

f^ml nUml n(R),

l=0 m=-l n=-l

(9)

2Technically, S2 is not a group and therefore does not have irreducible representations, but it is a quotient of groups SO(3)/ SO(2) and we have the relation Yml = Dml 0|S2

4

Under review as a conference paper at ICLR 2018

and similarly for S2. The maximum frequency b is known as the bandwidth, and is related to the resolution of the spatial grid (Kostelec and Rockmore, 2007).

Using the well-known (in fact, defining) property of the Wigner D-functions that Dl(R)Dl(R ) = Dl(RR ) and Dl(R-1) = Dl(R), it can be shown (see Appendix D) that the SO(3) Fourier
transform satisfies a convolution theorem3: f   = f^ · ^, where · denotes matrix multiplication of the two block matrices f^ and ^.

Similarly, using Y (Rx) = D(R)Y (x) and Yml = Dml 0|S2 , one can derive an analogous S2 convolu-

tion

theorem:

f



l


=

f^l

·

^l,

where

f^l

and

^l

are

now

vectors.

This

says

that

the

SO(3)-FT

of

the S2 convolution (as we have defined it) of two spherical signals can be computed by taking the

outer product of the S2-FTs of the signals. This is shown in figure 2.

We were unable to find a reference for the latter version of the S2 Fourier theorem, but its derivation is analogous to that of the well-known SO(3) Fourier theorem. The simplicity of the result shows that our definition of spherical convolution is natural.

S² FFT S² DFT

SO(3) IFFT

Figure 2: Spherical convolution in the spectrum. The signal f and the locally-supported filter  are Fourier transformed, block-wise tensored, summed over input channels, and finally inverse transformed. Note that because the filter is locally supported, it is faster to use a matrix multiplication (DFT) than an FFT algorithm for it. We parameterize the sphere using spherical coordinates , , and SO(3) with ZYZ-Euler angles , , .
4.1 IMPLEMENTATION OF G-FFT AND SPECTRAL G-CONV
Here we sketch the implementaton of GFFTs. For details, see Kostelec and Rockmore (2007).
The input of the SO(3) FFT is a spatial signal f on SO(3), sampled on a discrete grid and stored as a 3D array. The axes correspond to the ZYZ-Euler angles , , . The first step of the SO(3)-FFT is to perform a standard 2D translational FFT over the  and  axes. The FFT'ed axes correspond to the m, n axes of the result. The second and last step is a linear contraction of the  axis of the FFT'ed array with a precomputed array of samples from the Wigner-d functions dml n(). Because the shape of dl depends on l (it is (2l + 1) × (2l + 1)), this linear contraction is implemented as a custom GPU kernel. The output is a set of Fourier coefficients f^ml n for l  n, m  -l and l = 0, . . . , Lmax. The algorithm for the S2-FFTs is very similar, only in this case we FFT over the  axis only, and do a linear contraction with precomputed Legendre functions over the  axis.
The gradient for the FFT and linear projection are both easy to implement, so performing backprop through the G-FFT is feasible. Our code will be released after publication.
5 EXPERIMENTS
In a first sequence of experiments, we evaluate the numerical stability and accuracy of our algorithm. In a second sequence of experiments, we showcase that the new convolution layers we have introduced are indeed useful building blocks for several real problems involving spherical signals. Our examples for this are recognition of 3D shapes and predicting the atomization energy of molecules.
3This result is valid for real functions. For complex functions, conjugate  on the left hand side.
5

Under review as a conference paper at ICLR 2018

5.1 EQUIVARIANCE ERROR

In this paper we have presented the first instance

of a group equivariant CNN for a continuous group. In the discrete case, one can prove that

10-6 no act. & 1 layer no act. & res. of 30

the network is exactly equivariant, but although

2

we can prove [LRf ]   = LR[f  ] for continuous functions f and  on the sphere or rotation

1.5



group, this is not exactly true for the discretized

1

version that we actually compute. Hence, it is reasonable to ask if there are any significant

0.5

discretization artifacts and whether they affect

0

the equivariance properties of the network. If equivariance can not be maintained for many

ReLU & 1 layer 0.5

ReLU & res. of 30

layers, one may expect the convolutional weight sharing scheme to become much less effective.
We first tested the equivariance of a single SO(3) convolution at various resolutions

0.4 0.3 0.2

b. We do this by first sampling n = 500 0.1

random rotations Ri as well as n feature

maps fi with K = 10 channels. Then

we compute 

=

1 n

n i=1

std(LRi (fi)

-

(LRi fi))/ std((fi)), where  is a composi-

tion of SO(3) convolution layers with randomly

initialized filters. In case of perfect equivariance,

we expect this quantity to be zero. The results

0 0 10 20 30 1
resolution

5 10 layers

Figure 3:  as a function of the resolution and the number of layers.

(figure 3 (top)), show that although the approxi-

mation error  grows with the resolution and the number of layers, it stays manageable for the range

of resolutions of interest.

We repeat the experiment with ReLU activation function after each convolution operation. As shown in figure 3 (bottom), the error is higher but stays flat. This indicates that the error is not due to the network layers, but due to the feature map rotation, which is exact only for bandlimited functions.

5.2 ROTATED MNIST ON THE SPHERE
In this experiment we evaluate the generalization performance with respect to rotations of the input. For testing we propose a version MNIST dataset projected on the sphere (see fig. 4). The dataset is well understood for planar CNNs. Thus it seems well suited to compare planar CNNs with our S2CNN. We created two instances of this dataset: one in which each digit is projected on the northern hemisphere and one in which each projected digit is additionally randomly rotated.

Architecture and Hyperparameters As a

baseline model, we use a simple CNN with lay-

ers conv-ReLU-conv-ReLU-FC-softmax, with

filters of size 5 × 5, k = 57, 114, 10 chan-

nels, and stride 3 in both layers. We compare

to a spherical CNN with layers S2conv-ReLU-

SO(3)conv-ReLU-FC-softmax, bandwidth b = Figure 4: Two SO(3)-rotated MNIST digits

30, 10, 5 and k = 100, 200, 10 channels. Both (4/9) on S2 using stereographic projection onto

models have about 165K parameters.

a Driscoll-Healy grid. Mapping back to the plane

results into non-linear distortions.

Results We trained each model on the nonrotated (NR) and the rotated (R) training set and evaluated it on the non-rotated and rotated test set. See table 1. While the planar CNN achieves high accuracy in the NR / NR regime, its performance in the R / R regime is much worse, while the spherical CNN is unaffected. When trained on the non-rotated dataset and evaluated on the rotated dataset (NR / R), the planar CNN does no better than random chance. The spherical CNN shows a slight decrease in performance compared to R/R, but still performs quite well.

6

Under review as a conference paper at ICLR 2018

NR / NR R / R NR / R

planar

0.99 0.45 0.09

spherical 0.91 0.91 0.85

Table 1: Test accuracy for the networks evaluated on the spherical MNIST dataset. Here R = rotated, NR = non-rotated and X / Y denotes, that the network was trained on X and evaluated on Y.

distance sphere-impact

ray casting from the sphere to the origin

normal at impact
Figure 5: The ray line is casted from the surface of the sphere in direction of its center. The first intersection with the model gives the values of the sigal on the sphere. The two images of the right represent two spherical signals in (, ) representation. They contain respectively the distance from the sphere and the cosinus of the ray with the normal of the model. The red dot correcpond to the pixel set by the red line.

5.3 RECOGNITION OF 3D SHAPES
Next, we applied S2CNN to 3D shape classification. The SHREC17 task (Yi et al., 2017) contains 51300 3D models taken from the ShapeNet dataset (Chang et al., 2015) which have to be classifed into 55 common categories (tables, airplanes, persons, etc.). There is a consistently aligned regular dataset and a version in which all models are randomly perturbed by rotations. We concentrate on the latter to test the quality of our rotation equivariant representations learned by S2CNN.

Representation We project the 3D meshes onto an enclosing sphere using a straightforward raycasting scheme (see Fig. 5). For each point on the sphere we send a ray towards the origin and collect 3 types of information from the intersection: ray length and cos / sin of the surface angle. We further augment this information with raycasting information for the convex hull of the model, which in total gives us 6 channels for the signal. This signal is discretized using a Driscoll-Healy grid (Driscoll and Healy, 1994) with bandwidth b = 128. Ignoring non-convexity of surfaces we assume this projection captures enough information of the shape to be useful for the recognition task.

Architecture and Hyperparameters Our network consists of an initial S2conv-BN-ReLU block

followed by two SO(3)conv-BN-ReLU blocks. The resulting filters are pooled using a max pooling

layer followed by a last batch normalization and then fed into a linear layer for the final classification.

It is important to note that the the max pooling happens over the group SO(3): if fk is the k-th filter in

the final layer (a function on SO(3)) the result of the pooling is maxxSO(3) fk(x). We used 50, 70, and 350 features for the S2 and the two SO(3) layers, respectively. Further, in each layer we reduce

the resolution b, from 128, 32, 22 to 7 in the final layer. Each filter kernel  on SO(3) has non-local

support,

where

(, , )

=

0

iff



=

 2

and



=

0

and

the

number

of

points

of

the

discretization

is

proportional to the bandwidth in each layer. The final network contains  1.4M parameters.

Results We evaluated our trained model using the official metrics and compared to the top three competitors in each category (see table 2 for results). Except for precision and F1@N, in which our model ranks third, it is the runner up on each other metric. The main competitors, Tatsuma_ReVGG and Furuya_DLAN use input representations and network architectures that are highly specialized

7

Under review as a conference paper at ICLR 2018

Method Tatsuma_ReVGG Furuya_DLAN SHREC16-Bai_GIFT Deng_CM-VGG5-6DB Ours

P@N 0.705 0.814 0.678 0.412 0.701 (3rd)

R@N 0.769 0.683 0.667 0.706 0.711 (2nd)

F1@N 0.719 0.706 0.661 0.472 0.699 (3rd)

mAP 0.696 0.656 0.607 0.524 0.676 (2nd)

NDCG 0.783 0.754 0.735 0.624 0.756 (2nd)

Table 2: Experiment results for the SHREC17 task.

to the SHREC17 task. Given the rather task agnostic architecture of our model and the lossy input representation we use, we interpret our models performance as strong empirical support for the effectiveness of Spherical CNNs.

5.4 PREDICTION OF ATOMIZATION ENERGIES FROM MOLECULAR GEOMETRY
Finally, we apply S2CNN on molecular energy regression. In the QM7 task (Blum and Reymond, 2009; Rupp et al., 2012) the atomization energy of molecules has to be predicted from geometry and charges. Molecules contain up to N = 23 atoms of T = 5 types (H, C, N, O, S). They are given as a list of positions pi and charges zi for each atom i.
Representation by Coulomb matrices Rupp et al. (2012) propose a rotation and translation invariant representation of molecules by defining the coulomb matrix C  RN×N (CM). For each pair of atoms i = j they set Cij = (zizj)/(|pi - pj|) and Cii = 0.5zi2.4. Diagonal elements encode the atomic energy by nuclear charge, while other elements encode Coulomb repulsion between atoms. This representation is not permutation invariant. To this end Rupp et al. (2012) propose a distance measure between coulomb matrices used within Gaussian kernels whereas Montavon et al. (2012) propose sorting C or random sampling index permutations.

Representation as a spherical signal We utilize spherical symmetries in the geometry by defining

a sphere Si around around pi for each atom i. The radius is kept uniform across atoms and molecules

and chosen minimal such that no intersections among spheres in the training set happen. Generalizing

the coulomb matrix approach we define for each possible z and for each point x on Si potential

functions Uz(x) =

j=i,zj =z

zi ·z |x-pi |

producing

a

T

channel

spherical

signal

for

each

atom

in

the

molecule (see figure 6). This representation is invariant with respect to translations and equivariant

with respect to rotations. However, it is still not permutation invariant. The signal is discretized using

a Driscoll-Healy (Driscoll and Healy, 1994) grid with bandwidth b = 10 representing the molecule

as a sparse N × T × 2b × 2b tensor.

Architecture and Hyperparameters We use a deep ResNet style S2CNN. Each ResNet block is made of S2/SO(3)conv-BN-ReLU-SO(3)conv-BN after which the input is added to the result. We

Figure 6: The five potential channels Uz with z  {1, 6, 7, 8, 16} for a molecule containing atoms H (red), C (green), N (orange), O (brown), S (gray).
8

Under review as a conference paper at ICLR 2018

Method MLP / random CM LGIKA(RF) RBF kernels / random CM RBF kernels / sorted CM MLP / sorted CM Ours

Author (a) (b) (a) (a) (a)

RMSE 5.96 10.82 11.40 12.59 16.06 8.47

S2CNN DeepSet

Layer Input ResBlock ResBlock ResBlock ResBlock ResBlock Layer  (MLP)  (MLP)

Bandwidth
10 8 6 4 2 Input/Hidden 160/150 100/50

Features 5 20 40 60 80 160

Table 3: Left: Experiment results for the QM7 task: (a) Montavon et al. (2012) (b) Raj et al. (2016). Right: ResNet architecture for the molecule task.

share weights among atoms making filters permutation invariant, by pushing the atom dimension into the batch dimesion. In each layer we downsample the bandwidth, while increasing the number of features F . After integrating the signal over SO(3) each molecule becomes a N × F tensor. For permutation invariance over atoms we follow Zaheer et al. (2017) and embed each resulting feature vector of an atom into a latent space using a MLP . Then we sum these latent representations over the atom dimension and get our final regression value for the molecule by mapping with another MLP . Both  amd  are jointly optimized. Training a simple MLP only on the 5 frequencies of atom types in a molecule already gives a RMSE of  19. Thus, we train the S2CNN on the residual only, which improved convergence speed and stability over direct training. The final architecture is sketched in table 3.
Results We evaluate by RMSE and compare our results to Montavon et al. (2012) and Raj et al. (2016) (see table 3). Our learned representation outperforms all kernel-based approaches and a MLP trained on sorted coulomb matrices. Superior performance could only be achieved for an MLP trained on randomly permuted coulomb matrices. However, sufficient sampling of random permutations grows exponentially with N , so this method is unlikely to scale to large molecules.
6 DISCUSSION & CONCLUSION
In this paper we have presented the theory of Spherical CNNs and evaluated them on two important learning problems. We have defined S2 and SO(3)-convolution, analyzed their properties, and implemented a Generalized FFT-based convolution algorithm. Our numerical results confirm the stability and accuracy of this algorithm, even for deep networks. Furthermore, we have shown that Spherical CNNs can effectively generalize across rotations, and achieve near state-of-the-art results on competitive 3D Model Recognition and Molecular Energy Regression challenges, without excessive feature engineering and task-tuning.
For intrinsically volumetric tasks like 3D model recognition, we believe that further improvements can be attained by generalizing further beyond SO(3) to the roto-translation group SE(3). The development of Spherical CNNs is an important first step in this direction. Another interesting generalization is the development of a Steerable CNN for the sphere (Cohen and Welling, 2017), which would make it possible to analyze vector fields such as global wind directions, as well as other sections of vector bundles over the sphere.
Perhaps the most exciting future application of the Spherical CNN is in omnidirectional vision. Although very little omnidirectional image data is currently available in public repositories, the increasing prevalence of omnidirectional sensors in drones, robots, and autonomous cars makes this a very compelling application of our work.
REFERENCES
L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.
9

Under review as a conference paper at ICLR 2018
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.
G.S. Chirikjian and A.B. Kyatkin. Engineering Applications of Noncommutative Harmonic Analysis. CRC Press, 1 edition, may 2001. ISBN 9781420041767.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of The 33rd International Conference on Machine Learning (ICML), volume 48, pages 2990­2999, 2016.
Taco S Cohen and Max Welling. Steerable CNNs. In ICLR, 2017.
S. Dieleman, K. W. Willett, and J. Dambre. Rotation-invariant convolutional neural networks for galaxy morphology prediction. Monthly Notices of the Royal Astronomical Society, 450(2), 2015.
S. Dieleman, J. De Fauw, and K. Kavukcuoglu. Exploiting Cyclic Symmetry in Convolutional Neural Networks. In International Conference on Machine Learning (ICML), 2016.
John B Drake, Pat Worley, and Eduardo D Azevedo. Spherical Harmonic Transform Algorithms. X: 111­131, 2008.
JR Driscoll and DM Healy. Computing Fourier transforms and convolutions on the 2-sphere. Advances in applied mathematics, 1994.
G. B. Folland. A Course in Abstract Harmonic Analysis. CRC Press, 1995.
R. Gens and P. Domingos. Deep Symmetry Networks. In Advances in Neural Information Processing Systems (NIPS), 2014.
Nicholas Guttenberg, Nathaniel Virgo, Olaf Witkowski, Hidetoshi Aoki, and Ryota Kanai. Permutation-equivariant neural networks applied to dynamics prediction. 2016.
D. Healy, D. Rockmore, P. Kostelec, and S Moore. FFTs for the 2-Sphere ­ Improvements and Variations. The journal of Fourier analysis and applications, 9(4):340­385, 2003.
Peter J Kostelec and Daniel N. Rockmore. SOFT: SO(3) Fourier Transforms. (3):1­21, 2007.
Peter J. Kostelec and Daniel N. Rockmore. FFTs on the rotation group. Journal of Fourier Analysis and Applications, 14(2):145­179, 2008.
Stefan Kunis and Daniel Potts. Fast spherical Fourier algorithms. Journal of Computational and Applied Mathematics, 161:75­98, 2003.
David K. Maslen. Efficient Computation of Fourier Transforms on Compact Groups. 4(1), 1998.
Grégoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe, Alexandre Tkatchenko, O. Anatole von Lilienfeld, and Klaus-Robert Müller. Learning invariant representations of molecules for atomization energy prediction. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 449­457. 2012.
L. Nachbin. The Haar Integral. 1965.
Chris Olah. Groups and Group Convolutions, 2014. URL https://colah.github.io/ posts/2014-12-Groups-Convolution/.
Daniel Potts, Gabriele Steidl, and Manfred Tasche. Fast and stable algorithms for discrete spherical Fourier transforms. Linear Algebra and its Applications, 275:433­450, 1998.
Daniel Potts, J Prestin, and A Vollrath. A fast algorithm for nonequispaced Fourier transforms on the rotation group. Numerical Algorithms, pages 1­28, 2009.
Anant Raj, Abhishek Kumar, Youssef Mroueh, P Thomas Fletcher, et al. Local group invariant representations via orbit embeddings. arXiv preprint arXiv:1612.01988, 2016.
10

Under review as a conference paper at ICLR 2018

Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Deep Learning with Sets and Point Clouds. pages 1­12, 2016.
Daniel N Rockmore. Recent Progress and Applications in Group FFTS. NATO Science Series II: Mathematics, Physics and Chemistry, 136:227­254, 2004.
M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. von Lilienfeld. Fast and accurate modeling of molecular atomization energies with machine learning. Physical Review Letters, 108:058301, 2012.
Mitsuo Sugiura. Unitary Representations and Harmonic Analysis. John Wiley & Sons, New York, London, Sydney, Toronto, 2nd edition, 1990.
Michael E Taylor. Noncommutative Harmonic Analysis. American Mathematical Society, 1986. ISBN 0821815237.
Li Yi, Hao Su, Lin Shao, Manolis Savva, Haibin Huang, Yang Zhou, Benjamin Graham, Martin Engelcke, Roman Klokov, Victor Lempitsky, et al. Large-scale 3d shape reconstruction and segmentation from shapenet core55. arXiv preprint arXiv:1710.06104, 2017.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. arXiv preprint arXiv:1703.06114, 2017.

APPENDIX A: PARAMETERIZATION OF AND INTEGRATION ON S2 AND SO(3)

We use the ZYZ Euler parameterization for SO(3). An element R  SO(3) is written as

R = R(, , ) = Z()Y ()Z(),

(10)

where   [0, 2],   [0, ] and   [0, 2], and Z resp. Y are rotations around the Z and Y axes.

Using this parameterization, the normalized Haar measure is

d d sin() d dR =
2 2 2

(11)

We have SO(3) dR = 1. The Haar measure (Nachbin, 1965; Chirikjian and Kyatkin, 2001)

is sometimes called the invariant measure because it has the property that SO(3) f (R R)dR =

SO(3) f (R)dR (this is analogous to the more familiar property

f (x + y)dx =
R

f (x)dx for
R

functions on the line). This invariance property allows us to do many useful substitutions.

We have a related parameterization for the sphere. An element x  S2 is written

x(, ) = Z()Y ()n

(12)

where n is the north pole.
This parameterization makes explicit the fact that the sphere is a quotient S2 = SO(3)/ SO(2), where H = SO(2) is the subgroup of rotations around the Z axis. Elements of this subgroup H leave the north pole invariant, and have the form Z(). The point x(, )  S2 is associated with the coset representative x¯ = R(, , 0)  SO(3). This element represents the coset x¯H = {R(, , )|  [0, 2]}.

The normalized Haar measure for the sphere is

d d sin  dx =
2 2

(13)

The normalized Haar measure for SO(2) is d
dh = 2
So we have dR = dx dh, again reflecting the quotient structure.

(14)

11

Under review as a conference paper at ICLR 2018

We can think of a function on S2 as a -invariant function on SO(3). Given a function f : S2  C we associate the function f¯(, , ) = f (, ). When using normalized Haar measures, we have:

SO(3)

f¯(R)dR

=

1 82

2
d
0


sin d
0

2
df¯(, , )
0

1 2



2

= 82 0

d sin df (, )
00

d

1 2



= d sin df (, )

4 0

0

(15)

= f (x)dx
S2
This will allow us to define the Fourier transform on S2 from the Fourier transform on SO(3), by viewing a function on S2 as a -invariant function on SO(3) and taking its SO(3)-Fourier transform.

APPENDIX B: CONVOLUTION & EQUIVARIANCE

We have defined the S2 convolution as

K

[f  ](R) = f, LR =

fk (x)k (R-1 x)dx.

S2 k=1

Without loss of generality, we will analyze here the single-channel case K = 1.

This convolution is equivariant:

(16)

[[LR f ]  ](R) = f (R -1x)(R-1x)dx
S2
= f (x)(R-1R x)dx
S2
= f (x)(R -1R)-1x)dx
S2
= [f  ](R -1R) = [LR [f  ]](R)
A similar derivation can be made for the SO(3) convolution.
The spherical convolution defined by Driscoll and Healy (1994) is:

(17)

[f  ](x) =

f (Rn)(R-1x)dR

SO(3)

(18)

where n is the north pole. Note that in this definition, the output of the spherical convolution is a function on the sphere, not a function on SO(3) as in our definition. Note further that unlike our definition, this definition involves an integral over SO(3).

If we write out the integral in terms of Euler angles, noting that the north-pole n is invariant to Z-axis rotations by , i.e. R(, , )n = Z()Y ()Z()n = Z()Y ()n, we see that this definition implicitly integrates over  in only one of the factors (namely ), making it invariant wrt  rotation. In other words, the filter is first "averaged" (making it circularly symmetric) before it is combined with f . We consider to be much too limited for the purpose of pattern matching in spherical CNNs.

APPENDIX C: GENERALIZED FOURIER TRANSFORM
With each compact topological group (like SO(3)) is associated a discrete set of orthogonal functions that arise as matrix elements of irreducible unitary representations of these groups. For the circle (the group SO(2)) these are the complex exponentials (in the complex case) or sinusoids (for real functions). For SO(3), these functions are known as the Wigner D-functions.

12

Under review as a conference paper at ICLR 2018

As discussed in the paper, the Wigner D-functions are parameterized by a degree parameter l  0 and order parameters m, n  [-l, . . . , l]. In other words, we have a set of matrix-valued functions Dl : SO(3)  C(2l+1)×(2l+1).
The Wigner D-functions are orthogonal:

Dml n, Dml n

=

2 d 0 2

 d sin  02

2 0

d 2

Dml n

(,

,

)Dml

n

(, , )

=

ll

mm 2l +

nn 1

(19)

Furthermore, they are complete, meaning that any well behaved function f : SO(3)  C can be written as a linear combination of Wigner D-functions. This is the idea of the Generalized Fourier Transform F on SO(3):

 ll

f (R) = [F -1f^](R) = (2l + 1)

f^ml nDml n(R)

l=0 m=-l n=-l

(20)

where f^ml n are called the Fourier coefficients of f . Using the orthogonality property of the Wigner D-functions, one can see that the Fourier coefficients can be retreived by computing the inner product
with the Wigner D-functions:

[F f ]ml n = =

f (R)Dml n(R)dR
SO(3)




ll



 (2l + 1)

f^ml n Dml n (R) Dml n(R)dR

SO(3) l =0

m =-l n =-l

l
= (2l + 1)

l
f^ml n

l =0

m =-l n =-l

= f^ml n

Dml n (R)Dml ndR
SO(3)

(21)

APPENDIX D: CONVOLUTION THEOREMS

To derive the convolution theorems, we will use the defining property of the Wigner D-matrices: that they are (irreducible, unitary) representations of SO(3). This means that they satisfy:

Dl(R)Dl(R ) = Dl(RR ),

(22)

for any R, R  SO(3). Notice that the complex exponentials satisfy an analogous criterion for the circle group S1 = SO(2). That is, einxeiny = ein(x+y), where x + y is the group operation for SO(2). Unitarity means that Dl(R)Dl(R) = I. Irreducibility means, essentially, that the set of matrices {Dl(R) | R  SO(3)} cannot be simultaneously block-diagonalized.
To derive the Fourier theorem for SO(3), we use the invariance of the integration measure dR: SO(3) f (R R)dR = SO(3) f (R)dR.
13

Under review as a conference paper at ICLR 2018

With these facts understood, we can proceed to derive:

l
f  =

(f  )(R)Dl(R)dR

SO(3)

= f (R )(R-1R )dR Dl(R)dR
SO(3) SO(3)

= f (R )(R-1)Dl(R R)dR dR
SO(3) SO(3)

= f (R )Dl(R )dR
SO(3)
= f (R )Dl(R )dR
SO(3)
= f^l ^l

(R-1)Dl(R)dR
SO(3)

(R)Dl


(R) dR

SO(3)

(23)

So the SO(3)-Fourier transform of the SO(3) convolution of f and  is equal to the matrix product of the SO(3)-Fourier transforms f^ and ^.
For the sphere, we can derive an analogous transform that is sometimes called the spherical harmonics transform. The spherical harmonics Yml : S2  C are a complete orthogonal family of functions. The spherical harmonics are related to the Wigner D functions by the relation Dml n(, , ) = Yml (, )ein , so that Yml (, ) = Dml 0(, , 0).
The S2 convolution of f1 and f2 is equivalent to the SO(3) convolution of the associated rightinvariant functions f¯1, f¯2 (see Appendix A):

[f1  f2](R) = f1(x)f2(R-1x)dx
S2
= f1(x)f2(R-1x)dxdh
SO(2) S2
= f¯1(R )f¯2(R-1R )dR
SO(3)
= [f¯1  f¯2](R)

(24)

The Fourier transform of a right invariant function on SO(3) equals

[F f¯]lmn =

2 d 0 2

 d sin  02

2 0

d 2

f¯(,



,



)Dml n(,

,

)

=

2 d 0 2

 d sin  f (, )
02

2 0

d 2

Dml n(,

,

)

= n0

2 d 0 2

 0

d

sin 2



f (,

)Dml 0(,

,

0)

(25)

= n0 f (x)Yml (x)dx
S2
So we can think of the S2 Fourier transform of a function on S2 as the n = 0 column of the SO(3) Fourier transform of the associated right-invariant function. This is a beautiful result that we have not been able to find a reference for, though it seems likely that it has been observed before.

14

