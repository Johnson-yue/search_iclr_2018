Under review as a conference paper at ICLR 2018
TOWARDS PROVABLE CONTROL FOR UNKNOWN LINEAR DYNAMICAL SYSTEMS
Anonymous authors Paper under double-blind review
ABSTRACT
We study the control of symmetric linear dynamical systems with unknown dynamics and a hidden state. Using a recent spectral filtering technique for concisely representing such systems in a linear basis, we formulate optimal control in this setting as a convex program. This approach eliminates the need to solve the nonconvex problem of explicit identification of the system and its latent state, and allows for provable optimality guarantees for the control signal. We give the first efficient algorithm for finding the optimal control signal with an arbitrary time horizon  , with sample complexity (number of training rollouts) polynomial only in log  and other relevant parameters.
1 INTRODUCTION
Recent empirical successes of reinforcement learning involve using deep nets to represent the underlying MDP and policy. However, we lack any supporting theory, and are far from developing algorithms with provable guarantees for such settings. We can make progress by addressing simpler setups, such as those provided by control theory.
Control theory concerns the control of dynamical systems, a non-trivial task even if the system is fully specified and provable guarantees are not required. This is true even in the simplest setting of a linear dynamical system (LDS) with quadratic costs, since the resulting optimization problems are high-dimensional and sensitive to noise.
The task of controlling an unknown linear system is significantly more complex, often giving rise to non-convex and high-dimensional optimization problems. The standard practice in the literature is to first solve the non-convex problem of system identification--that is, recover a model that accurately describes the system--and then apply standard robust control methods. The non-convex problem of system identification is the main reason that we have essentially no provable algorithms for controlling even the simplest linear dynamical systems with unknown latent states.
In this paper, we take the first step towards a provably efficient control algorithm for linear dynamical systems. Despite the highly non-convex and high-dimensional formulation of the problem, we can efficiently find the optimal control signal in polynomial time with optimal sample complexity. Our method is based on wave-filtering, a recent spectral representation technique for symmetric LDSs (Hazan et al., 2017).
1.1 OUR RESULTS
A dynamical system converts input signals {1, . . . ,  }  R into output signals {1, . . . ,  }  R, incurring a sequence of costs 1, . . . ,   R. We are interested in controlling unknown dynamical systems with hidden states (which can be thought of as being partially "observed"via the output signals). A vast body of work focuses on linear dynamical systems with quadratic costs, in which the {} and {} are governed by the following dynamics:
+1 =  + ,  =  +  + ,  =   +  , where 1, . . . ,   R is a sequence of hidden states starting with a fixed 1. Matrices , , , , ,  of appropriate dimension describe the system and cost objective; the {} are Gaussian noise vectors. All of these matrices, as well as the parameters of the Gaussian, can be
1

Under review as a conference paper at ICLR 2018

unknown. The most fundamental control problem involves controlling the system for some time horizon  : find a signal 1, . . . ,  that minimizes the sum of these quadratic output costs  . Clearly, any algorithm for doing so must first learn the system parameters in some form, and this is often the source of computational intractability (meaning algorithms that take time exponential in the number of system parameters).
Previously known algorithms are of two types. The first type tries to solve the non-convex problem, with algorithms that lack provable guarantees and may take exponential time in the worst case: e.g., expectation-maximization (EM) or gradient-based methods (back-propagation through time, like the training of RNNs) which identify both the hidden states and system parameters.
Algorithms of the second type rely upon regression, often used in time-series analysis. Since  -step dynamics of the system involve the first  powers of , the algorithm represents these powers as new variables and learns the  -step dynamics via regression (e.g., the so-called VARX(, ) model) assuming that the system is well-conditioned; see Appendix A. This has moderate computational complexity but high sample complexity since the number of parameters in the regression scales with  , the number of time steps.
Our new method obtains the best of both results: few parameters to train resulting in low sample complexity, as well as polynomial computational complexity. Table 1 below compares the different methods, where we denote by |D| = max( ,  ,  ,  , , ) the size of the system, and by  the time horizon for planning.

Method System Identification
VARX(0, ) Ours

Sample complexity poly(-1, |D|) poly(-1, |D|, )
poly(-1, |D|, log  )

Time complexity exponential poly(, |D|) poly(, |D|)

Assumptions

none



=



Ä

1 1-2

ä

or



=



 symmetric

Table 1: Comparison of control algorithms.

In Section 2 we state the precise algorithm. The informal result is as follows.

Theorem 1.1 (Controlling an unknown LDS; informal). Let D be a linear dynamical system with
a symmetric transition matrix  and with size |D|. Then, for every  > 0, Algorithm 1 produces a sequence of controls (^1, . . . ^ )  B2 with ^1:2  , such that

E

1 



costD (^1:)

=1



min E
1:  B2 1: 2  

1 



costD (1:)

=1

+ .

(1)

Assuming i.i.d. Gaussian noise    (0, ), the algorithm samples ~ (poly (|D|, , Tr , 1/)) trajectories from D, and runs in time polynomial in the same parameters.

1.2 RELATED WORK
The field of optimal control for dynamical systems is extremely broad and brings together literature from machine learning, statistical time-series analysis, dynamical system tracking and Kalman filtering, system identification, and optimal control. For an extensive survey of the field, see e.g. (Todorov, 2006; Bertsekas, 2000).
Tracking a known system. A less ambitions goal than control is tracking of a dynamical system, or prediction of the output give a known input. For the special case of LDS, the well-known Kalman filter (Kalman, 1960) is an optimal recursive least-squares solution for maximum likelihood estimation (MLE) under Gaussian perturbations to a linear dynamical system.
System identification. When the underlying dynamical system is unknown, there are essentially no provably efficient methods for recovering it. For various techniques used in practice, see the

2

Under review as a conference paper at ICLR 2018

classic survey (Ljung, 1998). Roweis & Ghahramani (1999) suggest using the EM algorithm to learn the parameters of an LDS, nowadays widespread, but it is well-known that optimality is not guaranteed. The recent result of Hardt et al. (2016) gives a polynomial time algorithm for system recovery, although it applies only to the single-input-single-output case and makes various statistical assumptions on the inputs.
Model-free tracking. Our methods depend crucially on a new algorithm for LDS sequence prediction, at the heart of which is a new convex relaxation for the tracking formulation (Hazan et al., 2017). In particular, this method circumvent the obstacle of explicit system identification. We detail our usage of this result in Definition 2.3.
We note an intriguing connection to the recently widespread use of deep neural networks to represent an unknown MDP in reinforcement learning: the main algorithm queries the unknown dynamical system with exploration signals, and uses its responses to build a compact representation (denoted by ^ in Algorithm 1) which estimates the behavior of the system.
Time-series analysis. One of the most common approaches to modeling dynamical systems is the autoregressive-moving average (ARMA) model and its derivatives in the time-series analysis literature (Hamilton, 1994; Box et al., 1994; Brockwell & Davis, 2009). At the heart of this method is the autoregressive form of a time series, namely,

 = - + .
=1
Using online learning techniques, it is possible to completely identify an autoregressive model, even in the presence of adversarial noise (Anava et al., 2013). This technique lies at the heart of a folklore regression method for optimal control, given in the second row of table 1.
Optimal control. The most relevant and fundamental primitive from control theory, as applied to the control of linear dynamical systems, is the linear-quadratic-Gaussian (LQG) problem. In this setting, the system dynamics are assumed to be known, and the task is to find a sequence of inputs which minimize a given quadratic cost. A common solution, the LQG controller, is to combine Kalman filtering with a linear-quadratic regulator, a controller selected by solving the Bellman equation for the problem. Such an approach admits theoretical guarantees under varied assumptions on the system; see, for example, Dean et al. (2017).
Our setting also involves a linear dynamical system with quadratic costs, and thus can be seen as a special case of the LQG setup, in which the process noise is zero, and the transition matrix is assumed to be symmetrics. However, our results are not analogous: our task also includes learning the system's dynamics. As such, our main algorithm for control takes a very different approach than that of the standard LQR: rather than solving a recursive system of equations, we provide a formulation of control as a one-shot convex program.

2 STATEMENT OF MAIN THEOREM

First, we state the formal definitions of the key objects of interest.
Definition 2.1 (Dynamical system). A dynamical system D is a mapping that takes a sequence of input vectors 1, . . . ,   B2 = {  R : 2  1} to a sequence of output vectors 1, . . . ,   R and costs 1, . . . ,   R. Denote : = [ ; . . . ; ] as the concatenation of all input vectors from time  to , and write
D(1:) =  , costD (1:) = .
Definition 2.2 (Linear dynamical system). A linear dynamical system (LDS) is a dynamical system whose outputs and costs are defined by

+1 =  + ,  =  +  +   =   +  ,

with    (0, ),

3

Under review as a conference paper at ICLR 2018

where 1, . . . ,   R is a sequence of hidden states starting with fixed 1, and , , , , ,  are matrices (or vectors) of appropriate dimension. We assume op  1, i.e., all singular values
of  are at most one, and that  0,  0.

Our algorithm and its guarantees depend on the construction of a family of orthonormal vectors in R , which are interpreted as convolution filters on the input time series. We define the wave-filtering
matrix below; for more details, see Section 3 of Hazan et al. (2017).

Definition 2.3 (Wave-filtering matrix). Fix any ,  , and 1     . Let  be the eigenvec-

tor corresponding to the -th largest eigenvalue of the Hankel matrix   R × , with entries



:=

(+

2 )3 -(+ )

.

The wave-filtering matrix 



R×

is defined by  vertically stacked

block matrices {()  R× }, defined by horizontally stacked multiples of the identity matrix:

ïò () d=ef  (1)   (2)  . . .  ( )  .

Then, letting  range from 1 to  , :- then gives a dimension-wise convolution of the input time series by the filters {} of length  . Theorem 3.3 uses a structural result from Hazan et al. (2017), which guarantees the existence of a concise representation of D in the basis of these filters.
The main theorem we prove is the following.

Theorem 2.4 (Controlling an unknown LDS). Let D be a LDS with a symmetric transition matrix  and with  ,  ,  , op  . For every  > 0, Algorithm 1, with a choice of  = (log2  log(/)) and  =  22 Tr() log(/)-2 , produces a sequence of
controls (^1, . . . ^ )  B2 , such that with probability at least 1 - ,

E

1 



costD (^1:)

=1



min E
1:  B2 1:   

1 



costD (1:)

=1

+ ,

(2)

assuming that

min E
1:  B2 1:   

1 



costD (1:)

=1

 Tr() + (2)

(3)

where  is such that 

. Further, the algorithm samples

poly

1 

,

log

1 

, log , log , , , , Tr()

trajectories from the dynamical system, and

runs in time polynomial in the same parameters.

We remark on condition (3). Tr() is inevitable loss due to background noise, so (3) is an assumption on the system's controllability.  is bounded away from 0 when we suffer loss in all directions of .

We set up some notation for the algorithm. Let   R× be the wave-filtering matrix from Definition 2.3. Let  = 0 for   0, and let  = :- +1. Let  = max( ,  ,  ,  ) be an upper bound for the matrices involved in generating the outputs and costs.

3 PROOF OF MAIN THEOREM

To prove Theorem 2.4, we invoke Lemma 3.1 and Lemma 3.2, proved in Subsection 3.1 and Subsection 3.2, respectively.

Lemma 3.1 (Learning dynamics). For every  > 0 and symmetric LDS D, with probability  1 - ,

setting



=

(log2 

log(/))

and



=



Å

2 

Tr() 2

ln(

 

)

ã ,

step

7

in

Algorithm

1

computes

a matrix ^  R× such that for every sequence of input signals (1, . . . ,  )  B2 satisfying 1: 2   and 1     , we have that

^ - E  

(5)

where ^ = ^  +  and  = D(1:).

(6)

4

Under review as a conference paper at ICLR 2018

Algorithm 1: Control with an LDS oracle

Input : Oracle access to LDS D, cost matrices , , filter parameter , sample count .

Output: Control inputs ^1: .

1 Run D with the all-zeros input  times, recording responses 1(:) := D1: (0) for each 1    .

2

Average the zero-impulse responses:

1:

:=

1 

 =1

1(:)

.

3 for 1     do

4 Run D with input   times, recording responses (,) := D () for each 1    .

5

Average

the

exploration

responses:

()

:=

1 

 =1

(,)

.

6 end

7 Let ^ be the matrix whose -th column is ^ := () -  . 8 Solve the following convex program to obtain controls (^1, . . . ^ ):

^1: :=

arg min
1:  B2 1: 2  



î(^



+

)(^



+

)

+



ó 

.

=1

(4)

Lemma 3.2 (Robustness of control to uncertainty in dynamics). Let

^1: =

arg min
1:  B2 1:   


îD^ (1:)D^ (1:) +  ó ,
=1

where ^  R× is such that for every sequence of input signals (1, . . . ,  )  B2 with 1: 2   and    , Equation (5) holds with ^ = D^ (1:). Assume (3). Then it is true that

E

1 



costD (^1:)

=1



min E
1:  B2 1:   

1 



costD (1:)

=1

+  () ,

(7)

Moreover, the minimization problem posed above is convex (for ,  0), and can be solved to within opt acccuracy in poly(log , , , 1/opt) time using the ellipsoid method.

Proof of Theorem 2.4.

Use

Lemma

3.1

with







Ä

 

ä .

Note

that

1:

= 

is a valid input to the

LDS because 2 = 1. Now use Lemma 3.2 on the conclusion of Lemma 3.1.

3.1 LEARNING THE DYNAMICS

To prove Lemma 3.1, we will use the following structural result from Hazan et al. (2017) restated to match the setting in consideration.

Theorem 3.3. Let D be a symmetric LDS with the stated conditions from Section 2, with fixed 1. Let  > 0, and let  = (log2  log(/)). Let 1: be the output if the input signal is 0. Let   be the matrix such that for all input-output pairs (1: , 1: ), E( -  ) =   . Then for all 1:  B2, the matrix  =   satisfies

E( - ) -    ,

  .

(8)

Proof. This follows from Theorem 3b in Hazan et al. (2017) after noting four things.

1. E( - ) are the outputs when the system is started at 1 = 0 with inputs 1: and no noise. A linear relationship  -  =   holds by unfolding the recurrence relation.
2. Examining the construction of  in the proof of Theorem 3b, the  is exactly the projection of   onto the subspace defined by . (Note we are using the fact that the rows of  are orthogonal.)

5

Under review as a conference paper at ICLR 2018

3. Theorem 3b is stated in terms of the quantity , which is bounded by 2 by Lemma F.5 in Hazan et al. (2017).

4.

We

((

 

can

 

)

,

modify

(

 

)

,

(

the ),

system ). This

so that replaces

 

=  by by (max(,

replacing (, , , ) with )). Theorem 3b originally

had dependence of  on both  and , but if  =  then the dependence is only on

.

Proof of Lemma 3.1. Take 

=



Ä

2 2

Tr() log

 

ä .

Letting   be the matrix such that

 -  =  , and  =   as in Theorem 3.3, we have that

E[() -  ] =   =  .

(9)

Let 1     . We bound the error under controls 1:  B2 , 1:2   using the triangle inequality. Letting 1: be the output under 1:,

E -  - ^ 

2  E( - ) -  2 + E() - 2 +

  - ^ 

.
2

(10)

By Theorem 3.3, for  = (log2  log(/)), choosing constants appropriately, the first term is



 4

.

To bound the second term in (10), we show  concentrates around E. We have

 - E  ,







Å 0,

1

ã 

.



(11)

By concentration of sums of 2 random variables (see Hsu et al. (2012), for example),

P

(0,

1 

)

(

2



)





as

long

as





5 2

Tr() log

Å3ã .


(12)

Take 

=

 2

and 

=

 4 

and note 

was chosen to satisfy the above.

Use the union bound to

get that

Å P 



[1,  ], 

- E2



 ã 4 



 .
2

(13)

To bound the third term in (10), we first show that ^ concentrates around  . We have

^

=

1 



(,) - 

=1

so

  

^

- 

=

1 



(,) -  - (E(1,) - E )

=1

=  + (E -  ),

^ -  =  + (E -  )1,







Å 0,

1

ã 









Å 0,

1

ã  .



By 2 concentration,

We also have

P ( )

0,

1 



2



 4


(E -  )1    E -  2.



 .

2

(14)
(15) (16) (17)
(18)

6

Under review as a conference paper at ICLR 2018

With



1 - 

probability,

we

avoid

both

the

bad

events

in (13)

and

(18)

so



- E2



 4 

for all 1     and

^ - 





 + 



 =

(19)

 4

4  2

and for all 1:  , the third term of (10) is bounded by (note op = 1 because it has orthogonal rows)

  - ^ 

  - ^  2

  - ^  2



   = .

2 2

Thus by (10), E -  - ^    with probability  1 - .
2

(20) (21) (22)

3.2 ROBUSTNESS OF CONTROL TO UNCERTAINTY IN DYNAMICS

To prove Lemma 3.2, we need the following helpful lemma.

Lemma 3.4. For a symmetric LDS D with  = max( ,  ,  ,  op) and  ,  , and an approximation D^  whose predictions ^ satisfy the conclusion of Lemma 3.1, we have that for every sequence of controls (1, . . . ,  )  B2 , and for every 1     ,

Ã

îcostD^ (1:) + Tr() - E costD (1:) ó  2  min
=1

 
^2 , E 2
=1 =1

+  2 (23)

where costD^ (1:) = D^ (1:)D^ (1:) +  .

Proof of Lemma 3.4. Let ^ = D^ (1:) and  = D(^1:) = E[] + . Using the assumption that E[ ] = ,

costD^ (1:) + Tr() - E costD (1:)

(24)

= ^ ^ + Tr() - E (E[] + )(E[] + )

(25)

= ^ ^ - E[]E[]

 2 ^ - E op min(^ , E) + op 2



îcostD^ (1:) - E

costD (1:)

ó - Tr()

=1

Ã

 

 2 op   min

^2 , E 2 +  op 2

=1 =1

(26) (27) (28)
(29)

using Cauchy-Schwarz.

Proof of Lemma 3.2. Define



1*: = arg min E

D(1:)

1:  B2

=1

1:   

^1: =

arg min
1:  B2 1:   


D^ (1:)
=1

(30) (31)

7

Under review as a conference paper at ICLR 2018

Letting  =

 =1

[E[costD (1*:)]

-

Tr()],

for

inputs

1*:.

By assumption (3), 

= ( 2).

We have


 = (E) (E)   1: 2

=1

=

1:





...

 

.

(32) (33)

Now by Lemma 3.4,

E



costD^ (1*:)

 

...   + 2

+  2



=1

Letting ^1: be the outputs under D^ under the control ^1: , note that similar to (32),

(34)

Ã

...1 ^1:   


E costD^ (^1:) .
=1

(35)

Using Lemma 3.4,



îcostD^ (^1:)

+



Tr()

-

E[costD

ó (^1:)]

  2  ^1:  +  2.

=1

(36)

Now

costD(^1:) - costD(*1:)  (costD ^1: + Tr() - costD^ (^1:))

(37)

+ (costD^ (^1:) - costD^ (*1:)) + (costD^ (1*:) - Tr() - costD(1*:))

(38)

 



...  2



...   + 2

+



2

+

0

+

Ç... 2





å +  2 .

 



(39)

The assumption  = ( 2) gives the lemma.

4 CONCLUSION
We have presented an algorithm for finding the optimal control inputs for an unknown symmetric linear dynamical system, which requires querying the system only a polylogarithmic number of times in the number of such inputs  , while running in polynomial time. Deviating significantly from previous approaches, we circumvent the non-convex optimization problem of system identification by a new learned representation of the system. We see this as a first step towards provable, efficient methods for the traditionally non-convex realm of control and reinforcement learning.

REFERENCES
Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pp. 172­184, 2013.
Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition, 2000. ISBN 1886529094.
G. Box, G. Jenkins, and G. Reinsel. Time Series Analysis: Forecasting and Control. Prentice-Hall, 3 edition, 1994.
P. Brockwell and R. Davis. Time Series: Theory and Methods. Springer, 2 edition, 2009.

8

Under review as a conference paper at ICLR 2018
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.
J. Hamilton. Time Series Analysis. Princeton Univ. Press, 1994. Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
arXiv preprint arXiv:1609.05191, 2016. Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering.
In Advances in Neural Information Processing Systems, pp. 1­2, 2017. Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012. Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of Basic
Engineering, 82.1:35­45, 1960. Lennart Ljung. System identification: Theory for the User. Prentice Hall, Upper Saddle Riiver, NJ,
2 edition, 1998. Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural compu-
tation, 11(2):305­345, 1999. Emanuel Todorov. Optimal control theory. Bayesian brain: probabilistic approaches to neural
coding, pp. 269­298, 2006.
9

Under review as a conference paper at ICLR 2018

A AUTOREGRESSIVE MODELS AS A RELAXATION OF LDS

In this section, we verify the statement made in Section 1 on the time and sample complexity of approximating a linear dynamical system with an autoregressive model. Although this is wellknown (see, for example, Section 6 of Hardt et al. (2016)), we present a self-contained presentation for convenience and to unify notation.

The vector autoregressive model with exogenous variables, or VARX(, ), is a touchstone in timeseries analysis. Given a time series of inputs (sometimes known as biases) {}, it generates the time series of responses {} by the following recurrence:
 -1
 = - + - + .
=1 =0
Here,  and  are memory parameters, the {} and {} are matrices of appropriate dimension, and the {} are noise vectors.

In the special case of  = 0, the problem can be solved efficiently with linear regression: in this case,  is a linear function of the concatenated inputs [; -1; . . . , -+1].
A VARX(0, ) model is specified by  = [ (0), . . . ,  (-1)]  R× and predicts  =  :-+1.

We quantify the relationship between VARX(0, ) and linear dynamical systems, with a statement analogous to Theorem 3.1:

Theorem A.1. Let D be an LDS with size , fixed 1, and noise  = 0, producing outputs

{1, . . . ,  } from inputs {1, . . . ,  }. Suppose that the transition matrix of D has operator norm

at

most



<

1.

Then,

for

each



>

0,

there

is

a

VARX(0, )

model

with



=

(

1 1-

log(/)),

specified by a matrix  , such that

 -  :-+1  .

Proof. By the modification of D given in the proof of Theorem 3.3, we may assume without loss of generality that  = . Also, as in the discussion of Theorem 3.1, it can be assumed that the initial hidden state 1 is zero.

Then, we construct the block of  corresponding to lag  as

 () = .

This is well-defined for all 1    . Note that when   , the autoregressive model completely specifies the system D, which is determined by its (infinite-horizon) impulse response function. Furthermore, by definition of , we have

Noting that we conclude that

 ()2  2.

-1

 -  :-+1 =

 ()-,

=



-  :-+1



-1

2-2



2 1 -  ,

=

implying the claim by the stated choice of .

VARX(0, ) only serves as a good approximation of an LDS whose hidden state decays on a time scale shorter than ; when the system is ill-conditioned ( is close to 1), this can get arbitrarily large, requiring the full time horizon  =  .
On the other hand, it is clear that both the time and sample complexity of learning a VARX(0, ) model grows linearly in . This verifies the claim in the introduction.

10

