Under review as a conference paper at ICLR 2018
COVARIANT COMPOSITIONAL NETWORKS FOR LEARNING GRAPHS
Anonymous authors Paper under double-blind review
ABSTRACT
Most existing neural networks for learning graphs deal with the issue of permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on some standard graph learning benchmarks.
1 INTRODUCTION
Learning on graphs has a long history in the kernels literature including approaches based on random walks (Ga¨rtner, 2002a; Borgwardt & Kriegel, 2005; Feragen et al., 2013), counting subgraphs (Shervashidze et al., 2009), spectral ideas (Vishwanathan et al., 2010a), label propagation schemes with hashing (Shervashidze et al., 2011b; Neumann et al., 2016b) and even algebraic ideas (Kondor & Borgwardt, 2008b). Many of these papers address moderate size problems in chemo- and bioinformatics, and are, to some extent, always hand crafted to the domain in the sense that the kernel and hence the graph representation that they use is fixed.
Recently, with the advent of deep learning and the apperance of much larger datasets, a sequence of neural network based approaches have appeared which manage to integrate the classification or regression problem at hand with learning the data representation itself in a single, end-to-end system. This approach has the attractive property that the features are learned automatically, adapted to the domain, eliminating the need to design features by hand. Although work on graph neural networks goes back to (Scarselli et al., 2009), there has been a recent explosion in research activity in this area. Some of this work (Duvenaud et al., 2015; Kearns et al., 2016; Niepert et al., 2016) directly seeks inspiration from classical image based CNNs (LeCun et al., 1998; Krizhevsky et al., 2012). These methods involve first fixing a vertex ordering, then moving a filter across vertices while doing some computation as a function of the local neighborhood to generate a representation. This process is then repeated multiple times as in classical CNNs to build a deep graph representation. Some other notable works on graph neural networks include (Li et al., 2015; Schu¨tt et al., 2017; Battaglia et al., 2016; Kipf & Welling, 2017). More recently, most of these approaches have been showed to be specific instances of a message passing formalism and dubbed Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017).
While MPNNs have been very successful in various applications and are an active field of research, they differ from classical CNNs in a fundamental way: the internal feature representations in CNNs are equivariant to transformations of the input such as translation and rotations (Cohen & Welling, 2016a;b), contrasted with those in MPNNs, which are merely invariant. This is a direct result of the fact that MPNNs deal with the permutation invariance issue in graphs simply by summing the messages coming from each neighbor. In this paper we argue that this is a serious limitation that limits the representation power of MPNNs.
1

Under review as a conference paper at ICLR 2018
MPNNs are ultimately compositional (part-based) models, that build up the representation of the graph from the representations of a hierarchy of subgraphs. To address the covariance issue, we therefore study the covariance behavior of such networks in general, introducing a general new architecture that we dub compositional networks (comp-nets). One advantage of this generalization is that instead of focusing attention on the mechanics of how information propagates from node to node, it emphassizes the connection to convolutional networks, in particular, it shows that essentially what is missing from MPNNs is the analog of steerability.
Steerability implies that the activations (feature vectors) at a given neuron must transform according to a specific representation (in the algebraic sense) of the symmetry group of its receptive field, in our case, the group of permutations, Sm. In this paper we only consider the defining representation and its tensor products, leading to first, second, third etc. order tensor activations. We derive the general form of covariant tensor propagation in comp-nets, and we find that each "channel" in the network corresponds to a specific way of contracting a higher order tensor to a lower order one. Note that here by tensor activations we mean not just that each activation is expressed as a multidimensional array of numbers (as the word is usually used in the neural networks literature), but also that it transforms in a specific way under permutations, which is a stringent criterion. The parameters of our covariant comp-nets are the entries of the mixing matrix that prescribe how these channels communicate with each other at each node. Our experiments show that this new architecture can beat scalar message passing neural networks on some standard datasets.
2 LEARNING GRAPHS
Graph learning encompasses a broad range of problems where the inputs are graphs, and the outputs are class labels (classification), real valued quantities (regression) or more general, possibly combinatorial, objects. In the standard supervised learning setting this means that the training set consists of m input/output pairs {(G1, y1), (G2, y2), . . . , (Gm, ym)}, where each Gi is a graph and yi is the corresponding label, and the goal is to learn a function h : G  y that will successfully predict the labels of further graphs that were not in the training set.
By way of fixing our notation, in the following we assume the each graph G is a pair (V, E), where V is the vertex set of G and E  V ×V is its edge set. For simplicy, we assume that V = {1, 2, . . . , n}. We also assume that G has no self-loops ((i, i)  E for any i  V ) and that G is symmetric, i.e., (i, j)  E  (j, i)  E1. We will, howevever, allow for the possibility that each edge (i, j) has a corresponding weight wi,j, and that each vertex i has a corresponding feature vector (vertex label) li  Rd. The latter, in particular, is important in many scientific applications, where li might encode, for example, what type of atom occupies a particular site in a molecule, or the idenity of a protein in a biochemical interaction network. All the topological information about G can be summarized in an adjacency matrix A  Rn×n, where Ai,j = wi,j if i and j are connected by an edge, and otherwise Ai,j = 0. When dealing with labeled graphs, we also have to set (l1, . . . , ln) to fully specify G.
One of the most fascinating aspects of graphs, but also what makes graph learning challenging, is that they involve structure at multiple different scales. In the case when G is the graph of a protein, for example, an ideal graph learning algorithm would represent G in a manner that simultaneously captures structure at the level of individual atoms, functional groups, interactions between functional groups, subunits of the protein, and the protein's overall shape.
The other major requirement for graph learning algorithms relates to the fact that the usual ways to store and present graphs to learning algorithms have a critical spurious symmetry: If we permute the vertices of G by any permutation  : {1, 2, . . . , n}  {1, 2, . . . , n} (in other words, rename vertex 1 as (1), vertex 2 as (2), etc.), then the adjacency matrix changes to
Ai,j = A-1(i),-1(j),
and simulateneously the vertex labels change to (l1 , . . . , ln ), where li = l-1(i). However, G = (A, l1 , . . . , ln ) still represents exactly the same graph as G = (A, l1, . . . , ln). In particular, (a) in training, whether G or G is presented to the algorithm must not make a difference to the final
1Our framework has natural generalizations to non-symmetric graphs and graphs with self-loops, but in the interest of keeping our discussion as simple as possible, we will not discuss these cases in the present paper.
2

Under review as a conference paper at ICLR 2018

654

546

123

312

Figure 1: (a) A small graph G with 6 vertices and its adjacency matrix. (b) An alternative form G of the same graph, derived from G by renumbering the vertices by a permutation  : {1, 2, . . . , 6}  {1, 2, . . . , 6}. The adjacency matrices of G and G are different, but topologically they still represent the same graph. Therefore, we expect the feature map  to satisfy (G) = (G).

hypothesis h learned by the algorithm, and (b) h itself must satisfy h(G) = h(G). Most learning algorithms for combinatorial objects hinge on some sort of fixed or learned internal representation of data, called the feature map, which, in our case we denote (G). The set of all n! possible permutations of {1, 2, . . . , n} forms a group called the symmetric group of order n, denoted Sn. The renumbering invariance criterion can then be formulated as follows (Figure 1).
Definition 1. Let A be a graph learning algorithm that uses a feature map G  (G). We say that the feature map  (and consequently the algorithm A) is permutation invariant if, given any n  N, any n vertex labeled graph G = (A, l1, . . . , ln), and any permutation   Sn, letting G = (A, l1 , . . . , ln ), where Ai,j = A-1(i),-1(j) and li = l-1(i), we have that (G) = (G).
Capturing multiscale structure and respecting permutation invariance are the two the key constraints around which most of the graph learning literature revolves. In kernel based learning, for example, invariant kernels have been constructed by counting random walks (Ga¨rtner, 2002b), matching eigenvalues of the graph Laplacian (Vishwanathan et al., 2010b) and using algebraic ideas (Kondor & Borgwardt, 2008a).
3 COMPOSITIONAL NETWORKS
Many recent graph learning papers, whether or not they make this explicit, employ a compositional approach to modeling graphs, building up the representation of G from representations of subgraphs. At a conceptual level, this is similar to part-based modeling, which has a long history in machine learning (Fischler & Elschlager, 1973; Ohta et al., 1978; Tu et al., 2005; Felzenszwalb & Huttenlocher, 2005; Zhu & Mumford, 2006; Felzenszwalb et al., 2010). In this section we introduce a general, abstract architecture called compositional networks (comp-nets) for representing complex objects as a combination of their parts, and show that several exisiting graph neural networks can be seen as special cases of this framework.
Definition 2. Let G be a compound object with n elementary parts (atoms) E = {e1, . . . , en}. A composition scheme for G is a directed acyclic graph (DAG) M in which each node ni is associated with some subset Pi of E (these subsets are called the parts of G) in such a way that 1. If ni is a leaf node, then Pi contains a single atom e(i)2. 2. M has a unique root node nr, which corresponds to the entire set {e1, . . . , en}. 3. For any two nodes ni and nj, if ni is a descendant of nj, then Pi  Pj.
We define a comp-net as a composition scheme in which each node ni also carries a feature vector fi that provides a representation of the corresponding part (Figure 2). When we want to emphasize the connection to more classical neural architectures, we will refer to ni as the i'th neuron, Pi as its receptive field3, and fi as its activation.
Definition 3. Let G be a compound object in which each atom ei carries a label li, and M a composition scheme for G. The corresponding compositional network N is a DAG with the same structure as M in which each node ni also has an associated feature vector fi such that 1. If ni is a leaf node, then fi = l(i).
2 Here  is just a function that stablishes the mapping between each leaf node and the corresponding atom. 3 Here and in the following by the "receptive field" of a neuron ni in a feed-forward network we mean the set of all input neurons from which information can propapate to ni.

3

Under review as a conference paper at ICLR 2018

M

nr {e1, e2, e3, e4}

N fr (f8, f9, f10)

{e2, e3, e4} n8 {e3, e4} n5

n9 n10 {e1, e2, e4}

{e1, e4} n6

{e2, e3} n7

f8 n8

f9 n9

f5 f6 n5 n6

f10 n10
f7 n7

n1 {e1 }

n2 {e2 }

n3 {e3 }

n4 {e4 }

f1 f2 f3 f4 n1 n2 n3 n4

Atomic parts of G

Representations of atomic parts of G

Figure 2: (a) A composition scheme for an object G is a DAG in which the leaves correspond to atoms, the internal nodes correspond to sets of atoms, and the root corresponds to the entire object.
(b) A compositional network is a composition scheme in which each node ni also carries a feature vector fi. The feature vector at ni is computed from the feature vectors of the children of ni.

M nr

M nr

e1 e2 e3 e4

e1 e4 e2 e3

Figure 3: A minimal requirement for composition schemes is that they be invariant to permutation, i.e. that if the numbering of the atoms is changed by a permutation , then we must get an isomorphic DAG. Any node in the new DAG that corresponds to {ei1 , . . . , eik } must have a corrresponding node in the old DAG corresponding to {e-1(i1), . . . , e-1(ik)}.
2. If ni is a non-leaf node, and its children are nc1 , . . . , nck , then fi = (fc1 , fc2 , . . . , fck ) for some aggregation function . (Note: in general,  can also depend on the relationships between the subparts, but for now, to keep the discussion as simple as possible, we ignore this possibility.)
The representation (G) afforded by the comp-net is given by the feature vector fr of the root.
Note that while, for the sake of concreteness, we call the fi's "feature vectors", there is no reason a priori why they need to be vectors rather than some other type of mathematical object. In fact, in the second half of the paper we make a point of treating the fi's as tensors, because we will want to make sure that they transform in specific ways with respect to permutations.
In the case of compositional networks for graphs, the atoms will usually be the vertices, and the Pi parts will correspond to clusters of nodes or neighborhoods of given radii. Comp-nets are particularly attractive in this domain because they combine information from the graph at different scales. Another advantage of the comp-net approach is that it suggests a natural path to satisfying the permutation invariance criterion of Definition 1.
Definition 4. Let M be the composition scheme of an object G with n atoms and M the composition scheme of another object that is equivalent in structure to G, except that its atoms have been permuted by some permutation   Sn (ei = e-1(i) and i = -1(i)). We say that M (more precisely,

4

Under review as a conference paper at ICLR 2018

the algorithm generating M) is permutation invariant if there is a bijection  : M  M taking each na  M to some nb  M such that if Pa = {ei1 , . . . , eik }, then Pb = {e (i1), . . . , e (ik)}.
Proposition 1. Let (G) be the output of a comp-net based on a composition scheme M. Assume 1. M is permutation invariant in the sense of Definition 4. 2. The aggregation function (fc1 , fc2 , . . . , fck ) used to compute the feature vector of each node
from the feature vectors of its children is invariant to the permutations of its arguments.
Then the overall representation (G) is invariant to permutations of the atoms. In particular, if G is a graph and the atoms are its vertices, then  is a permutation invariant graph representation.

3.1 MESSAGE PASSING NEURAL NETWORKS AS A SPECIAL CASE OF COMP-NETS

Graph learning is not the only domain where invariance and multiscale structure are important: the most commonly cited reasons for the success of convolutional neural networks (CNNs) in image tasks is their ability to address exactly these two criteria in the vision context. Furthermore, if we consider that each neuron ni in a CNN aggregates information from a small set of neurons from the previous layer and therefore its receptive field, corresponding to Pi, is the union of the receptive fields of its "children", we get a hierarchical structure very similar to that described in the previous section. In this sense, CNNs are a specific kind of compositional network, where the atoms are pixels. This connection has inspired some authors to frame graph learning as a generalization of convolutional nets to the graph domain (Bruna et al., 2014; Henaff et al., 2015; Duvenaud et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2017).

While in mathematics convolution has a fairly specific meaning that is side-stepped by this analogy,
the CNN analogy does suggest that a natural way to define the  aggregation functions is to let
(fc1 , fc2 , . . . , fck ) be a linear function of fc1 , fc2 , . . . , fck followed by a pointwise nonlinearity, such as a ReLU operation.

To define a comp-net for graphs we also need to specify the composition scheme M. Many algorithms define M in layers, where each layer (except the last) has one node for each vertex of G:

M1. In layer  = 0 each node n0i represents the single vertex Pi0 = {i}. M2. In layers  = 1, 2, . . . , L, node ni is connected to all nodes from the previous level that are
neighbors of i in G, i.e., the children of ni will be

where

N

(i)

denotes

the

set

of

ch(ni) = { nj-1 | j neighbors of i in G.

 N (i)}, Therefore

Pi

=


jN (i)

Pj-1.

M3. In layer L+1 we have a single node nr that represents the entire graph and collects information

from all nodes at level L.

Since this construction only depends on topological information about G, the resulting composition

scheme is guaranteed to be permutation invariant in the sense of Definition 4.

A further important consequence of this particular way of defining M is that the corresponding comp-net can be equivalently interpreted as label propagation algorithm, where in each round  = 1, 2, . . . , L, each vertex aggregates information from its neighbors and then updates its own label.

Algorithm 1 The label propagation algorithm corresponding to M1­M3

for each vertex i fi0  li
for  = 1 to L
for each vertex i fi  (fi1-1, . . . , fik-1)
(G)  fr  (f1L, . . . , fnL)

where

N (i) = {i1, . . . , ik}

Many authors choose to describe graph neural networks exclusively in terms of label propagation, without mentioning the compositional aspect of the model. Gilmer et al. (2017) call this general approach message passing neural networks, and point out that a range of different graph learning architectures are special cases of it. More broadly, the classic Weisfeiler­Lehman test of isomorphism also follows the same logic (?Read & Corneil, 1977; Cai et al., 1992), and so does the related

5

Under review as a conference paper at ICLR 2018

Figure 4: In convolutional neural networks if the input image is translated by some amount (t1, t2),

what used Therefore,

to fall in the receptive field of neuron the activations transform in the very

ni,j is simple

moved way f

to the receptive

 i+t1 ,j +t2

=

fi,j

field . In

of ni+t1,j+t2 . contrast, rota-

tions not only move the receptive fields around, but also permute the neurons in the receptive field

internally,

therefore

in

general,

f



 j,-i

=

fi,j

.

The right hand figure shows that if the CNN has a

horizontal filter (blue) and a vertical one (red) then their activations are exchanged. In steerable

CNNs,

if

(i,

j)



(i,

j),

then

f



 i

,j



=

R(fi,j )

for

some

fixed

linear

function

of

the

rotation.

Weisfeiler­Lehman kernel, arguably the most successful kernel-based approach to graph learning (Shervashidze et al., 2011a). Note also that in label propagation or message passing algorithms there is a clear notion of the source domain of vertex i at round , as the set of vertices that can influence fi, and this corresponds exactly to the receptive field Pi of "neuron" ni in the comp-net picture.
The following proposition is immediate from the form of Algorithm 1 and reassurres us that message passing neural networks, as special cases of comp-nets, do indeed produce permutation invariant representations of graphs.
Proposition 2. Any label propagation scheme in which the aggregation function  is invariant to the permutations of its arguments is invariant to permutations in the sense of Definition 1.
In the next section we argue that invariant message passing networks are limited in their representation power, however, and describe a generalization via comp-nets that overcomes some of these limitations.

4 COVARIANT COMPOSITIONAL NETWORKS

One of the messages of the present paper is that invariant message passing algorithms, of the form described in the previous section, are not the most general possible compositional models for producing permutation invariant representations of graphs (or of compound objects in general).

Once again, an analogy with image recognition is helpful. Classical CNNs face two types of basic

image transformations: translations and rotations. With respect to translations (barring pooling,

edge effects and other complications), CNNs behave in a quasi-invariant way, in the sense that if the

input image is translated by any integer amount (tx, ty), the activations in each layer  = 1, 2, . . . L

translate the same way: the

i.e.,

f



 i+t1

,j

+tx=

fi,j

.

This

activation of any neuron ni,j is the simplest manifestation

is of

simply a well

transferred to neuron ni+t1,j+t2 , studied property of CNNs called

equivariance (Cohen & Welling, 2016a; Worrall et al., 2017).

With respect to rotations, however, the situation is more complicated: if we rotate the input image

by, e.g., 90 degrees, not only will the part of the image that fell in the receptive field of a particular

neuron ni,j move to the receptive field of a different neuron nj,-i, but the orientation of the receptive field will also change (Figure 4). Consequently, features which were, for example, previously picked

up

by

horizontal

filters

will

now

be

picked

up

by

vertical

filters.

Therefore,

in

general,

f



 j,-i

= fi,j .

It can be shown that one cannot construct a CNN for images that behaves in a quasi-invariant way

with respect to both translations and rotations unless every filter is directionless.

It is, however, possible to construct a CNN in which the activations transform in a predictable and re-

versible

way,

in

particular,

f

 j,-i

=

R(fi,j )

for

some

fixed

invertible

function

R.

This

phenomenon

is called steerability, and has a significant literature in both classical signal processing (Freeman &

Adelson, 1991; Simoncelli et al., 1992; Perona, 1995; Teo & Hel-Or, 1998; Manduchi et al., 1998)

and the neural networks field (Cohen & Welling, 2016b).

6

Under review as a conference paper at ICLR 2018
1
32
456
1
32
4 57 6
Figure 5: Top left: At level  = 1 n3 aggregates information from {n4, n5} and n2 aggregates information {n5, n6}. At  = 2, n1 collects this summary information from n3 and n2. Bottom left: This graph is not isomorphic to the top one, but the activations of n3 and n2 at  = 1 will be identical. Therefore, at  = 2, n1 will get the same inputs from its neighbors, irrespective of whether or not n5 and n7 are the same node or not. Right: Aggregation at different levels. For keeping the figure legible only the neighborhood around one node in higher levels is marked. The overlap mentioned in the text can be seen in the first level
The situation regarding compositional networks is similar. The comp-net and message passing architectures that we have examined so far, by virtue of the aggregation function being symmetric in its arguments, are all quasi-invariant (with respect to permutations) in the following sense. Definition 5. Let G be a compound object of n parts and G an equivalent object in which the atoms have been permuted by some permutation . Let N be a comp-net for G based on an invariant composition scheme, and N  be the corresponding network for G. We say that N is quasi-invariant if for any ni  N , letting nj be the corresponding node in N , fi = fj for any   Sn Quasi-invariance in comp-nets is equivalent to the assertion that the activation fi at any given node must only depend on Pi = {ej1 , . . . , ejk } as a set, and not on the internal ordering of the atoms ej1 , . . . , ejk making up the receptive field. At first sight this seems desirable, since it is exactly what we expect from the overall representation (G). On closer examination, however, we realize that this property is potentially problematic, since it means that ni has lost all information about which vertex in its receptive field has contributed what to the aggregate information fi. In the CNN analogy, we can say that we have lost information about the orientation of the receptive field. In particular, if further upstream, fi is combined with some other feature vector fj from a node with an overlapping receptive field, the aggregation process has no way of taking into account which parts of the information in fi and fj come from shared vertices and which parts do not (Figure 5). The solution is to upgrade the Pi receptive fields to be ordered sets, and explicitly establish how fi co-varies with the internal ordering of the receptive fields. To emphasize that henceforth the Pi sets are ordered, we will use parentheses rather than braces to denote their content. Definition 6. Let G, M, N and N  be as in Definition 5. Let ni be any node of N and nj the corresponding node of N . Assume that Pi = (ep1 , . . . , epm ) while Pj = (eq1 , . . . , eqm ), and let   Sm be the permutation that aligns the orderings of the two receptive fields, i.e., for which eq(a) = epa . We say that N is covariant to permutations if for any , there is a corresponding function R such that fj = R(fi).
4.1 FIRST ORDER COVARIANT COMP-NETS
The form of covariance prescribed by Definition 6 is very general. To make it more specific, in line with the classical literature on steerable representations, we make the assumption that the
7

Under review as a conference paper at ICLR 2018

{f  R(f )}Sm maps are linear, and by abuse of notion, from now on simply treat them as matrices (with R(f ) = Rf ). The linearity assumption automatically implies that {R}Sm is a representation of Sm in the group theoretic sense of the word (for the definition of group representations, see the Appendix)4.

Proposition 3. If for any   Sm, the f  R(f ) map appearing in Definition 6 is linear, then the corresponding {R}Sm matrices form a representation of Sm.

The representation theory of symmetric groups is a rich subject that goes beyond the scope of the

present paper (Sagan, 2001). However, there is one particular representation of Sm that is likely familiar even to non-algebraists, the so-called defining representation given by the P  Rn×n

permutation matrices

{

[P]i,j =

1 if (j) = i 0 otherwise.

It is easy to verify that P21 = P2 P1 for any 1,2  Sm, so {P}Sm is indeed a representation of Sm. If the transformation rules of the fi activations in a given comp-net are dictated by this representation, then each fi must necessarily be a |Pi| dimensional vector, and intuitively each component of f i carries information related to one specific atom in the receptive field, or the interaction
of that specific atom with all the others. We call this case first order permutation covariance.

Definition 7. We say that ni is a first order covariant node in a comp-net if under the permutation of its receptive field Pi by any   S|Pi|, its activation trasforms as fi  Pfi.

4.2 SECOND ORDER COVARIANT COMP-NETS

It is easy to verify that given any representation Rg of a group G, the matrices {Rg  Rg}gG also furnish a representation of G. Thus, one step up in the hierarchy from P­covariant comp-nets are P  P­covariant comp-nets, where the fi feature vectors are now |Pi|2 dimensional vectors that transform under permutations of the internal ordering by  as fi  (P  P)fi.
If we reshape fi into a matrix Fi  R|Pi|×|Pi|, then the action
Fi  P Fi P
is equivalent to P  P acting on fi. In the following, we will prefer this more intuitive matrix view, since it clearly expresses that feature vectors that transform this way express relationships between the different constituents of the receptive field. Note, in particular, that if we let APi be the restriction of the adjacency matrix to Pi (i.e., if Pi = (ep1 , . . . , epm ) then [APi ]a,b = Apa,pb ), then APi transforms exactly as Fi does in the equation above.
Definition 8. We say that ni is a second order covariant node in a comp-net if under the permutation of its receptive field Pi by any   S|Pi|, its activation trasforms as Fi  P Fi P.

4.3 THIRD AND HIGHER ORDER COVARIANT COMP-NETS

Taking the pattern further lets us consider third, fourth, and general, k'th order nodes in our comp-

net, in which the activations are k'th order tensors, transforming under permutations as

Fi  Fi

where

 

[Fi]j1,...,jk =

. . . [P]j1,j1 [P]j2,j2 . . . [P]jk,jk [Fi]j1 ,...,jk ,

j1 j2

jk

In the more compact, so called Einstein notation5,

[Fi]j1,...,jk

=

[P

] j1
j1

[P

] j2
j2

...

[P

] jk
jk

[Fi]j1 ,...,jk .

(1)

In general, we will call any quantity which transforms according to this equation a k'th order Ptensor. Note that this notion of tensors is distinct from the common usage of the term in neural

4 This notion of representation must not be confused with the neural networks sense of representations of objects, as in "fi is a representation of Pi"
5The Einstein convention is that if, in a given tensor expression the same index appears twice, once "up-
stairs" and once "downstairs", then it is summed over. For example, the matrix/vector product y = Ax would be written yi = Aij xj

8

Under review as a conference paper at ICLR 2018

networks, and more similar to how the word is used in Physics, because it not only implies that Fi is a quanity representable by an m × m × . . . × m array of numbers, but also expresses that Fi transforms in a specific way.

Since scalars, vectors and matrices can be considered as 0th, 1st and 2nd order tensors, respectively, the following definition encompasses Definitions 5, 7 and 8 as special cases (with quasi-invariance being equivalent to zeroth order equivariance). To unify notation and terminology, regardless of the dimensionality, in the following we will always talk about feature tensors rather than feature vectors and denote them Fi rather than our earlier fi.

Definition 9. We say that ni is a k'th order covariant node in a comp-net if the corresponding

activation Fi is a or the activation

k'th order P ­tensor, i.e., it is a sequence of c separate

transforms P ­tensors

uFni(d1e),r.p.e.r,mFui(tca)ticoonrsreosfpPoni daicncgortodincgditsoti(n1c)t,

channels.

5 TENSOR AGGREGATION RULES

The previous sections prescribed how activations must transform in comp-nets of different orders, but did not explain how this can be assurred, and what it entails for the  aggregation functions.
Fortunately, tensor arithmetic provides a compact framework for deriving the general form of these operations. Recall the four basic operations that can be applied to tensors6:
1. The tensor product of A  T k with B  T p yields a tensor C = A  B  T p+k where

C = A B .i1,i2,...,ik+p

i1,i2,...,ik ik+1,ik+2,...,ik+p

2. The elementwise product of A  T k with B  T p along dimensions (a1, a2, . . . , ap) yields a tensor C = A  (a1,...,ap)B  T k where

Ci1,i2,...,ik = Ai1,i2,...,ik B .ia1 ,ia2 ,...,iap

3. The projection (summation) of A  T k along dimensions {a1, a2, . . . , ap} yields a tensor C = Aa1,...,ap  Tp-k with  

Ci1,i2,...,ik =

. . . Ai1,i2,...,ik ,

ia1 ia2

iap

where we assume that ia1 , . . . , iap have been removed from amongst the indices of C. 4. The contraction of A  T k along the pair of dimensions {a, b} (assuming a < b) yields a k - 2

order tensor



Ci1,i2,...,ik =

A ,i1,...,ia-1,j,ia+i,...,ib-1,j,ib+1,...,k

j

where again we assume that ia and ib have been removed from amongst the indices of C. Using Einstein notation this can be written much more compactly as

Ci1,i2,...,ik = Ai1,i2,...,ik ia,ib ,

where ia,ib is the diagonal tensor with i,j = 1 if i = j and 0 otherwise. In a somewhat

unorthodox fashion, {{a11, . . . , ap11 }, {a21,

we ...

also , ap22

generalize }, . . . , {a1q,

c.o. n. ,traapqcqti}o}nsastoth(eco(km-binatjiopnjs)

of) larger sets order tensor

of

indices

C... = Ai1,i2,...,ik a11,...,a1p1 a12,...,ap22 . . . aq1,...,apqq .

Note that this subsumes projections, since it allows us to write Aa1,...,ap in the slightly unusual looking form
Aa1,...,ap = Ai1,i2,...,ik ia1 ia2 . . . iak .
The following proposition shows that, remarkably, all of the above operations (as well as taking linear conbinations) preserve the way that P ­tensors behave under permutations and thus they can be freely "mix and matched" within .

6 Here and in the following T k will denote the class of k'th order tensors (k dimensional tensors), regardless of their transformation properties.

9

Under review as a conference paper at ICLR 2018

Proposition 4. Assume that A and B are k'th and p'th order P ­tensors, respectively. Then

1. A  B is a k + p'th order P ­tensor.

2. A  (a1,...,ap)B is a k'th order P ­tensor.

3. 4.

Aa1,...,ap Ai1 ,i2 ,...,ik

is a k - p'th a11,...,ap11 . . .

order P ­tensor. aq1,...,apqq is a k

-


j

pj 'th

order

P

­tensor.



In addition, if A1, . . . , Au are P ­tensors and 1, . . . , u are scalars, then j jAj is a P ­tensor.

The more challenging part of constructing the aggregation scheme for comp-nets is establishing how to relate P ­tensors at different nodes. The following two propositions answer this question.

Proposition 5. Assume that node na is a descendant of node nb in a comp-net N , Pa = (ep1 , . . . , epm ) and Pb = (eq1 , . . . , eqm ) are the corresponding ordered receptive fields, and ab  Rm×m is an indicator matrix defined

{

ai,j b =

1 if qi = pj 0 otherwise.

Assume that F is a k'th order P ­tensor with respect to permutations of (ep1 , . . . , epm ). Then, dropping the ab superscript for clarity,

Fi1,...,ik = ij11 ij22 . . . ijkk Fj1,...,jk

(2)

is a k'th order P ­tensor with respect to permutations of (eq1 , . . . , eqm ) .
Equation 2 tells us that when node nb aggregates P ­tensors from its children, it first has to "promote" them to being P ­tensors with respect to the contents of its own receptive field by contracting along each of their dimensions with the appropriate ab matrix. This is a critical element in comp-nets to guarantee covariance.

Proposition 6. Let nc1 , . corresponding k'th order

. . , ncs tensor

be the children activations Fc1 ,

of nt+1 in a . . . , Fcs . Let

message

passing

type

comp-net

with

[Fcu ]i1,...,ik = [cut]i1j1 [cut]i2j2 . . . [cut]ikjk [Fcu ]j1,...,jk

be the promotions of these activations to P ­tensors of nt+1. Assume that Pt = (ep1 , . . . , epm ). Now let F be a k + 1'th order object in which the j'th slice is Fpj if pj if npj is one of the children, i.e.,

F i1,...,ik,j = [Fpj ]i1,...,ik ,
and zero otherwise. Then F is a k + 1 order P ­tensor of nt.
Finally, as already mentioned, the restriction of the adjacency matrix to Pi is a second order P ­ tensor, which gives an easy way of explicitly adding topological information to the activation.

Proposition 7. If Fi is a rank k P ­tensor at node ni, and APi is the restriction of the adjacency matrix to Pi as defined in Section 4.2, then F  APi is a k + 2'th order P ­tensor.

5.1 THE GENERAL AGGREGATION FUNCTION AND ITS SPECIAL CASES

Combining all the above results, assuming that node nt has children nc1 , . . . , ncs , we arrive at following general algorithm for the aggregation rule t:

10

Under review as a conference paper at ICLR 2018

1. Collect all the k'th order activations Fc1 , . . . , Fcs of the children. 2. Promote each activation to Fc1 , . . . , Fcs (Proposition 5). 3. Stack Fc1 , . . . , Fcs together into a k + 1 order tensor T (Proposition 6). 4. Optionally form the tensor product of T with APt to get a k+3 order tensor H (otherwise
just set H = T ) (Proposition 7).

5. Contract H along some number of combinations of dimensions to get s separate lower order tensors Q1, . . . , Qs (Proposition 4).
6. Mix Q1, . . . , Qs with a matrix W  Rs×s and apply a nonlinearity  to get the final activation of the neuron, which consists of the s output tensors

[ s

]

F (i) = 

Wi,j Qj + bi

i = 1, 2, . . . s,

j=1

where the bi scalars are bias terms.

A few remarks are in order about this general scheme:

1. Since Fc1 , . . . , Fcs are stacked into a larger tensor and then possibly also tensor multiplied by APt , the general tendency is for the tensor order to increase at every node, and the corresponding storage requirements to increase exponentially. The purpose of the contractions in Step 5

is to counteract this tendency, and pull the order of the tensors back to some small number,

typically 1, 2 or 3.

2. However, since contractions can be done in multiple ways, the number of channels will increase.

When the number of input channels is small, this is reasonable, since otherwise the number of

learnable weights in the algorithm would be too small. However, if unchecked, this can also

become problematic. Fortunately, mixing the channels by W on Step 6 gives an opportunity to stabilize the number of channels at some value s.

3. In the pseudocode above, for simplicity, the number of input channels is one and the number of output channels is s. More realistically, the inputs would also have multiple channels (say,

s0) which would be propagated through the algorithm independently up to the mixing stage, making W an s × s × s0 dimension tensor (not in the P ­tensor sense!).

4. The conventional part of the entire algorithm is Step 6, and the only learnable parameters are

the entries of the W matrix (tensor) and the bi bias terms. These parameters are shared by all

nodes in the network and learned in the usual way, by stochastic gradient descent.

5. Our scheme could be elaborated further while maintaining permutation covariance by, for ex-

ample taking the tensor product of T with itself, or by introducing APt in a different way. However, the way that Fc1 , . . . , Fcs and APt are combined by tensor products is already much more general and expressive than conventional message passing networks. 6. There are many design choices including the choice of order, the choice of contractions, and c.

However, the overall structure of Steps 1­5 is fully dictated by the covariance constraint on the

network.

7.

The final output the root node ni

of the network (G) must produce a tuple

= of

Fr must be zeroth order

permutation invariant. tensors (scalars) (Fr(1),

That means that . . . , Fr(c)). This

is similar to how many other graph representation algorithms compute (G) by summing the

activations at level L or creating histogram features.

We consider a few special cases to explain how tensor aggregation relates to more conventional message passing rules.

5.1.1 ZEROTH ORDER TENSOR AGGREGATION

Constraining both the input tensors Fc1 , . . . , Fcs and the outputs to be zeroth order tensors, i.e.,

scalars, no need

and for

pfororemgootiinognsm, ualntidplTicaistijounstbtyheAvePcttogrre(aFtcly1 ,s.i.m.p, lFificse)s.

the form There is

of . only

In this case there is one way to contract

a vector into a scalar, and that is to sum its elements. Therefore, in this case, the entire aggregation

algorithm reduces to the simple formula

( c

)

Fi =  w Fcu + b .

u=1

11

Under review as a conference paper at ICLR 2018

For a neural network this is too simplistic. However, it's interesting to note that the Weisfeiler­ Lehmann isomorphism test essentially builds on just this formula, with a specific choice of  (Read & Corneil, 1977). If we allow more channels in the inputs and the outputs, W becomes a matrix, and we recover the simplest form of neural message passing algorithms (Duvenaud et al., 2015).

5.1.2 FIRST ORDER TENSOR AGGREGATION

In first order tensor aggregation, assuming that |Pi| = m, Fc1 , . . . , Fcs are m dimensional column vectors, and T is an m × m matrix consisting of Fc1 , . . . , Fcs stacked columnwise. There are
two ways of contracting (in our generalized sense) a matrix into a vector: by summing over its

rows, or summing over its columns. The second of these choices leads us back to summing over all

contributions from the children, while the first is more interseting because it corresponds to summing

Fc1 , . . . , Fcs as vectors individually. In summary, we get an aggregation function giving rise to two

channels of the form

[ Fi(1) = 

[1m T ,

T

] 1m

[

w11 w12

]] + b1m ,

[ Fi(2) = 

[1m T,

][ T 1m

w21 w22

]] + b1m ,

where 1m denotes the m dimensional all ones vector. Thus, in this layer W  R2×2. However, unless constrained by c, in each subsequent layer the number of channels doubles further, so the W matrix will be R4×4, R8×8, etc..

5.1.3 SECOND ORDER TENSOR AGGREGATION WITHOUT THE ADJACENCY MATRIX

In second order tensor aggregation, T is a third order P ­tensor, which can be contracted back to

second order in three different ways, by projecting it along each of its dimensions. Therefore the

outputs will be the three matrices

F (i)

=

(  wi,1T 1

+

wi,2T 2

+

wi,3T 3

+

) bi1m×m

i  {1, 2, 3},

and the weight matrix is W  R3×3.

5.1.4 SECOND ORDER TENSOR AGGREGATION WITH THE ADJACENCY MATRIX

The first nontrivial tensor contraction case occurs when Fc1 , . . . , Fcs are second order tensors, and we multiply with APt , since in that case T is 5th order, and can be contracted down to second order in a total of 50 different ways:

1.

The its 5

"1+1+1" case contracts dimensions. This alone

T in the form can be done in

T(i531),i=2,i13,0i4d,ii5ffeiare1 ntiaw2 ayias37,

i.e.,

projects

it

along

3

of

2.

The "1+2" case contracts T in mension and contracts it along

the two

footrhmersT.iT1,hi2i,si3c,ai4n,ib5 eida1oniea2in,ia33(,35i).e=.,

projects it 30 ways.

along

one

di-

3. T(53h)e="31"0cdaisfefeirsenatswinagylse.3-fold contraction Ti1,i2,i3,i4,i5 ia1 ,ia2 ,ia3 , which again can be done in

Clearly, maintaining 50 channels in a message passing architecture is excessive, so in practice it is

reasonable to set c  10, making W  R10×50.

6 EXPERIMENTS
To test the effectiveness of our proposed framework for constructing covariant compositional neural networks(CCN), we implemented and tested a second order CCN, as described in section 5, on the Harvard Clean Energy Project dataset(Hachmann et al., 2011), and standard graph kernel datasets: MUTAG(Debnat et al., 1991), PTC(H.Toivonen et al., 2003), NCI1, and NCI109(Wale et al., 2008).
7For simplicity, we ignore the fact that symmetries, such as the symmetry of APt , might reduce the number of distinct projections somewhat.

12

Under review as a conference paper at ICLR 2018

w1
|-1 (w1 )|

|-1 (w3 )|

w3

v

(
stack
| (v)|

| (v)|

w2

)
 A (v) | (v)|

|-1 (w2 )|

Tensor to be contracted
Figure 6: The activations of each neighbor are stacked into a tensor T which is tensor multiplied by the restriction of the adjacency matrix, and then contracted in different ways.

6.1 HARVARD CLEAN ENERGY PROJECT
The Harvard Clean Energy Project dataset consists of 2.3 million molecules of candidate structures to potentially be used as organic photovoltaics. Each of these molecules has an associated real value Power Conversion Efficiency(PCE) target which we regressed onto. We randomly sampled 50,000 graphs from the HCEP dataset for this task. From each graph's SMILES string, we extract its adjacency matrix and the atoms of each node to be used as node labels.
6.2 QM9
QM9 is a dataset of 133k small organic molecules. These compounds correspond to all species with up to nine heavy atoms(Carbon, Hydrogen, Oxygen, Nitrogen, and Fluorine) out of the GDB17 universe of organic molecules. Each molecule has 13 target properties to predict, giving rise to 13 separate regression tasks. The dataset also contains spatial information of the atom configurations in each molecule, however, for our experiments we only used adjacency matrices and atom node labels. For our experiments we normalized each target variable to have mean 0 and standard deviation 1.
6.3 GRAPH KERNELS DATASETS
MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic compounds. PTC consists of 344 chemical compounds that have been tested for positive or negative toxicity in lab rats. Finally NCI1 and NCI109 have 4110 and 4127 compounds respectively, each screened for activity against small cell lung cancer and ovarian cancer lines. In each of these four datasets, we are given the graphs' adjacency matrices and discrete node labels for each node of each graph. Our prediction task is binary classification.

13

Under review as a conference paper at ICLR 2018
6.4 BASELINE COMPARISONS
We compared our results on the HCEP dataset against the following baselines: Lasso regression, Ridge regression, Random Forests, Gradient Boosted Trees, Optimal Assignment WL-graph kernel (Kriege et al., 2016), Neural Graph Fingerprints(Duvenaud et al., 2015), and Learning Convolutional Neural Networks for Graphs(LCNN)(Niepert et al., 2016).
For the first four baseline methods, we created simple feature vectors from each molecule: the number of bonds of each type(IE: number of H-H bonds, number of C-O bonds, etc) and the number of atoms of each type(IE: number of Carbons, etc). Molecular graph fingerprints uses atom labels of each vertex as base features. For Ridge and Lasso, we cross validated over . For Random Forests and Gradient Boosted Trees, we used 400 trees, and cross validated over max depth, minimum samples for a leaf, minimum samples to split a node, and learning rate(for GBT). In Neural Graph Fingerprint, we used 2 layers and a hidden size of 10. In LCNN, we used a patch size of 10 with two convolutional layers and a dense layer on top as described in their paper.
For the graph kernels datasets, we compare against graph kernel results as reported by Kondor & Pan (2016)(which computed kernel matrices using the Weisfeiler Lehman, Weisfeiler-Edge, Shortest Paths, Graphlets and Multiscale Laplacian Graph kernels and used a c-SVM on top), Neural Graph Fingerprint and Niepert et al. (2016). For Neural Graph Fingerprints comparison, the number of levels was set to 2 and hidden size 10.
For QM9, we compared against the Weisfeiler Lehman graph kernel(with c-SVM on top), neural graph fingerprints, and LCNN. The prescribed settings for NGF and LCNN are as described for the HCEP experiments.
6.5 EXPERIMENTAL SETUP
In all experiments(HCEP, kernel datasets, QM9), we used 80% of the dataset for training, 10% for validation, and evaluated on the remaining 10% test set. For the kernel datasets we performed the experiments on 10 separate training/validation/test stratified splits and averaged the resulting classification accuracies. In all experiments we used stochastic gradient descent with momentum 0.9. Our initial learning rate was set to 0.001 after experimenting on a held out set. The learning rate decayed linearly after each step towards a minimum of 10-6.
Our covariant compositional network(abbreviated CCN 2D for second order tensor message passing in the results tables) we initialized the base features of each vertex with computed Histogram Alignment Kernel Features(Kriege et al., 2016) of up to depth 10 - that is, each vertex receives a feature vector l(v) = concati1=0 1Hi(v) where Hi(v)  Rd, where d is the total number of distinct discrete node labels, denoting the relative frequencies of each label for the set of vertices at distance equal to i from vertex v.
CCN 2D uses 2 levels and doubles the intermediate channel size at each level. We only apply 10 contractions as described in section 5.1.4 instead of the full 50 contractions for computational efficiency.
6.6 DISCUSSION
On the HCEP subsampled dataset, our CCN 2D network outperforms all of the baseline methods by a considerable margin.
For the graph kernels datasets, SVM with the Weisfeiler Lehman kernels still achieve the highest accuracy. Perhaps this poor performance is to be expected since the datasets are so small and neural network approaches usually require in the tens of thousands of training examples at minimum to be effective. Indeed, Neural Graph Fingerprints and LCNN also perform poorly compared to the Weisfeiler Lehman kernels.
In the QM9 experiments, CCN 2D beats the results of the three other baselines. Note that our results show the test mean absolute error. Gilmer et al. (2017) obtained stronger results on QM9, but we cannot properly compare our results with theirs because our experiments only work with the adjacency matrices and atom labels of each node, while theirs includes comprehensive chemical
14

Under review as a conference paper at ICLR 2018

Method

Table 1: HCEP regression results Train MAE Train RMSE

Lasso Ridge Regression Random Forest

0.863 0.849 0.999

1.190 1.164 1.331

Gradient Boosted Tree Weisfeiler-Lehman Graph Kernel
Neural Graph Fingerprint Learning Convolution Neural Network

0.676 0.805 0.848 0.704

0.939 1.111 1.187 0.972

CCN 2D

0.562

0.773

Test MAE
0.867 0.854 1.004 0.704 0.805 0.851 0.718
0.570

Test RMSE
1.437 1.376 1.799 1.005 1.096 1.177 0.973
0.773

Table 2: Kernel Datasets Classification results(Accuracy +/- std)

Method

MUTAG

PTC

NCI1

NCI109

WL WL-Edge
SP Graphlet
p-RW MLG PSLR k = 10 (Niepert et al) Neural Graph Fingerprint

84.50 ± 2.16 82.94 ± 2.33 85.50 ± 2.50 82.44 ± 1.29 80.33 ± 1.35 87.94 ± 1.61 87.37 ± 7.88 89.00 ± 7.00

59.97 ± 1.60 60.18 ± 2.19 59.53 ± 1.71 55.88 ± 0.31 59.85 ± 0.95 63.26 ± 1.48 58.57 ± 5.46 57.85 ± 3.36

84.76 ± 0.32 84.65 ± 0.25 73.61 ± 0.36 62.40 ± 0.27
TIMED OUT 81.75 ± 0.24 70.00 ± 1.98 62.21 ± 4.72

85.12 ± 0.29 85.32 ± 0.34 73.23 ± 0.26 62.35 ± 0.28
TIMED OUT 81.31 ± 0.22
N/A 56.11 ± 4.31

CCN 2D

91.64 ± 7.24 70.62 ± 7.04 71.02 ± 4.26 71.42 ± 2.72

features that better inform the target quantum properties. Additionally, we did not have time to run our experiments on QM9 to full convergence before the submission deadline.
7 CONCLUSION
We have presented a general framework for producing covariant graph neural networks which encompasses other message passing approaches as special cases, but takes a more general and principled approach to ensuring covariance with respect to permutations. Our results on benchmark datasets demonstrate our framework's effectiveness.

Table 3: QM9 regression results(Test MAE) WLGK NGF LCNN CCN 2D

alpha 0.46 0.43 0.20 Cv 0.59 0.47 0.27 G 0.51 0.46 0.33 gap 0.72 0.67 0.60 H 0.52 0.47 0.34 HOMO 0.64 0.58 0.51 LUMO 0.70 0.65 0.59 mu 0.69 0.63 0.54 omega1 0.72 0.63 0.57 R2 0.55 0.49 0.22 U 0.52 0.47 0.34 U0 0.52 0.47 0.34 ZPVE 0.57 0.51 0.43

0.18 0.23 0.29 0.56 0.29 0.40 0.54 0.49 0.48 0.19 0.30 0.29 0.38

15

Under review as a conference paper at ICLR 2018
REFERENCES
P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pp. 4502­4510, 2016.
Karsten M. Borgwardt and Hans Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of the 5th IEEE International Conference on Data Mining(ICDM) 2005), 27-30 November 2005, Houston, Texas, USA, pp. 74­81, 2005.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. In Proceedings of International Conference on Learning Representations, 2014.
J.-Y. Cai, M. Furer, and N. Immerman. An optimal lower bound on the number of variables for graph identification. Combinatorica, 12:389­410, December 1992.
T. Cohen and M. Welling. Group equivariant convolutional networks. In Proceedings of the International Conference on Machine Learning, pp. 2990­2999, 2016a.
Taco Cohen and Max Welling. Steerable CNNs. arXiv preprint arXiv:1612.08498, 2016b.
A.K. Debnat, R. L. Lopez de Compadre, G. Debnath, A. j. Shusterman, and C. Hansch. Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J Med Chem, 34:786­97, 1991.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, 2016.
D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pp. 2224­2232, 2015.
Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Pictorial structures for object recognition. International Journal of Computer Vision, 61:55­71, 2005.
Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32:541­551, 2010.
Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, and Karsten M. Borgwardt. Scalable kernels for graphs with continuous attributes. In Advances in Neural Information Processing Systemss, 2013.
M. Fischler and R. Elschlager. The representation and matching of pictorial structures. IEEE Transactions on Computer, C-22:67­92, 1973.
William T. Freeman and Edward H. Adelson. The design and use of steerable filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13:891­906, September 1991.
T. Ga¨rtner. Exponential and geometric kernels for graphs. In NIPS*02 workshop on unreal data, volume Principles of modeling nonvectorial data, 2002a.
T. Ga¨rtner. Exponential and geometric kernels for graph. In In NIPS*02 workshop on unreal data, volume Principles of modeling nonvectorial data, 2002b.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Johannes Hachmann, Roberto Olivares-amaya, Sule Atahan-evrenk, Carlos Amador-bedolla, Roel S. Sanchez-carrera, Aryeh Gold-parker, Leslie Vogt, Anna M. Brockway, and Alan Aspuruguzik. The harvard clean energy project: Large-scale computational screening and design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2011.
16

Under review as a conference paper at ICLR 2018
M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, June 2015.
H.Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma. Statistical evaluation of the predictive toxicology challenge. Bioinformatics, pp. 1183­1193, 2003.
S. Kearns, K. McCloskey, M. Brendl, V. Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. Journal of Computer-Aided Molecular Design, 30:595­608, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of International Conference on Learning Representations, 2017.
R. Kondor and K. M. Borgwardt. The skew spectrum of graphs. In Proceedings of the International Conference on Machine Learning, pp. 496­503, 2008a.
R. Kondor and H. Pan. The multiscale laplacian graph kernel. In Neural Information Processing Systems, pp. 2982­2990, 2016.
Risi Kondor and Karsten Borgwardt. The skew spectrum of graphs. In Proceedings of the International Conference on Machine Learning (ICML), pp. 496­503. ACM, 2008b.
Nils M. Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal assignment kernels and applications to graph classification. Advances in Neural Information Processing Systems 29, 2016.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, pp. 2278­2324, 1998.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.
Roberto Manduchi, Pietro Perona, and Doug Shy. Efficient deformable filter banks. IEEE Transactions on Signal Processing, 46:1168­1173, April 1998.
Marion Neumann, Roman Garnett, Christian Baukhage, and Kristian Kersting. Propagation kernels: Efficient graph kernels from propagated information. Machine Learning, 102:209­245, 2016a.
Marion Neumann, Roman Garnett, Christian Baukhage, and Kristian Kersting. Propagation kernels: efficient graph kernels from propagated information. In Machine Learning, 2016b.
M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of the International Conference on Machine Learning, 2016.
Y. Ohta, T. Kanade, and T. Sakai. An analysis system for scenes containing objects with substructures. In Proceedings of 4th International Joint Conference on Pattern Recognition, pp. 752­754, 1978.
Pietro Perona. Deformable kernels for early vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17:488­499, May 1995.
R. C. Read and D. G. Corneil. The graph isomorphism disease. Journal of Graph Theory, 1:339­ 363, 1977.
Bruce E. Sagan. The Symmetric Group. Graduate Texts in Mathematics. Springer, 2001.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20:61­80, 2009.
Kristof T. Schu¨tt, Farhad Arbabzadah, Stefan Chmiela, Klaus R. Mu¨ller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature communications, 2017.
17

Under review as a conference paper at ICLR 2018
Nino Shervashidze, S. V. N. Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten M. Borgwardt. Efficient graphlet kernels for large graph comparison. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS, pp. 488­495, 2009.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwan, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12:2539­ 2561, 2011a.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research(JMLR), 12: 2539­2561, November 2011b.
Eero P. Simoncelli, William T. Freeman, Edward H. Adelson, and David J. Heeger. Shiftable multiscale transforms. IEEE Transactions on Information Theory, 38:587­607, March 1992.
Patrick C. Teo and Yacov Hel-Or. Lie generators for computing steerable functions. Pattern Recognition Letters, 16:7­17, October 1998.
Z. W. Tu, X. R. Chen, A. L. Yuille, and S. C. Zhu. Image parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision, 63:113­140, 2005.
S. V. N. Vishwanathan, Karsten Borgwardt, Risi Kondor, and Nicol Schraudolph. On graph kernels. Journal of Machine Learning Research (JMLR), 11, 2010a.
S. V. N. Vishwanathan, N. N. Schraudolf, Risi Kondor, and Karsten M. Bogwardt. Graph kernels. Journal of Machine Learning Research, 11:1201­1242, 2010b.
N. Wale, I. A. Watson, and G. Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems, pp. 347­375, 2008.
Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic networks: Deep translation and rotation equivariance. Technical report, 2017.
Song-Chun Zhu and David Mumford. A stochastic grammar of images. Foundations and Trends in Computer Graphics and Vision, 2:259­362, 2006.
A MATHEMATICAL BACKGROUND
Groups. A group is a set G endowed with an operation G × G  G (usually denoted multiplicatively) obeying the following axioms: G1. for any u, v  G, uv  G (closure); G2. for any u, v, w  G, u(vw) = (uv)w (associativity); G3. there is a unique e  G, called the identity of G, such that eu = ue = u for any u  G; G4. for any u  G, there is a corresponding element u-1  G called the inverse of u, such that
u u-1 = u-1u = e. We do not require that the group operation be commutative, i.e., in general, uv = vu. Groups can be finite or infinite, countable or uncountable, compact or non-compact. While most of the results in this paper would generalize to any compact group, the keep the exposition as simple as possible, throughout we assume that G is finite or countably infinite. As usual, |G| will denote the size (cardinality) of G, sometimes also called the order of the group.
Representations. A (finite dimensional) representation of a group G over a field F is a matrixvalued function  : G  Fd×d such that (x) (y) = (xy) for any x, y  G. In this paper, unless stated otherwise, we always assume that F = C, since this is the case in which the general theory takes on the simplest form. However, in case of certain groups, such as spatial translation and rotations groups and the symmetric group Sn we will be able to restrict ourselves to considering representations over R.
18

