Under review as a conference paper at ICLR 2018
SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE
Anonymous authors Paper under double-blind review
ABSTRACT
Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.
1 INTRODUCTION
Deep reinforcement learning (RL) provides a general framework for solving challenging goaloriented sequential decision-making problems, It has recently achieved remarkable successes in advancing the frontier of AI technologies (Silver et al., 2017; Mnih et al., 2013; Silver et al., 2016; Schulman et al., 2017). Policy gradient (PG) is one of the most successful model-free RL approaches that has been widely applied to high dimensional continuous control, vision-based navigation and video games (Schulman et al., 2016; Kakade, 2002; Schulman et al., 2015; Mnih et al., 2016).
Despite these successes, a key problem of policy gradient methods is that the gradient estimates often have high variance. A naive solution to fix this issue would be requiring a large amount of rollout samples to obtain a reliable gradient estimation in each step. Regardless of the cost of generating large samples, in many practical applications like developing driverless cars, it may not even be possible to generate as many samples as we want. A variety of variance reduction techniques have been proposed for policy gradient methods (See e.g. Weaver & Tao 2001, Greensmith et al. 2004, Schulman et al. 2016 and Asadi et al. 2017).
In this work, we focus on the control variate method, one of the most widely used variance reduction techniques in policy gradient and variational inference. The idea of the control variate method is to subtract a Monte Carlo gradient estimator by a baseline function that analytically has zero expectation, and hence introduces no additional bias. The resulted estimator may achieve much lower variance if the baseline function is properly chosen such that it cancels out the variance of the original gradient estimator. Different control variates yield different variance reduction methods. Let us mention of several examples here. In REINFORCE (Williams, 1992), a constant baseline function is chosen as control variate. Advantage actor-critic (A2C) (Sutton & Barto, 1998; Mnih et al., 2016) considers a state-dependent baseline function as control variate. This function is often set to be an estimated value function V (s). More recently, in Q-prop (Gu et al., 2016), a more general baseline function is proposed. It linearly depends on actions, and shows promising results on several challenging tasks. It is natural to expect even more flexible baseline functions which can arbitrarily depend on both states and actions to yield powerful variance reduction. Without a principled framework or guideline, however, constructing such a baseline function turns out to be fairly challenging.
To tackle this problem, we sort to the so-called Stein's identity (Stein, 1986) which defines a broad class of identities that are sufficient to fully characterize the distribution under consideration (e.g., Liu et al., 2016; Chwialkowski et al., 2016). By applying the Stein's identity and also drawing connection with the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014), we construct the Stein control variate, a new control variate method that allows us to use arbitrary
1

Under review as a conference paper at ICLR 2018

baseline functions that depend on both actions and states. Our approach tremendously extends the existing control variates used in REINFORCE, A2C and Q-prop.
We evaluate our method on a variety of reinforcement learning tasks. Our experiments show that our Stein control variate can significantly reduce the variance of gradient estimation with more flexible and nonlinear baseline functions. When combined with different policy optimization methods, including both proximal policy optimization (PPO) (Schulman et al., 2017; Heess et al., 2017) and trust region policy optimization (Schulman et al., 2015; 2016), it greatly improves the sample efficiency of the entire policy optimization.

2 BACKGROUND

In this section, we present basic materials on policy gradient and the control variate method for variance reduction. The readers who are familiar with these topics can safely skip this section.

2.1 REINFORCEMENT LEARNING AND POLICY GRADIENT

Reinforcement learning considers the problem of finding an optimal policy for an agent which in-
teracts with an uncertain environment and collects reward per action. The goal of the agent is to
maximize the long-term cumulative reward. Formally, this problem can be formulated as a Markov decision process over the environment states s  S and agent actions a  A, under an unknown environmental dynamic defined by a transition probability T (s |s, a) and a reward signal r(s, a) immediately following the action a performed at state s. The agent's action a is selected by a conditional probability distribution (a|s) called policy. In policy gradient methods, we consider a set of candidate policies (a|s) parameterized by  and obtain the optimal policy by maximizing the expected cumulative reward or return

J () = Es,a(a|s) [r(s, a)] ,

where (s) =

 t=1

t-1Pr(st

=

s)

is

the

normalized

discounted

state

visitation

distribution

with

discount factor   [0, 1). To simplify the notation, we denote Es,a(a|s)[·] by simply E[·] in

the rest of paper. According to the policy gradient theorem (Sutton & Barto, 1998), the gradient of

J() can be written as

J () = E [ log (a|s)Q(s, a)] ,

(1)

where Q(s, a) = E

 t=1

t-1r(st, at)|s1

=

s, a1

=

a

denotes the expected return under pol-

icy  starting from state s and action a. Different policy gradient methods are based on different

stochastic estimation of the expected gradient in Eq (1). Perhaps the most straightforward way is to simulate the environment with the current policy  to obtain a trajectory {(st, at, rt)}nt=1 and estimate J() using the Monte Carlo estimation:

^ J ()

=

1 n

n

t-1 log (at|st)Q^(st, at),

t=1

(2)

where Q^(st, at) is an empirical estimate of Q(st, at), e.g., Q^(st, at) = jt j-trj. Unfortunately, this naive method often introduces large variance in gradient estimation. It is almost always the case that we need to use control variates method for variance reduction, which we will introduce in the following. It has been found that biased estimators help improve the performance, e.g., by using biased estimators of Q or dropping the t-1 term in Eq (2). In this work, we are interested in improving the performance without introducing additional biases.

2.2 CONTROL VARIATE
The control variates method is one of the most widely used variance reduction techniques in policy gradient. Suppose that we want to estimate the expectation µ = E [g(s, a)] with Monte Carlo samples (st, at)tn=1 drawn from some distribution , which is assumed to have a large variance var (g). The control variate is a function f (s, a) with a special structure such that it has a zero expectation under 
E [f (s, a)] = 0.

2

Under review as a conference paper at ICLR 2018

With f , we can have an alternative unbiased estimator

1n

µ^ = n

(g(st, at) - f (st, at)) ,

t=1

where the variance of this estimator is var (g - f )/n, instead of var (g)/n for the Monte Carlo estimator. By taking f to be similar to g, e.g. f = g - µ in the ideal case, the variance of g - f can
be significantly reduced, thus resulting in a more reliable estimator.

The key step here is to find an identity that yields a large class of functional f with zero expectation. In most existing policy gradient methods, the following identity is used

E(a|s) [ log (a|s)(s)] = 0, for any function .

(3)

Combining it with the policy gradient theorem, we obtain

^ J ()

=

1 n

n

 log (at|st)

Q^(st, at) - (st)

,

t=1

(4)

Note that we drop the t-1 term in Eq (2) as we do in practice. The introduction of the function 
does not change the expectation but can decrease the variance significantly when it is chosen properly to cancel out the variance of Q(s, a). In REINFORCE,  is set to be a constant (s) = b
baseline, and b is usually set to approximate the average reward, or determined by minimizing var(^ J()) empirically. In advantage actor-critic (A2C), (s) is set to be an estimator of the value function V (s) = E(a|s)[Q(s, a)], so that Q^(s, a) - (s) is an estimator of the advantage function. For notational consistency, we call  the baseline function and f (s, a) =  log (a|s)(s) the corresponding control variate.

Although REINFORCE and A2C have been widely used, their applicability is limited by the possible choice of . Ideally, we want to set  to equal Q(s, a) up to a constant to reduce the variance of ^ J() to close to zero. However, this is impossible for REINFORCE or A2C because (s) only depends on state s but not action a by its construction. Our goal is to develop a more general control
variate that yields much smaller variance of gradient estimation than the one in Eq (4).

3 POLICY GRADIENT WITH STEIN CONTROL VARIATE

In this section, we present our Stein control variate for policy gradient methods. In Section 3.1 we start by introducing the Stein's identity, then develop a variant that yields a new control variate for policy gradient, and finally discuss its connection to the reparameterization trick and the Qprop method. We provide approaches to estimate the optimal baseline functions in Section 3.2, and discuss the special case of the Stein control variate for Gaussian policies in Section 3.3. We apply our control variate to proximal policy optimization (PPO) in Section 3.4.

3.1 STEIN CONTROL VARIATE

Given a policy (a|s), the Stein's identity w.r.t  is

E(a|s) [a log (a|s)(s, a) + a(s, a) ] = 0, s,

(5)

which holds for any real-valued function (s, a) with some proper conditions. To see this, note the left hand side of Eq (5) is equivalent to a ((a|s)(s, a)) da, which equals zero if (a|s)(s, a) equals zero on the boundary of the integral domain, or decay sufficiently fast (e.g., exponentially)
when the integral domain is unbounded.

The power of the Stein's identity lies in the fact that it defines an infinite set of identities, indexed by arbitrary function (s, a), which is sufficient to uniquely identify a distribution as shown in the work of Stein's method for proving central limit theorems (Stein, 1986; Barbour & Chen, 2005), goodness-of-fit test (Chwialkowski et al., 2016; Liu et al., 2016), and approximate inference (Liu & Wang, 2016). Oates & Girolami (2016) has applied the Stein's identity as a control variate for general Monte Carlo estimation, which is shown to yield a zero-variance estimator because the control variate is flexible enough to approximate the function of interest arbitrarily well.

3

Under review as a conference paper at ICLR 2018

Unfortunately, for the particular case of policy gradient, it is not straightforward to directly apply
the Stein's identity (5) as a control variate, since the dimension of the left-hand side of (5) does
not match the dimension of a policy gradient: the gradient in (5) is taken w.r.t. the action a, while
the policy gradient in (1) is taken w.r.t. the parameter . Therefore, we need a general approach to connect a log (a|s) to  log (a|s) in order to apply the Stein's identity as a control variate for policy gradient. We show in the following theorem that this is possible when the policy is reparameterizable in that a  (a|s) can be viewed as generated by a = f(s, ) where  is a random noise drawn from some distribution independently of . With an abuse of notation, we denote by (a, |s) the joint distribution of (a, ) conditioned on s, so that (a|s) = (a|s, )()d, where () denotes the distribution generating  and (a|s, ) = (a - f (s, )) where  is the Delta function.

Theorem 3.1. With the reparameterizable policy defined above, using Stein's identity, we can derive

E(a|s) [ log (a|s)(s, a)] = E() [f(s, )a(s, a)] .

(6)

Proof. See Appendix for the detail proof. To help understand the intuition, we can consider the

Delta

function as

a

Gaussian with

a

small variance

h,

i.e.

(a|s, )



exp(-

a - f (s, )

2 2

/2h),

for which it is easy to show that

 log (a,  | s) = -f(s, ) a log (a,  | s).

(7)

This allows us to convert between the derivative w.r.t. a and w.r.t. , and hence apply Stein's identity in (5).

Stein Control Variate Using Eq (6) as a control variate, we obtain the following general formula of policy gradient:

J() = E [ log (a|s)(Q(s, a) - (s, a)) + f(s, )a(s, a)] ,

(8)

where the choice of  does not introduce bias to the expectation. Given a sample set (st, at, t)tn=1 where at = f(st, t), an estimator of the gradient is:

^ J ()

=

1 n

n

 log (at | st)(Q^(st, at) - (st, at)) + f(st, t)a(st, at) ,

(9)

t=1

This estimator clearly generalizes the control variates used in A2C and REINFORCE. To see this,
let  be action independent, i.e. (s, a) = (s) or even (s, a) = b, in both cases the last term in (9) equals to zero because a = 0. When  is action-dependent, the last term (9) does not vanish in general, and in fact will play an important role for variance reduction as we will illustrate later.

Relation to Q-prop Q-prop is a recently introduced sample-efficient policy gradient method that constructs a general control variate using Taylor expansion. Here we show that Q-prop can be derived from (8) with a special  that depends on the action linearly, so that its gradient w.r.t. a is action-independent, i.e. a(a, s) = a(s). In this case, Eq (8) becomes
J() = E [ log (a|s)(Q(s, a) - (s, a)) + f(s, )a(s)] .

Furthermore, note that E()[f (s, )] = E()[f (s, )] := µ(s), where µ(s) is the expectation of the action conditioned on s. Therefore,

J () = E [ log (a|s) (Q(s, a) - (s, a)) + µ(s)a(s)] ,

which is the identity used in Q-prop to construct their control variate (see Eq 6 in Gu et al. 2016). In Q-prop, the baseline function is constructed empirically by the first-order Taylor expansion as

(s, a) = V^ (s) + aQ^(s, µ(s)), a - µ(s) ,

(10)

where V^ (s) and Q^(s, a) are parametric functions that the value function and Q function under policy . In contrast, our method allows us to use more general and flexible, nonlinear baseline functions  to construct the Stein control variate which is able to decrease the variance more significantly.

4

Under review as a conference paper at ICLR 2018

Relation to the reparameterization trick The identity (6) is closely connected to the reparameterization trick for gradient estimation which has been widely used in variational inference recently (Kingma & Welling, 2013; Rezende et al., 2014). Specifically, let us consider an auxiliary objective function based on function :

Ls() := E(a|s)[(s, a)] = (a|s)(s, a)da.

Then by the log-derivative trick, we can obtain the gradient of this objective function as

Ls() = (a|s)(s, a)da = E(a|s) [ log (a|s)(s, a)] ,

(11)

which is the left-hand side of (6). On the other hand, if a  (a|s) can be parameterized by
a = f(s, ), then Ls() = E()[(s, f(s, ))], leading to the reparameterized gradient in Kingma & Welling (2013):

Ls() = E() [f(s, )a(s, a)] .

(12)

Equation (11) and (12) are equal to each other since both are Ls(). This provides another way to prove the identity in (6).

3.2 CONSTRUCTING THE BASELINE FUNCTIONS FOR STEIN CONTROL VARIATE
We need to develop practical approaches to choose the baseline functions  in order to fully leverage the power of the flexible Stein control variate. In practice, we assume a flexible parametric form w(s, a) with parameter w, e.g. linear functions or neural networks, and hope to optimize w efficiently for variance reduction. Here we introduce two approaches for optimizing w and discuss some practical considerations.

Estimating  by Fitting Q Function Eq (8) provides an interpolation between the log-likelihood

ratio gradient (1) (by taking  = 0) and a reparameterized gradient (12) (by taking (s, a) =

Q(s, a))

J () = E[f (s, )aQ(s, a)].

(13)

It is well known that the reparameterized gradient tends to yield much smaller variance than the

log-likelihood ratio gradient from the variational inference literature (see e.g., Kingma & Welling,

2013; Rezende et al., 2014; Roeder et al., 2017; Tucker et al., 2017). An intuitive way to see this is to

consider the extreme case when the policy is deterministic. In this case the variance of (11) is infinite

because log (a|s) is either infinite or does not exist, while the variance of (12) is zero because  is

deterministic. Because optimal policies often tend to be close to deterministic, the reparameterized

gradient should be favored for smaller variance.

Therefore, one natural approach is to set  to be close to Q function, that is, (s, a) = Q^(s, a) so

that the log-likelihood ratio term is small. Any methods for Q function estimation can be used. In

our experiments, we optimize the parameter w in w(s, a) by

n
min (w(st, at) - Rt)2,
w t=1

(14)

where Rt an estimate of the reward starting from (st, at). It is worth noticing that with deterministic policies, (13) is simplified to the update of deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015; Silver et al., 2014). However, DDPG directly plugs an estimator Q^(s, a) into (13) to

estimate the gradient, which may introduce large bias. In constrast, our formula (8) provides an un-

biased estimator where the log-likelihood ratio terms serve to correct the bias in the reparameterized

gradient.

Estimating  by Minimizing the Variance Another approach for obtaining  is to directly mini-

mize the variance of the gradient estimator. Note that var(^ J()) = E[(^ J())2] - E[^ J()]2. Since E[^ J()] = J() which does not depend on , it is sufficient to minimize the first term.

Specifically, for w(s, a) we optimize w by

n
min

 log (at | st)

Q^(st, at) - w(st, at)

2
+ f (st, t)aw(st, at) .

w2

t=1

(15)

5

Under review as a conference paper at ICLR 2018

In practice, we find that it is difficult to implement this efficiently using the auto-differentiation in the current deep learning platforms because it involves derivatives w.r.t. both  and a. We develop a computational efficient approximation for the special case of Gaussian policy as we will discuss in Section 3.3.

Architectures of  Given the similarity between  and the Q function as we mentioned above, we may decompose  into
w(s, a) = V^ (s) + w(s, a).
The term V^ (s) is parametric function approximation of the value function which we separately estimate in the same way as in A2C, and w is optimized using the method above with fixed V^ (s). Here the function w(s, a) can be viewed as an estimate of the advantage function, whose parameter w is optimized using the two optimization methods introduced above. To see, we rewrite our gradient estimator to be

^ J ()

=

1 n

n

 log (at | st)(A^(st, at) - w(st, at)) + f(st, t)aw(st, at) , (16)

t=1

where A^(st, at) = Q^(st, at) - V^ (st) is an estimator of the advantage function. If we set w(s, a) = 0, then Eq (16) clearly reduces to A2C. We find that separating V^ (s) from w(s, a) works well in practice, because it effectively provides a useful initial estimation of , and allows us
to directly improve the  on top of the value function baseline.

3.3 STEIN CONTROL VARIATE FOR GAUSSIAN POLICIES

Gaussian policies have been widely used and are shown to perform efficiently in many practical continuous reinforcement learning settings. Because of their wide applicability, we derive the gradient estimator with the Stein control variate for Gaussian policies here and use it in our experiments.
Specifically, Gaussian policies take the form (a | s) = N (a; µ1 (s), 2 (s)), where mean µ and covariance matrix  are often assumed to be parametric functions with parameters  = [1, 2]. This is equivalent to generating a by a = f(s, ) = µ1 (s) + 2 (s)1/2, where   N (0, 1). Following Eq (8), the policy gradient w.r.t. the mean parameter 1 is
1 J () = E [1 log (a|s) (Q(s, a) - (s, a)) + 1 µ(s) a(s, a)] . (17)
For each coordinate  in the variance parameter 2, its gradient is computed as

 J () = E



log (a|s) (Q(s, a) - (s, a)) - 1 2

a log (a|s)a(s, a) ,  

,

(18)

where A, B := trace(AB) for two da × da matrices.
Note that the second term in (18) contains a log (a|s), we can further apply Stein's identity on it to obtain a simplified formula

 J () = E

 log (a|s) (Q(s, a) - (s, a))

+

1 2

a,a(s, a),  

.

(19)

The estimator in (19) may have lower variance compared to that in (18). To see this, consider the case when (s, a) is a linear function of a (like the case of Q-prop), then the second term in (19) vanishes to zero, while that in (18) does not.
We also find it is practically convenient to estimate the parameters w in  by minimizing var(^ µJ)+ var(^ J), instead of the exact variance var(^ J). Further details can be found in Appendix 7.2.

3.4 PPO WITH STEIN CONTROL VARIATE
Proximal Policy Optimization (PPO) (Schulman et al., 2017; Heess et al., 2017) is recently introduced for policy optimization. It uses a proximal Kullback-Leibler (KL) divergence penalty to

6

Under review as a conference paper at ICLR 2018

Algorithm 1 PPO with Stein Control Variate (the PPO procedure is adapoted from Algorithm 1 in Heess et al. 2017)
repeat Run policy  for n timesteps, collecting {st, at, t, rt}, where t is the random seed that generates action at, i.e., at = f(st, t). Set old  .
// Updating the baseline function  for K iterations do
Update w by one stochastic gradient descent step according to (14), or (15), or (23) for Gaussian policies. end for
// Updating the policy  for M iterations do
Update  by one stochastic gradient descent step with (20) (adapting it with (17) and (19) for Gaussian policies). end for
// Adjust the KL penalty coefficient  if KL[old|] > highKLtarget then
   else if KL[old|] < lowKLtarget then
  / end if until Convergence

regularize and stabilize the policy gradient update. Given an existing policy old, PPO obtains a new policy by maximizing the following surrogate loss function

Jppo() = Eold

 (a|s) old(a|s)

Q

(s,

a)

-

KL

[old(·|s)

||

 (·|s)]

,

where the first term is an approximation of the expected reward, and the second term enforces the
the updated policy to be close to the previous policy under KL divergence. The gradient of Jppo() can be rewritten as

Jppo() = Eold w(s, a) log (a|s)Q(s, a)

where w(s, a) := (a|s)/old(a|s) is the density ratio of the two polices, and Q(s, a) := Q(s, a) + w(s, a)-1 where the second term comes from the KL penalty. Note that Eold [w(s, a)f (s, a)] = E[f (s, a)] by canceling the density ratio. Applying (6), we obtain

Jppo() = Eold w(s, a)  log (a|s) Q(s, a) - (s, a) + f(s, a)a(s, a) . (20)

Putting everything together, we summarize our PPO algorithm with Stein control variates in Algorithm 1. It is also straightforward to integrate the Stein control variate with TRPO.

4 RELATED WORK
The Stein's identity has been shown to be a powerful tool in many areas of statistical learning and inference. An incomplete list includes Gorham & Mackey (2015), Oates & Girolami (2016), Chwialkowski et al. (2016), Liu et al. (2016), Sedghi et al. (2016), Liu & Wang (2016), Feng et al. (2017), Liu & Lee (2017). This work was originally motivated by Oates & Girolami (2016), which uses the Stein's identity as a control variate for general Monte Carlo estimation. However, as discussed in Section 3.1, the original formulation of the Stein's identity can not be directly applied to policy gradient, and we need the mechanism introduced in (6) that also connects to the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014).
7

Under review as a conference paper at ICLR 2018
Control variate method is one of the most widely used variance reduction techniques in policy gradient (see e.g., Greensmith et al., 2004). However, action-dependent baselines have not yet been well studied. Besides Q-prop (Gu et al., 2016) which we draw close connection to, the work of Thomas & Brunskill (2017) also suggests a way to incorporate action-dependent baselines, but is restricted to the case of compatible function approximation. More recently, Tucker et al. (2017) studied a related action-dependent control variate for discrete variables in learning latent variable models.
5 EXPERIMENTS
We evaluated our control variate method when combining with PPO and TRPO on continuous control environments from the OpenAI Gym benchmark (Brockman et al., 2016) using the MuJoCo physics simulator (Todorov et al., 2012). We show that by using our more flexible baseline functions, we can significantly improve the sample efficiency compared with methods based on the typical value function baseline and Q-prop.
All our experiments use Gaussian policies. As suggested in Section 3.2, we assume the baseline to have a form of w(s, a) = V^ (s) + w(s, a), where V^  is the valued function estimated separately in the same way as the value function baseline, and w(s, a) is a parametric function whose value w is decided by minimizing either Eq (14) (denoted by FitQ), or Eq (23) designed for Gaussian policy (denoted by MinVar). We tested three different architectures of w(s, a), including
Linear. w(s, a) = aqw(a, µ(s)), (a - µ(s)) , where qw is a parametric function designed for estimating the Q function Q. This structure is motivated by Q-prop, which estimates w by fitting qw(s, a) with Q. Our MinVar, and FitQ methods are different in that they optimize w as a part of w(s, a) by minimizing the objective in Eq (14) and Eq (23). We show in Section 5.2 that our optimization methods yield better performance than Q-prop even with the same architecture of w. This is because our methods directly optimize for the baseline function w(s, a), instead of qw(s, a) which serves an intermediate step. Quadratic. w(s, a) = -(a - µw(s)) w-1(a - µw(s)). In our experiments, we set µw(s) to be a neural network, and w a positive diagonal matrix that is independent of the state s.
MLP. w(s, a) is assumed to be a neural network in which we first encode the state s with a hidden layer, and then concatenate it with the action a and pass them into another hidden layer before the output.
Further, we denote by Value the typical value function baseline, which corresponds to setting w(s, a) = 0 in our case. All the results we report are averaged over three random seeds. See Appendix for implementation details.
5.1 COMPARING THE VARIANCE OF DIFFERENT GRADIENT ESTIMATORS
We start with comparing the variance of the gradient estimators with different control variates. Figure 1 shows the results on Walker2d-v1, when we take a fixed policy obtained by running the vanilla PPO for 200 steps and evaluate the variance of the different gradient estimators under different sample size n. In order to obtain unbiased estimates of the variance, we estimate all the baseline functions using a hold-out dataset with a large sample size. We find that our methods, especially those using the MLP and Quadratic baselines, obtain significantly lower variance than the typical value function baseline methods and Q-prop.
5.2 COMPARISON WITH Q-PROP USING TRPO FOR POLICY OPTIMIZATION
Next we want to check whether our Stein control variate will improve the sample efficiency of policy gradient methods over existing control variate, e.g. Q-prop (Gu et al., 2016). One major advantage of Q-prop is that it can leverage the off-policy data to estimate qw(s, a). Here we compare our methods with the original implementation of Q-prop which incorporate this feature for policy optimization. Because the best existing version of Q-prop is implemented with TRPO, we implement a variant of our method with TRPO for fair comparison. The results on Hopper-v1 and Walker2d-v1 are shown in Figure 2, where we find that all Stein control variates, even including FitQ+Linear and MinVar+Linear, outperform Q-prop on both tasks. This is somewhat surprising because the
8

Under review as a conference paper at ICLR 2018

Log10 MSE

0.5 0.5 1.0 1.0 1.5 1.5 2.0 2.0 2.5 2.5

Value QProp MinVar-Linear MinVar-Quadratic MinVar-MLP FitQ-Linear FitQ-Quadratic FitQ-MLP

450 600 800 1200 1800

450 600 800 1200 1800

Sample size

Sample size

Figure 1: The variance of gradient estimators of different control variates under a fixed policy obtained by running vanilla PPO for 200 iterations in the Walker2d-v1 environment.

2500

Hopper-v1

1500

4000 3000 2000

Walker2d-v1
TRPO-Q-Prop TRPO-MinVar-Linear TRPO-FitQ-Linear TRPO-MinVar-MLP TRPO-FitQ-Quadratic

1000 500

1250k 2500k 3750k

1500k 3000k 4500k 6000k

Figure 2: Evaluation of TRPO with Q-prop and Stein control variates on Hopper-v1 and Walker2d-v1.

Q-prop compared here utilizes both on-policy and off-policy data to update w, while our methods use only on-policy data. We expect that we can further boost the performance by leveraging the offpolicy data properly, which we leave it for future work. In addition, we noticed that the Quadratic baseline generally does not perform as well as it promises in Figure 1; this is probably because that in the setting of policy training we optimize w for less number of iterations than what we do for evaluating a fixed policy in Figure 1, and it seems that Quadratic requires more iterations than MLP to converge well in practice.
5.3 PPO WITH DIFFERENT CONTROL VARIATES
Finally, we evaluate the different Stein control variates with the more recent proximal policy optimization (PPO) method which generally outperforms TRPO. We first test all the three types of  listed above on Humanoid-v1 and HumanoidStandup-v1, and present the results in Table 1. We can see that all the three types of Stein control variates consistently outperform the value function baseline, and Quadratic and MLP tend to outperform Linear in general.
We further evaluate our methods on a more extensive list of tasks shown in Figure 3, where we only show the result of PPO+MinVar+MLP and PPO+FitQ+MLP which we find tend to perform the best according to Table 1. We can see that our methods can significantly outperform PPO+Value which is the vanilla PPO with the typical value function baseline (Heess et al., 2017).
It seems that MinVar tends to work better with MLP while FitQ works better with Quadratic in our settings. In general, we find that MinVar+MLP tends to perform the best in most cases. Note that the MinVar here is based on minimizing the approximate objective (23) specific to Gaussian policy, and it is possible that we can further improve the performance by directly minimizing the exact objective in (15) if an efficient implementation is made possible. We leave this a future direction.
9

Under review as a conference paper at ICLR 2018

Function
MLP Quadratic
Linear Value

Humanoid-v1

MinVar

FitQ

3847 ± 249.3 3334 ± 695.7

2356 ± 294.7 3563 ± 235.1

2547 ± 701.8 3404 ± 813.1

2207 ± 554

HumanoidStandup-v1

MinVar

FitQ

143314 ± 9471 139315 ± 10527

117962 ± 5798 141692 ± 3489

129393 ± 18574 132112 ± 11450

128765 ± 13440

Table 1: Results of different control variates and methods for optimizing , when combined with PPO. The reported results are the average reward at the 10000k-th time step on Humanoid-v1 and the 5000k-th time step on HumanoidStandup-v1.

160000 120000 80000 40000
0 4500

HumanoidStandup-v1

5000

4000

3000

2000

1000

1500k 3000k 4500k 6000k 3000k

Ant-v1

2500

Humanoid-v1

Walker2d-v1 7000

5000

3000

6000k 9000k Hopper-v1

1000 12000k
8000

2500k 5000k 7500k HalfCheetah-v1

10000k

3000 6000 1500

4000
1500 PPO-MinVar-MLP
500 PPO-FitQ-Quadratic 2000 PPO-Value

3000k 6000k 9000k 12000k 15000k 250k

500k

750k

1000k 1000k

6000k

11000k

16000k

Figure 3: Evaluation of PPO with the value function baseline and Stein control variates across different Mujoco environments: HumanoidStandup-v1, Humanoid-v1, Walker2d-v1, Ant-v1 and Hopper-v1, HalfCheetah-v1.

6 CONCLUSION
We developed the Stein control variate, a new and general variance reduction method for obtaining sample efficiency in policy gradient methods. Our method generalizes several previous approaches. We demonstrated its practical advantages over existing methods, including Q-prop and value-function control variate, in several challenging RL tasks. In the future, we will investigate how to further boost the performance by utilizing the off-policy data, and search for more efficient ways to optimize  more efficient for quadratic . We would also like to point out that our method can be useful in other challenging optimization tasks such as variational inference and Bayesian optimization where gradient estimation from noisy data remains a major challenge.
REFERENCES
Kavosh Asadi, Cameron Allen, Melrose Roderick, Abdel-rahman Mohamed, George Konidaris, and Michael Littman. Mean actor critic. arXiv preprint arXiv:1709.00503, 2017.
Andrew D Barbour and Louis Hsiao Yun Chen. An introduction to Stein's method, volume 4. World Scientific, 2005.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In International Conference on Machine Learning, pp. 2606­2615, 2016.

10

Under review as a conference paper at ICLR 2018
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational gradient descent. Conference on Uncertainty in Artificial Intelligence (UAI), 2017.
Jackson Gorham and Lester Mackey. Measuring sample quality with stein's method. In Advances in Neural Information Processing Systems, pp. 226­234, 2015.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471­1530, 2004.
Shixiang Gu, Timothy P. Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. International Conference on Learning Representations (ICLR), 2016.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.
Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, pp. 1531­1538, 2002.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2013.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2015.
Qiang Liu and Jason D Lee. Black-box importance sampling. International Conference on Artificial Intelligence and Statistics, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems, 2016.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In International Conference on Machine Learning, pp. 276­284, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928­1937, 2016.
Chris J. Oates and Mark A. Girolami. Control functionals for quasi-monte carlo integration. In International Conference on Artificial Intelligence and Statistics, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.
Geoffrey Roeder, Yuhuai Wu, and David K. Duvenaud. Sticking the landing: An asymptotically zero-variance gradient estimator for variational inference. Advances in Neural Information Processing Systems, 2017.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. In International Conference on Machine Learning, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. International Conference of Learning Representations (ICLR), 2016.
11

Under review as a conference paper at ICLR 2018
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. Advances in Neural Information Processing Systems, 2017.
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar. Provable tensor methods for learning mixtures of generalized linear models. In International Conference on Artificial Intelligence and Statistics, 2016.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning (International Conference on Machine Learning-14), pp. 387­395, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354­359, Oct 2017. ISSN 0028-0836.
Charles Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7: i­164, 1986.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Philip S. Thomas and Emma Brunskill. Policy gradient methods for reinforcement learning with function approximation and action-dependent baselines. arxiv, abs/1706.06643, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012.
George Tucker, Andriy Mnih, Chris J Maddison, and Jascha Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. Advances in Neural Information Processing Systems, 2017.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 538­ 545. Morgan Kaufmann Publishers Inc., 2001.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
12

Under review as a conference paper at ICLR 2018

7 APPENDIX

7.1 PROOF OF THEOREM 3.1

Proof. Denote (a, |s) as the joint density function of (a, ) conditioned on s. It can be written as

(a, |s) = (a|s, )()  exp

-

1 2h2

||a

-

f

(s,

)||2

().

Taking the derivative of log (a, |s) w.r.t. a gives

1

a

log

(a, |s)

=

- h2

(a

-

f (s,

))

.

Similarly, taking the derivative of log (a, |s) w.r.t. , we have

1  log (a, |s) = h2 f(s, ) (a - f (s, ))
= -f(s, )a log (a, |s).
Multiplying both sides with (s, a) and taking the conditional expectation yield

E(a,|s)  log (a, |s)(s, a) = -E(a,|s) f(s, )a log (a, |s)(s, a)

= E() f(s, )E(a|s,) - a log (a, |s)(s, a)

= E() f(s, )E(a|s,) a(s, a)

= E(a,|s) f(s, )a(s, a) where the third equality comes from the Stein's identity (5) of (a|, s).

7.2 ESTIMATING  FOR GAUSSIAN POLICIES

The parameters w in  should be ideally estimated by minimizing the variance of the gradient estimator var(J()) using (15). Unfortunately, it is computationally slow to directly solve (15) with the current deep learning platforms, due to the limitation of the auto-differentiation implementations.
In general, this problem might be solved with a customized implementation of gradient calculation. In the case of Gaussian policies, we find minimizing var(^ µJ()) + var(^ J()) provides a good approximation.

More specifically, recall that Gaussian policy has a form of

(a|s)  1 exp - 1 (a - µ(s)) (s)-1 (a - µ(s)) ,

|(s)|

2

where µ(s) and (s) are parametric functions of state s, and || is the determinant of . Following Eq (8) we have

µJ() = E [-a log (a|s)(Q(s, a) - (s, a)) + a(s, a)] ,

(21)

where we use the fact that µf(s, ) = 1 and

µ log (a|s) = -a log (a|s) = (s)-1(a - µ(s)).

Similarly, we have

J () = E



log

(a|s)(Q (s,

a)

-

(s,

a))

+

1 2

aa(s,

a)

,

(22)

where

1  log (a|s) = 2

--1(s) + (s)-1(a - µ(s))(a - µ(s))

(s)-1

.

13

Under review as a conference paper at ICLR 2018

Because the baseline function does not change the expectations in (21) and (22), we can frame minw var(^ µJ()) + var(^ J()) into

n

min
w

gµ(st, at)

2 2

+

g(st, at)

2 F

,

t=1

(23)

where gµ and g are the integrands in (17) and (19) respectively, that is, gµ(s, a) =

-a log (a|s)(Q(s, a) - (s, a)) + a(s, a) and g(s, a) =  log (a|s)(Q(s, a) -

(s, a))

+

1 2

aa(s,

a).

Here

A

2 F

:=

ij Ai2j is the matrix Frobenius norm.

7.3 EXPERIMENT DETAILS

The advantage estimation A^(st, at) in Eq 16 is done by GAE with  = 0.98, and  = 0.995 (Schulman et al., 2016), and correspondingly, Q^(st, at) = A^(st, at) + V^ (st, at) in (9). Observations and advantage are normalized as suggested by Heess et al. (2017). The neural networks of the policies (a|s) and baseline functions w(s, a) use Relu activation units, and the neural network of the value function V^ (s) uses Tanh activation units. All our results use Gaussian MLP policy in
our experiments with a neural-network mean and a constant diagonal covariance matrix.

Denote by ds and da the dimension of the states s and action a, respectively. Network sizes are fol-

lows: On Humanoid-v1 and HuamnoidStandup-v1, we use (ds, da · 5, 5) for both policy network

and value network; On other Mujoco environments, we use (10 · ds, 10 · ds · 5, 5) for both policy

network and value network, with learning rate 0.0009 for policy network and 0.0001 for value

(ds ·5)

(ds ·5)

network. The network for  is (100, 100) with state as the input and the action concatenated with

the second layer.

All experiments of PPO with Stein control variate selects the best learning rate from {0.001, 0.0005, 0.0001} for  networks. We use ADAM (Kingma & Ba, 2014) for gradient descent and evaluate the policy every 20 iterations. Stein control variate is trained for the best iteration in range of {250, 300, 400, 500, 800}.

14

