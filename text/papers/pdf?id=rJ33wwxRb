Under review as a conference paper at ICLR 2018
SGD LEARNS OVER-PARAMETERIZED NETWORKS THAT PROVABLY GENERALIZE ON LINEARLY SEPARABLE DATA
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.
1 INTRODUCTION
Neural networks have achieved remarkable performance in many machine learning tasks. Although recently there have been numerous theoretical contributions to understand their success, it is still largely unexplained and remains a mystery. In particular, it is not known why in the overparameterized setting, in which there are far more parameters than training points, stochastic gradient descent (SGD) can learn networks that generalize well, as been observed in practice (Neyshabur et al., 2014; Zhang et al., 2016).
In such over-parameterized settings, the loss function can contain multiple global minima that generalize poorly. Therefore, learning can in principle lead to models with low training error, but high test error. However, as often observed in practice, SGD is in fact able to find models with low training error and good generalization performance. This suggests that the optimization procedure, which depends on the optimization method (SGD) and the training data, introduces some form of inductive bias which directs it towards a low complexity solution. Thus, in order to explain the success of neural networks, it is crucial to characterize this inductive bias and understand what are the guarantees for generalization of over-parameterized neural networks.
In this work, we address these problems in a binary classification setting where SGD optimizes a two-layer over-parameterized network with the goal of learning a linearly separable function. Clearly, an over-parameterized network is not necessary for classifying linearly separable data, since this is possible with linear classifiers (e.g., with the Perceptron algorithm) which also have good generalization guarantees (Shalev-Shwartz & Ben-David, 2014). But, the key question which we address here is whether a large network will overfit in such a case or not. As we shall see, it turns out that although the networks we consider are rich enough to considerably overfit the data, this does not happen when SGD is used for optimization. In other words, SGD introduces an inductive bias which allows it to learn over-parameterized networks that can generalize well. Therefore, this setting serves as a good test bed for studying the effect of over-paramaterization.
1

Under review as a conference paper at ICLR 2018

2 PROBLEM FORMULATION

Define X = {x  Rd : x  1}, Y = {±1}. We consider a distribution over linearly separable points. Formally, let D be a distribution over X × Y such that there exists w  Rd for which P(x,y)D(y w, x  1) = 1. 1 Let S = {(x1, y1), . . . , (xn, yn)}  X × Y be a training set sampled i.i.d. from D. 2

Consider the following two-layer neural network, with 2k > 0 hidden units. 3 The network parameters are W  R2k×d, v  R2k, which we denote jointly by W = (W, v). The network output is given by the function NW : Rd  R defined as:

NW (x) = v (W x) where  is a non-linear activation function applied element-wise.

(1)

We define the empirical loss over S to be the mean hinge-loss:
1n LS (W ) = n max {1 - yiNW (xi), 0}
i=1
Note that for convenience of analysis, we will sometimes refer to LS as a function over a vector. Namely, for a matrix W  R2k×d, we will consider instead its vectorized version W  R2kd (where the rows of W are concatenated) and define, with abuse of notation, that LS(W ) = LS(W ).
kk
In our setting we fix the second layer to be v = (v . . . v, -v · · · - v) such that v > 0 and only learn the weight matrix W . We will consider only positive homogeneous activations (Leaky ReLU and ReLU) and thus the network we consider with 2k hidden neurons is as expressive as networks with k hidden neurons and any vector v in the second layer. 4 Hence, we can fix the second layer without limiting the expressive power of the two-layer network. Although it is relatively simpler than the case where the second layer is not fixed, the effect of over-parameterization can be studied in this setting as well.

Hence, the objective of the optimization problem is to find:

arg min LS (W )
W R2k×d

(2)

where min LS (W ) = 0 holds for the activations we will consider (Leaky ReLU and ReLU).
W R2k×d

We focus on the case where LS (W ) is minimized using an SGD algorithm with batch of size 1, and where only the weights of the first layer (namely W ) are updated. At iteration t, SGD randomly
chooses a point (xt, yt)  S and updates the weights with a constant learning rate . Formally, let Wt = (Wt, v) be the parameters at iteration t, then the update at iteration t is given by



Wt

=

Wt-1

-

 W

L{(xt,yt)}(Wt-1)

(3)

We

define

a

non-zero

update

at

iteration

t

if

it

holds

that

 W

L{(xt,yt)}(Wt-1)

=

0.

Finally,

we

will

need the following notation. For 1  i  k, we denote by wt(i)  Rd the incoming weight vector

of neuron i at iteration t. 5 Similarly, for 1  i  k we define u(ti)  Rd to be the incoming weight

vector of neuron k + i at iteration t.

1This implies that w  1. 2Without loss of generality, we will ignore the event that yi w, xi < 1 for some i, since this is an event
of measure zero. 3We have an even number of hidden neurons for ease of exposition. See the definition of v below. 4For example, consider a network with k hidden neurons with positive homogeneous activations, where each
hidden neuron i has incoming weight vector wi and outgoing weight vi. Then, we can express this network with the network defined in Eq. 1 as follows. For each i such that vi > 0, we define a neuron in the new network with incoming weight vector viwi and outgoing weight 1. Similarly, if vi < 0, we define a neuron in the new network with incoming weight vector -viwi and outgoing weight -1. For all other neurons in the
new network we define an incoming zero weight vector. Due to the positive homogeneity, it follows that this
network is equivalent to the network with k hidden neurons. 5These are the neurons with positive outgoing weight v > 0.

2

Under review as a conference paper at ICLR 2018

3 MAIN RESULT

We now present our main results, for the case where  is the Leaky ReLU function. Namely, (z) = max{z, z} where 0 <  < 1.
First, we show that SGD can find a global optimum of LS (W ). Note that this is by no means obvious, since LS (W ) is a non-convex function (see Proposition 5.1). Specifically, we show that SGD converges to such an optimum while making at most:

w 2

w 2

M = 2 + O min{, }

(4)

non-zero update steps (see Corollary 5.2). In particular, the bound is independent of the number of

neurons 2k. To the best of our knowledge, this is the first convergence guarantee of SGD for neural

networks with the hinge loss. Furthermore, we prove a lower bound of 

w 

+ w 2

for the

number of non-zero updates (see Theorem 2).

Next, we address the question of generalization. As noted earlier, since the network is large, it can in

principle overfit. Indeed, there are parameter settings for which the network will have arbitrarily bad

test error (see Section 6.2). However, as we show here, this will not happen in our setting where SGD

is used for optimization. In Theorem 4 we use a compression bound to show that the model learned

by SGD will have a generalization error of O

M log n n

.6

This implies that for any network size,

given a sufficiently large number of training samples that is independent of the network size, SGD

converges to a global minimum with good generalization behaviour. This is despite the fact that for

sufficiently large k there are multiple global minima which overfit the training set (see Section 6.2).

This implies that SGD is biased towards solutions that can be expressed by a small set of training

points and thus generalizes well.

To summarize, when the activation is the Leaky ReLU and the data is linearly separable, we provide provable guarantees of optimization, generalization and expressive power for over-parameterized networks. This allows us to provide a rigorous explanation of the performance of over-parameterized networks in this setting. This is a first step in unraveling the mystery of the success of overparameterized networks in practice.

We further study the same over-parameterized setting where the non-linear activation is the ReLU function (i.e., (z) = max{0, z}). Surprisingly, this case has different properties. Indeed, we show that the loss contains spurious local minima and thus the previous convergence result of SGD to a global minimum does not hold in this case. Furthermore, we show an example where overparameterization is favorable from an optimization point of view. Namely, for a sufficiently small number of hidden neurons, SGD will converge to a local minimum with high probability, whereas for a sufficiently large number of hidden neurons, SGD will converge to a global minimum with high probability.

The paper is organized as follows. We discuss related work in Section 4 . In Section 5 we prove the convergence bounds, in Section 6 we give the generalization guarantees and in Section 7 the results for the ReLU activation. We conclude our work in Section 8.

4 RELATED WORK
The generalization performance of neural networks has been studied extensively. Earlier results (Anthony & Bartlett, 2009) provided bounds that depend on the VC dimension of the network, and the VC dimension was shown to scale linearly with the number of parameters. More recent works, study alternative notions of complexity, such as Rademacher compexity (Bartlett & Mendelson, 2002; Neyshabur et al., 2015; Bartlett et al., 2017; Kawaguchi et al., 2017), Robustness (Xu & Mannor, 2012) and PAC-Bayes (Neyshabur et al., 2017b). However, all of these notions fail to explain the generalization performance of over-parameterized networks (Neyshabur et al., 2017a). This is because these bounds either depend on the number of parameters or on the number of hidden
6See discussion in Remark 5 on the dependence of the generalizaion bound on .

3

Under review as a conference paper at ICLR 2018

neurons (directly or indirectly via norms of the weights) and become loose when these quantities become sufficiently large. The main disadvantage of these approaches, is that they do not depend on the optimization method (e.g., SGD), and thus do not capture its role in the generalization performance. In our work, we give generalization guarantees based on a compression bound that follows from convergence rate guarantees of SGD, and thus take into account the effect of the optimization method on the generalization performance. This analysis results in generalization bounds that are independent of the network size and thus hold for over-parameterized networks.
In parallel to our work, Kawaguchi et al. (2017) give generalization bounds for neural networks that are based on Rademacher complexity. Here too, the analysis does not take into account the optimization algorithm and the bound depends on the norm of the weights. Therefore, the bound can become vacuous for over-parameterized networks.
Stability bounds for SGD in non-convex settings were given in Hardt et al. (2016); Kuzborskij & Lampert (2017). However, their results hold for smooth loss functions, whereas the loss function we consider is not smooth due to the non-smooth activation functions (Leaky ReLU, ReLU).
Other works have studied generalization of neural networks in a model recovery setting, where assumptions are made on the underlying model and the input distribution (Brutzkus & Globerson, 2017; Zhong et al., 2017; Li & Yuan, 2017; Du et al., 2017; Tian, 2017). However, in their works the neural networks are not over-parameterized as in our setting.
Soltanolkotabi et al. (2017) analyze the optimization landscape of over-parameterized networks and give convergence guarantees for gradient descent to a global minimum when the data follows a Gaussian distribution and the activation functions are differentiable. The main difference from our work is that they do not provide generalization guarantees for the resulting model. Furthermore, we do not make any assumptions on the distribution of the feature vectors.
In a recent work, Nguyen & Hein (2017) show that if training points are linearly separable then under assumptions on the rank of the weight matrices of a fully-connected neural network, every critical point of the loss function is a global minimum. Their work extends previous results in Gori & Tesi (1992); Frasconi et al. (1997); Yu & Chen (1995). Our work differs from these in several respects. First, we show global convergence guarantees of SGD, whereas they only analyze the optimization landscape, without direct implications on performance of optimization methods. Second, we provide generalization bounds and their focus is solely on optimization. Third, we consider non-differentiable activation functions (Leaky ReLU, ReLU) while their results hold only for continuously differentiable activation functions.

5 CONVERGENCE ANALYSIS

In this section we consider the setting of Section 2 with a leaky ReLU activation function. In Section 5.1 we show SGD will converge to a globally optimal solution, and analyze the rate of convergence. In Section 5.1 we also provide lower bounds on the rate of convergence. The results in this section are interesting for two reasons. First, they show convergence of SGD for a non-convex objective. Second, the rate of convergence results will be used to derive generalization bounds in Section 6.

5.1 UPPER BOUND

Before proving convergence of SGD to a global minimum, we show that every critical point is a global minimum and the loss function is non-convex. The proof is deferred to the appendix.
Proposition 5.1. LS (W ) satisfies the following properties: 1) Every critical point is a global minimum. 2) It is non-convex.

Let Wt = (w(t1) . . . w(tk)u(t1) . . . u(tk))  R2kd be the vectorized version of Wt and Nt := NWt where Wt = (Wt, v) (see Eq. 1). Since we will show an upper bound on the number of non-zero updates, we will assume for simplicity that for all t we have a non-zero update at iteration t.

We assume that SGD is initialized such that the norms of all rows of W0 are upper bounded by some constant R > 0. Namely for all 1  i  k it holds that:

w0(i) , u0(i)  R

(5)

4

Under review as a conference paper at ICLR 2018



Define Mk :=

+ +w 2
2

w 2 k v 2 2

R(8k2  2 v 2 +8 k) 2k(v)1.5

w

+1.5 2R w
v

. We give an upper bound on

the number of non-zero updates SGD makes until convergence to a critical point (which is a global

minimum by Proposition 5.1). The result is summarized in the following theorem.

Theorem 1. SGD converges to a global minimum after performing at most Mk non-zero updates.

We will briefly sketch the proof of Theorem 1. The full proof is deferred to the Appendix (see Section 9.1.2). The analysis is reminiscent of the Perceptron convergence proof (e.g. in ShalevShwartz & Ben-David (2014)), but with key modifications due to the non-linear architecture. Con-

cretely, assume SGD performed t non-zero updates. We consider the vector Wt and the vec-
kk

tor W  = (w . . . w, -w · · · - w)  R2kd which is a global minimum of LS. We define F (Wt) = Wt, W  and G(Wt) = Wt . Then, we give an upper bound on G(Wt) in terms

of G(Wt-1) and by a recursive application of inequalities we show that G(Wt) is bounded from

above by a square root of a linear function of t. Similarly, by a recursive application of inequal-

ities, we show that F (Wt) is bounded from below by a linear function of t. Finally, we use the

Cauchy-Schwartz

inequality,

|F (Wt)| G(Wt) W 

 1, to show that t  Mk.

To obtain a simpler bound than the one obtained in Theorem 1, we use the fact that we can set R, v

arbitrarily, and choose:7

R = v = 1 .

(6)

2k

Then by Theorem 1 we get the following. The derivation is given in the Appendix (Section 9.1.3).

Corollary 5.2. Let R = v = 1 , then SGD converges to a global minimum after perfoming at
2k

most Mk =

w 2

2

+O

minw{,2}

non-zero updates.

Thus the bound consists of two terms, the first which only depends on the margin (via w ) and the second which scales inversely with . More importantly, the bound is independent of the network size.

5.2 LOWER BOUND

We use the same notations as in Section 5.1. The lower bound is given in the following theorem, which is proved in the Appendix (Section 9.1.4).

Theorem 2. Assume SGD is initialized according to Eq. 6, then for any d there exists a sequence of

linearly separable points on which SGD will make at least 

w 

+

w 2

mistakes.

Although this lower bound is not tight, it does show that the upper bound in Corollary 5.2 cannot be much improved. Furthermore, the example presented in the proof of Theorem 2, demonstrates that    can be optimal in terms of optimization and generalization, i.e., SGD makes the minimum number of updates ( w 2) and the learned model is equivalent to the true classifier w. We will use this observation in the discussion on the dependence of the generalization bound in Theorem 4 on  (see Remark 5).

6 GENERALIZATION
In this section we give generalization guarantees for SGD learning of over-parameterized networks with Leaky ReLU activations. These results are obtained by combining Theorem 1 with a compression generalization bound (see Section 6.1). In Section 6.2 we show that over-parameterized networks are sufficiently expressive to contain global minima that overfit the training set. Taken together, these results show that although there are models that overfit, SGD effectively avoids these, and finds the models that generalize well.
7This initialization resembles other initializations that are used in practice (Bengio, 2012; Glorot & Bengio, 2010)

5

Under review as a conference paper at ICLR 2018

6.1 COMPRESSION BOUND

Given the bound in Theorem 1 we can invoke compression bounds for generalization guarantees with
respect to the 0-1 loss (Littlestone & Warmuth, 1986) . Denote by Nk a two-layer neural network with 2k hidden neurons defined in Section 1 where  is the Leaky ReLU. Let SGDk(S, W0) be the output of running SGD for training this network on a set S and initialized with W0 that satisfies Eq. 5. Define Hk to be the set of all possible hypotheses that SGDk(S, W0) can output for any S and W0 which satisfies Eq. 5.

Now, fix an initialization W0. Then the key observation is that by Theorem 1 we have SGDk(S, W0) = BW0 (xi1 , ..., xick ) for ck  Mk, some function BW0 : X ck  Hk and (i1, ..., ick )  [n]ck .8 Equivalently, SGDk(·, W0) and BW0 define a compression scheme of size ck for hypothesis class Hk (see Definition 30.4 in Shalev-Shwartz & Ben-David (2014)). Denote by V = {xj : j / {i1, ..., ick }} the set of examples which were not selected to define SGDk(S, W0). Let L0D-1(SGDk(S, W0)) and L0V-1(SGDk(S, W0)) be the true risk of SGDk(S, W0) and empirical risk of SGDk(S, W0) on the set V , respectively. Then by Theorem 30.2 and Corollary 30.3 in Shalev-Shwartz & Ben-David (2014) we can easily derive the following theorem. The proof is deferred to the Appendix (Section 9.2.1).
Theorem 3. Let n  2ck, then with probability of at least 1 -  over the choice of S and W0 we have

L0D-1(SGDk(S, W0))  LV0-1(SGDk(S, W0)) +

L0V-1

(S

GDk

(S,

W0))

4ck

log n

n 

+

8ck log n

n 

Since LV0-1(SGDk(S, W0)) = 0 holds at a global minimum of LS, then by Combining the results of Corollary 5.2 and Theorem 3, we get the following theorem.

Theorem 4. If n  2ck and assuming the initialization defined in Eq. 6, then with probability at least 1 -  over the choice of S and W0, SGD converges to a global minimum of LS with 0-1 test

error at most

8 w 2

w 2

n 2 + O min{, }

n log


(7)

Thus for fixed w and  we obtain a sample complexity guarantee that is independent of the network size (See Remark 5 for a discussion on the dependence of the bound on ). This is despite the fact that for sufficiently large k, the network has global minima that have arbitrarily high test errors, as we show in the next section. Thus, SGD and the linearly separable data introduce an inductive bias which directs SGD to the global minimum with low test error while avoiding global minima with high test error. In Figure 1 we demonstrate this empirically for a linearly separable data set (from a subset of MNIST) learned using over-parameterized networks. The figure indeed shows that SGD converges to a global minimum which generalizes well.
Remark 5. The generelization bound in Eq. 7 holds for   , which is unique for the setting that we consider, and may seem surprising, given that a choice of large  often fails in practice. Furthermore, the bound is optimal for   . To support this theoretical result, we show in Theorem 2 an example where indeed    is optimal in terms of the number of updates and generalization. On the other hand, we note that in practice, it may not be optimal to use large  in our setting, since this bound results from a worst-case analysis of a sequence of examples encountered by SGD. Finally, the important thing to note is that the bound holds for any , and is thus applicable to realistic applications of SGD.

6.2 EXPRESSIVENESS
Let X  Rd×n be the matrix with the points xi in its columns, y  {-1, 1}n the corresponding vector of labels and let NW (X) = v (W X) be the network defined in Eq. 1 applied on the matrix X. By Theorem 8 in (Soudry & Hoffer, 2017) we immediately get the following. For completeness, the proof is given in the Appendix (Section 9.2.2).
8We use a subscript W0 because the function is determined by W0.

6

Under review as a conference paper at ICLR 2018

(a) (b)
Figure 1: Classifying MNIST images with over-parameterized networks. The setting of Section 5 is implemented (e.g., SGD with batch of size 1, only first layer is trained, Leaky ReLU activations) and SGD is initialized according to the initialization defined in Eq. 6. The linearly separable data set consists of 4000 MNIST images with digits 3 and 5, each of dimension 784. The size of the training set is 3000 and the remaining 1000 points form the test set. Three experiments are performed which differ only in the number of hidden neurons, 10, 100 and 1000. In the latter two, the networks are over-parameterized. For each number of hidden neurons, 40 different runs of SGD are performed and their results are averaged. (a) shows that in all experiments SGD converges to a global minimum. (b) shows that the global minimum obtained by SGD generalizes well in all settings (including the over-parameterized).

Theorem 6.

Assume that k  2

n 2d-2

.

Then for any y



{-1, 1}n and for almost any X,9

kk

there exist W~ = (W~ , v~) where W~  R2k×d and v~ = (v~ . . . v~, -v~ · · · - v~)  R2k, v~ > 0 such that y = NW~ (X).

Theorem 6 implies that for sufficiently large networks, the optimization problem (2) can have arbitrarely bad global minima with respect to a given test set, i.e., ones which do not generalize well on a given test set.

7 RELU- SUCCESS AND FAILURE CASES

In this section we consider the same setting as in section 5, but with the ReLU activation function (x) = max{0, x}. In Section 7.1 we show that the loss function contains arbitrarely bad local minima. In Section 7.2 we give an example where for a sufficiently small network, with high probability SGD will converge to a local minimum. On the other hand, for a sufficiently large network, with high probability SGD will converge to a global minimum.

7.1 EXISTENCE OF BAD LOCAL MINIMA

The result is summarized in the following theorem and the proof is deferred to the Appendix (Section

9.3.1).

The

main

idea

is

to

construct

a

network

with

weight

paramater

W

such

that

for

at

least

|S| 2

points (x, y)  S it holds that w, x < 0 for each neuron with weight vector w. Furthermore, the

remaining

points

satisfy

yNW (x)

>

1

and

thus

the

gradient

is

zero

and

LS(W )

>

1 2

.

9That is, the set of entries of X which do not satisfy the statement is of Lebesgue measure 0. 9We can only conclude that the trained network is approximately a linear classifier because of the limited
resolution of the grid.

7

Under review as a conference paper at ICLR 2018

kk

Theorem 7. Fix v = (1 . . . 1, -1 · · · - 1)  R2k. Then, for every finite set of examples S  X × Y

that is linearly separable, i.e., for which there exists w  Rd such that for each (x, y)  S we have

y

w, x



1,

there

exists

W



R2k×d

such

that

W

is

a

local

minimum

point

with

LS

(W )

>

1 2

.

7.2 ORTHOGONAL VECTORS - SIMPLE CASE ANALYSIS

In this section we assume that S = {e1 . . . ed} × {1}  X × Y where {e1, . . . , ed} is the standard basis of Rd. We assume all examples are labeled with the same label for simplicity, as the same result holds for the general case. Let NWt be the network obtained at iteration t, where Wt = (Wt, v). Assume we initialize with
kk
fixed v = (1 . . . 1, -1 · · · - 1), and W0  R2k×d is randomly initialized from a continuous symmetric distribution with bounded norm, i.e |[W0]i,j|  C for some C > 0.

The main result of this section is given in the following theorem. The proof is given in the Appendix (Section 9.3.2). The main observation is that the convergence to non-global minimum depends solely on the initialization and occurs if and only if there exists a point x such that for all neurons, the corresponding initialized weight vector w satisfies w, x  0.

Theorem 8. Fix  > 0 and assume we run SGD with examples from S = {e1 . . . ed} × {1}.

If

k



log2(

-

d ln()

),

then

with

probability

of

at

least

1 - ,

SGD

will

converge

to

a

non

global

minimum point.

On

the

other

hand,

if

k



log2(

2d 

),

then

with

probability

of

at

least

1

-

,

SGD

will

converge

to

a

global minimum point after

max{

dC 

,

d 

}

iterations.

Note that in the first part of the theorem, we can make the basin of attraction of the non-global

minimum

exponentially

large

by

setting



=

e-d

for





1 2

.

8 CONCLUSION

Understanding the performance of over-parameterized neural networks is essential for explaining the success of deep learning models in practice. Despite a plethora of theoretical results for generalization of neural networks, none of them give guarantees for over-parameterized networks. In this work, we give the first provable guarantees for the generalization performance of over-parameterized networks, in a setting where the data is linearly separable and the network has Leaky ReLU activations. We show that SGD compresses its output when learning over-parameterized networks, and thus exhibits good generalization performance.
The analysis for networks with Leaky ReLU activations does not hold for networks with ReLU activations, since in this case the loss contains spurious local minima. However, due to the success of over-parameterized networks with ReLU activations in practice, it is likely that similar results hold here as well. It would be very interesting to provide convergence guarantees and generalization bounds for this case. Another direction for future work is to show that similar results hold under different assumptions on the data.

REFERENCES
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, pp. 437­478. Springer, 2012.

8

Under review as a conference paper at ICLR 2018
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017.
P Frasconi, M Gori, and A Tesi. Successes and failures of backpropagation: A theoretical. Progress in Neural Networks: Architecture, 5:205, 1997.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76­86, 1992.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. arXiv preprint arXiv:1703.01678, 2017.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv preprint arXiv:1705.09886, 2017.
Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical report, Technical report, University of California, Santa Cruz, 1986.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376­1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017b.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391­423, 2012.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300­1303, 1995.
9

Under review as a conference paper at ICLR 2018

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.

9 APPENDIX

9.1 MISSING PROOFS FOR SECTION 5

9.1.1 PROOF OF PROPOSITION 5.1
1. Denote by W = w(1) . . . w(k)u(1) . . . u(k)  R2kd the vector of all parameters where each w(i), u(i)  Rd. Let (x, y)  S, then if yNW (x) < 1, it holds that



 w(i)

L{(x,y)}(W

),

w

= -y

w(i), x x, w  -

w(i), x < 0

and similarly,

 u(i)

L{(x,y)}

(W

),

-w

= y

u(i), x x, -w  -

w(i), x < 0.

kk
Hence if we define W  = (w . . . w, -w · · · - w)  R2kd, then

 L{(x,y)}(W ), W  < 0 W

Otherwise, if yNW (x)  1, then the gradient vanishes and thus

 L{(x,y)}(W ), W  = 0 W

It follows that if there exists (x, y)  S, such that yNW (x) < 1, then we have

 LS(W ), W  W

1n =
n
i=1

 W

L{(xi,yi)}(W

),

W



<0

and

thus

 W

LS(W )

=

0.

Therefore,

for

any

critical

point

it

holds

that

yNW (x)



1

for

all (x, y)  S, which implies that it is a global minimum.

2. For simplicity consider the function fx(w, u) = ( w, x ) - ( u, x ) for x = 0. Define w1 = w2 = u1 = x and u2 = -x. Then

fx(w1, u1) = 0

fx(w2, u2) = (1 + ) x 2

and

fx(

w1

+ 2

w2

,

u1

+ 2

u2

)

=

x2

and

thus

fx

(

w1

+w2 2

,

u1

+u2 2

)

>

1 2

fx

(w1,

u1)

+

1 2

fx

(w2,

u2)

which

implies

that

the

func-

tion is not convex.

9.1.2 PROOF OF THEOREM 1
Assume SGD performed t non-zero updates. We will show that t  Mk. We note that if there is no (x, y)  S such that the corresponding update is non-zero, then SGD has reached a critical point

10

Under review as a conference paper at ICLR 2018

kk
of LS (which is a global minimum by Proposition 5.1). Let W  = (w . . . w, -w · · · - w)  R2kd and note that LS(W ) = 0, i.e., W  is a global minimum. Define the following two functions:

kk

F (Wt) = Wt, W  =

wt(i), w -

u(ti), w

i=1 i=1

G(Wt) = Wt =

kk

wt(i) 2 +

u(ti) 2

i=1 i=1

Then, from Cauchy-Schwartz inequality we have

|F (Wt)| = Wt, W   1

G(Wt) W 

Wt W 

(8)

Since the update at iteration t is non-zero, we have ytNt-1(xt) < 1 and the update rule is given by

w(ti) = wt(-i)1 + vpt(i)ytxt , ut(i) = ut(-i)1 - vqt(i)ytxt

(9)

where p(ti) = 1 if w(t-i)1, xt  0 and pt(i) =  otherwise. Similarly qt(i) = 1 if ut(-i)1, xt  0 and qt(i) =  otherwise. It follows that:

k
G(Wt)2 =
i=1 k

i=1 k
<
i=1

k

w(ti) 2 +

u(ti) 2

i=1

k

w(t-i)1 2 +

ut(-i)1 2 + 2vyt

i=1

kk

wt(-i)1, xt p(ti) -

ut(-i)1, xt

i=1 i=1

k

w(t-i)1 2 +

u(t-i)1 2 + 2 + 2k2v2 = G(Wt-1)2 + 2 + 2k2v2

i=1

qt(i)

+ 2k2v2 xt 2

where the second inequality follows since ytv

k i=1

wt(-i)1, xt

p(ti) -

k i=1

ut(-i)1, xt

qt(i)

=

ytNt-1(xt) < 1. Using the above recursively, we obtain:

G(Wt)2  G(W0)2 + t(2k2v2 + 2)

(10)

On the other hand,

kk

F (Wt) =

w(ti), w -

ut(i), w

i=1 i=1

kk

k

k

=

w(t-i)1, w -

u(t-i)1, w + v

ytxt, w pt(i) + v

ytxt, w qt(i)

i=1 i=1

i=1

i=1

kk
 w(t-i)1, w - u(t-i)1, w + 2kv = F (Wt-1) + 2kv
i=1 i=1

where the inequality follows since ytxt, w  1. This implies that

F (Wt)  F (W0) + 2kvt

(11)

By combining equations Eq. 8, Eq. 10 and Eq. 11 we get,
-G(W0) W  + 2kvt  F (W0) + 2kvt  F (Wt)  W  G(Wt)  W  G(W0)2 + t(2k2v2 + 2)

11

Under review as a conference paper at ICLR 2018

  Using a + b  a + b the above implies,
 -G(W0) W  + 2kvt  W  G(W0) + W  t

2k2v2 + 2

Since

w0(i) ,

u0(i)

  R we have G(W0)  2kR. Noting that

W

 = 2k w

we get,



at  b t + c

where a = 2kv, b = (4k22v2 + 4k) w and c = 4kR w . By inspecting the roots of

the

parabola

P (x)

=

x2

-

b a

x

-

c a

we

conclude

that

t

b2 +

a

c b c (4k22v2 + 4k) w 2

(4k22v2 + 4k) w

+= aa a

4k22v22

+

2kv

2R w 2R w +
v v

w 2

w 2

= 2 + kv22 +

R(8k22v2 + 8k) w 1.5 2R w

2k(v)1.5

+ v

= Mk (12)

9.1.3 PROOF OF COROLLARY 5.2

Since

R v

=

1,

we

have

by

Theorem

1

and

the

inequality

 a

+b



 a+

 b,

w 2

w 2

w 1.5

Mk = 2 + O



+O  

+O

w 2

w 2

= 2 + O min{, } .

w 1.5 +O


w 

(13)

9.1.4 PROOF OF THEOREM 2

We will prove a more general theorem. Theorem 2 follows by setting R = v = 1 .
2k

Theorem 9. For any d there exists a sequence of linearly separable points on which SGD will make

at least

max min B1, B2 , w 2

updates, where and

R w B1 = v + min

w 2 2kv2

-

w

,0

R w B2 = v

+ min

w 2 - 22kv2

w 

,0

Proof. Define a sequence S of size d,

(e1, 1), (e2, 1), ..., (ed, 1)
where {ei} is the standard basis of Rd and let w = (1, 1, ..., 1)  Rd. Note that d = w 2 and w, ei  1 for all 1  i  d. We will consider the case where SGD runs on a sequence of examples which consists of multiple copies of S one after the other.

Assume SGD is initialized with

w0(i)

=

-

d j=1

R ej d

u0(i)

=

d j=1

R ej d

12

Under review as a conference paper at ICLR 2018

for all 1  i  k. Note that w(0i) , u(0i)  R for all 1  i  k.

Since w(0i) = w0(j) and u(0i) = u(0j) for all i = j, we have by induction that w(ti) = wt(j) and u(ti) = u(tj) for all i = j and t > 0. Hence, we will denote wt = wt(i) and ut = ut(i) for all 1  i  d and t > 0. Then for all 1  j  d we have NWt (ej) = kv ( wt, ej ) wt, ej -
kv ( ut, ej ) ut, ej .

Since at the global minumum NWt,v(ej)  1 for all 1  j  d, it follows that a necessary

condition for convergence to a global minimum is that there exists an iteration t in which either

kv ( wt, ed ) wt, ed



1 2

or -kv (

ut, ed

)

ut, ed



1 2

.

Equivalently, either

wt, ed



1 2kv

or

ut, ed



-

1 2kv

.

Since

w0, ed

= - R , then by the gradient updates (Eq. 9) it follows that after at least R
d v d

copies of S, or equivalently, after at least Rd iterations we will have 0 
v d

wt, ed

 v.

Then,

after

at

least

d

min{

1 2kv

-v,0}

v

iterations

we

have

wt, ed



1 2kv

.

Thus,

in

total,

after

at

least

R w v

+

min{

w 2 2kv2

-

w

2, 0} iterations, we have

wt, ed



1 2kv

.

By the same reasoning, we have

ut, ed



-

1 2kv

after

at

least

R

w v

+ min{

w 2 22  kv 2

-

w 

2
, 0}

iterations. Finally, SGD must update on at least d points in order to converge to the global minimum.

The claim now follows.

9.2 MISSING PROOFS FOR SECTION 6

9.2.1 PROOF OF THEOREM 3

By Theorem 30.2 and Corollary 30.3 in Shalev-Shwartz & Ben-David (2014), for n  2ck we have that with probability of at least 1 -  over the choice of S

LD(SGDk(S, W0))  LV (SGDk(S, W0)) +

LV

(S

GDk

(S,

W0))

4ck

log n

n 

+

8ck

log n

n 

(14)

The above result holds for a fixed initialization W0. We will show that the same result holds with high probability over S and W0, where W0 is chosen independently of S and satisfies Eq. 5. Define B to be the event that the inequality Eq. 14 does not hold. Then we know that PS(B|W0)   for any fixed initialization W0. 10 Hence, by the law of total expectation,
PS,W0 (B) = EW0 [PS(B|W0)]  

9.2.2 PROOF OF THEOREM 6
We can easily extend Theorem 8 in (Soudry & Hoffer, 2017) to hold for labels in {-1, 1}. By the theorem we can construct networks NW1 and NW2 such that for all i:
1. NW1 (xi) = 1 if yi = 1 and NW1 (xi) = 0 otherwise. 2. NW2 (xi) = 1 if yi = -1 and NW2 (xi) = 0 otherwise.
Then (NW1 - NW2 )(xi) = yi and NW1 - NW2 = NW~ for W~ = (W~ , v~) where W~  R2k×d and
kk
v~ = (v~ . . . v~, -v~ · · · - v~)  R2k, v~ > 0.

9.3 MISSING PROOFS FOR SECTION 7
9.3.1 PROOF OF THEOREM 7
We first need the following lemma.
10This is where we use the independence assumption on S and W0 . In the proof of Theorem 30.2 in ShalevShwartz & Ben-David (2014), the hypothesis hI needs to be independent of V . Our independence assumption ensures that this holds.

13

Under review as a conference paper at ICLR 2018

Lemma 10. There exists w^  Rd that satisfies the following:

1. There exists  > 0 such that for each (x, y)  S we have | x, w^ | > .

2. #{(x, y)  S :

w^ , x

<

0}

>

1 2

|S|.

Proof. Consider the set V = {v  Rd : (x,y)S v, x = 0}. Clearly, V is a finite union of hyper-
planes and therefore has measure zero, so there exists w^  Rd \ V . Let  = min(x,y)S{| w^ , x |}, and since S is finite we clearly have  > 0. Finally, if

#{(x, y)  S : w^ , x < 0} > 1 |S| 2

we

can

choose

w^

and



=

 2

and

we

are

done.

Otherwise,

choosing

-w^

and



=

 2

satisfies

all

the

assumptions of the lemma.

We are now ready to prove the theorem. Choose w^  Rd that satisfies the assumptions in Lemma

10. Now, let c >

w 

, and let w = cw^ + w and u = cw^ - w. Define

kk
W = [w . . . w, u . . . u]  R2k×d

Let (x, y)  S be an arbitrary example. If w^ , x > , then

w, x = c w^ , x + w, x  c - w > 0 u, x = c w^ , x - w, x  c - w > 0

It follows that

kk
NW (x) = ( w, x ) - ( u, x )

11

kk
= (c w^ , x + w, x ) - (c w^ , x - w, x )

1
= 2k w, x

1

Therefore yNW (x) > 1, so we get zero loss for this example, and therefore the gradient of the loss will also be zero. If, on the other hand, w^ , x < -, then
w, x = c w^ , x + w, x  -c + w < 0 u, x = c w^ , x - w, x  -c + w < 0

and therefore
kk
NW (x) = ( w, x ) - ( u, x ) = 0.
11
In this case the loss on the example would be max{1 - yNW (x), 0} = 1, but the gradient will also be zero. Along with assumption 2, we would conclude that:

1

LS (W ) >

, 2

W LS (W ) = 0

Notice that since all the inequalities are strong, the following holds for all W  R2k×d that satisfies W - W < , for a small enough > 0. Therefore, W  R2k×d is indeed a local minimum.

14

Under review as a conference paper at ICLR 2018

9.3.2 PROOF OF THEOREM 8

Denote Wt = [wt(1) . . . wt(k)ut(1) . . . ut(k)] and define Kt = {ej : i[k] prove the following lemma.
Lemma 11. For every t we get Kt+1 = Kt.

w(ti), ej

 0}. We first

Proof. Let ej be the example seen in time t. If NWt (ej)  1 then there is no update and we are

done.

Otherwise,

if

ej



Kt

then

for

each

i



[k]

we

have



 w(ti)

NWt

(ej

)

=

0

and

therefore

the

update does not change the value of wt(i), and thus Kt+1 = Kt. If ej / Kt then there exists i  [k]

such that w(ti), ej > 0. In that case, we update wt(+i)1  wt(i) + ej. Now, note that

w(t+i)1, ej = wt(i), ej +  ej , ej > wt(i), ej > 0

and therefore ej / Kt+1. Furthermore, for each e where = j, by the orthogonality of the vectors we know that for each i  [k] it holds that

w(t+i)1, e = wt(i), e +  ej , e = wt(i), e Thus e  Kt if and only if e  Kt+1 and this concludes the lemma.

We can now prove the theorem. For each j  [d], by the symmetry of the initialization, with

probability

1 2

over

the

initialization

of

w(0i),

we

get

that

w0(i), ej

 0. Since all wi's are initialized

independently, we get that:

P (ej  K0) = P (i[k] w(0i), ej  0) =

P ( w(0i), ej

1  0) = 2k

i[k]

Now,

assuming

k



log2(

-

d ln()

),

from

the

independence

of

the

initialization

of

w(0i)'s

coordinates

we get

P (j[d]ej / K0) =

P (ej / K0)

j[d]

= (1 -

1 2k

)d



e-

d 2k



Therefore, with probability at least 1 - , there exists j  [k] for which ej  K0. By Lemma 11, this implies that for all t  N we will get ej  Kt, and therefore NWt (ej)  0. Since ej is labeled 1, this implies that LS (W ) > 0. By the separability of the data, and by the convergence of the SGD algorithm, this implies that the algorithm converges to a stationary point that is not a global minimum. Note that convergence to a saddle point is possible only if we define  (0) = 0,
and for all i  [k] we have at the time of convergence wt(i), ej = 0. This can only happen if
w0(i), ej = N for some N  N, which has probability zero over the initialization of wt(i).
Therefore, the convergence is almost surely to a non-global minimum point.

On

the

other

hand,

assuming

k



log2

(

d 

),

using

the

union

bound

we

get:

P (j[d]ej  K0) 

P (ej  K0)

j[d]

=

d 2k





So with probability at least 1 - , we get K0 =  and by Lemma 11 this means Kt =  for all t  N. Now, if ej / Kt for all t  N, then there exists i  [k] such that wt(i), ej > 0 for all t  N. If

15

Under review as a conference paper at ICLR 2018

after

performing

T

update

iterations

we

have

updated

N

>

max{

C 

,

1 

}

times

on

ej ,

then

clearly:

wt(i), ej i[k] s.t

T
= w(0i), ej +  ej , ej  w0(i), ej + N  > 1
t=0

u0(i), ej > 0,

T
u(ti), ej = u0(i), ej -  ej , ej  C - N   0
t=0

and therefore NWt (ej) > 1, which implies that L{(ej,1)}(Wt) = 0. From this, we can conclude

that for each j  [d], we perform at most

max{

C 

,

1 

}

update iterations on ej before reaching

zero loss, and therefore we can perform at most

max{

dC 

,

d 

}

update iterations until convergence.

Since we show that we never get stuck with zero gradient on an example with loss greater than zero,

this means we converge to a global optimum after at most

max{

dC 

,

d 

}

iterations.

16

