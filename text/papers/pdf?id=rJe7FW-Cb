Under review as a conference paper at ICLR 2018
A PAINLESS ATTENTION MECHANISM FOR CONVOLUTIONAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches. Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different finegrained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food100, obtaining competitive and state-of-the-art scores.
1 INTRODUCTION
Humans and animals process vasts amounts of information with limited computational resources thanks to attention mechanisms which allow them to focus resources on the most informative chunks of information. These biological mechanisms have been extensively studied (see Anderson (1985); Desimone & Duncan (1995)), concretely those mechanisms concerning visual attention, e.g. the work done by Ungerleider & G (2000).
In this work, we incorporate the advantages of visual and biological attention mechanisms for for fine-grained visual recognition with Convolutional Neural Networks (CNN) (see LeCun et al. (1998)). This is a particularly difficult task since it involves looking for details in large amounts of data (images) while remaining robust to deformation and clutter. In this sense, different attention mechanisms for fine-grained recognition exist in the literature: (i) iterative methods that process images using "glimpses" with recurrent neural networks (RNN) or long short-term memory (LSTM) (e.g. the work done by Sermanet et al. (2015); Zhao et al. (2017b)), (ii) feed-forward attention mechanisms that augment vanilla CNNs, such as the Spatial Transformer Networks (STN) by Jaderberg et al. (2015), or a top-down feed-forward attention mechanism (FAM) (Rodr´iguez et al. (2017)). Although it is not applied to fine-grained recognition, the Residual Attention introduced by Wang et al. (2017) is another example of feed-forward attention mechanism that takes advantage of residual connections (He et al. (2016)) to enhance or dampen certain regions of the feature maps in an incremental manner.
Inspired by all the previous research about attention mechanisms in computer vision, we propose a novel feed-forward attention architecture (see Figure 1) that accumulates and enhances most of the desirable properties from previous approaches:
1. Detect and process in detail the most informative parts of an image: more robust to deformation and clutter.
2. Feed-forward trainable with SGD: faster inference than iterative models, faster convergence rate than Reinforcement Learning-based (RL) methods like the ones presented by Sermanet et al. (2015); Liu et al. (2016).
1

Under review as a conference paper at ICLR 2018

Class Pred
Pred
Conv Block
Pred
Conv Block
Pred
Conv Block

Attention

Softmax

conv1x1

Input

c

Attention

Softmax

conv1x1

Input

c

Attention

Softmax

conv1x1

Input

c

c 

Figure 1: The proposed mechanism. Feature maps at different levels are processed to generate spatial attention masks and use them to output a class hypothesis based on local information and a confidence score (C). The final prediction consists of the average of all the hypotheses weighted by the normalized confidence scores.
3. Preserve low-level detail: unlike Residual Attention (Wang et al. (2017)), where low-level features are subject to noise after traversing multiple residual connections, our architecture directly uses them to make predictions. This is important for fine-grained recognition, where low-level patterns such as textures can help to distinguish two similar classes.
Moreover, the proposed mechanism possesses other interesting properties such as:
1. Modular and incremental: the attention mechanism can be replicated at each layer on any convolutional architecture, and it is easy to adapt to the task at hand.
2. Architecture independent: the mechanism can accept any pre-trained architecture such as VGG (Simonyan & Zisserman (2014)) or ResNet.
3. Low computational impact: While STNs use a small convnet to predict affine-transform parameters and Residual Attention uses the hourglass architecture, our attention mechanism consists of a single 1 × 1 convolution and a small fully-connected layer.
4. Simple: the proposed mechanism can be implemented in few lines of code, making it appealing to be used in future work.
The proposed attention mechanism has been included in a strong baseline like Wide Residual Networks (WRN) (Zagoruyko & Komodakis (2016)), and applied on five fine-grained recognition datasets. The resulting network, called Wide Residual Network with Attention (WRNA) systematically enhances the performance of WRNs, obtains competitive results using low resolution training images, and surpasses the state of the art in the Adience gender recognition task, Stanford dogs, and UEC Food-100. Table 1 shows the gain in performance of WRNA w.r.t. WRN for all the datasets considered in this paper.
In the next section, we review the most relevant work concerning attention mechanisms for visual fine-grained recognition.
2 RELATED WORK
As reviewed by Zhao et al. (2017a), there are different approaches to fine-grained recognition: (i) vanilla deep CNNs, (ii) CNNs as feature extractors for localizing parts and do alignment, (iii) ensembles, (iv) attention mechanisms.
In this paper we focus on (iv), the attention mechanisms, which aim to discover the most discriminative parts of an image to be processed in greater detail, thus ignoring clutter and focusing on the most distinctive traits. These parts are central for fine-grained recognition, where the inter-class variance is small and the intra-class variance is high.
2

Under review as a conference paper at ICLR 2018

Task
Age Birds Food Dogs Cars Gend

WRN
58.6 81.0 84.3 89.6 87.8 93.9

WRNA
59.7 82.0 85.5 89.9 90.0 94.8


+1.1 +1.0 +1.2 +0.4 +2.2 +0.9

Table 1: Performance of WRN and our approach (WRA) on the Adience benchmark for Age, Gender (Gend); CUB200-2011 (Birds); Stanford cars (Cars); Stanford dogs (Dogs); and UEC Food-100 (Food). The absolute accuracy improvement (in %) is marked as . Bold performance indicates outperforming the state of the art. The augmented network consistently outperforms the baseline up to a relative 18% relative error decrease on Cars.

Different fine-grained attention mechanisms can be found in the literature. Xiao et al. (2015) proposed a two-level attention mechanism for fine-grained classification on different subsets of the ICLR2012 (Russakovsky et al. (2012)) dataset, and the CUB200 2011. In this model, images are first processed by a bottom-up object proposal network based on R-CNN (Zhang et al. (2014)) and selective search (Uijlings et al. (2013)). Then, the softmax scores of another ILSVRC2012 pretrained CNN, which they call FilterNet, are thresholded to prune the patches with the lowest parent class score. These patches are then classified to fine-grained categories with a DomainNet. Spectral clustering is also used on the DomainNet filters in order to extract parts (head, neck, body, etc.), which are classified with an SVM. Finally, the part- and object-based classifier scores are merged to get the final prediction. The two-level attention obtained state of the art results on CUB200-2011 with only class-level supervision. However, the pipeline must be carefully fine-tuned since many stages are involved with many hyper-parameters.
Differently from two-level attention, which consists of independent processing and it is not endto-end, Sermanet et al. proposed to use a deep CNN and a Recurrent Neural Network (RNN) to accumulate high multi-resolution "glimpses" of an image to make a final prediction (Sermanet et al. (2015)), however, reinforcement learning slows down convergence and the RNN adds extra computation steps and parameters.
A more efficient approach was presented by Liu et al. in (Liu et al. (2016)), where a fullyconvolutional network is trained with reinforcement learning to generate confidence maps on the image and use them to extract the parts for the final classifiers whose scores are averaged. Compared to previous approaches, in the work done by Liu et al. (2016), multiple image regions are proposed in a single timestep thus, speeding up the computation. A greedy reward strategy is also proposed in order to increase the training speed. The recent approach presented by Fu et al. (2017) uses a classification network and a recurrent attention proposal network that iteratively refines the center and scale of the input (RA-CNN). A ranking loss is used to enforce incremental performance at each iteration.
Zhao et al. proposed Diversified Visual Attention Network (DVAN), i.e. enforcing multiple nonoverlapped attention regions (Zhao et al. (2017b)). The overall architecture consists of an attention canvas generator, which extracts patches of different regions and scales from the original image; a VGG-16 (Simonyan & Zisserman (2014)) CNN is then used to extract features from the patches, which are aggregated with a DVAN long short-term memory (Hochreiter & Schmidhuber (1997)) that attends to non-overlapping regions of the patches. Classification is performed with the average prediction of the DVAN at each region.
All the previously described methods involve multi-stage pipelines and most of them are trained using reinforcement learning (which requires sampling and makes them slow to train). In contrast, STNs, FAM, and our approach jointly propose the attention regions and classify them in a single pass. Moreover, they possess interesting properties compared to previous approaches such as (i) simplicity (just a single model is needed), (ii) deterministic training (no RL), and (iii) feed-forward training (only one timestep is needed). In addition, since our approach only uses one CNN stream, it is far more computationally efficient than STNs and FAM, as described next.
3

Under review as a conference paper at ICLR 2018

X Y

N N

C
Classes

(a) Attention Heads

(b) Output Head

Figure 2: Scheme of the submodules in the proposed mechanism. (a) depicts the attention heads, (b) shows the output head.

3 OUR APPROACH
Our approach consists of a universal attention module that can be added after each convolutional layer without altering pre-defined information pathways of any architecture. This is helpful since it allows to seamlessly augment any architecture such as VGG and ResNet with no extra supervision, i.e. no part labels are necessary. The attention module consists of three main submodules: (i) the attention heads H, which define the most relevant regions of a feature map, (ii) the output heads O, generate an hypothesis given the attended information, and (iii) the confidence gates G, which output a confidence score for each attention head. Each of these modules is explained in detail in the following subsections.

3.1 ATTENTION HEAD
Inspired by DVAN (Zhao et al. (2017b)) and the transformer architecture presented by Vaswani et al. (2017), and following the notation established by Zagoruyko & Komodakis (2016), we have identified two main dimensions to define attentional mechanisms: (i) the number of layers using the attention mechanism, which we call attention depth (AD), and (ii) the number of attention heads in each attention module, which we call attention width (AW). Thus, a desirable property for any universal attention mechanism is to be able to be deployed at any arbitrary depth and width.
This property is fulfilled by including K attention heads Hk (width), depicted in Figure 2a, into each attention module (depth)1. Then, the attention heads at layer l, receive the feature activations Zl of that layer as input, and output K weighted feature maps, see Equations 1 and 2:

Ml = spatial sof tmax(WHl  Zl),

(1)

Hl = Ml Zl,

(2)

where Hl is the output matrix of the lth attention module, WH is a 1 × 1 convolution kernel with output dimensionality K used to compute the attention masks corresponding to the attention heads Hk,  denotes the convolution operator, and is the element-wise product. Please note that Mkl is a 2d flat mask and the product with each of the N input channels of Z is done by broadcasting. Likewise, the dimensionality of Hlk is the same as Zl.
Since the different attention heads in an attention module are sometimes focusing on the same exact part of the image, similarly to DVAN, we have introduced a regularization loss LR that forces the
1Notation: H, O, G are the set of attention heads, output heads, and attention gates respectively. Uppercase letters are used as functions or constants, and lowercase letters are used as indices. Bold uppercase represent matrices and bold lowercase represent vectors.

4

Under review as a conference paper at ICLR 2018

multiple masks to be different. In order to simplify the notation, we set mlk to be kth flattened version of the attention mask M in Equation 1. Then, the regularization loss is expressed as:

K

LR =

||Mli(Mlj )T ||2,

i=1 j=i

(3)

i.e., it minimizes the Frobenius norm of the cross-correlation between each pair of attention masks. This loss is added to the network loss Lnet weighted by a constant factor  = 0.1 which was found to work best across all tasks:

Ln et = Lnet + LR

(4)

3.2 OUTPUT HEAD
The output of each attention module consists of a spatial dimensionality reduction layer: F : Rx×y×n  R1×n,

(5)

followed by a fully-connected layer that produces an hypothesis on the output space, see Figure 2b.

ol = F (Hl)WOl

(6)

We consider two different dimensionality reductions: (i) a channel-wise inner product by WF1×n, where WF is a dimensionality reduction projection matrix with n the number of input channels; and (ii) an average pooling layer. We empirically found (i) to work slightly better than (ii) but at a higher computational cost. WF is shared across all attention heads in an attention module.

3.3 ATTENTION GATES
Each attention module makes a class hypothesis given its local information. However, in some cases, the local features are not good enough to output a good hypothesis. In order to alleviate this problem, we make each attention module, as well as the network output, to predict a confidence score c by means of an inner product by the gate weight matrix WG:

cl = tanh(F (Hl)WGl ).

(7)

The gate weights g are then obtained by normalizing the set of scores by means of a sof tmax function:

gkl =

eclk

|G| i=1

eci

,

(8)

where |G| is the total number of gates, and ci is the ith confidence score from the set of all confidence scores. The final output of the network is the weighted sum of the output heads:

output = gnet · outputnet +

gkl · ohl .

h in{1..|H|} k{1..K}

(9)

Please

note

that

setting

the

output

of

G

to

1 |G|

,

corresponds

to

averaging

all

the

outputs.

Likewise,

setting {G \ Goutput} = 0, Goutput = 1, i.e. the set of attention gates is set to zero and the output

gate to one, corresponds to the original pre-trained model without attention.

In Table 2 we show the importance of each submodule of our proposal on WRN. Attention modules are placed after each pooling layer, starting from the fourth and going backward when AD increases.

5

Under review as a conference paper at ICLR 2018

AD AW G 

11 21 22 22

+1.2 +1.4 +1.5 +1.6

Table 2: Average performance impact across datasets on (in accuracy %) of the attention depth (AD), attention width (AW ), and the presence of gates (G) on WRN.

As it can be seen, just adding a single attention module with a single attention head is enough to increase the mean accuracy by 1.2%. Adding extra heads and gates increase an extra 0.1% each.
The fact that the attention mask is generated by just one 1 × 1 convolution and the direct connection to the output makes the module fast to learn, thus being able to generate foreground masks from the beginning of the training and refining them during the following epochs. A sample of these attention masks for each dataset is shown on Figure 3.

(a) (b) (c)
(d) (e) (f)
Figure 3: Attention masks for each dataset: (a) dogs, (b) cars, (c) gender, (d) birds, (e) age, (f) food. As it can be seen, the masks help to focus on the foreground object. In (c), the attention mask focuses on ears for gender recognition, possibly looking for earrings. In the next section, we test the performance of the previously described mechanisms on different datasets.
4 EXPERIMENTS
In this section, we demonstrate the effectiveness of the proposed mechanisms for fine-grained recognition on five different datasets: and age and gender (Adience dataset by Eidinger et al. (2014)), birds (CUB200-2011 by Wah et al. (2011)), Stanford cars Krause et al. (2013), Stanford dogs (Khosla et al. (2011)), and food (UECFOOD-100 by Matsuda et al. (2012)). In order to demonstrate that the proposed generalized attention can easily augment any recent architecture, we have trained a strong baseline, namely a Wide Residual Network (WRN) (Zagoruyko & Komodakis (2016)) pre-trained on the ImageNet. The modules described in the previous sections have been implemented on pytorch, and trained in a single workstation with two NVIDIA 1080Ti.
6

Under review as a conference paper at ICLR 2018

(a) Adience dataset.

(b) CUB200-2011.

(c) Stanford Cars.

(d) Stanford dogs.

(e) UEC Food100.

Figure 4: Samples from the five fine-grained datasets.

All the experiments are trained for 100 epochs, with a batch size of 64. The learning rate is first set to 10-3 to all layers except the attention modules and the classifier, for which it ten times higher. The learning rate is reduced by a factor of 0.5 every 30 iterations and the experiment is automatically stopped if a plateau is reached. The network is trained with standard data augmentation, i.e. random 224 × 224 patches are extracted from 256 × 256 images with random horizontal flips2.
For the sake of clarity and since the aim of this work is to demonstrate that the proposed mechanism universally improves CNNs for fine-grained recognition, we follow the same training procedure in all datasets. Thus, we do not use 512 × 512 images which are central in STNs, RA-CNNs, or B-CNNs to reach state of the art performances. Accordingly, we do not perform color jitter and other advanced augmentation techniques such as the ones used by Hassannejad et al. (2016) for food recognition. The proposed method is able to obtain state of the art results in Adience Gender, Stanford dogs and UEC Food-100 even when trained with lower resolution.
In the following subsections the proposed approach is evaluated on the five datasets.
4.1 ADIENCE DATASET
The adience dataset consists of 26.5 K images distributed in eight age categories (02, 46, 813, 1520, 2532, 3843, 4853, 60+), and gender labels. A sample is shown in Figure 4a.
The performance on this dataset is measured by both the accuracy in gender and age recognition tasks using 5-fold cross-validation in which the provided folds are subject-exclusive. The final score is given by the mean of the accuracies of the five folds. This dataset is particularly challenging due to the high level of deformation of face pictures taken in the wild, occlusions and clutter such as sunglasses, hats, and even multiple people in the same image.
As it can be seen in Table 3, the Wide ResNet augmented with generalized attention surpasses the baseline performance, etc.
4.2 CALTECH-UCSD BIRDS 200
The birds dataset (see Figure 4b) consists of 6K train and 5.8K test bird images distributed in 200 categories. The dataset is especially challenging since birds are in different poses and orientations, and correct classification often depends on texture and shape details. Although bounding box, bough
2The code will be publicly available on github.
7

Under review as a conference paper at ICLR 2018

Model Publication

DSP age gender

CNN VGG-16 FAM DEX WRN WRNA

Levi & Hassner (2015) Ozbulak et al. (2016) Rodr´iguez et al. (2017) Rothe et al. (2016) Zagoruyko & Komodakis (2016) This work

50.7 86.8 57.9 61.8 93.0 64.0 58.6 93.9 59.7 94.8

Table 3: Performance on the adience dataset. DSP indicates Domain-Specific Pre-training, i.e. pretraining on millions of faces.

segmentation, and attributes are provided, we perform raw classification as done by Jaderberg et al. (2015).

Model
FCAN PD B-CNN STN RA-CNN WRN WRNA

Publication
Liu et al. (2016) Zhang et al. (2016) Lin et al. (2015) Jaderberg et al. (2015) Fu et al. (2017) Zagoruyko & Komodakis (2016) This work

High Res.

Accuracy
82.0 82.6 84.1 84.2 85.3 81.0 82.0

Table 4: Performance on Caltech-UCSD Birds 200. High Res. indicates whether training is performed with images with resolution higher than 224 × 224.

In Table 4, the performance of our approach is shown in context with the state of the art. Please note that even our approach is trained in lower resolution crops, i.e. 224 × 224 instead of 448 × 448, we
reach the same accuracy as the recent fully convolutional attention by Liu et al. (2016).

4.3 STANFORD CARS
The Cars dataset contains 16K images of 196 classes of cars, see Figure 4c. The data is split into 8K training images and 8K testing images. The difficulty of this dataset resides in the identification of the subtle differences that distinguish between two car models.

Model
DVAN FCAN B-CNN RA-CNN WRN WRNA

Publication
Zhao et al. (2017b) Liu et al. (2016) Lin et al. (2017) Fu et al. (2017) Zagoruyko & Komodakis (2016) This work

High Res.

Accuracy
87.1 89.1 91.3 92.5 87.8 90.0

Table 5: Performance on Stanford Cars. High res. indicates that resolutions higher than 256 × 256 are used.

In Table 5 the performance of our approach with respect to the baseline and other state of the art is shown. The augmented WRN shows better performance than the baseline, and even surpasses recent approaches such as FCAN.

4.4 STANFORD DOGS
The Stanford Dogs dataset consists of 20.5K images of 120 breeds of dogs, see Figure 4d. The dataset splits are fixed and they consist of 12k training images and 8.5K validation images. Pictures

8

Under review as a conference paper at ICLR 2018

are taken in the wild and thus dogs are not always a centered, unique, pose-normalized object in the image but a small, cluttered region.

Model
VGG-16 DVAN FCAN RA-CNN WRN WRNA

Publication
Simonyan & Zisserman (2014) Zhao et al. (2017b) Liu et al. (2016) Fu et al. (2017) Zagoruyko & Komodakis (2016) This work

High Res.

Accuracy
76.7 81.5 84.2 87.3 89.6 89.9

Table 6: Performance on Stanford Dogs. High res. indicates that resolutions higher than 256 × 256 are used.

Table 6 shows the results on Stanford dogs. As it can be seen, performances are low in general and nonetheless, our model was able to increase the accuracy by a 0.3% (0.1% w/o gates), being the highest score obtained on this dataset to the best of our knowledge. This performance has been achieved thanks to the gates, which act as a detection mechanism, giving more importance to those attention masks that correctly guessed the position of the dog.

4.5 UEC FOOD 100
UEC Food100 is a Japanese food dataset with 14K images of 100 different dishes, see Figure 4e. Pictures present a high level of variation in the form of deformation, rotation, clutter, and noise. In order to follow the standard procedure in the literature (e.g. Chen & Ngo (2016); Hassannejad et al. (2016)), we use the provided bounding boxes to crop the images before training.

Approach
DCNN-FOOD VGG Inception V3 WRN WRNA

Publication
Yanai & Kawano (2015) Chen & Ngo (2016) Hassannejad et al. (2016) Zagoruyko & Komodakis (2016) This work

Accuracy
78.8 81.3 81.5 84.3 85.5

Table 7: Performance on UEC Food-100.

Table 7 shows the performance of our model compared to the state of the art. As it can be seen, our model is able to improve the baseline by a relative 7% with a 85.5% of accuracy, the best-reported result compared to previous publications.

5 CONCLUSION
We have presented a novel attention mechanism to improve CNNs for fine-grained recognition. The proposed mechanism finds the most informative parts of the CNN feature maps at different depth levels and combines them with a gating mechanism to update the output distribution.
Unlike previous work, the proposed mechanism is modular, architecture independent, fast, and simple and yet WRN augmented with it show higher accuracy in each of the following tasks: Age and Gender Recognition (Adience dataset), CUB200-2011 birds, Stanford Dogs, Stanford Cars, and UEC Food-100. Moreover, state of the art performance is obtained on gender, dogs, and cars.

REFERENCES
John Robert Anderson. Cognitive psychology and its implications. New York, NY, US: WH Freeman/Times Books/Henry Holt & Co, 1985.

9

Under review as a conference paper at ICLR 2018
Jingjing Chen and Chong-Wah Ngo. Deep-based ingredient recognition for cooking recipe retrieval. In ACM MM, pp. 32­41. ACM, 2016.
Robert Desimone and John Duncan. Neural mechanisms of selective visual attention. Annual review of neuroscience, 18(1):193­222, 1995.
Eran Eidinger, Roee Enbar, and Tal Hassner. Age and gender estimation of unfiltered faces. IEEE Transactions on Information Forensics and Security, 9(12):2170­2179, 2014.
Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition. In CVPR, 2017.
Hamid Hassannejad, Guido Matrella, Paolo Ciampolini, Ilaria De Munari, Monica Mordonini, and Stefano Cagnoni. Food image recognition using very deep convolutional networks. In MADIMA Workshop, pp. 41­49. ACM, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, pp. 2017­2025, 2015.
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In CVPR Workshop on Fine-Grained Visual Categorization (FGVC), volume 2, pp. 1, 2011.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In CVPR, pp. 554­561, 2013.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Gil Levi and Tal Hassner. Age and gender classification using convolutional neural networks. In CVPR Workshops, pp. 34­42, 2015.
Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained visual recognition. In ICCV, pp. 1449­1457, 2015.
Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear convolutional neural networks for fine-grained visual recognition. TPAMI, 2017.
Xiao Liu, Tian Xia, Jiang Wang, and Yuanqing Lin. Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition. arXiv preprint arXiv:1603.06765, 2016.
Y. Matsuda, H. Hoashi, and K. Yanai. Recognition of multiple-food images by detecting candidate regions. In ICME, 2012.
Gokhan Ozbulak, Yusuf Aytar, and Hazim Kemal Ekenel. How transferable are cnn-based features for age and gender classification? In BIOSIG, pp. 1­6. IEEE, 2016.
Pau Rodr´iguez, Guillem Cucurull, Josep M Gonfaus, F Xavier Roca, and Jordi Gonza`lez. Age and gender recognition in the wild with deep attention. PR, 2017.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. IJCV, July 2016.
Olga Russakovsky, Jia Deng, Jonathan Krause, Alex Berg, and Li Fei-Fei. The imagenet large scale visual recognition challenge 2012 (ilsvrc2012), 2012.
Pierre Sermanet, Andrea Frome, and Esteban Real. Attention for fine-grained categorization. In ICLR, 2015.
10

Under review as a conference paper at ICLR 2018
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
R Uijlings, A van de Sande, Theo Gevers, M Smeulders, et al. Selective search for object recognition. IJCV, 104(2):154, 2013.
Sabine Kastner Ungerleider and Leslie G. Mechanisms of visual attention in the human cortex. Annual review of neuroscience, 23(1):315­341, 2000.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical report, 2011.
Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. arXiv preprint arXiv:1704.06904, 2017.
Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and Zheng Zhang. The application of two-level attention models in deep convolutional neural network for fine-grained image classification. In CVPR, pp. 842­850, 2015.
Keiji Yanai and Yoshiyuki Kawano. Food image recognition using deep convolutional network with pre-training and fine-tuning. In ICMEW, pp. 1­6. IEEE, 2015.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based r-cnns for fine-grained
category detection. In ECCV, pp. 834­849. Springer, 2014. Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, and Qi Tian. Picking deep filter
responses for fine-grained image recognition. In CVPR, pp. 1134­1142, 2016. Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. A survey on deep learning-based fine-grained
object classification and semantic segmentation. IJAC, pp. 1­17, 2017a. Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng Yan. Diversified visual attention
networks for fine-grained object classification. IEEE Transactions on Multimedia, 19(6):1245­ 1256, 2017b.
11

Under review as a conference paper at ICLR 2018

APPENDIX

test_accuracy

0.64 0.62 0.60 0.58 0.56 0.54 0.52 WRNA
WRN 0.50 0 10 20 30 40 50 60
epoch
(a) Adience age accuracy curve (fold0).
0.84 0.82 0.80 0.78 0.76 0.74 0.72 WRNA
WRN 0.70 0 10 20 30 40 50 60
epoch
(c) CUB Birds accuracy curve.

test_accuracy

test_accuracy

0.98
0.96
0.94
0.92
0.90 WRNA WRN
0.88 0 10 20 30 40 50 60 epoch
(b) Adience gender accuracy curve (fold0).
0.95 0.90 0.85 0.80 0.75 0.70 0.65 WRNA
WRN 0.60 0 10 20 30 40 50 60
epoch
(d) Stanford Cars accuracy curve.

test_accuracy

test_accuracy

0.92 0.90 0.88 0.86 0.84 0.82 0.80 0.78 WRNA 0.76 WRN
0 10 20 30 40 50 60 epoch
(e) Stanford Dogs accuracy curve.

test_accuracy

0.85
0.80
0.75
0.70
0.65 WRNA WRN
0.60 0 10 20 30 40 50 60 epoch
(f) UEC Food-100 accuracy curve.

Figure 5: Test accuracy logs for the five fine-grained datasets. As it can be seen, the augmented models (WRNA) achieve higher accuracy at similar convergence rates. For the sake of space we only show one of the five folds of the Adience dataset.

12

