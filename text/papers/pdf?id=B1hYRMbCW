On the regularization of WGANs
ON THE REGULARIZATION OF WASSERSTEIN GANS
ABSTRACT
Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.
1 INTRODUCTION
General adversarial networks (GANs) Goodfellow et al. (2014) are a class of generative models that have recently gained a lot of attention. They are based on the idea of defining a game between two competing neural networks (NNs): a generator and a classifier (or discriminator). While the classifier aims at distinguishing generated from real data, the generator tries to generate samples which the classifier can not distinguish from the ones from the empirical distribution. Realizing the potential behind this new approach to generative models, different more recent contributions focused on the stabilization of training, including ensemble methods Tolstikhin et al. (2017), improved network structure Radford et al. (2015); Salimans et al. (2016a) and theoretical improvements Nowozin et al. (2016); Salimans et al. (2016b); Arjovsky & Bottou (2017); Chen et al. (2016), which helped to successfully model complex distributions using GANs.
It was proposed by Arjovsky et al. (2017) to train generator and discriminator networks by minimizing the Wasserstein-1 distance, a distance with properties superior to the Jensen-Shannon distance (used in the original GAN) in terms of convergence. Accordingly, this version of GAN was called Wasserstein GAN (WGAN). The change of metric introduces a new minimization problem, which requires the discriminator function to lie in the space of 1-Lipschitz functions. In the same paper, the Lipschitz constraint was guaranteed by performing weight clipping, i.e., by constraining the parameters of the discriminator NN to be smaller than a given value in magnitude. An improved training strategy was proposed by Gulrajani et al. (2017) based on results from optimal transport theory (see Villani (2008)). Here, instead of weight clipping, the loss gets augmented by a regularization term that penalizes any deviation of the norm of the gradient of the critic function (with respect to its input) from one.
We review these results and present both theoretical considerations and empirical results, leading to the proposal of a less restrictive regularization term for WGANs.1 More precisely, our contributions are as follows:
· We review the arguments that the regularization technique proposed by Gulrajani et al. (2017) is based on and make the following two observations: (i)The regularization strategy requires training samples and generated samples to be drawn from a certain joint distribution. However, in practice samples are drawn independently from their marginals. (ii) The arguments further assume the discriminator to be differentiable. We explain why both can be harmful for training.
1In the blog post https://lernapparat.de/improved-wasserstein-gan/ which was written simultaneously to our work, the author presents some ideas that follow a similar intuition as the one underlying our arguments.
1

On the regularization of WGANs

· We propose a less restrictive regularization term and present empirical results strongly supporting our theoretical considerations.

2 OPTIMAL TRANSPORT

We will require the notion of a coupling of two probability distributions. Although a coupling can
be defined more generally, we state the definition in the setting of our interest, i.e., we consider all spaces involved to equal Rn.
Definition 1. Let µ and  be two probability distributions on Rn. A coupling  of µ and  is a probability distribution on Rn × Rn such that (A, Rn) = µ(A) and (Rn, A) = (A) for all measurable sets A  Rn. The set of all couplings of µ and  is denoted by (µ, ).

Let us now recall the Kantorovich duality. Note, that the presented theorem is a less general, but to our needs adapted version of Theorem 5.10 from Villani (2008)2. A proof of how to derive our
version from the referenced one can be found in Appendix C.1. We will denote by Lip1 the set of all 1-Lipschitz functions, i.e., the set of all functions f such that f (y) - f (x)  ||x - y||2 for all
x, y.

Theorem 1 (Kantorovich). Let µ and  be two probability distributions on Rn such that Rn ||x||2 dµ(x) <  and Rn ||x||2 d(x) < . Then

(i)

min ||x - y||2 d(x, y) = max

(µ,) Rn×Rn

f Lip1

f (x) dµ(x) - f (x) d(x)
Rn Rn

.

(1)

In particular, both minimum and maximum exist.

(ii) The following two statements are equivalent:

(a)  is an optimal coupling (minimizing the value on the left hand side of (1)). (b) Any optimal function f   Lip1 (at which the maximum is attained for the right hand side
of (1)) satisfies that for all (x, y) in the support of : f (x) - f (y) = ||x - y||2.

3 WASSERSTEIN GANS

Formally, given an empirical distribution µ, a class of generative distributions  over some space X , and a class of discriminators d : X  [0, 1], GAN training Goodfellow et al. (2014) aims at solving the optimization problem3 given by min maxd Exµ[log(d(x))] + Ey [log(1 - d(y))]. In practice, the parameters of the generator and the discriminator networks are updated in an alternating fashion based on (several steps) of stochastic gradient descent. The discriminator thereby tries to assign a value close to zero to generated data points and values close to one to real data points. As an opposing agent, the generator aims to produce data where the discriminator expects to see real data. Theorem 1 by Goodfellow et al. (2014) shows that, if the optimal discriminator is found in each iteration, minimization of the resulting loss function of the generator leads to minimization of the Jensen-Shannon (JS) divergence. Instead of minimizing the JS divergence, Arjovsky et al. (2017) proposed to minimize the Wasserstein-1 distance, also known as Earth-Mover (EM) distance, which is defined for any Polish space (M, d) and probability distributions µ and  on M by

W (µ, ) = inf

d(x, y) d(x, y) .

(µ,) M ×M

(2)

From the Kantorovich duality (see Theorem 1, (i)) it follows that, in the special case we are consider-

ing, the infimum is attained and, instead of computing the minimum in Equation (2), the Wasserstein-

1 distance can also be computed as

W (µ, ) = max Exµ[f (x)] - Ey [f (y)] ,
f Lip1

(3)

2In Arjovsky et al. (2017), the theorem is called Kantorovich-Rubinstein, although this theorem only applies to compact metric spaces. There are several generalizations of the Kantorovich-duality. For a detailed account we refer the reader to Edwards (2011).
3Usually, both the generative distribution and the discriminator are modeled by NNs, whose structure determine the two classes we are optimizing over.

2

On the regularization of WGANs

where the maximum is taken over the set of all 1-Lipschitz functions Lip1. Thus, the WGAN objective is to solve

min


max
f Lip1

Exµ

[f

(x)]

-

Ey

[f

(y)]

,

(4)

which can be achieved by alternating gradient descent updates for the generating network and the 1Lipschitz function f (also modeled by a NN), just as in the case of the original GAN. The objective of the generator is still to generate real-looking data points and is led by function values of f that plays the role of an appraiser (or critic). The appraiser's goal is to assign a value of confidence to each data point, which is as low as possible on generated data points and as high as possible on real data. The confidence value it can assign is bounded by a constraint of similarity, where similarity is measured by the distance of data points. This can be motivated by the idea that similar points should have similar values of confidence for being real. The new role of the critic aims to solve convergence problems, but the interpretation of the absolute value as real (close to 1) and fake data (close to 0) is lost. We refer to Appendix A for a detailed discussion.

4 IMPROVED TRAINING OF WGANS

Modeling the WGAN critic function by a NN raises the question on how to enforce the 1-Lipschitz constraint of the objective in Equation (4). As proposed by Arjovsky et al. (2017) a simple way to restrict the class of functions f that can be modeled by the NN to -Lipschitz continuous functions (for some ) is to perform weight clipping, i.e. to enforce the parameters of the network not to exceed a certain value cmax > 0 in absolute value. As the authors note, this is not a good but simple choice. We further demonstrate this in Appendix B by proving (for a standard NN architecture) that, using weight clipping, the optimal function is in general not contained in the class of functions modeled by the network.

Recently, an alternative to weight clipping was proposed in Gulrajani et al. (2017). The basic idea

is to augment the WGAN loss by a regularization term that penalizes the deviation of the gradient

norm of the critic with respect to its input from one (leading to a variant referred to as WGAN-GP,

where GP stands for gradient penalty.) More precisely, the loss of the critic to be minimized is then

given by

Ey [f (y)] - Exµ[f (x)] + Ex^ [(||f (x^)||2 - 1)2] ,

(5)

where  is the distribution of x^ = tx + (1 - t)y for t  U [0, 1]. The regularization term is derived based on the following result.

Proposition 1. Let µ and  be two probability distributions on Rn. Let f  be an optical critic,

leading to the coupling with

maximum respect to

maxf Lip1 min(µ,)

Rn f (x)
Rn ×Rn

dµ(x) - ||x - y||2

Rn f (x) d(x, y)

d(x) , and let  be an optimal . If f  is differentiable and xt =

tx + (1 - t)y for 0  t  1, it holds that P(x,y)

(f (xt)

=

y-xt ||y-xt

||

)

= 1 . This in particular

implies, that the norms of the gradients are one -almost surely on such points xt.

For the convenience of the reader, we provide a simple argument for obtaining this result in Appendix C.2.
Note, that Proposition 1 holds only when f  is differentiable and x and y are sampled from the optimal coupling . However, sampling independently from the marginal distributions µ and  very likely results in points (x, y) that lie outside the support of . Furthermore, the optimal cost function f  does not need not to be differentiable everywhere. These two points will be discussed in more detail in the following subsections.

4.1 SAMPLING FROM THE MARGINALS INSTEAD OF THE OPTIMAL COUPLING
Observation 1. Suppose f   Lip1 is an optimal critic function and  the optimal coupling determined by the Kantorovich duality in Theorem 1. Then |f (y) - f (xt)| = ||xt - y||2 on the line xt = tx + (1 - t)y, 0  t  1, for (x, y) sampled from , but not necessarily on the lines connecting an arbitrary pair of a real and a generated data point.

3

On the regularization of WGANs

Consider the examples in Figure 1, where every X denotes a sample from the generator and every O a real data sample. Optimal couplings  are indicated in red, and values of an optimal critic function are indicated in blue (optimality is shown in AppendixA.1).

an optimal f*

2 XO 01

OXOXOX XOXOXOXO

1 XO a a+1 
f* optimal for a  [1 - 2, 2 - 1]

01234567

12

Figure 1: Left: f (O)-f (X)=|O-X| only holds for coupled pairs (X,O)  Right: A 2-dimensional example showing that f (O)-f (X)=|O-X| only holds for coupled pairs (X,O).

In the one-dimensional example on the left, the left-most X and the right-most O satisfy f (O) -

f (X)

=

1 7

|O

-

X|

=

|O - X|,

illustrating

that

the

basis

for

the

derivation

of

the

condition,

that

the

norm of the gradient equals one between generated and real points, only holds for points sampled

from the optimal coupling. Note, while here the gradient is still of norm 1 almost everywhere, this

does not hold in higher dimensions, where not all points lie on a line between some pair of points sampled from . This is exemplified for two dimensions on the right side of Figure 1, where blue

numbers and a  R denote the values of an optimal critic function at these points (the values at

these points is all that matters). We fixed the value at position (1, 2) to be zero, taking into account

that an optimal critic function remains optimal under addition of an arbitrary constant. Since the Lipschitzconstraint of f  must be satisfied, we get 1 - a  2 and a + 1  2. Therefore a  [1 - 2, 2 - 1] and one of the inequalities of the Lipschitz constraint must be strict.

4.2 DIFFERENTIABILITY OF THE CRITIC
Observation 2. The assumption of differentiability is not valid at points of interest.
Consider the example of two discrete probability distributions and its optimal critic function f  shown on the left in Figure 2, which is an excerpt of the example in Figure 1. We can see that the indicated function f (x) = 1 - |x|  Lip1 is optimal as it leads to an equality in the equation of the Kantorovich dual. (Also, it is the only continuous function, up to a constant, that realizes f (y) - f (x) = |y - x| for coupled points (x, y).) However, it is not differentiable at 0.

an optimal f*

1

O XO

X

an optimal f*

-1 0

1

Figure 2: Non-differentiable optimal critic functions f (shown in blue). Left: For two discrete distributions: Circles and crosses belong to samples from the empirical distribution and the generative model, respectively. A differentiable approximating function in shown in green. Right: For two continuous distributions: The empirical distribution µ is shown in gray, the generative distribution  is shown in green.

The counterexample can be made continuous by considering the points as the center points of Gaussians, as illustrated on the right on Figure 2. This is formalized by the following proposition showing that the the critic indicated in blue is indeed optimal for the depicted gray Gaussian of real data and the green mixture of two Gaussians of generated data.

Proposition 2. Let µ = N (0, 1) be a normal distribution centered around zero and  = -1 + 1

be

a

mixture of the

two normal distributions

-1

=

1 2

N

(-1,

1)

and 1

=

1 2

N

(1,

1)

over

the

real

4

On the regularization of WGANs

line. If µ describes the distribution of real data and  describes the distribution of the generative model, then the optimal critic function is given by (x) = -|x|.
The proof can be found in Supplement C.3.
The issue with non-differentiability can be generalized to higher-dimensional spaces based on the observation that an optimal coupling is in general not deterministic as defined in the following. Definition 2. Let (X, µ) and (Y, ) be two probability spaces. A coupling   (µ, ) is called deterministic if there is a measurable function  : X  Y such that supp()  {(x, (x)) | x  X}.
We can now formulate the following observation. Observation 3. Suppose  is a non-deterministic optimal coupling between two probability distributions over Rn so that there exist points (x, y) and (x, y ) in supp(). Suppose further that there is no  > 0 with (y - x) =  · (y - x) (in particular this implies y = y ). Then any optimal critic function f  is not differentiable at x.
The arguments can be found in Supplement C.4.
The critic function generated by the NN is (almost) everywhere differentiable (depending on the activation functions). By the Stone-Weierstrass theorem, on compact sets, we can approximate any (Lipschitz-)continuous function by differentiable functions uniformly. Close to a point of nondifferentiability, it seems to be a strong constraint on an approximating function to have a gradient of norm of one in the neighborhood of a non-differentiability (cf. Figure 2 (a)). Therefore, we argue ­ in contrast to the argumentation of Gulrajani et al. (2017) ­ that the gradient should not be assumed to equal one for arbitrary points on the line between x and y sampled from .

5 HOW TO REGULARIZE WGANS

In the following, we will discuss how the regularization of WGANs can be improved.

Penalizing the violation of the Lipschitz constraint. For the critic function, we have nothing more at hand than the inequality of the Lipschitz-constraint. Moreover (as shown in Lemma 1 in the supplementary material) the exhaustion of the Lipschitz constant is automatic by maximizing the objective function. Therefore, a natural choice of regularization is to penalize the given constraint directly, i.e., sample two points x  µ and y   and add the regularization term

max 0, |f (x) - f (y)| - 1 2 ||x - y||2

(6)

to the cost function. (We square to penalize larger deviations more than smaller ones.)

Alternatively, since the NN generates (almost everywhere) differentiable functions, we can penalize
whenever gradient norms are strictly larger than one, an option referred to as "one-sided penalty"and shortly discussed as an alternative to penalizing any deviation from one by Gulrajani et al. (2017)4.
Note that enforcing the gradient to be smaller than one in norm has the advantage that we penalize when the partial derivative has norm > 1 into the direction of steepest descent. Hence, all
partial derivatives are implicitly enforced to be bounded in norm by one, too. At the same time, enforcing  1 for the gradient of smooth approximating functions is not an unreasonable constraint
even at points of non-differentiability. For these reasons we suggest to add the regularization term max {0, ||f (x^)|| - 1} 2 to the cost function. Different ways of sampling the point x^ are ana-
lyzed in Supplement D.4. Thus, our proposed method (WGAN-LP, where LP stands for Lipschitz
penalty) alternates between updating the discriminator to minimize

Ey [f (y)] - Exµ[f (x)] + Ex^ [(max {0, ||f (x^)|| - 1})2] ,

(7)

(where  depends on the concrete sampling strategy chosen) and updating the generator network modeling  to minimize -Ey[f (y)] using gradient descent.

4While the authors note that " In practice, we found this [using the GP or two-sided penalty] to converge slightly faster and to better optima. Empirically this seems not to constrain the critic too much...", our experiments point towards another conclusion.

5

On the regularization of WGANs
A more general view. The Kantorovich duality theorem holds in a quite general setting. For example, a different metric can be substituted for the Euclidean distance ||·||2. Taking ||·||p2 for a different natural number p for example leads to the minimization of the Wasserstein distance of order p (i.e., the Wasserstein-p distance). Based on the dual problem to the computation of the Wasserstein distance of order p (as given by the Kantorovich duality theorem) we still need to maximize Equation (4) with the only difference that 1-Lipschitz-continuity is now measured with respect to ||·||22. For our training method this entails that the only modification to make is to use the regularization term given by (6), where the Euclidean distance is replaced by the metric of interest. We provide experimental results in Supplement D.5.
Recently, in Bellemare et al. (2017), the Wasserstein distance was replaced by the energy distance 5. For the training of Cramer GANs, the authors apply the GP-penalty term proposed by Gulrajani et al. (2017). We expect that using the LP-penalty term instead is also beneficial for Cramer GANs.
6 EXPERIMENTS
We perform several experiments on three toy data sets, 8Gaussians, 25Gaussians, and Swiss Roll 6, to compare the effect of different regularization terms. More specifically, we compare the performance of WGAN-GP and WGAN-LP as described in Equations (5) and (7) respectively, where the penalty was applied to points randomly sampled on the line between the training sample x and the generated sample y. Other sampling methods are discussed in Supplement D.4.
Both, the generator network and the critic network, are simple feed-forward NNs with three hidden Leaky ReLU layers, each containing 512 neurons, and one linear output layer. The dimensionality of the latent variables of the generator network was set to two. During training, 10 critic updates are performed for every generator update, except for the first 25 generator updates, where the critic is updated 100 times for each generator update in order to get closer to the optimal critic in the beginning of training. Both networks were trained using RMSprop (Tieleman & Hinton (2012)) with learning rate 5 · 10-5 and a batch size of 256.
To see whether our findings on toy data sets can be transferred to real world settings, we trained bigger WGAN-GPs and WGAN-LPs on MNIST and CIFAR-10 as it is described below.
Level sets of the critic. A qualitative way to evaluate the learned critic function for a twodimensional data set is by displaying its level sets, as it was done by Gulrajani et al. (2017) and Kodali et al. (2017). The level sets after 10, 50, 100 and 1000 training iterations of a WGAN trained with the GP and LP penalty on the Swiss Roll data set are shown in Figure 3. Similar experimental results for the 8Gaussians and 25Gaussian data sets can be found in Supplement D.1. It becomes clear that with a penalty weight of  = 10, which corresponds to the hyperparameter value suggested by Gulrajani et al. (2017), the WGAN-GP does neither learn a good critic function nor a good model of the data generating distribution. With a smaller regularization parameter,  = 1, learning is stabilized. However, with the LP-penalty a good critic is learned even with a high penalty weight in only a few iterations and the level sets show higher regularity. Training a WGAN-LP with lower penalty weight led to equivalent observations (results not shown). We also experimented with much higher values for , which led to almost the same results as for  = 10, which emphasizes that LP-penalty based training is less sensitive to the choice of .
Evolution of the critic loss. To yield a fair comparison of methods applying different regularization terms, we display values of the critic's loss functions without the regularization term throughout training. Results for WGAN-GPs and WGAN-LPs are shown in Figure 4.
The optimization of the critic with the GP-penalty and  = 5 is very unstable: the loss is oscillating heavily around 0. When we use the LP-penalty instead, the critic's loss smoothly reduces to zero, which is what we expect when the generative distribution  steadily converges to the empirical distribution µ. Also note that we would expect the negative of the critic's loss to be slightly positive, as a good critic function assigns higher values to real data points x  µ and lower values to generated
5The energy distance (Sze´kely & Rizzo (2013)) has the same convergence properties as the Wasserstein distance, but additionally satisfies that the sample based gradient approximation does not have a bias.
6The same data sets were also used in the analysis of Gulrajani et al. (2017)
6

On the regularization of WGANs
Figure 3: Level sets of the critic f of WGANs during training, after 10, 50, 100, and 1000 iterations. Yellow corresponds to high, purple to low values of f . Training samples are indicated in yellow, generated samples in green, and samples used for the penalty term in red. Top: GP-penalty with  = 10. Middle: GP-penalty with  = 1. Bottom: LP-penalty with  = 10.
points y  . This is exactly what we observe when using the LP-penalty Interestingly, when using the LP-penalty in combination with a very high penalty weight, like  = 100, we obtain the same results, indicating that the constraint is always fulfilled for  = 10 already. Using  = 1 in combination with the GP-penalty on the other hand stabilized training but still results in fluctuations in the beginning of the training (results shown in Supplement D.2).
Figure 4: Evolution of the WGAN critic's negative loss (without the regularization term) for  = 5. Median results over the 20 runs (blue area indicates quantiles, green dots outliers). Right: For the GP-penalty. Left: For the LP-penalty.
Estimating the Wasserstein distance. In order to estimate how the actual Wasserstein distance between the real and generated distribution evolves during training, we compute the cost of minimum assignment based on Euclidean distance between sets of samples from the real and generated distributions, using the Kuhn-Munkres algorithm Kuhn (1955). We use a sample set size of 500 to maintain reasonable computation time and estimate the distance every 10th iteration over the course of 500 iterations. All experiments were repeated 10 times for different random seeds. From the results for WGAN-GP and WGAN-LP with  = 5 shown in Figure 5, we conclude that the proposed LP-penalty leads to smaller estimated Wasserstein distance and less fluctuations during training. When training WGAN-GPs with a regularization parameter of  = 1, training is stabilized as well (see Supplement D.3), indicating that the effect of using a GP-penalty is highly dependent on the right choice of .
7

On the regularization of WGANs

Figure 5: Evolution of the approximated Wasserstein-1 distance during training of WGANs ( = 5, median results over 10 runs). Left: For the GP-penalty. Right: For the LP-penalty

METHOD PENALTY WEIGHT SCORE (unconditional) SCORE (conditional)

WGAN-GP WGAN-LP WGAN-GP WGAN-LP

10 10 200 200

7.989 7.472 7.721

8.537 8.462 ­ ­

Table 1: Inception Score on CIFAR-10, maximal values

Sample quality on CIFAR-10. We trained WGANs with the same ResNet generator and discriminator and the same hyperparameters as Gulrajani et al. (2017) and computed the Inception score Salimans et al. (2016b) throughout training (plots can be found in Appendix D.6). The final score reached after 100000 training iterations with different regularization parameters are reported in Table 1. WGAN-LP reaches the same Inception score as WGAN-GP with a penalty weight of 10, while being more stable to other choices of this hyperparameter. While achieving equivalent results with respect to the Inception score, we experienced more stable behavior in the loss on a validation set.
7 CONCLUSION
For stable training of Wasserstein GANs, we propose to use the following penalty term to enforce the Lipschitz constraint that appears in the objective function:
Ex^ [(max {0, ||f (x^)|| - 1})2] .
We presented theoretical and empirical evidence that this gradient penalty performs better than the previously considered approaches of clipping weights and of applying the stronger gradient penalty given by Ex^ [(||f (x^)||2 - 1)2]. In addition to more stable learning behavior, the proposed regularization term leads to lower sensitivity to the value of the penalty weight  (demonstrating smooth convergence and well-behaved critic scores throughout the whole training process for different values of ). We assume that the proposed penalty will also lead to improvements when modeling real world data, which we will investigate in future work.
REFERENCES
Mart´in Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. CoRR, abs/1701.04862, 2017.
Mart´in Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 214­223, 2017.
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, and Re´mi Munos. The cramer distance as a solution to biased wasserstein gradients. CoRR, abs/1705.10743, 2017.
8

On the regularization of WGANs
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016.
D.A. Edwards. On the Kantorovich­Rubinstein theorem. Expositiones Mathematicae, 29(4):387 ­ 398, 2011. ISSN 0723-0869. doi: https://doi.org/10.1016/j.exmath.2011.06.005.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Mart´in Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017.
Naveen Kodali, Jacob D. Abernethy, James Hays, and Zsolt Kira. How to train your DRAGAN. CoRR, abs/1705.07215, 2017.
Harold W. Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2: 83 ­ 97, 1955.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 271­279. Curran Associates, Inc., 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016a.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 2234­2242. Curran Associates, Inc., 2016b.
Ga´bor J Sze´kely and Maria L Rizzo. Energy statistics: A class of statistics based on distances. Journal of statistical planning and inference, 143(8):1249­1272, 2013.
T. Tieleman and G. Hinton. Lecture 6.5--RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scho¨lkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2008. ISBN 9783540710509.
9

On the regularization of WGANs
A PROPERTIES OF AN OPTIMAL CRITIC FUNCTION OF WGANS
An issue of the original GAN discriminator was that it outputs zero every time it is certain to see generated data, independent on how far away a generated data point lies from the real distribution. As a consequence, locally, there is no incentive for the generator to rather generate a value closer to (but still off) the real data; GAN critic's optimal value is zero in either case. The WGAN's optimal critic function measures this distance which helps for the generated distribution to converge, but the interpretation of the absolute value as real (close to 1) and fake data (close to 0) is lost. And worse, there is even no guarantee that the relative values of the optimal critic function help to decide what is real and what is fake. Although this does not seem to cause major problems for the iterative training procedure in practice, we still consider it worthwhile to give a specific example justifying the following observation. Observation 4. The WGAN generator could learn wrong things, basing its decision on the values of the optimal critic function, i.e., if it generates at locations of high critic function values.
Consider the following setting, where the X's represent generated and the O's represent real data points.
an optimal f*

X OOOOOO

XXXXX

Figure 6: Values of the WGAN critic function for some generated data points can be higher than the crtitic's values for some real data points. Thus, fake and real points can not be distinguished based on the critics values alone. Real data points are represented by O, generated by X.

An optimal coupling in this example is quite obvious: We connect the left-most O with the X on the left, and then extend by an arbitrary matching of the other O's with the other X's. It is then not hard to verify that the indicated critic function with slope 1 or -1 almost everywhere leads to an equality in the Kantorovich duality and hence is optimal.
The value of the critic function at the left-most 'X' is higher than the value at the right-most 'O', suggesting to generate images at the wrong position.
This issue might be fixed by the alternating updates of generator and critic at a later stage of training when less X's are generated so far on the right side of the O's. The critic function will then flatten the peak, eventually assigning a lower value to an X on the left than to any of the O's.
Remark 1. One can show that the same holds (with only a slight change of the critic function f ) if the X's and O's denote the centers of Gaussians. This can be shown with similar arguments as those in the proofs in Supplement C.3.

A.1 PROVING OPTIMALITY OF CERTAIN COMBINATIONS OF COUPLING AND CRITIC
FUNCTION

We show here that the coupling and critic function indicated in Figure 1 are indeed optimal.

In the one-dimensional example on the left,

R ×R

|x - y|

d(x, y)

=

1 7

(1

+

1

+

1

+

1

+

1

+

1

+

1)

=

f (x) d(x) - f (x) dµ(x) and thus  and f  are indeed optimal.

RR

10

On the regularization of WGANs

In the two-dimensional example on the right the coupling indicated in red and the critic func-

tion (described by its function values in blue) are optimal, since with this choice we have

Rn×Rn ||x - y||2 d(x, y) =

1 2

(1

+

1)

=

1 and

Rn f (y) d(y) -

Rn f (x) dµ(x) =

1 2

(1

+

a

+

1)

-

1 2

(0

+

a)

=

1.

Again,

equality

of

the

left

hand

side

and

right

hand

side

of

the

equation proves optimality on both sides.

B THE ISSUE WITH THE WEIGHT CLIPPING APPROACH

The critic function of WGAN is generated by a neural network, which raises the question on how to enforce the 1-Lipschitz constraint in the maximization problem of the objective in Equation (4). As Arjovsky et al. (2017) point out, it does not matter whether to maximize over 1-Lipschitz or Lipschitz continuous functions, since we can equivalently optimize  · W (µ, ) instead of W (µ, ). An easy consideration leads to the following lemma.
Lemma 1. The optimal critic function f  (leading to the maximum in Eq. (3)) exhausts the Lipschitz constraint for given  in the sense that there is a pair of points (x, y) such that f (x) - f (y) = ||x - y||2.

Proof. If supx=y

f (y)-f (x) ||x-y||2

=

c

<



, then

g

=

1 c

f



generates a contradiction to the opti-

mality of f . (Alternatively, in the case  = 1, it follows directly from Theorem 1, (ii), that the

transport is optimal if and only if the Lipschitz constraint of one is exhausted for any two points of

the coupling.)

Observation 5. Weight clipping is not a good strategy to enforce the Lipschitz constraint for the critic function.

First note that by clipping the weights we enforce a common Lipschitz constraint, where the common Lipschitz constant ¯ is defined as the minimal   R such that f (x)-f (y)  ||x - y||2 for all x, y and all functions f that can be generated by the network under weight clipping. The actual value of ¯ does not follow directly from the weight clipping constant cmax but can be computed from the structure of the network. From Lemma 1 we know that an optimal f  exhausts the Lipschitz constraint. We will now show exemplarily for deep NN with rectified linear unit (ReLU) activation functions that there is an extremely limited number of functions generated by the NN using weight clipping that do exhaust the implicitly given common Lipschitz constraint ¯. It follows that, in almost all cases, the optimal f  is not in the class of functions that can be generated by the network under the weight clipping constraint.
Proposition 3. Consider a (deep) NN with ReLU activation functions and linear output layer. A function generated by the NN under constraining each weight in absolute value by cmax exhausts the common Lipschitz constraint if and only if

(a) The weight matrix of the first layer consists of constant columns with value cmax or -cmax.
(b) The weights of all other layers are given by a matrix Cmma×x n with every entry equal to cmax.
Proof. We need to determine every function f  generated by the neural net, such that we can find points x = y with f (y) - f (x) = ¯||x - y||2. Recall that ¯ is defined as the minimal  satisfying f (y) - f (x)  ||x - y||2 for all functions f generated by the neural net and all points x, y.
In the following, we will denote by (f ) the Lipschitz constant of f , i.e., the smallest   R such that f (x) - f (y)  ||x - y||2 for all x, y.
Every function generated by the neural net is a composition of functions
f = fn  relu  fn-1  . . .  relu  f1.
with linear functions fi and relu denoting a layer of activation functions with rectifier linear units. Since each linear function fi is Lipschitz continuous with Lipschitz constant (fi) and relu is Lipschitz continuous with (relu) = 1, it follows that f is Lipschitz continuous with (f )  i (fi). Moreover, equality holds if there is a pair of points (x, y) such that the consecutive images witness

11

On the regularization of WGANs

the maximal Lipschitz constant (fi) and (relu) for each of the individual functions making up the composition of f . More formally, equality holds if and only if there is a tuple of pairs of points (x(i), y(i)), 1  i  n - 1, such that for all 1  i  n,

(i) x(i) = y(i)

(ii) x(i+1) = relu  fi(x(i)) and y(i+1) = relu  fi(y(i))

(iii) |fi(x(i)) - fi(y(i))| = (fi)||x(i) - y(i)||2

(iv) All entries of fi(x(i)) and fi(y(i)) are larger or equal to zero. This is equivalent to the condition that
|relu(fi(x(i))) - relu(fi(y(i)))| = (relu)||fi(x(i)) - fi(y(i))||2 .

It follows that to determine f  we need to maximize (fi) for the linear layers with weight constraint

cmax and find points shows

a sequence of that (f ) =

points (x(i), y(i)) that satisfy (i)-(iv). The existence of the

n i=1

(fi)

and

maximizing

each

(fi)

then

shows

that

sequence

of

n
(f ) = (fi) = ¯ .

i=1

Since, as we will show, the conditions in (a) and (b) maximize the Lipschitz constraint of each layer individually, the existence of suitable (x(i), y(i)) proves the if-direction of the proposition.

For the only-if direction, we will see that the ability to find the sequence of points gives restrictions on how to maximize (fi) of an individual layer, leading to the more restrictive condition of (b) for all but the first layer (cf. (a)).

So let us first maximize the Lipschitz constraint of each linear layer and then make sure that we can find the corresponding points. We write the linear layer as a matrix multiplication fi(x) = A(i)x. Using linearity,
(fi) = max ||A(i)z||2 ,
||z ||2 =1
and our goal can be reformulated to finding the matrix A(i) maximizing (fi).
For any fixed z, ||A(i)z||2 is maximized exactly when each vector entry is maximized in absolute value. Now, with A(i) = (a(ji,k) )j,k and sgn(·) denoting the sign function,

|(A(i)z)j | =

a(ji,k) zk  |a(ji,k) ||zk|  cmax|zk| (by the weight constraint)

kk

k

= (cmax · sgn(zk)) · zk
k
and equality holds if and only if A(i) or -A(i) consists of columns of constant entry with the value cmax · sgn(zk) in column k. It follows that

(fi) = max ||A(i)z||2 = max cmax · ||z||1

||z ||2 =1

||z ||2 =1

 max cmax dim(z) · ||z||2 (by Cauchy-Schwartz inequality)
||z ||2 =1

= cmax dim(z)

with

equality

if

and

only

if

zk

=

± 1
dim(z)

for

all

k.

Hence, for the first linear layer we need to choose a matrix A(1) satisfying (a) of the statement of the proposition.

Now, we find a pair (x(1), y(1)) with

x(1) - y(1) = a · (±1, ±1, . . . , ±1) for some a = 0

12

On the regularization of WGANs

such that sgn(xk(1)) = sgn(yk(1)) = sgn(xk(1) - yk(1)) = the sign of column k of A(1).

This is the only possibility to ensure (iii) and (iv) of the conditions above. Note that also (i) holds for (x(1), y(1)), and (ii) (together with (iv)) determines (x(2), y(2)) uniquely from (x(1), y(1)) as

&

x(2) = A(1)x(1) y(2) = A(1)y(1)

= cmax · ||x(1)||1 · (1, 1, ..., 1) = cmax · ||y(1)||1 · (1, 1, ..., 1)

.

We may assume that ||x(1)||1 > ||y(1)||1. (Otherwise, switch the roles of x and y. In the case of equality, we need to choose a different pair for (x(1), y(1)) not to violate (i) for (x(2), y(2)).) Then
we have that x(2) = y(2),

+1 = sgn(xk(2)) = sgn(yk(2)) = sgn(x(k2) - yk(2)) for all k.
Using the same arguments as above, it follows that for such (x(2), y(2)), to maximize the Lipschitz constant of f2 (and to guarantee that the maximum is reached at (x(2), y(2))), we need to have A(2) equal to a matrix with cmax at each position.

Now (i)-(iv) also hold for the second layer and one may now proceed by induction to show that for i  2, A(i) contains only cmax for each of its entries. This is the only way to maximize the Lipschitz constraint for functions generated by the neural net, and it does indeed hold ||f (x) - f (y)||2 = ¯||x - y||2 with x = x(1), y = y(1) and
(x(i), y(i)) = (fi  relu  fi-1  . . .  relu  f1(x), fi  relu  fi-1  . . .  relu  f1(y)).

C PROOFS

C.1 PROOF OF THEOREM 1

Proof. We provide the arguments how to derive our version from Theorem 5.10 of Villani (2008).

With c(x, y) = ||x - y||2, our assumptions imply (with cX = cY = ||·||2) that all conclusions of Theorem 5.10 (i) - (iii) hold. Moreover, 5.4 of Villani (2008) shows that in this case  = c (in the notation of Villani (2008)) and c-convexity is the same as 1-Lipschitz continuity. This leads to our formulation in (i) and the existence of an optimal coupling  and an optimal critic function f  by part (iii).

If we let

f = {(x, y)  Rn × Rn | f (x) - f (y) = ||x - y||2}

then it follows from the proof of Theorem 5.10 that the set  in part 5.10 (iii) is given by  = fLip1 optimal f , where f  being optimal means that it leads to a maximum on the RHS of
equation (1).

To prove our part (ii) from 5.10, let  be optimal then, by 5.10 (iii), () = 1. Hence, in particular, (f ) = 1 for all optimal f   Lip1. This shows that (a) implies (b). For the other direction, we use that if (f ) = 1 for all optimal f , then () = 1, which by Theorem 5.10 (iii) is equivalent to  being optimal.

C.2 PROOF OF PROPOSITION 1
Proof. It follows from Theorem 1 (ii) that for all (x, y) in the support of  we have |f (y) - f (x)| = ||x - y||2. Considering the line between x and y, the 1-Lipschitz constraint implies that the values of f  have to follow a linear function (since assuming that the slope was smaller than one at some point would imply that the function must have a slope larger than one somewhere else between x and y, which contradicts the 1-Lipschitz constraint). It follows that at each point on the line, the partial derivative has norm equal to one into the direction pointing from the real data point x to the generated one y (which are coupled by the corresponding optimal coupling). Since, by the 1-Lipschitz constraint, the maximal norm of a partial derivative at any point into any direction is one, the given direction is the direction of maximal descent, i.e. equals the gradient.

13

On the regularization of WGANs

C.3 PROOF OF PROPOSITION 2
To prove Proposition 2, we first prove that (x) = -|x| is the optimal critic function for certain distributions with non-overlapping support, and then reduce the Gaussian example to this simplified setting. Proposition 4. Let f and g be two continuous functions on the real line that satisfy the following conditions:
· f and g are symmetric with respect to the y-axis.
· f (x)  0 and g(x)  0 for all x.
· If supp(h) = {x  R | h(x) > 0} denotes the open support of a continuous function h, then supp(f )  supp(g) = .
· f has connected support (this implies that f is centered around 0 because of the symmetry).
· f (x)dx = g(x)dx.
RR
Then the maximum of R (x)(f (x) - g(x))dx over   Lip1 is maximized for (x) = -|x|.
Proof. We multiply both f and g by a constant number c such that

c · f (x)dx = c · g(x)dx = 1.
RR

Then c · f and c · g define probability density functions. A function   Lip1 maximizes

(x)(c ·
R

f (x) - c · g(x))dx if and only if it maximizes (x)(f (x) - g(x))dx. We therefore may assume

R

from now on that

f (x)dx = g(x)dx = 1.
RR
Now it suffices to find a coupling  of the probability distributions defined by f and g, which itself is defined by a probability density function  : R × R  R such that for (x) = -|x| we get

|x - y| · (x, y)dxdy = (x)(f (x) - g(x))dx.

R ×R

R

This holds by the Kantorovich duality theorem 1, because the right hand side is always smaller or
equal to the left hand side for arbitrary coupling  and function   Lip1 and is consequently maximized when equality holds. By the assumption of symmetry, we may write g = g1 + g2 where the support supp(g1)  {x | x < 0} and g2(x) = g1(-x) for all x. The area under g1(x) equals half the area under f (x), or put differently,

1

g1(x)dx =
R

R

f (x)(-,0](x)dx

=

. 2

We now consider the probability density function 1 : R × R  R given by

1(x, y) = 2g1(x) · 2f (y) · (-,0](y),

which defines a coupling between the two distributions given by the probability density functions 2g1 and 2f · (-,0]. For later use we note that

1(x, y) dx = 2 · f (y) · (-,0](y) and

1(x, y) dy = 2 · g1(x).

xR

yR

We define

2(x, y)

=

1(-x, -y)

for

y

=

0 and 2(x, 0)

=

0.

Further,

we

let 

=

1 2

1

+

1 2

2

.

Then  defines a coupling between g and f as can be seen by computing

11

(x, y)dx = xR 2

xR 1(x, y)dx + 2

2(x, y)dx
xR

14

On the regularization of WGANs

11

= 2

xR 1(x, y)dx + 2

1(-x, -y){y=0}(y)dx
xR

= f (y)(-,0](y) + f (y)(0,)(y) = f (y)

and

11

(x, y)dy = yR 2

yR 1(x, y)dy + 2

2(x, y)dy
yR

1 2

1 yR 1(x, y)dy + 2

1(-x, -y){y=0}(y)dy
yR

= g1(x) + g1(-x) = g1(x) + g2(x) = g(x)

We have established the existence of some coupling between f and g and we will now compute its transport costs. We will subsequently show that this equals (-|x|)(f (x) - g(x))dx, hence both
R
 and  are optimal by realizing the Kantorovich duality.

We wish to compute

R×R |x - y|(x, y)dxdy =

(-|x|)(f (x) - g(x))dx.
R

|x - y|(x, y)dxdy sym=metry

|x - y|1(x, y)dxdy

R ×R

R ×R

= (y - x)1(x, y)dxdy.
R ×R
The latter equation holds because for (x, y) in the essential support of 1, which is a subset of the essential support of g1 × f1, and therefore we have x  y. Let

x0 =

R x · g1(x)dx , R g1(x)dx

and

y0

=

R y · f (y) · (-,0)(y)dy . R f (y) · (-,0)(y)dy

Then

(x - x0) · g1(x)dx = 0 and (y - y0) · f (y) · (-,0](y)dy = 0.
RR
Now, it follows that

(y - x)1(x, y)dxdy =

(y - y0 + y0 - x)1(x, y)dxdy

R ×R

R ×R

= (y - y0)1(x, y)dxdy + (y0 - x)1(x, y)dxdy

xy

xy

= (y - y0) 1(x, y)dxdy +

(y0 - x)1(x, y)dxdy

yx

xy

= 2 (y - y0) · f (y) · (-,0](y)dy +

(y0 - x0 + x0 - x)1(x, y)dxdy

y xy

=0

= (y0 - x0)

1(x, y)dxdy + (x0 - x) 1(x, y)dy dx

xy

xy

Hence,

= (y0 - x0).

=2g1 (x)

|x - y|(x, y)dxdy = (y0 - x0) =

R

y

·

f (y)

·

(-,0)(y)dy
1

-

R

x

·

g1
1

(x)dx

.

R ×R

22

= 2 x · (f (x) · (-,0)(x) - g1(x))dx
R 0
= 2 x · (f (x) - g(x))dx
-

15

On the regularization of WGANs
0
= 2 (-|x|) · (f (x) - g(x))dx
-
sym=metry (-|x|) · (f (x) - g(x))dx
R

We are now able to proof Proposition 2

Proof to Proposition 2.

Let

f

denote

the

probability

density

function of N (0, 1)

and

g

=

1 2

g-1

+

1 2

g1

denote

the

sum

of

half

the

probability

density

functions

g-1

of

N (-1, 1)

and

g1

of

N (1, 1).

Let

f~(x) = max {0, (f (x) - g(x))} and g~(x) = max {0, (g(x) - f (x))} .

Then f~ and g~ satisfy the hypothesis of Proposition 4 and the maximum

max (x)(f (x) - g(x))dx = max (x)(f~(x) - g~(x))dx

Lip1 R

Lip1 R

is obtained for (x) = -|x|.

C.4 THE ARGUMENTS SUPPORTING OBSERVATION 3
For the coupled pairs (x, y) and (x, y ) we have that the partial derivatives at x into the directions of y and y respectively have an absolute value of one. If there are two such directions and f  is differentiable, then the norm of its gradient must be larger than one, contradicting the 1-Lipschitz constraint. Indeed, recall that, considering f as a function on the line {x +  · v |   R} with v of unit length, the slope of f at x is given by f (x) · v = Dv(f (x)). Now

f (x) · v = ||f (x)||2 · cos(v)

(8)

with v being the angle between the vector f (x) and the unit vector v. Equation (8) with cos(v) =

1

has

a

unique

solution

for

v

with

v

=

f (x) ||f (x)||2

.

It

follows

that,

if

for

two

different

directions

v, v

we have Dv(f (x)) = Dv (f (x)) = 1, then cos(v) = cos(v ) < 1 and ||f (x)||2 > 1.

16

On the regularization of WGANs
D ADDITIONAL EXPERIMENTAL RESULTS
D.1 LEVEL SETS OF THE CRITIC
Figure 7: Level sets of the critic (yellow corresponds to high, purple to low values) of WGANs during training (after 10,50,100, and 1000 iterations) on the 8Gaussian data set. Top: GP-penalty ( = 10). Middle: GP-penalty ( = 1). Bottom: LP-penalty ( = 10). D.2 EVOLUTION OF CRITICS LOSS
Figure 9: Evolution of the WGAN-GP critics loss without the regularization term ( = 1). Left: Median results over the 20 runs (blue area indicates quantiles, green dots outliers). Right: Single runs.
17

On the regularization of WGANs
Figure 8: Level sets of the critic (yellow corresponds to high, purple to low values) of WGANs during training (after 10,50,100, and 1000 iterations) on the 25Gaussian data set. Top: GP-penalty ( = 10). Middle: GP-penalty ( = 1). Bottom: LP-penalty ( = 10). D.3 EVOLUTION OF THE EM DISTANCE
Figure 10: Evolution of the approximated EM distance during training WGAN-GPs with  = 1. Left: Median results over the 10 runs. Right: Single runs. D.4 DIFFERENT SAMPLING METHODS We analyzed the effect of the GP- and the LP-penalty using different sampling procedures. In particular, we compared the sampling procedure proposed by Gulrajani et al. (2017) with variants, which generate the samples used for the regularization term by adding random noise either onto training points or onto both training and generated samples. We refer to this as "local perturbation" in the following.7 The evolution of the critics loss when using this local perturbation can be seen in Figure 11. Results are qualitatively similar to those when using the sampling procedure proposed by Gulrajani et al. (2017). Interestingly, later WGAN-GP training is stabilized if one only adds noise
7Note that applying the GP-penalty on samples generated by adding noise to the training examples was also suggested by Kodali et al. (2017), but using a different (asymmetric) noise distribution and in combination with the vanilla GAN objective.
18

On the regularization of WGANs to training examples and not to generated examples. This indicates that enforcing the GP-penalty close to the data manifold is less harmful. However, the critic's loss is still much more fluctuating than when training a WGAN-LP. The evolution of the approximated EM distance when using local perturbation (by adding noise to the training examples only) is shown in Figure 12. Training with the GP-penalty leads to larger fluctuations of the approximated Wasserstein-1 distance than training with the LP-penalty. However, fluctuations are less severe compared to the setting when the GP-penalty is used in combination with the sampling procedure proposed in Gulrajani et al. (2017).
Figure 11: Evolution of the WGAN critic's negative loss with local sampling (without the regularization term). Left: Median results over the 20 runs. Right: Single runs. Top: GP-penalty when generating samples by perturbing training samples only. Middle: For GP-penalty, perturbing training and generated samples. Bottom: LP-penalty, perturbing training and generated samples (very similar to perturbing only training samples))
19

On the regularization of WGANs

Figure 12: Evolution of the approximated EM distance during training of WGANs with local perturbation ( = 5). Left: Median results over the 10 runs. Right: Single runs. Top: For the GP-penalty. Bottom: For the LP-penalty

D.5 OPTIMIZING THE WASSERSTEIN-2 DISTANCE

We trained a WGAN with the objective of minimizing the Wasserstein-2 distance, that is, with the regularization term given by

max

0,

|f (x) - f (y)| ||x - y||2p

-

1

2
,

(9)

and penalty weight  = 10. Results for the evolution of the critics loss and the approximated EM distance during training on the Swiss Roll data set are shown in Figure 13. Both critic loss and EM reduce smoothly, which makes the Wasserstein-2 distance (in combination with its theoretical properties) an interesting candidate to further investigations.

Figure 13: Evolution of the WGAN critics loss (Left) and the approximated EM distance (Right) for a WGAN-LP trained to minimize the Wasserstein-2 distance ( = 10). Shown are the medians over 5 runs.
D.6 EXPERIMENTAL RESULTS ON CIFAR Inception score The evolution of the Inception score for WGAN-GP and WGAN-LP is shown in Figure 15 for a penalty weight  = 200.
20

On the regularization of WGANs
Figure 14: Evolution of Inception score on CIFAR for regularization parameter  = 200. WGANLP in green, WGAN-GP in red. Stability In Figure 15 we compare the performance of WGAN-LP and WGAN-GP on a separate validation set. The black curves correspond to the total loss, and demonstrate a more stable behavior for WGAN-LP and smooth convergence. Note however that a value close to zero does not necessarily indicate good performance of the generator, since the total loss is recorded after updates of the critic function. Furthermore, a comparison of the total loss may not be fair because of the different penalty terms (i.e., since a larger penalty is added in the case of WGAN-GP, see Figure 16, the total loss is closer to zero for wgan-gp than for WGAN-LP) When excluding the penalty term, it becomes obvious that the loss moves away from zero after an increase in the beginning of the training. This behavior could be explained by "overfitting" of the critic in the sense that the critic function learns to separate given samples very well.
Figure 15: Evolution of validation loss on CIFAR. Black curves indicate the total loss, colored curves the loss without regularization term. Left: WGAN-LP, right: WGAN-GP.
Figure 16: Comparison of the influence of regularization during training on CIFAR. The (one-sided) gradient penalty of WGAN-LP is depicted in blue, the gradient penalty of WGAN-GP is split in the two parts for penalizing gradient ¿1 (red) and penalizing gradient ¡1 (yellow).
21

