Under review as a conference paper at ICLR 2018
INFORMATION THEORETIC CO-TRAINING
Anonymous
ABSTRACT
This paper introduces an information theoretic co-training objective for unsupervised learning. We consider the problem of predicting the future. Rather than predict future sensations (image pixels or sound waves) we predict "hypotheses" to be confirmed by future sensations. More formally, we assume a population distribution on pairs (x, y) where we can think of x as a past sensation and y as a future sensation. We train both a predictor model P(z|x) and a confirmation model P(z|y) where we view z as hypotheses (when predicted) or facts (when confirmed). For a population distribution on pairs (x, y) we focus on the problem of measuring the mutual information between x and y. By the data processing inequality this mutual information is at least as large as the mutual information between x and z under the distribution on triples (x, z, y) defined by the confirmation model P(z|y). The information theoretic training objective for P(z|x) and P(z|y) can be viewed as a form of co-training where we want the prediction from x to match the confirmation from y. Initial experiments applying information theoretic co-training to unsupervised learning of phonetics are presented.
1 INTUITION AND FORMULATION
Consider the problem of predicting the future from the past. Intuitively we are not interested in predicting raw future sense data such as image pixels. Rather we are interested in predicting facts about the future as will be inferred from future sensations. Here we consider the joint problem of (1) learning to convert sensation to facts, and (2) learning to predict future facts. Information theoretic co-training aims to measure the mutual information between past sensation and the future sensation by demonstrating the ability to predict future facts.
We formulate information theoretic co-training by letting X be a space of possible past sensations, Y be a space of possible future sensations, and Z be a space of facts. We assume a population distribution P on sensation pairs (x, y)  X × Y. We assume models P(z|x) and P(z|x) to predict future facts from past and future sensations respectively. Here  and  are parameter vectors and we assume that the probabilities are differentiable in the parameters.
In our speech expriments x is 150 millisecond of speech (15 speech frames) and y is a future 150 millisecond of speech, where x and y are separated by a gap of 50 milliseconds. In the experiment Z is an alphabet of 64 symbols and  and  are the parameters of a neural network.
In the general case we assume that z is most accurately estimated from y. We then define a distribution on triples (x, z, y) where (x, y) is drawn from the population distribution P and z is drawn from P(z|y). In the speech experiment there is an intuitive symmetry between x and y but we still define the distribution on triple (x, z, y) by (x, y)  P and z  P(z|y). We note that by the data processing inequality we have
I(x, y)  I(x, z) = H(z) - H(z|x). Here H(z) and H(z|x) are determined by the distribution on triples which is itself determined by .
The information theoretic co-training object takes into account the difficulty of empirically measuring the entropies H(z) and H(z|x). In the phonetics experiment we have that z has only 64 possible values which implies that the entropy H(z) is at most six bits. In this case H(z) can be approximated directly by the empirical marginal over z in a large minibatch. The entropy H(z|x) cannot in general be measured directly. We assume that we can sample (x, y) from the population, and sample z from P(z|y), but have no way computing P(z|x). However, following standard
1

Under review as a conference paper at ICLR 2018

practice we can upper bound the entropy H(z|x) by the cross-entropy H+,(z|x) where the model probability P(z|x) is computable.

I(x, y)  H(z) - H+,(z|x) H+,(z|x) = E(x,y)P, zP(z|y) - ln P(z|x)
= H(z|x) + KL(P(z|y), P(z|x))  H(z|x)

A first information theoretic co-training objective is then defined by

 = argmax H(z) - H+,(z|x).
,

(1)

It is perhaps useful to rewrite (1) as



=

argmax H(z) -


min


H+,(z|x)

 = argmin H+,(z|x)


= argmin E(x,y)P, zP (z|y) - ln P(z|x).


(2)

It should be noted that the objective (2) is the standard objective for training on labeled data. In (2) z replaces y as a label for x. The term H(z) in objective (1) encourages  to extract as much factual information from the future sensation as possible while still making the extracted factual information predictable from the past. Here  and  cooperate to find agreement on a "language" (a semantics for symbols) grounded in sensation.

If z is allowed to be a structured object, such as a sequence of symbols, then H(z) becomes difficult
to measure. However, again following stadanrd practice, we can bound H(z) by a cross-entropy H+,(z). We then have the information theoretic co-training objective



=

argmax


min


H+,(z)

-

min


H+,(z|x)

H+,(z) = E(x,y)P EzP(z|y) - ln P(z).

(3)

Here  is adversarial to  and . Even in the case where z is a structured object such as a string,
it may be useful in practice to bound the amount of information in z by, for example, bounding the size of the alphabet and the length of the string. This will make H(z) and H(z|x) smaller which should improve the numerical stability of the measured difference H+, (z) - H+, (z|x).

2 RELATED LEARNING MODELS

Co-Training. Information theoretic co-training is closely related to classical co-training (Blum & Mitchell (1998); Dasgupta et al. (2002)). Classical co-training assumes the same three spaces X , Y and Z but takes the population P to be a distribution on triples (x, z, y) where z is not observed in the training data. The goal is to learn rules for predicting z by training on the pairs (x, y). For this to be possible we need additional assumptions such as that x and y are independent given z (in the population) and that HP (z) is large. In information theoretic co-training, on the other hand, the population is assumed to be a distribution on (x, y) only and the goal is to measure the mutual information between x and y.
Although the assumptions and theoretical analyses are different, the learning algorithms of information theoretic co-training and classical co-training are very similar. The goal in classical co-training is to find hard (non-stochastic) classifiers f : X  Z and g : Y  Z so as to maximize the probability over the draw of (x, y) that f (x) = h(y) and, at the same time, to require that the values of f (x) and g(x) are diverse. Information theoretic co-training makes the classifiers soft and makes the training objective information theoretic.
The Information Bottleneck. Like information theoretic co-training, Tishby's information bottleneck Tishby et al. (1999) assumes the spaces X , Y and Z and assumes a population distribution on

2

Under review as a conference paper at ICLR 2018

the pairs (x, y). The objective is to train a model P(z|y) defining a distribution on triples (x, z, y) using the training objective

 = argmax I(z, x) - I(z, y).


(4)

In information theoretic co-training the second term is dropped and we retain only I(z, x). One might immediately object that the choice of z = y maximizes I(z, x) so the objective is trivial if we drop the second term. But the goal of information theoretic co-training is not to maximize mutual
information but rather to measure it. Note that setting z = y eliminates  from the information theoretic co-training objective and we are left with setting  so as to minimize HP+,(y|x). This is the standard training objective for labeled data where we treat y as a label. This can also be viewed
as conditional density estimation. Conditional density estimation must be addressed to measure
mutual information. Setting z = y is expected to yield a poor measurement of mutual information
for two somewhat related reasons. First, the probabilistic modeling of raw sense data is difficult. Second HP+,(y) and HP+,(y|x) are both typically much larger than H+,(z) and H+,(z|x). So taking z = y exposes one to numerical instability in taking the difference HP+,(y) - HP+,(y|x).

Density Estimation. Many approaches to unsupervised learning can be viewed as some form of
density estimation. Density estimation is the problem of modeling a probability distribution given
the ability to draw samples. A paradigmatic example is language modeling. In general we assume a population distribution P over some set Y and a model P(y) assigning a probability to each y  Y. The density estimation objective is

 = argmin HP+,(y)

HP+,(y) = EyP - ln P(y)

(5)

The cross-entropy HP+,(y) is an upper bound on the unknown, and typically unknowable, true entropy HP (y).

Expectation maximization (EM) (Dempster et al. (1977)) and variational autoecoders (VAEs)

(Kingma & Welling (2014)) optimize (5) for the case where P(y) is a marginal distribution over a latent variable z.

P(y) = P(z, y)

(6)

z

Here P(z, y) is typically a generative model where y is generatively derived from z.

Data compression algorithms also implicity optimize (5). By Shannon's source coding theorem the most efficient code for instances drawn from a given population uses a number of bits equal to the entropy of the population distribution. The training objective (5) can be interpreted as optimizing the compressed bits per sample when drawing from the population but coding for the model.

Information theoretic co-training as defined by (1) and (3) differs from density estimation as defined by (5) in that information theoretic co-training uses only probability models for the "facts" z -- in information theoretic co-training there is no attempt to model distributions on the sensations.

GANs. Generative adversarial networks (GANs) (Schmidhuber (1992); Goodfellow et al. (2014))
are similar to variational autoencoders in that they define a generative model P(z, y) where y is generated from z and where we are interested in the marginal distribution (6). However, in GANs
there is no attempt to optimize, or even measure, a cross-entropy (5). Instead one define a distribution Q on pairs (y, ) by drawing y with equal probability either from the population distribution P or the model distribution P and setting = 1 is y is drawn from P and = -1 if y is drawn from P. A discriminator model P( |y) must predict which distribution y was drawn from. The GAN objective is



=

argmax


min


H+,(

|y)

H+,( |y) = E(y, )Q - ln P( |y)

(7)

In (7) the generator  is trying to generate values such that discriminator  cannot distinguish values generated from P from values drawn from P. InfoGANS (Chen et al. (2016)) add a term to the GAN objective to increase the mutual information between certain components of the latent variable

3

Under review as a conference paper at ICLR 2018
z and the generated variable y. This encourages the model to use independent components of the information in z.
A major issue with GANs is the lack of an objective measure of performance. The ability to fool a particular discriminator architecture does not imply low cross-entropy as defined by (5). It is quite plausible that large modes of the population density are omitted from the generator distribution (the problem of mode dropping). In contrast, information theoretic co-training provides a quantitative performance measure.
3 UNSUPERVISED LEARNING OF PHONETICS
We performed information theoretic co-training on the TIMIT training data (Garofolo et al. (1993)). The TIMIT training data consists of 3.7 thousand utterances with an average of 304 speech frames per utterance yielding 1.1 million frames of speech. Each frame is labeled with a 39 dimensional MFCC feature vector. We used the normalized data where each utterance is normalized so that the mean over the utterance of the squared norm of the vectors is 1. The TIMIT training data is converted to a set of pairs (x, y) by passing a 35 frame window over the data where x is taken to be the first 15 frames and y is taken to be the last 15 frames so that x and y are separated by five frames. We take Z to be an alphabet of 64 symbols. The models P(z|y) and P(z|x) are computed by GRUs (Chung et al. (2015); Hochreiter & Schmidhuber (1997)). The architecture is shown in figure 1. More details are given in the figure caption.
The model was trained with vanilla SGD. In the first few epochs all but 21 of the 64 available symbols died -- they were assigned essentially zero probability by both P(z|y) and P(z|x) for all placements of the window. After six epochs the surviving 21 symbols were "cloned" -- a noisy copy of each surviving symbol was created yielding 42 surviving symbols. The model was then trained for another 24 epochs (totaling 30 epochs). The learning curve is shown in figure 2. More details are given in the figure caption.
After training the model was used to label each frame of the data with an unsupervised symbol. More specifically, for each frame of data a window is placed centered on that frame and the frame is labeled with the symbol z maximizing P(z|y). After labeling the frames with unsupervised symbols, each unsupervised symbol z is tagged with the majority phoneme label for the frames labeled with z. Although 42 unsuervised symbols survived the training, many of these symbols get tagged with the same phoneme. Four symbols are tagged with silence ('sil') and four with 's'. Only 21 phonemes get used as tags. The distribution over unsupervised symbols is shown in figure 3.
Figure 4 shows P(z|y) and P(z|x) for a typical placement of the model window. We get that P(z|y) is sharp, essentially deterministic, while P(z|x) represents a more reasonably uncertain posterior belief. This asymmetry is due to the asymmetry of the cross-entropy
H+,(z|x) = EzP(z|y) - ln P(z|x).
This has an intuitive explanation if we think of  and  as agents in a cooperative game. Suppose that  has a reasonably uncertain believe about 's prediction of z. In this case 's best strategy is to place the bet deterministically on the value of z for which the expectation of - ln P(z|x) is largest. This is not true of  -- if  makes P(z|x) deterministic then there a large probability of an infinite loss if  guesses wrong.
Figure 5 gives a confusion matrix. This is computed by first labeling each frame with an unsupervised symbol by placing a window centered at that frame and selecting the unsupervised symbol with largest probability under P(z|y). Labeled data is then used to map each unsupervised symbol to the majority phoneme at the frames labeled with that symbol yielding a predicted phoneme at each frame. The resulting predictions turn out to be limited to 21 phonemes. If we consider only frames with gold labels from this set of 21, the model achieves an accuracy of 47.3%. On all frames the accuracy is 35.1%. The most common phonetic label ('sil') occurs in 13/% of the frames. State of the art supervised methods achieve approximately 80% frame level accuracy . We were unable to find comparable unsupervised results. It should be kept in mind that this is a very first attempt.
4

Under review as a conference paper at ICLR 2018
Figure 1: The architecture. A sliding window of 35 speech frames is slid over an utterance. At each placement of the window the architecture computes P(z|y) and P(z|x). Here z ranges over an alphabet of 64 unsupervised symbols. The GRUs use bias vectors in their weight layers and have an initial hidden state vector parameter. The GRUs use 64 dimensional hidden states. Each utterance is run as a single minibatch and the loss function for the minibatch is H+,(z|x) - H(z|u) where H(z|u) is the entropy of the empirical average of P(z|y) over that minimibatch (the utterance u). Conditioning on the utterance avoids the possibility of using speaker identification as a means of achieving co-training agreement. To get the correct expectation the first term in the loss should be averaged over the minibatch while the second term, which is already based on an average distribution, should not.
Figure 2: The learning curve. The model was trained with vanilla SGD. The learning rate was initialized to .4 for 216 hundred utterenace (six epochs). At that point the surviving 21 symbols were "cloned" - for each of the 21 live symbols z one of the dead symbols was replaced with a noisy version of z. The cloning event adds one bit of information to both H(z) and H(z|x). After another 216 hundred utterances (six more epochs), the learning rate was reduced to .2; at 540 hundred utterances the learning rate was dropped to .1; at 900 hundred utterances the learning rate was dropped to .05; and the training was stopped at 1080 hundred utterances (30 epochs). After unsupervised training the majority symbol predicted from x agreed with the majority symbol predicted from y on 78.6% of the window placements on the development set. H(z|u) on the development set was 5.0 bits (a perplexity of 32).
5

Under review as a conference paper at ICLR 2018
Figure 3: The distribution of the unsupervised symbols. The bar chart shows the distribution of the unsupervised symbols as measured by averaging P(z|x) (blue) and averaging P(z|y) (green) over the development data. Each unsupervised symbol is tagged with its most likely phoneme. Note that four unsupervised symbols are tagged with silence ('sil') and four with 's'. Only 21 phonemes occur as the tag of a symbol. Except for one of the silence symbols, the distribution is fairly uniform as one might expect from maximizing HP+,(z|u).
Figure 4: Probabilities at a typical frame. Note that P(z|y) is essentially deterministic while P(z|x) represents a more reasonably uncertain belief. This asymmetry is due to the asymmetry in the cross entroy H+,(z|x). This is discussed from a game-theoretic perspective in the text. The most likely z under P(z|y) agrees with the most likely z under P(z|x) in 78.6% of frames.
Figure 5: The confusion matrix. Each row of the confusion matrix gives the distribution over the actual phonetic label when a certain phonetic prediction is made. Only 21 of the phonemes are ever predicted. The accuracy on frames labeled with one of those 21 phonemes is 47.3%. On all frames the accuracy is 35.1%. The most common phonetic label ('sil') occurs in 13/% of the frames. State of the art supervised methods achieve approximately 80% frame level accuracy . We were unable to find comparable unsupervised results.
6

Under review as a conference paper at ICLR 2018
4 CONCLUSIONS
Information theory already plays a central role in the training objectives typically used in deep learning. Information theoretic co-training introduces a novel information theoretic objective for unsupervised learning in which one can avoid any attempt to measure the entropy, or conditional entropy, of raw sense data. Information theoretic co-training can also be viewed as a way of measuring mutual information by developing a "language" for carrying that information where the entropy of the facts stated in that language is small compared to the entropy of raw sense data. The experiments presented here are very preliminary. We expect that much stronger results in unsupervised learning of phonetics are possible with just a little more experimentation. We are also anxious to experiment with other data such as pairs of images from video and natural language translation pairs.
REFERENCES
A Blum and T Mitchell. Combining labeled and unlabeled data with co-training. In COLT-98, 1998. Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems 29, 2016. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent neural networks. arXive1502.02367, 2015. Sanjoy Dasgupta, Michael Littman, and David McAllester. Pac generalization bounds for cotraining. In NIPS, 2002. A. P. Dempster, N. M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B., 1977. John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathan G. Fiscus, David S. Pallett, Nancy L. Dahlgren, and Victor Zue. Timit: Acoustic-phonetic continuous speech corpus ldc93s1, 1993. Ian Goodfellow, Jean Pouget-Abadie, Hehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661, 2014. Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 1997. D.P. Kingma and M. Welling. Auto-encoding variational bayes. In The International Conference on Learning Representations (ICLR), 2014. Ju¨rgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 1992. Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. In The 37th annual Allerton Conference on Communication, Control, and Computing, 1999.
7

