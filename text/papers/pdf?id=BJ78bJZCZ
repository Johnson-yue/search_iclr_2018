Under review as a conference paper at ICLR 2018
Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit
Anonymous authors Paper under double-blind review
Abstract
Recurrent Neural Networks architectures excel at processing sequences by modelling dependencies over different timescales. The recently introduced Recurrent Weighted Average (RWA) unit captures long term dependencies far better than an LSTM on several challenging tasks. The RWA achieves this by applying attention to each input and computing a weighted average over the full history of its computations. Unfortunately, the RWA cannot change the attention it has assigned to previous timesteps, and so struggles with carrying out consecutive tasks or tasks with changing requirements. We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past. We empirically compare our model to RWA, LSTM and GRU units on several challenging tasks. On tasks with a single output the RWA, RDA and GRU units learn much quicker than the LSTM and with better performance. On the multiple sequence copy task our RDA unit learns the task three times as quickly as the LSTM or GRU units while the RWA fails to learn at all. On the Wikipedia character prediction task the LSTM performs best but it followed closely by our RDA unit. Overall our RDA unit performs well and is sample efficient on a large variety of sequence tasks.
1 Introduction
Many types of information such as language, music and video can be represented as sequential data. Sequential data often contains related information separated by many timesteps, for instance a poem may start and end with the same line, a scenario which we call long term dependencies. Long term dependencies are difficult to model as we must retain information from the whole sequence and this increases the complexity of the model.
A class of model capable of capturing long term dependencies are Recurrent Neural Networks (RNNs). A specific RNN architecture, known as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), is the benchmark against which other RNNs are compared. LSTMs have been shown to learn many difficult sequential tasks effectively. They store information from the past within a hidden state that is combined with the latest input at each timestep. This hidden state can carry information right from the beginning of the input sequence, which allows long term dependencies to be captured. However, the hidden state tends to focus on the more recent past and while this mostly works well, in tasks requiring equal weighting between old and new information LSTMs can fail to learn.
A technique for accessing information from anywhere in the input sequence is known as attention. The attention mechanism was introduced to RNNs by Bahdanau et al. (2014) for neural machine translation. The text to translate is first encoded by a bidirectional-RNN producing a new sequence of encoded state. Different locations within the encoded state are focused on by multiplying each of them by an attention matrix and calculating the weighted average. This attention is calculated for each translated word. Computing the attention matrix for each encoded state and translated word combination provides a great deal of flexibility in choosing where in the sequence to attend to, but the cost of computing these
1

Under review as a conference paper at ICLR 2018
matrices grows as a square of the number of words to translate. This cost limits this method to short sequences, typically only single sentences are processed at a time.
The Recurrent Weighted Average (RWA) unit, recently introduced by Ostmeyer and Cowell (2017), can apply attention to sequences of any length. It does this by only computing the attention for each input once and computing the weighted average by maintaining a running average up to the current timestep. Their experiments show that the RWA performs very well on tasks where information is needed from any point in the input sequence. Unfortunately, as it cannot change the attention it assigns to previous timesteps, it performs poorly when asked to carry out multiple tasks within the same sequence, or when asked to predict the next character in a sample of text, a task in which new information is more important than old.
We introduce the Recurrent Discounted Attention (RDA) unit, which extends the RWA by allowing it to discount the attention applied to previous timesteps. As this adjustment is applied to all previous timesteps at once, it continues to be efficient. It performs very well both at tasks requiring equal weighting over all information seen and at tasks in which new information is more important than old.
The main contributions of this paper are as follows:
1. We analyse the Recurrent Weighted Average unit and show that it cannot output certain simple sequences.
2. We propose the Recurrent Discounted Attention unit that extends the Recurrent Weighted Average by allowing it to discount the past.
3. We run extensive experiments on the RWA, RDA, LSTM and GRU units and show that the RWA, RDA and GRU units are well suited to tasks with a single output, the RDA performs best on the multiple sequence copy task while the LSTM unit performs better on the Hutter Prize Wikipedia dataset.
Our paper is setout as follows: we present the analysis of the RWA (sections 3 and 4) and propose the RDA (section 5). The experimental results (section 6), discussion (section 7) and conclusion follow (section 8).
2 Related Work
Recently many people have worked on using RNNs to predict the next character in a corpus of text. Sutskever et al. (2011) first attempted this on the Hutter Prize Wikipedia datasets using the MRNN archtecture. Since then many architectures (Graves, 2013; Chung et al., 2015; Kalchbrenner et al., 2015; Rocki, 2016; Zilly et al., 2016; Ha et al., 2016; Chung et al., 2016) and regularization techniques (Ba et al., 2016; Krueger et al., 2016) have achieved impressive performance on this task, coming close to the bit-per-character limits bespoke compression algorithms have attained.
Many of the above architectures are very complex, and so the Gated Recurrent Unit (GRU) is a much simpler design that achieves similar performance to the LSTM. Our experiments confirm previous literature (Chung et al., 2014) that reports it performing very well.
Attention mechanisms have been used in neural machine translation by Bahdanau et al. (2014). Xu et al. (2015) experimented with hard-attention on image where a single location is selected from a multinomial distribution. Xu et al. (2015) introduced the global and local attention to refer to attention applied to the whole input and hard attention applied to a local subset of the input.
An idea related to attention is the notion of providing additional computation time for difficult inputs. Graves (2016) introduce shows that this yields insight into the distribution of information in the input data itself.
Several RNN architectures have attempted to deal with long term dependencies by storing information in an external memory (Graves et al., 2014; 2016).
2

Under review as a conference paper at ICLR 2018

3 Recurrent Weighted Average

At each timestep the Recurrent Weighted Average model uses its current hidden state ht-1 and the input xt to calculate two quantities:

1. The features zt of the current input:
ut = Wu · xt + bu gt = Wg · [xt, ht-1] + bg zt = ut tanh gt
where ut is an unbounded vector dependent only on the input xt, and tanh gt is a bounded vector dependent on the input xt and the hidden state ht-1. Notation: W are weights, b are biases, (·) is matrix multiplication, and is the element-wise product.
2. The attention at to pay to the features zt:
at = eWa·[xt,ht-1]+ba

The hidden state ht is then the average of of the features zt, weighted by the attention at, and squashed through the hyperbolic tangent function:

ht = tanh

t i=1

zi

ai

t i=1

ai

This is implemented efficiently as a running average:

nt = nt-1 + zt

dt = dt-1 + at

ht = tanh

nt dt

at

where nt is the numerator and dt is the denominator of the average.

4 Properties of the Recurrent Weighted Average
The RWA shows superior experimental results compared to the LSTM on the following tasks:
1. Classifying whether a sequence is longer than 500 elements. 2. Memorising short random sequences of symbols and recalling them at any point in
the subsequent 1000 timesteps. 3. Adding two numbers spaced randomly apart on a sequence of length 1000. 4. Classifying MNIST images pixel by pixel.
All of these tasks require combining the full sequence of inputs into a single output. It makes perfect sense that an average over all timesteps would perform well in these tasks.
On the other hand, we can imagine tasks where an average over all timesteps would not work effectively:
1. Copying many input sequences from input to output. It will need to forget sequences once they have been output.
2. Predicting the next character in a body of text. Typically, the next character depends much more on recent characters than on those from the beginning of the text.
3. Outputting the parity of an input sequence ht = -1tc for 0 < c < 1.

3

Under review as a conference paper at ICLR 2018

All of these follow from the property that dt is monotonically increasing in t, which can be seen from at > 0 and dt = dt-1 + at. As dt becomes larger, the magnitude of at must increase to change the value of ht. This means that it becomes harder and harder to change the value of ht to the point where it almost becomes fixed. In the specific case of outputting the sequence ht = -1tc we can show that at must grow geometrically with time.
Lemma 1 Let the task be to output the sequence ht = -1tc for 0 < c < 1. Let ht be defined by the equations of the Recurrent Weighted Average, and let zt be bounded and fh be a continuous, monotonically increasing surjection from R  (-1, 1). Then, at grows geometrically with increasing t.
Proof. Provided in Appendix A.
Corollary 2 If at is also bounded then it cannot grow geometrically for all time and so the RWA cannot output the sequence ht = -1tc.
Corollary 2 suggests that the Recurrent Weighted Average may not actually be Turing Complete.
Overall, these properties suggest the the RWA is a good choice for tasks with a single result, but not for sequences with multiple results or tasks that require forgetting.

5 The Recurrent Discounted Attention unit

The RDA uses its current hidden state ht-1 and the input xt to calculate three quantities:

1. The features zt of the current input are calculated identically to the RWA:
ut = Wu · xt + bu gt = Wg · [xt, ht-1] + bg zt = ut tanh gt

2. The attention at to pay to the features: zt

at = fa(Wa · [xt, ht-1] + ba)

Here we generalize attention to allow any function fa which is non-negative and monotonically increasing. If we choose fa = exp, then we recover the RWA attention function.

3. The discount factor t to apply to the previous values in the average

t = (W · [xt, ht-1] + b )

where



is

the

sigmoid/logistic

function

defined

as

(x)

=

1 1+e-x

.

We use these values to calculate a discounted moving average. This discounting mechanism is crucial in remediating the RWA's inability to forget the past

nt = nt-1 t + zt

dt = dt-1 t + at

ht = fh

nt dt

at

Here we generalize RWA further by allowing fh to be any function, and we also introduce a final transformation to the hidden state ht to produce the output
ot = fo(ht)

5.1 Choices for the attention function fa, hidden state function fh and output function fo
The attention function fa(x) is a non-negative monotonically increasing function of x. There are several possible choices:

4

Under review as a conference paper at ICLR 2018

· fa(x) = ex - This is used in the RWA.
· fa(x) = max(0, x) - Using a ReLU allows the complete ignoring of some timesteps with linearly increasing attention for others.
· fa(x) = ln(1 + ex) - The softplus function is a smooth approximation to the ReLU.
· fa(x) = (x) - Using the sigmoid limits the maximum attention an input can be given.

The

domain

of

the

hidden

activation

function

fh

is

the

average

nt dt

.

This

average

is

bounded

by the minimum and maximum values of zt. Possible choices of fh include:

·

fh

(

nt dt

)

=

tanh(

nt dt

)

-

This

is

used

in

the

RWA.

We

observed

that

the

range

of

nt dt

mostly remained in the linear domain of tanh centred around 0, suggesting that

using this was unneccessary.

·

fh(

nt dt

)

=

nt dt

- The identity is our

choice

for

fh

in the RDA.

Possible choices for the output function fo are:

· fo(ht) = ht - The RWA uses the identity as its hidden state has already been transformed by tanh.
· fo(ht) = tanh(ht) - The output can be squashed between [-1, 1] using tanh.

6 Experiments
We ran experiments to investigate the following questions:
1. Which form of the RDA works best? (Section 6.2) 2. The RWA unit works remarkably well for sequences with a single task. Does the
RDA unit retain this strength? (Section 6.3) 3. We expect the RWA unit to struggle with consecutive independent tasks. Does this
happen in practice and does the RDA solve this problem? (Section 6.4) 4. How does the RDA unit scale up to very long sequences? We test character prediction
on the Hutter Prize Wikipedia dataset. (Section 6.5) 5. How does the RDA unit compare to RWA, LSTM and GRU units? Are some units
more suited to certain types of tasks than others? (Section 7)
We provide plots of the training process in Appendix B.

6.1 Implementation details
For all tasks except the Wikipedia character prediction task, we use 250 recurrent units. Weights are initialized using Xavier initialization (Glorot and Bengio, 2010) and biases are initialized to 0, except for forget gates and discount gates which are initialized to 1 (Gers, Schmidhuber, and Cummins, 2000). We use mini-batches of 100 examples and backpropagate over the full sequence length. We train the models using Adam Kingma and Ba (2014) with a learning rate of 0.001. Gradients are clipped between -1 and 1.
For the Wikipedia task, we use a character embedding of 64 dimensions, followed by a single layer of 1800 recurrent units, and a final softmax layer for the character prediction. We apply truncated backpropagation every 250 timesteps, and use the last hidden state of the sequence as the initial hidden state of the next sequence to approximate full backpropagation.
All of our experiments are implemented in TensorFlow (Abadi et al., 2016).

5

Under review as a conference paper at ICLR 2018

Addition

Model

Steps until loss < 0.001

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

2036 > 10000
1781 2016 1735

Classify Length

Model

Steps until accuracy = 1.0.

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

71 776 164 414 133

Table 1: Addition: steps until loss < 0.001. Table 2: Classify: steps until accuracy = 1.0.

MNIST

Model

Test Set Accuracy

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

0.985 0.114 0.985 0.987 0.979

Table 3: MNIST test set accuracy.

MNIST permuted

Model

Permuted Test Set Accuracy

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

0.944 0.915 0.905 0.913 0.899

Table 4: MNIST permuted test set accuracy.

6.2 Empirical evaluation of RDA activation functions
We ran our experiments with different combinations of fa and fo and found the following:
· Using a ReLU for the attention function fa almost always fails to train. Using a Softplus for fa is much more stable than a ReLU. However, it doesn't perform as well as using sigmoid or exponential attention.
· Exponential attention performs well in all tasks, and works best with the tanh output function fo(ht) = tanh(ht). We refer to this as RDA-exp-tanh.
· Sigmoid attention performs well in all tasks, and works best with the identity output function fo(ht) = ht. We refer to this as RDA-sigmoid-id.
· It is difficult to choose between RDA-exp-tanh and RDA-sigmoid-id. RDA-exptanh often trains faster, but it sometimes diverges with NaN errors during training. RDA-sigmoid-id trains slower but is more stable, and tends to have better loss. We include results for both of them.
6.3 Single task sequences
Here we investigate whether sequences with a single task can be performed as well with the RDA as with the RWA.
Each of the four tasks detailed below require the RNN to save some or all of the input sequence before outputting a single result many steps later.
1. Addition - The input consists of two sequences. The first is a sequence of numbers each uniformly sampled from [0, 1], and the second consists of all zeros except for two ones which indicate the two numbers of the first sequence to be added together. (Table 1)
2. Classify length - A sequence of length between 1 and 1000 is input. The goal is to classify whether the sequence is longer than 500. All RNN architectures could learn their initial hidden state for this task, which improved performance for all of them. (Table 2)
3. MNIST - The task is supervised classification of MNIST digits. We flatten the 28x28 pixel arrays into a single 784 element sequence and use RNNs to predict the digit class labels. This task challenges networks' ability to learn long-range dependencies,
6

Under review as a conference paper at ICLR 2018

Copy

Model

Steps until accuracy > 0.999

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

5329 > 20000
11831 9840 5660

Table 5: Copy: steps until accuracy > 0.999

Multicopy

Model

Steps until accuracy > 0.99

GRU LSTM RDA-exp-tanh RDA-sigmoid-id RWA

3984 4048 1114 1316 > 10000

Table 6: Multicopy: steps until accuracy > 0.99

Hutter Prize Wikipedia
Model
Stacked LSTM (Graves, 2013) MRNN (Sutskever et al., 2011) GF-LSTM (Chung et al., 2015) Grid-LSTM (Kalchbrenner et al., 2015)
MI-LSTM (Wu et al., 2016) Recurrent Memory Array Structures (Rocki, 2016a)
HyperNetworks (Ha et al., 2016) LayerNorm HyperNetworks (Ha et al., 2016) Recurrent Highway Networks (Zilly et al., 2016)
LayerNorm LSTM HM-LSTM
LayerNorm HM-LSTM
GRU (our implementation) LSTM (our implementation)
RDA-exp-tanh RDA-sigmoid-id
RWA
PAQ8hp12 (Mahoney, 2005) decomp8 (Mahoney, 2009)

BPC
1.67 1.60 1.58 1.47 1.44 1.40 1.35 1.34 1.32 1.39 1.34 1.32
1.535 1.492 N/A 1.529 5.067
1.32 1.28

Table 7: Bits per character on the Hutter Prize Wikipedia test set

as crucial pixels are present at the beginning, middle and end of the sequence. We implement two variants of this task:
(a) Sequential - the pixels are fed in from the top left to the bottom right of the image. (Table 3)
(b) Permuted - the pixels of the image are randomly permuted before the image is fed in. The same permutation is applied to all images. (Table 4)
4. Copy - The input sequence starts with randomly sampled symbols. The rest of the input is blanks except for a single recall symbol. The goal is to memorize the starting symbols and output them when prompted by the recall symbol. All other output symbols must be blank. (Table 5)
6.4 Multiple sequence copy task
Here we investigate whether the different RNN units can cope with doing the same task repeatedly.
The tasks consists of multiple copying tasks all within the same sequence. Instead of having the recall symbol randomly placed over the whole sequence it always appears a couple of steps after the sequence being memorized. This gives room for 50 consecutive copying tasks in a length 1000 input sequence. (Table 6)
7

Under review as a conference paper at ICLR 2018
6.5 Wikipedia character prediction task
The standard test for RNN models is character-level language modelling. We evaluate our models on the Hutter Prize Wikipedia dataset enwik8, which contains 100M characters of 205 different symbols including XML markup and special characters. We split the data into the first 90M characters for the training set, the next 5M for validation, and the final 5M for the test set. (Table 7)
7 Discussion
We start our discussion by describing the performance of each individual unit.
Our analysis of the RWA unit showed that it should only work well on the single task sequences and we confirm this experimentally. It learns the single sequence tasks quickly but is unable to learn the multiple sequence copy task and Wikipedia character prediction task.
Our experiments show that the RDA unit is a consistent performer on all types of tasks. As expected, it learns single task sequences slower than the RWA but it actually achieves better generalization on the MNIST test sets. We speculate that the cause of this improvement is because the ability to forget effectively allows it to compress the information it has previously processed, or perhaps discounting the past should be considered as changing the attention on the past and the RDA is able to vary its attention on previous inputs based on later inputs. On the multiple sequence copy task the RDA unit was far superior to all other units learning three times as fast as the LSTM and GRU units. On the Wikipedia character prediction task the RDA unit performed respectably, achieving a better compression rate than the GRU but worse than the LSTM.
The LSTM unit learns the single task sequences slower than all the other units and often fails to learn at all. This is surprising as it is often used on these tasks as a baseline against which other archtectures are compared. On the multiple sequence copy task it learns slowly compared to the RDA units but solves the task. The Wikipedia character prediction task is where it performs best, learning much faster and achieving better compression than the other units.
The GRU unit works very well on single task sequences often learning the fastest and achieving excellent generalization on the MNIST test sets. On the multiple sequence copy task it has equal performance to the LSTM. On the Wikipedia character prediction task it performs worse than the LSTM and RDA units but still achieves a good performance.
We now look at how our results show that different neural network architectures are suited for different tasks.
For our single output tasks the RWA, RDA and GRU units work best. Thus for similar real work applications such as encoding a molecule into a latent representation, classification of genomic sequences, answering questions or language translation, these units should be considered before LSTM units. However, our results are yet to be verified in these domains.
For sequences that contain an unknown number of independent tasks the RDA unit should be used.
For the Wikipedia character prediction task the LSTM performs best. Therefore we can't recommend RWA, RDA or GRU units on this or similar tasks.
8 Conclusion
We analysed the Recurrent Weighted Average (RWA) unit and identified its weakness as the inability to forget the past. By adding this ability to forget the past we arrived at the Recurrent Discounted Attention (RDA). We implemented several varieties of the RDA and compared them to the RWA, LSTM and GRU units on several different tasks. We showed that in almost all cases the RDA should be used in preference to the RWA and is a flexible RNN unit that can perform well on all types of tasks.
8

Under review as a conference paper at ICLR 2018
References
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jo´zefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Vi´egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467, 2016. URL http://arxiv. org/abs/1603.04467.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http: //arxiv.org/abs/1409.0473.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. 2014. URL http: //arxiv.org/abs/1412.3555.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Gated feedback recurrent neural networks. In International Conference on Machine Learning, pages 2067­2075, 2015.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. CoRR, abs/1609.01704, 2016. URL http://arxiv.org/abs/1609.01704.
Felix A Gers, Ju¨rgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural computation, 12(10):2451­2471, 2000.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Aistats, volume 9, pages 249­256, 2010.
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850.
Alex Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwin´ska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471­476, 2016.
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. CoRR, abs/1609.09106, 2016. URL http://arxiv.org/abs/1609.09106.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9 (8):1735­1780, 1997.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. CoRR, abs/1507.01526, 2015. URL http://arxiv.org/abs/1507.01526.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
9

Under review as a conference paper at ICLR 2018
David Krueger, Tegan Maharaj, J´anos Kram´ar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016.
Jared Ostmeyer and Lindsay Cowell. Machine learning on sequential data using a recurrent weighted average. arXiv preprint arXiv:1703.01253, 2017.
Kamil Rocki. Recurrent memory array structures. CoRR, abs/1607.03085, 2016. URL http://arxiv.org/abs/1607.03085.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017­1024, 2011.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048­2057, 2015.
Julian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn´ik, and Ju¨rgen Schmidhuber. Recurrent highway networks. CoRR, abs/1607.03474, 2016. URL http://arxiv.org/abs/1607. 03474.
10

Under review as a conference paper at ICLR 2018

Appendices

A Mathematical Proofs

Lemma 1 Let the task be to output the sequence ht = -1tc for 0 < c < 1. Let ht be defined by the equations of the Recurrent Weighted Average, and let zt be bounded and fh be a continuous, monotonically increasing surjection from R  (-1, 1).
Then, at grows geometrically with increasing t.

Proof

Given that the activation fh is a continuous, monotonically increasing surjection from R to (-1, 1), we know that there are two values x+ and x- such that f (x+) = c and f (x-) = -c. Define x+ - x- = .

Then

for

every

even

integer

i,

we

have

ni di

= x+

and

ni+1 di+1

= x-.

From the definitions of nt and dt we have

ni+1 di+1

=

ni + zi+1ai+1 di + ai+1

= x-

Substituting ni = dix+ and rearranging yields

ai+1

=

di(x- - x+) zi+1 - x-



di|| |z|max + |x|max

where |z|max = max{|z|} and |x|max = max{|x+|, |x-|}.

Substituting this into di+1 = di + ai+1 gives us

di+1  di

|| 1+
|z|max + |x|max

By a similar argument, we have the same growth for odd integers di+2  di+1 1 +

|| |z |max +|x|max

and we get geometric growth of dt.

From the definition of dt we have

|| ai = di+1 - di  di |z|max + |x|max

As dt grows geometrically, then so does at.
Corollary 2 If at is also bounded then it cannot grow geometrically for all time and so the RWA cannot output the sequence ht = -1tc
Proof Assume the RWA can output the sequence ht = -1tc. As at grows geometrically, it is unbounded, but this is a contradiction.

11

Under review as a conference paper at ICLR 2018

B Illustrated empirical results
We include figures of the loss function learning curves during training. In the case of MNIST, we report on validation accuracy instead. These experiments provide evidence that the two flavours of the RDA unit consistently perform close to the best across a broad range of tasks. Figures are best viewed in colour.

Loss

Loss (log scale)

0.5

GRU

0.8

0.4 LSTM

RDA-exp-tanh RDA-sigmoid-id

0.6

0.3 RWA

0.4 0.2

0.1 0.2

Validation accuracy Loss (log scale) Loss

0 0

500 1,000 1,500 Steps
(a) Addition task.

2,000

2,500

104

103

102

101

100

10-1

0 0.5 1 Steps
(c) Single copy task.

1.5

2 ·104

1

0

0 200 400 600 Steps
(b) Classification task.

800

1,000

105 104 103 102 101 100 10-1

0 1,000 2,000 3,000 4,000 Steps
(d) Multiple copy task.

5,000

1

0.8 0.8
0.6 0.6
0.4

0.2 0.4

0 0

123 Steps

4 ·104

(e) MNIST, pixel per pixel classification.

0 0.5 1 1.5 2 2.5

Steps

·104

(f) MNIST permuted classification.

Validation accuracy

12

