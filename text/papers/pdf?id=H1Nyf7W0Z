Under review as a conference paper at ICLR 2018
ALPHA-DIVERGENCE BRIDGES MAXIMUM LIKELI-
HOOD AND REINFORCEMENT LEARNING IN NEURAL
SEQUENCE GENERATION
Anonymous authors Paper under double-blind review
ABSTRACT
Neural sequence generation is commonly approached by using maximumlikelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using -divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to   0 and RL to   1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing . Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.
1 INTRODUCTION
Neural sequence models have been successfully applied to various types of machine learning tasks, such as neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Xu et al., 2015; Chen & Lawrence Zitnick, 2015), conversation (Vinyals & Le, 2015), and speech recognition (Chorowski et al., 2014; 2015; Bahdanau et al., 2016). Therefore, developing more effective and sophisticated learning algorithms can be beneficial.
Popular objective functions for training neural sequence models include the maximum-likelihood (ML) and reinforcement learning (RL) objective functions. However, both have limitations, i.e., training/testing discrepancy and sample inefficiency, respectively. Bengio et al. (2015) indicated that optimizing the ML objective is not equal to optimizing the evaluation metric. For example, in machine translation, maximizing likelihood is different from optimizing the BLEU score (Papineni et al., 2002), which is a popular metric for machine translation tasks. In addition, during training, ground-truth tokens are used for the predicting the next token; however, during testing, no ground-truth tokens are available and the tokens predicted by the model are used instead. On the contrary, although the RL-based approach does not suffer from this training/testing discrepancy, it does suffer from sample inefficiency. Samples generated by the model do not necessarily yield high evaluation scores (i.e., rewards), especially in the early stage of training. Consequently, RL-based methods are not self-contained, i.e., they require pre-training via ML-based methods. As discussed in Section 2, since these problems depend on the sampling distributions, it is difficult to resolve them simultaneously.
Our solution to these problems is to integrate these two objective functions. We propose a new objective function -DM (-divergence minimization) for a neural sequence generation, and we demonstrate that it generalizes ML- and RL- based objective functions, i.e., -DM can represent both functions as its special cases (  0 and   1). We also show that, for   (0, 1), the gradient of the -DM objective is a combinations of the ML- and RL-based objective gradients. We apply the same optimization strategy as Norouzi et al. (2016), who useed importance sampling, to optimize this proposed objective function. Consequently, we avoid on-policy RL sampling which
1

Under review as a conference paper at ICLR 2018

suffers from sample inefficiency, and optimize the objective function closer to the desired RL-based objective than the ML-based objective.
The experimental results for a machine translation task indicate that the proposed -DM objective outperforms the ML baseline and the reward augmented ML method (RAML; Norouzi et al., 2016), upon which we build the proposed method. We compare our results to those reported by Bahdanau et al. (2017), who proposed an on-policy RL-based method. We also confirm that -DM can provide a comparable BLEU score without pre-training.
The contributions of this paper are summarized as follows.
· We propose the -DM objective function using -divergence and demonstrate that it can be considered a generalization of the ML- and RL-based objective functions (Section 4).
· We prove that the -DM objective function becomes closer to the desired RL-based objectives as  increases in the sense that the upper bound of the maximum discrepancy between ML- and RL-based objective functions monotonically decreases as  increases.
· The results of machine translation experiments demonstrate that the proposed -DM objective outperforms the ML-baseline and RAML (Section 7).

2 COMPARING OBJECTIVE FUNCTIONS

In this section, we introduce ML-based and RL-based objective functions and the problems in association with learning neural sequence models using them. We also explain why it is difficult to resolve these problems simulataneously.

Maximum-likelihood An ML approach is typically used to train a neural sequence model. Given a context (or input sequence) x  X and a target sequence y = (y1, . . . , yT )  Y, ML minimizes the negative log-likelihood objective function

L() = -

q(y|x) log p(y|x),

xX yY

(1)

where q(y|x) denotes the true sampling distribution. Here, we assume that x is uniformly sampled from X and omit the distribution of x from Eq. (1) for simplicity. For example, in machine translation, if a corpus contains only a single target sentence y for each input sentence x, then q(y|x) = (y - y) and the objective becomes L() = - xX log p(y|x).
ML does not directly optimize the final performance measure; that is, training/testing discrepancy exists. This arises from at leset these two problems:

(i) Objective score discrepancy. The reward function is not used while training the model; however, it is the performance measure in the testing (evaluation) phase. For example, in the case of machine translation, the popular evaluation measures such as BLEU or edit rate (Snover et al., 2006) differ from the negative likelihood function.
(ii) Sampling distribution discrepancy. The model is trained with samples from the true sampling distribution q(y|x); however, it is evaluated using samples generated from the learned distribution p(y|x).

Reinforcement learning In most sequence generation task, the optimization of the final performance measure can be formulated as the minimization of the negative total expected rewards expressed as follows:

L() = -

p(y|x)r(y, y|x),

xX yY

(2)

where r(y, y|x) is a reward function associated with the sequence prediction y, i.e., the BLEU score
or the edit rate in machine translation. RL is an approach to solve the above problems. The objective function of RL is L in Eq. (2), which is a reward-based objective function; thus, there is no objective score discrepancy, thereby resolbing problem (i). Sampling from p(y|x) and taking the expectation with p(y|x) in Eq. (2) also resolves problem (ii). Ranzato et al. (2016) and Bahdanau et al. (2017)

2

Under review as a conference paper at ICLR 2018

directly optimized L using policy gradient methods (Sutton et al., 2000). A sequence prediction task that selects the next token based on an action trajectory (y1, . . . , yt-1) can be considered to be an RL problem. Here the next token selection corresponds to the next action selection in RL. In addition, the action trajectory and the context x correspond to the current state in RL.
RL can suffer from sample inefficiency; thus, it may not generate samples with high rewards, particularly in the early learning stage. By definition, RL generates training samples from its model distribution. This means that, if model p(y|x) has low predictive ability, only a few samples will exist with high rewards.
(iii) Sample inefficiency. The RL model may rarely draw samples with high rewards, which hinders to find the true gradient to optimize the objective function.
Machine translation suffers from this problem because the action (token) space is vast (typically >10, 000 dimensions) and rewards are sparse, i.e., positive rewards are observed only at the end of a sequence. Therefore, the RL-based approach usually requires good initialization and thus is not self-contained. Previous studies have employed pre-training with ML before performing on-policy RL-based sampling (Ranzato et al., 2016; Bahdanau et al., 2017).

Entropy regularized RL To prevent the policy from becoming overly greedy and deterministic, some studies have used the following entropy-regularized version of the policy gradient objective function (Mnih et al., 2016):

L()() :=

- H(p(y|x)) - p(y|x)r(y, y|x) .

xD

yY

Note that lim0 L() = L holds.

(3)

Reward augmented ML Norouzi et al. (2016) proposed RAML, which solves problems (i) and (iii) simultaneously. RAML replaces the sampling distribution of ML, i.e., q(y|x) in Eq. (1), with a reward-based distribution q()(y|x)  exp {r(y, y|x)/ }. In other words, RAML incorporates the reward information into the ML objective function. The RAML objective function is expressed as
follows:

L()() := -

q()(y|x) log p(y|x).

xX yY

(4)

However, problem (ii) remains.

Despite these various attempts, a fundamental technical barrier exists. This barrier prevents solving the three problems using a single method. The barrier originates from a trade-off between sampling distribution discrepancy (ii) and sample inefficiency (iii), because these issues are related to the sampling distribution. Thus, our approach is to control the trade-off of the sampling distributions by combining them.

3 -DIVERGENCE

The proposed method utilizes -divergence DA()(p q), which measures the asymmetric distance between two distributions p and q (Amari, 1985). A prominent feature of -divergence is that
it can behave as DKL(p q) or DKL(q p) depending on the value of , i.e., DA(1)(p q) := lim1 DA()(p q) = DKL(p q) and DA(0)(p q) := lim0 DA()(p q) = DKL(q p). This fact follows from the definition of -divergence

DA()(p

q) :=

1 (1 - )

1-

p(y)q1-(y) = - 1 

q(y) p(y) log() p(y) ,

yY

yY

(5)

where log()(·) is the generalized logarithm log()(x) := (1 - )-1(x1- - 1). Furthermore, -divergence becomes a Hellinger distance when  equals to 1/2.

3

Under review as a conference paper at ICLR 2018

4 PROPOSED OBJECTIVE FUNCTION: -DM
In this section, we describe the proposed objective function -DM and its gradient. Furthermore, we demonstrate that it can smoothly bridge both ML- and RL- based objective functions.
4.1 OBJECTIVE FUNCTION

Regularization  

=0 >0

Maximum likelihood based
0
Maximum likelihood 0
RAML (Norouzi et al., 2016)

0<<1

0

Proposed method

1

Reinforcement learning based
1
Policy gradient (w/o regularization)
0 Policy gradient (w/ entropy regularization)

Figure 1: -DM objective bridges ML- and RL-based objectives.

We define the -DM objective function as the -divergence between p and q():

L(,)() := 

DA()(p

q(

))

=

-

 

p(y|x) log()

xX

xX yY

q( ) (y|x) p (y |x)

.

(6)

This -divergence is equal to L() in Eq. (3) or L() in Eq. (4) by employing   1 or   0 limits, respectively (up to constant).

lim
1

L(, ) ()

=



DKL(p q()) = L()() + constant,

xX

(7)

lim
0

L(, ) ()

=



DKL(q() p) =  L()() + constant.

xX

(8)

Figure 1 illustrates how the -DM objective bridges the ML- and RL-based objective functions. Although the objectives L(,)(), L()(), and L()() have the same global minimizer p(y|x) = q()(y|x), empirical solutions often differ.

4.2 OBJECTIVE FUNCTION GRADIENT

To train neural network or other machine learning models via -divergence minimization, one can use the gradient of -DM objective function. The gradient of Eq. (6) can be expressed as

L(,)() = -

p(,)(y|x) log p(y|x),

xX yY

(9)

where

p(, ) (y|x)

=

1

 -



p (y|x)q(1-)(y|x)

(10)

is a weight that mixes sampling distributions p and q(). This weight makes it clear that the -DM objective can be considered as a mixture of ML- and RL-based objective functions. See Appendix A

for the derivation of this gradient. It converges to the gradient of entropy regularized RL or RAML by taking   1 or   0 limits, respectively (up to constant); i.e., lim1 L(,) = L() and lim0 L(,) =  L().

In Appendix C, we summarize all of the objective functions, gradients, and their connections.

5 -DM ANALYSIS
In this section, we characterize the difference between -DM objective function L(,) and the desired RL-based objective function L() with respect to sup-norm. Our main claim is that, with

4

Under review as a conference paper at ICLR 2018

respect to sup-norm, the discrepancy between L(,) and L() decreases linearly as  increases to 1. We utilize this analysis to motivate our -DM objective function with larger  if there are no
concerns about the sampling inefficiency.

Proposition 1 Assume that p has the same finite support S as that of q(), and that for any s  S, there exists  > 0 such that p(s) >  holds. For any   (0, 1), the following holds.

sup L()() - L~(,)()  C1(1 - ) + C2,

where L~(,) := L(,). Here, C1, C2 is universal constants irrelevant to .

(11)

The following proposition immediately proves the theorem above.

Proposition 2 Assume that probability distribution p has the same finite support S as that of q, and

that for any s  S there exists  > 0 such that p(s) >  holds. For any   (0, 1), the following

holds.

sup DKL(p q) - DA()(p q)  C(1 - ).
p

(12)

Here, C = max supp p log2(q/p) , supp q log2(q/p) .

For the proof of the Proposition 1 and Proposition 2, see Appendix B.

6 OPTIMIZATION OF -DM OBJECTIVE FUNCTION

In this paper, we employed the optimization strategy which is similar to that of RAML. We sample target sentence y for each x from another data augmentation distribution q0(y|x), and then estimate the gradient by importance sampling (IS). For example, we add some noise to the ground truth target sentence y by insertion, substitution, or deletion, and the distribution p0(y|x) assigns some probability to each modified target sentence. Given samples from this proposal ditribution p0(y|x), we update the parameter using the following IS estimator

L(,)() = -

q0(y|x)

xX yY

p(, ) (y|x) q0(y|x)

 log p(y|x)

(13)

N
- wi log p(yi|xi).

(14)

i=0

Here, {(x1, y1), . . . , (xN , yN )} are the N samples from the proposal distribution q0(y|x), and wi is

the importance weight which is proportional to p(,)(yi|xi):

wi  p(yi|xi)q(1-)(yi|xi).

(15)

Note that the difference betweene RAML and -DM is only this importance weight wi. In RAML, wi depends only on q()(yi|xi) but not on p(yi|xi). We normalize wi in each minibatch in order
to use same hyperparameter (e.g., learning rate) as ML baseline. Thus, this estimator becomes a

weighted IS estimator. A weighted IS estimator is not unbiased, yet but it has smaller variance.

Also, we found that normalizing q()(yi|xi) and p(yi|xi) in each minibatch leads to good results.

7 NUMERICAL EXPERIMENTS
We evaluate the effectiveness of -DM experimentally using neural machine translation tasks. We compare the BLEU scores of ML, RAML, and the proposed -DM on the IWSLT'14 German­ English corpus (Cettolo et al., 2014). In order to evaluate the impact of training objective function, we train the same attention-based encoder-decoder model (Bahdanau et al., 2015; Luong et al., 2015) for each objective function. Furthermore, we use the same hyperparameter (e.g., learning rate, dropout rate, and temperature  ) between all the objective functions. For RAML and -DM, we employ a data augmentation procedure similar to that of Norouzi et al. (2016), and thus we generate samples from a data augmentation distribution q0(y|x). Note that the difference between RAML and -DM is only the weight wi of Eq. (14). The details of data augmentation distribution are described in Section 7.2.

5

Under review as a conference paper at ICLR 2018

Figure 2: Performance on different hyperparameters . The -DM approach with larger  performs better than that with smaller .

Table 1: BLEU on IWSLT'14 German­English.

ML RAML -DM,  = 0.3 fixed -DM,  = 0.4 fixed -DM,  = 0.5 fixed -DM,  = 0.3 annealed -DM,  = 0.4 annealed -DM,  = 0.5 annealed

k=1
27.59 (±0.18) 27.74 (±0.06) 27.92 (±0.10) 28.05 (±0.07) 28.01 (±0.16) 27.99 (±0.11) 27.93 (±0.11) 27.98 (±0.11)

k = 10
28.15 (±0.16) 28.44 (±0.13) 28.51 (±0.10) 28.65 (±0.06) 28.52 (±0.10) 28.60 (±0.09) 28.51 (±0.15) 28.48 (±0.15)

IWSLT'14 German­English. The training data comprised approximately 153K German­English sentence pairs and 7K development/test sentence pairs. The vocabulary size for the source/target were 32 009 and 22 822, respectively. The model architecture and parameters follow that of Ranzato et al. (2016) and Bahdanau et al. (2017). Specifically, we trained attention-based encoder-decoder model with the encoder of a bidirectional LSTM with 256 units and the LSTM decoder with the same number of layers and units. We exponentially decay the learning rate, and the initial learning rate is chosen using grid search to maximize the BLEU performance of ML baseline on development dataset. The important hyperparameter  of RAML and -DM is also determined to maximize the BLEU performance of RAML baseline on development dataset. As a result, the initial learning rate of 0.5 and  of 1.0 were used. Our -DM used the same hyperparameters as ML and RAML including the initial learning rate,  , and so on. Details about the models and parameters are discussed in Section 7.2.
7.1 RESULTS
To investigate the impact of hyperparameter , we train the neural sequence models using -DM 5 times for each fixed   {0.0, 0.1, . . . , 0.9}, and then reported the BLEU score of test dataset. Moreover, assuming that the underfitted model prevents the gradient from being stable in the early stage of training, we train the same models with  being linearly annealed from 0.0 to larger values; we increase the value of  by adding 0.03 at each epoch. Here, the beam width k was set to 1 or 10. All BLEU scores and their averages are plotted in Figure 2. The results show that for both k = 1, 10, the models performance are better than smaller or larger  when  is around 0.5 ( = 0.5). However, for larger fixed , the performance was worse than RAML and ML baselines. On the other hand, we can see that the annealed versions of -DM improve the performance of the corresponding fixed versions in relatively larger . As a result, in the annealed scenario, -DM with wide range of   (0, 1) improves on the performance consistently. This implies that the underfitted model makes the performance worse.
6

Under review as a conference paper at ICLR 2018
We summarize the average BLEU scores and their standard deviation of ML, RAML, and -DM with   {0.3, 0.4, 0.5} in Table. 1. The result shows that the BLEU score (k = 10) of our -DM outperforms ML and RAML baseline. Furthermore, although the ML baseline performances differ between our results and those of Bahdanau et al. (2017), the proposed -DM performance with  = 0.5 without pre-training is comparable with the on-policy RL-based methods (Bahdanau et al., 2017). We believe that these results come from the fact that -DM with  > 0 has smaller bias than that of  = 0 (i.e., RAML).
7.2 DETAILS
We utilized a stochastic gradient descent with a decaying learning rate. The learning rate decays from the initial learning rate to 0.05 with dev-decay (Wilson et al., 2017), i.e., after training each epoch, we monitored the perplexity for the development set and reduced the learning rate by multiplying it with  = 0.5 only when the perplexity for the development set does not update the best perplexity. The mini-batch size is 128. We used the dropout with probability 0.3. Gradients are rescaled when the norms exceed 5. In addition, if an unknown token, i.e., a special token representing a word that is not in the vocabulary, is generated in the predicted sentence, it was replaced by the token with the highest attention in the source sentence (Jean et al., 2015). We implemented our models using a fork from the PyTorch1 version of the OpenNMT toolkit (Klein et al., 2017). We calculated the BLEU scores with multi-bleu.perl2 script for both the development and test sets.
We obtained augmented data in the same manner as the RAML framework (Norouzi et al., 2016). For each target sentence, some tokens were replaced by other tokens in the vocabulary and we used the negative Hamming distance as reward. We assumed that Hamming distance e for each sentence is less than [m × 0.25], where m is the length of the sentence and [a] denotes the maximum integer which is less than or equal to a  R. Moreover, the Hamming distance for a sample is uniformly selected from 0 to [m × 0.25]. One can also use BLEU or another machine translation metric for this reward. However, we assumed proposal distribution q0(y|x) different from that of RAML. We assumed the simplified proposal distribution q0(y|x), which is a discrete uniform distribution over [0, m × 0.25]. This results in hyperparameter  used in this experiment being different from that of RAML. We search the  , which maximize the BLEU score of RAML on the development set. As a results,  = 1.0 was chosen, and -DM also uses this fixed  in all the experiments.
8 RELATED WORKS
From the RL literature, reward-based neural sequence model training can be separated into on-policy and off-policy approaches, which differ in the sampling distributions. The proposed -DM approach can be considered an off-policy approach with importance sampling.
Recently, on-policy RL-based approaches for neural sequence predictions have been proposed. Ranzato et al. (2016) proposed a method that uses the REINFORCE algorithm (Williams, 1992). Based on Ranzato et al. (2016), Bahdanau et al. (2017) proposed a method that estimates a critic network and uses it to reduce the variance of the estimated gradient. Bengio et al. (2015) proposed a method that replaces some ground-truth tokens in an output sequence with generated tokens. Yu et al. (2017), Lamb et al. (2016), and Wu et al. (2017) proposed methods based on GAN (generative adversarial net) approaches (Goodfellow et al., 2014). Note that on-policy RL-based approaches can directly optimize the evaluation metric. Degris et al. (2012) proposed off-policy gradient methods using importance sampling, and the proposed -DM off-policy approach utilizes importance sampling to reduce the difference between the objective function and the evaluation measure when  > 0.
As mentioned previously, the proposed -DM can be considered an off-policy RL-based approach in that the sampling distribution differs from the model itself. Thus, the proposed -DM approach has the same advantages as off-policy RL methods compared to on-policy RL methods, i.e., computational efficiency during training and learning stability. On-policy RL approaches must generate samples during training, and immediately utilize these samples. This property leads to high computational costs during training and if the model falls into a poor local minimum, it is difficult to
1http://pytorch.org 2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl
7

Under review as a conference paper at ICLR 2018
recover from this failure. On the other hand, by exploiting data augmentation, the proposed -DM can collect samples before training. Moreover, because the sampling distribution is a stationary distribution independent of the model, one can expect that the learning process of -DM is more stable than that of on-policy RL approaches. Several other methods that compute rewards before training can be considered off-policy RL-based approaches, e.g., minimum risk training (MRT; Shen et al., 2016, RANDOMER (Guu et al., 2017), and Google neural machine translation (GNMT; Wu et al., 2016).
While the proposed approach is a mixture of ML- and RL-based approaches, this attempt is not unique. The sampling distribution of scheduled sampling (Bengio et al., 2015) is also a mixture of ML- and RL-based sampling distributions. However, the sampling distributions of scheduled sampling can differ even in the same sentence, whereas ours are sampled from a stationary distribution. To bridge the ML- and RL-based approaches, Guu et al. (2017) considered the weights of the gradients of the ML- and RL-based approaches by directly comparing both gradients. In contrast, the weights of the proposed -DM approach are obtained as the results of defining the -divergence objective function. GNMT (Wu et al., 2016) considered a mixture of ML- and RL-based objective functions by the weighted arithmetic sum of L and L. Comparing this weighted mean objective function and -DM's objective function could be an interesting research direction in future.
9 CONCLUSION
In this study, we have proposed a new objective function as -divergence minimization for neural sequence model training that unifies ML- and RL-based objective functions. In addition, we proved that the gradient of the objective function is the weighted sum of the gradients of negative loglikelihoods, and that the weights are represented as a mixture of the sampling distributions of the ML- and RL-based objective functions. We demonstrated that the proposed approach outperforms the ML baseline and RAML in the IWSLT'14 machine translation task.
In this study, we focus our attention on the neural sequence generation problem, but we expect our framework may be useful to broader area of reinforcement learning. The sample inefficiency is one of major problems in reinforcement learning, and people try to mitigiate this problem by using several type of supervised learning frameworks such as imitation learning or apprenticisip learning. This alternative approaches bring another problem similar to the neural sequence generaton problem that is originated from the fact that the objective function for training is different from the one for testing. Since our framework is general and independent from the task, our approach may be useful to combine these approaches.
REFERENCES
Shun-ichi Amari. Differential-Geometrical Methods in Statistics. Springer, 1985.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of International Conference on Learning Representations (ICLR), 2015.
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. Endto-end attention-based large vocabulary speech recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In Proceedings of International Conference on Learning Representations (ICLR), 2017.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS), 2015.
Mauro Cettolo, Jan Niehues, Sebastian Stu¨ker, Luisa Bentivogli, and Marcello Federico. Report on the 11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of International Workshop on Spoken Language Translation (IWSLT), 2014.
8

Under review as a conference paper at ICLR 2018
Xinlei Chen and C Lawrence Zitnick. Mind's eye: A recurrent visual representation for image caption generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech recognition using attention-based recurrent NN: First results. arXiv preprint arXiv:1412.1602, 2014.
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in Neural Information Processing Systems (NIPS), 2015.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of International Conference on Machine Learning (ICML), 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.
Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In Proceedings of the Annual Meeting of Association for Computational Linguistics (ACL), 2017.
Se´bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proceedings of the Annual Meeting of Association for Computational Linguistics (ACL), 2015.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. OpenNMT: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810, 2017.
Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances in Neural Information Processing Systems (NIPS), 2016.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of International Conference on Machine Learning (ICML), 2016.
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. In Advances In Neural Information Processing Systems (NIPS), 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of Association for Computational Linguistics (ACL), 2002.
Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Proceedings of International Conference on Learning Representations (ICLR), 2016.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2016.
9

Under review as a conference paper at ICLR 2018

Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas (AMTA), 2006.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS), 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NIPS), 2000.
Oriol Vinyals and Quoc Le. A neural conversational model. In Proceedings of International Conference on Machine Learning (ICML), 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229­256, 1992.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017.
Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Adversarial neural machine translation. arXiv preprint arXiv:1704.06933, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of International Conference on Machine Learning (ICML), 2015.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2017.

A GRADIENT OF -DM OBJECTIVE

The gradient of -DM can be obtained as follows:

L(,)() =  -

 (1 - )

1-

p (y|x)q(1-)(y|x)

xX

yY

=

-

 (1 -

)

p (y|x)q(1-)(y|x)

xX yY

=

-

1

 -



p (y|x)q(1-)(y|x) log p(y|x)

xX yY

=-

p(,)(y|x) log p(y|x),

xX yY

where

p(, ) (y|x)

=

1

 -



p(y

|x)q(1-)(y|x).

In Eq. (18), we used the so-called log-trick: p(y|x) = p(y|x) log p(y|x).

10

(16) (17) (18) (19)
(20)

Under review as a conference paper at ICLR 2018

B PROOFS OF PREPOSITIONS

Proposition 1 Assume that probability distribution p has the same finite support S as that of q, and that for any s  S there exists  > 0 such that p(s) >  holds. For any   (0, 1) the following
holds.

sup DKL(p q) - DA()(p q)  C(1 - ).
p

(21)

Here, C = max supp p log2(q/p) , supp q log2(q/p) .

Proof. By Taylor's theorem, there is an   (, 1) such that

Therefore,

x(1-) = 1 - log x · ( - 1) + x(1- ) log2 x · ( - 1)2 2

DA()(p

q) := - 1 

q p log() p

=

-

(1

1 -

)

p

q

(1-)
-1

p

= -1

q p log

p

- (1 - ) 2

p

q

(1- )
log2

p

q p

(22)
(23) (24) (25) (26)

Therefore, by Jensen's inequality we have

sup DKL(p q) - DA()(p q)
p

= (1 - ) sup
p

p

q

(1- )
log2

p

q p

= (1 - ) sup
p

p q1- log2 q p

 (1 - ) sup
p

( p + (1 -  )q) log2 q p

 (1 - )  sup
p

p log2 q + (1 -  ) sup pp

 (1 - ) { C + (1 -  )C}

= C(1 - ),

q log2 q p

(27)
(28)
(29) (30) (31) (32) (33)

where C = max supp p log2(q/p) , supp q log2(q/p) .

Proposition 2 Assume that p has the same finite support S as that of q(), and that for any s  S, there exists  > 0 such that p(s) >  holds. For any   (0, 1), the following holds.

sup L()() - L~(,)()  C1(1 - ) + C2,

where L~(,) := L(,). Here, C1, C2 is universal constants irrevant to .

(34)

11

Under review as a conference paper at ICLR 2018

Proof.

Note that L()() =  DKL(p|q()) - Z( ) where Z( )

xX yY exp(r(y, y|x)/ ). By Proposition 1 we have

:=

sup L()() - L~(,)()


(35)

=  DKL(p|q()) - Z( ) -  DA (p|q())
Xx
  DKL(p|q()) - DA (p|q()) + |Z( )|
xX
 C1(1 - ) + C2,

(36) (37) (38)

where C1 =  max sup xX yY p log2(q()/p) , sup xX yY q() log2(q()/p) and C2 = |Z( )|.

C CATALOG OF OBJECTIVE FUNCTIONS AND THEIR GRADIENTS

In this section, we summarize the objective functions of
· ML (Maximum Likelihood), · RL (Reinforcement Learning), · RAML (Reward Augmented Maximum Likelihood; Norouzi et al., 2016), · EnRL (Entropy regularized Reinforcement Learning), and · -DM (-Divergence Minimization Training).

Objectives. The objective functions of ML, RL, RAML, EnRL, and -DM are as follows

L() = -

q(y|x) log p(y|x),

xX yY

L() = -

p(y|x)r(y, y),

xX yY

L()() = -

q()(y|x) log p(y|x),

xX yY

L()() = -

p(y|x) r(y, y|x) -  log p(y|x) ,

xX yY

L(, ) ()

=

-

 (1 -

)

1 - p (y|x)q((1-) )(y|x) ,

xX yY

(39) (40) (41) (42) (43)

where q()(y|x)  exp {r(y, y|x)/ }. Typically, q(y|x) = (y, y|x) where y is the target with the highest reward.
We can rewrite some of these functions using KL or -divergences:

L()() = DKL(q() p) + constant,
xX

L()() = 

DKL(p q()) + constant,

xX

L(,)() = 

DA()(p q()).

xX

(44) (45) (46)

12

Under review as a conference paper at ICLR 2018

In the limits, there are the following connections between the objectives.

lim
 0

L( )

=

L(),

lim
 0

L(

)

=

L(),

lim
0

L(,

)()

=



L( ) ()

+

constant,

lim
1

L(,

)()

=

L( ) ()

+

constant.

(47) (48) (49) (50)

Gradients. We list the gradient of each objective function and summarize the connections of them in the limit.

L() = -

q(y|x) log p(y|x),

xX yY

L() = -

p(y|x)r(y, y) log p(y|x)),

xX yY

(51) (52)

L()() = -

q()(y|x) log p(y|x),

xX yY

L()() = -

p(y|x) r(y, y|x) -  log p(y|x)  log p(y|x),

xX yY

 L(,

)()

=

-

1

 -



p(y|x)q(1-)(y|x) log p(y|x)

xX yY

(53) (54) (55)

Each gradient corresponds to ML, RL, RAML, EnRL, and -DM. To derive Eq. (54), we used yY p(y|x) log p(y|x) =  yY p(y|x) = 0.
The following connections hold.

lim
 0

 L(

)()

=

 L()

lim
 0

 L(

)()

=

 L ()

lim
0

 L(, ) ()

=

 L()()

lim
1



L(,

)()

=

 L(

)()

+

constant.

(56) (57) (58) (59)

Here, Eq. (59) are derived by

lim
1



L(,

)()

= - lim  1 1 - 

p (y|x)q(1-)(y|x) log p(y|x)

xX yY

= - lim  1 1 - 

p (y|x)q(1-)(y|x) log p(y|x) -  p(y|x)

xX yY

yY

1

= -

p (y |x)

lim
1

1

-



xX yY

q( ) (y|x) p (y |x)

1-
-1

 log p(y|x)

= -

p(y|x) log

xX yY

q( ) (y|x) p (y |x)

 log p(y|x)

= - p(y|x) r(y, y|x) -  log p(y|x)  log p(y|x) + constant.
xX yY

(60) (61) (62)
(63) (64) (65)

13

