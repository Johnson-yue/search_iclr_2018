Under review as a conference paper at ICLR 2018

THE MULTILINEAR STRUCTURE OF RELU NETWORKS
Anonymous authors Paper under double-blind review

ABSTRACT
We study the loss surface of neural networks that involve only rectified linear unit (ReLU) nonlinearities from a theoretical point-of-view. Any such network defines a piecewise multilinear form in parameter space. As a consequence, optima of such networks generically occur in non-differentiable regions of parameter space and so any understanding of such networks must carefully take into account their non-smooth nature. We then proceed to leverage this multilinear structure in an analysis of a neural network with one hidden-layer. Under the assumption of linearly separable data, the piecewise bilinear structure of the loss allows us to provide an explicit description of all critical points.

1 INTRODUCTION

Empirical practice tends to show that modern neural networks have relatively benign loss surfaces, in the sense that training a deep network proves less challenging than the non-convex and non-smooth nature of the optimization would na¨ively suggest. Many theoretical efforts have attempted to explain this phenomenon and, more broadly, the successful optimization of deep networks in general (Gori & Tesi (1992); Choromanska et al. (2015); Kawaguchi (2016)). The properties of the loss surface of neural networks remain poorly understood despite these many efforts. Developing of a coherent mathematical understanding of them is therefore one of the major open problems in deep learning.

We focus on investigating the loss surfaces that arise from feed-forward neural networks where ReLUs (x) := max(x, 0) = (x)+ account for all nonlinearities present in the network. We allow the transformations defining the hidden-layers of the network to take the form of fully connected affine transformations, convolutional transformations or some other combination of structured affine maps. For the network criterion we elect to use the hinge loss objective

R
(y^, r) =  1 + y^q - y^r
q=1

for classification. We use this choice for two reasons. First, the hinge loss is the natural choice if we wish to maintain ReLU nonlinearities throughout the network. As the only nonlinearities from input to loss are ReLUs, each input simply flows through a succession of affine and piecewise linear transformations. This rather homogeneous structure allows us to derive results concerning loss surface of such networks. Second, this choice also allows us to avoid certain pathologies that arise with other objectives; global minimizers generally do not exist, for instance, when using a logistic loss instead of the hinge loss.

To see the type of structure that emerges in these networks, let  denote the space of network parameters and let L() denote the loss. Each nonlinearity involved in the network, including the hinge loss, is either active ((x) > 0) or inactive ((x) = 0) at any point  in parameter space. This dichotomy leads to a partition of the parameter space

 = 1  2  . . .  M

(1)

into cells, where each cell u corresponds to a given activation pattern of the nonlinearities. Crossing the boundary of a cell u corresponds to a ReLU switching from active to inactive, or vice-versa. The loss L() is therefore smooth in the interior of cells and (potentially) non-differentiable on cell
boundaries. In this way the decomposition (1) provides a description of the smooth and non-smooth
regions of parameter space.

1

Under review as a conference paper at ICLR 2018

(a) Loss Surface L()

(b) Parameter Space 

(c) Loss Surface L()

Figure 1: The loss surface corresponding to a piecewise multilinear form. In (a): Local minima
are located in the interior of the flat cells (3) and (5) (type I), on the boundary between cells
(1) and (2) (type II) and the boundary between cells (4) and (5) (type II). In (b): Parameter space  = R2 decomposes into a partition of five cells. The loss L on each cell is a sum of
multilinear forms. In (c): A rotation of (a) shows the saddle-like surface of the nontrivial forms on cells (1), (2) and (4).

We begin by using this decomposition to show that, when restricted to a fixed cell u, the loss L is a sum of multilinear forms. Thus the loss L() is a piecewise multilinear form1, and different multilinear forms characterize the loss on different cells. To see the significance of this structure, recall that a multilinear form is a function  : Rd1 × . . . × Rdn  R which is linear with respect to each of its inputs when the other inputs are fixed. That is, each of the n linear relations
(v1, . . . , vk + wk, . . . , vn) = (v1, . . . , vk, . . . , vn) + (v1, . . . , wk, . . . , vn)
hold. The functions (x, y) = xy or (x, y, z) = xyz provide canonical examples of multilinear forms, and both of these functions clearly have a saddle like structure. In fact any nontrivial multilinear form has such a saddle-type structure, for the Hessian matrix of a nontrivial multilinear form always has at least one strictly positive and one strictly negative eigenvalue (see appendix for a proof of this statement). Consequently, the graph of a nontrivial multilinear form always has at least one direction of positive curvature and at least one direction of negative curvature. We therefore have the following picture for the loss surface L() of a piecewise multilinear form: inside each cell u the loss is either flat, linear or has a saddle-like structure; see figure 1 for a visual example. It is therefore impossible for a local minima to occur in the interior of a cell on which the loss has a linear or saddle-like structure, and so neural networks with ReLU nonlinearities have only two types of local minima --
· Type I (Smooth): Those local minima that occur in the interior of a cell with constant loss.
· Type II (Non-smooth): Those local minima that occur on a cell boundary.
We state this result precisely in theorem 1, but figure 1 already shows the presence of these two types of local minima.
This observation has several consequences. A (continuous time) gradient decent algorithm can never reach a type I local minimum. As soon as the algorithm enters a flat cell it must stop since the gradient vanishes at such points. The descent therefore terminates on the boundary of the cell, and so only non-smooth local minima arise when using a local, gradient-based algorithm. Moreover, this structure has potential implications for other various optimization algorithms. An off-the-shelf Newton method, for example, is inappropriate for such networks since the Hessian of L is never positive definite and typically is indefinite. In other words, any study of algorithms for such a loss must take into account both the nonsmooth structure and the indefinite structure of the loss surface. Local minimizers simply cannot be studied using second-order (i.e. Hessian) information.
We then proceed to leverage this structure in an analysis of a neural network with one hidden-layer. Under the assumption of linearly separable data, the piecewise bilinear structure of the loss allows us to provide an explicit description of critical points. The reasons for this analysis are two-fold. First,
1By "piecewise multilinear form" we really mean a sum of piecewise multilinear forms.

2

Under review as a conference paper at ICLR 2018

(a) L() = 0

(b) L() = 3/12

(c) L() = 12/12

Figure 2: Three different local minima of the loss L() for a network with two hidden neurons. Points belonging to class +1 (resp. -1) are denoted by stars (resp. squares). Data points for which the loss is zero (solved points) are colored in green, while data points with non-zero loss (unsolved points) are in red.

it allows us to understand how the addition of depth (i.e. a hidden-layer) affects the loss surface of a simple classification task. With separable data and a purely linear classifier, the corresponding convex optimization problem has only global minimizers with zero loss. Adding a hidden-layer affects this structure and complicates the loss surface, in the sense that non-optimal local minima now occur. Our analysis characterizes this precisely. Secondly, this simple problem serves as a model for the top of a deep network. As we generally expect linearly separable features given enough depth, any non-optimal critical points in a network with one hidden-layer and separable data might manifest in a deep network as well.

To describe the results of this analysis we recall that the hinge loss for binary classification takes the

form

(y^, t) =  1 - ty^),

(2)

where y^ denotes the scalar output of the network and t  {+1, -1} the classification target. For simplicity of exposition, consider the resulting one hidden-layer network without an output bias, i.e.

K

y^ = vk wk, x + bk

(3)

k=1

with ·, · denoting the Euclidian inner product. The network has K hidden neurons, and each hidden neuron has an associated hyperplane wk, · + bk as well as a scalar weight vk used to form the output. Figure 2 shows three different local minima of such a network with two hidden neurons.

The first panel, figure 2(a), shows a global minimum where all the data points have zero loss. Figure

2(b) shows a local minimum. All unsolved data points, namely those that contribute a non-zero

value to the loss, lie on the "blind side" of the two hyperplanes. For each of these data points the

corresponding network output y^ vanishes and so the loss is (y^, t) = 1 for these unsolved points.

Small perturbations of the hyperplanes or of the values of the vk do not change the fact that these data points lie on the blind side of the two hyperplanes. Their loss will not decrease under small

perturbations, and so the configuration is, in fact, a local minimum. The same reasoning shows that

the configuration in figure 2(c) is also a local minimum. All data points have loss equal to one,

and so this local minimum is also a global maximum. Finally, these configurations still define local

minimizers if we allow small perturbations to the data points; If the data points represent features of

a deep network then such configurations will define local minimizers of a deep network as well.

Despite the presence of sub-optimal local minimizers, the local minima depicted in figure 2 are somehow trivial cases. They simply come from the fact that, due to inactive ReLUs, some data points are completely ignored by the network, and this fact cannot be changed by small perturbations. We show that for binary classification tasks with linearly separable data these are, in fact, the only possible local minima that occur. More precisely, let us say that a hyperplane wk, · + bk is active if the corresponding vk is non-zero. Then at any local minima, a data point with nonzero loss must lie in the blind side of all active hyperplanes. This result remains true if a bias is added in (3) to the output neuron. For multi-class tasks the answer is more delicate (c.f. section 4), but if we apply an appropriate modifications then the multilinear structure allows us to conclude the analogous result for multi-class partitioning problems as well. In fact, this analysis shows how to reduce the study of any multilinear deep network to that of a binary classification problem.

3

Under review as a conference paper at ICLR 2018

Previous work also address the loss surface of ReLU neural networks, c.f. Safran & Shamir (2016) and Choromanska et al. (2015). The first reference uses ReLU nonlinearities to partition the parameter space into basins that, while similar in spirit, are different from our notion of cells. They estimate the probability of initializing the network in a basin containing a good local minimum under various assumptions on the distribution of data points. However, as noted by the authors, there is no reason to believe that a descent algorithm initialized inside such a basin will actually converge to the local minimum within it. The second reference investigates "randomized" ReLU networks. It provides a description of the quality and asymptotic distribution of the local minima under the assuption that the ReLU activations are independent Bernoulli variables. Similar ideas were pursued in Dauphin et al. (2014) and Kawaguchi (2016). In a different vein, the loss surface of fully connected neural networks with smooth nonlinearities (i.e, sigmoid or tanh) and 2 loss have also received attention. The dominant strand of this line of work focuses on a search for situations wherein local minima and global minima coincide. For example, if the weight matrices and features at a given layer of the network satisfy certain structural assumptions (e.g. full rank conditions and linear independence) then such a "local equals global" result holds, c.f. Gori & Tesi (1992); Yu & Chen (1995); Frasconi et al. (1997); Nguyen & Hein (2017). Deep linear networks, i.e. a deep network with no nonlinearities, represent the extreme case of this line of work. It is shown in Baldi & Hornik (1989); Baldi & Lu (2012); Kawaguchi (2016) that these networks do not have sub-optimal local minimizers.

2 PIECEWISE MULTILINEAR STRUCTURE

We begin by describing the precise manner in which ReLU networks give rise to piecewise multilin-
ear forms. This will entail both a precise formulation of the decomposition of parameter space into
cells as well as an explicit description of the multilinear structure of the loss on each cell. We shall
employ the following notation when accomplishing these two tasks. Bold-face Roman and Greek letters such as x, y, ,  denote vectors in standard Euclidean space, with x, y := xT y the usual inner-product and x  y := xyT the standard outer-product of vectors. Their light-face Roman
counterparts with sub-scripts xi, yi, j, k denote individual entries. Capital Roman letters such as U, V, W will always refer to matrices while the corresponding lower-case letters uij, vij, wij will denote the corresponding matrix entries. We reserve Id for the identity matrix, 0 = (0, . . . , 0)t for the zero vector and 1 = (1, . . . , 1)t for the constant vector. We reserve parenthetical super-scripts
such as x(i) or W ( ) for enumerating a collection of vectors or matrices, respectively. We view a collection of N labelled data points as a set of ordered pairs (x(i), y(i)) with the x(i) representing
generic points belonging to Rd and y(i)  RR representing one-hot vectors coding for the class of the ith data point. All the proofs are presented in the appendix.

Our analysis considers the following multi-class model with hinge loss. Fix a target y and let y^ denote the prediction of the network. Then the expression

R
(y^, y) = -1 +  1 + y^q - y, y^ = -1 + 1 ,  (Id - 1  y)y^ + 1
q=1

(4)

furnishes the multi-class hinge loss. We consider a neural network with L hidden layers,

x(i, ) = (W ( )x(i, -1) + b( )) y^(i) = V x(i,L) + c

for = 1, . . . , L

z^(i) =  (Id - 1  y(i)) y^(i) + 1 ,

(5)

where x(i, ) denotes the feature vector of the ith data point at the th layer (with the convention that x(i,0) = x(i)), y^(i) denotes the output of the network for the ith datum and z^(i)  RR describes the loss of data point x(i) associated with each of the R classes. The matrices W ( ) and vector b( ) define the affine transformation at layer of the network, and V and c denote the weights and bias of the output layer. We allow for fully-connected as well as structured models, such as convolutional networks, by imposing the assumption that each W ( ) is a matrix-valued function that depends linearly on some set of parameters ( ) --
W ( ) ( ) + ^( ) = W ( ) ( ) + W ( ) ^( ) ;

4

Under review as a conference paper at ICLR 2018

thus the collection  = ((1), . . . , (L), V, b(1), . . . , b(L), c)   represent the parameters of the network and  denotes parameter space, i.e. a vector space. We let d denote the dimension of the features at layer of the network, with the convention that d0 = d (dimension of the input data) and dL+1 = R (number of classes). We use D = d1 + . . . + dL+1 for the total number of pointwise nonlinearities and Np := dim() for the total number of parameters. We then finally arrive at the expression
L() = 1 N 1, z^(i) - 1 N
i=1
for the total loss over all data points and all classes.

2.1 PARTITIONING  INTO CELLS

The lack of differentiability of the nonlinearity (x) induces a subsequent lack of differentiability of L(), but we may still characterize differentiable regions of the loss precisely. This characterization will prove essential for our analysis. Given a data point x(i) let us define the collection of functions

(i, )() :=  (W ( )x(i, -1) + b ) for = 1, . . . , L

(i)() :=  (Id - 1  y(i)) y^(i) + 1 ,

(6)

where we make the arbitrary re-definition  (0) := 1/2 to handle those points where  (x) does not exist. Thus (i, ) :   {0, 1/2, 1}d and (i) :   {0, 1/2, 1}R, and by collecting all of these functions into a single signature function
S() = (1,1)(), . . . , (1,L)(), (1)(); . . . . . . ; (N,1)(), . . . , (N,L)(), (N)()

we obtain a function S :   {0, 1/2, 1}ND since there are D total nonlinearities and N total data points. The signature function S describes how each ReLU in the network activates. These activations take one of three possible states, the fully active state (encoded by a one), the fully inactive state (encoded by a zero), or an in-between state (encoded by a 1/2). If none of the N D entries of S() equal 1/2 then all of the ReLUs are differentiable near , and so the loss L is smooth near such points. With this in mind, for a given u  {0, 1}ND we define the cell u as the (possibly empty) set
u := S-1(u) := {   : S() = u}
of parameter space. By choice L is smooth on each non-empty cell u, and so the cells u provide us with a partition of the parameter space



=

u

u{0,1}N D

N.

into smooth and potentially non-smooth regions. The set N contains those  for which at least one of the N D entries of S() takes the value 1/2, which implies that at least one of the nonlinearities is non-differentiable at such a point. Thus N consists of points at which the loss is potentially nondifferentiable. The following lemma collects the various properties of the cells u and of N that we will need in the rest of the paper.
Lemma 1. Each cell u for u  {0, 1}ND is an open set. If u = u then u and u are disjoint. The set N is closed and has Lebesgue measure 0.

2.2 PIECEWISE MULTILINEAR STRUCTURE

Let us briefly assume for the sake of exposition that instead of (5) we have a simplified model without any bias parameters

L((1), . . . , (L), V ) := -1 + 1

1T (T (i)V  W (L) · · · (W (2)(W (1)x(i)))) + 1

N

i

5

Under review as a conference paper at ICLR 2018

where T (i) := Id - 1  y^(i). By definition, inside a cell u each nonlinearity (x) acts as a linear function, i.e. matrix multiplication by a diagonal matrix, or mask, containing zeroes or ones in its diagonal. More precisely, restricted to the cell u the loss takes the form

1T E (i,u)1 1

L|u = -1 +

N

+ N

1T E (i,u)T (i)V (i,L,u)W (L) · · · (i,2,u)W (2)(i,1,u)W (1)x(i)

i

E (i,u) := diag((i,u)), (i, ,u) := diag((i, ,u));

the "u" super-scripts in (i,u), (i, ,u) indicate the dependence of the masks on the cell. Up to the constant factor -1 + 1, E(i,u)1 /N that simply counts the average number of errors on the cell, it is then clear that L|u is a multilinear form of its arguments. As a consequence, the loss is a piecewise multilinear form up to constants and it is therefore smooth in the interior of each cell. As non-zero multilinear forms do not have local minima, it is clear that a local minimum of the loss can only occur (i) in the interior of cells where the loss is constant, or (ii) on the boundary of one or more cells. Going back to our case of interest (5), the presense of bias parameters complicates the picture slightly -- the loss on a cell is now a sum of multilinear form rather than a single multilinear form. However, the overall conclusions regarding local minima remain unchanged. The following theorem describes the precise result.
Theorem 1 (Structure of the loss).

(i) For each cell u there exist multilinear forms 0u, u1 , . . . , uL, a linear function Lu+1 and a constant uL+2 such that
L|u ((1), . . . , (L), V, b(1), . . . , b(L), c) = 0u((1), (2), (3), (4) . . . , (L), V ) +1u(b(1), (2), (3), (4) . . . , (L), V ) +2u(b(2), (3), (4) . . . , (L), V ) +3u(b(3), (4) . . . , (L), V ) ...
+uL-1(b(L-1), (L), V ) +Lu (b(L), V ) +Lu +1 (c) +Lu +2 .
The constant uL+2 counts the average number of errors on the cell.
(ii) The loss L is smooth on each cell u. Moreover, if    \ N and the Hessian matrix HL() does not vanish then it must have at least one strictly positive and one strictly negative eigenvalue.

(iii) Local minima and maxima of L occur only on cell boundaries (i.e. on N ) or on those cells u where the loss is constant. In the latter case, L|u () = Lu+2 for all   u.

3 CRITICAL POINT ANALYSIS

Recall the hinge loss

(y^, y) :=  1 - yy^

for binary classification, where y  {+1, -1} denotes the target. For a given set of parameters  = (W, v, b, c) the expression

L(W, v, b, c) = 1

 1 - y(i) vT (W x(i) + b) + c

N

i

(7)

then defines the loss associated to a fully connected network with one hidden layer. Let {wk}1kK denote the rows of the linear transformation W defining the hidden layer. A straightforward com-
putation shows that we may specify the multilinear forms in theorem 1 more precisely.

6

Under review as a conference paper at ICLR 2018

Lemma 2 (Decomposition with L = 1). Let

L|u (W, v, b, c) = 0u(W, v) + 1u(b, v) + u2 (c) + u3

denote the loss on a cell u. For 1  k  K define



ak(u)

:=

1 N



(i,u)(ki,u)x(i) -

(i,u)k(i,u)x(i)

i:y (i) =1

i:y (i) =-1



k(u)

:=

1 N



(i,u)k(i,u) -

(i,u)k(i,u)

i:y (i) =1

i:y (i) =-1



(u) := 1 

(i,u) -

(i,u)

N

i:y (i) =1

i:y (i) =-1

u := 1

(i,u).

N

i

(8)
(9) (10) (11)

Then 3u = u and 2u(c) = -(u)c, while the relations
u0 (W, v) = - vk a(ku), wk , and u1 (b, v) = - vkk(u)bk,
kk
furnish the multilinear forms defining the loss on u.

(12)

With this description in hand, we may now explore the consequences of this decomposition under
the assumption of linearly separable data. Since the data are linearly separable there exists a unit vector q  Rd, a bias   R and a margin µ > 0 such that the family of inequalities

q, x(i) +   µ q, x(i) +   -µ

if y(i) = +1 if y(i) = -1

(13) (14)

hold. By combining (9)­(10) with (13)­(14) we easily obtain the following estimate

a(ku), q + k(u)  µ

1 N

(i,u)(ki,u) ,

i

(15)

which we may then use to find a decent direction for the loss whenever the right-hand-side of (15) does not vanish. The idea is simple, i.e. that adding a multiple of ±q to wk and a multiple of ± to bk will usually lead to a decrease of the loss. To see this let ek = (0, . . . , 1, . . . , 0)T denote the kth standard basis vector, u the closure of the cell u and sign(x) the signum function that vanishes at zero. The following lemma then makes this idea precise.
Lemma 3. Let  = (W, v, b, c)   denote any point. Define

W~ = sign(vk) ek  q and b~ =  sign(vk) ek. For t  R let (t) := (W + tW~ , v, b + tb~, c) denote a perturbation of . Then

(i) There exists t0 > 0 and u  {0, 1}ND such that (t)  u for all t  [0, t0).

(ii)

L()



L((t))

+

t|vk |

µ N

i (i,u)k(i,u) for all t  [0, t0).

We may now state and prove the theorem underyling figure 2 in full generality. If we let (i)()

denote

the contribution of the ith

data point x(i)

to

the total loss, so that L()

=

1 N

i (i)(),

then we may conclude

Theorem 2. Let  = (W, v, b, c) be a local minimum of the loss and assume the data {x(i)}Ni=1 are linearly separable. Then

(i)() > 0 = vk ( wk, x(i) + bk) = 0 for all k  {1, . . . , K}.

7

Under review as a conference paper at ICLR 2018

Essentially, this theorem says that local minima obey the property sketched in figure 2. If a data point x(i) has non-zero loss one of vk or ( wk, x(i) + bk) must vanish for all hidden neurons. We therefore have a dichotomy. Either x(i) lies in the blind side of the hyperplane wk, x(i) + bk or else vk = 0. In the latter case the kth feature is not used when forming network predictions and so the corresponding hyperplane is inactive. Succinctly, theorem 2 states that if a data point x(i) is unsolved it must lie on the blind side of every active hyperplane. Moreover, this result applies to both critical points as well as to local minimizers. While the proof of theorem 2 only yields the result for minimizers, it has the benefits of both transparency and directness ­ we invite the reader to read the proof of Lemma 3(ii) and theorem 2 in the appendix, which are particularly simple.
Extending the result of theorem 2 to include critical points is less direct, and it requires an invocation of machinery from non-smooth analysis. To begin, we recall that for a Lipschitz but nondifferentiable function f () the Clarke subdifferential of () of f at a point    provides a generalization of both the gradient f () and the usual subdifferential of a convex function. For a Lipschitz function f we may employ the following definition (c.f. page 133 of Borwein & Lewis (2010)).
Definition 1 (Clarke Subdifferential). Suppose that a function f :   R is locally Lipschitz around   , and differentiable on  \ M where M is a set of Lebesgue measure zero. Then the convex hull
of () := c.h. lim f (k) : k  , k / M
k
is the Clarke subdifferential of f at .
With this definition in hand, we can now state the following stronger version of theorem 2:
Theorem 3. Let  = (W, v, b, c) and assume that 0  oL(). Assume also that the data {x(i)}Ni=1 are linearly separable. Then
(i)() > 0 = vk ( wk, x(i) + bk) = 0 for all k  {1, . . . , K}.

4 EXACT PENALTIES AND MULTI-CLASS STRUCTURE

Unfortunately, the result of theorem 2 and 3 does not extend na¨ively to the multi-class case. To the contrary, counter-examples show that there exist non-trivial critical points for linearly separable data whenever the number of classes exceeds two. In other words, in the presence of three or more classes a critical point may contain active yet unsolved data points. This begs the question of whether some variant of theorem 3 holds in the multi-class context. A first attempt might simply modify the loss itself. We might hope that substituting the multi-class hinge loss (4) with its one-versus-all variant

R
¯(y^, y) :=  1 + y^r(i)(-1)yr(i)
r=1

L¯() = 1 N ¯ y^(i), y(i) N
i=1

(16)

would restore the two-class structure of critical points. The idea here is to use the binary hinge loss ¯ in the hope of decoupling a multi-class problem into R two-class problems. Yet similar counter-

examples dash this hope as well, as non-trivial critical points persist for network with modified loss

(16) and one hidden layer. The inherent difficulty comes from the fact that all of the parameters  in

the network still couple through the joint nonlinear minimization of (16), and so simply modifying

the hinge loss does not restore the two-class structure.

We may, however, introduce a sufficient amount of decoupling if we modify both the loss as well as the algorithm used in its optimization. Let us begin this process by recalling that

x(i,L) (1), . . . , (L), b(1), . . . , b(L) and y^(i) = V x(i,L) + c

denote the features and predictions of the network with L hidden layers, respectively. The subcollection of parameters
 := (1), . . . , (L), b(1), . . . , b(L)

therefore determine a common set of features x(i,L) while the parameters V, c determine R oneversus-all classifiers utilizing these features. We may write the loss for the rth class as

L(r)( , vr, cr)

=

1 N

N


1 + y^r(i)(-1)yr(i)

i=1

8

Under review as a conference paper at ICLR 2018

and then sum L¯() := (L(1) + · · · + L(R))() to recover the total objective. Thus each classifier in L¯ shares a common set of features, and the joint minimization over features and classifiers couples the R binary problems together. We may then seek to minimize L¯ by applying a soft-penalty approach. We introduce the R replicates
 (r) = (1,r), . . . , (L,r), b(1,r), . . . , b(L,r) 1  r  R

of the hidden-layer parameters  and include a soft 2-penalty

R  (1), . . . ,  (R) := R

LR
( ,r) - ¯( ) 2 + b( ,r) - b¯( ) 2

R-1

=1 r=1

to enforce that the replicated parameters ( ,r), b( ,r) remain close to their corresponding means (¯( ), b¯( )) across classes. We then proceed by minimizing the penalized loss

R
E (1), . . . , (R) := L(r) (r) + R  (1), . . . ,  (R)

(17)

r=1

for  > 0 some parameter controlling the strength of the penalty. Remarkably, performing this process yields

Theorem 4 (Exact Penalty and Recovery of Two-Class Structure). If  > 0 then the following hold for (17) --

(i) The penalty is exact, that is, at any critical point (1), . . . , (R) of E the equalities

hold for all 1 

( ,1) = · · · = ( ,R) = ¯( ) := 1

R
( ,r)

R

r=1

b( ,1) = · · · = b( ,R) = b¯( ) := 1

R
b( ,r)

R

r=1

 L.

(ii) At any critical point of E the two-class critical point relations 0  0L(r)( , vr, cr)
hold for all 1  r  R.

In other words, applying a soft-penalty approach to minimizing the coupled problem L¯ actually

yields an exact penalty method. By (i), at critical points we obtain a common set of features x(i,L)

for each of the R binary classification problems. Moreover, by (ii) these features simultaneously

yield critical points

0  0L(r)  , vr, cr

(18)

for all of these binary classification problems. If (18) holds then clearly the weaker critical point relation 0  0L¯() for the full loss holds as well, and so the penalty approach certainly yields critical points of the original loss. More importantly, the fact that (18) may fail for critical points of L¯ is responsible for the presence of non-trivial critical points in the context of a network with one
hidden layer. We may therefore interpret (ii) as saying that the penalty avoids pathological critical points where 0  0L¯() but (18) does not. To be clear, we may say

Corollary 1. Assume the hypotheses of theorem 3, and that the {x(i)} are linearly separable. Let  denote any critical point of E and (i,r)() the loss associated to the ith data point and the rth class. Then

(i,r)() > 0 = (vr)k ( wk, x(i) + bk) = 0 for all k  {1, . . . , K}.

The corollary follows immediately from (18) and the argument for the two class case. In principle, the penalty approach also provides a path forward for studying multi-class problems. Regardless of the number L of hidden layers, an understanding of the family of critical points (18) reduces to a study of critical points of binary classification problems.

9

Under review as a conference paper at ICLR 2018
REFERENCES
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
Pierre Baldi and Zhiqin Lu. Complex-valued autoencoders. Neural Networks, 33:136­147, 2012. Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory and
examples. Second Edition. Springer Science & Business Media, 2010. Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015. Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014. P Frasconi, M Gori, and A Tesi. Successes and failures of backpropagation: A theoretical. Progress in Neural Networks: Architecture, 5:205, 1997. Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76­86, 1992. Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016. Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pp. 2603­2612, 2017. Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016. Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300­1303, 1995.
10

Under review as a conference paper at ICLR 2018

APPENDIX: PROOFS OF LEMMAS AND THEOREMS

Multilinear forms have a saddle like structure
Lemma. The Hessian matrix of a non-trivial multilinear form has at least one strictly positive and one strictly negative eigenvalue.

Proof. A multilinear form  : Rd1 × . . . × Rdn  R can always be written as

d1 dn
(v1, . . . , vn) = . . . Aj1,...,jn v1,j1 . . . vn,jn
j1=1 jn=1

(19)

for some tensor {Aj1,...,jn : 1  jk  dk}. Here vk,j denotes the jth component of the vector vk.

From (19) it is clear that

2  vk2,j

= 0 and therefore the trace of the Hessian matrix of  is equal to zero.

This implies that the sum of the eigenvalues of the Hessian is equal to zero. So if the Hessian is not

the zero matrix, then it has at least one strictly positive and one strictly negative eigenvalue.

Proof of Lemma 1

The features x(i, ) at each hidden layer depend in a Lipschitz fashion on parameters. Thus each u defines an open set in parameter space. Moreover, if u = u~ then u and u~ are disjoint by definition. If  / u for all u  {0, 1}ND then at least one of the equalities

bj( ) = - wj( ), x(i, -1) or cs = - 1 + vs - vr, x(i,L) ,

(20)

must hold. The set of parameters N   where an equality of the form (20) holds corresponds to a Lipschitz graph in  of the bias parameter for that equality. We may therefore conclude that



N :=  \ 

u

u{0,1}N D

defines a set contained in a finite union of Lipschitz graphs. Thus N is (Np - 1)-rectifiable, and in particular, has Lebesgue measure zero. That N is closed follows from the fact that it is the
complement of an open set.

Proof of Theorem 1

Part (i) follows by carefully expanding

L = -1 + 1 N

1T  T (i)(V (WL(. . . W2(W1x(i) + b1) + b2 . . .) + bL) + c) + 1

i

where T (i) := Id-1y^(i). Part (ii) comes from the fact that the trace of the Hessian of a multilinear form is equal to zero. To prove part (iii), note part (ii) implies that for any ^  u there exists a small neighborhood
B(^ ) := {   :  - ^ < }  u
on which L|u () is constant. Thus
L|u (^ + ) = L|u (^ )
must hold for all  and all  small enough. Now use part (i) and multilinearity to expand the left-hand-side into powers of :

L
L|u (^ + ) = L|u (^ ) + kfk() + L+1 0u + u1 ().
k=1

(21)

11

Under review as a conference paper at ICLR 2018

That u0 + u1 () is, in fact, the highest-order term is a consequence of the multilinear decomposition from part (i). Since (21) must hold for all  small enough, all like powers must vanish

fk() = 0 and

u0 + 1u () = 0.

Now take any  with b(1) = 0 to conclude

u0 ((1), (2), (3), (4) . . . , (L), V ) = 0 for all ( ), V . But then 0u + u1 () = 0 for all  implies

u1 (b(1), (2), (3), (4) . . . , (L), V )
for all ( ), V, b(1) as well. Thus u0 + u1 is the zero function, and so u2 is the highest-order multilinear form in the decomposition from part (i). This implies that

fL() = 2u(b2, (3), . . . , (L), V ), but fL must vanish by (21). Thus u2 is the zero function as well. Continuing in this way shows that each u is the zero function for 0   L + 1, and so in fact

as claimed.

L|u () = Lu+2

Proof of Lemma 2

Restricted to a cell u, the loss can be written

1 L|u (W, v, b, c) = N
1 =
N

 -y(i) vT (W x(i) + b) + c + 1
i
(i,u) -y(i) vT (i,u)(W x(i) + b) + c
i

+1

Expanding parenthesis after parenthesis the above formula leads to:

L|u(W, v, b, c)

=

1 N

(i,u) -y(i) vT (i,u)W x(i) + vT (i,u)b + c + 1

i

1 =

(i,u) -y(i)vT (i,u)W x(i) - y(i)vT (i,u)b - y(i)c + 1

N

i

1 =

-(i,u)y(i)vT (i,u)W x(i) - (i,u)y(i)vT (i,u)b - (i,u)y(i)c + (i,u)

N

i

= (0u)(W, v) + 1(u)(b, v) + (2u)(c) + (3u)

Letting wk be the kth row of the matrix W , and noting that vT (i,u)W = k vkk(i,u)wkT we find that

0(u)(W,

v)

=

-

1 N

(i,u)y(i)

i

vkk(i,u)wkT x(i)
k

1 =-
N
i

(i,u)y(i)vkk(i,u) wk, x(i)
k

= - vk
k

1 N

(i,u)y(i)k(i,u)x(i), wk

i

= - vk a(ku), wk

k

12

Under review as a conference paper at ICLR 2018

where the vector ak(u) is defined by

ak(u)

=

1 N

y(i)(i,u)k(i,u)x(i)

i



1 =N

(i,u)k(i,u)x(i) -

(i,u)(ki,u)x(i)

i:y (i) =1

i:y (i) =-1

Similarly we find that

(1u)(b,

v)

=

-

1 N

(i,u)y(i)vT (i,u)b

i

=-1

(i,u)y(i)

N

ik

vk k(i,u) bk

= - vk
k

1 N

(i,u)y(i)(ki,u)

i

= - vkkubk
k

bk

where



k(u)

=

1 N



(i,u)k(i,u) -

(i,u)k(i,u)

i:y (i) =1

i:y (i) =-1

and finally we have

1(u)(c) = -(u)c

where



(u) = 1 

(i,u) -

(i,u)

N

i:y (i) =1

i:y (i) =-1

Proof of Lemma 3
Proof of (i)  (ii). We start by using (i) to prove (ii). First note that (8) holds for   u due to the continuity of the loss. By part (i), (t) remains in some fixed u for all t small enough. Thus (9-11) apply. The bilinearity of u0 and 1u then yield
L((t)) - L() = tu0 (W~ , v) + tu1 (b~, v) = -t|vk| a(ku), q + k(u) , which combined with (15) proves (ii).

Proof of (i). We now prove part (i). While straightforward, the proof is a little longer. Let us denote by (t) = (W + tW~ , v, b + tb~, c) = (W (t), v, b(t), c) the perturbation considered in the lemma. Without loss of generality, let us choose k = 1. Then the first row of W (t) and the first entry of b(t) are given by
w1(t) = w1 + tsign(v1)q, b1(t) = b1 + tsign(v1)
whereas the other rows and entries remains unchanged,

wk(t) = wk, and bk(t) = bk for k  2.

Define the activations,

1(i)(t) = w1(t), x(i) + b1(t) = w1, x(i) + b1 + tsign(v1)

k(i) = wk, x(i) + bk

for k  2.

q, x(i) + 

13

Under review as a conference paper at ICLR 2018

so that the functions involved in the signature S((t)) can be written as:

(1i)((t)) =  (1(i)(t))
(ki)((t)) =  (k(i)) for k  2
K
(i)((t)) =  1 - y(i) c + v1(1(i)(t)) + vk(k(i))
k=2

(22) (23)

Recall that that signature function, for the network considered here, is simply given by the collection of all the functions (i) = (1(i), . . . , (ki))T and (i):

S((t)) = (1)((t)), (1)((t)); . . . ; (N)((t)), (N)((t))

For the network considered here, since there are K hidden neurons, one output neuron, and N data points, we have that S :   {0, 1/2, 1}N(K+1). We now make the following claim, that will be proven at the end of this section:
Claim. There exists t0 > 0 such that the function t  S((t)) is constant on (0, t0).
Note that the above claim implies that for t  (0, t0), (t) either remains in a fixed cell u (if none of the entries of S((t)) are equal to 1/2) or on the boundary of a fixed cell u (if some of the entries of S((t)) are equal to 1/2). In both cases we have that (t)  u for all t  (0, t0). Since (t) is continuous and since u is closed, we then clearly have that (t)  u for all t  [0, t0), which conclude the proof of Lemma 3(i). We now prove the claim:

Proof of the claim. Let us fix i  {1, . . . , N }. Note that the function t  1(i)(t) is monotone and continuous (it is simply an affine function of t). As a consequence, the quantity appearing inside  [·] in equation (23), that is,
K
g(t) = 1 - y(i) c + v1(1(i)(t)) + vk(k(i))
k=2
is also continuous and monotone. Without loss of generality, let assume that g is non-decreasing. Continuity and monotonicity then implies that there are 3 intervals (-, a), [a, b] and (b, ) on which g is strictly negative, equal to zero, then strictly positive (with the understanding that a  b, and both a and b can take infinite values, in which case we would have less than three intervals). As a consequence (i)((t)) is equal to 0, then 1/2, then 1 on each of these three intervals. Note that one can always choose  (i) > 0 small enough so that (0,  (i)) is fully contained in one of these three intervals, and this implies that (i)((t)) is constant on the interval (0,  (i)). A similar line of reasoning shows that there exists ^(i) > 0 such that (1i)((t)) is constant on (0, ^(i)). The interval (0, t0) is then obtained by taking the intersection of all these intervals:
N
(0, t0) = (0,  (i))  (0, ^(i))
i=1

Proof of Theorem 2
The proof is by contradiction. Suppose (i)() > 0 and for some k both vk = 0 and ( wk, x(i) + bk) = 0 hold. Consider the perturbation (t) of lemma 3. Then there exists u  {0, 1}ND and t0 > 0 such that (t)  u for t  [0, t0). By continuity of (t) there exists ^ = (W^ , v^, b^, c^)  u such that (i)(^ ) > 0 and ( w^ k, x(i) + ^bk) = 0. Thus (i,u) = 1 and (ki,u) = 1 in u. As |vk| > 0 lemma 3(ii) implies that the perturbation leads to a strict decrease of the loss, which contradicts the assumption that  is a local minimizer.
14

Under review as a conference paper at ICLR 2018

Proof of Theorem 3

Definition 1 and theorem 1 allow us to compute the Clarke subdifferential of L at  relatively easily. First recall that the open cells u fill parameter space up to a set N of measure zero. If   N then  must lie on the boundary u of some cell. Define the incidence set

I() := u  {0, 1}ND :   u

of such a point   N as the collection of all such possible cells. Thus I() is both non-empty
and finite. If k   and k / N we may, by passing to a subsequence if necessary, assume
that k  u for some u  I() and all k sufficiently large. But then L(k) = L|u (k), and since L|u is a continuous function (i.e. a sum of multilinear gradients), it can be extended by continuity to the point   u and the limit L(k)  L|u () follows. We therefore conclude from definition 1 that the Clark subdifferential at   N is given by the convex hull



 0L() =

(u)L|u () : (u)  0,

 (u) = 1

uI()

u

(24)

where it has to be understood that L|u () denotes the extension by continuity of L|u to the point . If   u for some u we let I() = {u} denote its incidence set. As the gradient L depends continuously on  in cells we therefore have L(k)  L() and so (24) also gives
the Clark subdifferential in this case with (u) = 1.

Suppose now that 0  oL(), then we must have that

0 = (u)L|u ()
uI()

(25)

for some collection of positive coefficients (u) due to the characterization (24) of the subdifferential.
Using the explicit formula from lemma 2 we can compute the gradients L|u (). In particular, from equations (12) we find that

L|u wk

()

=

-vk ak(u)

and

L|u bk

()

=

-vk k(u)

Equation (25) then obviously implies:

(u)vka(ku) = 0
uI()

and

(u)vkk(u) = 0
uI()

for all k. The precise formula for ak(u) and bk provided in lemma 2 then give the equalities 

0 = vk 

(u)(i,u)k(i,u)x(i) -

(u)(i,u)k(i,u)x(i)

u i:y(i)=1

u i:y(i)=-1



0 = vk 

(u)(i,u)(ki,u) -

(u)(i,u)(ki,u)

u i:y(i)=1

u i:y(i)=-1

If vk = 0 then we may interchange summations to find

(ki)x(i) =

k(i)x(i)

i:y (i) =1

i:y (i) =-1

(i) k

=

(i) k

i:y (i) =1

i:y (i) =-1

where

(i) k

:=

(u)(i,u)k(i,u)

u

(26) (27)

We now claim that equalities (26)­(27) cannot happen unless all the

(i) k

vanish.

To

see this,

note

that if the

(i) k

do

not

vanish,

we

can

set

Q

=

i:y (i) =1

(i) k

=

i:y (i) =-1

(i) k

,

and

dividing

both

side of equality (26) by Q to obtain

(i) (i)

k
Q
i:y (i) =1

x(i) =

k

Q

i:y (i) =-1

x(i)

15

Under review as a conference paper at ICLR 2018

The above equation shows that a convex combination of data points of class +1 is equal to a convex combination of data points of class -1, which is not possible since the data points are linearly separable.

Assume now that for some data point x(i) we have (i)() > 0. Then by continuity of the loss we

also have (i) > 0 on each neighboring cell u, u  I(). Using the definition (6) of (i,u) we then

see that (i,u) = 1 for all u  I(). If vk = 0 for some k, then the corresponding

(i) k

must

be

equal

to zero, which necessarily implies that k(i,u) = 0 some u since the (i,u) are all equal to one and at

least one of the (u) is nonzero. This in turn implies ( wk, x(i) + bk) = 0 due to the definition of

the (ki).

Proof of Theorem 4

The notion of a cell u for the model (17) consists of sets (Cartesian products) of the form

u = u(1) × u(2) × · · · × u(R) ,

where each u(r)  {0, 1}ND denotes a signature collection for the individual two-class losses L(r)

and thus

u = u(1), . . . , u(R)  {0, 1}NDR

defines a signature collection for the full model. That sets of this form cover the product space  ×

· · ·× (R-copies) up to a set of measure zero follows easily from the fact that if (1), . . . , (R) /

u for all u then at least one of the u(r) (say u(1) WLOG) lies in the set



N :=  \ 

u(1) 

u(1) {0,1}N D

which has measure zero in . Thus u must lie in the set

N ××···×

which has measure zero in the product space  × · · · × , and so the union of the R measure zero sets of the form
×···×N ×···× contains all parameters (1), . . . , (R) that do not lie in a cell.

Now let (1), . . . , (R) denote any critical point. For each ( , r) we have

( R,r) =  ( ,r) - ~ ( ,r)

~( ,r) := 1

( ,s)

R-1

s=r

b( R,r) =  b( ,r) - b~( ,r)

b~( ,r) := 1

b( ,s)

R-1

s=r

by straightforward calculation. By definition of a critical point, for each cell u adjacent to the critical point there exist corresponding constants (u)  0 with u (u) = 1 so that the equalities

0=

(u)vr L¯|u

u

0=

(u) ( ,r) L¯|u + ( R,r) =  ( ,r) - ~ ( ,r) +

(u)( ,r) L¯|u

uu

0=

(u) b( ,r) L¯|u + b( R,r) =  b( ,r) - b~( ,r) +

(u)b( ,r) L¯|u

(28)

uu

hold for all 1   L and 1  r  R, where the final equalities in the second and third line follow

from the fact that R is smooth and so its gradients do not depend upon the cell. Now on any cell we

may decompose each L(r) into a sum of multilinear forms

L-1

L(r)|u = (0u,r) (1,r), . . . , (L,r), vr +

(u,r) b( ,r), ( +1,r), . . . , (L,r), vr

=1

+ L(u,r) b(L,r), vr + L(u+,r1)(cr) + L(u+,r2)

16

Under review as a conference paper at ICLR 2018

by theorem 1. For any multilinear form (v1, . . . , vn) we have

(v1, . . . , vn) = vk, vk (v1, . . . , vn)
for all 1  k  n by Euler's theorem for homogeneous functions. Taking the inner-product of (28) with vr, (L,r) and b(L,r) then shows

0 = (u) (0u,r) + · · · + (Lu,r)
u

0 = (u) 0(u,r) + · · · + L(u-,r1) +  (L,r) 2 - (L,r), ~(L,r)
u

0 = (u) (Lu,r) +  b(L,r) 2 - b(L,r), b~(L,r)
u

(29)

which upon adding the second and third equalities yields

(L,r) 2 + b(L,r) 2 = (L,r), ~ (L,r) + b(L,r), b~(L,r)

for all 1  r  R. By the definitions of ~(L,r) and b~(L,r) (c.f. lemma 4), this can happen if and only if
(L,1) = · · · = (L,R) and b(L,1) = · · · = b(L,R).
Using this in the second and third equations in (29) then shows that

0=

(u) (0u,r) + · · · + (Lu-,r1) =

(u) L(u,r)

uu

(30)

for all 1  r  R as well. Now take the inner-product of (28) with (L-1,r) and b(L-1,r) to find

0 = (u) 0(u,r) + · · · + (Lu-,r2) +  (L-1,r) 2 - (L-1,r), ~(L-1,r)
u

0=

(u) (Lu-,r1) +  b(L-1,r) 2 - b(L-1,r), b~(L-1,r)

u

Adding these equations and using (30) then reveals

(L-1,1) = · · · = (L-1,R) and b(L-1,1) = · · · = b(L-1,R)

must hold as well, and so also

0=

(u) 0(u,r) + · · · + (Lu-,r2) =

(u) L(u-,r1)

uu

must hold. Continuing from = L - 2 to = 1 by induction reveals

( ,1) = · · · = ( ,R) and b( ,1) = · · · = b( ,R).

for all 1   L and so the penalty is exact as claimed. Part (ii) then follows from part (i) since the

equalities

~( ,r) = ¯( ) = ( ,r)

b~( ,r) = b( ) = b( ,r)

for all ( , r) at any critical point. Thus (28) yields

0=

(u)vr L(r)|u(r)

u

0=

(u)(

L |(r)
,r) u(r)

u

0=

(u)b(

L |(r)
,r) u(r)

u

(31)

for all 1   L, 1  r  R. Now consider (31) for r = 1. Any cells appearing in the sum
(31) satisfy either ( , v1, c1)  u(1) or ( , v1, c1)  u(1) . If ( , v1, c1)  u(1) for some u(1) then (31) must consist only of gradients on the single cell u(1) and so ( , v1, c1)  u(1) is a critical point of L(1) in the classical sense. If ( , v1, c1)  u(1) for some u(1) in the sum then ( , v1, c1)  u(1) for all cells u the sum. Thus (31) consists of a positive combination of gradients of L(1) on cells adjacent to ( , v1, c1), and so ( , v1, c1) defines a critical point of L(1) in
the extended Clarke sense. Applying this reasoning for r = 2, . . . , R then yields part (ii) and proves
the theorem.

17

Under review as a conference paper at ICLR 2018

Lemma 4. For any R vectors x(1), . . . , x(R)  Rd, if

x(r) 2 = 1

x(s), x(r)

R-1

s=r

for all

then x1 = · · · = xR.

r  {1, . . . , R}

Proof. By relabelling if necessary, assume x(1) has largest norm. Thus x(1)  x(r) for all 1  r  R. If x(1) = 0 then there is nothing to prove. Otherwise apply Cauchy-Schwarz and the hypothesis of the lemma to find

x(1)

2

1 R-1

x(s)

s=1

x(1)

x(1)



1 R-1

x(s) .

s=1

The latter inequality implies x(1) = · · · = x(R) since x(1) has largest norm. Thus

x(1) 2 = 1 R-1

cos r x(1) 2

s=1

1 1 = R - 1 cos r
s=1

by the hypothesis of the lemma. The latter equality implies cos r = 1 for all r, and so the lemma is proved.

18

