Under review as a conference paper at ICLR 2018

LATENT SPACE ODDITY: ON THE CURVATURE OF DEEP GENERATIVE MODELS
Anonymous authors Paper under double-blind review

ABSTRACT
Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear "generator" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.

1 INTRODUCTION

Deep generative models (Goodfellow et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014) model the data distribution of observations x  X through corresponding latent variables z  Z and a stochastic generator function f : Z  X as

x = f (z).

(1)

Using reasonably low-dimensional latent variables and highly flexible generator functions allows these models to efficiently represent a useful distribution over the underlying data manifold. These approaches have recently attracted a lot of attention as deep neural networks are suitable generators, which leads to the impressive performance of current variational autoencoders (VAEs) (Kingma & Welling, 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014).

Consider the left panel of Fig. 1, which shows the latent representations of digits 0 and 1 from MNIST under a VAE. Three latent points are highlighted: one point (A) far away from the class boundary, and two points (B, C) near the boundary, but on opposite sides. Seemingly the points B and C near the boundary are very close to each other, while the third is far away from the others. Intuitively, we would hope that points from the same class (A and B) are closer to each other than to members of other classes (C), but this is seemingly not the case. In this paper we argue this seeming conclusion is incorrect and only due to a misinterpretation of the latent space -- in fact points A and B are closer to each other than to C in the latent representation. Correcting this misinterpretation not only improves our understanding of generative models, but also improves interpolations, clusterings, latent probability distributions, sampling algorithms, interpretability and more.

In general, latent space distances lack physical units (making them difficult to interpret) and are sensitive to specifics of the underlying neural nets. It is therefore more robust to consider infinitesimal distances along the data manifold in the input space. Let z be a latent point and let z1 and z2 be infinitesimals, then we can compute the squared distance

f (z + z1) - f (z + z2) 2 = (z1 - z2) (Jz Jz) (z1 - z2),

f

Jz =

z

,
z=z

(2)

using Taylor's Theorem. This imply that the natural distance function in Z is locally adaptive as it is governed by the local Jacobian. Mathematically, the latent space should then not be seen

1

Under review as a conference paper at ICLR 2018
Figure 1: Left: An example of how latent space distances does not reflect actual data distances. Right: Shortest paths on the surface spanned by the generator does not correspond to straight lines in the latent space as is assumed by the Euclidean metric.
as a linear Euclidean space, but rather as a curved space. The right panel of Fig. 1 provides an example of the implications of this curvature. The figure shows synthetic data from two classes, and the corresponding latent representation of the data. The background color of the latent space corresponds to det(Jz Jz), which can be seen as a measure of the local distortion of the latent space. We interpolate two points from the same class by walking along the connecting straight line (red); in the right panel, we show points along this straight line, which have been mapped by the generator to the input space. Since the generator defines a surface in the input space, we can alternatively seek the shortest curve along this surface that connects the two points; this is perhaps the most natural choice of interpolant. We show this shortest curve in green. From the center panel it is evident that the natural interpolant is rather different from the straight line. This is due to the distortion of the latent space, which is the topic of the present paper.
Outline. In Sec. 2 we briefly present the VAE as a representative instance of generative models. In Sec. 3 we connect generative models with their underlying geometry, and in Sec. 4 we argue that a stochastic Riemannian metric is naturally induced in the latent space by the generator. This metric enable us to compute length minimizing curves and corresponding distances. This analysis, however, reveals that the traditional variance approximations in VAEs are rather poor and misleading; we propose a solution in Sec. 4.1. In Sec. 5 we demonstrate how the resulting view of the latent space improves latent interpolations, gives rise to more meaningful latent distributions, clusterings and more. We discuss related work in Sec. 6 and conclude the paper with an outlook in Sec. 7.
2 THE VARIATIONAL AUTOENCODERS ACTING AS THE GENERATOR
The variational autoencoder (VAE) proposed by Kingma & Welling (2014) is a simple yet powerful generative model, which consist of two parts: (1) an inference network or recognition network (encoder) learns the latent representation (codes) of the data in the input space X = RD; and (2) the generator (decoder) learns how to reconstruct the data from these latent space codes in Z = Rd. Formally, a prior distribution is defined for the latent representations p(z) = N (0, Id), and there exist a mapping function µ : Z  X that generates a surface in X . Moreover, we assume that another function  : Z  RD+ captures the error (or uncertainty) between the actual data observation x  X and its reconstruction as x = µ(z) +  , where  N (0, ID). Then the likelihood is naturally defined as p(x | z) = N (x | µ(z), ID2(z)). The flexible functions µ and  are usually deep neural networks with parameters . However, the corresponding posterior distribution p(z | x) is unknown, as the marginal likelihood p(x) is intractable. Hence, the posterior is approximated using a variational distribution q(z | x) = N (z | µ(x), Id2(x)), where the functions µ : X  Z and  : X  R+d are again deep neural networks with parameters . Since the generator (decoder) is a composition of linear maps and activation functions, its smoothness is based solely on the chosen activation functions.
2

Under review as a conference paper at ICLR 2018

The optimal parameters  and  are found by maximizing the evidence lower bound (ELBO) of the

marginal likelihood p(x) as

{, } = argmax Eq(z|x)[log(p(x|z))] - KL(q(z|x)||p(z)),
,

(3)

where the bound follows from Jensen's inequality. The optimization is based on variations of gra-

dient descent using the reparametrization trick (Kingma & Welling, 2014; Rezende et al., 2014).

Further improvements have been proposed that provide more flexible posterior approximations

(Rezende & Mohamed, 2015; Kingma et al., 2016) or tighter lower bound (Burda et al., 2016).

In this paper we consider the standard VAE for simplicity. The optimization problem in Eq. 3 is dif-

ficult since poor reconstructions by µ can be explained by increasing the corresponding variance

2. A finally

common optimize

trick, which we for the variance

also 2.

follow,

is

to

optimize

µ

while

keeping

2

constant,

and

then

3 SURFACES AS THE FOUNDATION OF GENERATIVE MODELS

Mathematically, a deterministic generative model x = f (z) can be seen as a surface model (Gauss, 1827) if the generator f is sufficiently smooth. Here, we briefly review the basic concepts on surfaces as they form the mathematical foundation of this work.

Intuitively, a surface is a smoothly-connected set of points

embedded in X . When we want to make computations on

a surface it is often convenient to parametrize the surface

by a low-dimensional (latent) variable z along with an

appropriate function f : Z  X . We let d = dim(Z)

denote the intrinsic dimensionality of the surface, while

D = dim(X ) is the dimensionality of the input space.

If we consider a smooth (latent) curve t : [0, 1]  Z,

then it has length

1 0

 t dt, where  t = dt/dt denote

the velocity of the curve. In practice, the low-dimensional

Figure 2: The Jacobian J of a nonlinear function f provides a local basis in the
input space, while det(J J) measure the volume of an infinitesimal region.

parametrization Z often lack a principled meaningful metric, so we measure lengths in input space

by mapping the curve through f ,

11

Length[f (t)] =

f(t) dt =

Jt  t dt,

00

f Jt = z z=t

(4)

where the last step follows from Taylor's Theorem. This imply that the length of a curve t along

the surface can be computed directly in the latent space using the (locally defined) norm

J  = (J  ) (J  ) =  (J J ) =  M  .

(5)

Here, M = J J is a symmetric positive definite matrix, which acts akin to a local Mahalanobis distance measure. This gives rise to the definition of a Riemannian metric, which represents a
smoothly changing inner product structure.
Definition 1. A Riemannian metric M : Z  Rd×d is a smooth function that assigns a symmetric positive definite matrix to any point in Z.

It should be clear that if the generator function f is sufficiently smooth, then M in Eq. 5 is a Riemannian metric.

When defining distances across a given surface, then it is meaningful to seek the shortest curve

connecting two points. Then a distance can be defined as the length of this curve. The shortest curve

connecting points z0 and z1 is by (trivial) definition

t(shortest) = argmin Length[f (t)],
t

0 = z0, 1 = z1.

(6)

A classic result of differential geometry (do Carmo, 1992) is that solutions to this optimization

problem satisfy the following system of ordinary differential equations

¨ t

=

-

1 2

M-t1

2(Id



 t

)

vec Mt t

 t

-

vec Mt t

( t   t) ,

(7)

where vec[·] stacks the columns of a matrix into a vector and  is the Kronecker product. For

completeness, we provide a derivation of this result in Appendix A.

3

Under review as a conference paper at ICLR 2018

4 THE GEOMETRY OF STOCHASTIC GENERATORS

In the previous section we considered deterministic generators f to provide relevant background information. We now extend these results to the stochastic case; in particular we consider

f (z) = µ(z) + (z) , µ : Z  X ,  : Z  R+D,  N (0, ID).

(8)

This is the generator driving VAEs and related models. For our purposes, we will call µ(·) the mean function and 2(·) the variance function.

Following the discussion from the previous section, it is natural to consider the Riemannian metric Mz = Jz Jz in the latent space. Since the generator is now stochastic, this metric also becomes stochastic, which complicate analysis. The following results, however, simplify matters.
Theorem 1. If the stochastic generator in Eq. 8 has mean and variance functions that are at least twice differentiable, then the expected metric equals

Mz = Ep( )[Mz] = J(zµ)

J(zµ) + Jz()

where J(zµ) and J(z) are the Jacobian matrices of µ(·) and (·).

J(z) ,

(9)

Proof. See Appendix B.
Remark 1. By Definition 1, the metric tensor must change smoothly, which implies that the Jacobians must be smooth functions as well. This is easily ensured with activations functions for the neural networks that are C2 differentiable, e.g. tanh(·), sigmoid(·), and softplus(·).
Theorem 2 (Due to Tosi et al. (2014)). The variance of the metric under the L2 measure vanishes when the input dimension goes to infinity, i.e. limD Var (Mz) = 0.

Theorem 2 suggest that the (deterministic) expected metric Mz is a good approximation to the underlying stochastic metric when data dimension is large. We make this approximation, which allows us to apply the theory of deterministic generators.

This expected metric has a particular appealing form, where the two

terms capture the distortion of the mean and the variance functions

respectively. In particular, the variance term (Jz()) (Jz()) will be

large in regions of the latent space, where the generator has large

variance. This imply that induced distances will be large in regions of the latent space where the generator is highly uncertain, such that shortest paths will tend to avoid these regions. These paths will then

Figure 3: Example shortest paths and distances.

tend to follow the data in the latent space, c.f. Fig. 3. It is worth stressing, that no learning is needed

to compute this metric: it only consist of terms that can be derived directly from the generator.

4.1 ENSURING PROPER GEOMETRY THROUGH MEANINGFUL VARIANCE FUNCTIONS
Theorem 1 informs us on how the geometry of the generative model depends on both the mean and the variance of the generator. Assuming successful training of the generator, we can expect to have good estimates of the geometry in regions near the data. But what happens in regions further away from the data? In general, the mean function cannot be expected to give useful extrapolations to such regions, so it is reasonable to require that the generator has high variance in regions that are not near the data. In practice, the neural net used to represent the variance function is only trained in regions where data is available, which implies that variance estimates are extrapolated to regions with no data. As neural nets tend to extrapolate poorly, practical variance estimates tend to be arbitrarily poor in regions without data.
Figure 4 illustrate this problem. The first two panels show the data and its corresponding latent representations (here both input and latent dimensions are 2 to ease illustration). The third panel show the variance function under a standard architecture, deep multilayer perceptron with softplus nonlinearity for the output layer. It is evident that variance estimates in regions without data is not representative of neither uncertainty or error of the generative process; sometimes variance is high, sometimes it is low. From a probabilistic modeling point-of-view this is disheartening. An informal

4

Under review as a conference paper at ICLR 2018

1
0
-1 -1

Input space 0

data 1

3 0 -3

Latent space

latent repr.

-3 0

3

Standard variance estimate

0 3 -1

0 -2

-3 -3
-4

-3 0

3

Proposed variance estimate

3 0 -3
-3 0

3

9 6 3 0

Figure 4: From left to right: training data in X , latent representations in Z, the standard deviation

log(

D j=1

j

(z))

for

the

standard

variance

network,

and

the

proposed

solution.

survey of publicly available VAE implementations also reveal that it is common to enforce a constant unit variance everywhere; this is further disheartening.

For our purposes we need well-behaved variance functions to ensure a well-behaved geometry, but

reasonable variance estimates are of general use. Here we propose, as a general strategy, to model the

inverse variance with a network that extrapolates towards zero. This at least ensures that variances

are large in regions without data.

Specifically, we model the precision as (z)

=

1 2 (z)

,

where

all operations are element-wise. Then, we model this precision with a radial basis function (RBF)

neural network (Que & Belkin, 2016). Formally this is written

(z) = Wv(z) + ,

with

vk(z) = exp

-k

z - ck

2 2

, k = 1, . . . , K,

(10)

where



are

all

parameters,

W



D×K
R>0

are

the

positive

weights

of

the

network

(positivity

ensures

a positive precision), ck and k are the centers and the bandwidth of the K radial basis functions,

and   0 is a vector of positive constants to prevent division by zero. It is easy to see that with

this approach the variance of the generator increases with the distance to the centers. The right-

most panel of Fig. 4 shows an estimated variance function, which indeed has the desired property

that variance is large outside the data support. Further note the increased variance between the two

clusters, which captures that even interpolating between clusters comes with a level of uncertainty.

Training the variance network amounts to fitting the RBF network.

Standard measure

Assuming we have already trained the inference network (Sec. 2), we can encode the training data, and use k-means to estimate the RBF centers. Then, an estimate for the bandwidths of each kernel can be computed as

 -2

11 k = 2 a |Ck| zjCk

zj - ck

2

(11)

where the hyper-parameters a  R+ controls the curvature of the Riemannian metric, i.e. how fast it changes based on the uncertainty. Since the mean function of the generator is already trained, the weights of the RBF can be found using projected gradient descent to ensure positive weights.

3 0 -3
3 0

-3 0

3

Proposed measure

-2 -4 -6 -8
20 15 10 5

One visualization of the distortion of the latent space relative to the
input space is the geometric volume measure det(Mz), which capture the volume of an infinitesimal area in the input space. Figure 5 show this volume measure for both standard variance functions as well as our proposed RBF model. We see that the proposed model captures the trend of the data unlike the standard model.

-3 0 -5

-3 0

3

Figure 5: Measure comparison

(log scale). Top: standard vari-

ance network. Bottom: pro-

posed solution.

5 EMPIRICAL RESULTS

We demonstrate the usefulness of the geometric view of the latent space with several experiments. Model and implementation details can be found in Appendix C. In all experiments we first train a VAE and then use the induced Riemannian metric.

5

Under review as a conference paper at ICLR 2018

5.1 MEANINGFUL DISTANCES

First we seek to quantify if the induced Rie- Digits

Linear

Riemannian

mannian distance in the latent space is more useful than the usual Euclidean distance. For this we perform basic k-means clustering under the two metrics. We construct 3

{0, 1, 2} 77.57(±0.87)% 94.28(±1.14)% {3, 4, 7} 77.80(±0.91)% 89.54(±1.61)% {5, 6, 9} 64.93(±0.96)% 81.13(±2.52)%

sets of MNIST digits, using 1000 random samples for each digit. We train a VAE for

Table 1: The F -measure results for k-means.

each set, and then subdivide each into 10 sub-sets, and performed k-means clustering under both

distances. One example result is shown in Fig. 6. Here it is evident that since the latent points

roughly follow a unit Gaussian, there is little structure to be discovered by the Euclidean k-means,

and consequently it performs poorly. The Riemannian clustering is remarkably accurate. Summary

statistics across all subsets is provided in Table 1, which show the established F -measure for clus-

tering accuracy. Again, the Riemannian metric significantly improves clustering. This implies that

the underlying Riemannian distance is more useful than its Euclidean counterpart.

Figure 6: The result of k-means comparing the distance measures. For the decision boundaries we used 7-NN classification.

5.2 INTERPOLATIONS
Next we investigate if the Riemannian metric gives more meaningful interpolations. First, we train a VAE for the digits 0 and 1 from MNIST. The upper left panel of Fig. 7 shows the latent space with the Riemannian measure as background color, together with two interpolations. Images generated by both Riemannian and Euclidean interpolations are shown in the bottom of Fig. 7. The Euclidean interpolations seem to have a very abrupt change when transitioning from one class to another. The Riemannian interpolant gives smoother changes in the generated images. The top-right panel of the figure shows the auto-correlation of images along the interpolants; again we see a very abrupt change in the Euclidean interpolant, while the Riemannian is significantly smoother. We also train a convolutional VAE on frames from a video. Figure 8 shows the corresponding latent space and some sample interpolations. As before, we see more smooth changes in generated images when we take the Riemannian metric into account.

5.3 LATENT PROBABILITY DISTRIBUTIONS

We have seen strong indications that the Riemannian metric gives a more meaningful view of the latent space, which may also improve probability distributions in the latent space. A relevant candidate distribution is the locally adaptive normal distribution (LAND) (Arvanitidis et al., 2016)

LAND(z | µ, )  exp

-

1 2

dist2(z,

µ)

,

(12)

where dist is the Riemannian extension of Mahalanobis distance. We fit a mixture of two LANDs to the MNIST data from Sec. 5.2 alongside a mixture of Euclidean normal distributions. The first
column of Fig. 9 shows the density functions of the two mixture models. Only the Riemannian model
reveals the underlying clusters. We then sample 40 points from each component of these generative models1 (center column of the figure). We see that the Riemannian model generates high quality

1We do not follow common practice and sort samples by their likelihood as this hides low-quality samples.

6

Under review as a conference paper at ICLR 2018

Figure 7: Left: the latent space with example interpolants. Right: auto-correlations of Riemannian (top) and Euclidean (bottom) samples along the curves of left panel. Bottom: decoded images along Euclidean (top rows) and Riemannian (bottom rows) interpolants.

3

0

-3

-3 0

3

Figure 8: Left: the latent space and geodesic interpolants. Right: samples comparing Euclidean (top row) with Riemannian (bottom row) interpolation. Corresponding videos can be found here.

samples, whereas the Euclidean model generates several samples in regions where the generator is not trained and therefore produce blurry images. Finally, the right column of Fig. 9 shows all pairwise distances between the latent points under both Riemannian and Euclidean distances. Again, we see that the geometric view clearly reveals the underlying clusters.
5.4 RANDOM WALK ON THE DATA MANIFOLD
Finally, we consider random walks over the data manifold, which is a common tool for exploring latent spaces. To avoid the walk drifting outside the data support, practical implementations artificially restrict the walk to stay inside the [-1, 1]d hypercube. Here we consider unrestricted Brownian motion under both the Euclidean and Riemannian metric. We perform this random walk in the latent space of the convolutional VAE from Sec. 5.2. Figure 10 shows example walks, while Fig. 11 shows generated images (video here). While the Euclidean random walk moves freely, the Riemannian walk stays within the support of the data. This is explained in the left panel of Fig. 10, which shows that the variance term in the Riemannian metric creates a "wall" around the data which the random walk will only rarely cross. These "walls" also force shortest paths to follow the data.
7

Under review as a conference paper at ICLR 2018

Figure 9: From left to right: the mixture models, generated samples, and pairwise distances. Top row corresponds to the Riemannian model and bottom row to the Euclidean model.

6 34

02

0 Latent repr. -3 Riem annian
- 2 Euclidean En d

-3 0

3

- 15

- 10

-5

0

Figure 10: Left: the measure in the latent space. Right: the random walks.

6 RELATED WORK
Generative models. This unsupervised learning category attracted a lot of attention, especially, due to the advances on the deep neural networks. We have considered VAEs (Kingma & Welling, 2014; Rezende et al., 2014), but the ideas extend to similar related models. This include extensions that provide more flexible approximate posteriors (Rezende & Mohamed, 2015; Kingma et al., 2016). GANs (Goodfellow et al., 2014) also fall in this category as these models have an explicit generator. While the inference network is not a necessary component in the GAN model, it has been shown that incorporating them improves overall performance (Donahue et al., 2017; Dumoulin et al., 2017). Likewise, approaches that are based on the transformation of the latent space through sequence of bijective functions (Dinh et al., 2017).
Geometry in neural networks. Bengio et al. (2013) discuss the importance of geometry in neural networks as a tool to understand local generalization. For instance, the Jacobian matrix is a measure of smoothness for a function that interpolates a surface to the given data. This is exactly the implication in (Rifai et al., 2011) where the norm of the Jacobian acts as a regularizer for the deterministic autoencoder. Recently Kumar et al. (2017) used the Jacobian to inject invariances in a classifier.
Riemannian Geometry. Similarly with the present paper, Tosi et al. (2014) derive a suitable Riemannian metric in Gaussian process (GP) latent variable models (Lawrence, 2005), but the computational complexity of GPs cause practical concerns. Unlike works that explicitly learn a Riemannian metric (Hauberg et al., 2012; Peltonen et al., 2004), our metric is fully derived from the generator and requires no extra learning once the generator is available.
8

Under review as a conference paper at ICLR 2018
Figure 11: The comparison of the random walks, at the steps 200, 300, 3000, 4000 and 5000.
7 DISCUSSION AND FURTHER EXTENSIONS
The geometric interpretation of representation learning is that the latent space is a compressed and flattened version of the data manifold. We show that the actual geometry of the data manifold can be more complex than it first appears. Here we have initiated the study of proper geometries for generative models. We showed that the latent space not only provides a low-dimensional representation of the data manifold, but at the same time, can reveal the underlying geometrical structure. We proposed a new variance network for the generator, which provides meaningful uncertainty estimates while regularizing the geometry. The new detailed understanding of the geometry provide us with more relevant distance measures, as demonstrated by the fact that a k-means clustering, on these distances, is better aligned with the ground truth label structure than a clustering based on conventional Euclidean distances. We also found that the new distance measure produces smoother interpolation and when training Riemannian "LAND" mixture models based on the new geometry, the components aligned much better with the ground truth group structure. Finally, inspired by the recent interest in sequence generation by random walks in latent space, we found that geometrically informed random walks stayed on the manifold for much longer runs than sequences based on Euclidean random walks. The presented analysis easily extend to sophisticated generative models, where the latent space will be potentially endowed with more flexible nonlinear structures. This directly implies particularly interesting geometrical models. An obvious question is, if the geometry of the latent space can play a role while we learn the generative model? Either way, we believe that this geometric perspective provides a new way of thinking and further interpreting the generative models, while at the same time is appealing for developing new nonlinear models in the representation space.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. A Locally Adaptive Normal Distribution. In Advances in Neural Information Processing Systems (NIPS), 2016.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New Perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798­1828, August 2013.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. In International Conference on Learning Representations (ICLR), 2016.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In International Conference on Learning Representations (ICLR), 2017.
M.P. do Carmo. Riemannian Geometry. Mathematics (Boston, Mass.). Birkha¨user, 1992.
Jeff Donahue, Philipp Krhenbhl, and Trevor Darrell. Adversarial Feature Learning. In International Conference on Learning Representations (ICLR), 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially Learned Inference. In International Conference on Learning Representations (ICLR), 2017.
Carl Friedrich Gauss. Disquisitiones generales circa superficies curvas. Commentationes Societatis Regiae Scientiarum Gottingesis Recentiores, VI:99­146, 1827.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems (NIPS), 2014.
Søren Hauberg, Oren Freifeld, and Michael J. Black. A Geometric Take on Metric Learning. In Advances in Neural Information Processing Systems (NIPS) 25, pp. 2033­2041, 2012.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved Variational Inference with Inverse Autoregressive Flow. In Advances in Neural Information Processing Systems (NIPS), 2016.
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Improved Semi-supervised Learning with Gans using Manifold Invariances. In Advances in Neural Information Processing Systems (NIPS). 2017.
Neil Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of machine learning research, 6(Nov):1783­1816, 2005.
Jaakko. Peltonen, Arto Klami, and Samuel Kaski. Improved learning of riemannian metrics for exploratory analysis. Neural Networks, 17(8):1087­1100, 2004.
Qichao Que and Mikhail Belkin. Back to the future: Radial basis function networks revisited. In Artificial Intelligence and Statistics (AISTATS), 2016.
Danilo Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive AutoEncoders: Explicit Invariance During Feature Extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML), 2011.
Alessandra Tosi, Søren Hauberg, Alfredo Vellido, and Neil D. Lawrence. Metrics for Probabilistic Geometries. In The Conference on Uncertainty in Artificial Intelligence (UAI), July 2014.
10

Under review as a conference paper at ICLR 2018

A THE DERIVATION OF THE GEODESIC DIFFERENTIAL EQUATION

The shortest path between two points x, y  M on a Riemannian manifold M is found by optimizing the functional

1



(shortest) t

=

argmin

t 0

 t, Mt  t dt, (0) = x, (1) = y

(13)

where t

:

[0, 1]

 M and  t

=

t t

.

The minima of this problem can be found instead by

optimizing the curve energy (do Carmo, 1992), so the functional becomes

1



(shortest) t

=

argmin

t 0

 t, Mt  t dt,

The inner product can be written explicitly as

(0) = x, (1) = y.

(14)

dd

L(t,  t, Mt ) =  t, Mt  t =

 t(i) ·  t(j) · M(itj) = ( t   t) vec Mt

i=1 j=1

(15)

where the index in the parenthesis represents the corresponding element in the vector or matrix. In the derivation the  is the usual Kronocker product and the vec[·] stacks the column of a matrix into
a vector. We find the minimizers by the Euler-Lagrange equation

L  L =
t t  t

where

 L =    t, Mt  t

t  t t

 t

 =
t

2 · Mt  t

=2

Mt t

 t

+

Mt

¨ t

.

Since the term





Mt

 =

t 



 M(1t1) t
 M(2t1) t
...

···
··· ...

 M(1tD) t
 M(2tD) t
...

 











=

 









 M(1t1) t
 M(2t1) t
...

 t  t

···
··· ...

 M(1tD) t

t

 M(2D) t
...

 t

     



· · ·M(D1)
t

M(Dt D) t

 M(Dt 1) t

t

···

M(Dt D) t

t

we can write the right hand side of the Eq. 16 as

(16) (17)
(18)

 L =2
t  t

(Id



 t

)

vec Mt t

 t + Mt ¨t

.

(19)

The left hand side term of the Eq. 16 is equal to

L 

= t t

( t   t) vec Mt

= ( t   t)

vec Mt t

.

The final system of 2nd order ordinary differential equations is

¨ t

=

-

1 2

M-t1

2

·

(Id



 t

)

vec Mt t

 t

-

vec Mt t

( t   t) .

(20) (21)

B THE DERIVATION OF THE RIEMANNIAN METRIC
The proof of Theorem 2.

11

Under review as a conference paper at ICLR 2018

Proof. As we introduced in Eq. 8 the stochastic generator is f (z) = µ(z) + (z) , µ : Z  X ,  : Z  R+D,
Thus, we can compute the corresponding Jacobian as follows

 N (0, ID).

(22)

 fz(1)

z1



f (z)



z

=

Jz

=

 



 fz(2) z1
...



 fz(D)

z1


  =   

 µ(z1) z1
 µz(2) z1
...
 µ(zD) z1

 fz(1) z2  fz(2) z2
...
 fz(D) z2
 µ(z1) z2
 µz(2) z2
...
 µ(zD) z2

··· ··· ... ···
··· ··· ... ···

 fz(1) zd  fz(2) zd
...
 fz(D) zd


     
D×d

µz(1)

zd

 µz(2)

 

zd 

...

 

+ [S1 , S2 , · · · , Sd ]D×d,


 µ(zD)

B

zd D×d

(23) (24)

A

 z(1)
zi

0 ···

 0



0

where

Si

=

 



...

 z(2) zi
...

··· ...

 0



...

 

, i = 1, . . . , d



0

0

···

 z(D) zi

D×D

(25)

and the resulting "random" metric in the latent space is Mz = Jz Jz. The randomness is due to the random variable , and thus, we can compute the expectation

Mz = Ep( )[Mz] = Ep( )[(A + B) (A + B)] = Ep( )[A A + A B + B A + B B]. (26)

Using the linearity of expectation we get that

Ep(

)[A

B] = Ep( ) [A

[S1

, S2

, · · · , Sd

]] = A

0 [S1Ep( )[ :], . . . , 0] = 0

because Ep( )[ ] = 0. The other term

(27)

with

 



Ep(

)[B

B] = Ep(





)

 





S1 



S2 

...

 



[S1

, S2

, · · · , Sd

] 



Sd d×D



=

Ep(

)

 



S1S1 S2S1
...
SdS1

S1S2 S2S2
...
SdS2

··· ···
... ···

S1Sd S2Sd
...
SdSd


  







Ep( ) [

SiSj

] = Ep(

 )





1

 z(1) zi

,

2

 z(2) zi

,

·

·

·

,

 z(D) D zi



1

 z(1) zj

  

2

 z(2) zj

  

 

...

 

 

 z(D)

D zj

= Ep( )

2 1

z(1) z(1) zi zj

+

2 2

z(2) z(2) zi zj

+···

2 D

z(D) z(D) zi zj

(28) (29) (30) (31)

= diag(Si) diag(Sj),

(32)

12

Under review as a conference paper at ICLR 2018

because Ep( )[ i2] = 1, i = 1, . . . , D.

The matrix A = Jz(µ) and for the variance network

 z(1)
z1

  (z1) z2

···

(z1)
zd





Jz()

=

 



  z(2) z1
...

  (z2) z2
...

··· ...

  z(2) zd
...

    



· · ·(zD)
z1

  (zD) z2

  (zD) zd

(33)

it is easy to see that Ep( )[B B] = Jz() J(z). So the expectation of the induced Riemannian metric in the latent space by the generator is

M¯ z = J(zµ) Jz(µ) + Jz() J(z)

(34)

which concludes the proof.

C IMPLEMENTATION DETAILS FOR THE EXPERIMENTS

Algorithm 1 The training of a VAE that ensures geometry
Output: the estimated parameters of the neural networks , ,  1: Train the µ, , µ as in Kingma & Welling (2014), keeping  fixed. 2: Train the  as explained in Sec. 4.1.

Details for Experiments 5.1, 5.2 & 5.3. The pixels values of the images are scaled to the interval
[0, 1]. We use for the functions µ, , µ multilayer perceptron (MLP) deep neural networks, and for the  the proposed RBF model with 64 centers, so W  RD×64 and the parameter a of Eq. 11 is set to 2. We used L2 regularization with parameter equal to 1e-5.

Encoder/Decoder
µ  µ

Layer 1
64, (tanh) 64, (tanh) 32, (tanh)

Layer 2
32, (tanh) 32, (tanh) 64, (tanh)

Layer 3
d, (linear) d, (softplus) D, (sigmoid)

The number corresponds to the size of the layer, and in the parenthesis the activation function. For the encoder the mean and the variance functions share the weights of the Layer 1. The input space dimension D = 784. After the training, the geodesics can be computed by solving Eq. 7 numerically. The LAND mixture model is fitted as explained in (Arvanitidis et al., 2016).

Details for Experiments 5.4. In this experiment we used Convolutional Variational AutoEncoders. The pixel values of the images are scaled to the interval [0, 1]. For the  we used the proposed RBF model with 64 centers and the parameter a of Eq. 11 is set to 2.
Considering the variance network during the decoding stage, the RBF generates an image, which represents intuitively the total variance of each pixel for the decoded final image, but in an initial sub-sampled version. Afterwards this image is passed through a sequence of deconvolution layers, and at the end will represent the variance of every pixel for each RGB channel. However, it is critical that the weights of the filters must be clipped during the training to R+ to ensure positive variance.

Encoder Layer 1 (Conv) Layer 2 (Conv) Layer 3 (MLP) Layer 4 (MLP)

µ 32, 3, 2, (tanh) 32, 3, 2, (tanh) 1024, (tanh)

d, (linear)

 32, 3, 2, (tanh) 32, 3, 2, (tanh) 1024, (tanh) d, (softplus)

For the convolutional and deconvolutional layers the first number is the number of applied filters, the second is the kernel size, and third is the stride. Also, for the encoder the mean and the variance functions share the convolutional layers. We used L2 regularization with parameter equal to 1e-5.

13

Under review as a conference paper at ICLR 2018

Decoder L. 1 (MLP) L. 2 (MLP) L. 3 (DE) L. 4 (DE) L. 5 (DE) L.6 (CO) µ 1024, (t) D/4, (t) 32, 3, 2, (t) 32, 3, 2, (t) 3, 3, 1, (t) 3, 3, 1, (s)
For the decoder, the acronyms (DE) = Deconvolution, (CO) = Convolution and (t), (s) stands for tanh and sigmoid, respectively. Also, D = width × height × channels of the images, in our case 64,64,3. For all the convolutions and deconvolutions the padding is set to same. We used L2 regularization with parameter equal to 1e-5.

Decoder Layer 1 (RBF) Layer 2 (Deconv) Layer 3 (Conv)  W  R(D/2)×64 1, 3, 2 (linear) 3, 3, 1 (linear)
The Brownian motion over the Riemannian manifold in the latent space is presented in Alg. 2.

Algorithm 2 Brownian motion on a Riemannian manifold

Input: the starting point z  Rd×1, stepsize s, number of steps Ns, the metric tensor M(·). Output: the random steps Z  RNs×d.

1: for n = 0 to Ns do

2: L, U = eig (M(z)), (L: eigenvalues, U: eigenvectors)

3:

v

=

UL-

1 2

,

4: z = z + s · v

 N (0, Id)

5: Z(n, :) = z

6: end for

14

