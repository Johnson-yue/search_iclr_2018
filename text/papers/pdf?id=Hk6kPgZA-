Under review as a conference paper at ICLR 2018

CERTIFIABLE DISTRIBUTIONAL ROBUSTNESS WITH PRINCIPLED ADVERSARIAL TRAINING
Anonymous authors Paper under double-blind review

ABSTRACT
Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. We match or outperform heuristic approaches on supervised and reinforcement learning tasks.

1 INTRODUCTION

Consider the classical supervised learning problem, in which we minimize the expected loss EP0 [ (; Z)] over a parameter   , where Z  P0 is a distribution on a space Z and is a loss function. In many systems, robustness to changes in the data-generating distribution P0 is desirable, either from covariate shifts, changes in the underlying domain (Ben-David et al., 2010),
or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016). As deep networks become
prevalent in modern performance-critical systems (e.g. perception for self-driving cars, automated
detection of tumors), model failure increasingly leads to life-threatening situations; in these systems,
it is irresponsible to deploy models whose robustness we cannot certify.

However, recent works have shown that neural networks are vulnerable to adversarial examples; seemingly imperceptible perturbations to data can lead to misbehavior of the model, such as misclassifications of the output (Goodfellow et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016; Nguyen et al., 2015). Subsequently, many researchers have proposed adversarial attack and defense mechanisms (Rozsa et al., 2016; Papernot et al., 2016a;b;c; Trame`r et al., 2017; Carlini & Wagner, 2017; Madry et al., 2017; He et al., 2017). While these works provide an initial foundation for adversarial training, there are no guarantees on whether proposed white-box attacks can find the most adversarial perturbation and whether there is a class of attacks such defenses can successfully prevent. On the other hand, verification of deep networks using SMT solvers (Katz et al., 2017a;b; Huang et al., 2017) provides formal guarantees on robustness but is NP-hard in general; this approach requires prohibitive computational expense even on small networks.

We take the perspective of distributionally robust optimization and provide an adversarial training
procedure with provable guarantees on its computational and statistical performance. We postulate a class P of distributions around the data-generating distribution Z  P0 and consider the problem

minimize sup EP [ (; Z)].
 P P

(1)

The choice of P influences robustness guarantees and computability; we develop robustness sets P with computationally efficient relaxations that apply even when the loss is non-convex. We
provide an adversarial training procedure that, for smooth , enjoys convergence guarantees simi-
lar to non-robust approaches while certifying performance even for the worst-case population loss supP P EP [ (; Z)]. On a simple implementation in Tensorflow, our method takes 5­10× as long as stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for other

1

Under review as a conference paper at ICLR 2018

adversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017). We show that our procedure--which learns to protect against adversarial perturbations in the training dataset--generalizes, allowing us to train a model that prevents attacks to the test dataset.

Let us overview our approach briefly. Let c : Z × Z  R+, where c(z, z0) represents the "cost" for

an adversary to perturb the data point z0 to the point z (we typically use c(z, z0) =

z - z0

2 p

for

some p  1). We then consider the robustness region P = {P : Wc(P, P0)  }, a -neighborhood

of the data generating distribution P0 under the Wasserstein metric Wc(·, ·) (see Section 2 for a

formal definition). This formulation of problem (1) is still intractable with arbitrary robustness levels

--at least with deep networks or other complex models--so we instead consider its Lagrangian

relaxation for a fixed penalty parameter   0, which results in the reformulation

minimize


F () := sup {EP [ (; Z)] - Wc(P, P0)} = EP0 [(; Z)]
P

(2a)

where (; z0) := sup { (; z) - c(z, z0)} .
zZ

(2b)

(We show the equalities in Proposition 1.) That is, we have replaced the usual loss (; Z) by

the robust surrogate (; z); this surrogate (2b) allows adversarial perturbations of the data z, modulated by the magnitude of . We typically solve the penalty problem (2) with P0 replaced by

the empirical distribution Pn, as P0 is unknown (we refer to this as the penalty problem below).

The key feature of the penalty problem (2) is that moderate levels of robustness are achievable at es-
sentially no computational or statistical cost for smooth losses . More specifically, for large enough penalty  (by duality, small enough robustness ), the function z  (; z) - c(z, z0) in the robust surrogate (2b) is strongly concave and hence easy to compute if (, z) is smooth in z. As a conse-
quence, the stochastic gradient method applied to problem (2) has similar convergence guarantees as
for non-robust methods (ERM). In Section 3, we give a certificate of robustness showing that we are approximately protected against all distributional perturbations satisfying Wc(P, P0)  n, where n is the achieved robustness for the empirical objective. We upper-bound the population worst-case scenario supP :Wc(P,P0)n EP [ (; Z)] by an efficiently computable empirical counterpart. These results suggest advantages of networks with smooth activations rather than ReLUs. We experimen-
tally verify our results in Section 4 and show that, even for non-smooth losses, we are able to match
or achieve state-of-the-art performance on a variety of adversarial attacks.

Robust optimization and adversarial training The standard robust- optimization approach is to minimize losses of the form supuU (; z + u), where U is some uncertainty set (Ben-Tal et al., 2009; Ratliff et al., 2006; Xu et al., 2009). Unfortunately, this approach is intractable except for
specially structured losses, such as the composition of a linear and simple convex function (Ben-Tal
et al., 2009; Xu et al., 2009; 2012). Nevertheless, this robust approach underlies recent advances in
adversarial training (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini &
Wagner, 2017; Madry et al., 2017), which consider heuristically perturbing data during a stochastic
optimization procedure. One such heuristic uses a locally linearized loss function (proposed with p =  as the "fast gradient sign method" (Goodfellow et al., 2015)):

xi () := argmax{x (; xi, yi)T } and perturb xi  xi + xi ().
 p

(3)

One form of adversarial training simply trains on these perturbed losses (Goodfellow et al., 2015; Kurakin et al., 2016), and many others perform iterated variants (Papernot et al., 2016b; Trame`r et al., 2017; Carlini & Wagner, 2017; Madry et al., 2017). Madry et al. (2017) observe that these procedures attempt to optimize the objective EP0 [sup u p (; Z + u)], a constrained version of the penalty problem (2). This notion of robustness is typically intractable: the inner supremum is generally non-concave in u, so it is unclear whether model-fitting with these techniques converges, and there are possibly worst-case perturbations these techniques do not find. Indeed, when deep networks use ReLU activations, it is NP-hard to find worst-case perturbations, suggesting difficulties for fast and iterated heuristics (see Lemma 2 in Appendix B). Smoothness, which can be obtained in standard deep architectures with exponential linear units (ELU's) (Clevert et al., 2015), allows us to find Lagrangian worst-case perturbations with low computational cost.

Distributionally robust optimization To situate the current work, we review some of the substantial body of work on robustness and learning. The choice of P in the robust objective (1) affects

2

Under review as a conference paper at ICLR 2018

both the richness of the uncertainty set we wish to consider as well as the tractability of the resulting optimization problem. Previous approaches to distributional robustness have considered finitedimensional parametrizations for P, such as constraint sets for moments, support, or directional deviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametric distances for probability measures such as f -divergences (Ben-Tal et al., 2013; Bertsimas et al., 2013; Miyato et al., 2015; Lam & Zhou, 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), and Wasserstein distances (Blanchet et al., 2016; Esfahani & Kuhn, 2015; Shafieezadeh-Abadeh et al., 2015). In constrast to f -divergences (e.g. 2- or Kullback-Leibler divergences) which are effective when the support of the distribution P0 is fixed, a Wasserstein ball around P0 includes distributions Q with different support and allows (in a sense) robustness to unseen data.
Many authors have studied tractable classes of uncertainty sets P and losses . For example, Ben-Tal et al. (2013) and Namkoong & Duchi (2017) use convex optimization approaches for f divergence balls. For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015), Shafieezadeh-Abadeh et al. (2015) and Blanchet et al. (2016) show how to convert the saddle-point problem (1) to a regularized ERM problem, but this is possible only for a limited class of convex losses and costs c. In this work, we treat a much larger class of losses and costs and provide direct solution methods for a Lagrangian relaxation of the saddle-point problem (1).

2 PROPOSED APPROACH

Our approach is based on the following simple insight: assume that the function z  (; z) is

smooth, meaning there is some L for which z (; z) is L-Lipschitz. Then for any c : Z ×Z  R+ strongly convex in its first argument, a Taylor expansion yields

(; z ) - c(z , z0) 

(; z) - c(z, z0) +

z( (; z) - c(z, z0)), z

-z

L- +
2

z-z

2 2

.

(4)

For   L this is precisely the first-order condition for concavity of z  ( (; z) - c(z, z0)). Thus, whenever the loss is smooth enough in z and the penalty  is large enough (corresponding to

less robustness), computing the surrogate (2b) is a strongly-concave optimization problem.

We leverage the insight (4) to show that as long as we do not require too much robustness, this strong concavity approach (4) provides a computationally efficient and principled approach for robust optimization problems (1). Our starting point is a duality result for the minimax problem (1) and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connections between distributional robustness and the "lazy" surrogate (2b) clear. We then show (Section 2.1) how stochastic gradient descent methods can efficiently find minimizers (in the convex case) or approximate stationary points (when is non-convex) for our relaxed robust problems.

Wasserstein robustness and duality Wasserstein distances define a notion of closeness between distributions. Let Z  Rm be convex, and let (Z, A, P0) be a probability space. Let the transportation cost c : Z × Z  [0, ) be nonnegative, continuous, and convex in its first argument and
satisfy c(z, z) = 0. For example, for a differentiable convex h : Z  R, the Bregman divergence c(z, z0) = h(z) - h(z0) - h(z0), z - z0 satisfies these conditions. For probability measures P and Q supported on Z, let (P, Q) denote their couplings, meaning measures M on Z2 with
M (A, Z) = P (A) and M (Z, A) = Q(A). The Wasserstein distance between P and Q is

Wc(P, Q) := inf EM [c(Z, Z )].
M (P,Q)

For   0 and data generating distribution P0, we consider the Wasserstein form of the robust problem (1), with P = {P : Wc(P, P0)  }, and its Lagrangian relaxation (2) with   0. The following duality result, which we prove in Appendix C.1, gives the equality (2) and an analogous
result for the worst-case problem (1).

Proposition 1. Let :  × Z  R and c : Z × Z  R+ be continuous. Let (; z0) = supzZ { (; z) - c(z, z0)} be the robust surrogate (2b). For any distribution Q and any  > 0,

sup EP [ (; Z)] = inf  + EQ[(; Z)] ,

P :Wc(P,Q)

0

and for any   0, we have

(5)

sup {EP [ (; Z)] - Wc(P, Q)} = EQ[(; Z)].
P

(6)

3

Under review as a conference paper at ICLR 2018

Algorithm 1 Distributionally robust optimization with adversarial training
INPUT: Sampling distribution P0, constraint sets  and Z, stepsize sequence {t > 0}Tt=-01 for t = 0, . . . , T - 1 do
Sample zt  P0 and find an -approximate maximizer zt of (t; z) - c(z, zt) t+1  Proj(t - t (t; zt))

Leveraging the insight (4), we give up the requirement that we wish a prescribed amount  of robustness (solving the worst-case problem (1) for P = {P : Wc(P, P0)  }) and focus instead on the Lagrangian penalty problem (2) and its empirical counterpart

minimize


Fn() := sup
P

E[ (; Z)] - Wc(P, Pn)

= EPn [ (; Z)]

.

(7)

2.1 OPTIMIZING THE ROBUST LOSS BY STOCHASTIC GRADIENT DESCENT

We now develop stochastic gradient-type methods for the relaxed robust problem (7), developing a number of results that make clear the computational benefits of relaxing the strict robustness requirements of formulation (5). We begin with the assumptions we require for our development, which roughly quantify the amounts of robustness we can provide.
Assumption A. The function c : Z ×Z  R+ is continuous. For each z0  Z, c(·, z0) is 1-strongly convex with respect to the norm · .

To guarantee that the robust formulation (2b) is tractably computable, we also require a few smoothness assumptions. Let ·  be the dual norm to · ; we abuse notation by using the same norm · on  and Z, though the specific norm is clear from context. Assumption B. The loss :  × Z  R satisfies the Lipschitzian smoothness conditions
 (; z) -  ( ; z)   L  -  , z (; z) - z (; z )   Lzz z - z ,  (; z) -  (; z )   Lz z - z , z (; z) - z ( ; z)   Lz  -  .

These properties are enough to guarantee both (i) the well-behavedness of the robust surrogate  and (ii) its efficient computability. Making point (i) precise, the following lemma shows (more generically) that if  is large enough and Assumptions A and B hold, the surrogate  is still smooth.
Lemma 1. Let f :  × Z  R be differentiable and -strongly concave in z with respect to the norm · , and define f¯() = supzZ f (, z). Let g(, z) = f (, z) and gz(, z) = zf (, z), and assume g and gz satisfy the Lipschitz conditions of Assumption B. Then f¯ is differentiable, and letting z () = argmaxzZ f (, z), we have f¯() = g(, z ()). Moreover,

z (1) - z (2)

 Lz 

1 - 2

and

f¯() - f¯( )



L

+

LzLz 

- .

See Section C.2 for the proof. Focusing on the 2-norm case, an immediate application of Lemma 1

shows

that

if

Assumption

B

holds,

then



has

L

=

L

+

Lz Lz [ -Lzz ]+

-Lipschitz

gradients,

and

(; z0) =  (; z (z0, )) where z (z0, ) = argmax{ (; z) - c(z, z0)}.
zZ

This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7). The benefits
of Lagrangian relaxation become clear here: for (; z) smooth in z and  large enough, gradient ascent on (t; z) - c(z, zt) in z converges linearly and we can compute (approximate) zt efficiently.

Convergence properties of Algorithm 1 depend on the loss . When is convex in  and  is large enough that z  ( (; z) - c(z, z0)) is concave for all (, z0)   × Z, we have a stochastic monotone variational inequality,which is efficiently solvable (Juditsky et al., 2011; Chen et al., 2014) with convergence rate 1/ T . When the loss is nonconvex in , the following theorem guarantees convergence to a stationary point of problem (7) at the same rate when   Lzz. Recall that F () = EP0 [(; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).

4

Under review as a conference paper at ICLR 2018

Theorem 2 (Convergence of Nonconvex SGD). Let Assumptions A and B hold with the 2-norm

and let 

=

Rd.

Let F



F (0) - inf F ().

Assume E[

F () - (, Z)

2 2

]



2, and

take constant stepsizes  =

2F L 2 T

where L = L +

.Lz Lz
-Lzz

Then Algorithm 1 satisfies

1T TE
t=1

F (t)

2 2

- 2L2z  - Lzz



8 LF . T

See Section C.3 for the proof. We make a few remarks. First, the condition E[ F () - (, Z) 22]  2 holds (to within a constant factor) whenever  (, z) 2   for all , z. Theorem 2 shows that the stochastic gradient method achieves the rates of convergence on the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi & Lan, 2013). The accuracy parameter has a fixed effect on optimization accuracy, independent of T : approximate maximization has limited effects.
Key to the convergence guarantee of Theorem 2 and our ability to actually run Algorithm 1 is that the loss is smooth in z: the inner supremum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section B for a proof of this for ReLUs). The smoothness of is essential so that a penalized version (, z)-c(z, z0) is concave in z (which can be approximately verified by computing Hessians z2z (, z) for each training datapoint), allowing computation and our coming certificates of optimality. Replacing ReLU's with sigmoids or ELU's (Clevert et al., 2015) allows us to apply Theorem 2, making distributionally robust optimization tractable for deep learning.

3 ROBUSTNESS CERTIFICATE AND GENERALIZATION

From results in the previous section, Algorithm 1 provably learns to protect against adversarial perturbations of the form (7) on the training dataset. Now, we show that such procedures generalize, allowing us to prevent attacks on the test set. Our subsequent results hold uniformly over the space of parameters   , including WRM, the output of the stochastic gradient descent procedure in Section 2.1. Our first main result, presented in Section 3.1, gives a data-dependent upper bound on the population worst-case objective supP :Wc(P,P0) EP [ (; Z)] for any arbitrary level of robustness ; this bound is optimal for  = n, the level of robustness achieved for the empirical distribution by solving (7). Our bound is efficiently computable and hence certifies a level of robustness for the worst-case population objective. Second, we show in Section 3.2 that adversarial perturbations on the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees a similar level of robustness as directly solving its population counterpart (2).

3.1 ROBUSTNESS CERTIFICATE

Our main result in this section is a data-dependent upper bound for the worst-case population objective: supP :Wc(P,P0) EP [ (; Z)]   + EPn [ (; Z)] + O(1/ n) for all   , with high probability. To make this rigorous, fix  > 0, and consider the worst-case perturbation, typically called the transportation map or Monge map (Villani, 2009),

T(; z0) := argmax{ (; z) - c(z, z0)}.
zZ

(8)

Under our assumptions, it is easy to compute T whenever   Lzz. Letting z denote the point mass at z, Proposition 1 (or Kantorovich duality (Villani, 2009, Chs. 9­10)) shows the empirical
maximizers of the Lagrangian formulation (6) are attained by

Pn() := argmax
P

EP [ (; Z)] - Wc(P, Pn)

1n

= n

T (,Zi)

i=1

and

(9)

n() := Wc(Pn(), Pn) = EPn [c(T(; Z), Z)].

Our results will imply, in particular, that the empirical worst-case objective EPn [ (; Z)] gives a certificate of robustness to (population) Wasserstein perturbations of up to level n. The equalities (9) show that EPn()[ (; Z)] is efficiently computable, thereby providing a data-dependent performance guarantee for the worst-case population loss.

5

Under review as a conference paper at ICLR 2018

Our bound relies on the usual covering numbers for the model class { (; ·) :   } as the notion
of complexity (e.g. van der Vaart & Wellner, 1996), so, despite the infinite-dimensional problem (7),
we retain the same uniform convergence guarantees typical of empirical risk minimization. Recall that for a set V , a collection v1, . . . , vN is an -cover of V in norm · if for each v  V, there exists vi such that v - vi  . The covering number of V with respect to · is

N (V, , · ) := inf {N  N | there is an -cover of V with respect to · } .
For F := { (, ·) :   } equipped with the L(Z) norm f L(Z) := supzZ |f (z)|, we state our results in terms of · L(Z)-covering numbers of F . To ease notation, we let

M1 n,1(t) := b1 n 0 log N (F , M , · L(Z))d + b2M

t n

where b1, b2 are numerical constants. See Section C.4 for its proof.

Theorem 3. Assume that | (; z)|  M for all    and z  Z. Then, for a fixed t > 0 and numerical constants b1, b2 > 0, with probability at least 1 - e-t, simultaneously for all   ,
  0, and   0,

sup EP [
P :Wc(P,P0)

(; Z)]



 + EPn [(; Z)] +

n,1(t).

In particular, if  = n then with probability at least 1 - e-t, for all   

(10)

sup EP [ (; Z)]  sup EP [ (; Z)] + n,1(t).

P :Wc(P,P0)n()

P :Wc(P,Pn)n()

(11)

A key consequence of Theorem 3 is that setting  =  in the bound (10),  + EPn [(; Z)] certifies robustness for the worst-case population objective at any level . When  = n(), duality
shows that  =  minimizes the right hand side of the bound (10), and

EPn [ (; Z)] + n() = sup EP [ (; Z)] = EPn()[ (; Z)].
P :Wc(P,Pn)n() surrogate loss robustness

(12)

(See Section C.4 for a proof of these equalities.) The bound (11) gives a tight bound on performance for the n-robustness of the population loss, and the certificate (12) is easy to compute via expression (9): the transportation mappings T (, Zi) are efficiently computable for large enough , as noted in Section 2.1, and n = Wc(Pn, Pn) = EPn [c(T (, Z), Z)].
When the parameter set  is finite dimensional (  Rd), Theorem 3 provides a robustness guarantee scaling with d in spite of the infinite-dimensional Wasserstein penalty. Assuming there exist 0  , M0 <  such that such that | (0; z)|  M0 for all z  Z, we have the following corollary (see Section C.6 for a proof).
Corollary 1. Let (·; z) be L-Lipschitz with respect to some norm · for all z  Z. Assume that   Rd satisfies diam() = sup,   -  < . Then, the bounds (10) and (11) hold with

n,1(t) = b1

d(L diam() n

+

M0 )

+

b2(L

diam()

+

M0 )

for some numerical constants b1, b2 > 0.

t n

3.2 GENERALIZATION OF ADVERSARIAL EXAMPLES

We can also show that the level of robustness on the training set generalizes. Our starting point is Lemma 1, which shows that T(·; z) is smooth under Assumptions A and B:

T (1; z) - T (2; z)

 Lz [ - Lzz]+

1 - 2

(13)

for all 1, 2, where we recall that Lzz is the Lipschitz constant of z (; z). Leveraging this
smoothness, we show that n() = EPn [c(T(; Z), Z)], the level of robustness achieved for the empirical problem, concentrates uniformly around its population counterpart.

6

Under review as a conference paper at ICLR 2018

Theorem 4. Let Z  {z  Rm : z  Mz} so that Z  Mz almost surely and assume either that (i) c(·, ·) is Lc-Lipschitz over Z with respect to the norm · in each argument, or (ii) that (, z)  [0, M ] and z  (, z) is Lc-Lipschitz for all   . If Assumptions A and B hold, then with probability at least 1 - e-t,

sup


|EPn

[c(T

(;

Z ),

Z )] - EP0

[c(T

(;

Z ),

Z )]|



4D

1 n

t + log N

, [ - Lzz]+ t , · 4LcLz

.

(14)

where B = LcMz under assumption (i) and B = M / under assumption (ii).

See Section C.5 for the proof. For   Rd, we have log N (, , · )  d log(1 + diam() ) so
that the bound (14) gives the usual d/n generalization rate for the distance between adversarial perturbations and natural examples. Another consequence of Theorem 4 is that n(WRM) in the certificate (11) is positive as long as the loss is not completely invariant to data. To see this, note from the optimality conditions for T(; Z) that EP0 [c(T(; Z), Z)] = 0 iff z (; z) = 0 almost surely, and hence for large enough n, we have n() > 0 by the bound (14).

4 EXPERIMENTS

Our technique for distributionally robust optimization with adversarial training extends beyond supervised learning. To that end, we present empirical evaluations on supervised and reinforcement learning tasks where we compare performance with empirical risk minimization (ERM) and, where appropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), its iterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madry et al., 2017). PGM augments stochastic gradient steps for the parameter  with projected gradient ascent over x  (; x, y), iterating (for data point xi, yi)

xti+1() := argmax{x
 p

(; xit, yi)T }

and

xti+1 := B ,p(xti)

xit + txti()

(15)

for t = 1, . . . , Tadv, where  denotes projection onto B ,p(xi) := {x : x - xi p  }. We use the

squared Euclidean cost c(z, z ) :=

z-z

2 2

for

WRM

and

p

=

2

for

FGM,

IFGM,

PGM

training

in all experiments; we test against adversarial perturbations with respect to the norms p = 2, . We

use Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.

Larger adversarial budgets correspond to smaller  for WRM and larger for other models.

In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate the benefits of certified robustness. In Section 4.2 we consider a supervised learning problem for MNIST where we adversarially perturb the test data. Finally, we consider a reinforcement learning problem in Section 4.3, where the Markov decision process used for training differs from that for testing.

4.1 VISUALIZING THE BENEFITS OF CERTIFIED ROBUSTNESS

For our first experiment, we generate synthetic data Z = (X, Y )  P0 by Xi iid N(02, I2) with labels Yi = sign( x 2 - 2), where X  R2 and I2 is the identity matrix inR2. Furthermore, to create a wide margin separating the classes, we remove data with X 2  ( 2/1.3, 1.3 2). We train a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELU activations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM. For our approach, we use  = 0.5 and to make fair comparisons with FGM, we use

2 = n(WRM) = Wc(Pn(WRM), Pn) = EPn [c(T (WRM, Z), Z)], for the fast-gradient perturbation magnitude , where WRM is the output of Algorithm 1.1

(16)

Figure 1 illustrates the classification boundaries for the three training procedures over the ReLUactivated (Figure 1(a)) and ELU-activated (Figure 1(b)) models. Since 70% of the data are of the

1We verified that for ELU activations with scale parameter  = 1, this  is large enough for strong concavity of the inner problem (2b). We computed 2zz (, z)over the training data. ReLU's have no guarantees, but we use 15 gradient steps with stepsize decreasing as 1/ t for both activations.

7

Under review as a conference paper at ICLR 2018

(a) ReLU model

(b) ELU model

Figure 1. Experimental results on synthetic data. Training data are shown in blue and red. Classification boundaries are shown in yellow, purple, and green for ERM, FGM, and and WRM respectively. The boundaries are shown with the training data as well as separately with the true class boundaries.

 blue class ( X 2  2/1.3), distributional robustness favors pushing the classification boundary outwards; intuitively, adversarial examples are most likely to come from pushing blue points outwards across the boundary. ERM and FGM suffer from sensitivities to various regions of the data, as evidenced by the lack of symmetry in their classification boundaries. For both activations, WRM pushes the classification boundaries further outwards than ERM or FGM. However, WRM with ReLU's still suffers from sensitivities (e.g. asymmetry in the classification surface) due to the lack of robustness guarantees. WRM with ELUs provides a certified level of robustness, yielding an axisymmetric classification boundary that hedges against adversarial perturbations in all directions.

4.2 LEARNING A MORE ROBUST CLASSIFIER

We now consider a standard benchmark--training a neural network classifier on the MNIST dataset.

The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional filter layers with ELU activa-

tions followed by a fully connected layer and softmax output. We train our method (WRM) with



=

3 10

EPn

[

X

2], and for the other methods we choose

as the level of robustness achieved

by WRM (16).2 In the figures, we scale the budgets 1/adv and adv for the adversary with

C := EPn [ X p].3 All methods achieve at least 99% test-set accuracy, implying there is little

test-time penalty for the robustness levels ( and ) used for training the adversarial models.

It is thus important to distinguish the methods' abilities to combat attacks. We first test performance of the five methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (15) with respect to 2and -norms. In Figure 2(a) and (b), all adversarial methods outperform ERM, and WRM offers
more robustness even with respect to these PGM attacks. Training with the Euclidean cost still provides robustness to -norm fast gradient attacks. We provide further evidence in Appendix A.1.

Next we study stability of the loss surface with respect to perturbations to inputs. First, consider the distance to adversarial examples under the models  = ERM, FGM, IFGM, PGM, WRM,

test() := EPtest [c(Tadv (, Z), Z)],

(17)

where Ptest is the test distribution, c(z, z ) :=

x-x

2 2

as

usual,

and

Tadv (, Z)

=

argmaxz{ (; z) - advc(z, Z)} is the adversarial perturbation of Z (Monge map) for the model

. We note that small values of test() correspond to small magnitudes of z (; z) in a neigh-

borhood of the nominal input, which ensures stability of the model. Figure 3(a) shows that test

differs by orders of magnitude between the training methods; the trend is nearly uniform over all

adv, with WRM being the most stable. Thus, we see that our adversarial-training method defends

against gradient-exploiting attacks by reducing the magnitudes of gradients near the nominal input.

In Figure 3(b) we provide a qualitiative picture by adversarially perturbing a single test datapoint until the model misclassifies it. Specifically, we again consider WRM attacks and we decrease adv

2For this ,  (WRM; z) is strongly concave for 98% of the training data. 3For the standard MNIST dataset, C2 := EPn X 2 = 1.24 and C := EPn X  = 0.52.

8

Under review as a conference paper at ICLR 2018

100 100

10-1 10-1

10-2 0 0.5 1 1.5 2

10-2 0

0.1 0.2 0.3 0.4

(a) Test error vs. adv for · 2 attack

(b) Test error vs. adv for ·  attack

Figure 2. PGM attacks on the MNIST dataset. (a) and (b) show test misclassification error vs. the adversarial perturbation level adv for the PGM attack with respect to Euclidean and  norms respectively. The vertical bar in (a) indicates the perturbation level used for training the FGM, IFGM, and
PGM models as well as the estimated radius n(WRM). For MNIST, C2 = 1.24 and C = 0.52.

104

102

100

10-2

0 2 4 6 8 10 12

(a) test vs. 1/adv

(b) Perturbations on a test datapoint

Figure 3. Stability of the loss surface. In (a), we show the average distance of the perturbed distribution test for a given adv, an indicator of local stability to inputs for the decision surface. The vertical bar in (a) indicates the  we use for training WRM. In (b) we visualize the smallest WRM perturbation (largest adv) necessary to make a model misclassify a datapoint. More examples are in Appendix A.2.

until each model misclassifies the input. The original label is 8, whereas on the adversarial examples IFGM predicts 2, PGM predicts 0, and the other models predict 3. WRM's "misclassifications" appear consistently reasonable to the human eye (see Appendix A for examples with other digits); WRM defends against gradient-based exploits by learning a representation that makes gradients point towards inputs of other classes. Together, Figures 3(a) and (b) depict our method's defense mechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitude of gradients and improving their interpretability.

4.3 ROBUST MARKOV DECISION PROCESSES

For our final experiments, we consider distributional robustness in the context of Q-learning, a
model-free reinforcement learning techinque. We consider Markov decision processes (MDPs)
(S, A, Psa, r) with finite state space S, action space A, state-action transition probabilities Psa, and rewards r : S  R. The goal of a reinforcement-learning agent is to maximize (discounted) cumulative rewards t tE[r(st)] (with discount factor ); this is the analogue of minimizing EP [ (; Z)] in supervised learning. A robust MDP considers an ambiguity set Psa for the state-action transitions, and the goal is to maximize the worst-case realization infP Psa t tEP [r(st)]; this is the analogue of problem (1).

In a standard MDP, Q-learning learns a quality function Q : S × A  R via the iterations

Q(st, at)  Q(st, at) + t

r(st+1) +  max Q(st+1, a) - Q(st, at)
a

(18)

such that argmaxa Q(s, a) is (eventually) the optimal action to take in state s to maximize cumulative reward. In scenarios where the underlying environment has a continuous state-space, we can

9

Under review as a conference paper at ICLR 2018

Environment Regular

Robust

Original 399.7 ± 0.1 400.0 ± 0.0

Easier environments

Light

400.0 ± 0.0 400.0 ± 0.0

Long

400.0 ± 0.0 400.0 ± 0.0

Soft g

400.0 ± 0.0 400.0 ± 0.0

Harder environments

Heavy

150.1 ± 4.7 334.0 ± 3.7

Short

245.2 ± 4.8 400.0 ± 0.0

Strong g 189.8 ± 2.3 398.5 ± 0.3

Table 1. Episode length over 1000 trials (mean ± standard error)

400 350 300 250 200 150 100
50 0
0

500

1000

1500

2000

Figure 4. Episode lengths during training. The environment caps episodes to 400 steps.

easily modify the update (18) to include distributional robustness by an adversarial state perturbation. Namely, we draw the nominal state-transition update st+1  psa(st, at), and proceed with the
update (18) using the its adversarial perturbation

st+1  argmin r(s) + c(s, s^t+1) .
s

(19)

Since the underlying state-space is continuous, we can solve problem (19) efficiently using gradient descent. This procedure provides robustness to uncertainties in state-action transitions.4

We test this formulation of adversarial training in the classic cart-pole environment, where the goal is to balance a pole on a cart by moving the cart left or right. The environment caps episode lengths to 400 steps and ends the episode prematurely if the pole falls too far from the vertical or the cart translates too far from its origin. We use the reward r() := exp{-||}, where  is the angle of the pole form the vertical. Furthermore, we use a simple tabular representation for Q with 30 discretized states for  and 15 for its time-derivative . The action space is binary: push the cart left or right with a fixed force. Due to the nonstationary, policy-dependent effective radius for the Wasserstein ball, an analogous for the fast-gradient method (or other variants) is not well-defined. Thus, we only compare with an agent trained on the nominal MDP. We test the models with various perturbations to the physical parameters. Namely, we consider reducing/magnifying the pole's mass by 2, reducing/magnifying the pole's length by 2, and reducing/magnifying the strength of gravity g by 5. The dynamics of the system are such that the heavy, short, and strong-gravity cases are more physically unstable than the original environment, whereas their counterparts are less unstable.

Table 1 shows the performance of the trained models over the original MDP and all of the perturbed MDPs. Both models perform similarly over easier environments, but the robust model greatly outperforms in harder environments. Interestingly, as shown in Figure 4, the robust model also learns more efficiently than the nominal model in the original MDP. We hypothesize that a potential sideeffect of robustness is that adversarial perturbations encourage better exploration of the environment.

5 CONCLUSION
Explicit distributional robustness of the form (5) is intractable except in limited cases. We provide a method for efficiently guaranteeing distributional robustness with a simple form of adversarial data perturbation. Using only assumptions about the smoothness of the loss function , we prove that our method enjoys strong statistical guarantees and fast optimization rates for a large class of problems. The NP-hardness of certifying robustness for ReLU networks, coupled with our empirical success and theoretical certificates for smooth networks in deep learning, suggest that using smooth networks may be preferrable if we wish to guarantee robustness. Empirical evaluations indicate that our methods are in fact robust to perturbations in the data, and they outperform less-principled adversarial training techniques. The major benefit of our approach is its simplicity and wide applicability across many models and machine-learning scenarios.
4For tabular Q-learning, we can then round st+1 as usual. Since the update (19) simply modifies the stateaction transitions (independent of Q), all standard results on convergence for tabular Q-learning (e.g. Szepesva´ri & Littman (1999)) apply under these adversarial dynamics.

10

Under review as a conference paper at ICLR 2018
REFERENCES
P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463­482, 2002.
T. Bas¸ar and P. Bernhard. H-infinity optimal control and related minimax design problems: a dynamic game approach. Springer Science & Business Media, 2008.
S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Vaughan. A theory of learning from different domains. Machine Learning, 79:151­175, 2010.
A. Ben-Tal, L. E. Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press, 2009.
A. Ben-Tal, D. den Hertog, A. D. Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341­357, 2013.
D. Bertsimas, V. Gupta, and N. Kallus. Data-driven robust optimization. arXiv:1401.0212 [math.OC], 2013. URL http://arxiv.org/abs/1401.0212.
P. Billingsley. Convergence of Probability Measures. Wiley, Second edition, 1999. J. Blanchet, Y. Kang, and K. Murthy. Robust Wasserstein profile inference and applications to
machine learning. arXiv:1610.05627 [math.ST], 2016. S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification: a survey of some recent
advances. ESAIM: Probability and Statistics, 9:323­375, 2005. S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: a Nonasymptotic Theory of
Independence. Oxford University Press, 2013. N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and
Privacy (SP), 2017 IEEE Symposium on, pp. 39­57. IEEE, 2017. X. Chen, M. Sim, and P. Sun. A robust optimization perspective on stochastic programming. Oper-
ations Research, 55(6):1058­1071, 2007. Y. Chen, G. Lan, and Y. Ouyang. Accelerated schemes for a class of variational inequalities.
arXiv:1403.4164 [math.OC], 2014. D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by expo-
nential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application
to data-driven problems. Operations Research, 58(3):595­612, 2010. J. C. Duchi, P. W. Glynn, and H. Namkoong. Statistics of robust optimization: A generalized
empirical likelihood approach. arXiv:1610.03425 [stat.ML], 2016. URL https://arxiv. org/abs/1610.03425. P. M. Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. arXiv:1505.05116 [math.OC], 2015. S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341­2368, 2013. J. Goh and M. Sim. Distributionally robust optimization and its tractable approximations. Operations Research, 58(4):902­917, 2010. I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. W. He, J. Wei, X. Chen, N. Carlini, and D. Song. Adversarial example defenses: Ensembles of weak defenses are not strong. arXiv:1706.04701 [cs.LG], 2017. X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pp. 3­29. Springer, 2017. A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with the stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17­58, 2011. G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. arXiv:1702.01135 [cs.AI], 2017a.
11

Under review as a conference paper at ICLR 2018
G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards proving the adversarial robustness of deep neural networks. arXiv:1709.02802 [cs.LG], 2017b.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. arXiv:1611.01236 [cs.CV], 2016.
H. Lam and E. Zhou. Quantifying input uncertainty in stochastic optimization. In Proceedings of the 2015 Winter Simulation Conference. IEEE, 2015.
D. Luenberger. Optimization by Vector Space Methods. Wiley, 1969. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. arXiv:1706.06083 [stat.ML], 2017. T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual
adversarial training. arXiv:1507.00677 [stat.ML], 2015. S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool
deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574­2582, 2016. H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization with f -divergences. In Advances in Neural Information Processing Systems 29, 2016. H. Namkoong and J. C. Duchi. Variance regularization with convex objectives. In Advances in Neural Information Processing Systems 30, 2017. A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427­436, 2015. N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks against deep learning systems using adversarial examples. arXiv:1602.02697 [cs.CR], 2016a. N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372­387. IEEE, 2016b. N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582­597. IEEE, 2016c. N. Ratliff, J. A. Bagnell, and M. Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning, 2006. R. T. Rockafellar and R. J. B. Wets. Variational Analysis. Springer, New York, 1998. A. Rozsa, M. Gunther, and T. E. Boult. Towards robust deep neural networks with bang. arXiv:1612.00138 [cs.CV], 2016. S. Shafieezadeh-Abadeh, P. M. Esfahani, and D. Kuhn. Distributionally robust logistic regression. In Advances in Neural Information Processing Systems, pp. 1576­1584, 2015. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv:1312.6199 [cs.CV], 2013. C. Szepesva´ri and M. L. Littman. A unified analysis of value-function-based reinforcement-learning algorithms. Neural computation, 11(8):2017­2060, 1999. F. Trame`r, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv:1705.07204 [stat.ML], 2017. A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York, 1996. C. Villani. Optimal Transport: Old and New. Springer, 2009. H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines. The Journal of Machine Learning Research, 10:1485­1510, 2009. H. Xu, C. Caramanis, and S. Mannor. A distributional interpretation of robust optimization. Mathematics of Operations Research, 37(1):95­110, 2012.
12

Under review as a conference paper at ICLR 2018

A ADDITIONAL EXPERIMENTS
A.1 MNIST ATTACKS We repeat Figure 2 using FGM (Figure 5) and IFGM (Figure 6) attacks. The same trends are evident as in Figure 2.
100 100

10-1 10-1

10-2 0 0.5 1 1.5 2

10-2 0

0.1 0.2 0.3 0.4

(a) Test error vs. adv for · 2 attack

(b) Test error vs. adv for ·  attack

Figure 5. Fast-gradient attacks on the MNIST dataset. (a) and (b) show test misclassification error vs. the adversarial perturbation level adv for the FGM attack with respect to the Euclidean and  norms respectively. The vertical bar in (a) indicates the perturbation level that was used for training the FGM,
IFGM, and PGM models and the estimated radius n(WRM).

100 100

10-1 10-1

10-2 0 0.5 1 1.5 2

10-2 0

0.1 0.2 0.3 0.4

(a) Test error vs. adv for · 2 attack

(b) Test error vs. adv for ·  attack

Figure 6. Iterated fast-gradient attacks on the MNIST dataset. (a) and (b) show test misclassification error vs. the adversarial perturbation level adv for the IFGM attack with respect to the Euclidean and  norms respectively. The vertical bar in (a) indicates the perturbation level that was used for training
the FGM, IFGM, and PGM models and the estimated radius n(WRM).

A.2 MNIST STABILITY OF LOSS SURFACE
In Figure 7, we repeat the illustration in Figure 3(b) for more digits. WRM's "misclassifications" are consistently reasonable to the human eye, as gradient-based perturbations actually transform the original image to other labels. Other models do not exhibit this behavior with the same consistency (if at all). Reasonable misclassifications correspond to having learned a data representation that makes gradients interpretable.
A.3 MNIST EXPERIMENTS WITH VARIED 
In Figure 8, we choose a fixed WRM adversary (fixed adv) and perturb WRM models trained with various penalty parameters . We note that as the bound (10) with  =  suggests, even when the adversary has more budget than that used for training (1/ < 1/adv), degradation in performance is still smooth. Further, as we decrease the penalty , we see that the amount of achieved robustness-- measured here by test error on adversarial perturbations with adv--has diminishing gains; this is
13

Under review as a conference paper at ICLR 2018

(a) (b) (c)

(d) (e) (f)

(g) (h) (i)
Figure 7. Visualizing stability over inputs. We illustrate the smallest WRM perturbation (largest adv) necessary to make a model misclassify a datapoint.
102 100

101

100 10-1

10-1

0 2 4 6 8 10 12

0 2 4 6 8 10 12

(a) n vs. 1/

(b) Test error vs. 1/

Figure 8. (a) Stability and (b) test error for a fixed adversary. We train WRM models with various levels of  and perturb them with a fixed WRM adversary (adv indicated by the vertical bar).

again consistent to our theory which says that the inner problem (2b) is not efficiently computable for small values of .

14

Under review as a conference paper at ICLR 2018

B FINDING WORST-CASE PERTURBATIONS WITH RELU'S IS NP-HARD
We show that computing worst-case perturbations supuU (; z + u) is NP-hard for a large class of feedforward neural networks with ReLU activations. This result is essentially due to Katz et al. (2017a). In the following, we use polynomial time mean polynomial growth with respect to m, the dimension of the inputs z.
An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution grows polynomially, (ii) the language {u  U } can be recognized in polynomial time (i.e. a deterministic algorithm can decide in polynomial time whether u  U), and (iii) can be evaluated in polynomial time. We restrict analysis to feedforward neural networks with ReLU activations such that the corresponding worst-case perturbation problem is NPO.5 Furthermore, we impose separable structure on U , that is, U := {v  u  w} for some v < w  Rm.
Lemma 2. Consider feedforward neural networks with ReLU's and let U := {v  u  w}, where v < w such that the optimization problem max .uU (; z + u) is NPO. There exist  such that this optimization problem is also NP-hard.
Proof First, we introduce the decision reformulation of the problem: for some b, we ask whether there exists some u such that (; z + u)  b. The decision reformulation for an NPO problem is in NP, as a certificate for the decision problem can be verified in polynomial time. By appropriate scaling of , v, and w, Katz et al. (2017a) show that 3-SAT Turing-reduces to this decision problem: given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with a polynomial number of calls to D. The decision problem is thus NP-complete.
Now, consider an oracle O for the optimization problem. The decision problem Turing-reduces to the optimization problem, as the decision problem can be solved with one call to O. Thus, the optimization problem is NP-hard.

C PROOFS

C.1 PROOF OF PROPOSITION 1

We provide a slightly more general duality result, for which Proposition 1 is an immediate special
case. Recalling Rockafellar & Wets (1998, Def. 14.27 and Prop. 14.33), we say that a function g : X × Z  R is a normal integrand if for each , the mapping

z  {x | g(x, z)  }

is closed-valued and measurable. We recall that if g is continuous, then g is a normal integrand (Rockafellar & Wets, 1998, Cor. 14.34); therefore, g(x, z) = c(x, z) - (; x) is a normal
integrand. We have the following theorem.

Theorem 5. Let f, c be such that for any   0, the function g(x, z) = c(x, z) - f (x) is a normal integrand. (For example, continuity of f and closed convexity of c is sufficient.) For any  > 0 we have

sup f (x)dP (x) = inf

P :Wc(P,Q)

0

sup {f (x) - c(x, z)} dQ(z) +  .
xX

Proof First, the mapping P  Wc(P, Q) is convex in the space of probability measures. As taking P = Q yields Wc(Q, Q) = 0, Slater's condition holds and we may apply standard (infinite dimensional) duality results (Luenberger, 1969, Thm. 8.7.1) to obtain

sup f (x)dP (x) = sup inf

P :Wc(P,Q)

P :Wc(P,Q) 0

f (x)dP (x) - Wc(P, Q) + 

= inf sup
0 P :Wc(P,Q)

f (x)dP (x) - Wc(P, Q) +  .

5Note

that

z,

u



m
R

,

so

trivially

the

dimensionality

of

the

solution

grows

polynomially.

15

Under review as a conference paper at ICLR 2018

Now, noting that for any M  (P, Q) we have f dP = f (x)dM (x, z), we have that the rightmost quantity in the preceding display satisfies

f (x)dP (x) -  inf
M (P,Q)
That is, we have

c(x, z)dM (x, z) = sup
M (P,Q)

[f (x) - c(x, z)]dM (x, z) .

sup f (x)dP (x) = inf sup

P :Wc(P,Q)

0 P,M (P,Q)

[f (x) - c(x, z)]dM (x, z) +  .

(20)

Now, we note a few basic facts. First, because we have a joint supremum over P and measures M  (P, Q) in expression (20), we have that

sup [f (x) - c(x, z)]dM (x, z)  sup[f (x) - c(x, z)]dQ(z).

P,M (P,Q)

x

We would like to show equality in the above. To that end, we note that if P denotes the space of regular conditional probabilities (Markov kernels) from Z to X, then

sup [f (x) - c(x, z)]dM (x, z)  sup [f (x) - c(x, z)]dP (x | z)dQ(z).

P,M (P,Q)

P P

Recall that a conditional distribution P (· | z) is regular if P (· | z) is a distribution for each z and for each measurable A, the function z  P (A | z) is measurable. Let X denote the space of all measurable mappings z  x(z) from Z to X. Using the powerful measurability results of
Rockafellar & Wets (1998, Theorem 14.60), we have

sup [f (x(z)) - c(x(z), z)]dQ(z) = sup[f (x) - c(x, z)]dQ(z)

xX

xX

because f - c is upper semi-continuous, and the latter function is measurable. Now, let x(z) be any measurable function that is -close to attaining the supremum above. Define the conditional distribution P (· | z) to be supported on x(z), which is evidently measurable. Then using the preceding display, we have

[f (x) - c(x, z)]dP (x | z)dQ(z) = [f (x(z)) - c(x(z), z)]dQ(z)

As > 0 is arbitrary, this gives

 sup[f (x) - c(x, z)]dQ(z) -
xX

 sup

[f (x) - c(x, z)]dM (x, z) - .

P,M (P,Q)

sup [f (x) - c(x, z)]dM (x, z) = sup[f (x) - c(x, z)]dQ(z)

P,M (P,Q)

xX

as desired, which implies both equality (6) and completes the proof.

C.2 PROOF OF LEMMA 1

Differentiability is a consequence of one of the many forms of Danskin's Theorem (e.g. Appendix
B in Bas¸ar & Bernhard (2008)). For smoothness, we first argue that z () is continuous in . For any , optimality of z () implies that gz(, z ())T (z - z ())  0. By strong concavity, for any 1, 2 and z1 = z (1) and z2 = z (2), we have

 2

z1 - z2

2



f (2, z2 )-f (2, z1 )

and

f (2, z2 )



f

(2

,

z1

)+gz(2

,

z1

)T

(z2

-z1

)-

 2

z1 - z2 2 .

16

Under review as a conference paper at ICLR 2018

Summing these inequalities gives

 z1 - z2 2  gz(2, z1 )T (z2 - z1 )  (gz(2, z1 ) - gz(1, z1 ))T (z2 - z1 ),
where the last inequality follows because gz(1, z1 )T (z2 - z1 )  0. Using a cross-Lipschitz condition from above and Holder's inequality, we obtain

 z1 - z2 2  gz(2, z1 ) - gz(1, z1 ) z1 - z2  Lz 1 - 2

that is, Then we have

z1 - z2

 Lz 

1 - 2

.

z1 - z2 ,

(21)

g(1, z1 ) - g(2, z2 )

 g(1, z1 ) - g(1, z2 ) + g(1, z2 ) - g(2, z2 )

 Lz z1 - z2 + L 1 - 2



L

+

LzLz 

1 - 2 ,

where we have used inequality (21) again. This is the desired result.

C.3 PROOF OF THEOREM 2

Our proof is based on that of Ghadimi & Lan (2013).

For shorthand, let f (, z; z0) = (; z) - c(z, z0), noting that we perform gradient steps with
gt = f (t, z(t; zt); zt)
for zt an -approximate maximizer of f (, z; zt) in z, and t+1 = t - tgt. By a Taylor expansion using the L-smoothness of the objective F , we have

F (t+1)  F (t) +

F (t), t+1 - t

L +
2

t+1 - t

2 2

= F (t) - t

F (t)

2 2

+

Lt2 2

gt

2 2

+

t

F (t), F (t) - gt

= F (t) - t

1 - Lt 2

F (t)

2 2

(22)

+ t

1 + Lt 2

F (t), F (t) - gt

+ Lt2 2

gt - F (t)

2 2

.

Recalling the definition (2b) of (; z0) = supzZ f (, z; z0), we define the potentially biased errors t = gt - (t; zt). Letting zt = argmaxz f (t, z; zt), these errors evidently satisfy

t

2 2

=

 (t; zt) - f (, zt; zt)

2 2

=



(, zt ) - 

(, zt)

2 2

 L2z

zt - zt

2 2



L2z 

,

where the final inequality uses the  =  - Lzz strong-concavity of z  f (, z; z0). For shorthand,

let

= 2L2z
-Lzz

. Substituting the preceding display into the progress guarantee (22), we have

F (t+1) = F (t) - t

1 - Lt 2

F (t)

2 2

-

t

1 + Lt 2

F (t), t

+ t

1 + Lt 2

F (t), F (t) - (; zt)

+ Lt2 2

 (; zt) + t - F (t)

2 2



F (t)

-

t 2

(1

-

Lt)

F (t)

2 2

+

t 2

1 + Lt 2

t

2 2

t

1 + Lt 2

F (t), F (t) - (; zt)

+ Lt2(

 (t; zt) - F (t)

2 2

+

t

2 2

).

17

Under review as a conference paper at ICLR 2018

Noting that E[(t; zt) | t] = F (t), we take expectations to find

E[F

(t+1)

-

F

(t)

|

t]



-

t 2

(1

-

Lt)

F (t)

2 2

+

t + 5Lt2 24

+ Lt22,

(23)

where we have used that E[ (; Z) - F () 22]  2 by assumption. The bound (23) gives the theorem essentially immediately for fixed stepsizes , as we have

 2

(1

-

L)E

T

F (t)

2 2



F (0)

-

E[F (T +1)]

+

T 2

t=1

Noting that inf F ()  F (T +1) gives the final result.

5L 1+
4

+ T L22.

C.4 PROOF OF THEOREM 3

We first show the bound (10). From the duality result (5), we have the deterministic result that

sup EQ[ (; Z)]   + EQ[(; Z)]
P :Wc(P,Q)

for all  > 0, distributions Q, and   0. Next, we show that EPn [(; Z)] concentrates around its population counterpart at the usual rate (Boucheron et al., 2005). First, we have that

(; z)  [-M , M ],
because -M  (; z)  (; z)  supz (; z)  M . Thus, the functional   Fn() satisfies bounded differences (Boucheron et al., 2013, Thm. 6.2), and applying standard results on Rademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Wellner, 1996, Ch. 2.2) gives the result.

To see the second result (11), we substitute  = n in the bound (10). Then, with probability at least 1 - e-t, we have

sup EP [
P :Wc(P,P0)n()

(; Z)]



n() + EPn [ (; Z)] +

n,1(t).

Since we have

sup EP [ (; Z)] = EPn [(; Z)] + n().
P :Wc(P,Pn)n()

from the strong duality in Proposition 1, our second result follows.

C.5 PROOF OF THEOREM 4

Define

Pn() := argmax EP [ (; Z)] - Wc(P, Pn) ,
P
P () := argmax {EP [ (; Z)] - Wc(P, P0)} .
P

Fnoirtsatt,iownealshsiomwpltihcaittyPan(d)onalnydsPhonw(t)haerreesauttlatifnoerdPfor(a)llas

 . We omit the case for Pn

the ()

dependency on  is symmetric. Let

for P

be an -maximizer, so that

EP [ (; Z)] - Wc(P , P0)  sup {EP [ (; Z)] - Wc(Pn, P0)} - .
P

As Z is compact, the collection {P 1/k}kN is a uniformly tight collection of measures. By Prohorov's theorem (Billingsley, 1999, Ch 1.1, p. 57), (restricting to a subsequence if necessary), there exists some distribution P  on Z such that P 1/k d P  as k  . Continuity properties of
Wasserstein distances (Villani, 2009, Corollary 6.11) then imply that

lim Wc(P 1/k, P0) = Wc(P , P0).
k

(24)

18

Under review as a conference paper at ICLR 2018

Combining (24) and the monotone convergence theorem, we obtain

EP  [ (; Z)] - Wc(P , P0) = lim
k

EP 1/k [ (; Z)] - Wc(P 1/k, P0)

 sup {EP [ (; Z)] - Wc(P, P0)} .
P

We conclude that P  is attained for all P0.

Next, we show the concentration result (14). Recall the definition (8) of the transportation mapping

T (, z) := argmax { (; z ) - c(z , z)} ,
z Z
which is unique and well-defined under our strong concavity assumption that  > Lzz, and smooth (recall Eq. (13)) in . Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani, 2009, Chs. 9­10)), we have

EPn()[ (; Z) = EPn [ (; T (; Z))] and EP ()[ (; Z) = EP0 [ (; T (; Z))] Wc(Pn(), Pn) = EPn [c(T (; Z), Z)] and Wc(P (), P0) = EP0 [c(T (; Z), Z)].

We now proceed by showing the uniform convergence of

EPn [c(T (; Z), Z)] to EP0 [c(T (; Z), Z)]
under both cases (i), that c is Lipschitz, and (ii), that is Lipschitz in z, using a covering argument on . Recall inequality (13) (i.e. Lemma 1), which is that

T (1; z) - T (2; z)

 Lz [ - Lzz]+

1 - 2

.

We have the following lemma.

Lemma 3. Assume the conditions of Theorem 4. Then for any 1, 2  ,

|c(T (1; z), z)

-

c(T (2; z), z)|



LcLz [ - Lzz]+

1 - 2

.

Proof In the first case, that c is Lc-Lipschitz in its first argument, this is trivial: we have

|c(T (1; z), z) - c(T (2; z), z)|  Lc

T (1; z) - T (2; z)

 LcLz [ - Lzz]+

1 - 2

by the smoothness inequality (13) for T .

In the second case, that z  (, z) is Lc-Lipschitz, let zi = T (i; z) for shorthand. Then we have

c(z2, z) - c(z1, z) = c(z2, z) - (2, z2) + (2, z2) - c(z1, z)  c(z1, z) - (2, z1) + (2, z2) - c(z1, z) = (2, z2) - (2, z1),

and similarly,

c(z2, z) - c(z1, z) = c(z2, z) - (1, z1) + (1, z1) - c(z1, z)  c(z2, z) - (1, z1) + (1, z2) - c(z2, z) = (1, z2) - (1, z1).

Combining these two inequalities and using that

| (, z2) - (, z1)|  Lc z2 - z1

for any  gives the result.

Using Lemma 3 we obtain that   |EPn [c(T (; Z), )] - EP0 [c(T (; Z), Z)]| is

2LcLz/ [

- Lzz]+-Lipschitz.

Let

cover

=

{1, · · ·

, N }

be

a

[ -Lzz ]+ 4Lc Lz

t

-cover

of



with

respect

19

Under review as a conference paper at ICLR 2018

to · . From Lipschitzness of |EPn [c(T (; Z), Z)] - EP0 [c(T (; Z), Z)]|, we have that if for all   {cover},

|EPn [c(T (; Z), Z)]

-

EP0 [c(T (; Z), )]|



t ,
2

then it follows that

sup


|EPn

[c(T

(;

Z ),

Z )]

-

EP0

[c(T

(;

Z ),

Z )]|



t.

Under the first assumption (i), we have |c(T (; Z), Z)|  2LcMz. Applying Hoefdding's inequality, for any fixed   

P

|EPn [c(T (; Z), Z)]

-

EP0 [c(T (; Z), Z)]|



t 2

nt2  2 exp - 32Lc2Mz2

.

Taking a union bound over 1, · · · , N , we conclude that

P

sup


|EPn

[c(T

(;

Z ),

Z )]

-

EP0

[c(T

(;

Z ),

Z )]|



t

 2N

, [ - Lzz]+ t , · 4LcLz

exp

- nt2 32L2c Mz2

which was our desired result (14).

Under the second assumption (ii), we have from the definition of the transport map T

c(T (; z), z)  (; z)  M

and hence |c(T (; Z), Z)|  M /. The result for the second case follows from an identical reasoning.

C.6 PROOF OF COROLLARY 1

The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for completeness.

Note that for F = { (; ·) :   }, any ( , · )-covering {1, . . . , N } of  guarantees that mini | (; z) - (i; z)|  L for all , z, or

N (F , , · L(Z))  N (, /L, · ) 

diam()L 1+

d
,

where diam() = sup,   -  . Noting that | (; Z)|  L diam() + M0 =: M , we have the result.

20

