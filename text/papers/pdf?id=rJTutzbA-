Under review as a conference paper at ICLR 2018
ON THE INSUFFICIENCY OF EXISTING MOMENTUM SCHEMES FOR STOCHASTIC OPTIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these "fast gradient" methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching. Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, denoted as ASGD, is a simple to implement stochastic algorithm, based on a relatively less popular version of Nesterov's AGD. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.
1 INTRODUCTION
First order optimization methods, which access a function (to be optimized) through its gradient or an unbiased approximation of its gradient, are the workhorses for modern large scale optimization problems, which include training the current state-of-the-art deep neural networks. Gradient descent (Cauchy, 1847) is the simplest first order method that is used heavily in practice. However, it is known that for the class of smooth convex functions, gradient descent is suboptimal (Nesterov, 2004) and there exists a class of algorithms called fast gradient/momentum based methods which achieve optimal convergence guarantees. The heavy ball method (Polyak, 1964) and Nesterov's accelerated gradient descent (Nesterov, 1983) are two of the most popular methods in this category.
On the other hand, training deep neural networks on large scale datasets have been possible through the use of Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951), which samples a random subset of training data to compute gradient estimates that are then used to optimize the objective function. The advantages of SGD for large scale optimization and the related issues of tradeoffs between computational and statistical efficiency was highlighted in Bottou & Bousquet (2007).
The above mentioned theoretical advantages of fast gradient methods (Polyak, 1964; Nesterov, 1983) (albeit for smooth convex problems) coupled with cheap to compute stochastic gradient estimates led to the influential work of Sutskever et al. (2013), which demonstrated the empirical advantages possessed by SGD when augmented with the momentum machinery. This work has led to the wide spread adoption of momentum methods for training deep neural networks; so much so that, in the context of neural network training, gradient descent often refers to momentum methods.
But, there is a subtle difference between classical momentum methods and their implementation in practice ­ classical momentum methods work in the exact first order oracle model (Nesterov, 2004),
1

Under review as a conference paper at ICLR 2018
i.e., they employ exact gradients (computed on the full training dataset), while in practice (Sutskever et al., 2013), they are implemented with stochastic gradients (estimated from a randomly sampled subset/mini-batch of the training data). This leads to a natural question:
Are momentum methods optimal even in the stochastic first order oracle (SFO) model, where we access stochastic gradients computed on a small constant sized minibatches (or a batchsize of 1?)
Even disregarding the question of optimality of momentum methods in the SFO model, it is not even known if momentum methods (say, Polyak (1964); Nesterov (1983)) provide any provable improvement over SGD in this model. While these are open questions, a recent effort of Jain et al. (2017) showed that improving upon SGD (in the stochastic first order oracle) is rather subtle as there exists problem instances in SFO model where it is not possible to improve upon SGD, even information theoretically. Jain et al. (2017) studied a variant of Nesterov's accelerated gradient updates (Nesterov, 2012) for stochastic linear regression and show that their method improves upon SGD wherever it is information theoretically admissible. Through out this paper, we refer to the algorithm of (Jain et al., 2017) as Accelerated Stochastic Gradient Method (ASGD) while we refer to a stochastic version of the most widespread form of Nesterov's method (Nesterov, 1983) as NAG; HB denotes a stochastic version of the heavy ball method. Critically, while the result of Jain et al. (2017) shows that ASGD improves on SGD in any information-theoretically admissible regime, it is still not known whether HB and NAG can achieve a similar performance gain.
A key contribution of this work is to show that HB does not provide similar performance gain to ASGD even when it is informationally-theoretically admissible. That is, we provide a problem instance where it is indeed possible to improve upon SGD (and in fact ASGD achieves this improvement), but HB cannot achieve any improvement over SGD W e validate this claim empirically as well. In fact, we provide empirical evidence to the claim that NAG also do not achieve any improvement over SGD for several problems where ASGD can still achieve better rates of convergence.
This raises a question about why HB and NAG provide better performance than SGD in practice (Sutskever et al., 2013), especially for training deep networks. Our conclusion (that is well supported by our theoretical result) is that HB and NAG's improved performance is attributed to mini-batching and hence, these methods will often struggle to improve over SGD with small constant batch sizes. Note that this result indicates that there is a natural tension between the gains offered by momentum methods with larger batches and the superior generalization properties offered by training with small mini-batches (Keskar et al., 2016), which is a regime that may not be amenable for HB/NAG to improve upon SGD. This is in stark contrast to methods like ASGD, which are designed to improve over SGD across small or large mini-batch sizes. In fact, based on our experiments, we observe that on the task of training deep residual networks (He et al., 2016a) on the cifar-10 dataset, we note that ASGD offers noticeable improvements by achieving 5 - 7% better test error over HB and NAG even with commonly used batch sizes like 128 during the initial stages of the optimization.
1.1 CONTRIBUTIONS
The contributions of this paper are as follows.
1. In Section 3, we prove that HB is not optimal in the SFO model. In particular, there exist linear regression problems for which the performance of HB (with any step size and momentum) is either the same or worse than that of SGD while ASGD improves upon both of them.
2. Experiments on several different linear regression problems suggest that the suboptimality of HB in the SFO model is not restricted to special cases ­ it is rather widespread. Empirically, the same holds true for NAG as well (Section 5).
3. The above observations suggest that the only reason for the superiority of momentum methods in practice is minibatching, which reduces the variance in stochastic gradients and moves closer to exact first order oracle. This conclusion is supported by empirical evidence through training deep residual networks on cifar-10, with a batch size of 8 (see Section 5.3).
4. We present an intuitive and easier to tune version of ASGD (see Section 4) and show that ASGD can provide significantly faster convergence to a reasonable accuracy than SGD, HB, NAG, while still providing asymptotically optimal accuracy.
Hence, the take-home message of this paper is: HB and NAG are not optimal in the SFO model. The only reason for the superiority of momentum methods in practice is minibatching. ASGD provides a distinct advantage in training deep networks over SGD, HB and NAG.
2

Under review as a conference paper at ICLR 2018

Algorithm 1 HB: Heavy ball with a SFO

Algorithm 2 NAG: Nesterov's AGD with a SFO

Require: Initial w0, stepsize , momentum  Require: Initial w0, stepsize , momentum 

1: w-1  w0; t  0

/*Set w-1 to w0*/ 1: v0  w0; t  0

/*Set v0 to w0*/

2: while wt not converged do

2: while wt not converged do

3: wt+1  wt - ·ft(wt)+·(wt - wt-1) 3: vt+1  wt -  · ft(wt) /*SGD step*/ /*Sum of stochastic gradient step and mo- 4: wt+1 = (1 + )vt+1 - vt /*Sum of SGD

mentum*/

step and previous iterate*/

4: t  t + 1

5: t  t + 1

Ensure: wt

/*Return the last iterate*/ Ensure: wt

/*Return the last iterate*/

2 NOTATION

We denote matrices by bold-face capital letters and vectors by lower-case letters. f (w) =

1/n i fi(w) denotes the function to optimize w.r.t. w. f (w) denote gradient of f at w while

ft(w) denote a stochastic gradient of f . That is, ft(wt) = fit (w) where it is sampled uniformly at random from [1, . . . , n]. For linear regression, fi(w) = (y - w, xi )2 where y is the target

variable and x  d is the covariate, and ft(wt) = -(yt - wt, xt )xt. In this case, H = E xx

denotes

the

Hessian

of

f

and



=

1 (H) d (H)

denotes

it's

condition

number.

Algorithm 1 provides a pseudo-code of HB method (Polyak, 1964). wt-wt-1 is the momentum term and  denotes the momentum parameter. Next iterate wt+1 is obtained by a linear combination of the SGD update and the momentum term. Algorithm 2 provides pseudo-code of a stochastic version
of the most commonly used form of Nesterov's accelerated gradient descent (Nesterov, 1983)

3 SUBOPTIMALITY OF HEAVY BALL METHOD

In this section, we show that there exist linear regression problems where the performance of HB (Algorithm 1) is no better than that of SGD, while ASGD significantly improves upon this performance.Let us now describe the problem instance.

Fix w  R2 and let (x, y)  D be the following distribution over R2:

x=

1 · z · e1 w.p. 0.5 2 · z · e2 w.p. 0.5,

and y = w, x ,

where e1, e2  R2 are canonical basis vectors, 1 > 2 > 0. Let z be a random variable such that E z2 = 2 and E z4 = 2c  4. Hence, we have: E (x(i))2 = i2, E (x(i))4 = ci4, for i = 1, 2. Now, our goal is to minimize:

f (w) d=ef E ( w, x - y)2 , Hessian H d=ef E xx

=

12 0

0 22

.

Let  and ~ denote the computational and statistical condition numbers ­ see Jain et al. (2017) for

definitions.

For the problem above, we have 

=

2c12 22

and ~

=

2c.

Then we obtain following

convergence rates for SGD and ASGD when applied to the above given problem instance:

Corollary 1 (of Theorem 1 of Jain et al. (2016)). Let wtSGD be the tth iterate of SGD on the above

problem

with

starting

point

w0

and

stepsize

1 c12

.

The

error

of

wtSGD

can

be

bounded

as,

E f wtSGD

- f (w)  exp

-t 

f (w0) - f (w) .

On the other hand, ASGD achieves the following superior rate.

Corollary 2 (of Theorem 1 of Jain et al. (2017)). Let wtASGD be the tth iterate of ASGD on the

above problem with starting point w0 and appropriate parameters. The error of wtASGD can be

bounded as,

E f wtASGD

- f (w)  poly() exp

-t ~

f (w0) - f (w) .

3

Under review as a conference paper at ICLR 2018

Algorithm 3 Accelerated stochastic gradient descent ­ ASGD



Input: Initial w0, short step , long step parameter   1, statistical advantage parameter   

1: w¯0  w0; t  0

/*Set running average to w0*/

2:

1-

 

/*Set momentum value*/

3: while wt not converged do

4:

w¯t+1   · w¯t + (1 - ) ·

wt

-

· 0.7

·

ft(wt)

/*Update the running average as a

weighted average of previous running average and a long step gradient */

5:

wt+1



0.7 0.7+(1-)

·

wt -  · ft(wt)

+

1- 0.7+(1-)

·

w¯t+1

/*Update the iterate as

weighted average of current running average and short step gradient*/ 6: t  t + 1

Output: wt

/*Return the last iterate*/

Note that ~

=

2c is a constant while  

=

2c12 22

can be arbitrarily large.

Hence,

ASGD improves

upon rate of SGD by a factor of . The following proposition, which is the main result of this

section, establishes that HB (Algorithm 1) cannot provide a similar improvement. In fact, we

show that despite selecting best parameters for HB, it's convergence rate is exactly same as that

SGD (Corollary 1), up to constants.

Proposition 3. Let wtHB be the tth iterate of HB (Algorithm 1) on the above problem with starting point w0. For any choice of stepsize  and momentum   [0, 1], T large enough such that t  T ,

we have,

E f wtHB

- f (w)  C(, , ) · exp

-500t 

f (w0) - f (w) ,

where C(, , ) depends on ,  and  (but not on t).

That is, to obtain hand, ASGD can

w s.t. obtain

w-a-ppwroximat,ioHnBtorewquirinesO((lolgog1

)
1

samples and ) iterations.

iterations. While we

On the do not

other prove

it theoretically, we observe empirically that for the same problem instance, NAG also obtains nearly

same rate as HB and SGD.

4 ALGORITHM
In this section, we will present and explain an intuitive version of ASGD. Algorithm 3 presents the pseudocode. The algorithm takes three inputs: short step , long step parameter  and statistical advantage parameter . The short step  is precisely the same as the step size in SGD, HB or NAG. For convex problems, this scales inversely with the smoothness of the function. The long step parameter  is intended to give an estimate of the ratio of the largest and smallest curvatures of the function; for convex functions, this is just the condition number. The statistical advantage parameter  captures trade off between statistical and computational condition numbers ­ in the deterministic case,  =  and ASGD is equivalent to NAG, while in the high stochasticity regime,  is much smaller. The algorithm itself maintains two iterates: descent iterate wt and a running average w¯t. The running average is a weighted average of the previous average and a long gradient step from the descent iterate, while the descent iterate is updated as a convex combination of short gradient step from the descent iterate and the running average. The idea is that since the algorithm takes a long step as well as short step and an appropriate average of both of them, it can make progress on different directions at a similar pace. Appendix B shows the equivalence between Algorithm 3 and ASGD as proposed in Jain et al. (2017). Note that the constant 0.7 appearing in Algorithm 3
has no special significance. Jain et al. (2017) require it to be smaller than 1/6 but any constant smaller than 1 seems to work in experiments.

5 EXPERIMENTS
We now present our experimental results exploring performance of SGD, HB, NAG and ASGD. Our experiments are geared towards answering the following questions:
· Even for linear regression, is the suboptimality of HB restricted to specific distributions given in Section 3 or does it hold for more general distributions as well? Is the same true of NAG?

4

Under review as a conference paper at ICLR 2018

Figure 1: Plot of 1/rate (refer equation (1)) vs condition number () for various methods for the linear regression problem. Discrete distribution in the left, Gaussian to the right.

Algorithm Slope ­ discrete Slope ­ Gaussian

SGD HB NAG ASGD

0.9302 0.8522
0.98 0.5480

0.8745 0.8769 0.9494 0.5127

Table 1: Slopes (i.e. ) obtained by fitting a line to the curves in Figure 1. A value of  indicates

that the error decays at a rate of exp

-t 

. A smaller value of  indicates a faster rate of error decay.

· What is the reason for the superiority of HB and NAG in practice? Is it because momentum methods have better performance that SGD for stochastic gradients or due to minibatching? Does this superiority hold even for small minibatches?
· How does the performance of ASGD compare to that of SGD, HB and NAG, especially while training deep networks?

Section 5.1 and parts of Section 5.2 address the first two questions. Section 5.2 and 5.3 address Question 2 partially and the last question. We use Matlab to conduct experiments presented in Section 5.1 and use PyTorch pyt for our deep networks related experiments.

5.1 LINEAR REGRESSION In this section, we will present results on performance of the four optimization methods (SGD, HB, NAG, and ASGD) for linear regression problems. We consider two different class of linear regression problems, both of them in two dimensions. Given  which stands for condition number, we consider the following two distributions:

Discrete:

x

=

e1

w.p.

0.5

and

x

=

2 

·

e2

with

0.5;

ei

is

the

ith

standard

basis

vector.

Gaussian : x  R2 is distributed as a Gaussian random vector with covariance matrix

1 0

0 1.


We fix a randomly generated w  R2 and for both the distributions above, we let y = w, x . We vary  from {24, 25, ..., 212} and for each  in this set, we run 100 independent runs of all four
methods, each for a total of t = 5 iterations. We define that the algorithm converges if there is no
error in the second half (i.e. after 2.5 updates) that exceeds the starting error - this is reasonable
since we expect geometric convergence of the initial error.

Unlike ASGD and SGD, we do not know optimal learning rate and momentum parameters for NAG and HB in the stochastic gradient model. So, we perform a grid search over the values of the learning rate and momentum parameters. In particular, we lay a 10 × 10grid in [0, 1] × [0, 1] for learning rate and momentum and run NAG and HB. Then, for each grid point, we consider the subset of 100 trials that converged and computed the final error using these. Finally, the parameters that yield the minimal error are chosen for NAG and HB, and these numbers are reported. We measure convergence performance of a method using:

rate = log(f (w0)) - log(f (wt)) , t

(1)

5

Under review as a conference paper at ICLR 2018
Figure 2: Training loss (left) and test loss (right) while training deep autoencoder for mnist with minibatch size 8. Clearly, ASGD matches performance of NAG and outperforms SGD on the test data. HB also outperforms SGD.
We compute the rate (1) for all the algorithms with varying condition number . Given a rate vs  plot for a method, we compute it's slope (denoted as ) using linear regression. Table 1 presents the estimated slopes (i.e. ) for various methods for both the discrete and the Gaussian case. The slope values clearly show that the rate of SGD, HB and NAG have a nearly linear dependence on  while that of ASGD seems to scale linearly with .
5.2 DEEP AUTOENCODERS FOR MNIST
In this section, we present experimental results on training deep autoencoders for the mnist dataset, and we closely follow the setup of Hinton & Salakhutdinov (2006). This problem is a standard benchmark for evaluating the performance of different optimization algorithms e.g., Martens (2010); Sutskever et al. (2013); Martens & Grosse (2015); Reddi et al. (2017). The network architecture follows previous work Hinton & Salakhutdinov (2006) and is represented as 784 - 1000 - 500 - 250 - 30 - 250 - 500 - 1000 - 784 with the first and last 784 nodes representing the input and output respectively. All hidden/output nodes employ sigmoid activations except for the layer with 30 nodes which employs linear activations and we use MSE loss. Initialization follows the scheme of Martens (2010), also employed in Sutskever et al. (2013); Martens & Grosse (2015). We perform training with two minibatch sizes -1 and 8. The runs with minibatch size of 1 were run for 30 epochs while the runs with minibatch size of 8 were run for 50 epochs. For each of SGD, HB, NAG and ASGD, a grid search over learning rate, momentum and long step parameter (whichever is applicable) was done and best parameters were chosen based on achieving the smallest training error in the same protocol followed by (say,)Sutskever et al. (2013). The grid was extended whenever the best parameter fell at the edge of a grid. For the parameters chosen by grid search, we perform 10 runs with different seeds and averaged the results. The results are presented in Figures 2 and 3. Note that the final loss values reported here are suboptimal compared to those in published literature e.g., Sutskever et al. (2013); while Sutskever et al. (2013) report results after 750000 updates with a large batch size of 200 (which implies a total of 750000 × 200 = 150M gradient evaluations), whereas, our results are after 1.8M updates of SGD with a batch size 1 (which is just 1.8M gradient evaluations). Effect of minibatch sizes: While HB and NAG decay the loss faster compared to SGD for a minibatch size of 8 (Figure 2), this superior decay rate does not hold for a minibatch size of 1 (Figure 3). This supports our intuitions from the stochastic linear regression setting, where we demonstrate that HB and NAG are suboptimal in the stochastic first order oracle model. Comparison of ASGD with momentum methods: While ASGD performs slightly better than NAG for batch size 8 in the training error (Figure 2), ASGD decays the error at a faster rate compared to all the three other methods for a batch size of 1 (Figure 3).
5.3 DEEP RESIDUAL NETWORKS FOR CIFAR-10
In this section, we will present experimental results on training deep residual networks He et al. (2016b) with pre-activation blocks as introduced in He et al. (2016a) for classifying images in cifar10 Krizhevsky & Hinton (2009); the network we use has 44 layers (dubbed preresnet-44). The code for these experiments was downloaded from pre. One of the most distinct characteristics of this
6

Under review as a conference paper at ICLR 2018

Figure 3: Training loss (left) and test loss (right) while training deep autoencoder for mnist with minibatch size 1. Interestingly, SGD, HB and NAG, all decrease the loss at a similar rate, while ASGD decays at a faster rate.
experiment compared to our previous experiments is learning rate decay. We use a validation set based decay scheme, wherein, after every 3 epochs, we decay the learning rate by a certain factor (which we grid search on) if the validation zero one error does not decrease by at least a certain amount (precise numbers are provided in the appendix since they vary across batch sizes). Due to space constraints, we present only a subset of training error plots. Please see Appendix C.3 for some more plots on training errors.
Effect of minibatch sizes: Our first experiment tries to understand how the performance of HB and NAG compare with that of SGD and how it varies with minibatch sizes. Figure 4 presents the test zero one error for minibatch sizes of 8 and 128. While training with batch size 8 was done for 40 epochs, with batch size 128, it was done for 120 epochs. We perform a grid search over all parameters for each of these algorithms. See Appendix C.3 for details on the grid search parameters. We observe that final error achieved by SGD, HB and NAG are all very close for both batch sizes. While NAG exhibits a superior rate of convergence compared to SGD and HB for batch size 128, this superior rate of convergence disappears for a batch size of 8.

Figure 4: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value for batch size 8 (right) for SGD, HB and NAG.

Comparison of ASGD with momentum methods: The next experiment tries to understand how ASGD compares with HB and NAG. The errors achieved by various methods when we do grid search over all parameters are presented in Table 2. Note that the final test errors for batch size 128 are better than those for batch size 8 since the former was run for 120 epochs while the latter was run only for 40 epochs (due to time constraints).

Algorithm
SGD HB NAG ASGD

Final test error ­ batch size 128
8.32 ± 0.21 7.98 ± 0.19 7.63 ± 0.18 7.23 ± 0.22

Final test error ­ batch size 8
9.57 ± 0.18 9.28 ± 0.25 9.07 ± 0.18 8.52 ± 0.16

Table 2: Final test errors achieved by various methods for batch sizes of 128 and 8. The hyperparameters have been chosen by grid search.

7

Under review as a conference paper at ICLR 2018
Figure 5: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value for batch size 8 (right) for ASGD compared to HB. In the above plots, both ASGD and ASGD-HbParams refer to ASGD run with the learning rate and decay schedule of HB. ASGD-Fully-Optimized refers to ASGD where learning rate and decay schedule were also selected by grid search.
Figure 6: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value for batch size 8 (right) for ASGD compared to NAG. In the above plots, ASGD was run with the learning rate and decay schedule of NAG. Other parameters were selected by grid search.
While the final error achieved by ASGD is similar to that of all other methods, we are more interested in understanding whether ASGD has a superior convergence speed. In order to do this experiment however, we need to address the issue of different learning rates used by various algorithms and different places where they decay learning rate. So, for each of HB and NAG, we choose the learning rate and decay factors by grid search, use these values for ASGD and do grid search only over long step parameter  and momentum  for ASGD. The results are presented in Figures 5 and 6. For batch size 128, ASGD decays error at a faster rate compared to both HB and NAG. For batch size 8, while we see a superior convergence of ASGD compared to NAG, we do not see this superiority over HB. The reason for this turns out to be that the learning rate for HB, which we also use for ASGD, turns out to be quite suboptimal for ASGD. So, for batch size 8, we also compare fully optimized (i.e., grid search over learning rate as well) ASGD with HB. The superiority of ASGD over HB is clear from this comparison. These results suggest that ASGD decays error at a faster rate compared to HB and NAG across different batch sizes.
6 RELATED WORK
First order oracle methods: The primary method in this family is Gradient Descent (GD) (Cauchy, 1847). As mentioned previously, GD is suboptimal for smooth convex optimization (Nesterov, 2004), and this is addressed using momentum methods such as the Heavy Ball method (Polyak, 1964) (for quadratics), and Nesterov's Accelerated gradient descent (Nesterov, 1983). Stochastic first order methods: The simplest method employing the SFO is Stochastic Gradient Descent (Robbins & Monro, 1951)(SGD); the effectiveness of SGD has been immense, and its applicability goes well beyond optimizing convex objectives. Accelerating SGD is a tricky proposition given the instability of fast gradient methods in dealing with noise, as evidenced by several negative results which consider both statistical (Proakis, 1974; Polyak, 1987; Roy & Shynk, 1990) and adversarial errors (Devolder et al., 2014). A result of Jain et al. (2017) developed the first provably accelerated SGD method for linear regression inspired by a method of Nesterov (2012). Other schemes such as Ghadimi & Lan (2012; 2013); Dieuleveut et al. (2016), which indicate acceleration
8

Under review as a conference paper at ICLR 2018
is possible with noisy gradients do not hold in the SFO model satisfied by algorithms that are run in practice (see Jain et al. (2017) for more details). While HB (Polyak, 1964) and NAG (Nesterov, 1983) are known to be effective in case of exact first order oracle, for the SFO, the theoretical performance of HB and NAG is not well understood. Polyak (1987) describes HB to be rather brittle when provided with noisy gradient estimates. Practical methods for training deep networks: Momentum based methods employed with stochastic gradients (Sutskever et al., 2013) have become standard and very popular in practice. These schemes tend to outperform standard SGD on several important practical problems. As previously mentioned, we attribute this improvement to effect of minibatching rather than improvement by HB or NAG when working with stochastic gradients. Other schemes such as Adagrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) represent an important and useful class of algorithms. The advantages offered by these methods are orthogonal to the advantages offered by fast gradient methods; it is an important direction to explore augmenting these methods with ASGD.
7 CONCLUSIONS AND FUTURE DIRECTIONS
In this paper, we show that the performance gain of HB over SGD in stochastic setting is attributable to minibatching rather than the algorithm's ability to accelerate with stochastic gradients. Concretely, we provide a formal proof that for several easy problem instances, HB does not outperform SGD despite large condition number of the problem; we observe this trend for NAG in our experiments. In contrast, ASGD (Jain et al., 2017) provides significant improvement over SGD for the same problem instances. We observe similar trend when training a resnet on cifar-10 and an autoencoder on mnist. This work motivates several directions such as understanding the behavior of ASGD on other domains such as NLP, combining ASGD with adagrad (Duchi et al., 2011)/adam (Kingma & Ba, 2014) and possibly developing automatic tuning schemes similar to (Zhang et al., 2017).
REFERENCES
Preresnet-44 for cifar-10. https://github.com/D-X-Y/ResNeXt-DenseNet. Accessed: 2017-10-25.
Pytorch. https://github.com/pytorch. Accessed: 2017-10-25.
Le´on Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.
Louis Augustin Cauchy. Me´thode ge´ne´rale pour la re´solution des syste´mes d'e´quations simultanees. C. R. Acad. Sci. Paris, 1847.
Olivier Devolder, Franccois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146:37­75, 2014.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM Journal on Optimization, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV (4), Lecture Notes in Computer Science, pp. 630­645. Springer, 2016a.
9

Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016b.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint arXiv:1610.03774, 2016.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
James Martens. Deep learning via hessian-free optimization. In International conference on machine learning, 2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, 2015.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372­376, 1983.
Yurii E. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization. Kluwer Academic Publishers, 2004.
Yurii E. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341­362, 2012.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
Boris T. Polyak. Introduction to Optimization. Optimization Software, 1987.
John G. Proakis. Channel identification for high speed digital communications. IEEE Transactions on Automatic Control, 1974.
Sashank Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, and Alexander Smola. A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434, 2017.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, vol. 22, 1951.
Sumit Roy and John J. Shynk. Analysis of the momentum lms algorithm. IEEE Transactions on Acoustics, Speech and Signal Processing, 1990.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139­1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Jian Zhang, Ioannis Mitliagkas, and Christopher R. Yellowfin and the art of momentum tuning. CoRR, abs/1706.03471, 2017.
10

Under review as a conference paper at ICLR 2018

A SUBOPTIMALITY OF HB: PROOF OF PROPOSITION 3

Before proceeding to the proof, we introduce some additional notation. Let (t+j)1 denote the concatenated and centered estimates in the jth direction for j = 1, 2.

(t+j)1 d=ef

wt(+j)1 - (w)(j) wt(j) - (w)(j)

,

j = 1, 2.

Since the distribution over x is such that the coordinates are decoupled, we see that (t+j)1 can be written in terms of (tj) as:

t(+j)1 = A(t+j)1(tj), with At(+j)1 =

1 +  - (xt(+j)1)2 1

- 0

.

Let (t+j)1 d=ef E (t+j)1  (t+j)1 denote the covariance matrix of t(+j)1. We have t(+j)1 = B(j)t(j) with, B(j) defined as

 E

(1 +  - (x(j))2)2

B(j)

d=ef

 

E

E

(1 +  - (x(j))2) (1 +  - (x(j))2)

1

E -(1 +  - (x(j))2) 0 - 0

(1 +  - j2)2 + (c - 1)(j2)2

=

 



(1 +  - j2) (1 +  - j2)

1

-(1 +  - j2) 0
-
0

E -(1 +  - (x(j))2 -

0 0

-(1 +  - j2) -
0
0

2

0 0

  

.

0

2
0 
0
0

We prove Proposition 3 by showing that for any choice of stepsize and momentum, either of the two holds:

· B(1) has an eigenvalue larger than 1, or,

·

the largest eigenvalue of B(2) is greater than 1 -

500 

.

This is formalized in the following two lemmas.

Lemma 4.

If the stepsize  is such that 12



2(1-2)
c+(c-2)

,

then

B(1)

has

an

eigenvalue



1.

Lemma 5.

If the stepsize  is such that 12

<

2(1-2)
c+(c-2)

,

then

B(2)

has

an

eigenvalue

of

magnitude



1

-

500 

.

Given this notation, we can now consider the jth dimension without the superscripts; when needed, they will be made clear in the exposition. Denoting x d=ef 2 and t d=ef 1 +  - x, we have:

t2 + (c - 1)x2 -t -t 2

B= 

t t

0 - 0 
- 0 0 

1 0 00

A.1 PROOF
The analysis goes via computation of the characteristic polynomial of B and evaluating at different values to obtain bounds on its roots. Lemma 6. The characteristic polynomial of B is:
D(z) = z4 - (t2 + (c - 1)x2)z3 + (2t2 - 22)z2 + (-t2 + (c - 1)x2)2z + 4.

11

Under review as a conference paper at ICLR 2018

Proof. We first begin by writing out the expression for the determinant:

t2 + (c - 1)x2 - z -t -t 2

Det(B - zI) =

t t

-z -

- -z

0 0

.

1 0 0 -z

expanding along the first column, we have:

Det(B - zI) = (t2 + (c - 1)x2 - z)(2z - z3) - t(-tz2 + 2tz) + t(-t(z) + z · tz) - (z · 2z - 4) = (t2 + (c - 1)x2 - z)(2z - z3) - 2t(2tz - tz2) - (2z2 - 4).

Expanding the terms yields the expression in the lemma.

The next corollary follows by some simple arithmetic manipulations. Corollary 7. Substituting z = 1 -  in the characteristic equation of Lemma 6, we have:
D(1 -  ) =  4 +  3(-4 + t2 + (c - 1)x2) +  2(6 - 3t2 - 3(c - 1)x2 - 22 + 2t2) +  (-4 + 3t2 + 3(c - 1)x2 + 42 - 4t2 - (c - 1)x22 + t22) + (1 - t2 - (c - 1)x2 - 22 + 2t2 + (c - 1)x22 - t22 + 4) =  4 +  3[-(3 + )(1 - ) - 2x(1 + ) + cx2] +  2[(3 - 4 - 2 + 23) - 2x(1 + )(2 - 3) + x2(2 - 3c)] +  [-(1 - )2(1 - 2) - 2x(3 - )(1 - 2) + x2(3c - 4 + (2 - c)2)] + x(1 - )[2(1 - 2) - x(c + (c - 2))].

(2)

Proof of Lemma 4. The first observation necessary to prove the lemma is that the characteristic polynomial D(z) approaches  as z  , i.e., limz D(z) = +.
Next, we evaluate the characteristic polynomial at 1, i.e. compute D(1). This follows in a straightforward manner from corollary (7) by substituting  = 0 in equation (2), and this yields,

D(1) = (1 - )x · 2(1 - 2) - x(1 - ) - (c - 1)x(1 + ) .

As  < 1, x = 2 > 0, we have the following by setting D(1)  0 and solving for x:

2(1 - 2)

x



c

+

(c

-

. 2)

Since D(1)  0 and D(z)  0 as z  , there exists a root of D(·) which is  1.

Remark 8. The above characterization is striking in the sense that for any c > 1, increasing the

momentum parameter  naturally requires the reduction in the step size  to permit the convergence

of the algorithm, which is not observed when fast gradient methods are employed in deterministic

optimization. For instance, in the case of deterministic optimization, setting c = 1 yields 12 <

2(1 + ). On the other hand, when employing the stochastic heavy ball method with x(j) = 2j2,

we

have

the

condition

that

c

=

2,

and

this

implies,

12

<

2(1-2 ) 2

=

1

-

2.

We now prove Lemma 5. We first consider the large momentum setting.

Lemma 9. When the momentum parameter  is set such that 1 - 450/    1, B has an

eigenvalue

of

magnitude



1

-

450 

.

Proof. This follows easily from the fact that det(B) = 4 =

4 j=1

j

(B)



(max(B))4,

thus

implying 1 - 450/    |max(B)|.

Remark 10. Note that the above lemma holds for any value of the learning rate , and holds for every eigen direction of H. Thus, for "large" values of momentum, the behavior of stochastic heavy ball does degenerate to the behavior of stochastic gradient descent.

12

Under review as a conference paper at ICLR 2018

We now consider the setting where momentum is bounded away from 1.
Corollary 11. Consider B(2), by substituting  = l/, x = min = c(12)/ in equation (2) and accumulating terms in varying powers of 1/, we obtain:

G(l)

d=ef

c3(12)2l3 5

+

l4

- 2c(12)l3(1

+ ) + 4

(2

- 3c)c2(12)2l2

+

-(3

+

)(1

-

)l3

-

2(1

+

)(2

-

3)c(12)l2 3

+

(3c

-

4

+

(2

-

c)2)c2(12)2l

+

(3

-

4

-

2

+

23)l2

-

2c(12)l(3

-

)(1 2

-

2)

-

c2(12)2(1

-

)(c

+

(c

-

2))

+ -(1 - )2(1 - 2)l + 2c(12)(1 - )(1 - 2) 

(3)

Lemma 12.

Let 2 < c < 3000, 0    1 -

450 

,

l

=

1

+

2c(12 1-

)

.

Then,

G(l)



0.

Proof.

Since (12) 

2(1-2 ) c+(c-2)

,

this

implies

( 12 ) 1-



2(1+) c+(c-2)



4 c

,

thus

implying,

1



l



9.

Substituting the value of l in equation (3), the coefficient of O(1/) is -(1 - )3(1 + ).

We will bound this term along with (3 - 4 - 2 + 23)l2/2 = (1 - )2(3 + 2)l2/2 to obtain:

-(1 - )3(1 + ) 

+

(1 - )2(3 + 2)l2 2



-(1 - )3(1 + ) 

+

405(1 - )2 2

 (1 - )2 405 - (1 - 2) 

 (1 - )2 

405 - (1 - ) 



-

45

· 4502 4

,

where, we use the fact that  < 1, l  9. The natural implication of this bound is that the terms that are lower order, such as O(1/4) and O(1/5) will be negative owing to the large constant above. Let us verify that this is indeed the case by considering the terms having powers of O(1/4) and O(1/5) from equation (3):

c3(12)2l3 5

+

l4

-

2c(12)l3(1

+

) + 4

(2

-

3c)c2(12)2l2

-

45

· 4502 4



c3(12)2l3 5

+

l4 4

-

45

· 4502 4

cl3 (94 - (45 · 4502)) 93c + 94 - (45 · 4502)

 5 + 4 

4

The expression above evaluates to  0 given an upperbound on the value of c. The expression above follows from the fact that l  9,   1.

Next, consider the terms involving O(1/3) and O(1/2), in particular,

(3c

-

4

+

(2

- c)2)c2(12)2l 3

-

c2(12)2(1

-

)(c 2

+

(c

-

2))



c2(12)2 2

l(3c + 2) - (1 - )(c + (c - 2)) 

 c2(12)2 2

5cl - (1 - )(c + (c - 2)) 



c2(12)2 2

5cl - (1 - )c 



c3(12)2 2

5l - 450 



c3(12)2 2

·

-405 



0.

13

Under review as a conference paper at ICLR 2018

Next,

-2(1

+

)(2 - 3

3)c(12)l2

-

2c(12)l(3 - )(1 2

-

2)

 2(1 + )c(12)l 2

-(2 - 3)l - (3 - )(1 - ) 

 2(1 + )c(12)l 2

3l - 2(1 - ) 



2(1

+ )c(12)l 2

3l - 2 · 450 



2(1

+ )c(12)l 2

3 · 27 - 2 · 450 

 0.

In both these cases, we used the fact that 



1-

450 

implying -(1 - )



-450 

.

Finally, other

remaining terms are negative.

We are now ready to prove Lemma 5.

Proof of Lemma 5. Combining Lemmas 9 and 12, we see that no matter what stepsize and momen-

tum we

choose,

B(j)

has an eigenvalue

of magnitude

at

least 1 -

500 

for

some j



{1, 2}.

This

proves the lemma.

B EQUIVALENCE OF ALGORITHM 3 AND ASGD

We begin by writing out the updates of ASGD as written out in Jain et al. (2017), which starts with two iterates a0 and d0, and from time t = 0, 1, ...T - 1 implements the following updates:

bt = 1at + (1 - 1)dt

(4)

at+1 = bt - 1^ ft+1(bt)

(5)

ct = 1bt + (1 - 1)dt

(6)

dt+1 = ct - 1^ ft+1(bt).

(7)

 Next, we specify the step sizes 1 = c23/ , 1 = c3/(c3 + ), 1 = /(c3min) and 1 = 1/R2, where  = R2/min. Note that the step sizes in the paper of Jain et al. (2017) with c1 in their paper set to 1 yields the step sizes above. Now, substituting equation 6 in equation 7 and substituting the

value of 1, we have:

dt+1 = 1 = 1

bt

-

1 c3min

^ ft+1

(bt)

+ (1 - 1)dt

bt

-

 c3

^ ft+1

(bt)

+ (1 - 1)dt.

(8)

We see that dt+1 is precisely the update of the running average w¯t+1 in the ASGD method employed in this paper.

We now update bt to become bt+1 and this can be done by writing out equation 4 at t + 1, i.e:

bt+1 = 1at+1 + (1 - 1)dt+1 = 1 bt - 1^ ft+1(bt) + (1 - 1)dt+1.

(9)

By substituting the value of 1 we note that this is indeed the update of the iterate as a convex combination of the current running average and a short gradient step as written in this paper. In this
paper, we set c3 to be equal to 0.7, and any constant less than 1 works. In terms of variables, we note that  in this paper's algorithm description maps to 1 - 1.

14

Under review as a conference paper at ICLR 2018
C MORE DETAILS ON EXPERIMENTS
In this section, we will present more details on our experimental setup.
C.1 LINEAR REGRESSION
In this section, we will present some more results on our experiments on the linear regression problem. Just as in Appendix A, it is indeed possible to compute the expected error of all the algorithms among SGD, HB, NAG and ASGD, by tracking certain covariance matrices which evolve as linear systems. For SGD, for instance, denoting tSGD d=ef E wtSGD - w  wtSGD - w , we see that St+G1D = B  tSGD, where B is a linear operator acting on d × d matrices such that B  M d=ef M - HM - M H + 2E x, M x xx . Similarly, HB, NAG and ASGD also have corresponding operators (see Appendix A for more details on the operator corresponding to HB). The largest magnitude of the eigenvalues of these matrices indicate the rate of decay achieved by the particular algorithm ­ smaller it is compared to 1, faster the decay. We now detail the range of parameters explored for these results: the condition number  was varied from {24, 25, .., 228} for all the optimization methods and for both the discrete and gaussian problem. For each of these experiments, we draw 1000 samples and compute the empirical estimate of the fourth moment tensor. For NAG and HB, we did a very fine grid search by sampling 50 values in the interval (0, 1] for both the learning rate and the momentum parameter and chose the parameter setting that yielded the smallest max(B) that is less than 1 (so that it falls in the range of convergence of the algorithm). As for SGD and ASGD, we employed a learning rate of 1/3 for the Gaussian case and a step size of 0.9 for the discrete case. The statistical advantage parameter of ASGD was chosen to be 3/2 for the Gaussian case and 2/3 for the Discrete case, and the a long step parameters of 3 and 2 were chosen for the Gaussian and Discrete case respectively. The reason it appears as if we choose a parameter above the theoretically maximal allowed value of the advantage parameter is because the definition of  is different in this case. The  we speak about for this experiment is max/min unlike the condition number for the stochastic optimization problem. In a manner similar to actually running the algorithms (the results of whose are presented in the main paper), we also note that we can compute the rate as in equation 1 and join all these rates using a curve and estimate its slope (in the log scale). This result is indicated in table 3. Figure 7 presents these results, where for each method, we did grid search over all parameters and chose parameters that give smallest max. We see the same pattern as in Figure 1 from actual runs ­ SGD,HBand NAG all have linear dependence on condition number , while ASGD has a dependence of .
Figure 7: Expected rate of error decay (equation 1) vs condition number for various methods for the linear regression problem. Left is for discrete distribution and right is for Gaussian distribution.
15

Under review as a conference paper at ICLR 2018

Algorithm
SGD HB NAG ASGD

Slope ­ discrete
0.9990 1.0340 1.0627 0.4923

Slope ­ Gaussian
0.9995 0.9989 1.0416 0.4906

Table 3: Slopes (i.e. ) obtained by fitting a line to the curves in Figure 7. A value of  indicates

that the error decays at a rate of exp

-t 

. A smaller value of  indicates a faster rate of error decay.

C.2 AUTOENCODERS FOR MNIST

We begin by noting that the learning rates tend to vary as we vary batch sizes, which is something that is known in theory (Jain et al., 2016). Furthermore, we extend the grid especially whenever our best parameters of a baseline method tends to land at the edge of a grid. The parameter ranges explored by our grid search are:

Batch Size 1: (parameters chosen by running for 20 epochs)

 



· SGD: learning rate: {0.01, 0.01 10, 0.1, 0.1 10, 1, 10, 5, 10, 20, 10 10, 40, 60, 80, 100.

 

· NAG/HB: learning rate:

{0.01 10, 0.1, 0.1 10, 1, 10, 10}, momentum

{0, 0.5, 0.75, 0.9, 0.95, 0.97}.

· ASGD: learning rate: {2.5, 5}, long step {100.0, 1000.0}, advantage parameter {2.5, 5.0, 10.0, 20.0}.

Batch Size 8: (parameters chosen by running for 50 epochs)

  

· SGD: learning rate: {0.001, 0.001 10.0, 0.01, 0.01 10, 0.1, 0.1 10, 1, 10, 5, 10

, 10 10, 40, 60, 80, 100, 120, 140}.



· NAG/HB: learning rate:

{5.0, 10.0, 20.0, 10 10, 40, 60}, momentum

{0, 0.25, 0.5, 0.75, 0.9, 0.95}.

· ASGD: learning rate {40, 60}. For a long step of 100, swept over advantage parameters of {1.5, 2, 2.5, 5, 10, 20}. For a long step of 1000, we swept over advantage parameters of {2.5, 5, 10}.

C.3 DEEP RESIDUAL NETWORKS FOR CIFAR-10
In this section, we will provide more details on our experiments on cifar-10, as well as present some additional results. We used a weight decay of 0.0005 in all our experiments. The grid search parameters we used for various algorithms are as follows. Note that the ranges in which parameters such as learning rate need to be searched differ based on batch size (Jain et al., 2016). Furthermore, we tend to extrapolate the grid search whenever a parameter (except for the learning rate decay factor) at the edge of the grid has been chosen; this is done so that we always tend to lie in the interior of the grid that we have searched on. Note that for the purposes of the grid search, we choose a hold out set from the training data and add it in to the training data after the parameters are chosen, for the final run.
Batch Size 8: Note: (i) parameters chosen by running for 40 epochs and picking the grid search parameter that yields the smallest validation 0/1 error. (ii) The validation set decay scheme that we use is that if the validation error does not decay by at least 1% every three passes over the data, we cut the learning rate by a constant factor (which is grid searched as described below). The minimal learning rate to use is fixed to be 6.25 × 10-5, so that we do not decay far too many times and curtail progress prematurely.
· SGD: learning rate: {0.0033, 0.01, 0.033, 0.1, 0.33}, learning rate decay factor {5, 10}.
· NAG/HB: learning rate: {0.001, 0.0033, 0.01, 0.033}, momentum {0.8, 0.9, 0.95, 0.97}, learning rate decay factor {5, 10}.

16

Under review as a conference paper at ICLR 2018
· ASGD: learning rate {0.01, 0.0330, 0.1}, long step {1000, 10000, 50000}, advantage parameter {5, 10}, learning rate decay factor {5, 10}.
Batch Size 128: Note: (i) parameters chosen by running for 120 epochs and picking the grid search parameter that yields the smallest validation 0/1 error. (ii) The validation set decay scheme that we use is that if the validation error does not decay by at least 0.2% every four passes over the data, we cut the learning rate by a constant factor (which is grid searched as described below). The minimal learning rate to use is fixed to be 1 × 10-3, so that we do not decay far too many times and curtail progress prematurely.
 · SGD: learning rate: {0.01, 0.03, 0.09, 0.27, 0.81}, learning rate decay factor {2, 10, 5}. · NAG/HB: learning rate: {0.01, 0.03, 0.09, 0.27}, momentum {0.5, 0.8, 0.9, 0.95, 0.97},
learning rate decay factor {2, 10, 5}. · ASGD: learning rate {0.01, 0.03, 0.09, 0.27}, longstep {100, 1000, 10000}, advantage pa-
rameter {5, 10, 20}, learning rate decay factor {2, 10, 5}. As a final remark, for any comparison across algorithms, such as, (i) ASGD vs. NAG, (ii) ASGD vs HB, we fix the starting learning rate, learning rate decay factor and decay schedule chosen by the best grid search run of NAG/HB respectively and perform a grid search over the long step and advantage parameter of ASGD. In a similar manner, when we compare (iii) SGD vs NAG or, (iv) SGD vs. HB, we choose the learning rate, learning rate decay factor and decay schedule of SGD and simply sweep over the momentum parameter of NAG or HB and choose the momentum that offers the best validation error. We now present plots of training function value for different algorithms and batch sizes. Effect of minibatch sizes: Figure 8 plots training function value for batch sizes of 128 and 8 for SGD, HB and NAG. We notice that in the initial stages of training, NAG obtains substantial improvements compared to SGD and HB for batch size 128 but not for batch size 8. Towards the end of training however, NAG starts decreasing the training function value rapidly for both the batch sizes. The reason for this phenomenon is not clear. Note however, that at this point, the test error has already stabilized and the algorithms are just overfitting to the data.
Figure 8: Training loss for batch sizes 128 and 8 respectively for SGD, HB and NAG.
Comparison of ASGD with momentum methods: We now present the training error plots for ASGD compared to HB and NAG in Figures 9 and 10 respectively. As mentioned earlier, in order to see a clear trend, we constrain the learning rate and decay schedule of ASGD to be the same as that of HB and NAG respectively, which themselves were learned using grid search. We see similar trends as in the validation error plots from Figures 5 and 6. Please see the figures and their captions for more details.
17

Under review as a conference paper at ICLR 2018
Figure 9: Training function value for ASGD compared to HB for batch sizes 128 and 8 respectively.
Figure 10: Training function value for ASGD compared to NAG for batch sizes 128 and 8 respectively.
18

