Under review as a conference paper at ICLR 2018
DEMYSTIFYING MMD GANS
Anonymous authors Paper under double-blind review
ABSTRACT
We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.
1 INTRODUCTION
Generative Adversarial Networks (GANs; Goodfellow et al., 2014) provide a powerful method for general-purpose generative modeling of datasets. Given examples from some distribution, a GAN attempts to learn a generator function, which maps from some fixed noise distribution to samples that attempt to mimic a reference or target distribution. The generator is trained to trick a discriminator, or critic, which tries to distinguish between generated and target samples.
This alternative to standard maximum likelihood approaches for training generative models has brought about a rush of interest over the past several years. Likelihoods do not necessarily correspond well to sample quality (Theis et al., 2016), and GAN-type objectives focus much more on producing plausible samples, as illustrated particularly directly by Danihelka et al. (2017). This class of models has recently led to many impressive examples of image generation (e.g. Huang et al., 2017a;b; Jin et al., 2017; Zhu et al., 2017).
GANs are, however, notoriously tricky to train (Salimans et al., 2016). This might be understood in terms of the discriminator class: Goodfellow et al. (2014) showed that, when the discriminator is trained to optimality among a rich enough function class, the generator network attempts to minimize the Jensen-Shannon divergence between the generator and target distributions. This result has been extended to general f -divergences by Nowozin et al. (2016). According to Arjovsky & Bottou (2017), however, it is likely that both the GAN and model probability measures are supported on manifolds within a larger space, as occurs for the set of images in the space of possible pixel values. These manifolds might not intersect at all, or at best intersect on sets of measure zero. In this case, the Jensen-Shannon divergence is constant, and the KL and reverse-KL divergences are infinite, meaning they provide no useful gradient for the generator to follow. This helps to explain some of the instability of GAN training.
The lack of sensitivity to distance, meaning that nearby but non-overlapping regions of high probability mass are not considered similar, is a long-recognised problem for KL divergence-based discrepancy measures (e.g. Gneiting & Raftery, 2007, Section 4.2). It is natural to address this problem using Integral Probability Metrics (IPMs; Müller, 1997): these measure the distance between probaility measures via the largest discrepancy in expectation over a class of "well behaved" witness functions. Thus, IPMs are able to signal proximity in the probability mass of the generator and reference distributions.
Arjovsky et al. (2017) proposed to use the Wasserstein distance between distributions as the discriminator, which is an integral probability metric constructed from the witness class of 1-Lipschitz functions. To implement the Wasserstein critic, weight clipping of the distriminator network was
1

Under review as a conference paper at ICLR 2018
initially used to enforce k-Lipschitz smoothness of the discriminator. Gulrajani et al. (2017) improved on this result by directly constraining the gradient of the discriminator network at points between the generator and reference samples. This new Wasserstein GAN implementation, called WGAN-GP, is more stable and easier to train.
A second integral probability metric used in GANs is the maximum mean discrepancy (MMD), for which the witness function class is a unit ball in a reproducing kernel Hilbert space (RKHS). Generative adversarial models based on minimizing the MMD were first considered by Li et al. (2015) and Dziugaite et al. (2015). These works optimized a generator to minimize the MMD with a fixed kernel, either using a generic kernel on image pixels or by modeling autoencoder representations instead of images directly. Sutherland et al. (2017) instead minimized the statistical power of an MMD-based test with a fixed kernel. Such approaches struggle with complex natural images, where pixel distances are of little value, and fixed representations can easily be tricked, as in the adversarial examples of Szegedy et al. (2014).
Adversarial training of the MMD loss is thus the obvious choice. Here the kernel MMD is defined on the output of a convolutional network, which is trained adversarially. Recent notable work has made use of the IPM representation of the MMD to employ the same witness function regularisation strategies as Arjovsky et al. (2017); Gulrajani et al. (2017), effectively corresponding to an additional constraint on the MMD function class. Without such constraints, the convolutional features are unstable and difficult to train (Sutherland et al., 2017). Li et al. (2017b) used the weight clipping strategy of Arjovsky et al., with additional constraints to ensure that the kernel distribution embeddings remained injective. In light of the observations by Gulrajani et al., however, we use a gradient constraint on the MMD witness function in the present work (see Sections 2.1 and 2.2). Bellemare et al. (2017)'s method, the Cramér GAN, also used the gradient constraint strategy of Gulrajani et al. in their discriminator network. As we discuss in Section 2.3, the Cramér GAN discriminator is related to the energy distance, which is an instance of the MMD (Sejdinovic et al., 2013), and which can therefore use a gradient constraint on the witness function. Note, however, that there are important differences between the Cramér GAN critic and the energy distance, which make it more akin to the optimization of a scoring rule: we provide further details in Appendix A. Weight clipping and gradient constraints are not the only approaches possible: variance features (Mroueh et al., 2017) and constraints (Mroueh & Sercu, 2017) can work, as can other optimization strategies (Berthelot et al., 2017; Li et al., 2017a).
Given that both the Wasserstein distance and the MMD are integral probability metrics, it is of interest to consider how they differ when used in GAN training. Bellemare et al. (2017) pointed out a particular flaw of the Wasserstein distance as an objective: that gradients of both the duality-based estimator and of the alternative estimator of Genevay et al. (2017) are biased. Optimization over biased gradients can lead to incorrect parameter values, even in expectation; Bellemare et al. provide an explicit example where this occurs. As our main theoretical contribution, we show in Section 3 that the natural maximum mean discrepancy estimator has unbiased gradients, and therefore does not suffer from this problem.
There are additional empirical advantages of the MMD-GAN over the Wasserstein GAN. Certainly we would not expect the MMD on its own to perform well on raw image data, since these data lie on a low dimensional manifold embedded in a much higher dimensional pixel space. Once the images are mapped through appropriately trained convolutional layers, however, they can follow a much simpler distribution with broader support across the mapped domain: a phenomenon also observed in autoencoders (Bengio et al., 2013). In this setting, the MMD with characteristic kernels (Sriperumbudur et al., 2010) shows strong discriminative peformance between distributions. To achieve comparable performance, a purely convolutional WGAN requires many more layers in the critic. In our experiments (Section 5), we find that MMD GANs achieve the same generator performance as WGAN-GPs with smaller discriminator networks, resulting in GANs with fewer parameters and easier training. Thus, the MMD-GAN discriminator can be understood as a hybrid model that plays to the strengths of both the initial convolutional mappings and the kernel layer that sits on top.
2

Under review as a conference paper at ICLR 2018

2 LOSSES AND WITNESS FUNCTIONS

We begin with a review of the MMD and relate it to the loss functions used by other GAN variants. Through its interpretation as an integral probability metric, we show that the gradient penalty of Gulrajani et al. (2017) applies to the MMD GAN.

2.1 MAXIMUM MEAN DISCREPANCY AND WITNESS FUNCTIONS

We consider a random variable X with probability measure P , which we associate with the generator, and a second random variable Y with probability measure Q, which we associate with the reference sample that we wish to learn. Our goal is to measure the distance from P to Q using samples drawn independently from each distribution.

The maximum mean discrepancy is a metric on probability measures (Gretton et al., 2012), which
falls within the family of integral probability metrics (Müller, 1997); this family includes the Wasserstein and Kolmogorov metrics, but not for instance the KL or 2 divergences. Integral probability
metrics make use of a class of witness functions to distinguish between P and Q, choosing the
function with the largest discrepancy in expectation over P, Q,

DF (P, Q) = sup EP f (X) - EQ f (Y ).
f F

(1)

The particular witness function class F determines the probability metric: the Wasserstein-1 metric is defined using the 1-Lipschitz functions, and the Kolmogorov metric from the functions of bounded variation 1. The loss functions of many other GAN variants also fall into this class (e.g. Mroueh et al., 2017; Mroueh & Sercu, 2017; Berthelot et al., 2017).

In this work, our witness function class F will be the unit ball in a reproducing kernel Hilbert space H, with positive definite kernel k(x, x ). The key property of a reproducing kernel Hilbert space is the reproducing property: for all f  H, f (x) = f, k(x, ·) H. We define the mean embedding of the probability measure P as the element µP  H such that EP f (X) = f, µP H; it is given by µP = EXP k(·, X).1
The maximum mean discrepancy (MMD) is defined as (1) with F the unit ball in H:

MMD(P, Q; H) = sup EP f (X) - EQ f (Y ).
f H, f H1

The witness function f  that attains the supremum has a straightforward expression (Gretton et al.,

2012, Section 2.3):

f (x)  EP k(x, X) - EQ k(x, Y ).

(2)

Given samples X = {xi}mi=1 drawn i.i.d. from P , and Y = {yj}jn=1 drawn i.i.d. from Q, the empirical witness function is

f^(x)  1 m

m1 k(xi, x) - n

n

k(yi, x),

i=1 i=1

and an unbiased estimator of the squared MMD is (Gretton et al., 2012, Lemma 6)

(3)

1 mm

1n

m(m - 1)

k(xi, xj) + n(n - 1)

n

k(yi,

yj )

-

2 mn

m

n
k(xi, yj).

i=1 j=i

i=1 j=i

i=1 j=1

(4)

When the kernel is characteristic (Sriperumbudur et al., 2010; 2011), the embedding µP is injective (i.e., associated uniquely with P ). Perhaps the best-known characteristic kernel is the exponentiated
quadratic, also known as the Gaussian RBF, kernel,

krbf (x, y) = exp

1 - 22

x-y

2

.

(5)

1This is well defined for Bochner-integrable kernels (Steinwart & Christmann, 2008, Definition A.5.20), for which EP k(x, ·) H <  for the class of probability measures P being considered. For bounded kernels, the condition always holds, but for unbounded kernels, additional conditions on the moments might apply.

3

Under review as a conference paper at ICLR 2018

Both the kernel and its derivatives decay exponentially, however, causing significant problems in

high dimensions, and especially when used in gradient-based representation learning. The rational

quadratic kernel

krq (x, y) =

1+

x-y 2 2

-

(6)

with  > 0 corresponds to a scaled mixture of exponentiated quadratic kernels, with a Gamma(, 1) prior on the inverse lengthscale (Rasmussen & Williams, 2006, Section 4.2). This kernel will be the mainstay of our experiments, as its tail behaviour is much superior to that of the exponentiated quadratic kernel; it is also characteristic.

2.2 WITNESS FUNCTION AND GRADIENT PENALTIES
The MMD has been a popular choice for the role of a critic in a GAN. This idea was proposed simultaneously by Dziugaite et al. (2015) and Li et al. (2015), with numerous recent follow-up works (Sutherland et al., 2017; Liu, 2017; Li et al., 2017b; Bellemare et al., 2017). As a key strategy in these recent works, the MMD of (4) is not computed directly on the samples; rather, the samples first pass through a mapping function h, generally a convolutional network. Note that we can think of this either as the MMD with kernel k on features h(x), or simply as the MMD with kernel (x, y) = k(h(x), h(y)). The challenge is to learn the features h so as to maximize the MMD, without causing the critic to collapse to a trivial answer early in training.
Bearing in mind that the MMD is an integral probability metric, strategies developed for training the Wasserstein GAN critic can be directly adopted for training the MMD critic. Li et al. (2017b) employed the weight clipping approach of Arjovsky et al. (2017), though they motivated it from different considerations. Gulrajani et al. (2017) found a number of issues with weight clipping, however: it oversimplifies the loss functions given standard architectures, the gradient decays exponentially as we move up the network, and it seems to require the use of slower optimizers such as RMSProp rather than standard approaches such as Adam (Kingma & Ba, 2015).
It thus seems preferable to adopt Gulrajani et al.'s proposal of regularising the critic witness (3) by constraining its gradient norm to be nearly 1 along randomly chosen convex combinations of generator and reference points, xi + (1 - )yj for   Uniform(0, 1). This was motivated by the observation that the Wasserstein witness satisfies this property (their Lemma 1), but perhaps its main benefit is one of regularization: if the critic function becomes too flat anywhere between the samples, the generator cannot easily follow its gradient. We will thus follow this approach, as did Bellemare et al. (2017), whose model we describe next.

2.3 THE ENERGY DISTANCE AND ASSOCIATED MMD

Liu (2017) and Bellemare et al. (2017, Section 4) proposed to use the energy distance as the critic in an adversarial network. The energy distance (Székely & Rizzo, 2004; Lyons, 2013) is a measure of divergence between two probability measures, defined as

De(P,

Q)

=

-

1 2

EP

(X,

X

)

-

1 2

EQ

(Y,

Y

) + EP Q (X, Y ),

(7)

where EP (X, X ) is an expectation over two independent samples from the generator P (likewise,
Y and Y are independent samples from the reference Q), and (x, y) is a semimetric of negative type.2 We will focus on the case (xi, xj) = X - X  for 0 <   2. When  = 1,  is a metric.

It has been shown (Sejdinovic et al., 2013, Lemma 12) that the energy distance is an instance of

the maximum mean discrepancy, where the corresponding distance-induced kernel family for the

distance (7) is

kzd0ist (x, y)

=

1 2

[(x, z0)

+

(y, z0)

-

(x, y)]

(8)

for any choice of z0  X (often z0 = 0 is chosen to simplify notation, which corresponds to a fractional Brownian motion kernel; Sejdinovic et al., 2013, Example 15). Note that the resulting

2 must satisfy the properties of a metric besides the triangle inequality, and for all n  2, x1, . . . , xn  X ,

and a1, . . . , an  R with

i ai = 0,

n i=1

n j=1

aiaj (xi,

xj )



0.

4

Under review as a conference paper at ICLR 2018

kernel is not translation invariant. It is characteristic (when  < 2), and the MMD is well defined, for a class of distributions P that satisfy a moment condition (Sejdinovic et al., 2013, Remark 21), namely that there exists z0  X such that

(z, z0) dP (z) <  P  P.

To apply the regularization strategy of Gulrajani et al. (2017) in training the critic of an adversarial network, we need to compute the form taken by the witness function (2) given the kernel (8). Bearing in mind that
EP k(x, X) = (x, z0) + EP (X, z0) - EP (x, X),
where the second term of the above expression is constant, and substituting into (2), we have
f (x)  (x, z0) - EX (x, X) - d(x, z0) + EY (x, Y ) + C = EQ (x, Y ) - EP (x, X) + C.
This is in agreement with Bellemare et al.'s function f  (their page 5), albeit via a different argument (note that the function g in their footnote 4 differs from the above, and appears to be in error).

We now turn to the divergence implemented by the critic in Bellemare et al.'s Algorithm 1, which is somewhat different from the energy distance (7). The Cramér GAN witness function is defined as

fc(x) = EP (x, X) - (x, 0),

(9)

which is regularized using Gulrajani et al.'s gradient constraint. The expected surrogate loss associated with this witness function, and used for the Cramér critic, is

Dc(P, Q) = EP (X, X ) + EQ (Y, 0) - EP (X, 0) - EP,Q (X , Y ).

(10)

In brief, Y in (7) is replaced by the origin: it is explained that this is necessary for instance in the conditional case, where two independent reference samples Y, Y are not available. Following this change, it becomes straightforward to define P and Q which are different, yet have an expected Dc(P, Q) loss of zero. For example, if P is a point mass at the origin in R, and Q is a point mass a distance t from the origin, then P = Q, and yet Dc(P, Q) = 0 because

EP (X, X ) = EP (X, 0) = 0, EP,Q (X , Y ) = EQ (Y, 0) = t.

Nevertheless, good empirical performance has been obtained in practice for the Cramér critic, both by Bellemare et al. (2017) and in our Section 5 experiments. Insight into this behaviour can be obtained by considering its relation to the score function associated with the energy distance. We describe this relation in Appendix A.

3 UNBIASED GRADIENTS
One of the main criticisms of the Wasserstein GAN is that its gradient is biased (Bellemare et al., 2017, Section 3), which can cause problems when performing stochastic gradient descent. By contrast, Bellemare et al. claim that the energy distance used in the Cramér GAN critic does not suffer from biased gradients. They do not, however, properly prove this claim: the essential step in the reasoning ­ the exchange in the order of the expectations and the derivatives ­ is simply assumed.3
We prove here that under reasonable assumptions, the expectation and gradient can be interchanged for kernels on representations given by deep networks, and hence gradients of (4) are unbiased estimates of the gradient of the true MMD. Theorem 1. Let P, Q be distributions with moments of order   1 (see Assumption A), and let k be a continuously differentiable kernel subject to some growth conditions (Assumption B). Let (X) be a feedforward neural network with parameters  as in Definition 2; in particular, convolutional networks with ReLU-like activations are covered.
The function   EP,Q[k((X), (Y ))] is well-defined and differentiable for µ-almost all   Rm, where µ is the Lebesgue measure. Moreover, the following formula holds:
 EP,Q[k((X), (Y ))] = EP,Q[k((X), (Y ))].
3If f^() is an unbiased estimator of a function f (), so that E f^() = f (), then if we can exchange expectations and gradients, it is immediate that E f^() =  E f^() = f ().

5

Under review as a conference paper at ICLR 2018

The proof is given in Appendix B. The main idea is to first prove the result for differentiable activation functions, using the standard dominated convergence theorem. We then consider a sequence of differentiable activation functions that converge to the activation function of interest, which is differentiable except at the origin: for example the ReLU activation. The key point is to first show that the set of parameters under which the expected gradient is undefined has zero Lebesgue measure, regardless of the probability distributions P and Q. We then prove that the expected gradients using the differentiable activations converge locally uniformly towards the expected gradient obtained using the singular activation function. We then conclude using standard results of gradients of uniformly convergent sequences of functions.

4 EVALUATION METRICS

One problem with comparing GAN models is that quantitative comparisons are difficult. Some insight can be gained by visually examining samples, but we also consider the following approaches to evaluate GAN methods.

Inception score This metric, proposed by Salimans et al. (2016), is based on the classification output p(y | x) of the Inception model (Szegedy et al., 2016). Defined as exp (Ex KL(p(y | x) p(y))), it is highest when each image's predictive distribution has low entropy, but the marginal p(y) = Ex p(y | x) has high entropy. This score correlates somewhat with human judgement of sample quality on natural images, but it has some issues, particularly when applied to domains which do
not represent a variety of the types of classes in ImageNet. In particular, it knows nothing about the
desired distribution for the model.

FID The Fréchet Inception Distance, proposed by Heusel et al. (2017), avoids some of the problems of Inception by measuring the similarity of the samples' representations in the Inception architecture (at the pool3 layer) to those of samples from the target distribution. The FID fits a Gaussian distribution to the hidden activations for each distribution and then computes the Fréchet distance, also known as the Wasserstein-2 distance, between those Gaussians. Heusel et al. show that unlike the Inception score, the FID worsens monotonically as various types of artifacts are added to images. Note, however, that the estimator of FID is biased.4

KID We propose a metric similar to the FID, the Kernel Inception Distance, to be the squared

MMD between Inception representations. We use a polynomial kernel, k(x, y) =

1 d

xTy

+

1

3

where d is the representation dimension, to avoid correlations with the objective of MMD GANs

as well as to avoid tuning any kernel parameters.5 Compared to the FID, the KID has several

advantages. First, it does not assume a parametric form for the distribution of activations; this

is particularly sensible since the representations have ReLU activations, and therefore are not only

never negative, but do not even have a density: about 2% of components in Inception representations

are typically exactly zero. With the cubic kernel we use here, the KID compares skewness as well

as the mean and variance. Also, unlike the FID, the KID has a simple unbiased estimator.

Figure 1 demonstrates the bias of the FID and the unbiasedness of the KID by comparing the CIFAR10 train and test sets. The KID (Figure 1a) converges quickly to its presumed true value of 0; even for very small n, simple Monte Carlo estimates of the variance provide a reasonable measure of uncertainty. By contrast, the FID estimate (Figure 1b) does not behave so nicely: at n = 2 000, when the KID estimator is essentially always 0, the FID estimator is still quite large. Even at n = 10 000, the full size of the CIFAR test set, the FID still seems to be decreasing from its estimate of about 5.7 towards zero, showing the strong persistence of bias. More importantly, Monte Carlo estimates of the variance are extremely small even when the estimate is very far from its asymptote, so it is difficult to judge the reliability of an estimate.

For models on MNIST, we replace the Inception featurization with features from a LeNet-like convolutional classifier6 (LeCun et al., 1998), but otherwise compute the scores in the same way.

4This is easily seen when the true FID is 0: here the estimator may be positive, but can never be negative. 5k is the default polynomial kernel in scikit-learn (Pedregosa et al., 2011).
6 github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py

6

Under review as a conference paper at ICLR 2018

KID FID

60

0.004 50 40
0.002 30 0.000 20

0.002 10

0 250 500 750 10n00 1250 1500 1750 2000

0

2000 4000 n 6000 8000 10000

(a) KID estimate based on sampling both train and (b) FID estimate based on a with-replacement resamtest sets to size n without replacement, 100 times. pling of the test set to size n and the full train set. 10
samples; all standard deviations are less than 1.

Figure 1: Estimates of distances between the CIFAR-10 train and test sets. Lines show means, error

bars

standard

deviations,

dark

colored

regions

a

2 3

coverage

interval

of

the

samples,

light

colored

regions a 95% interval. Note the differing n axes.

We also considered the diagnostic test of Arora & Zhang (2017), which estimates the approximate number of "distinct" images produced by a GAN. The amount of subjectivity in what constitutes a duplicate image, however, makes it hard to reliably compare models based on this diagnostic. Comparisons probably need to be performed both with a certain notion of duplication in mind and by a user who does not know which models are being compared, to avoid subconscious biases; we leave further exploration of this intriguing procedure to future work.

5 EXPERIMENTS

We compare the quality of samples generated by MMD-GAN with various kernels with the ones obtained by WGAN-GP (Gulrajani et al., 2017) and Cramér GAN (Bellemare et al., 2017) on two standard benchmark datasets: the MNIST dataset of 28 × 28 handwritten digits7 and the CIFAR-10 dataset of 32 × 32 photos (Krizhevsky, 2009).
For all of our experiments we used the DCGAN architecture (Radford et al., 2016) for both generator and critic. For MMD losses, we used only 16 top-layer neurons in the discriminator; more did not seem to improve performance. For the generator we used the standard number of convolutional filters (64 in the second-to-last layer); for the critic, we compared networks with 16 and 64 filters in the first convolutional later.
Networks with smaller critic run considerably faster: on our systems, the 16-filter networks typically ran at about twice the speed of the 64-filter ones. Note that the critic size is far more important to training runtime than the generator size: we update the critic 5 times for each generator step, and moreover the critic network is run on two batches each time we use it, one from P and one from Q. Given the same architecture, all models considered here run at about the same speed.
We evaluate several MMD GAN kernel functions in our experiments. The simplest is the linear kernel: kdot (x, y) = x, y , whose MMD corresponds to the distance between means (this is somewhat similar to the feature matching idea of Salimans et al., 2016). We also use the exponentiated quadratic (5) and rational quadratic (6) functions, with mixtures of lengthscales,

krbf (x, y) = krbf (x, y),


krq (x, y) = krq (x, y),
A

where  = {2, 5, 10, 20, 40, 80}, A = {.2, .5, 1, 2, 5}. Lastly we use the distance-induced kernel k0dist of (8), using the Euclidean distance, so that the MMD is the energy distance, and the Cramér critic in (10).

7 yann.lecun.com/exdb/mnist/

7

Under review as a conference paper at ICLR 2018

Table 1: Mean (standard deviation) of score evaluations for the MNIST models.

Model rq rbf dot
Cramér GAN WGAN-GP
test set

Critic size 16 16 16 16 16 ­

Inception 9.11 (0.01) 8.98 (0.02) 8.86 (0.02) 9.25 (0.02) 9.12 (0.02) 9.78 (0.02)

FID 4.206 (0.05) 8.264 (0.02) 6.245 (0.06) 3.385 (0.10) 6.915 (0.10) 4.305 (0.16)

KID 0.005 (0.004) 0.011 (0.006) 0.006 (0.004) 0.006 (0.005) 0.009 (0.004) 0.003 (0.003)

9.5 Inception
9.0 8.5 8.0 7.5 0 10,000 20,000 30,000 40,000 50,000
generator iterations

MMD dot MMD mix rq MMD mix rbf Cramer GAN WGAN-GP

50 40 30 20 10 00

FID
10,000 20,000 30,000 40,000 50,000 generator iterations

0.12 0.10 0.08 0.06 0.04 0.02 0.00 0

KID
10,000 20,000 30,000 40,000 50,000 generator iterations

Figure 2: Score estimates over the learning process for MNIST training.

Each model was trained with a batch size of 128, and 5 discriminator updates per generator update. For CIFAR-10 we trained for 150 000 generator updates, while for MNIST we used 50 000.
MNIST After training for 50 000 generator iterations, all variants we tried achieved reasonable results. Table 1 shows the quantitative measures, computed on the basis of a LeNet model. All have achieved KIDs of essentially zero, and FIDs around the same as that of the test set, with Inception scores slightly lower. Figure 4, in Appendix C, shows samples.
Figure 2, however, shows the evolution of our quantitative criteria throughout the training process for several models. This shows that the linear kernel dot is clearly worse than the other models for most of the training process, but it makes a sharp improvement around 40 000 iterations and basically catches up to the other models.
There is also some evidence that rbf , and perhaps WGAN-GP, converge more slowly than rq and Cramér GAN. Examining samples during training, we observed that rbf more frequently produces extremely "blurry" outputs, which can persist for a substantial amount of time before eventually resolving. This makes sense, given the very fast gradient decay of the rbf kernel: when generator samples are extremely far away from the reference samples, slight improvements yield very little reward for the generator, and so bad samples can stay bad for a long time. Given their otherwise similar properties, we thus recommend the use of rq kernels over rbf in MMD GANs.
CIFAR-10 Scores for various models trained on CIFAR-10 are shown in Table 2. The scores for rq with a small critic network approximately match those of WGAN-GP with a large critic network, at substantially reduced computational cost. Here the distance kernel, with a large critic, seemed to do somewhat worse. With a small critic, WGAN-GP and Cramér GAN both performed very poorly.

These results illustrate the benefits of using the MMD on deep convolutional feaures as a GAN critic. In this hybrid system, the initial convolutional layers map the generator and reference image distributions to a simpler representation, which is well suited to comparison via the MMD. The MMD in turn employs an infinite dimensional feature space to compare the outputs of these convolutional layers. By comparison, WGAN-GP requires a larger discriminator network to achieve similar performance. It is interesting to consider the question of kernel choice: the distance kernel and RQ kernel are both characteristic (Sriperumbudur et al., 2010), and neither suffers from the fast decay of the exponentiated quadratic kernel, yet the RQ kernel performs slightly better in our experiments. The relative merits of different kernel families for GAN training will be an interesting topic for further study.
8

Under review as a conference paper at ICLR 2018

Table 2: Mean (standard deviation) of score evaluations for the CIFAR-10 models.

Model rq dist
Cramér GAN WGAN-GP WGAN-GP
test set

Critic size 16 64 16 16 64 ­

Inception 6.55 (0.02) 6.03 (0.01) 5.07 (0.01) 4.63 (0.01) 6.53 (0.02) 11.21 (0.13)

FID 39.17 (0.13) 46.53 (0.11) 69.11 (0.09) 88.11 (0.13) 37.52 (0.19)
6.11 (0.05)

KID 0.027 (0.001) 0.034 (0.001) 0.055 (0.002) 0.063 (0.001) 0.026 (0.001) 0.000 (0.000)

(a) MMD rq, critic size 16

(b) MMD dist, critic size 64

(c) Cramér GAN, critic size 16

(d) WGAN-GP, critic size 16

(e) WGAN-GP, critic size 64

(f) Test set

Figure 3: Comparison of samples from various models, as well as true samples from the test set. WGAN-GP samples with critic size 16 are quite bad, as well as being under-diverse. Cramér GAN samples with critic size 16 are more appealing to the eye, but seem to be under-diverse: note the several near-repeats in the 64 images shown here. Large-critic WGAN-GP and the two MMD GAN variants are of similar quality.

9

Under review as a conference paper at ICLR 2018
REFERENCES
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017. arXiv:1701.04862.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017. arXiv:1701.07875.
S. Arora and Y. Zhang. Do GANs actually learn the distribution? an empirical study, 2017. arXiv:1706.08224.
M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer, and R. Munos. The Cramer distance as a solution to biased Wasserstein gradients, 2017. arXiv:1705.10743.
Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai. Better mixing via deep representations. In ICML, 2013.
D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial networks, 2017. arXiv:1703.10717.
I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of maximum likelihood and GAN-based training of Real NVPs, 2017. arXiv:1705.05263.
G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015. arXiv:1505.03906.
A. Genevay, G. Peyré, and M. Cuturi. Learning generative models with Sinkhorn divergences, 2017. arXiv:1706.00292.
T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. JASA, 102 (477):359­378, 2007.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014. arXiv:1406.2661.
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. A kernel two-sample test. JMLR, 13, 2012.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein GANs. In NIPS, 2017. arXiv:1704.00028.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. In NIPS, 2017. arXiv:1706.08500.
R. Huang, S. Zhang, T. Li, and R. He. Beyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis, 2017a. arXiv:1704.04086.
X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie. Stacked generative adversarial networks. In CVPR, 2017b. arXiv:1612.04357.
Y. Jin, K. Zhang, M. Li, Y. Tian, H. Zhu, and Z. Fang. Towards the automatic anime characters creation with generative adversarial networks, 2017. arXiv:1708.05509.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. arXiv:1412.6980.
A. Krizhevsky. Learning multiple layers of features from tiny images, 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.
C. Li, D. Alvarez-Melis, K. Xu, S. Jegelka, and S. Sra. Distributional adversarial networks, 2017a. arXiv:1706.09549.
10

Under review as a conference paper at ICLR 2018
C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. MMD GAN: Towards deeper understanding of moment matching network. In NIPS, 2017b. arXiv:1705.08584.
Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In ICML, 2015. arXiv:1502.02761.
Lydia Tingruo Liu. On the two-sample statistic approach to generative adversarial networks. Master's thesis, University of Princeton Senior Thesis, April 2017. URL http://arks. princeton.edu/ark:/88435/dsp0179408079v.
R. Lyons. Distance covariance in metric spaces. The Annals of Probability, 41(5):3051­3696, 2013.
Y. Mroueh and T. Sercu. Fisher GAN. In NIPS, 2017. arXiv:1705.09675.
Y. Mroueh, T. Sercu, and V. Goel. McGan: Mean and covariance feature matching GAN. In ICML, 2017. arXiv:1702.08398.
A. Müller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429­443, 1997.
S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In NIPS 29, pp. 271­279. 2016.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:2825­2830, 2011.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. arXiv:1511.06434.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, Cambridge, MA, 2006.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In NIPS, 2016. arXiv:1606.03498.
D. Sejdinovic, B. K. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. The Annals of Stastistics, 41(5):2263­2291, 2013. arXiv:1207.6076.
B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Schölkopf. Hilbert space embeddings and metrics on probability measures. JMLR, 11:1517­1561, 2010. arXiv:0907.5309.
B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. JMLR, 12:2389­2410, 2011. arXiv:1003.0887.
I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer, 2008.
D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In International Conference on Learning Representations, 2017. arXiv:1611.04488.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2014. arXiv:1312.6199.
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer vision. In CVPR, 2016. arXiv:1512.00567.
G. Székely and M. Rizzo. Testing for equal distributions in high dimension. InterStat, 5, 2004.
L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In ICLR, 2016. arXiv:1511.01844.
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017. arXiv:1703.10593.
11

Under review as a conference paper at ICLR 2018

A SCORE FUNCTIONS, DIVERGENCES, AND THE CRAMÉR GAN

It is not immediately obvious how to interpret the surrogate loss (10). An insight comes from considering the score function associated with the energy distance, which we now briefly review (Gneiting & Raftery, 2007). A scoring rule is a function S(P, y), which is the loss incurred when a forecaster makes prediction P , and the event y is observed. The expected score is the expectation under Q of the score,
S(P, Q) := EY Q S(P, Y ).
If a score is proper, then the expected score obtained when P = Q is greater or equal than the expected score for P = Q,
S(Q, Q)  S(P, Q).
A strictly proper scoring rule shows an equality only when P and Q agree. We can define a divergence measure based on this score,

DS(P, Q) = S(Q, Q) - S(P, Q).

(11)

Bearing in mind the definition of the divergence (11), it is easy to see (Gneiting & Raftery, 2007, eq. 22) that the energy distance (7) arises from the score function

S(P, y)

=

1 2

EP

(X, X

)

-

EP

(X, y).

The interpretation is straightforward: the score of a reference sample y is determined by comparing its average distance to a generator sample with the average distance among independent generator samples, EP (X, X ).

As discussed earlier, the Cramér GAN critic does not use the energy distance (7) directly on the samples, but first maps the samples through a function h, for instance a convolutional network; this should be chosen to maximize the discriminative performance of the critic. Writing this mapping as h, we break the energy distance down as De(P, Q) = S(Q, Q) - S(P, Q), where

S(Q,

Q)

=

-

1 2

EQ

(h(Y

),

h(Y

))

(12)

and

1

S(P, Q) = 2 EP (h(X), h(X )) - EP Q (h(X), h(Y )).

(13)

When training the discriminator, the goal is to maximize the divergence by learning h, and so both

(12) and (13) change: in other words, divergence maximization is not possible without two indepen-

dent samples Y, Y from the reference distribution Q.

An alternative objective in light of the score interpretation, however, is to simply optimize the average score (13). In other words, we would find features h that make the average distance from
generator to reference samples much larger than the average distance between pairs of generator samples. We no longer control the term encoding the "variability" due to Q, EQ (h(Y ), h(Y )), which might therefore explode: for instance, h might cause h(Y ) to disperse broadly, and far from the support of P , assuming sufficient flexibility to keep EP (h(X), h(X )) under control. We can mitigate this by controlling the expected norm EQ (h(Y ), 0), which has the advantage of only requiring a single sample to compute. For example, we could maximize

-

1 2

EP

(h(X ),

h(X

))

+

EP Q

(h(X ),

h(Y

))

-

EQ

(h(Y

),

0).

This resembles the Cramér GAN critic (10), but the generator-to-generator distance is scaled differently, and there is an additional term: EP (h(X), 0) is being maximized, which is more difficult to interpret. An argument has been made (in personal communication with Bellemare et al.) that this last term is required if the function fc in (9) is to be a witness of an integral probability metric (1), although the asymmetry of this witness in P vs Q needs to be analyzed further.

B PROOF OF UNBIASED GRADIENTS
In this section we prove the interversion of the expectation and the gradient in Theorem 1. We first consider a class of "well-behaved" activation functions:

12

Under review as a conference paper at ICLR 2018

Definition 1. An activation function will be said to be well behaved if it satisfies the following:

· (0) = 0

·  is differentiable on R

·

The

derivatives

of



are

bounded

|

d dx

|



L

· There exist a family of activation functions (t)t>0 that are all differentiable on R and such that

· The family t converges uniformly towards  on R.

· The family of derivatives t converges locally uniformly towards  on R.

· t(0) converges towards a finite value c

Given such conditions, the family t can be extended at the value 0 by 0 = . By convention we denote 0(0) = c, although it is not the derivative of 0.
Definition 2. Given an input X in Rd, we define the feedforward neural network  with K hidden layers by the following:

h0 = X hk = (bk + gk(W k)hk-1)

k  {1, ..., K}

Where  is a well-behaved activation function as defined in Definition 1. bk is a vector in Rdk and W k is a matrix of weights. The functions gk are introduced to account for convolutions and are known linear transformations. the only requirement is that gk(W k) is a dk by dk-1 matrix
with (d0 = d). We will denote by  the concatenation of all the parameters of the network  = (bk, W k, )Kk=1 and we will denote the last layer hK by (X),   Rm. For simplicity we will drop the subscript for functions g.

We will first consider the case where  is differentiable at 0. The case where  is non differentiable will then be obtained by considering a parametric family of differentiable activation functions  such that  converges point-wise towards  when  goes to 0. We will then extend the results by taking the limit when  approches 0. We have the following results:
Theorem 2. If the activation function  is differentiable on R and has bounded derivatives, then   (X) is differentiable for all X in Rd. Moreover, there exist continuous real valued functions defined on Rm (), (), a() and b() such that:
(X)  b() + a() X (X)  () + () X
for all X in Rd.

Proof. First note that  is L-Lipchitz, therefore : (X)  L X for any finite dimensional vector, where  is applied point-wise and the norm used is the euclidian norm. Let now k be in {1, ..., K}, we have the following:

hk  (bk + g(W k)hk-1)|  L bk + g(W k)hk-1|  L bk + L g(W k) Op hk-1

A simple recursion then gives:

hk  X ak() + bk()

With ak() = Lk

k j=1

g(W j) Op and bk() = Lk

n i=1

bi

n j=i+1

g(W j ) Op

By setting b() = bL() and a() = aL() one gets the first bound.

13

Under review as a conference paper at ICLR 2018
For the gradient, we will proceed by recursion. For k = 0 we trivially have h0 = 0. Now for some k in {1, ...K}, we assume that hk-1  k-1() + k-1() X . For the parameters ¯ of layers higher than k we have ¯hk 2 = 0. Now for the parameters  of layers lower than k we get:
hk  L g(W k)hk-1  L g(W k) Op hk-1  L g(W k) Op hk-1
By recursion we get hk  L g(W k) (k-1() + k-1() X ) Now for the parameters at layer k k = (bk, W k) we also have:
k hk  L 1 + hk-1  L(1 + bk() + ak() X Using Minkowski inequality we finally get:
k hk  k X + k() with k = Lak() + L g(W k) k-1() and k = L(1 + bk) + L g(W k) k-1(). The results follows by taking k = K.
Let now P and Q be two probability distribution over the input space Rd and let X and Y be two random variables following P and Q respectively. We also consider a kernel K defined on the feature space. We make the following assumptions on P, Q and the kernel K for a given   1:
A EP[ X ] <  and EQ[ Y ] <  B The kernel is differentiable and satisfies the growth conditions:
K(U, V )  C( U  + V ) K(U, V )  C( U -1 + V -1) C The gradient of the kernel is M -Lipchitz for some positive M , i.e: K(X, Y ) - K(X , Y )  M (X, Y ) - (X , Y )
Here C is a positive constant. We have the following result: Theorem 3. If the activation function  is differentiable then   EP,Q[K((X), (Y ))] is welldefined and differentiable for all   Rm. Moreover, the following formula holds:
 EP,Q[K((X), (Y ))] = EP,Q[K((X), (Y ))]
Proof. The integrability of EP,Q[K((X), (Y ))] is straightforward using the bound on the network and on the kernel. For the differentiability, we will use the dominated convergence theorem . Indeed, by the chain rule one has:
K((X), (Y )) = K((X), (Y ))  ((X), (Y )) Taking the norm for each operator respectively, one gets:
K((X), (Y ))  K((X), (Y )) ((X), (Y ))  C( (X) -1 + (Y ) -1)( (X) + (Y ) ))
We will use now the bounds on (X) and (X). First recall that the functions , , a and b are continuous on Rm and considering a closed ball B(0, µ) for some 0 in Rm and µ > 0, each of those functions attain a maximum value on that ball. We simply denote by b, a,  and  these maximum values and use them directly in the bounds. We also introduce a threshold R > 0 on the norms X and Y . Let G(X, Y ) := K((X), (Y )) for convenience, we then get 4 different cases:
14

Under review as a conference paper at ICLR 2018

· If X  R and Y  R, then:

-1

G(X, Y ) F1(X, Y ) := 4 b + aR

 + R

· If X > R and Y > R, then:

G(X, Y ) F2(X, Y )

with

F2(X, Y ) :=

b + a -1 R

X -1 + Y -1 (2 + ( X + Y ))

· If X > R and Y  R, then:

G(X, Y ) F3(X, Y )

with F3(X, Y ) :=

b + a -1 X -1 + b + aR -1 (2 + ( X + R)) R

· If Y > R and X  R, then
G(X, Y ) F4(X, Y )
with F4(X, Y ) = F3(Y, X)

It is clear from Assumption A that the functions F1, F3 and F4 are integrable. F2 involves terms of the form X -1 Y which can be bounded in expectation using Holder inequality:

EP,Q[ X -1 Y ] 

EP[

X

]

1-

1 

1
EQ[ Y ] 

Therefore F2 is also integrable. Now denoting by:

F (X, Y ) :=F1(X, Y ) X R, Y R + F2(X, Y ) X >R, Y >R + F3(X, Y ) X >R, Y R + F4(X, Y ) X R, Y >R

we have that G(X, Y ) is upper bounded by F uniformly on the parameter ball B(0, ). The dominated convergence applies then and we get the interversion result for all 0 in Rm.

Now we would like to consider the case when the activation function  is not differentiable in 0. We will consider a family of activation functions t that are differentiable on R and that converges point-wise towards  as t goes to 0. To keep the proof simple, we consider the case of the RELU activation function (x) = max(0, x) and the following family of approximating functions:

t(x)

=

t(log(1

+

e

x t

)

-

log

2)

for t > 0. It is easy to see why these functions converge towards . Moreover, the derivative of t is given by:

dt(x) dx

=

1

-

1

1

+

e

x t

It

is

also

clear

that

dt (x) dx

converge

towards

d(x) dx

for

all

x



R.

For

x

=

0,

dt (x) dx



1 2

,

which

is

a

sub-gradient

of



in

x

=

0.

And

dt (x) dx

is

uniformly

bounded

in

x

and

t.

A key property of this family uniformly convergent on R.

is that it is uniformly convergent In other words if C is a compact

and the derivatives set included in R

dt (x) dx
then:

are

locally

dt - d dx dx

,C t0 0

15

Under review as a conference paper at ICLR 2018

where norm used is the supremum norm over the set C. We will extend the family to t = 0 by

setting 0 =  on R and

 and we also denote by has a defined value in 0.

0

=

limt0

t.

Note

that

0

coincides

with

the

derivative

of

Let's define the family of functions ft() := EP,Q[K((t)(X), (t)(Y ))] and gt() := EP,Q[K((t)(X), (t)(Y ))]. Here we denoted by (t)(X) the network with activation function t.

We have the following proposition:

Lemma 1. For a fixed , X and Y , then the following point-wise convergence hold:

K((t)(X), (t)(Y )) t0 K((0)(X), (0)(Y )) K((t)(X), (t)(Y )) t0 K((0)(X), (0)(Y )) Moreover, ft() t0 f0() and gt() t0 g0()

Proof. The point-wise convergence is obtained directly by taking the limit. The limits in expectation are obtained using the dominated convergence theorem and Assumptions A and B as well the growth properties of the network Theorem 2.

Now we would like to prove that f0() = g0() for µ-almost all . For this to hold we need to prove first that gt() converges locally uniformly towards g0() as t goes to 0. Let's first introduce some notations. For a given 0  Rd and  > 0 we define the set
N (0, ) = X  Rd   B(0, ), k  {1, ..., K}, i  {1, ..., dk} : bik + g(W k)ihk-1 = 0
N (0, ) is the set of X where at least one linear form in the network vanishes. This allows to introduce the set of "well-behaved" parameters:
P :=   Rm ( > 0)( > 0) : P(N (, )) 

Note that N (0, ) is always a closed set. We have the following uniform convergence result: Lemma 2. For all k  {0, ..., K}, the family of hidden layers (hkt )t0 obtained using the activations functions t converge uniformly on B(0, ) × Rd towards h0k for every 0  Rm and  > 0.

Proof. We proceed by recursion on the layers. For k = 0 the statement trivially holds. Assume now that for k - 1, (htk-1)t0 converges uniformly on a ball B(0, ) × Rd. We have the following:

hkt - h0k

= t(bk + g(W k)hkt -1) - 0(bk + g(W k)h0k-1)  t(bk + g(W k)hkt -1) - 0(bk + g(W k)htk-1)
+ 0(bk + g(W k)htk-1) - 0(bk + g(W k)hk0-1) .

For the first term we use the fact that t is uniformly convergent on R, for > 0 there exists  > such that for all t <  we have that t(bk + g(W k)hkt -1) - 0(bk + g(W k)hkt -1)  . For the second term we use the Lipchitz continuity of 0:

0(bk + g(W k)htk-1) - 0(bk + g(W k)hk0-1)  L g(W k)(htk-1 - h0k-1)  L g(W k) (hkt -1 - h0k-1) .

Since g(W k) is bounded on the ball B(0, ), one gets the desired result by recursion.

Lemma 3. Under assumption Assumption C, and for all   P  Q, gt() converges locally uniformly towards g0().

Proof. Let 0 be in P Q and let  > 0. For simplicity we drop the subscripts of the expectations. Using Assumption C one has:
gt() - g0()  E[ K((t)(X), (t)(Y )) - K((0)(X), (0)(Y )) ] E[ t(X), t(Y ) + E[ K((0)(X), (0)(Y )) ] E[ t(X) - 0(X), t(Y ) - 0(Y ) ]

16

Under review as a conference paper at ICLR 2018

All the expectations that do not involve a difference between two terms can be upper bounded

using Assumption B and the growth properties of the network in Theorem 2 over all parameters in

B(0, ). We will first control the difference E[ K((t)(X), (t)(Y )) - K((0)(X), (0)(Y )) ]. It can be uniformly controlled using the the uniform convergence of (t) towards 0 and continuity

of the gradient K. Let An be a partition of the set Rd × Rd, such that An are compact. The exist

N0 > 0 such that

 i=N0

+1

EP,Q

(An

)

<

On one hand we have that E[ K((t)(X), (t)(Y )) -

K((0)(X), (0)(Y )) ] is bounded uniformly on the ball B(0, ) by some constant S. This comes

by virtue of the growth properties of the network in Theorem 2 and of the gradient of the kernel in

Assumption B. It comes that:


E[ K((t)(X), (t)(Y )) - K((0)(X), (0)(Y )) |(X, Y )  An] EP,Q(An) 
n=N0 +1

On the other hand by uniform convergence of (t) towards (0)(), there exists  > 0 such that (t)(X) - (0)(X)   and (t)(X) - (0)(X)   for all (X, Y )  An for a given . Moreover, The image U of B(0, ) × An by (, X, Y )  ((t)(X), (t)(Y )) is included in a compact set C. By continuity of the gradient K, one deduces that it is uniformly continuous on
the compact set C, therefore provided that  is small enough, one can get that:

N0
E[ K((t)(X), (t)(Y )) - K((0)(X), (0)(Y )) |(X, Y )  An] EP,Q(An) 
n=1

Now we will control the error:
E[ t(X) - 0(X), t(Y ) - 0(Y ) ]
Here we will only provide a proof for the variable X, the same holds for Y . We proceed by recursion over the layers. Indeed for k = 0, hk = X and hk = 0 which define a constant family in t that is uniformly convergent. Assume now that for layer k - 1 for every > 0, there exists k-1 <  such that E[ htk-1 - hk0-1 ]  for all   B(, ). Note that hk-1 depends on  although it is not explicitly shown by the notation. We have the following decomposition:
E[ htk - hk0 ] = E[ t(bk + g(W k)htk-1) - 0(bk + g(W k)h0k-1) ] = E[ t(bk + g(W k)hkt -1) - 0(bk + g(W k)h0k-1) ] + E[ t(bk + g(W k)hkt -1)htk-1 - 0(bk + g(W k)hk0-1)hk0-1 ] + E[ t(bk + g(W k)hkt -1)g(W k)hkt -1 - 0(bk + g(W k)hk0-1)g(W k)hk0-1 ]

We get three terms, in the second and third terms one can add and subtract a control term that allows the use of the recursion assumption for htk-1 and hkt -1. These terms can therefore be bounded by
uniformly on the ball B(0, ) provided that t  k-1. The remaining terms are proportional to: Et := E[ t(bk + g(W k)hkt -1) - 0(bk + g(W k)hk0-1) ]
which can further be upper-bounded by: Et  E[ t(bk + g(W k)htk-1) - t(bk + g(W k)hk0-1) ] + E[ t(bk + g(W k)h0k-1) - 0(bk + g(W k)h0k-1) ]
We will control now the errors:
A1 := E[ t(bk + g(W k)hkt -1) - t(bk + g(W k)hk0-1) ] A2 := E[ t(bk + g(W k)h0k-1) - 0(bk + g(W k)hk0-1) ]

17

Under review as a conference paper at ICLR 2018

Let's consider the set N (0, ), Note that its complementary N c(0, ) is an open set. Let Dn be

a partition of this the parameters 

open set: N c(0, to a closed ball B¯

()0=, )tnh=e1nDB¯n(,w0,ea)s×sumDen

that is a

Dn are compact. compact set. We

If we restrict denote by V

the image of B¯(0, ) × Dn by the mapping R : (, X)  bk + g(W k)h0k-1. One can easily see

that there exists a ball B(0, r) centered on 0 with some radius r such that its intersection with V is

empty. This comes from the fact that V is continuous on the compact set B¯(0, ) × Dn. It follows

then from the uniform convergence of hkt -1 towards h0k-1 that there exists an 1 > 0 such that for

all t  1 Vt the image of B¯(0, ) × Dn by the application Rt : (, X)  bk + g(W k)hkt -1 has

an

empty

intersection

with

a

smaller

ball

B(0,

r 2

).

One

can

then

safely

use

the

fact

that

t

is

locally

Lipchitz

on

R

\

B(0,

r 2

)

with

some

constant

Lr .

Note that the Lipchitz constant Lr depends on r which in turn depends on the set Dn. Therefore, Lr

might diverge as Dn gets closer to the critical set N (0, ). To avoid this issue, we will make sure

to cut that

off
 N0

the contribution of sets +1 P(Dn)  . One can

Dn that are too small anyways. Indeed there exists therefore use the following decomposition for A1:

N0

>

0

such

A1 = E[ t(bk + g(W k)hkt -1) - t(bk + g(W k)hk0-1) XN ( ,) ]
N0
+ E[ t(bk + g(W k)hkt -1) - t(bk + g(W k)h0k-1) XDn ]
n=1 
+ E[ t(bk + g(W k)hkt -1) - t(bk + g(W k)hk0-1) XDn ]
n=N0 +1

The third term is bounded by 2L while the second term is uniformly controlled by C(

N0 n=1

LDn

)

.

Remains the first term. since 0 belongs to , provided that  is small enough then P(N (0, ))  ,

the result follows using the fact that t is bounded.

The term A2 can be controlled in exactly the same way:

A2 = E[ t(bk + g(W k)h0k-1) - 0(bk + g(W k)h0k-1) XN ( ,) ]
N0
+ E[ t(bk + g(W k)hk0-1) - 0(bk + g(W k)h0k-1) XDn ]
n=1 
+ E[ t(bk + g(W k)hk0-1) - 0(bk + g(W k)hk0-1) XDn ]
n=N0 +1

Here the first and third terms are controlled by the sizes of the sets Dn and N ( , ), while for the
second term one can simply use the locally uniform convergence of t towards 0 since the images of the compact sets B¯(0, ) × Dn by the mapping R : (, X)  bk + g(W k)h0k-1 are far away from 0 by at least a positive number r.

We have shown that for > 0 there exists  > 0 and  > 0 such that:

gt() - g0() 

for all   B(0, ) and all t  , which exactly means that gt converges locally uniformly to g0.

Lemma 4.

The

set

c
P

has

0

Lebesgue

measure.

Proof. The key idea here be more than a countable

is to prove number of

ntheaigt hfobrorasnoyfthatcPa,raenadlsaoloinngsPco.me

coordinates,

there

cannot

One should first notice that:

Pc =   Rm ( > 0)P(N (B(, 0))) >

18

Under review as a conference paper at ICLR 2018

Indeed, if   cP then there exists > 0 such that for all  > 0 we have: P(N (B(, ))) >

We also have that X,  X, from the continuity of the network. Therefore by the dominated convergence theorem one gets that P(N (B(, 0))) = lim0 P(N (B(, ))) > . The
converse is trivial.

Let 0 be in cP, then by definition there exists > 0 such that P(N (0, 0)) > . This also means that there is a layer k and a coordinate i in the layer k : i  {1, ..., dk} such that the set {X  Rd|bi + g(W k)ihk-1 = 0} has a non zero measure. Without loss of generality, one can assume that N (0, 0) = {X  Rd|bi + g(W k)ihk-1 = 0}.
Consider now the family of parameters (h)hR such that all the coordinates of h match those of 0 except at layer k and for the node i where bhi = bi + h. It is clear that the family of sets N (h, 0) are jointly disjoint. That is, if h, h  are such that h = h then N (h, 0)  N (h, 0) = . We will prove now that the intersection A between this family (h)0h and cP can be at most countable. Indeed if A was uncountable, then this would define an uncountable family of sets (N (, 0))A with positive measures. Let's now introduce the following family of sets:

An

=

{



A|P(N (, 0))

>

1} n

Since A is uncountable, then there exists n such that An is infinite. Now consider the set B obtained

by taking the union sequence in An then

of all P(B)

the 

disjoint sets N (, 0 with  

 m=1

P(N

(m,

0)



1 m=1 n

An and = +.

if

(m)m=1

is

an

injective

As this can't happen, it means that the intersection set A is at most countable.

So far we have shown that while keeping all the other parameters fixed for , only a countable possible values of the coordinate bki can be taken in cP. One can then easily conclude using Fubini theorem:

µ(Pc ) =

Rm

c d P

=

W 1,...,W K

(bjk

)
(k

,j)=(k,i)

bki

c
P

dbik

dbjk dW1...dWK = 0

1k K,1jd ,

k

The integral over bik is equal to zero which leads to the result.

We are now ready to prove the main result.
Proof of Theorem 1. By Theorem 3 we have that ft() = gt() for all t > 0 and   Rm. We also have by Lemma 1 that ft() converges point-wise towards f0(), and by Lemma 3, we know that gt() converges uniformly towards g0() for all  be in P  Q. This allows to conclude that f0() is differentiable on P  Q and its gradient is given by g0() = EP,Q[K((X), (Y ))]. Moreover, (P  Q)c = cP  cQ has zero Lebesgue measure as it its the union of two Lebesgue 0 measure sets by Lemma 4. The conclusion follows.
C MODEL SAMPLES

19

Under review as a conference paper at ICLR 2018

(a) MMD rq

(b) MMD rbf

(c) MMD dot

(d) Cramer GAN

(e) WGAN-GP

(f) MNIST test set

Figure 4: Samples from the models listed in Table 1. Rational-quadratic and Gaussian kernels obtain retain sample quality despite reduced discriminator complexity. Each of these models generates good quality samples with the standard DCGAN discriminator (critic size 64).

20

