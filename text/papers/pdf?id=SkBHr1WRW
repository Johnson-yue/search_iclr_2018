Under review as a conference paper at ICLR 2018
EGO-CNN: AN EGO NETWORK-BASED REPRESENTATION OF GRAPHS DETECTING CRITICAL STRUCTURES
Anonymous authors Paper under double-blind review
ABSTRACT
While existing graph embedding models can generate useful embedding vectors for graph-related tasks, what valuable information can be jointly learned from a graph embedding model is less discussed. In this paper, we consider the possibility of detecting critical structures by a graph embedding model. We propose Ego-CNN to embed graphs, which works in a local-to-global manner to take advantages of CNNs that gradually expands the detectable local regions on the graph as the network depth increases. Critical structures can be detected if Ego-CNN is combined with a supervised task model. We show that Ego-CNN is (1) competitive to state-of-the-art graph embeddings models, (2) can nicely work with CNNs visualization techniques to show the detected structures, and (3) is efficient and can incorporate with scale-free priors, which commonly occurs in social network datasets, to further improve the training efficiency.
1 INTRODUCTION
A graph embedding algorithm converts graphs from structural representation to fixed-dimensional vectors. It is typically trained in a unsupervised manner for general learning tasks but recently, deep learning approaches such as Structure2vec (Dai et al. (2016)) and Diffusion Convolution Neural Network (Atwood & Towsley (2016)) are trained in a supervised manner and shown superior results against unsupervised approaches on many tasks such as node classification and graph classification. While many algorithms perform well on graph-related tasks, what valuable information can be jointly learned from the graph embedding is less discussed. In this paper, we aim to develop a graph embedding model that jointly discovers the critical structures, i.e., partial graphs that are dominant to a learning task (e.g., graph classification) where the embedding is applied to. This helps people running the learning task to understand the reason behind the task results, and is particularly useful in certain domains such as the bioinformatics and social network analysis. For example, people in bioinformatics may wish to tell whether a protein is an enzyme or not and why , as shown in Figure 1(a). If a model can backtrack the most critical parts like the ones shown in Figure 1(b), it may help people understand that it is the circular structures (the "holes") that distinguish enzyme from other substances. New knowledge may then be discovered by investigating the functionality of these holes.
(a) (b) Figure 1: (a) A common task in bioinformatics whose goal is to tell whether a protein is of a specified class (enzyme in this case) or not. (b) Critical structure dominating the learning task. Unfortunately, identifying critical structures is a challenging task. The first challenge is that critical structures are task-specific--the shape and location of critical structures may vary from task to task. This means that the graph embedding model needs to be learned together with the task model (e.g.,
1

Under review as a conference paper at ICLR 2018

Alkane ...

Alcohol ...

Alkane ...

Alcohol ...

2methylnonane

5methylnoane

(asymmetric isomer) (symmetric isomer)

... ... ...

2methylnonane (a) 5methylnoane (asymmetric isomer) (symmetric isomer)

(b)

Figure 2: (a) OH function group is the critical structure to tell Alcohols from Alkanes. (b) The sym-

metry hydrocarbon group (at two sides of the methyl branch) is the critical structure to discriminate

between symm.e..tric and asymmet.r.i.c isom...er of methylnonane.

a classifier or regressor). Another challenge is that critical structures may be found at the globalscale, as shown in Figure 2. To discriminant Alcohols from Alkanes (Figure 2(a)), one can check if there exists an OH-base and if the OH-base is at the end of the compound. In this task, the critical structure (OH-base) is relative small and can be identified at the local-scale. But for the other task (Figure 2(b)), to identify if a methyl-nonane is symmetric or not, one must check the entire graph to know if the methyl is branched at the center position of the long carbon chain. In this task, the critical structure is the symmetric hydrocarbon at the two sides of the methyl branch, which can only be found at the global-scale. The second challenge is that finding out all matches of substructures in a graph is known as subgraph isomorphism and proven to be an NP-complete problem by Cook (1971). To the best of our knowledge, there is no existing graph embedding algorithm that can identify critical structures up to the global-scale in an efficient way. Most existing approaches has only limited ability in identifying critical structures. For example, Structure2vec (Dai et al. (2016)) and Spectrum GCN (Defferrard et al. (2016)) can only find critical structures of very simple shape. Patchy-San (Niepert et al. (2016)) can only find critical structures within a small area around each node (i.e., at the local-scale). The only work that is able to identify critical structures at the globalscale is Spatial GCN (Bruna et al. (2013)), but it has the complexity O(N 2), where N is the number of nodes, in both time and space, therefore is inefficient for large graphs. In this paper, we propose the Ego-CNN that embeds a graph into distributed (multi-layer), fixeddimensional tensors. The Ego-CNN can be jointly learned with a supervised task model (e.g., fully-connected layers for graph classification) and help identify the task-specific critical structure at the global-scale. Some existing studies like DCNN (Atwood & Towsley (2016)) and Patchy-San (Niepert et al. (2016)) have borrowed the concept of CNN to embed graphs. The idea is to model the filters/kernels that scan through different parts of the graph (which we call the neighborhoods) to learn patterns most helpful to the learning task. Unlike images where a neighborhood can be clearly defined as the nearby pixels across channels, graphs have no standard notion of neighborhoods. Currently, the neighborhoods defined by above works have some limitations in that (1) a neighborhood does not represent a local region of the graph so the embedding model cannot take advantage of the location invariant patterns to speed up the learning process, or (2) a neighborhood represents a local region of the graph but cannot be recursively defined over the hidden layers, making the embedding model shallow and incapable of detecting critical structure at the global-scale. Different from most existing approaches, Ego-CNN defines a neighborhood at layer l as the (l + 1)-hop ego network centered at a specific node.1 This makes the Ego-CNN efficient since it learns the location invariant patterns. Also, a neighborhood at layer l enlarges the receptive field of the neighborhood centered at the same node at layer l 1. Ego-CNN can exploit the multi-layer architecture to detect the critical structure at the global-scale in the deeper layers. Furthermore, Ego-CNN works nicely with some common visualization techniques for CNNs (e.g., deconvolution proposed by Zeiler et al. (2011)) and can output the critical structures behind each prediction made by the trained task model. Our contributions are
· We propose the Ego-CNN and show that an graph embedding based on ego networks can perform as well as (if not better than) existing approaches. We conduct experiments on
1In a graph, an l-hop ego network centered at node i is a subgraph consisting of the node i and all its l-hop neighbors as well as the edges between these nodes.
2

Under review as a conference paper at ICLR 2018

many graph classification tasks and the results show that Ego-CNN can lead to the stateof-the-art performance. · By letting a neighborhood (an ego network) at a deeper layer share the same centering node with a neighborhood at the preceding layer (which we call the egocentric design), EgoCNN supports a multi-layer architecture with enlarged receptive fields at deeper layers, thus can learn the critical structures at the global-scale. · Ego-CNN, which learns location invariant patterns recursively, is efficient. Each convolution takes only O(N kd) in both space and time for a filter to scan a graph with N nodes. We also show that Ego-CNN can readily incorporate the scale-free prior, which commonly exists in large (social) graphs, to further improve the training efficiency.
To the best of our knowledge, Ego-CNN is the first embedding model that can efficiently detect task-dependent critical structures at the global-scale. We hope it helps practitioners better explain the learning results and discover new knowledge in the future.

2 RELATED WORK

We compare existing graph embedding models in Table 1 on the aspects of our goal, i.e. whether it jointly detects the global-scale critical structure and its computational complexity.

Graph embedding model
WL kernel (Shervashidze et al. (2011)) DGK (Yanardag & Vishwanathan (2015)) Subgraph2vec (Narayanan et al. (2016))
MLG (Kondor & Pan (2016)) Structure2vec (Dai et al. (2016)) Spatial GCN (Bruna et al. (2013)) Spectrum GCN (Bruna et al. (2013); Defferrard et al. (2016); Kipf & Welling (2017)) DCNN (Atwood & Towsley (2016)) Patchy-San (Niepert et al. (2016)) Neural Fingerprint (Duvenaud et al. (2015))
Ego-Convolution

Can identify critical structures (upto the
global-scale)?

Efficient on large graph?

Time complexity to embed a graph G = (V, E)

O(knL|V |) O(kdn|V |) O(kdn|V |) O(L|V |5) O(kd|V |) O(dL|V |3) O(dnf |E|)
O(hnf |V |2) O(k2nf |V |) O(knf |V |)
O(kdnf L|V |)

Table 1: A comparison of graph embedding models. Let d be the embedding dimension, k be maximum node degree in G. Other variables such as L, nf , n are model dependent hyperparameters.

Graph Kernels All graph kernels have a common drawback in that the embeddings are generated in a unsupervised manner, which means critical structure cannot be jointly detected at the generation of embeddings. Here we introduce three state-of-the-art graph kernels. The Weisfeiler-Lehman kernel (Shervashidze et al. (2011)) grows the coverage of each node by collecting information from neighbors, which is conceptually similar to our method, but differs in that WL kernel collects only node labels, while our method collects the complete labeled neighborhood graphs from neighbors. Deep Graph Kernels (Yanardag & Vishwanathan (2015)) and Subgraph2vec (Narayanan et al. (2016)) were inspired by word2vec (Mikolov et al. (2013)) models that embed structures by predicting neighbors' structures. Multiscale Laplacian Graph Kernels (Kondor & Pan (2016)) can compare graphs at multiple scales by recursively comparing graphs based on the comparison of subgraphs. However, it's very inefficient in that comparing two graphs takes O(L|V |5), where L represents the number of comparing scales. Graphical Models Graph can also express the conditional dependence (edge) between random variables (node) in graphical models. models. Structure2vec (Dai et al. (2016)) introduced a novel layer that makes the optimization procedures of approximation inference directly trainable by SGD. It's efficient on large graphs with time complexity linear to number of nodes. However, it's weak at identifying critical structures since approximation inference makes too much simplification on structures. For example, mean-field approximation assumes variables are independent of each other. As a result, Structure2vec can only identify critical structures of very simple shape. Convolution-based Methods Recently, many works are proposed by borrowing the concept of CNN to embed graphs. Figure 3 summarizes their definition of filters and neighborhoods.

3

Under review as a conference paper at ICLR 2018

W
(a)

scan  by filters
(b)

scan  by filters
(c)

scan  by filters
(d)

scan  by filters
(e)

Figure 3: Filters and neighborhood in (a) Spatial GCN by Bruna et al. (2013). The filter W is a sparse matrix, which is not aimed to scan for detecting local patterns, but to learn connectivity of clusters based on cluster features X. (b) DCNN by Atwood & Towsley (2016) that scans through the diffusion matrix of each node. (c) Patchy-San by Niepert et al. (2016) that scans adjacency matrix of local neighborhood of each node. (d) Neural Fingerprints by Duvenaud et al. (2015) that scans approximated neighborhood (specially, the summation of neighbors' embeddings) of each node. (e) Ego-Convolution scans egocentric designed neighborhoods of each node.

The Spatial Graph Convolutional Network (GCN) was proposed by Bruna et al. (2013). The design of Spatial GCN (Figure 3(a)) is very different from other convolution-based methods since its goal is to perform hierarchical clustering of nodes. A neighborhood is defined as a cluster. However, the filter is not aim to scan for local patterns but to learn the connectivity of all clusters. This means each filter is of size O(N 2) if there're N clusters. Also, a filter requires the global-scale information, i.e. the features of all clusters to train, so it's very inefficient on large graphs. Hence, in the same paper, Bruna et al. (2013) proposed another version, the Spectrum GCN, to perform hierarchical clustering in the spectrum domain. Although the efficiency is improved and many recent work such as Defferrard et al. (2016); Kipf & Welling (2017) further improve the training efficiency, the major drawback is that spectrums is weak at identifying structures since only special families of graphs such as complete graphs or star-like trees can be precisely described by spectrums. Diffusion Convolutional Neural Networks (Atwood & Towsley (2016)) embed graphs by detecting the diffusion. The neighborhood is defined as the diffusion, which are paths starting from a node to other nodes in h hops. The diffusion can be represented by a diffusion matrix.2 They use filters to scan through each node's diffusion matrix, so DCNN is possible to detect useful diffusion patterns. But it cannot detect critical structures since the diffusion patterns cannot precisely describe the location and the shape of structures. DCNN reported impressive results on node classification. But it's inefficient on graph classification tasks since their notion of neighborhood is at the global-scale, which takes O(hN 2) to embed a graph with N nodes. Also, their definition of neighborhood makes the embedding model shallow, which is only a single layer in the neural network. Patchy-San (Niepert et al. (2016)) detects local neighborhoods by filters. The definition of local neighborhood is based on the adjacency matrix of the graph, which is defined as the local graphs formed by its k nearest neighbors. The filter scans on the adjacency matrix of local neighborhoods. Since expressing graphs by adjacency matrices is not invariant under different vertex ordering, they proposed neighborhood normalization to generate similar adjacency matrices for isomorphic neighborhoods. Patchy-San can detect precise structures but it is inefficient to scan for large local neighborhood, which limits it from detecting global-scale critical structures. Also, the embedding model of Patchy-San is only a single layer, a question to ask is that is it possible to improve the efficiency by generalizing its definition of neighborhood into multiple layers? If the adjacency matrix of neighborhoods is generated (e.g. the similarity of all embedding vectors of all neighborhoods), the neighborhood at next layer can be defined as the k most similar neighborhoods. However, this naive generalization don't work for two reasons. First, the concept of local neighborhood is lost since two neighborhoods that have similar embedding vectors are not necessary to be adjacent on the graph. Second, the structure of neighborhood is dynamic since the k most similar neighborhoods changes during training. Therefore, the idea of Patchy-San cannot be readily extended to multiple layers. To the best of our knowledge, the only work that scans on local neighborhoods and supports multiple layers is Neural Fingerprints (Duvenaud et al. (2015)). But the neighborhood at layer l represents
2A diffusion matrix D is of size h  N for a graph of N nodes and each element Dij represents if this node connects to node vj in i hops.
4

Under review as a conference paper at ICLR 2018

only the approximation (specially, the summation) of the l-hop neighbors. They use the approximation to avoid the vertex-ordering problem of adjacency matrix. However, at the same time, Neural Fingerprint loses the ability of detecting precise critical structures. We know that CNN has two advantages: (1) detecting location invariant patterns and (2) overcoming the curse of dimensionality by multi-layer representations. To keep these advantages on graphs, we have to rethink the definition of neighborhood from scratch.

3 EGO-CNN

The reason why the idea of Patchy-San fails to generalize into multiple layers is that its definition of neighborhood, which is based on adjacency matrix, is not static and may not corresponding to local regions in the graph. Our main idea is to use the egocentric design, i.e. the neighborhood at next layer is defined on the same node. The overview of our method is depicted in Figure 4.

neighborhood  embeddings

EgoCNN

Task Model  

lookup neighboring neighborhoods

scan by   filters

...

lookup neighboring neighborhoods

scan by   filters

...

...

st EgoConvolution Layer

th EgoConvolution Layer

Figure 4: The model architecture of Ego-CNN. Our egocentric design defines neighborhood on the same node across all Ego-Convolution layers, which is shown in dashed line representing neighborhoods of the same node at different layers.

Given a graph G of N nodes, (1) To prepare the input X 2 RNd for Ego-CNN, we have to select the fixed-size neighborhood and k neighbors for each node in the graph, and convert neighborhoods into d-dimensional vectors. Any existing graph embedding models can be used, but we recommend to use Patchy-San since our Ego-CNN can be directly stacked on top of it. (2) The Ego-CNN is composed of several Ego-Convolution layers. To enlarge neighborhoods layerby-layer, the trick is to define neighborhood on the same node across all layers, which we called the egocentric design. As shown in Figure 5, we define the neighborhood of a node at layer l as the node and its k neighbors' neighborhoods at layer l 1 (which we abbreviated as the neighboring neighborhoods). In this way, the definition of neighborhoods is static and results in larger neighborhoods at next level.

Figure 5: Our definition of neighborhood is egocentric to a node (in bold black), which includes the node itself and its neighboring neighborhoods (where the center of neighboring neighborhoods are colored in black). The idea of egocentric design is implemented in Ego-Convolution, whose procedure is listed in Algorithm 1.
5

Under review as a conference paper at ICLR 2018

Algorithm 1 l-th Ego-Convolution layer Require:
Hl 1 2 RNdl 1 : embeddings of neighborhoods generated by (l-1)-th layer; N br: k neighbors for each node;

Z1, · · · , Zk lookupNeighboringNeighborhood(Hl 1, N br); Z 2 RNkdl 1 concatNeighboringNeighborhood(Z1, · · · , Zk); Z normalizeNeighboringNeighborhood(Z);

Z 2 RN(k+1)dl 1 concatSelfEmbedding(Z, Hl 1);

Hl 2 RNdl convolution(Z);

. Scan by dl filters of size (k + 1)  dl 1;

return Hl;

The l-th Ego-Convolution layer takes Hl 1 2 RNdl 1 , i.e. embeddings of neighborhoods generated by layer l 1, as input. To generate embeddings of neighborhood Hl 2 RNdl . for each node i, we lookup its k neighboring neighborhoods and concate together with its neighborhood embedding to form into a (k + 1)  dl 1 matrix. The goal of concating each node's neighborhood embedding is to let next level neighborhood know the structure between its k neighbors. However, a problem is that it is possible for two isomorphic neighborhoods to have different concatenated matrix if the embeddings of k neighborhood are put in non-consistent order. This problem is similar to the vertex reordering problem encounter by Niepert et al. (2016) and can be solved by their neighborhood normalization procedure, whose idea is simply to assigned a global ordering of all nodes and then neighbors are sorted in non-increasing/decreasing global order. (3) The graph embedding HL 2 RNdL generated by Ego-CNN can be used by task models such as SVM or classification layers to trained for a specific learning task.

(a) (b) (c) (d) (e) (f) Figure 6: A receptive field of Ego-CNN effective enlarges the coverage on graph as depth increases. (a) Suppose input to Ego-CNN represents 1-hop ego networks of at most k neighbors. (b)-(e) the coverage of receptive fields of the same node on the graph at 1st to 4-th Ego-Convolution layer, which are the neighborhoods (colored in grey) centering at the same node (colored in black) at different scales. (f) the coverage of receptive field by another node at 4th layer only covers local regions of graph. The different ambient coverage of graph reflects the position of the center node. Effective Receptive Field on Ambient Graph A receptive field in Ego-CNN has the ambient coverage on graphs. The coverage of a receptive field is enlarged as network depth increases (Figure 4.2). Suppose the input (i.e. embeddings of neighborhoods) to the Ego-CNN represents 1-hop ego networks (Figure 4.2(a)) and k neighbors of each node are selected from 1-hop neighbors. The neighborhood defined at 1st Ego-Convolution layer includes k 1-hop ego networks of its 1-hop neighbors, which approximately represents a 2-hop ego network (Figure 4.2(b)). It is an approximation of 2-hop ego network since edges across two neighbor's ego networks are not captured by receptive fields. As the depth of Ego-CNN increases, a receptive field of the l-th Ego-Convolution layer approximates covers a (l + 1)-hop ego network. As shown in Figure 4.2(e), it's possible to cover the entire graph by a single receptive field of Ego-CNN. Therefore, Ego-CNN can detect global-scale structures and can find critical structures by combining with a task model (e.g. the classification layers) to trained for learning task. Also, the different coverage of receptive fields reflects the position of nodes (see Figure 4.2(e)-(f)). While the receptive field shown in Figure 4.2(e) covers entire graph, the receptive field in Figure 4.2(f) only covers part of the graph. This difference is related to the global position of the two node. We think it's possible for Ego-CNN to be aware of position of nodes by comparing receptive fields and take advantages of it.
6

Under review as a conference paper at ICLR 2018

4 ADVANTAGES OF EGO-CNN

4.1 DETECTING CRITICAL STRUCTURES
Given a trained Ego-CNN, there are many ways to visualize critical structures. Here we consider Ego-CNN is used with the general task model, which can be several neural network layers or can be an additional classifier such as SVM. Our approach takes in two steps.
1. Find out the most important neighborhoods. The simplest way is to add an Attention layer (Itti et al. (1998)) to generate a score representing the importance of each neighborhood. To train the weights of the Attention layer, we fix weights of previous layers (i.e. Patchy-San and Ego-Convolution) and add a Dense layer to train for the task. We can pickup important neighborhoods by setting a threshold.
2. Identify critical regions in those important neighborhoods. Since the critical parts is considered in the network's perspective, we take the approach similar to Deconvolution (Zeiler et al. (2011)), i.e. we interpolate outputs with filters to reconstruct the neighborhoods (represented by receptive fields) in a layer-by-layer fashion, from the deepest Ego-Convolution to 1st Ego-Convolution to Patchy-San layer, where the adjacency matrix of neighborhoods are interpolated. The interpolated neighborhood graphs (in adjacency matrix) are critical structures to the task, and the edge weight in adjacency matrix corresponds to the importance of an edge.

4.2 EFFICIENCY AND THE SCALE-FREE PRIOR

Computation Complexity Given a graph with N nodes. To generate neighborhood embed-

dings at the l-th Ego-Convolution. It takes O(N ) to lookup embeddings of k neighboring

neighborhoods and O(N k log k) to concat neighborhoods according to neighbors' global order,

and O (N (k + 1)dl 1dl) to perform convolution through each receptive fields (of size (k +

1) O

 N

dl (k

+1)1)byPdLl=l 1fidllter1sd. l

In to

total, for embed the

a Ego-CNN graph.

with

L

Ego-Convolution

layers,

it

takes

Scale-Free Regularizer The scale-free property commonly exists in real social networks. The most notable property of scale-free networks is that the degree distribution follows a power-law.

Figure 7: The degree distribution of Reddit dataset follows a power-law in log-log plot. Figure 7 shows the degree distribution of the Reddit dataset, where we use it to benchmark model performance on classification tasks. We found degree distribution of graphs in Reddit datasets follows a power-law. This motivates us to think of the possibility of introducing scale-free priors into Ego-CNN. Recall that filters of the l-th Ego-Convolution layer is detecting the combinations of how neighborhoods at the (l 1)-th layer are combined into neighborhoods at the l-th layer. In fact, one way to generate a scale-free network is to repeat the combination at each layer, which means we can simply introduce scale-free prior into the Ego-CNN by weight-tying, i.e. let the filters share across different Ego-Convolution layers.
7

Under review as a conference paper at ICLR 2018

5 EXPERIMENTS
We perform a set of experiments to verify (i) the performance compared to existing graph embedding models on classification datasets, (ii) the effectiveness of scale-free regularizer, and (iii) visualization of identified critical structures. All experiments were performed on a computer with 48-core Intel(R) Xeon(R) E5-2690 CPU, 64 GB RAM, and NVidia Geforce GTX 1070 GPU. We use Tensorflow 1.0 to implement our methods.

5.1 GRAPH CLASSIFICATION We benchmark on both bioinformatic and social-network datasets, which are processed by Kersting et al. (2016). Most graphs in bioinformatic datasets are labeled graphs that contain node and edge labels or attributes, while graphs are unlabeled (i.e. only structure of graph is given) in social network datasets. More details about these datasets can be found in DGK (Yanardag & Vishwanathan (2015)). The average test accuracy of 10-fold CV is reported to compared with existing methods. The results is listed in Table 2, where figures of most baselines are directly taken from their papers since experimental settings are the same. For simplicity, we use the same network architecture for all datasets. To embed neighborhood graphs, we choose Patchy-San since it deals with labeled graphs and works nicely with CNN visualization techniques. The architecture of network is composed of 1 Patchy-San layer (128 filter and k=10, which is the best setting for k reported in their paper), then follows our Ego-CNN with 5 Ego-Convolution layers (128 filters and k=16), and 2 Dense layers with 128 neurons as the task model. To normalize neighbors, we follow Patchy-San's approach to use 1-WL labels. We train the network by Adam with learning rate 0.0001 and use Dropout with droprate 0.5 and Batch Normalization for regularization.

Dataset

MUTAG PTC PROTEINS NCI1 DD

Size Max # node
# class

188 344 28 64 22

1113 620 2

4110 1178 125 5748 22

WL kernel

82.1 57.0

73.6

82.2 78.0

DGK

82.9 59.2

73.3

80.3

Subgraph2vec 87.2 60.1

MLG

84.2 63.6

Structure2vec 88.3

DCNN

67.0 56.6

73.4 76.1

80.3 80.8 83.7 82.2 62.6

Patchy-San Ego-CNN

92.6 60.0 93.1 63.8

75.9 73.8

78.6 78.1 80.7 75.6

Table 2: 10-Fold CV test accuracy (%) on bioinformatic datasets.

Dataset Size
Max # node # class DGK
Patchy-San Ego-CNN

IMDB (B) 1000 270 2 67.0 71.0 72.3

IMDB (M) 1000 176 3 44.6 45.2 48.1

REDDIT (B) 2000 3782 2 78.0 86.3 87.8

COLLAB 5000 982 3 73.0 72.6 74.2

Table 3: 10-Fold CV test accuracy (%) on social-network datasets.

Our Ego-CNN improves the performance of Patchy-San on most datasets since the fixed-size neighborhoods (each contains at most 10 neighbors) detected by Patchy-San are grown into larger neighborhoods by Ego-Convolution. It makes sense on datasets such as MUTAG, where the task is to predict if compounds are mutagenic to DNA. It is reported by Debnath et al. (1991) that compounds with more than 3 benzene rings are very likely to be positive. To detect benzene rings in compounds, it's easier for Ego-CNN since we detect much larger local neighborhoods. This shows that representing graphs by larger neighborhoods is helpful on these datasets.

8

Under review as a conference paper at ICLR 2018

5.2 SCALE-FREE REGULARIZER Since we found most graphs in Reddit dataset are scale-free graphs, we verify the effectiveness of scale-free regularizer on it. The results are shown in Table 4.

Network architecture Patchy-San + 1 Ego-Convolution Patchy-San + 5 Ego-Convolution Patchy-San + 5 Ego-Convolution

Weight-tying?

10-Fold CV test acc (%) 84.9 88.4 87.8

Total # of parameters 1.3M 1.3M 2.3M

Table 4: GCC network with scale-free prior on Reddit dataset.

To introduce scale-free priors into network by weight-tying, we expand 1 Ego-Convolution layer to 5 Ego-Convolution layers with the same weight, so the total parameters is 1.3M, which is the same as network in 1st row but the advantage is that we improve the test accuracy by 3.5%. Also, in the 3rd row, we compare against the network with same architecture but without using the scale-free regularizer. The number of parameter is 1M more but does not give better performance. This shows that introducing scale-free priors improves the learning efficiency, so that the same (or even better) performance can be obtained by training with fewer parameters.

5.3 VISUALIZATION OF CRITICAL STRUCTURES
As a sanity check, we generate two example compound datasets to visualize critical structures at the local-scale (Alkanes vs Alcohols) and global scale (Symmetric vs Asymmetric Isomers). The structures of compounds are generated of different size and under different vertex-orderings. First, we'd like to know if the network considers OH-base as critical structures. The visualization result is shown in 8, where critical structures are plotted in grey color, and the node/edge size are proportional to its important score. We find that the OH-base in Alcohols is always consider critical no matter how long it its.

(a) C14 H29 OH

(b) C82 H185 OH

Figure 8: Visualization of critical structures on Alkane vs Alcohol. The critical structures are colored in grey and the node/edge size is proportional to its importance. The OH-base in Alcohols is always considered critical.

Next, we'd like to know how the network detects the concept of symmetry. The visualization result of Symmetric vs Asymmetric Isomers dataset is shown below. For symmetric isomers (Figure 9(a)), the critical structure, i.e. the symmetric hydrocarbon chains, is detected. By carefully observing the nodes and edge inside the detected structures (color in grey), we find an interesting thing that the importance of nodes/edges (i.e. which is plotted in different size proportional to the importance score for nodes and weights in interpolated adjacency for edges) are also roughly to be symmetric to the methyl-base. This symmetric phenomenon of detected structure can also be observed in asymmetric isomers (Figure 9(b)). We conjecture the network learns to compare if the two long hydrocarbon chains (which are branched from the methyl-base) are symmetric or not by starting comparing nodes and edges from the methyl-base all the way to the end of the hydrocarbon chains, which is similar to how people check if a structure is symmetric.

9

Under review as a conference paper at ICLR 2018

(a) Symmetric Isomer

(b) Asymmetric Isomer

Figure 9: Visualization of critical structures on Asymmetric Isomers vs Symmetric Isomers. The critical structures are colored in grey and the node/edge size is proportional to its importance. The critical patterns detected are roughly symmetric from the methyl branching node, which means the network somehow learned to count from the branching node to see if the structure is symmetric or not.

In the last experiment, we'd like to visualize the detected structures on Reddit dataset to see if it can help us to derive more knowledge about the dataset. In Reddit dataset, each graph represents a discussion thread. A node represent a user, and there's an edge if two users discuss with each other. The task is to classify the discussion style of threads. i.e. wether it is a discussion-based thread (e.g. threads under Atheism subreddit) or a QA-based thread (e.g. threads under AskReddit subreddit).

(a) Discussion-based thread.

(b) QA-based thread.

Figure 10: Visualization of critical structures on Reddit dataset. The critical structures are colored in grey and the node/edge size is proportional to its importance.

From the visualization result (Figure 10), we observe that the detected structures seemed to be the "bridges" that interconnect highly active users who has many discussions with other users. If each active user represents an strong opinion in the thread, the visualization result suggests that the discussion of different opinions are the key to discriminant discussion-based threads from QA-based threads.

6 CONCLUSIONS
We propose Ego-CNN with idea of the egocentric design to define neighborhood on the same node across layers. The egocentric design enables Ego-CNN to fully take advantages of CNN to perform efficient learning on graphs, so that neighborhoods are enlarged as depth increases. Also, training efficiency can be increased by introducing scale-free priors, and visualization techniques for CNN can be used to visualize structures detected by Ego-CNN. We hope Ego-CNN helps people in domains such as bioinformatic, cheminformatics to understand their experiment results. The future direction is to further reduce the space requirement of Ego-CNN. Since receptive fields at deeper Ego-Convolution layer represents larger neighborhoods, the overlapping area among neighborhoods of different nodes may also be enlarged. So, instead of using all neighborhoods, it might be enough to embed graphs by only part of the neighborhoods.

10

Under review as a conference paper at ICLR 2018
REFERENCES
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Proceedings of NIPS, 2016.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In Proceedings of ICLR, 2013.
Stephen A Cook. The complexity of theorem-proving procedures. In Proceedings of the third annual ACM symposium on Theory of computing. ACM, 1971.
Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In Proceedings of ICML, 2016.
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786­797, 1991.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Proceedings of NIPS, 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of NIPS, 2015.
Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):1254­ 1259, 1998.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund. de.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of ICLR, 2017.
Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. In Proceedings of NIPS, 2016. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. 2013. Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu, and Santhoshkumar Sam-
inathan. subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs. In Workshop on Mining and Learning with Graphs, 2016. Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of ICML, 2016. Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 12(Sep):2539­2561, 2011. Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of SIGKDD. ACM, 2015. Matthew D Zeiler, Graham W Taylor, and Rob Fergus. Adaptive deconvolutional networks for mid and high level feature learning. In Proceedings of ICCV. IEEE, 2011.
11

