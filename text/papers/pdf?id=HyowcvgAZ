Under review as a conference paper at ICLR 2018
DISTRIBUTIONAL INCLUSION VECTOR EMBEDDING FOR UNSUPERVISED HYPERNYMY DETECTION
Anonymous authors Paper under double-blind review
ABSTRACT
Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limit the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are aware--evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions-- we find that our method provides up to double or triple the precision of previous unsupervised methods, and the highest average performance, using a much more compact word representation, and also sometimes outperforms previous semi-supervised embeddings, yielding many new state-of-the-art results. In addition, the meaning of each dimension in DIVE is interpretable, which leads to a novel approach on word sense disambiguation as another promising application of DIVE.
1 INTRODUCTION
Numerous applications benefit from compactly representing context distributions, which assign meaning to objects under the rubric of distributional semantics. In natural language processing, distributional semantics has long been used to assign meanings to words (that is, to lexemes in the dictionary, not individual instances of word tokens). The meaning of a word in the distributional sense is often taken to be the set of textual contexts (nearby tokens) in which that word appears, represented as a large sparse bag of words (SBOW). Without any supervision, word2vec (Mikolov et al., 2013), among other approaches based on matrix factorization (Levy et al., 2015a), successfully compress the SBOW into a much lower dimensional embedding space, increasing the scalability and applicability of the embeddings while preserving (or even improving) the correlation of geometric embedding similarities with human word similarity judgments.
While embedding models have achieved impressive results, context distributions capture more semantic features than just word similarity. The distributional inclusion hypothesis (DIH) (Weeds & Weir, 2003; Geffet & Dagan, 2005; Cimiano et al., 2005) posits that the context set of a word tends to be a subset of the contexts of its hypernyms. For a concrete example, most adjectives that can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle. For instance, both can be obedient. However, the converse is not necessarily true -- a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW based on DIH have been developed for automatic hypernymy detection (Weeds & Weir, 2003; Geffet & Dagan, 2005; Santus et al., 2017).
Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entailment (Sammons et al., 2011), coreference (Ponzetto & Strube, 2006), relation extraction (Demeester et al., 2016) and question answering (Huang et al., 2008). Leveraging the variety of contexts and inclusion properties in context distributions can greatly increase the ability to discover taxonomic structure among words (Santus et al., 2017). The inability to preserve these features limits the se-
1

Under review as a conference paper at ICLR 2018

mantic representation power and downstream applicability of some popular existing unsupervised learning approaches such as word2vec.
Several recently proposed methods aim to encode hypernym relations between words in dense embeddings, such as Gaussian embedding (Vilnis & McCallum, 2015; Athiwaratkun & Wilson, 2017), order embedding (Vendrov et al., 2016), H-feature detector (Roller & Erk, 2016), HyperScore (Nguyen et al., 2017), dual tensor (Glavas & Ponzetto, 2017), and Poincare´ embedding (Nickel & Kiela, 2017). However, the methods focus on supervised or semi-supervised setting (Vendrov et al., 2016; Roller & Erk, 2016; Nguyen et al., 2017; Glavas & Ponzetto, 2017), do not learn from raw text (Nickel & Kiela, 2017) or lack comprehensive experiments on the hypernym detection task (Vilnis & McCallum, 2015; Athiwaratkun & Wilson, 2017).
Recent studies (Levy et al., 2015b; Santus et al., 2017) have underscored the difficulty of generalizing supervised hypernymy annotations to unseen pairs -- classifiers often effectively memorize prototypical hypernyms (`general' words) and ignore relations between words. These findings motivate us to develop more accurate and scalable unsupervised embeddings to detect hypernymy and propose several scoring functions to analyze the embeddings from different perspectives.
1.1 CONTRIBUTIONS
· A novel unsupervised low-dimensional embedding method to model inclusion relations among word contexts via performing non-negative matrix factorization (NMF) on a weighted PMI matrix, which can be efficiently optimized using modified skip-grams.
· Several new asymmetric comparison functions to measure inclusion and generality properties and to evaluate different aspects of unsupervised embeddings.
· Extensive experiments on 11 datasets demonstrate the learned embeddings and comparison functions achieve state-of-the-art performances on unsupervised hypernym detection while requiring much less memory and compute than approaches based on the full SBOW.
· A qualitative experiment illustrates DIVE can be used to solve word sense disambiguation, especially when efficiently modeling word senses at multiple granularities is desirable.

2 METHOD

The distributional inclusion hypothesis (DIH) suggests that the context set of a hypernym tends to contain the context set of its hyponyms. That is, when representing a word as the counts of contextual co-occurrences, the count in every dimension of hypernym y tends to be larger than or equal to the corresponding count of its hyponym x:

x y  c  V, #(x, c)  #(y, c),

(1)

where x y means y is a hypernym of x, V is the set of vocabulary, and #(x, c) indicates the number of times that word x and its context word c co-occur in a small window with size |W | in corpus D.

Our goal is to produce lower-dimensional embeddings that preserve the inclusion property that the embedding of hypernym y is larger than or equal to the embedding of its hyponym x in every dimension. Formally, the desirable property can be written as

x y  x[i]  y[i] , i  {1, ..., d0},

(2)

where d0 is number of dimensions in the embedding space. We add additional non-negativity constraints, i.e. x[i]  0, y[i]  0, i, in order to increase the interpretability of the embeddings (the
reason will be explained later in this section).

This is a challenging task. In reality, there are a lot of noise and systematic biases which cause the violation of DIH in Equation (1) (i.e. #(x, c) > #(y, c) for some neighboring word c), but the general trend can be discovered by processing several thousands of neighboring words in SBOW together. After the compression, the same trend has to be estimated in a much smaller embedding space which discards most of the information in SBOW, so it is not surprising to see most of the unsupervised hypernymy detection studies use SBOW (Santus et al., 2017) and the existing unsupervised embeddings like Gaussian embedding have degraded accuracy (Vulic´ et al., 2016).

2

Under review as a conference paper at ICLR 2018

2.1 INCLUSION PRESERVING MATRIX FACTORIZATION

Popular methods of unsupervised word embedding are usually based on matrix factorization (Levy
et al., 2015a). The approaches first compute a co-occurrence statistic between the wth word and the
cth context word as the (w, c)th element of the matrix M [w, c]. Next, the matrix M is factorized such that M [w, c]  wT c, where w is the low dimension embedding of wth word and c is the cth
context embedding.

The statistic in M [w, c] is usually related to pointwise mutual information: P M I(w, c) =

log(

P

P (w,c) (w)·P (c)

),

where

P (w, c)

=

#(w,c) |D|

,

|D|

=

#(w, c) is number of co-occurrence

wV cV

word

pairs

in

the

corpus,

P (w)

=

#(w) |D|

,

#(w)

=

#(w, c) is the frequency of the word w times

cV

the window size |W |, and similarly for P (c). For example, M [w, c] could be set as positive PMI

(PPMI), max(P M I(w, c), 0), or shifted PMI, P M I(w, c) - log(k), like skip-grams with negative

sampling (SGNS) (Levy et al., 2015a). Intuitively, since M [w, c]  wT c, larger embedding values

of w at every dimension seems to imply larger wT c, larger M [w, c], larger P M I(w, c), and thus

larger co-occurrence count #(w, c). However, the derivation has two flaws: (1) c could be negative

and (2) lower #(w, c) could still lead to larger P M I(w, c) as long as the #(w) is small enough.

To preserve DIH, we propose a novel word embedding method, distributional inclusion vector embedding (DIVE), which fixes the two flaws by performing non-negative factorization (NMF) on the matrix M , where

M [w,

c]

=

log( P

P (w, c) (w) · P (c)

·

#(w) k·Z )

=

log(

#(w, c)|V #(c)k

|

),

(3)

where k

is a constant which

shifts PMI value

like

SGNS, Z

=

|D| |V |

is

the average word

frequency,

and |V | is the vocabulary size.

The design encourages the inclusion property in DIVE (i.e. Equation (2)) to be satisfied because

the property implies that Equation (1) (DIH) holds if the matrix is reconstructed perfectly. The

derivation is simple: Since context vector c is non-negative, if the embedding of hypernym y is

greater than or equal to the embedding of its hyponym x in every dimension, xT c  yT c. Then,

M [x, c]  M [y, c] tends to be true because wT c  M [w, c]. This leads to #(x, c)  #(y, c)

because

M

[w,

c]

=

log(

#(w,c)|V #(c)k

|

)

and

only

#(w,

c)

change

with

w.

2.2 OPTIMIZATION

Due to its appealing scalability properties during training time (Levy et al., 2015a), we optimize our embedding based on the skip-gram with negative sampling (SGNS) (Mikolov et al., 2013). The objective function of SGNS is

lSGNS =

#(w, c) log (wT c) + k #(w, c) E [log (-wT cN)],

wV cV

wV cV

cN PD

(4)

where w  R, c  R, cN  R, k is a constant hyper-parameter indicating the ratio between positive and negative samples.

Levy & Goldberg (2014) prove SGNS is equivalent to factorizing a shifted PMI matrix M , where

M

[w, c]

=

log(

P

P (w,c) (w)·P (c)

·

1 k

).

By

setting

k

=

k·Z #(w)

and

applying non-negativity constraints

to

the embeddings, DIVE can be optimized using the similar objective function:

lDIV E

=
wV

cV

#(w, c) log (wT c)

+k
wV

Z #(w)
cV

#(w, c) E [log (-wT cN)],
cN PD

(5)

where w  0, c  0, cN  0,  is the logistic sigmoid function, and k is a constant hyper-parameter. PD is the distribution of negative samples, which we set to be the corpus word frequency distribution

in this paper. Equation (5) is optimized by ADAM (Kingma & Ba, 2015), a variant of stochastic

gradient descent (SGD). The non-negativity constraint is implemented by projection (i.e., clipping

any embedding which crosses the zero boundary after an update).

3

Under review as a conference paper at ICLR 2018

... when a electron be remove from a core level of a atom ... ...
... both basic cpus and standard product built around a CPU core ... ...
... the innovation of the common core , a educational strategy ...

id Top 1-5 words 1 element, gas, atom, rock, carbon 2 system, architecture, develop, base, language 3 also, well, several, early, see 4 part, almost, see, addition, except 5 star, orbit, sun, orbital, planet 6 several, main, province, include, consist 7 science, philosophy, theory, philosopher, term 8 version, game, release, original, file 9 electron, current, electric, circuit, voltage 10 tank, cylinder, wheel, engine, steel 11 school, university, student, education, college 12 network, user, server, datum, protocol 13 high, low, temperature, energy, speed 14 acid, carbon, product, use, zinc 15 access, need, require, allow, program

Top 101-105 words methane, crystalline, complex, surround, proton functional, requirement, processing, compatible, api
fall, eventually, main, rise, mostly incorporate, stage, instead, opening, add bright, position, centauri, interaction, universe
designate, exist, swiss, branch, thai ethical, advocate, topic, basic, universe
cassette, virtual, code, project, kb anode, wire, ac, perform, resistor aluminum, automatic, pilot, prevent, remove doctorate, doctoral, middle, arts, compulsory technology, rout, agent, microsoft, command atmospheric, fast, blood, m, population ph, monoxide, phosphorus, bond, manufacture size, ability, format, run, typically

Figure 1: The top 15 dimensions in the distributional inclusion vector embedding (DIVE) of the word core trained by the co-occurrence statistics of context words. The index of dimensions is sorted by the embedding values. The words in each row of the table are sorted by its embedding value in the dimension.

The optimization process provides an alternative angle to explain how DIVE preserves DIH. The gradients for the word embedding w is

dlDIV E = dw

#(w, c)(1 - (wT c))c - k

#(cN |V |

)

(wT

cN

)cN.

cV

cN V

(6)

Assume hyponym x and hypernym y satisfy DIH in Equation (1) and their initial embeddings x and y are the same. In the case, the gradients coming from negative sampling (the second term) decrease the same amount of embedding values for both x and y (assume updates are small enough so none clip to zero). However, the embedding of hypernym y would get higher or equal positive gradients from the first term than x in every dimension because #(x, c)  #(y, c). This means Equation (1) tends to imply Equation (2). Combining the analysis from the matrix factorization viewpoint, DIH in Equation (1) is approximately equivalent to the inclusion property in DIVE (i.e. Equation (2)).

2.3 PMI FILTERING

For a frequent target word, there must be many neighboring words that incidentally appear near the

target word without being semantically meaningful, especially when a large context window size

is used. The unrelated context words cause noise in both the word vector and the context vector of

DIVE. We address this issue by filtering out context words c for each target word w when the PMI of

the

co-occurring

words

is

too

small

(i.e.,

log(

P

P (w,c) (w)·P (c)

)

<

log(kf

)).

That

is,

we

set

#(w,

c)

=

0

in

the objective function. This preprocessing step is similar with computing PPMI in SBOW (Bullinaria

& Levy, 2007), where low PMI co-occurrences are removed from the count-based representation.

2.4 INTERPRETABILITY
After applying the non-negativity constraint, we observe that each dimension roughly corresponds to a topic, as previous findings suggest (Pauca et al., 2004; Murphy et al., 2012). This gives rise to a

4

Under review as a conference paper at ICLR 2018
natural and intuitive interpretation of our word embeddings: the word embeddings can be seen as unnormalized probability distributions over topics. By removing the normalization of the target word frequency in the shifted PMI matrix, specific words have values in few dimensions (topics), while general words appear in more topics and correspondingly have high values in more dimensions, so the concreteness level of two words can be easily compared using the magnitude of their embeddings. In other words, general words have more diverse context distributions, so we need more dimensions to store the information in order to compress SBOW well (Nalisnick & Ravi, 2015).
In Figure 1, we present three mentions of the word core and its surrounding contexts. These various context words increase the embedding values in different dimensions. Each dimension of the learned embeddings roughly corresponds to a topic, and the more general or representative words for each topic tend to have the higher value in the corresponding dimension (e.g. words in the second column of the table). The embedding is able to capture the common contexts where the word core appears. For example, the context of the first mention is related to the atom topic (dimension id 1) and the electron topic (id 9), while the second and third mention occur in the computer architecture topic (id 2) and education topic (id 11), respectively.
3 EXPERIMENT SETUP
We describe four experiments in Section 4-7. The first 3 experiments compare DIVE with other embeddings and SBOW using different hypernymy scoring functions. The last experiment presents qualitative results on word sense disambiguation.
3.1 DATASETS AND TESTING SETUP
The SBOW and embeddings are tested on 11 datasets. The first 4 datasets come from the recent review (Santus et al., 2017): BLESS (Baroni & Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller & Erk, 2016): Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney & Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HyperNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vulic´ et al., 2016) are also evaluated.
The F1 and accuracy measurements are sometimes very similar even though the quality of prediction varies, so average precision AP@all is adopted as the main evaluation metric. Nevertheless, when we compare F1 with the results from other papers, we use 20 fold cross validation to determine prediction thresholds, as done by Roller & Erk (2016). The HyperLex dataset has a continuous score on each candidate word pair, so we adopt Spearman rank coefficient  as suggested by the review study (Vulic´ et al., 2016). Any OOV (out-of-vocabulary) word encountered in the testing data is pushed to the bottom of the prediction list (effectively assuming the word pair does not have a hypernym relation).
3.2 TRAINING SETUP
We use WaCkypedia corpus (Baroni et al., 2009), a 2009 Wikipedia dump, to compute SBOW and train the embedding. For the datasets without Part of Speech (POS) information (i.e. Medicl, LEDS, TM14, Kotlerman 2010, and HyperNet), the training data of SBOW and embeddings are raw text. For other datasets, we concatenate each token with the Part of Speech (POS) of the token before training the models except the case when we need to match the training setup of another paper.
All words are lower cased. Stop words and rare words (occurs less than 10 times) are removed during our preprocessing step. The number of embedding dimensions in DIVE d0 is set to be 100. Other hyper-parameters used in the experiments are listed in the supplementary materials. The hyper-parameters of DIVE were decided based on the performance of HyperNet training set. To train embeddings more efficiently, we chunk the corpus into subsets/lines of 100 tokens instead of using sentence segmentation. Preliminary experiments show that this implementation simplification does not hurt the performance.
5

Under review as a conference paper at ICLR 2018

Table 1: Comparison with previous unsupervised or semi-supervised embeddings. All values are percentages. The number in parentheses beside each approach indicates the number of annotated hypernymy word pairs used to train the model. The performances of Gaussian embedding (GE) (Vilnis & McCallum, 2015) are taken from Vilnis & McCallum (2015); Athiwaratkun & Wilson (2017); Vulic´ et al. (2016). Previous semi-supervised embeddings (with limited training data) include HyperScore (Nguyen et al., 2017) and H-feature (Roller & Erk, 2016). Note that HyperScore ignores POS in the testing data, so we follow the setup when comparing with it.

Dataset Metric Baselines (#Training Hypernymy) DIVE + C·S (0)

LEDS HyperLex AP@all Spearman 
GE (0) 80/76 20.6 30 83.5 34.5

EVALution LenciBenotto AP@all
HyperScore (1337) 39 44.8 33.8 52.9

Weeds
58.5 70.0

Medical F1
H-feature (897) 26 25.3

In the following experiments, we train both SBOW and DIVE on only the first 512,000 lines (51.2 million tokens) because we find this way of training setting provides better performances (for both SBOW and DIVE) than training on the whole WaCkypedia or training on randomly sampled 512,000 lines. We suspect this is due to the corpus being sorted by the Wikipedia page titles, which makes some categorical words such as animal and mammal occur 3-4 times more frequently in the first 51.2 million tokens than the rest. The performances of training SBOW PPMI on the whole WaCkypedia is also provided for reference in Table 4 and Table 5.
The SBOW model which represents a word by the frequency of its neighboring words is denoted as SBOW Freq, while the SBOW which uses PPMI of its neighboring words as the features (Bullinaria & Levy, 2007) is denoted as SBOW PPMI.

4 EXPERIMENT 1: COMPARISON WITH OTHER EMBEDDINGS

If a pair of words has the hypernym relation, the words tend to be similar and the hypernym should be more general than the hyponym. As in HyperScore (Nguyen et al., 2017), we score the hypernym candidates by multiplying two factors corresponding to these properties. The C·S (i.e. the cosine similarity multiply the difference of summation) scoring function is defined as

C · S(wq

 wp) =

wTq wp ||wq||2 · ||wp||2

·(

wp

1-

wq

1),

(7)

where wp is the embedding of hypernym and wq is the embedding of hyponym.
As far as we know, Gaussian embedding is the only unsupervised embedding method which can capture the asymmetric relations between a hypernym and its hyponyms. In addition to the unsupervised approach, we also compare DIVE with semi-supervised approaches. When there are sufficient training data, there is no doubt that the semi-supervised embedding approaches such as HyperNet (Shwartz et al., 2016), H-feature detector (Roller & Erk, 2016), and HyperScore (Nguyen et al., 2017) can achieve better performance than all unsupervised methods. However, in many domains such as scientific literature, there are often not many annotated hypernymy pairs (e.g. Medical dataset (Levy et al., 2014)). When training data are limited, our unsupervised methods can produce similar or even better results than the previous semi-supervised approaches, as shown in Table 1. This demonstrates the robustness of our approach and the difficulty of generalizing hypernymy annotations with semi-supervised approaches.

5 EXPERIMENT 2: HYPERNYMY SCORING FUNCTIONS ANALYSIS
In Experiment 1, we show that there exists a scoring function (C·S) which detects hypernymy accurately using the embedding space of DIVE. Nevertheless, different scoring functions measure different signals in SBOW or embeddings. Since there are so many scoring functions and datasets available in the domain, we first introduce and test the performances of various scoring functions so as to select the representative ones for a more comprehensive evaluation of DIVE on the hypernymy detection tasks. We denote the embedding/context vector of the hypernym candidate and the hyponym candidate as wp and wq, respectively.

6

Under review as a conference paper at ICLR 2018
5.1 UNSUPERVISED SCORING FUNCTIONS
5.1.1 SIMILARITY
A hypernym tends to be similar to its hyponym, so we measure the cosine similarity between word vectors of the SBOW features (Levy et al., 2015b) or DIVE. We refer to the symmetric scoring function as Cosine or C for short in the following tables. We also train the original skip-grams with 100 dimensions and measure the cosine similarity between the resulting word2vec embeddings. This scoring function is referred to as Word2vec or W.
5.1.2 GENERALITY
The distributional informativeness hypothesis (Santus et al., 2014) observes that in many corpora, semantically `general' words tend to appear more frequently and in more varied contexts. Thus, Santus et al. (2014) advocate using entropy of context distributions to capture the diversity of context. We adopt the two variations of the approach proposed by Santus et al. (2017): SLQS Row and SLQS Sub functions. We also refer to SLQS Row as E because it measures the entropy difference of context distributions. For SLQS Sub, the number of top context words is fixed as 100.
Although effective at measuring diversity, the entropy totally ignores the frequency signal from the corpus. To leverage the information, we measure the generality of a word by its L1 norm (|wp|1) and L2 norm (||wp||2). Recall that Equation (2) indicates that the embedding of the hypernym y should have a larger value at every dimension than the embedding of the hyponym x. When the inclusion property holds, |y|1 = i y[i]  i x[i] = |x|1 and similarly ||y||2  ||x||2. Thus, we propose two scoring functions, difference of vector summation (|wp|1 - |wq|1) and the difference of vector 2-norm (||wp||2 - ||wq||2). Notice that when applying the difference of vector summations (denoted as S) to SBOW Freq, it is equivalent to computing the word frequency difference between the hypernym candidate pair.
5.1.3 COMBINATION
The combination of 2 similarity functions (Cosine and Word2vec) and the 3 generality functions (difference of entropy, summation, and 2-norm of vectors) leads to six different scoring functions as shown in Table 2, and C·S is the same scoring function we used in Experiment 1. It should be noted that if we use skip-grams with negative sampling (word2vec) as the similarity measurement (i.e., W ·  {E,S,Q}), the scores are determined by two embedding/feature spaces together (word2vec and DIVE/SBOW).
5.1.4 INCLUSION
Several scoring functions are proposed to measure inclusion properties of SBOW based on DIH. Weeds Precision (Weeds & Weir, 2003) and CDE (Clarke, 2009) both measure the magnitude of the intersection between feature vectors (|wp  wq|). For example, wp  wq is defined by the elementwise minimum in CDE. Then, both scoring functions divide the intersection by the magnitude of the potential hyponym vector (|wq|). invCL (Lenci & Benotto, 2012) (A variant of CDE) is also tested.
We choose these 3 functions because they have been shown to detect hypernymy well in a recent study (Santus et al., 2017). However, it is hard to confirm that their good performances come from the inclusion property between context distributions -- it is also possible that the context vectors of more general words have higher chance to overlap with all other words due to their high frequency. For instance, considering a one dimension feature which stores only the frequency of words, the naive embedding could still have reasonable performance on the CDE function, but the embedding in fact only memorizes the general words without modeling relations between words (Levy et al., 2015b) and loses lots of inclusion signals in the word co-occurrence statistics.
In order to measure the inclusion property without the interference of the word frequency signal from the SBOW or embeddings, we propose a new measurement called asymmetric L1 distance. We first get context distributions dp and dq by normalizing wp and wq, respectively. Ideally, the context distribution of the hypernym dp will include dq. This suggests the hypernym distribution dp is larger than context distribution of the hyponym with a proper scaling factor adq (i.e., max(adq - dp, 0) should be small). Furthermore, both distributions should be similar, so adq should not be too
7

Under review as a conference paper at ICLR 2018

Table 2: Micro average AP@all (%) of 10 datasets using different scoring functions. The feature space is SBOW using word frequency.

Word2vec (W) 24.8 W·E 28.8 Weeds 19.0

Cosine (C) 26.7 C·E 29.5 CDE 31.1

SLQS Sub 27.4 W·S 31.6
invCL 30.7

SLQS Row (E) 27.6 C·S 31.2
Asymmetric L1 (AL1) 28.2

Summation (S) 31.5
W·Q 31.4

Two norm (Q) 31.2 C·Q 31.1

Table 3: Comparison with previous methods based on sparse bag of word (SBOW). All values are percentages. The results of invCL (Lenci & Benotto, 2012), APSyn (Santus et al., 2016), and CDE (Clarke, 2009) are selected because they have the best AP@100 in the first 4 datasets (Santus et al., 2017). Cosine similarity (Levy et al., 2015b), balAPinc (Kotlerman et al., 2010) in 3 datasets (Turney & Mohammad, 2015), SLQS (Santus et al., 2014) in HyperNet dataset (Shwartz et al., 2016), and Freq ratio (FR) (Vulic´ et al., 2016) are compared.

Dataset Metric
Baselines
DIVE + C·S DIVE + W·S
Dataset Metric
Baselines
DIVE + C·S DIVE + W·S

BLESS EVALution LenciBenotto

AP@all

invCL

APSyn

5.1 35.3

38.2

16.3 33.0

50.4

18.6 32.3

51.5

LEDS TM14 Kotlerman 2010

AP@all

balAPinc

73 56

37

83.5 57.2

36.6

86.4 57.3

37.4

Weeds
CDE 44.1 65.5 68.6
HyperNet F1
SLQS 22.8 41.9 38.6

Medical F1
Cosine 23.1 25.3 25.7
HyperLex Spearman  Freq ratio
27.9 32.8 33.3

different from dp (i.e., max(dp - adq, 0) should also be small). Therefore, we define asymmetric L1 distance as

AL1

=

min
a

w0· max(adq[c] - dp[c], 0) + max(dp[c] - adq[c], 0),

c

(8)

where w0 is a constant which emphasizes the inclusion penalty. If w0 = 1 and a = 1, AL1 is equivalent to L1 distance. The lower AL1 distance implies a higher chance of observing the hypernym relation. We tried w0 = 5 and w0 = 20. w0 = 20 produces a worse micro-average AP@all on SBOW Freq, SBOW PPMI and DIVE, so we fix w0 to be 5 in all experiments. An efficient way to solve the optimization in AL1 is presented in the supplementary materials.

5.2 RESULTS AND DISCUSSIONS
We show the micro average AP@all on 10 datasets using different hypernymy scoring functions in Table 2. We can see the combination functions such as C·S and W·S perform the best overall. Among the unnormalized inclusion based scoring functions, CDE works the best. AL1 performs well compared with other functions which remove the frequency signal such as Word2vec, Cosine, and SLQS Row. The summation is the most robust generality measurement. In the table, the scoring functions are applied to SBOW Freq, but the performances of hypernymy scoring functions on the other feature spaces (e.g. DIVE) have a similar trend.

6 EXPERIMENT 3: COMPARISON WITH SBOW
6.1 COMPARISON WITH PREVIOUSLY REPORTED RESULTS
In Table 3, DIVE with two of the best scoring functions (C·S and W·S) is compared with the previous unsupervised state-of-the-art approaches based on SBOW on different datasets. In these experiments, unsupervised approaches refer to the hypernymy detection methods that do not require any hypernymy or lexicon annotations.
There are several reasons which might cause the large performance gaps in some datasets. In addition to the effectiveness of DIVE, some improvements come from our proposed scoring functions.

8

Under review as a conference paper at ICLR 2018

The fact that every paper uses a different training corpus also affects the performances. Furthermore, Santus et al. (2017) select the scoring functions and feature space for the first 4 datasets based on AP@100, which we believe is too sensitive to the hyper-parameter settings of different methods. To isolate the impact of each factor, we perform a more comprehensive comparison next.

6.2 PERFORMANCE ANALYSIS
In this experiment, we examine whether DIVE successfully preserves the signals for hypernymy detection tasks, which are measured by the same scoring functions designed for SBOW. Summation difference (S) and CDE perform the best among generality and inclusion functions in Table 2, respectively. AL1 could be used to examine the inclusion properties after removing the frequency signal. Therefore, we will present the results using these 3 scoring functions, along with W·S and C·S.

6.2.1 BASELINES

In addition to classic representations such as SBOW Freq and SBOW PPMI, we compare distributional inclusion vector embedding (DIVE) with additional 4 baselines in Table 4.

· SBOW PPMI with additional frequency weighting (PPMI w/ FW). Specifically, w[c] =

max(log(

P

P (w,c) (w)P (c)

Z #(w)

),

0).

This

forms

the

matrix

reconstructed

by

DIVE

when

k

=

1.

· DIVE without the PMI filter (DIVE w/o PMI)

· NMF on shifted PMI: Non-negative matrix factorization (NMF) on the shifted PMI without frequency weighting for DIVE (DIVE w/o FW). This is the same as applying the nonnegative constraint on the skip-gram model.

· K-means (Freq NMF): The method first uses Mini-batch k-means (Sculley, 2010) to cluster
words in skip-gram embedding space into 100 topics, and hashes each frequency count in
SBOW into the corresponding topic. If running k-means on skip-grams is viewed as an
approximation of clustering the SBOW context vectors, the method can be viewed as a
kind of NMF (Ding et al., 2005).
Let the N × N context matrix be denoted as Mc, where the (i, j)th element stores the count of word j appearing beside word i. K-means hashing creates a N × 100 matrix G with orthonormal rows (GT G = I), where the (i, k)th element is 0 if the word i does not belong to cluster k. The orthonormal G is also an approximated solution of a type of NMF (Mc  F GT ) (Ding et al., 2005). Hashing context vectors into topic vectors can be written as McG  F GT G = F .

6.2.2 RESULTS AND DISCUSSIONS
In Table 4, we first confirm the finding of the previous review study (Santus et al., 2017): there is no single hypernymy scoring function which always outperforms others. One of the main reasons is that different datasets collect negative samples differently. This is also why we evaluate our method on many datasets to make sure our conclusions hold in general. For example, if negative samples come from random word pairs (e.g. WordNet dataset), a symmetric similarity measure is already a pretty good scoring function. On the other hand, negative samples come from related words in HyperNet, so only computing generality difference leads to the best performance. The negative samples in many datasets are composed of both random samples and similar words (such as BLESS), so the combination of similarity and generality difference yields the most stable results.
DIVE performs similar or better on all the scoring functions compared with SBOW consistently across all datasets in Table 4, while using many fewer dimensions (see Table 6). Its results on combination scoring functions outperform SBOW Freq. Meanwhile, its results on AL1 outperform SBOW PPMI. The fact that combination scoring functions (i.e., W·S or C·S) usually outperform generality functions suggests that only memorizing general words is not sufficient. The best average performance on 4 and 10 datasets are both produced by W·S on DIVE.
SBOW PPMI improves the combination functions from SBOW Freq but sacrifices AP on the inclusion functions. It generally hurts performance to change the frequency sampling of PPMI (PPMI w/

9

Under review as a conference paper at ICLR 2018

Table 4: AP@all (%) of 10 datasets. The box at lower right corner compares the micro average AP across all 10 datasets. Numbers in different rows come from different feature or embedding spaces. Numbers in different columns come from different datasets and unsupervised scoring functions. We also present the micro average AP across the first 4 datasets (BLESS, EVALution, Lenci/Benotto and Weeds). All wiki means SBOW using PPMI features trained on the whole WaCkypedia. FW refers to frequency weighting on the shifted PMI matrix.

AP@all (%)

Freq

SBOW

PPMI PPMI w/ FW

All wiki

Full

DIVE w/o PMI

w/o FW

Kmean (Freq NMF)

AP@all (%)

Freq

SBOW

PPMI PPMI w/ FW

All wiki

Full

DIVE w/o PMI

w/o FW

Kmean (Freq NMF)

AP@all (%)
SBOW Freq SBOW PPMI
All wiki DIVE

AP@all (%)
SBOW Freq SBOW PPMI
All wiki DIVE

CDE 6.3 13.6 6.2 12.1 9.3 7.8 9.0 6.5
CDE 69.5 61.0 67.6 61.3 59.2 60.4 49.2 69.4
CDE 82.7 84.4 83.1 83.3
CDE 37.5 23.8 23.0 25.3

BLESS AL1 S W·S 7.3 5.6 11.0 5.1 5.6 17.2 5.0 5.5 12.4 5.2 6.9 12.5 7.6 6.0 18.6 6.9 5.6 16.7 6.2 7.3 6.2 7.3 5.6 10.9
Weeds AL1 S W·S 58.0 68.8 68.2 50.3 70.3 69.2 52.2 69.4 68.7 48.6 70.0 68.5 55.0 69.7 68.6 56.4 69.3 68.6 47.3 45.1 45.1 51.1 68.8 68.2
LEDS AL1 S W·S 70.4 70.7 83.3 50.2 72.2 86.5 49.7 67.9 82.9 74.7 72.7 86.4
HyperNet AL1 S W·S 28.3 46.9 35.9 24.0 47.0 32.5 24.5 40.5 30.5 24.2 49.3 33.6

C·S 5.9 15.3 5.8 13.4 16.3 7.1 7.3 5.8
C·S 68.4 69.3 67.7 70.4 65.5 64.8 44.9 68.9
C·S 73.3 84.5 81.4 83.5
C·S 43.4 33.1 29.7 32.0

CDE 35.3 30.4 36.0 28.5 30.0 32.8 24.3 33.7

EVALution AL1 S W·S 32.6 36.2 33.0 27.7 34.1 31.9 27.5 36.3 32.9 27.1 30.3 29.9 27.5 34.9 32.3 32.2 35.7 32.5 25.0 22.9 23.5 27.2 36.2 33.0

C·S 36.3 34.3 36.4 31.0 33.0 35.4 23.9 36.2

Micro Average (4 datasets) CDE AL1 S W·S C·S 23.1 21.8 22.9 25.0 23.0 24.7 17.9 22.3 28.1 27.8 23.2 18.2 22.9 25.8 22.9 23.4 17.7 21.7 24.6 25.8 22.1 19.8 22.8 28.9 27.6 22.2 21.0 22.7 28.0 23.1 18.9 17.3 17.2 16.8 17.5 22.5 19.3 22.9 24.9 23.0

TM14 CDE AL1 S W·S C·S 55.6 53.2 54.9 55.7 55.0 56.2 52.3 54.4 57.0 57.6 54.7 50.5 52.6 55.1 54.9 55.3 52.6 55.2 57.3 57.2

CDE 56.6 57.7 57.4 60.2

WordNet AL1 S W·S 55.2 55.5 56.2 53.9 55.6 56.8 53.1 56.0 56.4 58.9 58.4 61.1

C·S 55.6 57.2 57.3 60.9

CDE 51.8 47.2 52.0 47.1 46.7 47.6 38.8 49.6

Lenci/Benotto AL1 S W·S 47.6 51.0 51.8 39.7 50.8 51.1 43.1 50.9 51.9 39.9 48.5 48.7 43.2 51.3 51.5 44.9 50.9 51.6 38.1 38.2 38.2 42.5 51.0 51.8

C·S 51.1 52.0 50.7 51.1 50.4 49.7 38.4 51.2

CDE 19.4 23.4 22.8 22.3 11.7 10.7 10.9 12.6

Medical AL1 S W·S 19.2 14.1 18.4 8.7 13.2 20.1 10.6 13.7 18.6 8.9 12.2 17.6 9.3 13.7 21.4 8.4 13.3 19.8 9.8 7.4 7.6 10.9 14.0 18.1

C·S 15.3 24.4 17.0 21.1 19.2 16.2 7.7 14.6

CDE 35.9 39.1 38.5 35.3

Kotlerman 2010 AL1 S W·S 40.5 34.5 37.0 30.9 33.0 37.0 31.2 32.2 35.4 31.6 33.6 37.4

C·S 35.4 36.3 35.3 36.6

Micro Average (10 datasets) CDE AL1 S W·S C·S 31.1 28.2 31.5 31.6 31.2 30.1 23.0 31.1 32.9 33.5 29.0 23.1 29.2 30.2 31.1 27.6 25.3 32.1 34.1 32.7

Table 5: Spearman  (%) in HyperLex.

Spearman  (%)
SBOW Freq SBOW PPMI
All wiki DIVE

CDE 31.7 28.1 25.3 28.9

HyperLex AL1 S W·S 19.6 27.6 29.6 -2.3 31.8 34.3 -2.2 28.0 30.5 18.7 31.2 33.3

C·S 27.3 34.5 31.0 32.8

Table 6: The average number of non-zero dimensions across all testing words in 10 datasets.

SBOW Freq SBOW PPMI DIVE

5799

3808

20

FW) or compute SBOW PPMI on the whole WaCkypedia (all wiki) instead of the first 51.2 million tokens. The similar trend can also be seen in Table 5. Note that AL1 completely fails in HyperLex dataset using SBOW PPMI, which suggests that PPMI might not necessarily preserve the distributional inclusion property, even though it can have good performance on combination functions.
Removing the PMI filter from DIVE slightly drops the overall precision while removing frequency weights on shifted PMI (w/o FW) leads to poor performances. K-means (Freq NMF) produces similar AP compared with SBOW Freq, but has worse AL1 scores. Its best AP scores on different datasets are also significantly worse than the best AP of DIVE. This means that only making word2vec (skip-grams with negative sampling) non-negative or naively accumulating topic distribution in contexts cannot lead to satisfactory embeddings.
7 EXPERIMENT 4: WORD SENSE DISAMBIGUATION
In addition to hypernymy detection, Athiwaratkun & Wilson (2017) show that the mixture of Gaussian distributions can also be used to discover multiple senses of each word. In our qualitative experiment, we show that DIVE can achieve the similar goal without fixing the number of senses before training the embedding.
Recall that each dimension roughly corresponds to one topic. Given a query word, the higher embedding value on a dimension implies higher likelihood to observe the word in the context of the topic.
10

Under review as a conference paper at ICLR 2018

Table 7: Spectral clustering on the non-zero DIVE dimensions of the query word. When the number of clusters is set to be 2, we present the top 4 dimensions in each cluster, which have the highest values on the query word embedding. Otherwise, the top 2 dimensions are presented. CID refers to cluster ID. The top 5 words with the highest values of each dimension are presented.

Query CID

Top 5 words in the top dimensions

1 rock
2

element, gas, atom, rock, carbon find, specie, species, animal, bird band, song, album, music, rock early, work, century, late, begin

sea, lake, river, area, water point, side, line, front, circle write, john, guitar, band, author include, several, show, television, film

1 bank
2

county, area, city, town, west building, build, house, palace, site
money, tax, price, pay, income united, states, country, world, europe

several, main, province, include, consist sea, lake, river, area, water
company, corporation, system, agency, service state, palestinian, israel, right, palestine

apple

1 2

food, fruit, vegetable, meat, potato war, german, ii, germany, world
version, game, release, original, file system, architecture, develop, base, language

goddess, zeus, god, hero, sauron write, john, guitar, band, author car, company, sell, manufacturer, model include, several, show, television, film

1 star
2

film, role, production, play, stage wear, blue, color, instrument, red element, gas, atom, rock, carbon give, term, vector, mass, momentum

character, series, game, novel, fantasy write, john, guitar, band, author star, orbit, sun, orbital, planet
light, image, lens, telescope, camera

1 tank
2

tank, cylinder, wheel, engine, steel acid, carbon, product, use, zinc
army, force, infantry, military, battle however, attempt, result, despite, fail

industry, export, industrial, economy, company network, user, server, datum, protocol aircraft, navy, missile, ship, flight war, german, ii, germany, world

1 win, world, cup, play, championship

race 2

railway, line, train, road, rail

3 population, language, ethnic, native, people

two, one, three, four, another car, company, sell, manufacturer, model female, age, woman, male, household

1 system, architecture, develop, base, language

run 2

railway, line, train, road, rail

3 game, team, season, win, league

access, need, require, allow, program also, well, several, early, see
game, player, run, deal, baseball

tablet

1 2 3

bc, source, greek, ancient, date use, system, design, term, method system, blood, vessel, artery, intestine

book, publish, write, work, edition version, game, release, original, file patient, symptom, treatment, disorder, may

The embedding of a polysemy would have high values on different groups of topics/dimensions. This allows us to discover the senses by clustering the topics/dimensions of the polysemy. We use the embedding values as the feature each dimension, compute the pairwise similarity between dimensions, and apply spectral clustering (Stella & Shi, 2003) to group topics as shown in the Table 7. See more implementation details in the supplementary materials.
In the word sense disambiguation tasks, it is usually challenging to determine how many senses/clusters each word should have. Many existing approaches fix the number of senses before training the embedding (Tian et al., 2014; Athiwaratkun & Wilson, 2017). Neelakantan et al. (2014) make the number of clusters approximately proportional to the diversity of the context, but the assumption does not always hold. Furthermore, the training process cannot capture different granularity of senses. For instance, race in the car context could share the same sense with the race in the game topic because they all mean contest, but the race in the car context actually refers to the specific contest of speed. Therefore, they can also be viewed as separate senses (like the results in Table 7). This means the correct number of clusters is not unique, and the methods, which fixes the cluster numbers, need to re-train the embedding many times to capture such granularity.
In our approach, clustering dimensions is done after the training process of DIVE is completed, so it is fairly efficient to change the cluster numbers and hierarchical clustering is also an option. Similar to our method, Pelevina et al. (2016) also discover word senses by graph-based clustering. The main difference is that they cluster the top n words which are most related to the query word instead of topics. However, choosing the hyper-parameter n is difficult. Large n would make graph clustering algorithm inefficient, while small n would make less frequent senses difficult to discover.
11

Under review as a conference paper at ICLR 2018
8 RELATED WORK
Most previous unsupervised approaches focus on designing better hypernymy scoring functions for sparse bag of word (SBOW) features. They are well summarized in the recent study (Santus et al., 2017). Santus et al. (2017) also evaluate the influence of different contexts, such as changing the window size of contexts or incorporating dependency parsing information, but neglect scalability issues inherent to SBOW methods.
A notable exception is the Gaussian embedding model (Vilnis & McCallum, 2015). The context distribution of each word is encoded as a multivariate Gaussian distribution, where the embeddings of hypernyms tend to have higher variance and overlap with the embedding of their hyponyms. However, since a Gaussian distribution is normalized, it is difficult to retain frequency information during the embedding process, and experiments on HyperLex (Vulic´ et al., 2016) demonstrate that a simple baseline only relying on word frequency can achieve good results. Follow-up work models contexts by a mixture of Gaussians (Athiwaratkun & Wilson, 2017) relaxing the unimodality assumption but achieves little improvement on hypernym detection tasks.
Kiela et al. (2015) show that images retrieved by a search engine can be a useful source of information to determine the generality of lexicons, but the resources might not be available for some corpora such as scientific literature.
Order embedding (Vendrov et al., 2016) is a supervised approach to encode many annotated hypernym pairs (e.g. all of the whole WordNet (Miller, 1995)) into a compact embedding space, where the embedding of a hypernym should be smaller than the embedding of its hyponym in every dimension. Our method learns embedding from raw text, where a hypernym embedding should be larger than the embedding of its hyponym in every dimension. Thus, DIVE can be viewed as an unsupervised and reversed form of order embedding.
Other semi-supervised hypernym detection methods aim to generalize from sets of annotated word pairs using raw text corpora. The goal of HyperScore (Nguyen et al., 2017) is similar to our model: the embedding of a hypernym should be similar to its hyponym but with higher magnitude. However, their training process relies heavily on annotated hypernym pairs, and the performance drops significantly when reducing the amount of supervision. In addition to context distributions, previous work also leverages training data to discover useful text pattern indicating is-a relation (Shwartz et al., 2016; Roller & Erk, 2016), but it remains challenging to increase recall of hypernym detection because commonsense facts like cat is-a animal might not appear in the corpus.
Non-negative matrix factorization (NMF) has a long history in NLP, for example in the construction of topic models (Pauca et al., 2004). Non-negative sparse embedding (NNSE) (Murphy et al., 2012) and Faruqui et al. (2015) indicate that non-negativity can make embeddings more interpretable and improve word similarity evaluations. The sparse NMF is also shown to be effective in cross-lingual lexical entailment tasks but does not necessarily improve monolingual hypernymy detection (Vyas & Carpuat, 2016). In our study, a new type of NMF is proposed, and the comprehensive experimental analysis demonstrates its state-of-the-art performances on unsupervised hypernymy detection.
9 CONCLUSIONS
Compressing unsupervised SBOW models into a compact representation is challenging while preserving the inclusion, generality, and similarity signals which are important for hypernym detection. Our experiments suggest that simple baselines such as accumulating K-mean clusters and non-negative skip-grams do not lead to satisfactory performances in this task.
To achieve this goal, we proposed an interpretable and scalable embedding method called distributional inclusion vector embedding (DIVE) by performing non-negative matrix factorization (NMF) on a weighted PMI matrix. We demonstrate that scoring functions which measure inclusion and generality properties in SBOW can also be applied to DIVE to detect hypernymy, and DIVE performs the best on average, slightly better than SBOW while using many fewer dimensions.
Our experiments also indicate that unsupervised scoring functions, which combine similarity and generality measurements, work the best in general, but no one scoring function dominates across all datasets. A combination of unsupervised DIVE with the proposed scoring functions is comparable
12

Under review as a conference paper at ICLR 2018
with or even better than semi-supervised methods when the amount of training data are limited, and produce new state-of-the-art performances on many datasets under the unsupervised setup. Finally, a qualitative experiment shows that clusters of the topics discovered by DIVE often correspond to the word senses, which allow us to do word sense disambiguation without the need to know the number of senses before training the embeddings.
REFERENCES
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal word distributions. In ACL, 2017.
Marco Baroni and Alessandro Lenci. How we BLESSed distributional semantic evaluation. In Workshop on GEometrical Models of Natural Language Semantics (GEMS), 2011.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. The WaCky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209­226, 2009.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. Entailment above the word level in distributional semantics. In EACL, 2012.
Giulia Benotto. Distributional models for semantic relations: A study on hyponymy and antonymy. PhD Thesis, University of Pisa, 2015.
John A Bullinaria and Joseph P Levy. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods, 39(3):510­526, 2007.
Philipp Cimiano, Andreas Hotho, and Steffen Staab. Learning concept hierarchies from text corpora using formal concept analysis. J. Artif. Intell. Res.(JAIR), 24(1):305­339, 2005.
Daoud Clarke. Context-theoretic semantics for natural language: an overview. In workshop on geometrical models of natural language semantics, pp. 112­119, 2009.
Thomas Demeester, Tim Rockta¨schel, and Sebastian Riedel. Lifted rule injection for relation embeddings. In EMNLP, 2016.
Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix factorization and spectral clustering. In ICDM, 2005.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah Smith. Sparse overcomplete word vector representations. In ACL, 2015.
Maayan Geffet and Ido Dagan. The distributional inclusion hypotheses and lexical entailment. In ACL, 2005.
Goran Glavas and Simone Paolo Ponzetto. Dual tensor model for detecting asymmetric lexicosemantic relations. In EMNLP, 2017.
Zhiheng Huang, Marcus Thint, and Zengchang Qin. Question classification using head words and their hypernyms. In EMNLP, 2008.
Douwe Kiela, Laura Rimell, Ivan Vulic, and Stephen Clark. Exploiting image generality for lexical entailment detection. In ACL, 2015.
Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. In ICLR, 2015.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359­389, 2010.
Alessandro Lenci and Giulia Benotto. Identifying hypernyms in distributional semantic spaces. In SemEval, 2012.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, 2014.
13

Under review as a conference paper at ICLR 2018
Omer Levy, Ido Dagan, and Jacob Goldberger. Focused entailment graphs for open IE propositions. In CoNLL, 2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211­ 225, 2015a.
Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. Do supervised distributional methods really learn lexical inference relations? In NAACL-HTL, 2015b.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.
George A. Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11): 39­41, 1995.
Brian Murphy, Partha Talukdar, and Tom Mitchell. Learning effective and interpretable semantic models using non-negative sparse embedding. COLING, pp. 1933­1950, 2012.
Eric Nalisnick and Sachin Ravi. Infinite dimensional word embeddings. arXiv preprint arXiv:1511.05392, 2015.
Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. Efficient nonparametric estimation of multiple embeddings per word in vector space. In EMNLP, 2014.
Kim Anh Nguyen, Maximilian Ko¨per, Sabine Schulte im Walde, and Ngoc Thang Vu. Hierarchical embeddings for hypernymy detection and directionality. In EMNLP, 2017.
Maximilian Nickel and Douwe Kiela. Poincare´ embeddings for learning hierarchical representations. arXiv preprint arXiv:1705.08039, 2017.
V. Paul Pauca, Farial Shahnaz, Michael W Berry, and Robert J. Plemmons. Text mining using non-negative matrix factorizations. In ICDM, 2004.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12(Oct):2825­2830, 2011.
Maria Pelevina, Nikolay Arefyev, Chris Biemann, and Alexander Panchenko. Making sense of word embeddings. In Workshop on Representation Learning for NLP, 2016.
Simone Paolo Ponzetto and Michael Strube. Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution. In ACL, 2006.
Stephen Roller and Katrin Erk. Relations such as hypernymy: Identifying and exploiting hearst patterns in distributional vectors for lexical entailment. In EMNLP, 2016.
Mark Sammons, V Vydiswaran, and Dan Roth. Recognizing textual entailment. Multilingual Natural Language Applications: From Theory to Practice. Prentice Hall, Jun, 2011.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. Chasing hypernyms in vector spaces with entropy. In EACL, 2014.
Enrico Santus, Frances Yung, Alessandro Lenci, and Chu-Ren Huang. EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models. In Workshop on Linked Data in Linguistics (LDL), 2015.
Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, and Chu-Ren Huang. Unsupervised measure of word similarity: how to outperform co-occurrence and vector cosine in vsms. arXiv preprint arXiv:1603.09054, 2016.
Enrico Santus, Vered Shwartz, and Dominik Schlechtweg. Hypernyms under siege: Linguisticallymotivated artillery for hypernymy detection. In EACL, 2017.
14

Under review as a conference paper at ICLR 2018
David Sculley. Web-scale k-means clustering. In WWW, 2010. Vered Shwartz, Yoav Goldberg, and Ido Dagan. Improving hypernymy detection with an integrated
path-based and distributional method. In ACL, 2016. X Yu Stella and Jianbo Shi. Multiclass spectral clustering. In ICCV, 2003. Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. A proba-
bilistic model for learning multi-prototype word embeddings. In COLING, 2014. Peter D Turney and Saif M Mohammad. Experiments with three approaches to recognizing lexical
entailment. Natural Language Engineering, 21(3):437­476, 2015. Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. In ICLR, 2016. Luke Vilnis and Andrew McCallum. Word representations via gaussian embedding. In ICLR, 2015. Ivan Vulic´, Daniela Gerz, Douwe Kiela, Felix Hill, and Anna Korhonen. Hyperlex: A large-scale
evaluation of graded lexical entailment. arXiv preprint arXiv:1608.02117, 2016. Yogarshi Vyas and Marine Carpuat. Sparse bilingual word representations for cross-lingual lexical
entailment. In HLT-NAACL, 2016. Julie Weeds and David Weir. A general framework for distributional similarity. In EMNLP, 2003. Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. Learning to distinguish
hypernyms and co-hyponyms. In COLING, 2014.
15

Under review as a conference paper at ICLR 2018

Table 8: We show the top 30 words with the highest embedding magnitude after dot product with the query embedding q (i.e. showing w such that |wT q|1 is one of the top 30 highest values). The
rows with the empty query word sort words based on |w|1.

Query
species system

use large american design study specie human gene fish evidence system standard allow process network

name state life work lead species bird tree disease breed use type function code file

Top 30 general words

system

include

group

power

may small

produce

control

type people

animal

find

genus

family

name

genetic

live food

protein

wild

design

provide

computer application

datum

device

via base

development service

base death find great high plant organism study cell similar operate develop control program transport

city form body write create may suggest occur mammal fossil model method information software law

Table 9: Accuracy (%) of hypernym directionality prediction across 10 datasets.

Micro Average (10 datasets) SBOW Freq + SLQS Sub SBOW Freq + S SBOW PPMI + S DIVE + S
64.4 66.8 66.8 67.0

A SUPPLEMENTARY MATERIALS
A.1 GENERALITY ESTIMATION AND HYPERNYM DIRECTIONALITY DETECTION
In Table 8, we show the most general words in DIVE under different queries as constraints. We also present the accuracy of judging which word is a hypernym (more general) given word pairs with hypernym relations in Table 9. The direction is classified correctly if the generality score is greater than 0 (hypernym is indeed predicted as the more general word). For instance, summation difference (S) classifies correctly if |wp|1 - |wq|1 > 0 (|wp|1 > |wq|1).
From the table, we can see that the simple summation difference performs better than SQLS Sub, and DIVE predicts directionality as well as SBOW. Notice that whenever we encounter OOV, the directionality is predicted randomly. If OOV is excluded, the accuracy of predicting directionality using unsupervised methods can reach around 0.7-0.75.
A.2 EXPERIMENTAL DETAILS FOR HYPERNYMY DETECTION
In HyperNet and WordNet, some hypernym relations are determined between phrases instead of words. Phrase embeddings are composed by averaging word embeddings or SBOW features. For WordNet, we assume the Part of Speech (POS) tags of the words are the same as the phrase. All part-of-speech (POS) tags in the experiments come from NLTK.
The window size |W | of both SBOW and DIVE are set as 20 (left 10 words and right 10 words). The number of epochs is 15, the learning rate is 0.001, the batch size is 128, the threshold in PMI filter kf is set to be 30, and the ratio between negative and positive samples (k) is 1.5. The hyper-parameters of DIVE were decided based on the performance of HyperNet training set.
The number of testing pairs N and the number of OOV word pairs is presented in Table 10. For reference, Table 11 shows AP@all using the following two baselines:
· Assigning a random score on each word pair (RND). · Cosine similarity using skip-grams (W). The window size of skip-grams (word2vec) is 10.
The number of negative samples (k ) in skip-gram is set as 5. When composing skip gram into phrase embedding, average embedding is used.

16

Under review as a conference paper at ICLR 2018

Table 10: Dataset sizes. N denotes the number of word pairs in the dataset, and OOV shows how many word pairs are not processed by all the methods in Table 4 and Table 5.

BLESS N OOV 26554 1507 Medical N OOV 12602 3711 WordNet N OOV 8000 3596

EVALution N OOV 13675 2475
LEDS N OOV 2770 28 Avg (10 datasets) N OOV 94337 24110

Lenci/Benotto N OOV
5010 1464 TM14
N OOV 2188 178
HyperLex N OOV 2616 59

Weeds N OOV 2928 643 Kotlerman 2010 N OOV 2940 89

Avg (4 datasets) N OOV
48167 6089 HyperNet N OOV
17670 9424

Table 11: AP@all (%) for 10 datasets and Spearman  (%) for HyperLex. RND and W are random and Word2vec baselines, respectively.

BLESS RND W 5.3 9.2
Medical RND W 8.5 11.2
WordNet RND W 55.2 63.0

EVALution RND W 26.6 25.4
LEDS RND W 50.5 71.8 Avg (10 datasets) RND W 23.2 25.3

Lenci/Benotto RND W 41.2 40.8
TM14 RND W 52.0 52.1
HyperLex RND W
0 16.3

Weeds RND W 51.4 51.6 Kotlerman 2010 RND W 30.8 39.5

Avg (4 datasets) RND W 17.9 19.7
HyperNet RND W 24.5 20.7

A.3 CLUSTERING DIMENSIONS FOR WORD SENSE DISAMBIGUATION

We use all the default hyper-parameters of the spectral clustering library in Scikit-learn 0.18.2 (Pe-
dregosa et al., 2011) except the number of clusters is set manually. A simple way to prepare the
feature of each dimension f (ci) is to use the embedding values in that dimension w[ci] of all the words in our vocabulary w  V . That is,

f (ci) = [w[ci]]wV .

(9)

However, clustering on the global features might group topics together based on the co-occurrence of words which are unrelated to the query words and we want to make the similarity dependent on the query word. For example, a country topic should be clustered together with a city topic if the query word is place, but it makes more sense to group the country topic with the money topic together if the query word is bank like we did in the word sense disambiguation experiment (Table 7). This means we want to focus on the geographical meaning of country when the query is related to geography, while focus on the economic meaning of country when the query is about economics.

To create query dependent similarity measurement, we only consider the embedding of words which are related to the query word when preparing the features of dimensions. Specifically, given a query word q, the feature vector of the ith dimension f (ci, q) is defined as:

f (ci,

q)

=

d0
j=1

[w[ci]

·

wq[cj ]]wCj (n)

,

(10)

where wq[cj] is the value of jth dimension of query word embedding, Cj(n) is the set of embeddings of top n words in the jth dimension, and the operator  means concatenation. This means instead of considering all the words in the vocabulary, we only take the top n words of every dimension j (n is fixed as 100 in the experiment), weight the feature based on how likely to observe query word in dimension j (wq[cj]), and concatenate all features together. That is, when measuring the similarity of dimensions, we only consider the aspects related to query word (e.g. mostly considering words related to facility and money when the query word is bank).
After the features of all dimensions are collected, we normalize the feature of each dimension to have the norm 1, compute the pairwise similarity and run the spectral clustering to get the clustering results.

17

Under review as a conference paper at ICLR 2018

dq

a*dq-dp a*dq

dp-a*dq dp

Figure 2: An example of AL1 distance. If the word pair indeed has the hypernym relation, the context distribution of hyponym (dq) tends to be included in the context distribution of hypernym (dp) after proper scaling according to DIH. Thus, the context words only appears beside the hyponym candidate (adq[c] - dp[c]) causes higher penalty.

A.4 EFFICIENT WAY TO COMPUTE ASYMMETRIC L1 (AL1) Recall that Equation (8) defines AL1 as follows:

AL1 = L = min

w0 max(adq[c] - dp[c], 0) + max(dp[c] - adq[c], 0),

a

c

where dp[c] is one of dimension in the feature vector of hypernym dp, adq is the feature vector of hyponym after proper scaling. In Figure 2, an simple example is visualized to illustrate the intuition behind the distance function.
By adding slack variables  and , the problem could be converted into a linear programming problem:

L = min w0 c + c
a,, cc
c  adq[c] - dp[c], c  dp[c] - adq[c], a  0,

c  0 c  0

so it can be simply solved by a general linear programming library.
Nevertheless, the structure in the problem actually allows us to solve this optimization by a simple sorting. In this section, we are going to derive the efficient optimization algorithm.
By introducing Lagrangian multiplier for the constraints, we can rewrite the problem as

L = min max w0 c + c - c(c - adq[c] + dp[c])
a,, ,,, ccc
- c(c - dp[c] + adq[c]) - cc - cc
c cc
c  0, c  0, c  0, c  0, c  0, c  0, a  0

18

Under review as a conference paper at ICLR 2018

First, we eliminate the slack variables by taking derivatives with respect to them:

L c

=

0

= 1 - c

- c

c = 1 - c, c  1

L c

=

0

= 1 - c - c

c = w0 - c, c  w0.

By substituting in these values for c and c, we get rid of the slack variables and have a new Lagrangian:
L = min max - c(-adq[c] + dp[c]) - c(-dp[c] + adq[c])
a , cc
0  c  w0, 0  c  1, a  0

We can introduce a new dual variable c = c - c + 1 and rewrite this as:

L = min max
a

(c - 1)(adq[c] - dp[c])

c

0  c  w0 + 1, a  0

Let's remove the constraint on a and replace with a dual variable :

L = min max
a

(c - 1)(adq[c] - dp[c]) - a

c

0  c  w0 + 1,   0

Now let's differentiate with respect to a to get rid of the primal objective and add a new constraint:

L =0=
a

cdq[c] -

dq[c] - 

cc

cdq[c] = dq[c] + 
cc

L = max dp[c] - cdp[c]
 cc

cdq[c] = dq[c] + 
cc
0  c  w0 + 1,   0

Now we have some constant terms that are just the sums of dp and dq, which will be 1 if they are distributions.
L = max 1 - cdp[c]
 c
cdq[c] = 1 + 
c
0  c  w0 + 1,   0

Now we introduce a new set of variables µc = cdq[c] and we can rewrite the objective as:

L = max 1 -
µ

c

µc

dp[c] dq [c]

µc = 1 + 
c
0  µc  (w0 + 1)dq[c],   0

19

Under review as a conference paper at ICLR 2018

Note that for terms where dq[c] = 0 we can just set dq[c] = for some very small epsilon, and in practice, our algorithm will not encounter these because it sorts.

So µ we can think of as some fixed budget that we have to spend up until it adds up to 1, but it has a limit of how much we can spend for each coordinate, given by (w0 + 1)dq[c]. Since we're trying to minimize the term involving µ, we want to allocate as much budget as possible to the smallest terms in the summand, and then 0 to the rest once we've spent the budget. This also shows us that our optimal value for the dual variable  is just 0 since we want to minimize the amount of budget we have to allocate.

To

make

presentation

easier,

lets

assume

we

sort

the

vectors

in

order

of

increasing

dp [c] dq [c]

,

so

that

dp [1] dq [1]

is

the

smallest

element,

etc.

We

can

now

give

the

following

algorithm

to

find

the

optimal

µ.

init S = 0, c = 1, µ = 0 while S  1 :
µc = min(1 - S, (w0 + 1)dq[c]) S = S + µc c=c+1
At the end we can just plug in this optimal µ to the objective to get the value of our scoring function.

20

