Under review as a conference paper at ICLR 2018
BINARYFLEX: ON-THE-FLY KERNEL GENERATION IN BINARY CONVOLUTIONAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper we present BinaryFlex, a binary neural network architecture that learns coefficients of predefined orthogonal binary basis instead of the conventional approach of learning binary parameters. We have demonstrated the feasibility of our approach for complex computer vision datasets such as ImageNet. Our architecture trained on ImageNet is able to achieve top-5 accuracy of 65.9% while being around 2x smaller than binary networks capable of achieving similar accuracy levels. By using deterministic basis our architecture offers a great deal of flexibility in memory footprint by allowing the application to decide whether to store the basis or to generate them on-the-fly when required.
1 INTRODUCTION
Since the success of AlexNet (Krizhevsky et al., 2012), convolutional neural networks (CNNs) have become the preferred option for computer vision related tasks. While traditionally the research community has been fixated on goals such as model generalization and accuracy in detriment of model size, recently, several approaches (Iandola et al., 2016; Courbariaux & Bengio, 2016; Rastegari et al., 2016) aim to reduce the model's on-device memory footprint while maintaining high levels of accuracy. Binary networks and compression techniques have become a popular option to reduced model size. In addition, binary operations offer 2x speedup (Rastegari et al., 2016) in convolutional layers since they can be replaced with bitwise operations, enabling this networks to run on CPUs. It is also a common strategy to employ small size filters (e.g. 5x5, 3x3 or even 1x1) aiming to reduce the model size. Regardless of the filter dimensions, they represent a big percentage of the total model's size. We present a flexible architecture that facilitates the deployment of these networks on resource-constrained embedded devices such as ARM Cortex-M processors. BinaryFlex offers the following contributions: (1) We present a new architecture that generates the convolution filters by using a weighted combination of orthogonal binary basis that can be generated on-the-fly very efficiently. This approach translates into not having to store the filters and therefore reduce the model's memory footprint. (2) The number of parameters needed to be updated during training is greatly reduced since we only need to update the weights and not the entire filter, leading to a faster training stage. (3) Competitive results on ImageNet at a much cheaper training cost and with a 2x smaller model than already optimized binary networks.
2 RELATED WORK
Our work is closely related to the following areas of research. CNN for Computer Vision. Deep convolutional neural networks (CNN) have been adopted for various computer vision tasks and are widely used for commercial applications. While CNNs have achieved state-of-the-art accuracies, a major drawback is their large memory footprint caused by their high number of parameters. AlexNet (Krizhevsky et al., 2012) for instance, uses 60 million parameters. Denil et al. (2013) have shown significant redundancy in the parameterization of CNNs, suggesting that training can be done by learning only a small number of weights without any drop in accuracy. Such over-parameterization presents issues in testing speed and model storage, therefore
1

Under review as a conference paper at ICLR 2018

a lot of recent research efforts have been devoted to optimizing these architecture so that they are faster and smaller.
Network Optimization. There have been many attempts in reducing model size through use of low-precision model design. Linear quantization is one approach that has been rigorously studied: One direction in quantization involves taking a pre-trained model and normalizing its weights to a certain range. This is done in Vanhoucke et al. (2011) which uses an 8 bits linear quantization to store activations and weights. Another direction is to train the model with low-precision arithmetic as in Courbariaux et al. (2014) where experiments found that very low precision could be sufficient for training and running models. In addition to compressing weights in neural networks, researchers have also been exploring more light-weight architectures. SqueezeNet (Iandola et al., 2016) uses the strategy of using 1 × 1 filters instead of 3 × 3, reducing the model to 50× smaller than AlexNet while maintaining the same level accuracy.
Binary Networks. In binary networks, parameters are represented with only one bit, reducing the model size by 32×. Expectation BackPropagation (EBP) (Soudry et al., 2014) proposes a variational Bayes method for training deterministic Multilayer Neural Networks, using binary weights and activations. This and a similar approach (Esser et al., 2015) give great speed and energy efficiency improvements. However, the improvement is still limited as binarised parameters were only used for inference. Many other proposed binary networks suffer from the problem of not having enough representational power for complex computer vision tasks, e.g. BNN (Courbariaux & Bengio, 2016), DeepIoT (Yao et al., 2017), eBNN (McDanel et al., 2017) are unable to support the complex ImageNet dataset. In this paper, we also considered the few binary networks that are able to handle the ImageNet dataset, i.e. BinaryConnect and BWN. BinaryConnect (Courbariaux et al., 2015) extends the probabilistic concept of EBP, it trains a DNN with binary weights during forward and backward propagation, done by putting a threshold for real-valued weights. Binary-Weight-Networks (BWN) is able to achieve same accuracies as the full-precision AlexNet on the complex dataset ImageNet.

3 BINARYFLEX

3.1 CONVOLUTION USING BINARY ORTHOGONAL BASIS
CNNs for image classification applications perform convolution operation between input images and kernels learned detect features in the input image. Kernels in the first layers usually decompose the input into a set of coarse features (e.g. edges, corners, arches) while deeper layer kernels capture details of more complex features present in the dataset (e.g. human faces, car wheels) (Zeiler & Fergus, 2013). Formally, the discrete convolution operation centered in pixel pi,j between a W × H image I and a m × n kernel F , where in general m  W and n  H is defined as:

mn

A(i, j) = Iim,j,n  F =

Iim,j,n(w, h)F (w, h)

w=0 h=0

(1)

where Iim,j,n denotes the m × n patch I centered around pixel pi,j. The activation map A contains the real-valued responses of F evaluated for all (i, j)  I. Intuitively, the convolution between an image and a kernel results in an activation map where higher response values correspond to regions where the feature represented in the kernel is present. The kernel parameters are learned through back-propagation (LeCun et al., 1989) process where the network updates the kernels in order to minimize a predefined cost function that measures the discrepancy between true input image labels and network's predictions.
In this paper we explore and design CNN architectures that use linear combination of deterministic binary basis to approximate kernels and study their performance on challenging tasks such as ImageNet dataset. Some of the immediate benefits of such architecture would be: (1) The number of parameters that need to be updated during training would be dramatically reduced and (2) the memory requirements of our model becomes flexible in the sense that the application exchange storing parameters with runtime computations. In (1), because the filters are weighted combinations of known basis and these are shared all across the network, the training problem is reduced to obtain the weight associated to for each base. In other words, learn the contribution of each base to each of

2

Under review as a conference paper at ICLR 2018

the convolution filters. This makes our model flexible (2) when deploying it to devices with memory constrains like an ARM Cortex-M7 which provides 512 KB of RAM.

The N-dimensional binary vectors we use as basis are orthogonal to each other and therefore their linear combinations can be used to approximate an arbitrary vector from RN . More formally, we
can measure the quality of our kernel approximation as:

K
Ek = |fk - fk| = | wibi - fk| <
i=0

(2)

Where K is the total number of basis to use in order to approximate kernel fj, bi is a base and wi  R its associated weight. is the difference between the the approximated, fk and the real kernel, fk. Intuitively,  0 as we increase the number of binary basis, K. In 3.2 we will describe how these basis are generated and why it is beneficial for our flexible model with on-the-fly kernel
generation.

3.2 GENERATION OF BINARY ORTHOGONAL BASIS
The binary filters that are used as basis for expressing our convolution filters are generated using Orthogonal Variable Spreading Factor (OVSF). OVSF codes were originally proposed and adopted by 3GPP for use in W-CDMA based 3G mobile cellular systems 1 to provide multi-user access. In these systems the signal from each user occupies the entire available bandwidth and the orthogonality of the codes assigned uniquely to each user allows the desired signal to be recovered from the interference signal of all other users.

C2,1=(1,1) C1,1=(1)
C2,2=(1,-1)

SF=1

SF=2

C4,1=(1,1,1,1) C4,2=(1,1,-1,-1) C4,3=(1,-1,1,-1) C4,4=(1,-1,-1,1)
SF=4

C8,1=(1,1,1,1,1,1,1,1) C8,2=(1,1,1,1,-1,-1,-1,-1) C8,3=(1,1,-1,-1,1,1,-1,-1) C8,3=(1,1,-1,-1,-1,-1,1,1) C8,5=(1,-1,1,-1,1,-1,1,-1) C8,6=(1,-1,1,-1,-1,1,-1,1) C8,7=(1,-1,-1,1,1,-1,-1,1) C8,8=(1,-1,-1,1,-1,1,1,-1)
SF=8

Figure 1: Code Tree for OVSF Code Generation
Figure 1 shows the procedure for generating OVSF codes of different lengths as a recursive process in a binary tree (Adachi et al., 1997). Assuming code C with length N on a branch, the two codes on branches leading out of C are derived by arranging C in (C, C) and (C, -C). It is clear from this arrangement that C can only be defined for N s that are powers of 2.
Efficient hardware implementations of OVSF code generators have been crucial for designing power-efficient cellular transceivers and given the maturity of W-CDMA standard is a well-studied problem in the wireless community (Andreev et al., 2003; Kim et al., 2009; Rintakoski et al., 2004; Purohit et al., 2013). Availability of such designs makes OVSF codes a suitable choice for efficient on-device deep learning where memory and power are at a premium.
3.3 TRADING COMPUTE FOR MEMORY: ON-THE-FLY BASIS GENERATION
BinaryFlex architecture enables a great deal of flexibility in terms of amount and type of memory required during inference. This flexibility comes from the fact that the generation of binary basis is deterministic and repeatable. An implementation can therefore choose not to hold any of the basis and/or kernel coefficients in memory and generate them on-the-fly when required. This is particularly fitting for on-device deep learning applications as the base generation can be achieved using simple hardware logic on silicon.
13GPP TS 25.213, v 3.0.0, Spreading and modulation (FDD), Oct. 1999

3

Under review as a conference paper at ICLR 2018

On-the-fly base generation can be done with high granularity. The application can maintain just a subset of basis in memory or given that OVSF codes are conventionally generated recursively from the code tree described in 3.2, it can store only parts of the basis and generate the remaining parts when required.
When memory footprint is not a concern the application can cache kernel coefficients and make BinaryFlex behave exactly similar to conventional CNNs while a middle-ground approach would be to only store the basis in memory and combine them to recover the kernels. The decision of which approach to take can either be made at design time based on the specification and requirements of the target device but a smart implementation can mix-and-match these approaches at run-time depending on the amount of available resources at inference time.
The deterministic nature of basis generation also gives hardware implementations flexibility in terms of the type of memory that can be used. For on-device learning on very resource-stringent devices this approach allows the implementation to move a big portion of network parameters into ROM instead of RAM resulting in much smaller silicon area and power consumption.

3.4 BINARYFLEX ARCHITECTURE OVERVIEW
Our BinaryFlex architecture, Figure 2, is inspired by SqueezeNet(Iandola et al., 2016) and ResNet(He et al., 2015). Macroarchitecturally, it resembles SqueezeNet in the sense that after the initial convolution and max-pooling layers, the rest of the pipeline is comprised of 3 cascades of modules or blocks separated by max-pooling layers. The final elements are a convolutional and pooling layer followed by a softmax stage. Microarchitecture wise, Binaryflex resembles ResNet in the sense the flow of date in the building blocks follow the same pattern as in ResNet's bottleneck architecture. In BinaryFlex we name this building block FlexModule.
Input

Image
Conv
Max-pool
Sparse 1
ReLu
Sparse 2
ReLu
Sparse 3
ReLu

Max-pool
Sparse 4
ReLu
Sparse 5
ReLu
Sparse 6
ReLu

Max-pool
Sparse 7
ReLu
Sparse 8
ReLu
Conv Avg-pooling
Softmax

Label
Figure 2: BinaryFlex architecture.

Convolution ReLu

OVSF Bases Generation
+

Weights

Convolution ReLu

Filter Bank

Convolution

Output
Figure 3: FlexModule overview.

BinaryFlex is comprised of eight FlexModules and no fully connected layer aiming to reduce the number of parameters in our model. We adopted SqueezeNet's strategy of late down-sampling that in summary consists in using stride s = 1 in each of the convolution layers in our FlexModules and perform downsampling after each cascade of those modules (i.e. after blocks Sparse3, Sparse6 and Sparse8). This microarchitecture enables the convolutional layers to have large activation maps. Delaying the downsampling in CNNs leads to better accuracy in certain architectures (He & Sun, 2014).
In isolation, a FlexModule looks like Figure 3. Each convolution layer uses a different set of filters from the filter bank. The three sets of filters are generated by doing a linear combination of the orthogonal binary basis form the OVSF generator using the per-base weights. During training, the weights are sequentially updated after each back-propagation stage and the number of basis to use in order for each filter to be kept fixed. In our BinaryFlex implementation, a per-FlexModule OVSF basis-generator is not needed since the basis are deterministic and we only have to decide how many basis to combine in order to create each filter. Further explanation on how the basis are generated and how they are efficiently used during inference can be found in 3.2 and 3.3, respectively.

4

Under review as a conference paper at ICLR 2018
4 EVALUATION
In this section, we evaluate BinaryFlex's performance when compared to binary networks on image classification tasks. We first present results on different vision problems to demonstrate the generalizability of BinaryFlex. Then we present a detailed study of BinaryFlex's performance on ImageNet, followed by a case study of BinaryFlex's flexibility in memory footprint. The main findings are:
· BinaryFlex is comparable in accuracy under common image datasets. In particular, under ImageNet BinaryFlex accuracies dominate the low end but only lags the high end by 17%.
· Our work demonstrates, at least for image datasets, deterministic kernels are a viable option and fairly high accuracy models result with key performance benefits from model architecture view.
· Binaryflex offers the best accuracy to memory ratio previously seen. One BinaryFlex model is able to support ImageNet at a reasonable accuracy of 56.5% while being just 1.6MB in size.
· BinaryFlex can be easily tuned to trade-offs between computation and architecture representation size, in order to match the memory hierarchy of target platform for good performance. The breadth of inference compute and memory size is much wider than breadth offered by a full set of constrained models, with model size ranging from 1MB to 9MB.
4.1 RESULTS
To train BinaryFlex on ImageNet, we trained BinaryFlex models using learning rate 0.1, decade factor 0.1 per 30 epochs and batch size 64 and 128 for ImageNet and the other datasets respectively. For comparison, we use the accuracies reported in their original paper. On ImageNet, BinaryFlex is compared to BinaryConnect and BinaryNeuralNet; otherwise, BinaryFlex is compared to BinaryConnect and BNN. BinaryConnect (Courbariaux et al., 2015) is a binarization method that trains neural network with binary weights during forward and backward propagation. BinaryNeuralNet (BNN) (Courbariaux & Bengio, 2016) is a binarization method that constraints both weights and activations to binary values. Three BinaryFlex models of different sizes are trained on ImageNet, with names appended by sizes in MB, e.g. BinaryFlex-3.6.
4.1.1 ACCURACY
We find that the accuracy of BinaryFlex on Imagenet is comparable with that of best binary models when size constraints of BinaryFlex are relaxed, but BinaryFlex can also be made a lot smaller, for instance BinaryFlex-1.6 gives a 4.5× reduction in model size but only gives up 7% in accuracy.
ImageNet. In Table 1, we see BinaryFlex's ability to offer good ratios of size to accuracy which are not otherwise possible. On the one hand, if size constraints are removed, we find that BinaryFlex model can achieve comparable accuracy, as in the case of BinaryFlex-3.4, which is 21% lower in accuracy but with a 42% smaller size compared with BWN. On the other hand, BinaryFlex can be made a lot smaller, as BinaryFlex-1.6 gives a 4.5× reduction in model size but only gives up 7% in accuracy. This small model size of 1.6MB is of critical importance for hardware implementations as many processors (e.g. ARM Cortex processor M7) only allows a maximum combined ROM/RAM space of 2MB. By fitting into ROM/RAM of embedded devices without paging to SD card, BinaryFlex offers great computational advantage, as the timescales of data movement from SD card to RAM can be 1250× longer than that from ROM to RAM (Gregg, 2013).
Other datasets. Table 2 shows that BinaryFlex gives competitive results in a general setting, despite not having its architectures optimized for each task. On MNIST, BinaryFlex gives the best accuracy amongst baselines. The BinaryFlex models implemented vary slightly across the datasets, it is a model of size 1.6MB on MNIST and 5.9MB on CIFAR10 and SVHN.
4.1.2 CASE STUDY: PERFORMANCE TRADE-OFFS OF BINARYFLEX UNDER IMAGENET.
In this section we elaborate on the performance of BinaryFlex model on ImageNet, as our main goal is to improve the performance of binary architecture on large-scale dataset. There two things we want to emphasize, the flexibility of different model sizes and good memory trade-off. First, by
5

Under review as a conference paper at ICLR 2018

Model BinaryConnect Binary-Weight-Network BinaryFlex-3.4 BinaryFlex-2.2 BinaryFlex-1.6

Top-1 Accuracy (%) 35.4 56.8 40.4 36.5 31.5

Top-5 Accuracy (%) 61.0 79.4 65.7 61.9 56.5

Model Size (MB) 7.8 7.8 3.4 2.2 1.6

Table 1: Accuracies of binary networks on ImageNet.

BinaryConnect Binarized Neural Network
BinaryFlex

CIFAR10 90.1 89.9 85.0

MNIST 98.7 99.0 99.4

SVHN 97.7 97.2 92.4

Table 2: Accuracy of BinaryFlex and baselines on three simple classification datasets, shown in %.

adjusting the number of basis used for each filter generation, we can easily find a model that meets the memory constraint of any device which the model is going to be deployed on. Second, it is very important to take into account both the accuracy and model size when training models, since we do not want to compromise the accuracy too much while reducing the model size. And so, we introduce the concept of accuracy to size ratio (ASR) to evaluate the trade-off between accuracies and model sizes. As shown in Figure 4, BinaryFlex not only has wide range of model sizes, from 9MB to 1MB but also maintains high ASR, ranging from 10 to 35(%/MB), when the model size gets reduced.
4.1.3 CASE STUDY: FLEXIBILITY OF BINARYFLEX
In this case study, we further emphasize the benefit attributed to other on-the-fly generation. The overhead of on-the-fly generation is extremely low. To generate basis with dimension n, it only requires 2 × (2log2n - 1) operations. And with dedicated hardware, the whole generation can be done in just one operation cycle. As a result, on-the-fly generation gives rise to substantial memory footprint reduction with just little overhead. In Figure 5, we analyzed the model size and number of Mult-Adds resulting from different number of basis used for filter generation and percentage of basis that are generated during runtime. We examined three different generation configurations: 1) 100% generation ­ All the binary basis are generated during runtime. 2) 50% generation ­ Half of the binary basis are generated during during runtime and other half are stored in the memory and retrieved during filter generation. 3) No generation ­ All the basis are stored in the memory. The on-the-fly generation results in 5X to 10X memory with negligible compute overhead. The gain is even more pronounced for models with large size. This suggests that in scenarios where we have deploy large model on a resource-limited device, we can apply our method to reduce the model size without compromising the accuracy. In other words, we can determine the fraction of basis to be generated during runtime based on the memory and compute resource on the device.
5 DISCUSSION
Here we further describe the main contributions of BinaryFlex and why we believe they are relevant.
Our network architecture is flexible. We could define this as a 2-dimensional property in the sense that it can be split into two distinctive components: (1) reduction of the model size by enabling on-the-fly generation of basis and (2) reduction in the number of forward operations depending on the number of basis we use to generate each filter. The first component could be defined as FOT F (TRAM , modelMB), where subscript OT F stands for on-the-fly, TRAM is the available RAM in the target platform and modelMB is the size of the BinaryFlex model. This property was first described in 3.3. Figure 6 provides an intuition on how BinaryFlex could be deployed into platforms with limited on-chip memory. The model accuracy is not being affected by FOT F .
We define the second flexibility component as FOP (TCOMP , A ), where TCOMP is the compute capability of the target platform T and A is the maximum allowed accuracy error allowed in application A (i.e. image classification). Intuitively, the FOP flexibility component enables BinaryFlex to run on compute-constrained devices without having to modify the network architecture, at the
6

Under review as a conference paper at ICLR 2018

Mult-Add (Millions)

3500 2625 1750
875 0 0

4 8 12 Model Size (MB)

BinaryFlex SqueezeNet BinaryConnect/XNOR-Net MobileNet
16

Figure 4: Size and number of operations of BinaryFlex configurations.

Mult-Add (Millions)

400,000 300,000 200,000 100,000
0 0

100% generation 50% generation No generation
67 133 200 267 333 400 Model Size (MB)

Figure 5: Size and number of operations of BinaryFlex configurations.

cost of reducing the model accuracy. We believe that, unlike other methods that limit their efforts to reduce the complexity of the operations (e.g. by using binary operations), this characteristic of BinaryFlex, F = {FOT F , FOP }, would lead to a new type of CNNs specifically designed to perform well in a broad range of constrained devices.
In this work we have focused on reducing already small CNNs, in the order of a few MBs, to sub-MB models by avoiding to store the filters of each convolutional layer. This characteristic of BinaryFlex is equally applicable to the other side of spectrum, big models like AlexNet(240MB) or VGG(540MB). This would enable training much larger models while keeping their memory footprint at an order of magnitude less. Figure 5 visually represents this idea.
So far, we have discussed why BinaryFlex's property of on-the-fly basis generation leads to a reduction in the model size. In addition to this, our approach dramatically reduces the number of parameters that need to be updated during each training iteration. Figure 7 compares the number of parameters learned in each architecture for the ImageNet classification task. Because BinaryFlex only needs to store the weights for each basis and not the entire filters, the amount of parameters is reduced by an order of magnitude. Implicity to this property is that, since the size of a BinaryFlex model depends on the number of basis used to generate the filters and not on the dimensions of this filters, FlexModules in our architecture enables the usage of arbitrarily big filters without translating this into an increase in model memory footprint. We find this novel approach to be the to-go option to enable training with more and bigger filters that better capture relevant image features.
6 CONCLUSION
We have introduced BinaryFlex, a small-memory, flexible and accurate binary network which learns through a predefined set of orthogonal basis. We have shown that BinaryFlex can be trained on
7

Under review as a conference paper at ICLR 2018

No. of Parameters (Millions)

% bases 100

Stored bases

On-the-fly bases

70 52.5

35

40 17.5

TRAM

Size (MB) modelMB

0 ResNet

AlexNet BinnaryConnect XNOR BinaryFlex

Figure 6: Scenario where the model doesn not fit in the Figure 7: Num. of parameters updated during Im-

RAM of the target platform.

ageNet training.

ImageNet, CIFAR10, MNIST and SVHN with good accuracy and size. On ImageNet, BinaryFlex models are able to reduce the size by 4.5× without great compromise in accuracy. Further, we have introduced techniques such as on-the-fly basis generation, allowing powerful management of inference-time overhead on low-resource devices.
REFERENCES
F. Adachi, M. Sawahashi, and K. Okawa. Tree-structured generation of orthogonal spreading codes with different lengths for forward link of ds-cdma mobile radio. Electronics Letters, 33(1):27­28, Jan 1997. ISSN 0013-5194.
Boris D. Andreev, Edward L. Titlebaum, and Eby G. Friedman. Orthogonal code generator for 3g wireless transceivers. In Proceedings of the 13th ACM Great Lakes Symposium on VLSI, GLSVLSI '03, pp. 229­232, New York, NY, USA, 2003. ACM. ISBN 1-58113-677-3. doi: 10.1145/764808.764868.
Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR, abs/1602.02830, 2016.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Low precision arithmetic for deep learning. CoRR, abs/1412.7024, 2014.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. CoRR, abs/1511.00363, 2015.
Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. CoRR, abs/1306.0543, 2013.
Steve K Esser, Rathinakumar Appuswamy, Paul Merolla, John V. Arthur, and Dharmendra S Modha. Backpropagation for energy-efficient neuromorphic computing. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 1117­1125. Curran Associates, Inc., 2015.
Brendan Gregg. Systems Performance: Enterprise and the Cloud. Prentice Hall Press, Upper Saddle River, NJ, USA, 1st edition, 2013. ISBN 0133390098, 9780133390094.
Kaiming He and Jian Sun. Convolutional neural networks at constrained time cost. CoRR, abs/1412.1710, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016.
8

Under review as a conference paper at ICLR 2018
S. Kim, M. Kim, C. Shin, J. Lee, and Y. Kim. Efficient implementation of ovsf code generator for umts systems. In 2009 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, pp. 483­486, Aug 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. Curran Associates, Inc., 2012.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541­551, December 1989. ISSN 0899-7667. doi: 10.1162/neco.1989.1.4.541.
Bradley McDanel, Surat Teerapittayanon, and H. T. Kung. Embedded binarized neural networks. In Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks, EWSN 2017, Uppsala, Sweden, February 20-22, 2017, pp. 168­173, 2017.
G. Purohit, V. K. Chaubey, K. S. Raju, and P. V. Reddy. Fpga based implementation and testing of ovsf code. In 2013 International Conference on Advanced Electronic Systems (ICAES), pp. 88­92, Sept 2013.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016.
T. Rintakoski, M. Kuulusa, and J. Nurmi. Hardware unit for ovsf/walsh/hadamard code generation [3g mobile communication applications]. In 2004 International Symposium on System-on-Chip, 2004. Proceedings., pp. 143­145, Nov 2004.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 963­971. Curran Associates, Inc., 2014.
Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed of neural networks on cpus. In Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.
Shuochao Yao, Yiran Zhao, Aston Zhang, Lu Su, and Tarek F. Abdelzaher. Compressing deep neural network structures for sensing systems with a compressor-critic framework. CoRR, abs/1706.01215, 2017.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013.
9

