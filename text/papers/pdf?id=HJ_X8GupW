Under review as a conference paper at ICLR 2018
MULTI-LABEL LEARNING FOR LARGE TEXT CORPORA USING LATENT VARIABLE MODEL WITH PROVABLE GUARANTEES
Anonymous authors Paper under double-blind review
ABSTRACT
Here we study the problem of learning labels for large text corpora where each document can be assigned a variable number of labels. The problem is trivial when the label dimensionality is small and can be easily solved by a series of one-vs-all classifiers. However, as the label dimensionality increases, the parameter space of such one-vs-all classifiers becomes extremely large and outstrips the memory. Here we propose a latent variable model to reduce the size of the parameter space, but still efficiently learn the labels. We learn the model using spectral learning and show how to extract the parameters using only three passes through the training dataset. Further, we analyse the sample complexity of our model using PAC learning theory and then demonstrate the performance of our algorithm on several benchmark datasets in comparison with existing algorithms.
1 INTRODUCTION
Multi-label learning for large text corpora is an upcoming problem in Large-Scale Machine Learning. Unlike the multi-class classification where a document is assigned only one label from a set of labels, here a document can have a variable number of labels. A basic approach to the problem is to use 1-vs-all classification by training a single binary classifier for every label. If the vocabulary size of a text corpus is D and the label dimensionality is L, then the 1-vs-all model require O(DL) parameters. Most of the text corpora have moderate to high vocabulary size (D), and 1-vs-all classification is feasible as long as D L. However, as the number of labels increases to a point when L  D, the size of the parameter space increases to O(D2), and we can no longer store the 1-vs-all models in the memory Yu et al. (2014). A fairly intuitive approach to reducing the size of the parameter space is to use low-rank models. The existing low-rank approaches in the literature are mainly based on discriminative models that use a mapping  : RD  RL between the word space and the label space. If the rank of the model is restricted to K D, then such mappings have the form Z = HW , where W  RD×K and H  RL×K contain the low-rank features of the words and the labels respectively. The parameter space of such models has the size  ((D + L)K), which is much smaller than O(DL) for large values of L and can be conveniently stored the memory. Amongst the recent literature on discriminative low-rank models, WSABIE Weston et al. (2011) defines weighted approximate pair-wise rank (WARP) loss on the mapping and optimises the loss on the training dataset. LEML Yu et al. (2014) generalises the loss function to squared-loss, sigmoid loss or hinge loss, which are typical of Linear Regression, Logistic Regression, and Linear SVM respectively. Both of LEML and WSABIE use gradient descent to optimise the loss on training datasets and therefore are susceptible to overfitting. Also, both of them run several iterations through the dataset and can slow down for large datasets containing millions of text instances. Here we propose a latent variable model for multi-label learning. Our model can be viewed as a generative counterpart of the discriminative low-rank models like WSABIE or LEML. Unlike the usual cases where such latent variable models are trained using EM algorithm, we use Spectral Learning or Method of Moments Anandkumar et al. (2014) to extract the parameters. Method of Moments is an upcoming non-iterative approach for latent variable based models, and has successfully been applied in Spherical Gaussian Model Hsu & Kakade (2013), Hidden Markov Models
1

Under review as a conference paper at ICLR 2018
1:N 1:D 1:K 1:L

Figure 1: Latent Variable Model

(Hsu et al. (2012) and Song et al. (2010)), Bayesian Non-parametrics Tung & Smola (2014), Topic Models Anandkumar et al. (2012), and in various NLP applications (Dhillon et al. (2012) and Cohen et al. (2014)). We first derive a closed form expression to estimate the model parameters using the Method of Moments. Further, we derive the convergence bounds on the estimated parameters, and then compare the performance of our algorithm with the discriminative low-rank models both from the perspective of accuracy and computation time. There are other approaches beyond low-rank mappings, such as FastXML Prabhu & Varma (2014) that uses tree-based hierarchies for the labels, or SLEEC Bhatia et al. (2015) that uses kNN based embedding of the texts in the training dataset. However, these approaches use a combination of different algorithms and are geared towards end-to-end solutions. Whereas, we look into the problem from a modelling perspective and limit our discussion to the low-rank models since they have a similar model complexity to the latent variable model.

2 METHODOLOGY
We build our model based on the "bag of words" assumption on the documents taking into account only the occurrence of the words, not their counts. Let us assume that there are N labeled documents in a corpus with vocabulary size D and label dimensionality L. Then using a latent variable h that assumes K states, the probability of a label l being assigned to a document d can be expressed as:

K
P [l|d] = P [l|h = k]P [h = k|d]

(1)

k=1

where the conditional probability of a document d given h can be modelled from the set of distinct words Wd present in it as:

P [d|h = k] =

P [v|h = k]

1 - P [v|h = k]

vWd

v/Wd

From here, P [h = k|d] can be expressed using Bayes Theorem as:

(2)

P [h = k|d] =

P [h = k]P [d|h = k]

K k=1

P

[h

=

k]P

[d|h

=

k]

(3)

The latent variable model is described by the plate notation in Figure 1, where the words are denoted by v, the labels by l and the latent variable by h. Our model parameters are P [h], P [v|h] and P [l|h], and the number of parameters is  ((D + L)K), same as the discriminative low-rank models.

Let us define k  [0, 1] as the probability of the latent variable h assuming the state k  [K].

k = P [h = k]

(4)

Let us define µk  [0, 1]D as the probability vector of a word v conditional to the latent state k  [K].

µk = P [v|h = k]

(5)

2

Under review as a conference paper at ICLR 2018

Similarly, let us define k  [0, 1]L as the probability vector of a label l conditional to the latent

state k  [K].

k = P [l|h = k]

(6)

One key difference of our parameterization from Anandkumar et al. (2012) and Wang & Zhu (2014)

is that they define {µk}kK=1 as the expected count of the words (E[v|h = k]) for each topic k, while

we the

ddeoficunme {enµtks}ukKs=in1gaEs qthueatpioronb2a,bwilhitiychofisthneotwpoorsdssib. lAe

probabilistic formulation enables us to model using expectations. We later use the document

probabilities to predict the labels for a test document (Section 4). Therefore, unlike Anandkumar

et al. (2012) or Wang & Zhu (2014) that uses moments on the word counts defined on the domain

p RD (for pth order moment), we use moments based on the joint probability of the words defined on the domain p[0, 1]D. We outline the derivation for the probability moments in this section, and

then show that our formulation achieves a tighter convergence bound on the parameters in Section

3.1. The label parameters {k}kK=1 are unique to our model.

2.1 MOMENT FORMULATION

We first try to formulate the joint probability of a pair of words in terms of  and µ. Let us assume that we choose two words w1 and w2 from a document at random. The term P [w1 = vi, w2 = vj] represents the probability by which any pair of words picked at random from a document turns out to be vi and vj, and it is nothing but P [vi, vj], where i, j  [D]. Similarly, P [w1 = vi] is same as P [vi], and the same holds for P [w2 = vj]. Following the generative model in Figure 1, any two words w1 and w2 in a document are conditionally independent given h. Therefore,
P [w1 = vi, w2 = vj]
K
= P [w1 = vi, w2 = vj|h = k]P [h = k]
k=1 K
= P [w1 = vi|h = k]P [w2 = vj|h = k]P [h = k]
k=1 K
= P [vi|h = k]P [vj|h = k]P [h = k]
k=1 K
= kµkiµkj
k=1
where µki is the ith element of the vector µk and so on.

K
 P [vi, vj] = kµkiµkj i, j  [D]
k=1

(7)

If M2  [0, 1]D×D is the matrix P [vi, vj] i, j  [D], then it can

containing the joint be expressed as:

probability

of

the

pairs

of

words

with

M2i,j

=

K
M2 = kµkµk
k=1

(8)

Similarly, if the tensor M3  [0, 1]D×D×D is defined as the moment containing the joint probability of a triplet of words with M3i,j, = P [vi, vj, v], i, j,   [D], then

K
M3 = kµk  µk  µk
k=1
where  stands for the tensor product.

(9)

3

Under review as a conference paper at ICLR 2018

Finally, we define ML  [0, 1]L×D×D as the probability moment of a label occurring with a pair of words such that ML,i,j = P [l, vi, vj],   [L] and i, j  [D]. Then,

K
ML = kk  µk  µk

(10)

k=1

2.2 CLOSED FORM EXPRESSION OF THE PARAMETERS

We derive the equations for extracting , µ and  in this section. The first step is to whiten the matrix M2, where we try to find a matrix low rank W such that W M2W = I. This is similar to the whitening in ICA Hyva¨rinen et al. (2004), with the covariance matrix being replaced by the cooccurrence probability matrix. The whitening is usually done through singular value decomposition of M2. If the K maximum singular values of M2 are {k}kK=1, and {k}Kk=1 are the corresponding left singular vectors, then the whitening matrix of rank K is computed as W = -1/2, where  = 1|2| . . . |K and  = diag (1, 2, . . . , K ). Upon whitening M2 takes the form

W M2W = W

KK
kµkµk W =

kW µk

k=1

k=1

kW µk

K
= µ~kµ~k = I (11)
k=1

Hence µ~k = kW µk are orthonormal vectors. Multiplying M3 along all three dimensions by W , we get

K
M~3 = M3(W, W, W ) = k(W
k=1

µk)  (W

µk)  (W

µk )

=

K k=1

1k

µ~k



µ~k



µ~k

(12)

M~ 3 is a tensor in the domain RK×K×K . Upon the factorisation of M~3, if eigenvectors are {k}kK=1 and {uk}kK=1 respectively, then k = 1k = k

the eigenvalues = k-2, and

and

 uk = µ~k = kW

µk

=

1 k

W

µk

(13)

Hence, {µk}Kk=1 can be recovered as µk = kW uk, where W  = W W W -1 is the pseudo-

inverse of W . P [v|h = k] =
sation.

Since we

µkv v µkv

,

it

compute P [v|h] by normalising µk as suffices to compute µk = W uk as k

will

be

cancelled

during

normali-

It is possible to compute  through the factorisation of second and third order moments of the labels in a similar way to µ. However, there is no guarantee that the label probabilities of k will correspond to the same topic as the word probabilities in µk for every k when we compute them separately. Without the topic alignment between µ and , label prediction is not possible.

To overcome this limitation, we use the cross-moment ML to compute . If we multiply ML twice by W , then

K
ML(W, W ) = kk 

W

µk



W

µk

K
= k 

 k W

µk



 k W

µk

k=1

k=1

K
= k  µ~k  µ~k

(14)

k=1

If uk is the kth eigenvector of M~3, then

K

uk ML(W, W )uk = µ~k ML(W, W )µ~k = µ~k

k  µ~k  µ~k µ~k = k

(15)

k=1

since µ~ks are orthonormal. Computing k by this method ensures that k and µ~k correspond to the same topic for every k, which in turn ensures that k and µk correspond to the same topic.

4

Under review as a conference paper at ICLR 2018

3 PARAMETER ESTIMATION

So far we have shown how to extract the parameters from the global moments M2, M3 and ML defined on population. In practice, we cannot compute the population parameters; all we can do is to estimate them from the sample in hand and compute an error bound for the estimation. We denote the estimated values of M2, M3, ML, , µ, , W and U from the sample as M^2, M^3, M^L, ^, µ^, ^, W^ and U^ , conforming with the notations used in the previous literature. If we take into account only the occurrence of each distinct word in a document, then using "one hot encoding," we can represent the words as a binary vector x  {0, 1}D, and the labels as another y  {0, 1}L. If there are N samples, then we can represent the words in the entire corpus as X  {0, 1}N×D, and the associated labels as Y  {0, 1}N×L. The pairwise counts of the words can be estimated by X X, whose sum of all elements is,

DD
(X
v1=1 v2=1

D DN

ND D

N

X )v1,v2 =

xi,v1 xi,v2 =

xi,v1 xi,v2 =

nnz(xi)2

v1=1 v2=1 i=1

i=1 v1=1 v2=1

i=1

where xi is the row of X corresponding to the ith document, xi,v is the vth element in it (v  [D]), and nnz(xi) is the number of non-zero elements in xi, i.e., the number of distinct words in the ith document.

Therefore, the joint probability matrix of the words (M2) can be estimated as,

M^ 2 =

N i=1

1 nnz(xi)2

X

X

(16)

Similarly, the triple-wise occurrences of the words can be estimated by the tensor X  X  X, and the sum of all of the elements of this tensor is

DDD

N

(X  X  X)v1,v2,v3 =

nnz(xi)3

v1=1 v2=1 v3=1

i=1

Therefore, M3 can be estimated as

M^ 3 =

N i=1

1 nnz(xi)3

X



X



X

(17)

The dimensions of M^2 and M^3 are D2 and D3 respectively. But in practice, these quantities are extremely sparse. We do not need to compute M^3 in practice. If the whitening matrix computed through the Singular Value Decomposition of M^2 is W^ , then M~3 can be estimated straight away using Equation 12 as,

M~^ 3 =

N i=1

1 nnz(xi)3

X W^



X W^



X W^

(18)

Since M^~3 has a dimension of K3 and K D, it can be conveniently stored in the memory. The computation of M^2 takes one pass through the dataset, and the computation of M^~3 takes another. Also, the counts of the labels occurring with the pairs of words can be estimated by the tensor Y  X  X, whose sum of all element is

LD D

N

(Y  X  X)l,v1,v2 = nnz(yi)nnz(xi)2

l=1 v1=1 v2=1

i=1

where yi represents the ith row of Y and nnz(yi) is the number of labels associated with the ith document. Therefore, ML can be estimated as,

M^ L =

N i=1

nnz

1 (yi)nnz(xi)2

Y

X

X

(19)

5

Under review as a conference paper at ICLR 2018

Algorithm 1 Three-pass Algorithm for Parameter Extraction

Input: Sparse Binary Data X  {0, 1}N×D, Labels Y  {0, 1}N×L and K  Z+ Output: ^, P^[v|h] and P^[l|h]

1. Estimate M^2 = (X X)/

N i=1

nnz(xi)2

(pass # 1)

2. Compute K maximum singular values of M^2 as {k}kK=1, and corresponding left singu-

lar vectors as {k}kK=1. Define  = 1|2| . . . |K , and  = diag (1, 2, . . . , K )

3. Estimate the whitening matrix W^ = -1/2  RD×K

4. Estimate M~^3 = (XW^  XW^  XW^ )/

N i=1

nnz(xi)3

(pass # 2)

5. Compute eigenvalues {^k}kK=1 and eigenvectors {u^k}Kk=1 of M~^ 3. Assign U^ =

[u^1|u^2| . . . |u^K ]

6. Estimate µ^k = W^ u^k, where W^  = W^ (W^ 7. Estimate

W^ )-1 , and ^k = ^-k 2, k  1, 2 . . . K.

P^[v|h = k] =

µ^kv v µ^kv

,

k



1

.

.

.

K,

v



v1

.

.

.

vD

8. Estimate ^ = Y (pass # 3)
9. Estimate

(XW^ U^ ) · (XW^ U^ ) , where · stands for the element-wise product

P^[l|h = k] =

^ lk l ^lk

,

k



1

.

.

.

K,

l



l1

.

.

.

lL

If kth eigenvector of M~^3 is u^k, then from Equation 15

^k = u^k M^ L(W^ , W^ )u^k

=

N i=1

nnz

1 (yi)nnz(xi)2

u^k

Y  XW^  XW^

u^k

=

Z

1 (X,

Y

)

Y



XW^ u^k

 XW^ u^k

=

1 Z(X, Y

)Y

(XW^ u^k) · (XW^ u^k)

where · stands for the element-wise product, and Z(X, Y ) =

N i=1

nnz

(yi)nnz(xi)2

is

a

constant.

If we assign ^ = [^1|^2| . . . |^K ], then it can be computed as,

^ = [^1|^2| . . . |^K ]

=

1 Z(X, Y

)Y

XW^ u^1 · XW^ u^1

XW^ u^2 · XW^ u^2

. . . XW^ u^K · XW^ u^K

=

1 Z(X, Y

)Y

(XW^ U^ ) · (XW^ U^ )

(20)

where U^ = [u^1|u^2| . . . |u^K ]. Since we estimate P [l|h] by normalising the columns of ^ as P^[l|h =

k] be

=canc^elll^klelkd,

it is sufficient to compute out during normalisation.

^ = Y Instead

(XW^ U^ ) · (XW^ U^ of computing all the

) as the constant Z(X, Y ) will ^ks separately, we can compute

^ in one step by just one pass through the entire dataset. The overall method takes three passes

through the dataset as outlined in Algorithm 1. There is no need to compute M^L explicitly since ^ can be computed straight away from X, Y and the intermediate parameters.

6

Under review as a conference paper at ICLR 2018

3.1 CONVERGENCE BOUNDS

Theorem 1. Let us assume that we run Algorithm 1 on N i.i.d. samples with word vectors

x1, x2 . . . xN and label vectors y1, y2 . . . yN . Let us define 1 =

1+

log(1/) 2

and 2 =

1+

log(2/) 2

for some   (0, 1). If N  max(n1, n2, n3), where

1. n1 = c2

log K + log log

K c1

·

max min

2. n2 = 

1 2 d~2sK (M2)

2

3. n3 = 

K2

+10
d~2sK (M2)5/2

22 d~3sK (M2)3/2

21

for some constants c1 and c2, then following bounds on the estimated parameters hold with probability at least 1 - ,

||µk - µ^k||  ||k - ^k||  |k - ^k| 

160 1(M2) d~2sK (M2)5/2

+

32 21(M2) d~3sK (M2)3/2

+

4 1(M2) d~2sK (M2)



160 d~2sK (M2)7/2

+

32 2 d~3sK (M2)5/2

+

2+ 2 d~2sK (M2)2



200 K (M2)5/2

+

40 2 K (M2)3/2

d~3s1 N

1 N

21 N

+

d~ls

K

82 (M2

 )N

The terms 1(M2) . . . K (M2) are the K largest eigenvalues of the matrix M2, whereas d~2s = E nnz(x)2 , d~3s = E nnz(x)3 and d~ls = E nnz(y)nnz(x)2 , with nnz(x) representing the number of distinct words and nnz(y) representing the number of labels present in an instance. The proof is included in the appendix.

The convergence bounds on  and µ are very similar to those in Anandkumar et al. (2012) and Wang & Zhu (2014), except for the terms d~2s, d~3s and d~ls in the denominator. These terms arise in our convergence bounds since we use the probability moments, whereas Anandkumar et al. (2012) and Wang & Zhu (2014) use the moments of word counts. The parameter  is unique to our model, although its bound contains those terms too.
We need at least one document with 3 distinct words or more to construct the third order moment M3, i.e., nnz(x)  3 for at least one document. Therefore, d~2s = E nnz(x)2 > 1 and d~3s = E nnz(x)3 > 1. Also, since every document has least one label, nnz(y)  1 for all the documents, and therefore, and d~ls > 1. In practice, these terms can be much larger than 1 for any real-life text corpus (The estimates of them for different experimental datasets are in Table 1). Therefore, our algorithm achieves a tighter convergence bound than Anandkumar et al. (2012) or Wang & Zhu (2014). Further, since the terms d~2s and d~3s appear in the denominators of n2 and n3 too, using probability moments also lowers the minimum number of samples required.

3.2 COMPUTATIONAL COMPLEXITY

The bottleneck of the algorithm is the whitening step of M^2, especially for the large datasets. M^2 computed using Equation 16 is a symmetric p.s.d. matrix with very high sparsity. The number of

elements of M^2 is O

N i=1

nnz(xi)2

, with the worst case occurring when no two documents have

any word in common, and every element in X X is 1. The top K eigenvalues for large symmetric

matrices are usually computed through Lanczos method. For a sparse matrix of size D × D with

O

N i=1

nnz(xi

)2

number of elements, this step has a complexity of O

DK2 Mahadevan (2008).

N i=1

nnz(xi)2

K

+

7

Under review as a conference paper at ICLR 2018

We do not compute the third order tensors M^3 or M^L explicitly. The step to compute M~^3 using

Equation 18 has a complexity of O(N K3), while the computation of ^L using Equation 20 has a

complexity of O

N i=1

nnz(yi)

K2

.

We carry out the tensor factorisation of M~^3



RK×K×K

using the Tensor Toolbox Bader et al. (2015), and this step has a complexity of O(K4 log ) to

compute each of the K eigenvalues up to an accuracy of Kolda & Mayo (2011). These steps

contribute the most to the computational cost. The overall time complexity of Algorithm 1 is

O

N i=1

nnz(xi

)2

K+

D+

N i=1

nnz(yi)

K2

+ NK3

+ K4 log

.

As per as space requirement is concerned, the storage of M^2 takes up O

N i=1

nnz

(xi

)2

space,

whereas the word parameters like W^ and µ^1:K take up a space of (DK), the label parameter ^

takes up a space of (LK), and M~^3 takes up a space of (K3). The overall space complexity is

O

N i=1

nnz(xi)2

+

(D

+

L)K

+

K3

.

4 LABEL PREDICTION

Once we have extracted the model parameters P^[h], P^[v|h] and P^[l|h] using Algorithm 1, a new test document d with a distinct set of words Wd can be modelled as,

P^[d|h = k] =

P^[v|h = k]

1 - P^[v|h = k]

vWd

v/Wd

This step will take D multiplications involving floating point operations. However, since v P^[v|h = k] = 1 for v  {v1 . . . vD} and D can be very large, the values of P^[v|h = k]
are much less than one for the most, and 1 - P^[v|h = k]  1. If we threshold P^[v|h = k] when v / Wd using some threshold  (say  = 0.01 or 0.001), then P^[d|h = k] takes the form:

P^[d|h = k] =

P^[v|h = k]

1 - P^[v|h = k]

vWd

v/Wd P^[v|h=k]>

Usually only a few of P^[v|h = k] will be greater than the threshold, and this will significantly reduce the number of multiplications when v / Wd without sacrificing accuracy. Also, since a document has only a few distinct words, i.e. |Wd| D, the left multiplicand (when v  Wd) accounts for only a few multiplications. From there, we can estimate the following using Bayes rule.

P^[h = k|d] =

^k P^[v|h = k]

1 - P^[v|h = k]

P^[h = k]P^[d|h = k]

K k=1

P^[h

=

k]P^[d|h

=

k]

=

vWd
K
^k

v/Wd P^[v|h=k]>
P^[v|h = k]

1 - P^[v|h = k]

k=1 vWd

v/Wd P^[v|h=k]>

And finally, the label probabilities for the document (P^[l|d]) can be estimated from P^[l|h] and P^[h|d] using Equation 1.

5 EXPERIMENTAL RESULTS
We carry out our experiments on six datasets ranging from small datasets like Bibtex with 4, 880 training instances to large datasets like WikiLSHTC with more than 1.7M training instances. The description of the datasets are listed in Table 1. We categorise the datasets into three groups:
1. Small: Bibtex and Delicious 2. Medium: Wiki-31K and NYTimes 3. Large: AmazonCat and WikiLSHTC

8

Under review as a conference paper at ICLR 2018

Name

# of

Train

Points

Bibtex

4,880

Delicious 12,920

Wiki-31K 14,146

NYTimes 14,669

AmazonCat 1,186,239

WikiLSHTC 1,778,351

# of Test Points 2,515 3,185 6,616 15,989 306,782 587,084

Feature Dimen-
sion 1,836 500 101,938 24,670 203,882 1,617,899

Label Dimen-
sion 159 983 30,938 4,185 13,330 325,056

Average # of
Features 68.67 18.29 669.05 373.91 71.09 42.15

Median # of
Features 69 6 513 354 45 30

Average # of Labels 2.4 19.02 18.64 5.40 5.04 3.19

Median # of Labels 2 20 19 5 4 2

d^~2s 5,957 1,892 559K 175K 6,301 2,210

d~^3s 597K 363K 545M 95M 651K 135K

d~^ls 15K 24K 11M 755K 27K 7,594

Table 1: Description of the Datasets

True Labels "airlines and airplanes", "hijacking", "terrorism" "armament, defense and
military forces", "civil
war and guerrilla warfare", "politics and government"

LEML "airlines and airplanes" (0.34), "terrorism" (0.30), "united states international relations" (0.27), "elections" (0.22), "armament, defense and military forces" (0.18), "internationalrelations" (0.18), "bombs and explosives" (0.15), "murders and attempted murders " (0.13), "biographical information" (0.13), "islam"
(0.12) "civil war and guerrilla warfare" (0.62),
"united states international relations" (0.39), "united states armament and defense" (0.23), "armament, defense and
military forces" (0.23), "internationalrelations" (0.17), "oil (petroleum) and gasoline" (0.11), "surveys and series" (0.10),"military action" (0.09), "foreign aid" (0.08), "independence
movements" (0.08)

MoM "terrorism" (0.12), "united states international relations" (0.08), "airlines and airplanes" (0.07), "world trade center (nyc)" (0.07), "hijacking" (0.07), "united states armament and defense" (0.07), "pentagon building" (0.03), "bombs and explosives" (0.03), "islam" (0.02),
"missing persons " (0.02) "civil war and guerrilla warfare" (0.09),
"united states international relations" (0.09), "united states armament and
defense" (0.06), "politics and government" (0.04), "armament, defense
and military forces" (0.03), "internationalrelations" (0.02), "immigration and refugees" (0.02), "foreign aid" (0.02), "terrorism" (0.02), "economic conditions and trends" (0.02)

Table 2: Examples of label prediction from the NYTimes dataset. The numbers in parenthesis are the scores for the top 10 labels. The scores of LEML and MoM have different ranges.

Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in Yu et al. (2014), we benchmark the performance of our method against LEML. Also, there are other methods proposed on topic based embedding of the labels, most recent of which is Rai et al. (2015) that extends Latent Dirichlet Allocation Blei et al. (2003) to Multi-label learning, and uses Bayesian Learning through Gibbs Sampling. However, use of Gibbs sampler limits the use of the algorithms only to the datasets of limited size. The largest dataset used in Rai et al. (2015) is EurLex that is similar to NYTimes dataset in size. Using any MCMC based sampling scheme is not viable for the large datasets such as AmazonCat or WikiLSHTC containing millions of training instances. In our experiments, we chose to measure AUC (of Receiver Operator Characteristics) against the latent dimensionality (K). AUC is a versatile measure and has been used in a range of problems from binary classification to ranking. Also, it is shown that there exists a one-to-one relation between AUC and Precision-Recall curves in Davis & Goadrich (2006), i.e., the same algorithm achieving a higher AUC will also produce a better Precision-Recall curve. We carried out our experiments on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 16GB memory, and no multi-threading or any other performance enhancement method is used in the code.1 For the label prediction step of MoM, we chose  = 0.001. For LEML, we ran ten iterations for the small datasets and five iterations for the medium and large datasets, since the authors of LEML chose a similar number of iterations for their experiments in Yu et al. (2014). For AmazonCat and WikiLSHTC datasets, we ran LEML on an i2.4xlarge instance of Amazon EC2 with 122 GB of memory since LEML needs significantly larger memory for these two datasets (Figure 2).
1To be shared later
9

Under review as a conference paper at ICLR 2018

AUC

AUC

0.92

0.9

0.88

MoM LEML

0.86

Memory (GB)

2.5

0.87

2.5

MoM

2

0.86

LEML

2

Memory (GB)

AUC

1.5

MoM

0.85

1

LEML

0.84

1.5 MoM 1 LEML

0.5

0.83

0.5

0.8450

100 150 050

100 150

(a) AUC and Memory (GB) of Bibtex Dataset

0.985 0.98

MoM LEML

4 3.5
3

MoM LEML

Memory (GB)

0.975

2.5 2

AUC

0.8250

100 150 050

100

(b) AUC and Memory (GB) of Delicious Dataset

0.93 0.92

MoM LEML

5 MoM
4 LEML

Memory (GB)

3

0.91

2

150

0.9750 100 150 1.550

100

(c) AUC and Memory (GB) of NYTimes Dataset

0.98

40

150

0.950

100 150 150

100

(d) AUC and Memory (GB) of Wiki-31K Dataset

0.88

50

150

Memory (GB)

0.978 0.976 0.974

30
MoM LEML 20

0.972 0.9750

MoM LEML
100 150

1050

100 150

(e) AUC and Memory (GB) of AmazonCat Dataset

AUC

Memory (GB)

0.86

40

0.84

30

MoM LEML

0.82 0.850

MoM LEML
100 150

20 1050

100 150

(f) AUC and Memory (GB) of WikiLSHTC Dataset

Figure 2: AUC and Memory vs Latent State Dimensionality(K) for LEML and MoM for Different Datasets

AUC

Dataset Bibtex Delicious

LEML FigurMeoM2: AUC aSpneedd-Mup (einmo) ry vs. Lraattheenrt tDhainmoepntsimioiznianlgitayny(Kta)rget function. It is not suscep-

160s.

300s.

60s.Dataset 150s.

0.53
LEML 0.4

MoM

tOibnlethteSopooetvheederr--ufihptatin(n×dg,,)MwohMichhiassetvhiedernetqufriroemmeitnst

performance. N (K2)

NYTimes Wiki-31K AmazonCat WikiLSHTC

1 hourBibtex 6 min.

160s.

31h3ouhroDu4r0emliicn io1uh1so5urm1in5 min 60s.

> 2 daNysY times3 hours 1 hour

10 15 10 16

300s.

on not

twheornkumif0bN.5er3<ofd(oKcu2m).eHntoswinevtehre,

training set, and it will for smaller text corpora

150s. where N <0.4(K2) hold, 1-vs-all classifiers are usually

6

min.

sufficient reduction

tteo1c0hpnrieqduiecst

the for

labels. We need large text corpora

dimensionality where 1-vs-all

 Runtime on i2.4xlargeWinsitkanic-e3o1fKAmazon 3ECh2r. 40 min.

15 min. classifiers fa1il5, and MoM provides a very competitive choice

TABLE 3:ATmraaiznoinngCTatime and13Spher.ed-up 1 hr. 15 mifno.r such cas1e0s.

WikiLSHTC >2 days 

3 hr. Reference1s6

toohnpeleyrpaatthrioarmenese,tpeiatrssis.seSseinmthcberoaMrurgaohsMsitnhcgeolnytrsaipisnatsriRnaoulglfnedolt,ianmtalaynesdeloitTnnctaeaoiabn2rle.ea4xaltxg3rslaei:alcbyrtrgTaaebirleclianisntiann[1gc]eToJv2i.mof7c7WAa0eeb.msutloaanrzy, oSin.mBaEgeenCg2aion,noantadtioNn.,"UisnunIiJeCr,A"IW, vsaobl.ie1:1S,c2a0li1n1g,

up to large pp. 2764­

scaled up in any parallel eco-system using linear algebra libraries. In our implementation, we used Matlab's linear

[2] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, "Large-scale video classification with convolutional neural

adsoalingdmWmMtheBpnbteaeerloroannetattihscthnLsiloonii,LnNbEmdcgarEoMa.noprMrpdfuypHLoLetMrreo.braefwdaaMtoopnseeremdeAmvoadaenMUeMatryon,neCnocdtptwMesaaLtfkhrphoAaweheeelrnPasl.rrevAfe.ellteowixoC.vzrpneeeKamtrrteghr/iyrAocoeevionrrmRtem.enbtProesuopAytnmuufiCtntsdbesnKidoemif,rcstoohihauflreolamtOffKhroAloe(yr1tunUr,/=tgatChbpshineu{Niawtonsn5ngem)td0h,aep7llme5[rd3,ef]a1omt0ranaTtiomno0sne.gprted,Nwei,yct1Pvd.osmoa2rfRkltsoo.5tauseid,o8b",rne8inmtli1nc,snp,R5enfaPrAeoo0irccr.t.ono}erm1cCgtone-nuh2e(o-isla,dFttaimoifipo-nvilprnbggaMe.,ebsfur1r2eosao5al0r,fc7d1egMtP­ho4t.i2oec,2nS0upIr)i8mmEgpis.,Es.ye2anE1tsaBh0t7itc,c1g2icoo2roal5nanto.n­nshfdi1essfiir7sfitMeL3acnc2.taEcka.hteSineMoeotsetnnyl,"LdymCveMoorlaumsace,nccpush"udhs-SitneteartVliesiatsiirconan-l

dtdiaMennaismslmoiMotdttitaMafseestonohtnder,tercaetsMaheedoittopeao.intonasifTfiinertantmhaihntAlms,imiTeietsmysabeetosh(chbatehtKeeoezhtlcoraoesmeswo)nkrAwm3asCpefhrU.otaneiaeahrtCtrnheeembdsxcidpo,yntooorominfatssraLocteseatLniEfeibtdnaoeEhdiMaitnwlMenitfdtaargLyLonLshemm.oEmotofsFduopMoM,oeotrrrivicseeomLrner.mcercitW-laiTzheoufitsaaenhdestitnttrkiisteionalnFiyn1gtLtgawi.rMcoSaaititFnhltnioHghneotrsLoriiT.razrEnlliiaaaHCttngMrthhtiige,omnoeLtnewignLrt,.mmEeeveMsde[[rioL54,u]]famtLsaolHSfiRenoEnktat..agre-hrneAeFMWnwrew.sgdiioennYrirbLtsgagmuhllwidpa,wCzaamaoPrWioleg.gtn,imrheilJAdleedsoaimm,eo."ifnnidMGtiWtis,shttnasuehePiaepotoPnb.,eatfng.rKaM2os,ldl0acaeaYtr1ebIab,wt.ne3feestadPtl,olaeson,isrp:rnrd,as"npMgbRead.dIshit.ent1iauiooscoS3f,IfyonM.­fCagmats2enDhMlr4mderhoW.Ltetei2Moa,lwnnol2ovkd.rntlnosfiiVdedn,l,d.agnsi"Wrnt3aLmihat1astieadad,aerhrv,egfn2se,"era0LerWM-at1tsiwitE4ocsceuse.nablthMrltaiaeiCl-iobrllmLcoaineedobnuneflpelfteirhl-ererlenaaancbsrcneeeess-l

6 CONCLUSION
Here we propose a latent variable model for multi-label learning in large-scale text corpus and show how to extract the model parameters based on the spectral decomposition of the probability moments of the words and the labels. Our method gives similar performance in comparison with state-of-art algorithms like LEML while taking a fraction of time and memory for the medium and the large datasets. Our method takes only three passes through the training dataset to extract all

10

Under review as a conference paper at ICLR 2018

the parameters, which contributes to its superior time performance. Also, the memory requirement

of our method is nominal when compared to the existing algorithms, and it scales to large datasets

like WikiLSHTC containing millions of Wikipedia articles just on a single node with 16GB of

memory. Lastly, since our method consists of only linear algebraic operations, it is embarrassingly

parallel and can easily be scaled up in any parallel ecosystem using linear algebra libraries. In our

implementation, we used Matlab's linear algebra library based on LAPACK/ARPACK, although we

did not incorporate any parallelisation.

LEML has a bound of O has a convergence bound

1o/fON1/onNthe

training loss Yu et al. (2014), whereas our method (MoM) on the model parameters w.r.t. the number of training in-

stances N . However, when we compute AUC on the test dataset, the AUC of LEML decreases with

the latent dimensionality(K) for three datasets including AmazonCat with more than a million of

training instances. It shows the possibility of overfitting in LEML. MoM, on the other hand, is not an

optimisation algorithm, and the parameters are extracted through spectral decomposition of matrices

and tensors, rather than optimising any target function. It is not susceptible to over-fitting, which

is evident from its performance. But MoM has the requirement of N  (K2), and will not work

when N < (K2). However, for smaller text corpora where N < (K2) holds, 1-vs-all classifiers

are usually sufficient to predict the labels. We need low-rank models for large text corpora where

1-vs-all classifiers fail, and MoM provides a very competitive choice for such cases.

REFERENCES
Anima Anandkumar, Yi-kai Liu, Daniel J Hsu, Dean P Foster, and Sham M Kakade. A spectral algorithm for latent dirichlet allocation. In Advances in Neural Information Processing Systems, pp. 917­925, 2012.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15: 2773­2832, 2014. URL http://jmlr.org/papers/v15/anandkumar14b.html.
Brett W. Bader, Tamara G. Kolda, et al. Matlab tensor toolbox version 2.6. Available online, February 2015. URL http://www.sandia.gov/~tgkolda/TensorToolbox/.
Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. Sparse local embeddings for extreme multi-label classification. In Advances in Neural Information Processing Systems, pp. 730­738, 2015.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, March 2003. ISSN 1532-4435.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle H. Ungar. Spectral learning of latent-variable pcfgs: algorithms and sample complexity. Journal of Machine Learning Research, 15(1):2399­2449, 2014.
Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In Proceedings of the 23rd international conference on Machine learning, pp. 233­240. ACM, 2006.
Paramveer S. Dhillon, Jordan Rodu, Michael Collins, Dean P. Foster, and Lyle H. Ungar. Spectral dependency parsing with latent variables. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju Island, Korea, pp. 205­213, 2012.
Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: Moment methods and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science, ITCS '13, pp. 11­20, 2013. ISBN 978-1-4503-1859-4.
Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460­1480, 2012.
Aapo Hyva¨rinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John Wiley & Sons, 2004.

11

Under review as a conference paper at ICLR 2018

Tamara G. Kolda and Jackson R. Mayo. Shifted power method for computing tensor eigenpairs. SIAM Journal on Matrix Analysis and Applications, 32(4):1095­1124, October 2011. doi: 10. 1137/100801482.
Sridhar Mahadevan. Fast spectral learning using lanczos eigenspace projections. In AAAI, pp. 1472­1475, 2008.
Yashoteja Prabhu and Manik Varma. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 263­272. ACM, 2014.
Piyush Rai, Changwei Hu, Ricardo Henao, and Lawrence Carin. Large-scale bayesian multi-label learning via topic-based label embeddings. In Advances in Neural Information Processing Systems, pp. 3222­3230, 2015.
Le Song, Byron Boots, Sajid M Siddiqi, Geoffrey J Gordon, and Alex J Smola. Hilbert space embeddings of hidden markov models. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 991­998, 2010.
Hsiao-Yu Tung and Alex J. Smola. Spectral methods for indian buffet process inference. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1484­1492, 2014.
Yining Wang and Jun Zhu. Spectral methods for supervised topic models. In Advances in Neural Information Processing Systems, pp. 1511­1519, 2014.
Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI, volume 11, pp. 2764­2770, 2011.
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit S Dhillon. Large-scale multi-label learning with missing labels. In ICML, volume 31, 2014.

A MATRIX NORM INEQUALITIES

Here we paraphrase the inequalities regarding the matrix norms from Anandkumar et al. (2014),

which we will use further in our final proof. Let the true pairwise probability matrix and the third

order probability moment be M2 = p(v, v) and M3 = p(v, v, v), where v stands for the words. Let us assume that we select N i.i.d. samples x1, . . . xN from the population, and the estimates

of pairwise matrix and third order moment are M^2 = p^(y, y) and M^3 = p^(y, y, y). Let M2 =

||M2 - M^ 2||2. We K (M2)/2, where

use K

the second is the Kth

order operator norm of the largest eigenvalue of M2.

matrices We will

here. Let derive the

ucsonasdsiutimoneswMh2ich

satisfies this later.

If  = diag(1, 2 . . . K ) are the top-K eigenvalues of M2, and U are the corresponding eigenvectors, then the whitening matrix W = U -1/2. Also, W M2W = IK×K . Then,

||W ||2 =

max eig(W W ) =

max eig(-1) =

1 K (M2)

Similarly, if W  = W (W W )-1, then W  = W  = U 1/2. Therefore, ||W ||2 = max eig() = 1(M2)

(21)

Let W^ be the whitening matrix for M^2, i.e., W^ M^2W^ = IK×K . Then by Weyl's inequality, k(M2) - k(M^ 2)  ||M2 - M^ 2||, k = 1, 2 . . . K.

Therefore,

||W^ ||22

=

1 K (M^ 2)



K

1 (M2) - ||M2

- M^ 2||



K

2 (M2)

(22)

12

Under review as a conference paper at ICLR 2018

Also, by Weyl's Theorem,

||W^ ||22 = 1(M^ 2)  1(M2) + M2  1.51(M2) = ||W^ ||2 

1.51(M2)  1.5 1(M2) (23)

Let U  be the eigenvectors of W^ M2W^ , and  be the corresponding eigenvalues. Then we can write, W^ M2W^ =U U  . Then W = W^ U -1/2U  whitens M2, i.e., W M2W = I. Therefore,

||I - ||2 = ||I - U U  ||2

= ||I - W^ M2W^ ||2

= ||W^ M^ 2W^ - W^ M2W^ ||2

 ||W^ ||22||M2 - M^2||



K

2 (M2

)

M2

(24)

W = ||W - W^ ||

= ||W - W U 1/2U  ||2

= ||W ||2||I - U 1/2U  ||2

= ||W ||2||I - 1/2||2

 ||W ||2||I - ||2



K

2 (M2

)3/2

M

2

(25)

W  = ||W  - W^ ||2 = ||W^ U 1/2U  - W^ ||2 = ||W^ ||2||I - U 1/2U  ||2



||W^ ||2||I

-

||2



2 1(M2 K (M2)

)

M2

(26)

B TENSOR NORM INEQUALITIES

Let us define the second order operator norm of a tensor T  RD ×D×D with D, D  Z+ as,

||T ||2 = sup{||T (·, u, u)|| : u  RD&||u|| = 1}
u

(27)

Lemma 1. For a tensor T  RD ×D×D, ||T ||2  ||T ||F , where ||T ||F is the Frobenius norm defined as,

||T ||F =

(Ti,j,k )2

i,j,k

Proof. The tensor T can be unfolded as an array of D matrices each of size D × D, as T = [T1, T2 . . . TD ]. Then,
T (·, u, u) = u T1u, u T2u, . . . u TD u  RD

13

Under review as a conference paper at ICLR 2018

Therefore,

||T ||22 = sup [u T1u, u T2u, . . . u TD u] 2
||u||=1

= sup |u T1u|2 + |u T2u|2 + · · · + |u TD u|2
||u||=1

 sup |u T1u|2 + sup |u T2u|2 + · · · + sup |u TD u|2

||u||=1

||u||=1

||u||=1

(28)

Let us assume that the singular value decomposition of Td has the form (d  [D ]),
Td = 111 + 222 + · · · + DDD wthheesrieng{ulia}rDiv=a1luaerse. the left singular vectors, {i}iD=1 are the right singular vectors and {i}Di=1 are Each of [1, 2 . . . D] and [1, 2 . . . D] forms a spanning set in RD, and any vector u  RD can be spanned using both 1:D and 1:D as the basis. Let u = 11 + 22 + · · · + DD = 11 + 22 + · · · + DD, where 1:D and 1:D are scalars. Since ||u|| = < u, u > = 1,

|||| = 12 + 22 + . . . D2 = 1 and |||| = 12 + 22 + . . . D2 = 1

Therefore, |u Tdu| = |111 + 222 . . . DDD|  12 + 22 + · · · + D2 1212 + 2222 . . . D2 D2 (Cauchy-Schwartz Inequality)
 12 + 22 + · · · + D2 12 + 22 · · · + D2 12 + 22 · · · + D2
= 12 + 22 + · · · + D2 = ||Td||F

Equality holds when Td is of rank 1. Since this holds for any vector u  RD such that ||u|| = 1, sup |u Tdu|  ||Td||F for d  [D ]. Therefore, from Equation 28,
||u||=1
||T ||22  ||T1||F2 + |T2||F2 + · · · + ||TD ||F2 = ||T ||F2 = ||T ||2  ||T ||F

Lemma 2. (Robust Power Method from (Anandkumar et al., 2014)) If T^ = T + E  RK×K×K ,

where T is an symmetric tensor with orthogonal decomposition T =

K k=1

k

uk

 uk



uk

with

each k > 0, maxkK=1 {k }. log log (max/

and E has operator norm Let there exist constants c1, )). Then if Algorithm 1 in

||E||2  . c2 such that (Anandkumar

Letc1m· (inm=in/mKin),Kka=n1d{Nk}ancd2(lomgaKx

= +

et al., 2014) is called for K times, with

L = poly(K) log(1/) restarts each time for some   (0, 1), then with probability at least 1 - ,

there exists a permutation  on [K], such that,

||u(k) - u^k||  8 (k) , |k - (k)|  5 k  [K]

(29)

Since

 c1 · (min/K) and k = 1k , k  [K], we need

N  c2

log K + log log

K max c1min

= c2 log K + log log

K c1

This contributes in the first lower bound (n1) of N in Theorem 1.

max min

(30)

14

Under review as a conference paper at ICLR 2018

C TAIL INEQUALITY
Lemma 3. If we draw N i.i.d. sample documents x1, x2 . . . xN , and probability mass function, pairwise probability and triplet probability of the words v estimated from these N samples are p^(v), p^(v, v) and p^(v, v, v) respectively, whereas the true probabilities are p(v), p(v, v) and p(v, v, v) respectively with v  {v1, v2 . . . vD} , then with probability at least 1 -  with   (0, 1),

||p^(v) - p(v)||F  d~1s2N ||p^(v, v) - p(v, v)||F  d~2s2N ||p^(v, v, v) - p(v, v, v)||F  d~3s2N

1+ 1+ 1+

log(1/) 2
log(1/) 2
log(1/) 2

(31) (32) (33)

where, d~1s = E [nnz(x)], d~2s = E nnz(x)2 , d~3s = E nnz(x)3 , and nnz(x) is the non-zero entries in the binary vector representing the words in the documents as described in Section 3 (main paper).

Proof. Since the number of distinct words in a document is always bounded, we can assume ||x||  1 x without loss of generality. Then from Lemma 7 of supplementary material of Wang & Zhu (2014), with probability at least 1 -  with   (0, 1),

E^[x] - E[x]

 2 FN

1+

log(1/) 2

(34)

E^[xx ] - E[xx ]  2 FN

1+

log(1/) 2

(35)

E^[x  x  x] - E[x  x  x]  2 FN

1+

log(1/) 2

(36)

where E stands for true expectation, and E^ stands for the expectation estimated from the N samples, i.e.,

E^ [x]

=

1 N

N

xi

=

1 N

X

1

i=1

E^ [xx

]

=

1 N

N

xi



xi

=

1 N

X

X

i=1

E^ [x



x



x]

=

1 N

N

xi



xi

 xi

=

1 N

X

X

X

i=1

Now, since the samples contains binary data, probability of the items can be computed from as

Similarly,

p(v) =

E[x] v E[xv]

p^(v) =

E^ [x] v E^[xv]

(37) (38)

15

Under review as a conference paper at ICLR 2018

D
Now, since x is a binary vector of dimension D, the sum across its dimensions is xv = nnz(x),

v=1

and therefore, D E[xv] = E[ D xv] = E[nnz(x)]  E^[nnz(x)] = D E^[xv]. Assigning d~1s =

v=1

v=1

E[nnz(x)], from Equation 37 and 38, we get

v=1

p^(v)

-

p(v)

=

E^[x] - E[x] d~1s

(39)

DD

Similarly, summing across the respective dimensions, we get

(xx )v1,v2

v1=1 v2=1

DD

DDD

xv1 xv2 = nnz(x)2 and

(x  x  x)v1,v2,v3 = nnz(x)3.

v1=1 v2=1

v1=1 v2=1 v3=1

=

D
Therefore,

D E[(xx )v1,v2 ] = E[nnz(x)2] = d~2s  D

D E^[(xx )v1,v2 ], and

v1=1 v2=1

v1=1 v2=1

D D D E[(x  x  x)v1,v2,v3 ] = E[nnz(x)3] = d~3s  D D D E^[(x  x  x)v1,v2,v3 ].

v1=1 v2=1 v3=1

v1=1 v2=1 v3=1

From here, we can show that,

p^(v, v) - p(v, v) = E^[xx

] - E[xx d~2s

]

p^(v, v, v)

-

p(v, v, v)

=

E^ [x



x



x] - E[x d~3s



x



x]

(40)

Plugging these equations in Equation 34, 35 and 36, we can complete the proof.

Also, if y represents the label vector of the documents, since the number of labels per documents is limited, we can assume ||y||  1 without a loss of generality. Then,

E^[y  x  x] - E[y  x  x]

=

1 N

N

yi  xi  xi - E[y  x  x]

i=1

=

1 N

N

yi



xi



xi

-

1 N

N

yi



E[x



x]

+

1 N

N

yi  E[x  x] - E[y  x  x]

i=1 i=1

i=1

=

1 N

N

yi 

xi  xi - E[x  x]

+

1 N

N

yi  E[x  x] - E[y  x  x]

i=1 i=1

Therefore,
E^[y  x  x] - E[y  x  x] F  E^[y] = E^[y]

E^[x  x] - E[x  x] F + E^[y] - E[y] E[x  x] F E^[xx ] - E[xx ] F + E^[y] - E[y] E[xx ] F

Since ||y||  1, ||E^[y]||  1. Also, from ||x||  1, we get ||E[xx ]||  ||x||2  1. Therefore,

E^[y  x  x] - E[y  x  x] F  E^[xx ] - E[xx ] F + E^[y] - E[y]

(41)

16

Under review as a conference paper at ICLR 2018

From Equation 34 and 35,

P

E^[y] - E[y]  2 FN

1+

log(1/) 2

1-

P

E^[xx ] - E[xx ]  2 FN

1+

log(1/) 2

1-

Reversing the tail inequalities,

P

E^[y] - E[y]  2 1 + FN

log(1/) 2



P

E^[xx ] - E[xx ]  2 FN

1+

log(1/) 2



If E1 and E2 are two events,
P (E1  E2) = P (E1) + P (E2) - P (E1  E2)  P (E1) + P (E2)

Therefore,

P

E^[y] - E[y] + E^[xx ] - E[xx ]  4 F FN

1+

log(1/) 2

 2

Or,

P

E^[y] - E[y] + E^[xx ] - E[xx ]  4 F FN

1+

log(1/) 2

 1 - 2

From Equation 41,

P

E^[y  x  x] - E[y  x  x]

F



4 N

1+

log(1/) 2

 1 - 2

Replacing  by /2,

P

E^[y  x  x] - E[y  x  x]

F



4 N

1+

log(2/) 2

1-

LD D

Similar to previous cases

(y  x  x)l,v1,v2 = nnz(y)nnz(x)2, and therefore,

l=1 v1=1 v2=1

L D D E[(y  x  x)l,v1,v2 ] = E[nnz(y)nnz(x)2]  E^[nnz(y)nnz(x)2]. Assigning

l=1 v1=1 v2=1

d~ls = E[nnz(y)nnz(x)2], we can show that,

p^(l, v, v)

-

p(l, v, v)

=

E^ [y



x



x] - E[y d~ls



x



x]

where l stands for the labels (l  {l1, l2 . . . lL}). Therefore, with probability at least 1 - ,

(42)

||p^(l, v, v) - p(l, v, v)||F  d~ls4 N

1+

log(2/) 2

(43)

17

Under review as a conference paper at ICLR 2018

D COMPLETING THE PROOF

Assigning 1 = 1 +

log(1/) 2

in the inequalities of Lemma 3, we get

M2 = ||M2 - M^ 2||2 = ||p^(v, v) - p(v, v)||2  ||p^(v, v) - p(v, v)||F  d~22s1N , and M3 = ||M3 - M^ 3||2 = ||p^(v, v, v) - p(v, v, v)||2  ||p^(v, v, v) - p(v, v, v)||F  d~32s1N

since operator norm is smaller than Frobenius norm for both matrices and tensors (Lemma 1).

Therefore, to satisfy M2  K (M2)/2, we need N   second lower bound (n2) of N in Theorem 1.

2

1 d~2sK (M2)

. This contributes in the

Similarly, assigning 2 = 1 +

log(2/) 2

in Equation 43, we get

ML = ||ML - M^ L||2 = ||p^(l, v, v) - p(l, v, v)||2  ||p^(l, v, v) - p(l, v, v)||F  d~l4s2N

From Appendix B in (?),

tw = ||M3(W, W, W ) - M^ 3(W^ , W^ , W^ )||2

 ||M3||2 ||W^ ||22 + ||W^ ||2||W ||2 + ||W ||22 W + ||W^ ||3M3





||M3

||2

(2

+ 2+ K(M2)

1)

W

+

K

22 (M2)3/2

M3 



||M3

||2

(3 + 2) K (M2)

·

2 K (M2)3/2

M 2

+

22 K (M2)3/2

M3



10||M3||2 K (M2)5/2

· M2

+

K

22 (M2)3/2

M3





10 d~2sK (M2)5/2

+

22 d~3sK (M2)3/2

21 N

(44)

From Lemma 1, ||M3||2  ||M3||F  1, because M3 is a tensor with individual elements as probabilities, with the sum of all elements being 1.

From Lemma 2,  c1 · (min/K), and we can assign as the upper bound of tw. To satisfy this, we need



10 d~2sK (M2)5/2

+

22 d~3sK (M2)3/2



10 d~2sK (M2)5/2

+

22 d~3sK (M2)3/2

2 N



c1

min K

,

or,

2 N

 c1 K1max

2

Since max  1, we need N  

K2

+10
d~2sK (M2)5/2

22 d~3sK (M2)3/2

2 . This contributes to

n3 in Theorem 1.

18

Under review as a conference paper at ICLR 2018

Here, we will derive the convergence bounds for the parameters. Since µk = W uk (Algorithm 1 in main paper), with probability at least 1 - ,

||µk - µ^k||

= ||W uk - W^ u^k||

= ||W uk - W u^k + W u^k - W^ u^k||

 ||W ||2||uk - u^k|| + ||W  - W^ ||2||u^k||



||W

||2

8 k

+ W 

8

1(M2)

+

2 1(M2 K (M2)

)

M2

(45)

where ||uk - u^k||  8 /k from Lemma 2, and ||u^k|| = 1. Since 1/k = k  1, assigning as the upper bound of tw in Equation 44, we can say that with probability at least 1 - ,

||µk - µ^k||  8 1(M2)



10 d~2sK (M2)5/2

+

22 d~3sK (M2)3/2

21 N

+

2 1(M2 K (M2)

)

d~22s1N



160 1(M2) d~2sK (M2)5/2

+

32 21(M2) d~3sK (M2)3/2

+

4 1(M2) d~2sK (M2)

1 N

(46)

Also,

|k - ^k| =

1 2k

-

1 ^k2

=

(k + ^k)(k - ^k) k2 ^k2

=

 2|k - ^k|  10

k^k k + ^k (k - ^k) (47)

since |k - ^k|  5 from Lemma 2. Therefore, with probability at least 1 - , we get

|k - ^k| 



200 K (M2)5/2

+

40 2 K (M2)3/2

d~3s1 N

(48)

where  = 1 +

log(1/) 2

.

Further, since  = uk ML(W, W )uk,

k - ^k

= uk ML(W, W )uk - u^k M^ L(W^ , W^ )u^k

 uk ML(W, W )uk - u^k ML(W, W )u^k + u^k ML(W, W )u^k - u^k M^ L(W^ , W^ )u^k

 uk ML(W, W )uk - u^k ML(W, W )uk + u^k ML(W, W )uk - u^k ML(W, W )u^k + u^k 2 ML(W, W ) - M^ L(W^ , W^ )

 uk - u^k ML(W, W ) 2 uk + uk - u^k ML(W, W ) 2 u^k + u^k 2 ML(W, W ) - M^ L(W^ , W^ ) 2

= 2 uk - u^k ML(W, W ) 2 + ML(W, W ) - M^ L(W^ , W^ ) 2

 2 uk - u^k W 2 ML 2 + ML(W, W ) - M^ L(W^ , W^ ) 2

 2 uk - u^k W 2 + ML(W, W ) - M^ L(W^ , W^ ) 2

(49)

since uk = u^k = 1. Also, from Lemma 1, tensor with the sum of its elements as 1.

ML 2 

ML F  1, since

ML 2 is a

19

Under review as a conference paper at ICLR 2018

Now,

ML(W, W ) - M^ L(W^ , W^ ) 2

= ML(W, W ) - ML(W^ , W^ ) + ML(W^ , W^ ) - M^ L(W^ , W^ ) 2

 ML(W, W ) - ML(W^ , W^ ) 2 + W^ 2 ML - M^ L 2

= ML(W, W ) - ML(W, W^ ) + ML(W, W^ ) - ML(W^ , W^ ) 2 + W^ 2 ML - M^ L 2

 ML(W, W ) - ML(W, W^ ) 2 + ML(W, W^ ) - ML(W^ , W^ ) 2 + W^ 2 ML - M^ L 2

 W W - W^ ML 2 + W^ W - W^ ML 2 + W^ 2 ML - M^ L 2

= ( W + W^ ) ML 2W + W^ 2ML

 ( W + W^ )W + W^ 2ML

(50)

Therefore,

k - ^k  2 uk - u^k W 2 + ( W + W^ )W + W^ 2ML  16 k W 2 + ( W + W^ )W + W^ 2ML  16 K (M2) + ( W + W^ )W + W^ 2ML
Since 1/k = k  1. Assigning as the upper limit of tw in Equation 44, with probability 1 - ,

k - ^k





16 K (M2)

10 d~2sK (M2)5/2

+

22 d~3sK (M2)3/2

21 + N

1+ 2 K (M2)

2 K (M2)3/2

N2d1~2s

+

2 K (M2)

4N2d~ls





80 d~2sK (M2)7/2

+

16 2 d~3sK (M2)5/2

+

1+ 2 d~2sK (M2)2

41 N

+

d~ls

K

82 (M2

 )N

where, 1 = 1 + 1.

log(1/) 2

and 2 =

1+

log(2/) 2

. This completes the proof of Theorem

20

