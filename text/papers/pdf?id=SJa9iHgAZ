Under review as a conference paper at ICLR 2018
RESIDUAL CONNECTIONS ENCOURAGE ITERATIVE IN-
FERENCE
Anonymous authors Paper under double-blind review
ABSTRACT
Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem.
1 INTRODUCTION
Traditionally, deep neural network architectures (eg. VGG Simonyan & Zisserman (2014), alexnet Krizhevsky et al. (2012), etc.) have been compositional in nature, meaning a hidden layer applies an affine transformation followed by non-linearity, with a different transformation at each layer. However, a major problem with deep architectures has been that of vanishing and exploding gradients. To address this problem, solutions like better activations (ReLU Nair & Hinton (2010)), weight initialization methods Glorot & Bengio (2010); He et al. (2015) and normalization methods Ioffe & Szegedy (2015); Arpit et al. (2016) have been proposed. Nonetheless, training compositional networks deeper than 15 - 20 layers remains a challenging task.
Recently, residual networks (Resnets He et al. (2016a)) were introduced to tackle these issues and are considered a breakthrough in deep learning because of their ability to learn very deep networks and achieve state-of-the-art performance. Besides this, performance of Resnets are generally found to remain largely unaffected by removing individual residual blocks or shuffling adjacent blocks Veit et al. (2016). These attributes of Resnets stem from the fact that residual blocks transform representations additively instead of compositionally (like traditional deep networks). This additive framework along with the aforementioned attributes has given rise to two school of thoughts about Resnets­ the ensemble view where they are thought to learn an exponential ensemble of shallower models Veit et al. (2016), and the unrolled iterative estimation view Liao & Poggio (2016); Greff et al. (2016), where Resnet layers are thought to iteratively refine representations instead of learning new ones.
While the success of Resnets may be attributed partly to both these views, our work takes steps towards achieving a deeper understanding of Resnets in terms of its iterative feature refinement perspective. Our contributions are as follows:
1. We study Resnets analytically and provide a formal view of iterative feature refinement using Taylor's expansion, showing that for any loss function, a residual layer naturally encourages representations to move along the negative gradient of the loss with respect to hidden representations. Each residual layer is therefore encouraged to take a gradient step in order to minimize the loss in the hidden representation space. We empirically confirm this by measuring the cosine between hidden representations and gradient of loss with respect to the hidden representations.
1

Under review as a conference paper at ICLR 2018
2. We empirically observe that Resnet layers can perform both representation learning and iterative feature refinement. Specifically in Resnets, lower residual blocks learn to perform representation learning, meaning that they change representations significantly and removing these layer can sometime drastically hurts prediction performance. The higher blocks on the other hand essentially learn to perform iterative inference, minimizing the loss function by moving along the negative gradient direction. In the presence of shortcut connections1, representation learning is dominantly performed by the compositional layers and most of residual blocks tend to perform iterative feature refinement. We also show that unrolling the top blocks (performing iterative inference) at test time, improves test accuracy of Resnet models.
3. The iterative refinement view suggests that deep network models can potentially leverage intensive parameter sharing for the layer performing iterative inference. But sharing large number of residual blocks without loss of performance has not been successfully achieved yet. Towards this end, we study the applicability of sharing residual blocks in Resnets given their iterative feature refinement view and expose reasons for the failure of sharing in Resnets and investigate a preliminary fix for this problem.
2 BACKGROUND AND RELATED WORK
Residual Networks and their analysis:
Recently, several papers have investigated the behavior of Resnets (He et al., 2016a). In (Veit et al., 2016; Littwin & Wolf, 2016), authors argue that Resnets are an ensemble of relatively shallow networks. This is based on the unraveled view of Resnets where there exist exponential number of paths between the input and prediction layer. Further, observations like shuffling and dropping of residual blocks do not affect performance significantly also support this claim. Other works discuss the possibility that residual networks are approximating recurrent network like models (Liao & Poggio, 2016; Greff et al., 2016). This view is in part supported by the observation that the mathematical formulation of Resnets bares similarity to LSTM (Hochreiter & Schmidhuber, 1997), and that successive layers cooperate and preserve the feature identity. Resnets have also been studied from the perspective of boosting theory Huang et al. (2017). In this work the authors propose to learn Resnets in a layerwise manner using a local classifier.
Our work bears critical differences compared with the aforementioned studies. Most importantly we focus on a precise definition of iterative inference. In particular, we show that a residual block approximate a gradient descent step in the activation space. Our work can also be seen as relating the gap the boosting and iterative inference interpretations since having a residual block whose output is aligned with negative gradient of loss is similar to how gradient boosting models work.
Iterative refinement and weight sharing:
Humans frequently perform predictions with iterative refinement based on the level of difficulty of the task at hand. A leading hypothesis regarding the nature of information processing that happens in the visual cortex is that it performs fast feedforward inference (Thorpe et al., 1996) for easy stimuli or when quick response time is needed, and performs iterative refinement of prediction for complex stimuli (Vanmarcke et al., 2016). The latter is thought to be done by lateral connections within individual layers in the brain that iteratively act upon the current state of the layer to update it. This mechanism allows the brain to make fine grained predictions on complex tasks. A characteristic attribute of this mechanism is the recursive application of the lateral connections which can be thought of as shared weights in a recurrent model. The above views suggest that it is desirable to have deep network models that perform parameter sharing in order to make the iterative inference view complete.
3 FORMALIZING ITERATIVE INFERENCE
Our goal in this section is to formalize notion of iterative inference in Resnets. We study the properties of representations that residual blocks tend to learn, as a result of being additive in nature, in
1A shortcut connection is a convolution layer between residual blocks useful for changing the hidden space dimension (see He et al. (2016a) for instance).
2

Under review as a conference paper at ICLR 2018

contrast to traditional compositional networks. Specifically, we consider Resnet architectures (see figure 1) where the first hidden layer is a convolution layer, which is followed by L residual blocks which may or may not have shortcut connections in between residual layers.

A residual layer applied on a representation hi transforms the representation as,

hi+1 = hi + Fi(hi)

(1)

Consider L such residual blocks stacked on top of each other fol-
lowed by a loss function. Then, we can Taylor expand any given loss function L recursively as,

L(hL) = L(hL-1 + FL-1(hL-1))

=

L(hL-1)

+

FL-1(hL-1).

L(hL-1) hL-1

+ O(FL2-1(hL-1))

=

L(hi)

+

L-1 j=i

Fj

(hj

).



L(hj hj

)

+

O(Fj2 (hj ))

(2) (3)
(4)

Notice we have explicitly only written the first order terms of each
expansion. The rest of the terms are absorbed in the higher order terms in O(.). Furthermore, the first order term is a good approximation when the magnitude of Fj is small enough. In other cases, the higher order terms come into effect as well.

Figure 1: A typical residual network architecture.

Thus in part, the loss equivalently minimizes the dot product be-

tween

F (hi)

and

L(hi hi

)

,

which

can

be

achieved

by

making

F (hi)

point

in

the opposite half

space to that

of



L(hi hi

)

.

In

other

words,

hi

+

F

(hi)

approximately

moves

hi

in

the

same

half

space

as

that

of

-



L(hi hi

)

.

The

overall

training

criteria can then be seen as approximately minimizing the dot product between these 2 terms along

a path in the h space between hi and hL such that loss gradually reduces as we take steps from

hi to hL. The above analysis is justified in practice, as Resnets' top layers output Fj has small

magnitude (Greff et al., 2016), which we also report in Fig. 2.

Given our analysis we formalize iterative inference in Resnets as moving down the energy (loss) surface. One can therefore also expect accuracy to increase gradually as the representations go through residual layers.It is also noting resemblance of function of residual block to stochastic gradient descent. We make a more formal argument for that in appendix.

4 EMPIRICAL ANALYSIS

Experiments are performed on CIFAR-10 (Krizhevsky & Hinton, 2009) and CIFAR-100 (see appendix) using the original Resnet architecture He et al. (2016b) and two other architectures that we introduce for the purpose of our analysis (described below). Our main goal is to validate that residual networks perform iterative refinement as discussed above, showing its various consequences. Specifically, we set out to empirically answer the following questions:

· Do residual blocks in Resnets behave similar to each other or is there a distinction between

blocks that perform iterative refinement vs. representation learning?

·

Is the cosine between

 L(hi ) hi

and Fi(hi) negative in residual networks?

· What kind of samples do residual blocks target?

· What happens when layers are shared in Resnets?

Resnet architectures: We use the following four architectures for our analysis:

1. Original Resnet-110 architecture: This is the same architecture as used in He et al. (2016b) starting with a 3 × 3 convolution layer with 16 filters followed by 54 residual blocks in three different
stages (of 18 blocks each with 16, 32 and 64 filters respectively) each separated by a shortcut connections (1 × 1 convolution layers that allow change in the hidden space dimensionality) inserted

3

Under review as a conference paper at ICLR 2018

||F(h)|| / ||h||

Accuracy

0.4 0.3 0.2 0.1
0

Original Resnet on CIFAR-10
train validation
10Resid2u0al blo3c0k ind4e0x 50

||F(h)|| / ||h||

Single Representation Resnet on CIFAR-10

2.0 1.5

train validation

1.0

0.5

0 R2esidua4l block i6ndex 8

||F(h)|| / ||h||

Avg-Pooling Resnet on CIFAR-10
train
2 validation

1

00

5

Resi1d0ual

b1l5ock

20
index

25

30

||F(h)|| / ||h||

0.6 0.4 0.2
0

wResnet on CIFAR-10
train validation

2

Res4idual6block8

10
index

12

Figure 2: Average ratio of 2 norm of output of residual block to the norm of the input of residual block for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-10. (Train and validation curves are overlapping.)

1.00 0.98 0.96 0.94 0.92
0

Original Resnet on CIFAR-10
validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy
1R0esid2u0al blo3c0k ind4e0x 50

Accuracy

1.0Single Representation Resnet on CIFAR-10 0.8 0.6 validation accuracy (no layer dropped)
train accuracy (no layer dropped)
0.4 validation accuracy 0.2 train accuracy
0 R2esidua4l block i6ndex 8

Accuracy

1.0 Avg-Pooling Resnet on CIFAR-10

0.8

0.6 0.4
0

validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy
5Resi1d0ual b1l5ock i2n0dex 25 30

Accuracy

0.95 0.90 0.85
0

wResnet on CIFAR-10

validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy

2

Res4idual6block8

10
index

12

Figure 3: Final prediction accuracy when individual residual blocks are dropped for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-10.

000000......011000050227050550

Original Resnet on CIFAR-10
train validation

0 1R0esidu20al blo3c0k in4d0ex 50

cosine loss

Single Representation Resnet on CIFAR-10

0.01

train validation

0.02

0.03

0.04 0 Re2sidual4block6index 8

cosine loss

000000......010000068420

Avg-Pooling Resnet on CIFAR-10
train validation

0 5Resid1u0al b1l5ock 2in0dex25 30

cosine loss

wResnet on CIFAR-10

0.02

0.04

0.06

train validation

0

2Resi4dual6block8

10
index

12

Figure

4:

Average

cos

loss

between

residual

block

F (hi)

and

 L(hi ) hi

for

(left

to

right)

original

Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-10.

0.8 0.6 0.4 0.2
0

Original Resnet on CIFAR-10 25 50 75Ep1o0c0hs125 150 175

200

iacc

Single Representation Resnet on CIFAR-10 0.8 0.6 0.4 0.2
0 20 4E0poch6s0 80 100

iacc

Avg-Pooling Resnet on CIFAR-10

0.8

0.6

0.4

0.2 0

20 4E0poch6s0 80

100

iacc

0.8 0.6 0.4
0

wResnet on CIFAR-10 20 E4p0ochs 60 80

Figure 5: Prediction accuracy when plugging classifier after hidden states in the last stage of Resnets(if any) during training for (left to right) original Resnet, single representation Resnet, avgpooling Resnet, and wideResnet on CIFAR-10. (Blue to red spectrum denotes lower to higher residual blocks)

cosine loss

iacc

after the 18th and 36th residual blocks such that the 3 stages have hidden space of height-width 32 × 32, 16 × 16 and 8 × 8. The model has a total of 1, 742, 762 parameters.
2. Single representation Resnet: This architecture starts with a 3 × 3 convolution layer with 100 filters. This is followed by 10 residual blocks such that all hidden representations have the same height and width of 32 × 32 and 100 filters are used in all the convolution layers in residual blocks as well.
3. Avg-pooling Resnet: This architecture repeats the residual blocks of the single representation Resnet (described above) three times such that there is a 2 × 2 average pooling layer after each set of 10 residual blocks that reduces the height and width after each stage by half. Also, in contrast to single representation architecture, it uses 150 filters in all convolution layers. This is followed by the classification block as in the single representation Resnet. It has 12, 201, 310 parameters. We call this architecture avg-pooling architecture. We also ran experiments with max pooling instead of average pooling but do not report results because they were similar except that max pool acts more non-linearly compared with average pooling, and hence the metrics from max pooling are more similar to those from original resnet.
4. Wide Resnet: This architecture starts with a 3 × 3 convolution layer followed by 3 stages of four residual blocks with 160, 320 and 640 number of filters respectively, and 3 × 3 kernel size in all convolution layers. This model has a total of 45,732,842 parameters.
4

Under review as a conference paper at ICLR 2018

Experimental details: For all architectures, we use He-normal weight initialization as suggested in He et al. (2015), and biases are initialized to 0. For residual blocks, we use BatchNormReLUConvBatchNormReLUConv as suggested in He et al. (2016b). The classifier is composed of the following elements: BatchNormReLUAveragePool(8,8)FlattenFully-Connected-Layer(#classes)Softmax. This model has 1, 829, 210 parameters. For all experiments for single representation and pooling Resnet architectures, we use SGD with momentum 0.9 and train for 200 epochs and 100 epochs (respectively) with learning rate 0.1 until epoch 40, 0.02 until 60, 0.004 until 80 and 0.0008 afterwards. For the original Resnet we use SGD with momentum 0.9 and train for 300 epochs with learning rate 0.1 until epoch 80, 0.01 until 120, 0.001 until 200, 0.00001 until 240 and 0.000011 afterwards. We use data augmentation (horizontal flipping and translation) during training of all architectures. For the wide Resnet architecture, we train the model with with learning rate 0.1 until epoch 60 and 0.02 until 100 epochs.
Note: All experiments on CIFAR-100 are reported in the appendix. In addition, we also record the metrics reported in sections 4.2 and 4.1 and plot them as a function of epochs (shown in the appendix due to space limitations). The conclusions are similar to what is reported below.

4.1 COSINE LOSS OF RESIDUAL BLOCKS

In this experiment we directly validate our theoretical prediction about Resnet minimizing dot

product between gradient of loss and block output. To this end compute the cosine loss

Fi

(hi

).

L(hi hi

)

Fi(hi) 2

 L(hi ) hi

.
2

A negative cosine loss and small Fi(.) together suggest that Fi(.) is refining

features

by

moving

them

in

the

half

space

of

-



L(hi hi

)

,

thus

reducing

the

loss

value

for

the

corre-

sponding data samples. Figure 4 shows the cosine loss for CIFAR-10 on train and validation sets.

These figures show that cosine loss is consistently negative for all residual blocks but especially

for the higher residual blocks. Also, notice for deeper architectures (original Resnet and pooling

Resnet), the higher blocks achieve more negative cosine loss and are thus more iterative in nature.

Further, since the higher residual blocks make smaller changes to representation (figure 2), the first

order Taylor's term becomes dominant and hence these blocks effectively move samples in the half

space of the negative cosine loss thus reducing loss value of prediction. This result formalizes the

sense in which residual blocks perform iterative refinement of features­ move representations in the

half

space

of

-

 L(hi ) hi

.

4.2 REPRESENTATION LEARNING VS. FEATURE REFINEMENT

In this section, we are interested in investigating the behavior of residual layers in terms of representation learning vs. refinement of features. To this end, we perform the following experiments.
1. 2 ratio Fi(hi) 2/ hi 2: A residual block Fi(.) transforms representation as hi+1 = hi + Fi(hi). For every such block in a Resnet, we measure the 2 ratio of Fi(hi) 2/ hi 2 averaged across samples. This ratio directly shows how significantly Fi(.) changes the representation hi; a large change can be argued to be a necessary condition for layer to perform representation learning. Figure 2 shows the 2 ratio for CIFAR-10 on train and validation sets. For single representation Resnet and pooling Resnet, the first few residual blocks (especially the first residual block) changes representations significantly (up to twice the norm of the original representation), while the rest of the higher blocks are relatively much less significant and this effect is monotonic as we go to higher blocks. However this effect is not as drastic in the original Resnet and wide Resnet architectures which have two 1 × 1 (shortcut) convolution layers, thus adding up to a total of 3 convolution layers in the main path of the residual network (notice there exists only one convolution layer in the main path for the other two architectures). This suggests that residual blocks in general tend to learn to refine features but in the case when the network lacks enough compositional layers in the main path, lower residual blocks are forced to change representations significantly, as a proxy for the absence of compositional layers. Interestingly, recent advances in image classification models point towards importance of compositional layers between stages in residual models (Huang et al., 2016).
2. Effect of dropping residual layer on accuracy: We drop individual residual blocks from trained Resnets and make predictions using the rest of network on validation set. This analysis shows

5

Under review as a conference paper at ICLR 2018

accuracy log loss entropy

1.0 0.8 0.6 0.4 0.2 0.0 14

train borderline train correct train all test borderline test correct test all

15 blo1c6k id 17

18

2.0 1.5 1.0 0.5 0.0 0.5 1.0
14

15 blo1c6k id 17

2.0 1.5 1.0 0.5 0.0 18 14

15 blo1c6k id 17

18

Figure 6: Accuracy, loss and entropy for last 5 blocks of Resnet-110. Performance on bordeline examples improves at the expense of performance (loss) of already correctly classified points (correct). This happens because last block output is encouraged by training to be negatively correlated (around -0.1 cosine) with gradient of the loss.

the significance of individual residual blocks towards the final accuracy that is achieved using all the residual blocks. Note, dropping individual residual blocks is possible because adjacent blocks operate in the same feature space. Figure 3 shows the result of dropping individual residual blocks. As one would expect given above analysis, dropping the first few residual layers (especially the first) for single representation Resnet and pooling Resnet leads to catastrophic performance drop while dropping most of the higher residual layers have minimal effect on performance. On the other hand, performance drops are not drastic for the original Resnet and wide Resnet architecture, which is in agreement with the observations in 2 ratio experiments above.
In another set of experiments, we measure validation accuracy after individual residual block during the training process. This set of experiments is achieved by plugging the classifier right after each residual block in the last stage of hidden representation (i.e., after the last shortcut connection, if any). This is shown in figure 5. The figures show that accuracy increases very gradually when adding more residual blocks in the last stage of all architectures.
4.3 BORDERLINE EXAMPLES
Since individual residual blocks in general lead to small improvements in performance, it is worth identifying qualitatively what kind of samples they constitute. Intuitively, since these layers move representations minimally (as shown by above analysis), the samples that lead to these minor accuracy jump should be near the decision boundary but getting misclassified by a slight margin. To confirm this intuition, we first define borderline samples as examples with probability margin below 10% as measured at some non-final step. We measure this predicting labels from the hidden state using frozen final Resnet classifier. We examine how loss, accuracy and entropy over these samples evolves over last 5 blocks of the network. Figure 6 shows borderline samples for Resnet110 on CIFAR-10 test and train set. Clearly for every hidden representation, a significant chunk of borderline examples get corrected by the residual blocks above it. This exposes the qualitative nature of samples that these feature refinement layers focus on, which is further reinforced by the fact that entropy decreases for all considered subsets. We also note that while train loss drops uniformly across layers, test sets loss increases after last block. Correcting this phenomenon could lead to improved generalization in Resnets, which we leave for future work.
4.4 UNROLLING RESIDUAL NETWORK
The experiments so far suggest that higher residual layers learn to perform iterative refinement of features. However, a fundamental requirement for a procedure to be truly iterative is to apply the same function. In this section we explore what happens when we unroll the last block of a trained residual network for more steps than it was trained for. Our main insight is that because cosine loss is negative also on test distribution (in all runs we observe negative cosine loss on both train and test set), it should continue to refine examples within some neighbourhood, and help correct prediction of samples.
6

Under review as a conference paper at ICLR 2018

accuracy log loss entropy

0.54 1.00 0.805

0.52

0.99 0.800

10 1

0.50 0.98

0.48

0.97 0.795 0.96

10 2

0.46 0.790

0.95

0.44 0.94 0.785 10 3

19 22 25 bl2oc8k id31 34 37

19 22 25 bl2oc8k id31 34 37

0.87 0.05

0.86 0.04

0.85 0.03

0.84 train borderline

0.02

0.83

test borderline train all

0.01

test all

0.82 19 22 25 bl2oc8k id31 34 37

Figure 7: Accuracy, loss and entropy for last 5 blocks of Resnet-110 unrolled for 20 additional steps (with appropriate scaling). Borderline examples are corrected and overall performance slightly improves. Note different scales for train and test. Curves are averaged over 4 runs of Resnet-110.

Accuracy Gradient norm ratio
||hi||1

3000 80 2500

70.0 Resnet-110 Resnet-110 naive sharing
60.0

60 2000 1500

50.0 40.0

40 1000

30.0 20.0

20 500 10.0

Resnet-32

00

Resnet-110 naive sharing
50 Ep1o00ch 150 200

0 0

50 Ep1o00ch 150

0.0
200 0 8 16Bloc2k4 ind32ex 40 48 53

Figure 8: Resnet-110 with naively shared top 13 layers of each block compared with unshared

Resnet-32. Left plot present training and validation curves, shared Resnet-110 heavily overfits.

In the right plot we track gradient norm ratio between first block in first and last stage of resnet

(i.e.

r

=

||

L h1

||/



L h1+2n

||).

Significantly

larger

ratio

in

the

naive

sharing

model

suggests,

that

the

overfitting is caused by early layers dominating learning.

We focus first on the same model as discussed in previous section, Resnet-110, and unroll the last residual block for 20 extra steps2. Figure 7 shows again evolution of accuracy of three groups of examples: borderline examples, already correctly classified and the whole dataset. There are on average 51 bordeline examples in test set3, on which performance is improved from 43% to 53%, which yields overall slight improvement in accuracy on test set. At the same time loss on test set slightly increases, which can be explained by decreased confidence of prediction on already correctly predicted examples. Loss on train set improved uniformly from 0.0012 to 0.001 (all results are averaged over 4 seeds). We observe described behavior to be generally consistent across Resnets with different number of blocks (32, 38, 56, 110). To summarize, unrolling residual network to more steps than trained on improves both loss and accuracy on train set, and accuracy on test set. Similar behavior can be observed in the last blocks of Resnet (as described in the previous section).
4.5 SHARING RESIDUAL LAYERS
Given the iterative inference view, we now study the effects of sharing residual blocks. Contrary to (Liao & Poggio, 2016) we observe that naively sharing the higher (iterative refinement) residual blocks of a Resnets in general leads to overfitting4 (especially for deeper Resnets). In particular, reducing the number of parameters in Resnet-110 (by sharing top 13 blocks in each stage) leads to 91.2% training accuracy on CIFAR-100, below that of unshared Resnet-110 (98.74%) but significantly more than Resnet-32 (87.38%). Although Resnet-110 with shared parameters has worse validation error as Fig. 8 report.
2We add scaling factor so that overall change in 2 norm of h is on average no larger than approximately 2%.
3All examples from train set have confident predictions by last block in the residual network. 4 (Liao & Poggio, 2016) compared shallow Resnets with shared network having more residual blocks.
7

Under review as a conference paper at ICLR 2018

Model
Resnet-32 Resnet-38 Resnet-110-UBN Resnet-146-UBN Resnet-182-UBN
Resnet-56 Resnet-110

CIFAR10
1.53 / 7.14 1.20 / 6.99 0.63 / 6.62 0.68 / 6.82 0.48 / 6.97
0.58 / 6.53 0.22 / 6.13

CIFAR100
12.62 / 30.08 10.04 / 29.66 7.75 / 29.94 7.21 / 29.49 6.42 / 29.33
5.19 / 28.99 1.26 / 27.54

Parameters
467k-473k 565k-571k 570k-576k 573k-579k 576k-581k
857k-863k 1734k-1740k

Table 1: Train and test error of ResNet sharing top layers blocks (while using unshared both statistics and ,  in Batch Normalization) compared to baseline ResNet of varying depth. Runs are repeated 4 times. Training ResNet with unrolled layers can bring additional gain of 0.3%, while adding marginal amount of extra parameters.

Accuracy L1 norm of last state

100

90

80

70

60

50

40 BN gamma 0.1 init

30

naive unshare BN stats

unshare BN + gamma 0.1 init

0 25 50 7E5po1c0h0 125 150 175

80 70 60 50 40 30 20 10
0 25 50 7E5po1c0h0 125 150 175

Figure 9: Ablation study of different strategies to remedy sharing leading to overfitting phenomenon in Residual Networks. Left figure shows effect on training and test accuracy. Right figure studies norm explosion. All components are important, but it is most crucial to unshare BN statistics.

Looking at the network naively sharing top layers, we observe that sharing layers make the layer activations explode during the forward propagation at initialization due to the repeated application of the same operation (Fig 8, right). Consequently the norm of the gradients also explodes at initialization (Fig. 8, center). While the Resnets network is able to cope from this phenomenon from an optimization perpective, this initial gradient explosion impacts its generalization capability (Fig. 9).
To address this generalization issue, we introduce a variant of recurrent batch normalization (Cooijmans et al., 2016), which proposes to initialize  to 0.1 and unshare statistics for every step. On top of this strategy, we also unshare  and  parameters. Tab. 1 shows that using our strategy alleviates explosion problem and leads to small improvement over baseline with similar number of parameters. We also perform an ablation to study, see figure. 9 (left), which show that all additions to naive strategy are necessary and drastically reduce the initial activation explotion.
Unshared Batch Normalization strategy therefore mitigates this exploding activation problem. This problem, leading to exploding gradient in our case, appears frequently in recurrent neural network. This suggests that future unrolled Resnets should use insights from research on recurrent networks optimization, including careful initialization (Henaff et al., 2016) and parametrization changes (Hochreiter & Schmidhuber, 1997).
5 CONCLUSION
We show that Resnets perform both representation learning and iterative refinement in different architectures and depending on depth. Our main contribution is formalizing the view of iterative refinement and showing analytically that residual blocks naturally encourage representations to move in the half space of negative loss gradient, thus implementing a gradient descent in the activation space (each block reduces loss and improves accuracy).
8

Under review as a conference paper at ICLR 2018
Qualitatively we find that this mechanism helps correcting the predictions for near misclassified samples. In addition, we observe that Resnets can also learn to perform representation learning, especially in the absence of multiple compositional (convolution) layers in the main path of the network (Eg. single representation Resnet and pooling Resnet used for our analysis). But in general we find that the representation learning behavior tends to concentrate in the first few blocks while the rest of the higher blocks perform iterative refinement of features. Finally, we surprisingly find that sharing residual blocks naively leads to overfitting. We show that this happens because of representation explosion. Empirical analysis shows that this problem is mitigated by using a variation of recurrent batch normalization with unshared parameters, which solves the problem by reducing the explosion. Iterative view suggests that top blocks of residual network could be fully shareable, and we leave this for future work. Additionally, the developed formal view could help explaining the success of other architectures using shortcut connections.
REFERENCES
D. Arpit, Y. Zhou, B. U Kota, and V. Govindaraju. Normalization propagation: A parametric technique for removing internal covariate shift in deep networks. ICML, 2016.
Tim Cooijmans, Nicolas Ballas, César Laurent, Çaglar Gülçehre, and Aaron Courville. Recurrent batch normalization. arXiv preprint arXiv:1603.09025, 2016.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Aistats, 2010.
K. Greff, R. Srivastava, and J. Schmidhuber. Highway and residual networks learn unrolled iterative estimation. arXiV, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016a.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016b.
M. Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In ICML, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997.
Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep resnet blocks sequentially using boosting theory. arXiv preprint arXiv:1706.04964, 2017.
G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely Connected Convolutional Networks. ArXiv e-prints, August 2016.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Q. Liao and T. Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiV, 2016.
E. Littwin and L. Wolf. The loss surface of residual networks: Ensembles and the role of batch normalization. arXiV, 2016.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv, 2014.
9

Under review as a conference paper at ICLR 2018 S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature, 1996. S. Vanmarcke, F. Calders, and F. Wagemans. The time-course of ultrarapid categorization: The
influence of scene congruency and top-down processing. i-Perception, 2016. A. Veit, M. Wilber, and S. Belongie. Residual networks are exponential ensembles of relatively
shallow networks. arXiV, 2016.
10

Under review as a conference paper at ICLR 2018

Appendices

A FURTHER ANALYSIS

A.1

A

SIDE-EFFECT

OF

MOVING

IN

THE

HALF

SPACE

OF

-

L(ho ho

)

Let ho = Wx + b be the output of the first layer (convolution) of a ResNet. In this analysis we

show

that

if

ho

moves

in

the

half

space

of

-

 L(ho ) ho

,

then

it

is

equivalent

to

updating

the

parameters

of the convolution layer using a gradient update step. To see this, consider the change in ho from

updating parameters using gradient descent with step size . This is given by,

ho

=

(W

-



L W

)x

+

(b

-



L )
b

-

(Wx

+

b)

= - L x -  L W b

= - L ho x + ho ho W b

= - L x 2 + 1 ho

 - L ho

(5) (6) (7) (8) (9)

Thus,

moving

ho

in

the

half

space

of

-

L ho

has

the

same

effect

as

that

achieved

by

updating

the

parameters W, b using gradient descent. Although we found this insight interesting, we don't build

upon it in this paper. We leave this as a future work.

B ANALYSIS ON CIFAR-100
Here we report the experiments as done in sections 4.2 and 4.1, for CIFAR-100 dataset. The plots are shown in figures 10, 11 and 12. The conclusions are same as reported in the main text for CIFAR-10.

C ANALYSIS OF INTERMEDIATE METRICS ON CIFAR-10 AND CIFAR-100
Here we plot the accuracy, cosine loss and 2 ratio metrics corresponding to each individual residual block on validation during the training process for CIFAR-10 (figures 13, 14, 5) and CIFAR-100 (figures 15, 16, 17). These plots are recorded only for the residual blocks in the last space for each architecture (this is because otherwise the dimensions of the output of the residual block and the classifier will not match). In the case of cosine loss after individual residual block, this set of experiments is achieved by plugging the classifier right after each hidden representation and measuring the cosine between the gradient w.r.t. hidden representation and the corresponding residual block's output.
We find that the accuracy after individual residual blocks increases gradually as we move from from lower to higher residua blocks. Cosine loss on the other hand consistently remains negative for all architectures. Finally 2 ratio tends to increase for residual blocks as training progresses.

11

cosine loss

Under review as a conference paper at ICLR 2018

0.00 Original Resnet on CIFAR-100

0.05

0.10 train

0.15

0

validation
1R0esidu20al blo3c0k in4d0ex

50

cosine loss

0.S0i1ngle Representation Resnet on CIFAR-100

0.02

0.03

0.04

train validation

0 Re2sidual4block6index 8

cosine loss

Avg-Pooling Resnet on CIFAR-100

0.02

0.04

0.06 0.08

train validation

0 R5esid1u0al b1l5ock 2in0dex25

30

cosine loss

000000......000000425163 0

wResnet on CIFAR-100
train validation
2Resi4dual6bloc8k ind1e0x 12

Figure

10:

Average

cos

loss

between

residual

block

F (hi)

and

 L(hi ) hi

for

(left

to

right)

original

Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-100.

Original Resnet on CIFAR-100 0.3 train
validation
0.2 0.1
0 1R0esid2u0al blo3c0k ind4e0x 50

||F(h)|| / ||h||

Single Representation Resnet on CIFAR-100 1.5 train
validation
1.0 0.5
0 R2esidua4l block 6index 8

||F(h)|| / ||h||

1.25 1.00

Avg-Pooling Resnet on CIFAR-100
train validation

0.75

0.50

0.25

0 5Resi1d0ual b1l5ock i2n0dex25 30

||F(h)|| / ||h||

000000......345621 0

wResnet on CIFAR-100
train validation
2Res4idual6block8 ind1e0x 12

Figure 11: Average ratio of 2 norm of output of residual block to the norm of the input of residual block for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-100. (Train and validation curves are overlapping.)

1.0 0.9 0.8 0.7
0

Original Resnet on CIFAR-100
validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy
10Resid2u0al blo3c0k ind4e0x 50

Accuracy

1.0Single Representation Resnet on CIFAR-100 0.8 0.6
validation accuracy (no layer dropped)
0.4 train accuracy (no layer dropped) validation accuracy
0.2 train accuracy 0 R2esidua4l block i6ndex 8

Accuracy

1.0 Avg-Pooling Resnet on CIFAR-100

0.8 0.6
0

validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy

5Resi1d0ual

b1l5ock

20
index

25

30

Accuracy

0.9 0.8 0.7 0.6 0.5 0

wResnet on CIFAR-100
validation accuracy (no layer dropped) train accuracy (no layer dropped) validation accuracy train accuracy
2 Res4idual6block8 ind1e0x 12

Figure 12: Final prediction accuracy when individual residual blocks are dropped for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-100.

||F(h)|| / ||h||

Accuracy

cos_loss

||F(h)|| / ||h||

Original Resnet on CIFAR-10 00000.....000010275000505
0 25 50 7E5p1o0c0h1s25150 175 200

cos_loss

0.S0i0ngle Representation Resnet on CIFAR-10 0.02 0.04 0.06 0.08 0 20 4E0poch6s0 80 100

cos_loss

Avg-Pooling Resnet on CIFAR-10 0.000 0.025 0.050 0.075 0.100 0 20 4E0poch6s0 80 100

cos_loss

000000......000010025072050505 0

wResnet on CIFAR-10 20 E4p0ochs60 80

Figure

13:

Average

cos

loss

between

residual

block

F (hi)

and

 L(hi ) hi

during

training

for

(left

to

right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-

10. (Blue to red spectrum denotes lower to higher residual blocks)

0.20 0.15 0.10 0.05 0

Original Resnet on CIFAR-10 25 50 75Ep1o0c0hs125 150 175

200

||F(h)|| / ||h||

0.8Single Representation Resnet on CIFAR-10 0.6 0.4 0.2
0 20 4E0poch6s0 80 100

||F(h)|| / ||h||

000000......100111668042 Avg-Pooling Resnet on CIFAR-10 0 20 4E0poch6s0 80 100

||F(h)|| / ||h||

000001......002468 0

wResnet on CIFAR-10 20 E4p0ochs 60 80

Figure 14: Average ratio of 2 norm of output of residual block to the norm of the input of residual block during training for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-10. (Blue to red spectrum denotes lower to higher residual blocks)

12

Under review as a conference paper at ICLR 2018

cos_loss

||F(h)|| / ||h||

iacc iacc iacc iacc

000000......000000024264

Original Resnet on CIFAR-100 0 25 50 E7p5oc1h0s0 125 150

175

cos_loss

Single Representation Resnet on CIFAR-100 00000.....0000034251
0 20 4E0poch6s0 80 100

cos_loss

Avg-Pooling Resnet on CIFAR-100 0.10 0.05 0.00 0.05
0 20 4E0poch6s0 80 100

cos_loss

0.025 0.000 0.025 0.050 0.075 0

wResnet on CIFAR-100 20 E4p0ochs60 80

Figure

15:

Average

cos

loss

between

residual

block

F (hi)

and

 L(hi ) hi

during

training

for

(left

to

right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-

100. (Blue to red spectrum denotes lower to higher residual blocks)

000000......021123000555

0

Original Resnet on CIFAR-100 25 50 E7p5och1s00 125 150

175

||F(h)|| / ||h||

Single Representation Resnet on CIFAR-100 0.5 0.4 0.3 0.2
0 20 4E0poch6s0 80 100

||F(h)|| / ||h||

0.20 Avg-Pooling Resnet on CIFAR-100 0.15 0.10
0 20 4E0poch6s0 80 100

||F(h)|| / ||h||

000001......246008 0

wResnet on CIFAR-100 20 E4p0ochs60 80

Figure 16: Average ratio of 2 norm of output of residual block to the norm of the input of residual block during training for (left to right) original Resnet, single representation Resnet, avg-pooling Resnet, and wideResnet on CIFAR-100. (Blue to red spectrum denotes lower to higher residual blocks)

Original Resnet on CIFAR-100 0.6 0.4 0.2 0.0 0 25 50 E7p5och1s00 125 150 175

Single Representation Resnet on CIFAR-100

0.6

0.4

0.2

0.0 0

20 4E0poch6s0 80

100

0.8 Avg-Pooling Resnet on CIFAR-100 0.6 0.4 0.2
0 20 4E0poch6s0 80 100

0.6 0.4 0.2
0

wResnet on CIFAR-100 20 E4p0ochs60 80

Figure 17: Prediction accuracy when plugging classifier after hidden states in the last stage of Resnets(if any) during training for (left to right) original Resnet, single representation Resnet, avgpooling Resnet, and wideResnet on CIFAR-100. (Blue to red spectrum denotes lower to higher residual blocks)

13

