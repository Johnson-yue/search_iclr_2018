Under review as a conference paper at ICLR 2018
ADAPTIVE DROPOUT WITH RADEMACHER COMPLEXITY REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-ofthe-art deep learning algorithms impose dropout strategy to prevent feature coadaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to tradeoff between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of-the-art dropout algorithms.
1 INTRODUCTION
Dropout training (Srivastava et al., 2014) has been proposed to regularize deep neural networks for classification tasks. It has been shown to work well in reducing co-adaptation of neurons--and hence, preventing model overfitting. The idea of dropout is to stochastically set a neuron's output to zero according to Bernoulli random variables. It has been a crucial component in the winning solution to visual object recognition on ImageNet (Krizhevsky et al., 2012). Ever since, there have been many follow-ups on novel learning algorithms (Goodfellow et al., 2013; Baldi & Sadowski, 2013), regularization techniques (Wager et al., 2013), and fast approximations (Wang & Manning, 2013).
However, the classical dropout model has two limitations. First, the model requires to specify the retain rates, i.e., the probabilities of keeping a neuron's output, a priori to model training. It is often not clear how to choose the retain rates in an optimal way. They are usually set via grid-search over hyper-parameter space or simply according to some rule-of-thumb, and kept consistent throughout the training process thereafter. Another limitation is that all neurons in the same layer share the same retain rate. This exponentially reduces the search space of hyper-parameter optimization. For example, Srivastava et al. (2014) use a fixed retain probability throughout training for all dropout variables in each layer.
In this paper, we propose a novel regularizer based on the Rademacher complexity of a neural network (Shalev-Shwartz & Ben-David, 2014). Without loss of generality, we use multilayer perceptron with dropout as our example and prove its Rademacher complexity is bounded by a term related to the dropout probabilities. This enables us to explicitly incorporate the model complexity term as a regularizer into the objective function.
This Rademacher complexity bound regularizer provides us a lot of flexibility and advantage in modeling and optimization. First, it combines the model complexity and the loss function in an unified objective. This offers a viable way to trade-off the model complexity and representation power through the regularizer weighting coefficient. Second, since this bound is a function of dropout probabilities, we are able to incorporate them explictly into the computation graph of the optimization procedure. We can then adaptively optimize the objective and adjust the dropout probabilities throughout training in a way similar to ridge regression and the lasso (Hastie et al., 2009). Third, our
1

Under review as a conference paper at ICLR 2018
proposed regularizer assumes a neuron-wise dropout manner and models different neurons to have different retain rates during the optimization. Our empirical results demonstrate interesting trend on the changes in histograms of dropout probabilities for both hidden and input layers. We also discover that the distribution over retain rates upon model convergence reveals meaningful pattern on the input features.
To the best of our knowledge, this is the first ever effort of using the Rademacher complexity bound to adaptively adjust the dropout probabilities for the neural networks. We organize the rest of the paper as following. Section 2 reviews some past approaches well aligned with our motivation, and highlight some major difference to our proposed approach. We subsequently detail our proposed approach in Section 3. In Section 4, we present our thorough empirical evaluations on the task of image and document classification on several benchmark datasets. Finally, Section 5 concludes this paper and summarizes some possible future research ideas.
2 RELATED WORKS
There are several prior works well aligned with our motivation and addressing similar problems, but significantly different from our method. For example, the standout network (Ba & Frey, 2013) extends dropout network into a complex network structure, by interleaving a binary belief network with a regular deep neural network. The binary belief network controls the dropout rate for each neuron, backward propagates classification error and adaptively adjust according to training data. Zhuo et al. (2015) realize the dropout training via the concept of Bayesian feature noising, which incorporates dimension-specific or group-specific noise to adaptively learn the dropout rates.
In addition to these approaches, one other family of solution is via the concept of regularizer. Wang & Manning (2013) propose fast approximation methods to marginalize the dropout layer and show that the classical dropout can be approximated by a Gaussian distribution. Later, Wager et al. (2013) show that the dropout training on generalized linear models can be viewed as a form of adaptive regularization technique. Gal & Ghahramani (2016) develop a new theoretical framework casting dropout training as approximation to Bayesian inference in deep Gaussian processes. It also provides a theoretical justification and formulates dropout into a special case of Bayesian regularization. In the mean time, Maeda (2014) discuss a Bayesian perspective on dropout focusing on the binary variant, and also demonstrate encourage experimental results. Generalized dropout (Srinivas & Babu, 2016) further unifies the dropout model into a rich family of regularizers and propose a Bayesian approach to update dropout rates.
One popular method along with these works is the variational dropout method (Kingma et al., 2015), which provides an elegant interpretation of Gaussian dropout as a special case of Bayesian regularization. It also proposes a Bayesian inference method using a local reparameterization technique and translates uncertainty of global parameters into local noise. Hence, it allows inference on the parameterized Bayesian posteriors for dropout rates. This allows us to adaptively tune individual dropout rates on layer, neuron or even weight level in a Bayesian manner. Recently, Molchanov et al. (2017) extend the variational dropout method with a tighter approximation which subsequently produce more sparse dropout rates. However, these models are fundamentally different than our proposed approach. They directly operates on the Gaussian approximation of dropout models rather than the canonical multiplicative dropout model, whereas our proposed method directly bounds the model complexity of classical dropout model.
Meanwhile, the model complexity and the generalization capability of deep neural networks have been well studied in theoretical perspective. Wan et al. (2013) prove the generalization bound for the DropConnect neural networks--a weight-wise variant of dropout model. Later, Gao & Zhou (2016) extend the work and derive a Rademacher complexity bound for deep neural networks with dropout. These works provide a theoretical guarantee and mathematical justification on the effectiveness of dropout method in general. However, they both assume that all input and hidden layers have the same dropout rates. Thus their bound can not be applied to our algorithm.
2

Under review as a conference paper at ICLR 2018

3 REGULARIZATION WITH RADEMACHER COMPLEXITY

We would like to focus on the classification problem and use multilayer perceptron as our example.
However, note that the similar idea could be easily extended to general feedforward networks. Let us assume a labeled dataset S = {(xi, yi)|i  {1, 2, . . . , n}, xi  Rd, yi  {0, 1}k}, where xi is the feature of the ith sample, yi is the one-hot class label for the ith sample, and k is the number of
classes in prediction. Without loss of generality, an L-layer multilayer perceptron with dropout can be modeled as a series of recursive function compositions. Let kl be the number of neurons of the lth layer. In particular, the first layer takes sample features as input, i.e., k0 = d, and the last layer outputs the prediction, i.e., kL = k.

We denote Wl  Rkl-1×kl as the linear coefficient matrix from the (l - 1)th layer to the lth layer, and Wil be the ith column of Wl. For dropout, we denote l  [0, 1]kl as the vector of retain rates for the lth layer. We also define rl  {0, 1}kl as a binary vector formed by concatenating kl independent Bernoulli dropout random variables, i.e., rjl  Bernoulli(jl ). To simplify our notation, we further refer W:l = {W1, . . . , Wl}, r:l = {r0, . . . , rl}, :l = {0, . . . , l}, W = W:L, r = r:(L-1), and
 = :(L-1).

For an input sample feature vector x  Rd, the function before the activation of the jth neuron in the lth layer fjl is
fjl(x; W:l, r:l) = t Wtlj rtl-1(ftl-1(x; W:l-1, r:l-1)), l  {2, 3, . . . , L}
where  : R  R+ is the rectified linear activation function (Nair & Hinton, 2010, ReLU). In vector form, if we denote as the Hadamard product, we could write the output of the lth layer as
f l(x; W, r) = rl-1 (f l-1(x; W:l-1, r:l-1)) Wl.

Without loss of generality, we also apply Bernoulli dropout to the input layer parameter 0  Rd, i.e., f 1(x; W, r0) = (r0 x)W1. Note that the output of the neural network f L(x; W, r)  Rk is a random vector due to the Bernoulli random variables r. We use the expected value of f L(x; W, r)
as the deterministic output

f L(x; W, ) = Er[f L(x; W, r)].

(1)

The final predictions are made through a softmax function, and we use the cross-entropy loss as our optimization objective. To simplify our analysis, we follow Wan et al. (2013) and reformulate the cross-entropy loss on top of the softmax into a single logistic function

loss(f L(x; W, ), y) = -

j yj log

.efjL (x;W,)
j efjL(x;W,)

3.1 RADEMACHER COMPLEXITY OF DROPOUT NEURAL NETWORK

Define loss  f L as the composition of the logistic loss function loss and the neural function f L returned from the Lth (last) layer, i.e.,

loss  f L = {(x, y)  loss(f L(x; W, ), y)}.

Theorem 3.1. Let X  Rn×d be the sample matrix with the ith row xi  Rd, p  1,

1 p

+

1 q

= 1.

If the p-norm of every column of Wl is bounded by a constant Bl, denote

W = W | maxj Wjl p  Bl, l  {1, 2, . . . , L} , given , the empirical Rademacher com-

plexity of the loss for the dropout neural network defined above is bounded by

RS(loss  f L)

=

1 n

E{i

}

supWW

n i=1

iloss(f

L

(xi

;

W,

),

yi)

k

2 log(2d) n

X max

lL=1Bl

l-1

q

,

where k is the number of classes to predict, l is the kl-dimensional vector of Bernoulli parameters for the dropout random variables in the lth layer, is are i.i.d. Rademacher random variables, and
· max is the matrix max norm defined as A max = maxij |Aij|.

Please refer to the appendix for the proof.

3

Under review as a conference paper at ICLR 2018

3.2 REGULARIZE DROPOUT NEURAL NETWORK WITH RADEMACHER COMPLEXITY

We have shown that the Rademacher complexity of a neural network is bounded by a function of the dropout rates, i.e., Bernoulli parameters . This makes it possible to unify the dropout rates and the network coefficients W in one objective. By imposing our upper bound of Rademacher complexity to the loss function as a regularizer, we have

Obj(W, ) = Loss(S, f L(·; W, )) + Reg(S, W, )

(2)

where the variable   R+ is a weighting coefficient to trade off the training loss and the generalization capability. The empirical loss Loss(S, f L(·; W, )) and regularizer function Reg(S, W, ) are
defined as

Loss(S, f L(·; W, ))

=

1 n

(xi,yi)S loss(f L(xi; W, ), yi),

Reg(S, W, ) = k

log d n

X max

Ll=1 l-1 q maxj

Wjl

p

,

where Wjl is the jth column of Wl and l is the retain rate vector for the lth layer. The variable k is the number of classes to predict and X  Rn×d is the sample matrix.

In addition to the Rademacher regu-

larizer Reg(S, W, ), the empirical loss term Loss(S, f L(·; W, )) also

3.5

Loss (left axis) Regularizer (right axis)

0.0016

depends on the dropout Bernoulli
parameters . Intuitively, when
 becomes smaller, the loss term Loss(S, f L(·; W, )) becomes larger, since the model is less capable to

3.0 2.5 2.0

0.0014 0.0012 0.0010

Loss Regularizer

fit the training samples (i.e., less representation power), the empirical Rademacher complexity bound becomes smaller (i.e., more generaliz-

1.5 1.0

0.0008 0.0006 0.0004

able), and vice versa. Figure 1 plots 0.5 the values of the cross-entropy loss

0.0002

function and empirical Rademacher p = q = 2 regularizer upon model

0.2 0.4 0.6 0.8 1.0 Retain Rates

convergence under different fixed set-

tings of retain rates. In the extreme Figure 1: Empirical cross-entropy loss (left axis) and

case, when all jl become zeros, the model always makes random guess for

Rademacher p = q = 2 regularizer (right axis) as a function of retain rates. We observe that the empirical loss and

prediction, leading to a large fitness
error Loss(S, f L(·; W, )), and the Rademacher complexity RS(lossf L) approaches 0.1

Rademacher regularizer increase or decrease roughly in a monotonic way as a function of retain rates on training data. The experiments are evaluated on MNIST dataset with a hidden layer of 128 ReLU units. We apply dropout on the hidden

layer only, and keep the retain rates fixed throughout train-

3.3 OPTIMIZE DROPOUT RATES

ing. We optimize the neural network with the empirical loss Loss(S, f L(·; W, )) only, i.e., without any regularizer. All

We now incorporate the Bernoulli pa- the values of the Rademacher p = q = 2 regularizer are rameters  into the optimization ob- computed after every epoch in post-hoc manner. We use jective as in Eqn. (2), i.e., the objec- minibatch size of 100, 200 epochs, initial learning rate of tive is a function of both weight co- 0.01, and decay it by half every 40 epochs. We plot the efficient matrices W and retain rate samples from last 20 epochs under each settings.

vectors . In particular, the model pa-

rameters and the dropout out rates are

optimized using a block coordinate de-

scent algorithm. We start with an initial setting of W and , and optimize W and  in an alternating

fashion. During the optimization of , we used the expected value of the dropout layer to rescale the

output from each layer. It significantly speeds up the forward propagation process, as we do not need

1To balance out the regularizer and the loss function so that they scale similarly as the sample size and internal layer nodes grow, we add some scaling factors to , which is discussed in Section 4.

4

Under review as a conference paper at ICLR 2018

to iteratively sample the dropout variables. Note that this is an approximation to the true f L(x; W, ), however, in practice, we find the difference are negligible on each minibatch. Essentially, it makes the layer output deterministic and the underlying network operates as if without dropout, i.e., similar to the approximation used in (Srivastava et al., 2014) during testing time.

4 EXPERIMENTS

We apply our proposed approach

with different network architectures, on the task of image and text classification using several public available benchmark datasets. All hidden neurons and convolutional filters are rectified linear units (Nair & Hinton, 2010, ReLU). We found that our approach achieves superior

Model
Multilayer Perceptron + Dropout + VARDROP + SPARSEVARDROP + Rademacher p = q = 2 + Rademacher p = 1, q =  + Rademacher p = , q = 1

1024 1.69 1.22 1.20 1.34 1.14 1.13 1.11

800 × 2 1.62 1.28 1.16 1.30 1.05 1.04 1.08

1024 × 3 1.61 1.25 1.07 1.27 0.95 0.96 0.95

performance against strong baselines on all datasets. For all

Table 1: Classification error on MNIST dataset.

datasets, we hold out 20% of the training data as validation set for parameter tuning and model

selection. After then, we combine both of these two sets to train the model and report the classifica-

tion error rate on test set. We optimize categorical cross-entropy loss on predicted class labels with

Rademacher regularization. We update the parameters using mini-batch stochastic gradient descent

with Nesterov momentum of 0.95 (Sutskever et al., 2013).

For Rademacher complexity term, we perform a grid search on the regularization weight   {0.5, 0.1, 0.05, 0.01, 0.005, 0.001}, and update the dropout rates after every I  {1, 5, 10, 50, 100} minibatches. For variational dropout method (Kingma et al., 2015, VARDROP), we examine the both Type-A and Type-B variational dropout with per-layer, per-neuron or per-weight adaptive dropout rate. We found the neuron-wise adaptive regularization on Type-A variational dropout layer often reports the best performance under most cases. We also perform a grid search on the regularization noise parameter in {0.1, 0.01, 0.001, 1e-4, 1e-5, 1e-6}. For sparse variational dropout method (Molchanov et al., 2017, SPARSEVARDROP), we find the model is much more sensitive to regularization weights, and often gets diverged. We examine different regularization weight in {1e - 3, 1e - 4, 1e - 5}. We follow similar weight adjustment scheme and scale it up by 10 after first {100, 200, 300} epochs, then further scale up by 5 and 2 after same number of epoch.

Scales of Regularization In practice, we want to stablize regularization term within some managable variance, so its value does not vary significantly upon difference structure of the underlying neural networks. Hence, we design some heuristics to scale the regularizer to offset the multipler effects raised from network structure. For instance, recall the neural network defined in Section 3, the Rademacher complexity regularizer with p = q = 2 after scaling is

k

log d n

maxi

xi 

L maxj Wjl 2 l-1 2
l=1 kl

,kl-1 +kl
log kl

where Wjl is the jth column of the weight coefficient matrix Wl and l is the retain rate vector for the lth layer. The variable k is the number of classes to predict and X  Rn×d is the sample matrix. Similarly, we could rescale the Rademacher complexity regularizers under other settings of p = 1, q =  and p = , q = 1. Please refer to the appendix for the scaled Rademacher complexity
bound regularizers and detailed derivations.

4.1 MNIST
MNIST dataset is a collection of 28 × 28 pixel hand-written digit images in grayscale, containing 60K for training and 10K for testing. The task is to classify the images into 10 digit classes from 0 to 9. All images are flattened into 784 dimension vectors, and all pixel values are rescaled to gray scale. We examine several different network structures, including architectures of 1 hiddel layer with 1024 units, 2 hidden layers with 800 neurons each, as well as 3 hidden layers with 1024 units each.

5

Under review as a conference paper at ICLR 2018

# of neurons # of neurons

Table 1 compares the performance of our proposed models against

800 1200

700 1000

600 500

800

400 600

other techniques. We use a learning

300
200 100

400 200

rate of 0.01

0

0

and decay it by

1.0 0.8

1.0 0.8

0.5 after every {300, 400, 500}
epochs. We

0

200

400 epoch

600

800

0.2

0.6 0.4retain

rate

1000 0.0

0

200

400 epoch

600

800

0.2

0.6 0.4retain

rate

1000 0.0

0

let all models

run sufficiently

5

long with 100K

updates. For all

10

models, we also explore different

15

initialization for neuron retaining

20

rates, including {0.8, 1.0} for
input layers,

25 0 5 10 15 20 25

{0.5, 0.8, 1.0} Figure 2: Changes in retain rates with Rademacher p = q = 2 regularization on

for hidden layers. MNIST dataset. Top-Left: changes in retain rate histograms for input layer (784

In practice, we gray scale pixels) through training. The retain rates get diffused over time, and

find initializing only a handful of pixels have retain rates close to 1. Top-Right: changes in retain

the retaining rate histograms for hidden layer (1024 ReLU units) through training process.

rates to 0.8 for Bottom-Left: sample images from MNIST dataset. Bottom-Right: retain rates

input layer and for corresponding input pixels upon model convergence. The surrounding pixels

0.5 for hidden of input image yield smaller retain rates (corresponds to the dark background

layer yields bet- area), and the center ones have significantly larger retain rates (corresponds to

ter performance the number pixels).

for all models,

except for SPARSEVARDROP model, initializing retaining rate to 1.0 for input layer seems to give

better result.

Figure 2 illustrates the changes in retain rates for both input and hidden layers under Rademacher regularization (p = q = 2) with 0.01 regularization weight. The network contains one hidden layer of 1024 ReLU units. The retain rates were initialized to 0.8 for input layer and 0.5 for hidden layer. The learning rate is 0.01 and decayed by 0.5 after every 300 epochs. We observe the retain rates for all layers are diffused throughout training process, and finally converged towards a unimodal distribution. We also notice that the retain rates for input layer upon model convergence demonstrate interesting feature pattern of the dataset. For example, the pixels in surrounding margins yield smaller retain rates, and the center pixels often have larger retain rates. This is because the digits in MNIST dataset are often centered in the image, hence all the surrounding pixels are not predictive at all when classifying an instance. This demonstrates that our proposed method is able to dynamically determine if an input signal is informational or not, and subsequently gives higher retain rate if it is, otherwise reduce the retain rate over time.

4.2 CIFAR
CIFAR10 and CIFAR100 datasets are collections of 50K training and 10K testing RGB images from 10 and 100 different image categories. Every instance consists of 32 × 32 RGB pixels. We preprocess all images by subtracting the per-pixel mean computed over all training set, then with ZCA whitening as suggested in Srivastava et al. (2014). No data augmentation is used. The neural network architecture we evaluate on uses three convolutional layers, each of which followed by a max-pooling layer. The convolutional layers have 96, 128, and 256 filters respectively. Each convolutional layer has a 5×5 receptive field applied with a stride of 1 pixel, and each max-pooling layer pools from 3×3
6

Under review as a conference paper at ICLR 2018

pixel region with strides of 2 pixels. These convolutional layers are followed by two fully-connected layer having 2048 hidden units each.

Table 2 summarizes the performance of our proposed models against other baselines. We initialize dropout rates settings with {0.9, 1.0} for input layers, {0.75, 1.0} for convolutional layers and {0.5, 1.0} for fully-connected layers. Similary to the MNIST evaluation, we find setting the corresponding retaining probabilities for input layers, convolutional layers and fullyconnected layers to 0.9, 0.75 and 0.5 respectively yields best performance under all models. We initialize the learning rate to 0.001 and decay it exponentially every {200, 300, 400} epochs.

Model
Convolutional neural network + Dropout in fully-connected + VARDROP + SPARSEVARDROP + Rademacher p = q = 2 + Rademacher p = 1, q = inf + Rademacher p = inf, q = 1 + Dropout in all layers + VARDROP + SPARSEVARDROP + Rademacher p = q = 2 + Rademacher p = 1, q =  + Rademacher p = , q = 1

CIFAR10 18.01 17.05 16.85 17.87 16.78 16.85 16.89 15.16 15.03 15.87 13.70 13.75 13.81

CIFAR100 50.28 45.81 45.47 45.74 44.99 45.14 45.35 41.00 39.15 42.67 38.11 38.51 38.63

Figure 3 illustrates the changes in re-

Table 2: Classification error on CIFAR datasets.

tain rates for both input and hidden

layers under Rademacher regularization (p = q = 2) with 0.01 regularization weight. The network

contains two convolution layers with 32 and 64 convolutional filters followed by one fully-connected

layer with 1024 neurons. All hidden units use ReLU activation functions. The retain rates were

initialized to 0.9 for input layer, 0.75 for convolutional layer and 0.5 for fully-connected layer. The

learning rate is 0.001 and exponentially decayed by half after every 300 epochs. Similar to MNIST

dataset, we observe the retain rates for all layers are diffused throughout training process, and fi-

nally converged towards a unimodal distribution. However, unlike MNIST dataset, we do not see

similar pattern for retain rates of input layer. This is mainly due to the nature of dataset, such that

CIFAR10 images spread over the entire range, hence all pixels are potentially informational to the

classification process. This again demonstrates that the Rademacher regularizer is able to distinguish

the informational pixels and retain them during training.

4.3 TEXT CLASSIFICATION
In addition, we also compare our proposed approach on text classification datasets--SUBJ and IMDB. SUBJ is a dataset containing 10K subjective and objective sentences (Pang & Lee, 2004) with nearly 14.5K vocabulary after stemming. All subjective comments come from movie reviews expressing writer's opinion, whereas objective sentences are from movie plots expressing purely facts. We randomly sample 20% from the collections as test data, and use other 80% for training and validation. IMDB is a collection of movie reviews from IMDB website, with 25K for training and another 25K for test (Maas et al., 2011), containing more than 50K vocabulary after stemming. It contains an even number of positive (i.e., with a review score of 7 or more out of a scale of 10) and negative (i.e., with a review score of 4 or less out of 10) reviews. The dataset has a good movie diversity coverage with less than 30 reviews per movie. For each sentence or document in these datasets, we normalize it into a vector of probability distribution over all vocabulary.
Table 3 summarizes the performance of our proposed models against other baselines. We initialize dropout rates settings with {0.8, 1.0} for input layers and {0.5, 1.0} for fully-connected layers. Similarly, by setting the corresponding retaining probabilities for input layers and fully-connected layers to 0.8 and 0.5 respectively, the model often yields the best performance. We use a constant learning rate of 0.001, as well as an initialization learning rate of 0.01 and decay it by half every {200, 300, 400} epochs. We notice that overall the improvement of dropout is not as significant as MNIST or CIFAR datasets.
Figure 4 illustrates the changes in retain rates for both input and hidden layers under Rademacher regularization (p = q = 2) with 0.005 regularization weight on IMDB dataset. The network contains one hidden layer of 1024 ReLU units. The retain rates were initialized to 0.8 for input layer and 0.5 for hidden layer. The learning rate is 0.001 and decayed by 0.5 after every 200 epochs. Similar to
7

Under review as a conference paper at ICLR 2018

# of neurons

3500 8000 3500

3000 7000 3000

# of neurons

# of neurons

2500 6000 2500

2000 1500 1000

5000
4000 3000 2000

2000 1500 1000

500 1000 500

000

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

30

1200

1000

800

600

400

200

0

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

# of neurons

25 20 15 10 5 0 0 5 10 15 20 25 30

Figure 3: Changes in retain rates with Rademacher p = q = 2 regularization on CIFAR10 dataset. Top-Left: changes in retain rate histograms for input layer (32 × 32 × 3 RGB pixels) through training. Top-Middle: changes in retain rate histograms for first convolutional layer (32 × 15 × 15 units) through training process. Top-Right: changes in retain rate histograms for second convolutional layer (64 × 7 × 7 units) through training process. Bottom-Left: changes in retain rate histograms for fully-connected layer (1024 ReLU units) through training process. Bottom-Middle: sample images from CIFAR10 dataset. Bottom-Right: retain rates for corresponding input pixels in both superposition and individual RGB channels upon model convergence. Unlike MNIST datasets, there is no clear pattern from the retain rates out of these channel pixels, since they are all informational towards prediction.

MNIST datasets, we observe the retain rates for all layers are diffused slightly, and the retain rates for input layer upon model convergence demonstrate interesting feature pattern of the dataset.

Recall that the task for IMDB dataset is

to classify movie reviews into negative or positive labels. Generically speaking, adjectives are more expressive than nouns or verbs in this scenario, and our findings seems to be consistent with this intuition. From our model, words like "lousi(y)", "flawless", "obnoxi(ous)", "finest" and "unwatch(able)" yield large retain rates and hence indicative feature to predication. We

Model
Multi-layer Perceptron + Dropout
+ VARDROP + SPARSEVARDROP + Rademacher p = q = 2 + Rademacher p = 1, q =  + Rademacher p = , q = 1

SUBJ 11.50 10.95 10.45 10.35 10.00 10.13 10.15

IMDB 12.18 12.02 11.82 11.97 11.81 11.79 11.83

also notice that "baldwin" and "kurosawa" are also very informative features. On the

Table 3: Classification error on text dataset.

other hand, words like "young", "review", "role", "anim(ation)" and "year" have near zero retain

rates upon model convergence, which are less informative. One other interesting observation is that

the word "oscar" also yields near zero retain rate, which implies the positivity or negativity of a

movie review is not necessarily correlated with the mention of Academy Awards.

5 CONCLUSION
Imposing regularizaiton for a better model generalization is not a new topic. However we tackle the problem for the dropout neural network regularization in a different way. The theoretical upper bound we proved on the Rademacher complexity facilitates us to directly incorporate the dropout rates into the objective function. In this way the dropout rate can be optimized by block coordinate
8

Under review as a conference paper at ICLR 2018

60000

50000

40000

30000

20000

10000

0

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

1200

1000

800

600

400

200

0

1.0

0.8

0

100

200epo3c0h0

400

500

0.2

0.6 0.4retain

rate

600 0.0

# of neurons

# of neurons

retain rates

1.0 0.8

aylrmeiugalsonhrtuf3tfuusokfniuinrnngrndteeieuwrsfsetnrrtautreectrbsocpsahhnermlidvknsiiwusolnnhdirbictogonylnifslspliocaaexawwirlaess made tocatalll

0.6

0.4

anim act

0.2

yocuonmgplet

zombi

actual music

0.00 100 thin2g00 guyr3o0le0 reviemwpa4rke0et0ti epoch

kill 500

600

Figure 4: Changes in retain rates with Rademacher p = q = 2 regularization on IMDB dataset. TopLeft: changes in retain rate histograms for input layer (more than 50K word features) through training. Bottom-Left: changes in retain rate histograms for hidden layer (1024 ReLU units) through training process. Right: changes in retain rates for word features associated with 20 largest and smallest retain rates upon model convergence. Some of the most indicative features (in the top half) include "alright", "finest", "flawless", "forgett(able)", "hype", "lousi(y)", "mildli(y)", "obnoxi(ous)", "refresh(ing)", "sensit(ive)", "surprisingli(y)", "unconvinc(ing)", "underr(ated)", "unfunni(y)", and "unwatch(able)". In addition, some actor, directory or show names also appear in the top informative word list, such as "baldwin", "kurosawa" and "mst3k". Some of the word features with low retaining probability--hence, possibly less indicative--include "actual", "call", "complete", "make", "pretti(y)", "review", "thing", "total", "year", "young". Moreover, some genre and generic movie plot information "act", "anim(ation)", "guy", "kill", "music", "role", and "zombi". One interesting observation is that we find the word "oscar" is also in the list of less informative features, which implies movie reviews and Academy Awards are not necessarily correlated. Note that higher retaining rate means the corresponding features are more indicative in classifying IMDB reviews into positive and negative labels, i.e., no explicit association with the label itself.

descent procedure with one consistent objective. Our empirical evaluation demonstrates promising results and interesting patterns on adapted retain rates.
In the future, we would like to investigate the sparsity property of the learnt retain rates to encourage a sparse representation of the data and the neural network structure (Wen et al., 2016), similar to the sparse Bayesian models and relevance vector machine (Tipping, 2001). We would also like to explore the applications of deep network compression (Han et al., 2015a; Iandola et al., 2016; Ullrich et al., 2017; Molchanov et al., 2017; Louizos et al., 2017). In addition, one other possible research direction is to dynamically adjust the architecture of the deep neural networks (Srinivas & Babu, 2015; Han et al., 2015b; Guo et al., 2016), and hence reduce the model complexity via dropout rates.
ACKNOWLEDGMENTS
Available after blind review.
REFERENCES
Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Proceedings of Advances in Neural Information Processing Systems, pp. 3084­3092, 2013.
Pierre Baldi and Peter J Sadowski. Understanding dropout. In Proceedings of Advances in Neural Information Processing Systems, pp. 2814­2822, 2013.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International

9

Under review as a conference paper at ICLR 2018
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1050­1059, New York, New York, USA, 20­22 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/ gal16.html.
Wei Gao and Zhi-Hua Zhou. Dropout rademacher complexity of deep neural networks. Science China Information Sciences, 59(7):072104, Jun 2016. ISSN 1869-1919. doi: 10.1007/s11432-015-5470-z. URL https://doi.org/10.1007/s11432-015-5470-z.
Ian J. Goodfellow, David Warde-farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In Proceedings of the International Conference of Machine Learning, 2013.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Advances In Neural Information Processing Systems, pp. 1379­1387, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135­1143, 2015b.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference and prediction. Springer, 2 edition, 2009. URL http://www-stat.stanford.edu/~tibs/ ElemStatLearn/.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Proceedings of Advances in Neural Information Processing Systems, pp. 2575­2583. Curran Associates, Inc., 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 2012.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. arXiv preprint arXiv:1705.08665, 2017.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the Association for Computational Linguistics, HLT '11, pp. 142­150, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. ISBN 978-1-932432-87-9. URL http://dl.acm.org/citation.cfm?id=2002472.2002491.
Shin-ichi Maeda. A bayesian encourages dropout. arXiv preprint arXiv:1412.7003, 2014.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2498­2507, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes Fürnkranz and Thorsten Joachims (eds.), Proceedings of the International Conference of Machine Learning, pp. 807­814. Omnipress, 2010. URL http://www.icml2010.org/papers/432.pdf.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the Association for Computational Linguistics, Proceedings of the Association for Computational Linguistics, Stroudsburg, PA, USA, 2004. Proceedings of the Association for Computational Linguistics.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, New York, NY, USA, 2014. ISBN 1107057132, 9781107057135.
Suraj Srinivas and R Venkatesh Babu. Learning neural network architectures using backpropagation. arXiv preprint arXiv:1511.05497, 2015.
Suraj Srinivas and R Venkatesh Babu. Generalized dropout. arXiv preprint arXiv:1611.06791, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 2014.
10

Under review as a conference paper at ICLR 2018

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the International Conference of Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1139­1147, Atlanta, Georgia, USA, 17­19 Jun 2013.
Michael E Tipping. Sparse bayesian learning and the relevance vector machine. Journal of machine learning research, 1(Jun):211­244, 2001.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In Advances in neural information processing systems, 2013.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the International Conference of Machine Learning, 2013.
Sida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th International Conference on Machine Learning, 2013.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074­2082, 2016.
Jingwei Zhuo, Jun Zhu, and Bo Zhang. Adaptive dropout rates for learning with corrupted features. In Qiang Yang and Michael Wooldridge (eds.), International Joint Conference on Artificial Intelligence. AAAI Press, 2015.

6 APPENDIX

6.1 PROOF OF THEOREM 3.1

Proof. In the analysis of Rademacher complexity, we treat the functions fed into the neurons of the lth layer as one function class Fl = f l(x; w:l). Here again we are using the notation w:l = {w1, . . . , wl}, and w = w:L. As a consequence j, fjl(x; wl)  Fl.
To simplify our analysis, we follow Wan et al. (2013) and reformulate the cross-entropy loss on top of the softmax into a single logistic function

loss(f L(x; W), y) = -

j yj log

.efjL (x;W)
j efjL(x;W)

The function class fed into the neurons of the lth layer f l(x; w:l) admits a recursive expression

f l(x; w:l, r:(l-1)) =

(fkl-1(x; w:l-1, r:l-2))rkl-1wkl

k

f l(x; w:l; :(l-1)) = Er:(l-1) f l(x; w:l, r:(l-1))

(3) (4)

Given the neural network function (1) and the logistic loss function l is 1 Lipschitz, by Contraction lemma (Shalev-Shwartz & Ben-David, 2014), the empirical Rademacher complexity of the loss function is bounded by

RS(l



fL)

=

1 n Emaxw

il(f L(xi; w), yi)

i



k n Emaxw

if L(xi; w) = kRS(f L)

i

(5)

Note the empirical Rademacher complexity of the function class of the Lth layer, i.e., the last output layer, is

RS(f L)

=

1 n E{i}

sup
w

i

if L(xi; w)

(6)

11

Under review as a conference paper at ICLR 2018

To prove the bound in a recursive way, let's also define a variant of the Rademacher complexity with absolute value inside the supremum:

R~S(f )

=

1

n

E{i }

sup
w

if (xi; w)
i

(7)

Note here R~S(f ) is not exactly the same as the Rademacher complexity defined before in this paper. And we have

RS(f L)  R~S(f L)

(8)

Now we start the recursive proof. The empirical Rademacher complexity (with absolute value inside supremum) of the function class of the lth layer is

R~S(f l)

=

1

n

E{i }

sup
w

1

=

n

E{i }

sup
w

1

=

n

E{i }

sup
w

if l(xi; w)
i

iErf l(xi; w, r)
i

iErl-1 Er:l-2

wjl rjl-1(fjl-1(xi; w:l-1, r:l-2))

ij

1

=

n

E{i }

sup
w

i Er:l-2

wjl jl-1(fjl-1(xi; w:l-1, r:l-2))

ij

(9)

Let gjl-1(x; w:l-1, :(l-2)) = E r:(l-2) fjl-1(xi; w:l-1, r:l-2) , then

R~S(f l)

=

1

n

E{i }

sup
w:l-1

sup
wl

i wjl jl-1gjl-1(xi; w:l-1, :(l-2))
ij

(10)

According to the assumption, p  1, q = p/(p - 1), and wl p  Bl, from equation (10), by Holder's inequality, we have l  {2, . . . , L}

R~S(f l)



1 n E{i}

sup |wl
wl pBl

l-1|1 sup sup | igjl-1(xi; w:l-1, :(l-2))|

w:l-1 j

i



sup jl-1|wjl |
wl pBl j

1

n

E{i }

sup
w:l-1

sup
j

igjl-1(xi; w:l-1, :(l-2))
i

 sup l-1 q wl p R~S(gl-1)  Bl l-1 qR~S(gl-1)
wl pBl

(11)

Suppose the activation function (·) used in the neural network is 1-Lipschitz, and (0) = 0 (for example, the RELU function). Then by the Ledoux-Talagrand contraction lemma, l  {1, . . . , L - 1}

R~S(gl(x; w:l))  R~S(f l(x; w:l)).

(12)

For the first layer, i.e., the feature layer with dropout but without activation function, if |w1|p  B1, then

R~S(f 1(x; w1))

=

1 n E{i}

n
sup iEr0 < w1, xi
w1 pB1 i=1

r0 >

1 = n E{i}

n

sup i wj1xij j0

w1 pB1 i=1

j

1 = n E{i}

sup
w1 pB1

n

wj1 j0

ixij

j i=1



1 n E{i}

sup w1
w1 pB1

0 1

n
ixi 
i=1



B1 0 n

q E{i}

n
ixi 
i=1

(13)

12

Under review as a conference paper at ICLR 2018

By Lemma 26.11 in Shalev-Shwartz & Ben-David (2014), we know the last term in (13) is bounded by

1 n E{i}

n
ixi   max xi  2 log(2d)/n. i i=1

Thus we get

R~S(f

1(x;

w1))



max i

xi

B1

0

q

2 log(2d) n

Combining the inequalities (5), (8), (11), (12), and (14), we have

(14)

RS(l  f L)  k

2 log(2d) max
ni

xi 

Ll=1Bl l-1 q

.

(15)

6.2 GENERALIZATION BOUND ON THE DROPOUT NEURAL NETWORKS

Here we need to define truncated cross entropy loss function:

~l  f L = ~l(f L, (x, y)) = min(loss(f L(x; W, ), y), Cl)

(16)

where Cl is a constant. Note with the truncation, the cross entropy loss is still 1-Lipschitz so the empirical Rademacher complexity bound still holds for the truncated loss ~l(f L(x; W, ), y).
Theorem 6.1. For the dropout neural network defined in section (3), if truncated cross entropy loss ~l (16) is used, then   0, with probability at least 1 - 2, ~l  f L :

E(x,y)D(~l(f L(x; W, ), y)



1 n

~l(f L(xi; W, ), yi) + RS(~l  f L) + 3Cl 1/(2n)

i

(17)

Note here the empirical Rademacher complexity for the bounded loss function RS(~l  f L) admits the same bound as the empirical Rademacher complexity for the unbounded cross entropy loss RS(l  f L).

6.3 TOWARDS AN UNIFIED VIEW OVER RADEMACHER REGULARIZATION

In fact, adding a Rademacher related regularizer, though not investigated much, is not new at least for linear functions.

It is well known (Shalev-Shwartz & Ben-David, 2014) that the empirical Rademacher complexity of the linear class
H2 = {x  w, x : w 2  B2}

is bounded by



RS



max i

xi

2B2/

n.

Note the l2 loss function is 2-Lipschtz. In this way, we may interpret the regularizer in the ridge regression related an upper bound for the empirical Rademacher complexity of the linear function class.

Similarly for the linear class

H1 = {x  w, x : w 1  B1},

the empirical Rademacher complexity is bounded by

RS  max xi B1 i

2 log(2d) .
n

So the lasso problem can also be viewed as adding a Rademacher-related regularization to the empirical loss minimization objective.

Note that the content in Section 6.3 and 6.4 is based purely on heuristics, and derived on an ad hoc basis.

13

Under review as a conference paper at ICLR 2018

6.4 SCALES OF REGULARIZATION

In application we do not want the regularization term to vary too much when the neural network has different number of internal neurons. To overcome that we design some heuristics to add to the regularizer. Note here all the scales mentioned in this section are added in a heuristic fashion. It is purely empirical.
When p = q = 2, the regularizer is bounded by

RS(l  f L)  k

log(d)

n

max xi  i

Ll=1

max j

Wjl

2

l-1

2

.

(18)

where

Wjl



kl-1
R

is

the

j-th

column

of

Wl.

Suppose

we

use

i.i.d.

uniform

random

variable

to

initialize

Wl

such that l wilj  U [- 6/(kl + kl+1), 6/(kl + kl+1)]. Considering the scales of the maximum among

the 2-norms of Wjl , we use a scaled regularizer instead:

k

log d

n

max xi  i

lL=1 maxj

Wjl 2 l-1 2 kl

kl + kl-1 log kl .

Similarly, when p =  and q = 1, the scaled regularizer we used is



k

log d

n

max xi  i

Ll=1 Wl max l-1 1

kl + kl-1 kl

.

When p = 1 and q = , we used the scaled regularizer

k

log d

n

max xi  i

lL=1 kl-1

Wl 1 l-1  log kl/(kl + kl-1)

.

14

