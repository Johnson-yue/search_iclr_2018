Under review as a conference paper at ICLR 2018

SCHEDULED LEARNING WITH DECLINING DIVERSITY AND INCREMENTAL DIFFICULTY
Anonymous authors Paper under double-blind review

ABSTRACT
We study how to adaptively select training subsets for different stages of iterative machine learning. We introduce minimax curriculum learning (MCL), which trains a model on a diverse few samples at first, and then later on a larger training set containing concentrated hard samples, thereby avoiding wasted efforts on redundant samples in early stages and on disperse outliers in later stages. At each stage, model weights and training sets are updated by solving a minimax optimization, whose objective is composed of a loss (reflecting the hardness of the training set) and a submodular regularization (measuring its diversity). MCL repeatedly solves a sequence of such optimizations with decreasing diversity and increasing training set size. Unlike the expensive alternative minimization used in previous work, we reduce MCL to minimization of a surrogate function that can be handled by submodular maximization and optimized by gradient methods. We show that MCL achieves better performance by using fewer labeled samples for both shallow and deep models.

1 INTRODUCTION

Inspired by the interaction between teacher and student in human education, recent studies (Khan

et al., 2011b; Basu & Christensen, 2013; Spitkovsky et al., 2009) support that learning algorithms

can be improved by updating a model on a designed sequence of training sets, i.e., a curriculum.

This problem is addressed in curriculum learning (CL) (Bengio et al., 2009), where the sequence is

designed by a human expert or heuristic before training begins. Instead of relying on a teacher to

provide the curriculum, self-paced learning (SPL) (Kumar et al., 2010; Tang et al., 2012a; Supancic III

& Ramanan, 2013; Tang et al., 2012b) learns the curriculum during the training process, by letting the

student (i.e., the algorithm) determine which samples to learn from based on their hardness. Given

a training set D = {(x1, y1), . . . , (xn, yn)} of n samples and loss function L(yi, f (xi, w)), where xi  Rm represents feature vector for the ith sample, yi is its label, and f (xi, w) is the predicted label provided by model with weight w, SPL (Kumar et al., 2010) aims at solving the following

min-min optimization.

nn

min min
wRm v[0,1]n

viL (yi, f (xi, w)) - 

vi.

i=1 i=1

(1)

It jointly learns model weights w and sample weights v, which are 0-1 indicators of selected samples,

by alternating minimization. Fixing w, minimization w.r.t. v equals selecting samples with loss

L(yi, f (xi, w)) < , where  controls the learning amount ("learning pace" in SPL) and works as a threshold to the hardness of enrolled samples. More samples will be selected with a larger .

Self-paced curriculum learning (Jiang et al., 2015) introduces a blending of "teacher mode" in CL

and "student mode" in SPL, where the teacher can define a region of v by attaching a linear constraint aT v  c to Eq. (1). Evidence in previous work (Khan et al., 2011b; Tang et al., 2012b; Basu &

Christensen, 2013; Bengio, 2014) shows that solving a series of Eq. (1) with increasing  can avoid

bad local minima and reduce generalization error.

Selection of training samples has also been studied in other learning settings, often with different motivations. In active learning (AL) (Settles, 2010) and experimental design (Montgomery, 2006), the learner can actively query labels of samples from an unlabeled pool during the training process, and the goal is to reduce annotation costs. Their curriculum design aims to achieve the same performance by using fewer labeled samples and ruling out redundant uninformative ones. In machine teaching (Khan et al., 2011a; Zhu, 2015; Patil et al., 2014), the teacher designs the optimal training set so it

1

Under review as a conference paper at ICLR 2018

can take the minimal efforts (e.g., the smallest number of samples) to guide the learning algorithm to find a target model. In boosting (Schapire, 1990; Freund & Schapire, 1997), the goal is to learn an ensemble of weak classifiers sequentially. It assigns large weights to samples with large loss or that are misclassified by the model in previous steps. Then the weighted error is minimized. Since the weights are not 0-1 indicators, it needs labels of all samples.

Comparing to CL and SPL, the selection criteria of the above methods do not change for different

learning stages. CL and SPL use a continuation scheme (Allgower & Georg, 2003), which handles

a hard task by solving a sequence of tasks from easy to hard, where the solution to each task is

the "warm start" for the next harder task. In particular, they update a classifier on a sequence of

training sets from easy to hard. It has been declared in (Bengio et al., 2013; Bengio, 2014) that such

continuation scheme can reduce the impact of local minima when applied to neural nets. For example,

after each round of alternating minimization in SPL (Eq. (1)), the threshold  to loss L(yi, f (xi, w))

(which measures the hardness) increases and enforces selection of more samples with larger loss. The

accretion of hardness leads to an increased entropy of the training set, which can also be achieved by

increasing diversity. SPL with diversity (SPLD) (Jiang et al., 2014) adds to Eq. (1) a negative group

sparse regularization - v 2,1

-

b j=1

v(j)

2 (all samples are divided into b groups and v(j) is

the weight vector for the jth group), which favours samples from different groups.

Active learning and boosting, by contrast, always favor samples that are difficult to predict, since they are the most informative to learn. For example, uncertainty sampling (Culotta & McCallum, 2005; Scheffer et al., 2001; Dagan & Engelson, 1995; Dasgupta & Hsu, 2008) select samples that are most uncertain, while query by committee (Seung et al., 1992; Dagan & Engelson, 1995; Abe & Mamitsuka, 1998) selects the ones that multiple models most disagree on. Boosting assigns large weights to misclassified samples with large loss. Recently, diversity modeling was introduced to AL (Wei et al., 2015). It uses submodular maximization to select diverse training batches from the most uncertain samples. However, changing diversity during the learning process has not been investigated.

It is advantageous to gradually change both hardness and diversity of the training set over different learning stages. But increasing both, as in SPLD, might not help to select the most informative samples. In early stages of SPLD (Jiang et al., 2014), selected samples tend to be grouped into a few easy regions due to small diversity. However, since the prediction of the model is already precise in these regions, selecting more from them cannot yield much improvement (due to redundancy). In its later stages, difficult samples are more favored but large diversity enforces the selected ones to be dispersed over the input space. However, hard samples usually gather around the margin of local decision boundaries where the prediction has high variance. To effectively reduce the loss in these regions, we need more training samples from the same region. Moreover, diverse hard samples might be outliers, especially in later stages when the model is already "mature." Selecting them into the curriculum might make the training unstable.

Evidence for changing hardness and diversity in a curriculum can also be found in human education. For example, courses in primary and middle school usually cover a broad range of basic knowledge/skills and fundamentals of many subjects, while in college and graduate school students mainly focus on advanced topics in their majors. In addition, recent studies of bilingualism (Bialystok et al., 2012; Li et al., 2014; Mechelli et al., 2004; Kova´cs & Mehler, 2009) show that learning two or multiple languages in childhood is beneficial for future brain development. These indicate that large diversity can be helpful during early learning stages.

1.1 OUR APPROACH

Motivated by these observations, we introduce a new form of curriculum learning called "minimax curriculum learning (MCL)." It increases the hardness but reduces the diversity of the curriculum during training. This is accomplished by solving a sequence of minimax optimizations of the following form with a fading weight  and growing k.

min max

L (yi, f (xi, w)) + F (A).

wRm AV,|A|k

iA

(2)

The objective is composed of the loss on a subset A of samples evaluating the hardness of A, and a

normalized monotone non-decreasing submodular function F : 2V  R+ 1 of subset A measuring

1In this paper, we focus on F (·) being monotone non-decreasing with A and cardinality constraint |A|  k. But other submodular functions and constraints are also potentially interesting and worth studying.

2

Under review as a conference paper at ICLR 2018

its diversity, where V is the ground set of all available samples. Larger loss implies that the subset A is harder to learn, while a larger F (A) indicates more diversity. The weight  controls the trade-off between diversity and hardness, while k is the "learning amount" controlling the size of A. In practice, we select k clusters rather than k samples to avoid the annotation costs and computation of loss on all samples (details are given in Section 2.3).

The submodular function F (·) can be chosen from a large family (e.g., including but not limited to

facility location and set cover functions). All have the following diminishing returns property: given

a finite ground set V , and any A  B  V and a v / B,

F (v  A) - F (A)  F (v  B) - F (B).

(3)

This implies v is more important to the smaller set A than to the larger set B. The marginal gain of v

conditioned on A is f (v|A) f (v  A) - f (A) and reflects the importance of v to A. Due to this

property, submodular functions (Fujishige, 2005) have been widely used for diversity models (Batra

et al., 2012; Prasad et al., 2014; Gillenwater et al., 2012; Iyer & Bilmes, 2015).

In MCL, we gradually reduce  and augment k to increase the hardness and decrease the diversity of A, and alternatively update A and w in each iteration. At early stages,  is large and k is small, so the maximization tends to select a few diverse samples with large F (A). At later stages,  decreases and k increases, so the maximization prefers larger but less diverse training set composed of more concentrated hard samples. This scheduling of  and k helps to select the most informative samples for different learning stages, and saves annotation/computation on redundant ones. At early stages, rather than wasting efforts on many samples the model is already accurate on, a few diverse and representative samples are sufficiently informative to improve the model. When the model becomes "mature" after some training, the most effective way to reduce loss is to train it on more samples from where it usually fails to predict, i.e., where hard samples are located. The reduced diversity can rule out spurious hard samples, which are far away from the others and likely to be distractive outliers.

Although Eq. (2) is a hybrid optimization involving both continuous variable w and discrete variable A, it can be reduced to minimization of a piecewise function, where each piece is defined by a subset A achieving the maximum in an interval of w. It is convex when the loss is convex, so various off-the-shelf algorithms can be applied once A is known for each piece. However, the number of feasible A is O(2n), and enumerating them all to find the maximum is intractable. Thanks to submodularity of the objective, fast approximate algorithms (Nemhauser et al., 1978; Minoux, 1978; Leskovec et al., 2007; Mirzasoleiman et al., 2015) exist to find a sub-optimal A. We instead minimize a surrogate of the piecewise function defined by a sub-optimal A in each interval of w. It is unknown at first but can be tracked by submodular maximization between gradient descent steps of w.

2 MINIMAX CURRICULUM LEARNING

The minimax problem in Eq. (2) can be explained as a two-person zero-sum game between a teacher (the maximizer) and a student (the minimizer): the teacher chooses training set A based on the student's feedback of hardness (i.e., loss achieved by current model w) and how diverse according to the teacher's expertise, while the student updates w to reduce the loss on training set A given by the teacher. Similar teacher-student interaction also exists in real life. In addition, the teacher usually introduces basic concepts at the beginning and asks easy questions from diverse topics to get sufficient feedback from the student, and then trains the student with more practices from the topics that the student finds difficult.
This minimax formulation is essentially different from the min-min formulation often used in previous CL and SPL (Kumar et al., 2010; Jiang et al., 2014; 2015). Minimizing the worst case loss is a widely used strategy in machine learning (Lanckriet et al., 2003; Farnia & Tse, 2016) to achieve better generalization performance and model robustness, especially when strong assumptions cannot be made to the data distribution. With submodular regularization, MCL can further avoid overfitting on a few outliers having large loss. Moreover, comparing to min-min formulation that alternately solves two minimizations per iteration (Alternative Convex Search (ACS) (Bazaraa et al., 1993)) until converge for each fixed , only one minimization is required to solve Eq. (2). Furthermore, SPL grows the training set up to the ground set to avoid overfitting on an easy subset. In MCL, however, the training set's size also increases but can grow much slowly (since it is diverse), and may end with a much smaller subset. In addition, introducing any diversity regularization of v to SPL usually leads to the loss of bi-convexity and requires re-design of the algorithm. Any convexity and convergence analysis, however, always hold for any convex loss and submodular regularization in MCL.

3

Under review as a conference paper at ICLR 2018

The goal of this section is to solve the minimax problem in Eq. (2), which equals to a minimization minwRm g(w) of the following objective g(w).

g(w) max

L (yi, f (xi, w)) + F (A)

AV,|A|k iA

(4)

In each region of w, an analytic form of g(w) can be derived from A achieving the maximum by enumerating all possible subsets. Different regions of w are associated with different A. So g(w) is piecewise convex if the loss function L(yi, f (xi, w)) is convex w.r.t. w. Unfortunately, the number of feasible subsets A is O(2n) so enumerating them all is intractable. However, there exists efficient

algorithms such as the greedy procedure and its variants (Nemhauser et al., 1978; Minoux, 1978) that can find a suboptimal solution to the maximization in Eq. (4) (with w fixed), which is

max G(A)

L (yi, f (xi, w)) + F (A).

AV,|A|k

iA

(5)

The suboptimality is due to the submodularity of G(A), since G(A) is a weighted sum of a modular

function

n iA

L

(yi,

f (xi,

w))

and

a

submodular

function

F

(A).

,

We

assume

L

(yi,

f (xi,

w))



0

w.l.o.g., so G(A) is also monotone non-decreasing. For each region of w, the greedy procedure gives

a sub-optimal solution A^ of Eq. (5) and defines a surrogate function g^(w) of g(w)

g^(w)

L (yi, f (xi, w)) + F (A^)

(6)

iA^

that satisfies g^(w)  [ · g(w), g(w)] ( is the approximation factor) in this region. Similar to

g(w), g^(w) is piecewise convex if the loss function L(yi, f (xi, w)) is convex w.r.t. w. So different regions of w are associated with different A^. We prove in Section 2.2 that minimizing g^(w) gives an

approximate solution of Eq. (2).

Algorithm 1 Minimax Curriculum Learning (MCL)
1: input: (·, ), , p,  2: output: w0 3: initialize: w0, , k
4: while not converge do 5: for t  {0, · · · , p} do 6: G(A)  iA L (yi, f (xi, wt)) + F (A); 7: A^  SUBMODULARMAX(G, k);

With g^(w) given, our algorithm is simply gradient descent for minimizing g^(w), where many off-the-shelf methods can be invoked, e.g., SGD, momentum methods, Nesterov's accelerated gradient (Nesterov, 2005), Adagrad (Duchi et al., 2011), etc. The key problem is how to obtain g^(w), which depends on suboptimal solutions in

8:

g^(wt)

=

 w

iA^ L (yi, f (xi, wt));

9: wt+1  wt +  {w1:t}, {g^(w1:t)},  ;

10: end for

11: w0  wp,   (1 - ) · , k  k + ;

12: end while

different regions of w. However, it is not necessary to run submodular maximization for every region of w. Since we use gradient descent methods, we only need to know g^(w) for w on the optimization path. At the begining of

each iteration in our algorithm, we fix w and use submodular maximization to achieve A^, which

defines g^(w) in the current region of w, then any gradient descent method is applied to g^(w).

Let A represent the optimal solution of Eq. (5), then A^ with approximation factor  satisfies

G(A^)  G(A). The greedy algorithm produces an A^ with  = 1 - e-1 (Nemhauser et al., 1978).

Given A^, g^(w) has gradient

g^(w) =  w

L (yi, f (xi, w)) .

iA^

(7)

Then any gradient descent method can update w and thus minimize g^(w). We can treat A^ as one

batch if k is small, and update w by w  w - g^(w) with learning rate . For large A^, we use

mini-batch SGD that applies the same update rule to every mini-batch of A^. More complex gradient

descent rules (·, ) can take all the historical gradients and wt in previous steps into account. With

superscript t to index the step,

wt+1  wt +  {w1:t}, {g^(w1:t)},  .

(8)

Hence, each iteration of the algorithm aims to minimize g^(w) and update w based on a training set A^

chosen by the submodular maximization. The whole algorithm (approximately) solves a sequence of Eq. (2) with decreasing  and augmenting k, where the solution w minimizing g^(w) in one iteration is

4

Under review as a conference paper at ICLR 2018

the "warm-start" for the next iteration. This equals repeatedly updating the model w on a sequence of training sets A^ that changes from small (easy) and diverse to large set with clusters of hard samples.
Algorithm 1 gives the details of MCL. Step 5-10 aims to solve optimization in Eq. (2) with  and k scheduled in Step 11. Step 6-7 finds a suboptimal subset A^ by submodular maximization, which will be discussed in Section 2.1. Step 8-9 updates w based on A^ by gradient descent (·, ) with learning rate . We stop the optimization after p steps to avoid overfitting. Then  is reduced by factor   [0, 1] and k is increased by . We set p  50 due to the warm start in continuation scheme.

2.1 SUBMODULAR MAXIMIZATION

We now introduce the submodular maximization algorithm used in Step 7 of Algorithm 1, whose goal is to maximize G(A) in Eq. (5) and select training set A^ based on the hardness of samples to the
current model and diversity. Though the problem in Eq. (5) is NP-hard, a near-optimal solution can be achieved by the greedy algorithm, which holds an approximation factor  = 1 - e-1 (Nemhauser et al., 1978). It starts with A  , and selects the next element with the largest marginal gain f (v|A) from V \A, i.e., A  A  {v} where v  argmaxvV \A f (v|A), and this repeats until |A| = k. It is simple to implement and usually outperforms other methods, e.g., those based on integer linear programming. However, it requires O(nk) function evaluations for ground set size |V | = n.

The lazy, or accelerated, greedy algorithm (Minoux, 1978; Leskovec et al., 2007) reduces the number of function evaluations per step by lazily updating a priority queue of marginal gains over all elements. It has the same output and guarantee as the original greedy algorithm but significantly reduces computation in practice.

Greedy and lazy greedy can guarantee a better approximation factor  better than 1 - e-1 when

the objective G(A) is close to modular, which is exactly the case for later stage of MCL when  decreases to a small value. Specifically, the approximation factor is  = (1 - e-G )/G (Conforti & Cornuejols, 1984), which depends on the curvature G  [0, 1] of G(A) (Fujishige, 2005) below
describing how modular G(A) is.

G

1 - min G(j|G\j) . jV G(j)

(9)

In the extreme case of G = 0, G(A) is modular. As G increases towards 1, G(A) becomes more submodular, and the approximation factor  reduces to 1 - e-1. In MCL, G decreases with the weight  of submodular regularization F (A). This increases the approximation factor  and results

in the surrogate function g^(w) being closer to the true objective g(w).

2.2 CONVERGENCE ANALYSIS

From the optimization perspective, Algorithm 1 uses a continuation scheme to solve a sequence of minimax problems in Eq. (2) with decreasing  and increasing k. Each problem is solved by Step 5-10, and conveys its solution wp as a warm start w0 to the next problem. Instead of directly minimizing the true objective g(w) in Eq. (4) that could be NP-hard to obtain, Step 5-10 minimizes a surrogate function g^(w)  g(w) by using gradient descent rule (·, ). It is interesting to study how close the solution w^ of applying gradient descent to g^(w) until convergence approximates the real solution w of minwRm g(w).
Proposition 1. The maximum of multiple -strongly convex functions is -strongly convex as well.

The proof can be found in Anonymous (2018).

Theorem 1. For minimax problem in Eq. (2) with given , if the loss function L (yi, f (xi, w)) is -strongly convex and |V |  k, running Step 6-9 in Algorithm 1 for iterations until convergence

yields a solution w^ satisfying

w^ - w

2 2



2 k

1 - 1 · g(w), 

(10)

where w is the optimal solution, g(w) is the objective value achieved on w, and  is the approxi-

mation factor that submodular maximization can guarantee for G(A).

Proof. The objective g(w) of the minimax problem in Eq. (2) after eliminating A is given in Eq. (4). Since G(A) in Eq. (5) is monotone non-decreasing submodular, the optimal subset A when defining

5

Under review as a conference paper at ICLR 2018

g(w) in Eq. (4) always has size k if |V |  k. In addition, because the loss function L (yi, f (xi, w)) is -strongly convex, g(w) in Eq. (4) is the maximum over multiple k-strongly convex functions

with different A. According to Proposition 1, g(w) is also k-strongly convex, i.e.,

g(w^)  g(w) + g(w)T (w^ - w) + k 2

w^ - w

22,

g(w)  g(w).

(11)

Since the convex function g(w) achieves minimum on w, it is valid to substitute g(w) = 0 

g(w) into Eq. (11). After rearrangement, we have

w^ - w

2 2



2 [g(w^) - g(w)] . k

(12)

In the following, we will prove g(w)   · g(w^), which together with Eq. (12) will lead to the final

bound showing how close w^ is to w.

Note g^(w) (Eq. (6)) is a piecewise function, each piece of whom is convex and associated with different A^ achieved by a submodular maximization algorithm of approximation factor . Since A^ is not guaranteed to be a global maxima, unlike g(w), the whole g^(w) cannot be written as the
maximum of multiple convex functions and thus can be non-convex. Therefore, gradient descent in Step 6-9 of Algorithm 1 can lead to either 1) w^ is a global minima of g^(w); or 2) w^ is a local minima of g^(w). Saddle points and local maxima do not exist on g^(w) because each piece of it is convex.

1) When w^ is a global minima of g^(w), we have g(w)  g^(w)  g^(w^)   · g(w^).

(13)

The first inequality is due to g(·)  g^(·). The second inequality is due to the global optimality of w^.

The third inequality is due to the approximation bound g^(·)   · g(·) guaranteed by the submodular

maximization in Step 7 of Algorithm 1.

2) When w^ is a local minima of g^(w), we have g^(w^) = 0. Let h(w) to be the piece of g^(w) where w^ is located, then w^ has to be a global minima of h(w) due to the convexity of h(w). Let A denote the ground set of A^ on all pieces of g^(w), we define an auxiliary convex function g~(w) as

g~(w) max L (yi, f (xi, w)) + F (A).
AA iA

(14)

It is convex because it is defined as the maximum of multiple convex function. So we have

g^(w)  g~(w)  g(w), w  Rm.

(15)

The first inequality is due to the definition of A, and the second inequality is a result of A  V by

comparing g(w) in Eq. (4) with g~(w) in Eq. (14). Let w~ denote a global minima of g~(w), we have

g(w)  g~(w)  g~(w~)  h(w~)  h(w^) = g^(w^)   · g(w^).

(16)

The first inequality is due to Eq. (15), the second inequality is due to the global optimality of w~ on

g~(w), the third inequality is due to the definition of g~(w) in Eq. (14) (g~(w) is the maximum of all

pieces of g^(w) and h(w) is one piece of them), the fourth inequality is due to the global optimaliy of w^ on h(w), the last inequality is due to the approximation bound g^(·)   · g(·) guaranteed by the

submodular maximization in Step 7 of Algorithm 1.

Therefore, in both cases we have g(w)   · g(w^). Applying it to Eq. (12) results in

w^ - w

2 2



2 k

1 - 1 · g(w). 

(17)

In Theorem 1, we analyze the upper bound for

w^ - w

2 2

based

on

two

assumptions:

1) the loss

L (yi, f (xi, w)) being -strongly convex w.r.t. w; and 2) w^ is achieved by running gradient descent

in Step 6-9 until convergence. In case the loss L (yi, f (xi, w)) is convex but not -strongly convex, a commonly used trick to modify it to -strongly convex is to add an 2 regularization (/2) w 22. In addition, for non-convex L (yi, f (xi, w)), it is possible to prove that with high probability, a noise

perturbed SGD on g^(w) can hit an -optimal local solution of g(w) in polynomial time steps. We

will leave this to our future works. In our empirical study (Section 3), MCL achieves compelling

performance when applied to deep neural nets when loss is usually non-convex.

In Step 5-10 of Algorithm 1, we stop gradient descent after p steps rather than waiting for convergence as in Assumption 2). This is because in the continuation scheme, wp is sufficiently good as an
initialization for the next iteration, and g^(w) is small enough after p steps due to the warm start from
its previous iteration.

6

Under review as a conference paper at ICLR 2018

In later stages of MCL when  is small, G(A) tends to be more modular, i.e., with small curvature G. As discussed in Section 2.1, when G is close to 0, lazy greedy can have approximation factor of  = (1 - e-G )/G, larger than 1 - e-1 and potentially close to 1. With g(w) upper bounded, the bound in Eq. (10) can be nearly 0. Hence, w^ obtained by our algorithm is sufficiently close to w.

2.3 ADDITIONAL EMPIRICAL IMPROVEMENTS
Step 6-7 of Algorithm 1 require computing the loss on all the available samples, and each step of submodular maximization needs to evaluate the marginal gains of all the unselected samples. This may lead to expensive computation and annotation cost (the loss computation need to know the labels) in practice. Empirically we use two tricks to achieve improvements in efficiency.

Firstly, instead of selecting individual samples into A, we select clusters. In particular, we replace the per-sample loss L (yi, f (xi, w)) with per-cluster loss L Y (i), f (X(i), w) that sums up losses of all samples in the cluster (X(i) is the ith cluster and Y (i) denotes the labels). To save annotation costs, we further approximate it by the loss on the sample closest to the cluster centroid, i.e.,

L Y (i), f (X(i), w)

jC(i) L (yj , f (xj , w))  |C(i)|L y(i), f (x(i), w) , (18)

where C(i) contains the indices of the samples in the cluster, and x(i) with label y(i) is the sample

closest to the centroid. In practice, the loss on x(i) is sufficiently representative to reflect the hardness

of the cluster. When computing F (A) reflecting the diversity of selected clusters, we use the cluster

centroid to represent each cluster. In Step 8, the gradient is computed on all the samples in the

selected clusters rather than on x(i). By using this method, we only need to annotate and compute the

loss for samples in the selected clusters and the representative samples x(i) of other clusters. The size

of ground set in submodular maximization is also reduced to the number of clusters.

We can further reduce the ground set to save computation during submodular maximization via pruning methods, which lead to zero loss (Wei et al., 2014a) or sufficiently small loss (Zhou et al., 2017) on objective G(A). In MCL, as  decreases and G(A) becomes close to modular, pruning method can rule out more elements. We provide more details and discuss other speedup methods for our submodular maximization in 4.2.

3 EXPERIMENTS

Dataset

News20 MNIST CIFAR10

In this section, we apply different curriculum learning methods to train a logistic regression model on 20newsgroups (Lang, 1995), LeNet5 on MNIST (Lecun et al., 1998), and a convolutional neural nets (2Conv-Pool-2Conv-Pool-3DenseLayer)2 on CIFAR10 (Krizhevsky & Hinton, 2009). Details on the datasets can be found

SGD(random) SPL SPLD MCL(,  = 0) MCL( = 0) MCL+random MCL+k

14.36 15.43 16.23 15.99 16.54 16.23 14.12

0.96 1.25 1.18 1.25 1.21 1.09 0.94

18.52 21.14 20.79 18.04 17.33 17.12 12.87

in Table 3 of (Anonymous, 2018). We Table 1: Test error rate (%) of different methods (for compare MCL and its variants to SPL SGD we show the lowest error of 10 random trials). (Kumar et al., 2010), SPLD (Jiang et al.,

2014) and SGD with a random curriculum (i.e., with random batches). They all use mini-batch SGD

(·, ) with the same learning rate strategy to update the parameters w. They differ only in the

curriculum, i.e., different training sequences.

In SPL and SPLD, the training set starts from a fixed size (4000 samples for 20newsgroups, 5000 samples for MNIST and CIFAR10), and increases by a factor µ = 0.1 per round of alternating minimization. The model w is updated after  passes of the selected training set per round. In SPLD, we further have a weight for negative group sparsity starting from  and increasing by a factor of 0.1 per round. We tried 5 different combinations of {, µ} and {, } for SPL and SPLD respectively. The best combination with the smallest error rate is reported. Although both SPL and SPLD can be reduced to SGD when  = 0, we do not include this special case because SGD is already a baseline. For SGD with a random curriculum, results of 10 independent trials are reported.
In our MCL experiments, a feature based submodular function (Wei et al., 2014b) is used for
regularization, i.e., F (A) = uU u cu(A), where U is a set of features. For a subset A of

2The "v3" network from https://github.com/jseppanen/cifar_lasagne.

7

Under review as a conference paper at ICLR 2018

clusters, cu(A) = iA cu(i), where cu(i) is the nonnegative score of centroid for cluster i. We use TF-IDF features for 20newsgroup, and the input feature to the output layer (given by ReLU) for MNIST and CIFAR10. The submodularity of F (A) holds because these vectors are nonnegative.

We consider four variants of MCL: 1) MCL

Dataset Total time
SUBMODULARMAX

News20
2649.19s 62.44s

MNIST
3418.97s 35.33s

CIFAR10
3677.73s 127.36s

with  = 0 and  = 0, having neither submodular regularization that promotes diversity nor scheduling of k that increases hardness; 2) MCL with  = 0, without sub-

Table 2: Total time (seconds) of MCL+k and the time modular regularization but with scheduling

spent only on SUBMODULARMAX.

of k; 3) MCL+random, which inserts one

round that randomly samples r clusters as training set A^ after every q rounds of the outer loop in

Algorithm 1; 4) MCL+k, which has scheduling of  and k, but does not use a random training set.

We tried 5 different combinations of {q, r} for MCL+random and 5 different  for MCL+k, and

report the one with the smallest test error. Other parameters such as initial values for , k, , p and

the total number of clusters are the same for different variants (exact values are given in Table 4 of

(Anonymous, 2018)).

90 50

SPL: 25,0.1

SPL: 25,0.1

80

SPLD: 25,0.4 MCL+random: 3,12

SPLD: 25,0.4 MCL+random: 3,12

MCL:  = 0,  = 0

MCL:  = 0,  = 0

70

MCL:  = 0

40

MCL:  = 0

MCL+k:  = 8

MCL+k:  = 8

60

30 50

Test error rate (%) Test error rate (%)

40 20
30

20 10
10

50000

6000

7000

8000

9000

10000

11000

Number of distinct training samples

00

20000

40000

60000

Number of training batches (batch size = 64)

80000

Figure 1: Test error rate (%) vs. number of distinct training sample (left) and number of training batches (right) on 20newsgroups (grey curves represents 10 random trials of SGD).

In MCL, SUBMODULARMAX is the only extra computation comparing to normal SGD. To show that its time cost is ignorable (but still brings a clear advantage as shown later), we report the total time cost of MCL+k and the time spent on SUBMODULARMAX in Table 2.

We summarize the main results in Figure 1-4. More results are given in (Anonymous, 2018). In all figures, grey curves correspond to the 10 trials of SGD with a random curriculum. The legend gives the parameters used in different methods of the following formats: 1) SPL: , µ; 2) SPLD: , ; 3) MCL+random: q, r.

90 90

SPL: 15,0.2

SPL:15,0.2

SPLD: 15,0.1

SPLD:15,0.1

80

MCL+random: 4,48

80

MCL+random: 4,48

MCL:  = 0,  = 0

MCL:  = 0,  = 0

MCL:  = 0

MCL:  = 0

70

MCL+k:  = 8

70

MCL+k:  = 8

60 60

Test error rate (%) Test error rate (%)

50 50

40 40

30 30

20 20

10 10000

20000

30000

Number of distinct training samples

40000

50000

100

10000

20000

30000

40000

Number of training batches (batch size = 128)

50000

Figure 2: Test error rate (%) vs. number of distinct training sample (left) and number of training batches (right) on CIFAR10 (grey curves represents 10 random trials of SGD).

Figure 1-2 show how the test error changes with the number of distinct training samples and the number of training batches, which reflects the training time. The left plot in each figure implies the "sample complexity" of different methods, i.e., how many distinct training samples are needed to achieve the error rate. The right plot shows the convergence rate.

8

Under review as a conference paper at ICLR 2018

On all datasets, MCL and its variants outperform SPL and SPLD for both sample complexity and convergence rate. They are slightly slower than SGD on convergence but can achieve much smaller error when using the same number of labeled samples. Moreover, when using the same learning rate strategy, they are more robust to overfitting, as shown in Figure 2. In addition, they reduce the error faster than others in later stages, and MCL+k always achieves the lowest test error, as shown in Table 1. Comparing Figure 1 with Figure 2-3, MCL has significant advantages when applied to deep models.

20

SPL: 15,0.1

SPLD: 25,0.01

MCL+random: 3,16

MCL:  = 0,  = 0

15

MCL:  = 0 MCL+k:  = 2

SPL: 15,0.1 SPLD: 25,0.01 MCL+random: 3,16 MCL:  = 0,  = 0 MCL:  = 0 MCL+k:  = 2

101 10

Test error rate (%) Test error rate (%)

5

0 10000

20000

30000 Number of distinct training samples

40000

50000

100 0

5000

10000

15000

20000

25000

30000

Number of training batches (batch size = 50)

Figure 3: Test error rate (%) vs. number of distinct training sample (left) and number of training batches (right) on MNIST (grey curves represents 10 random trials of SGD).

Among the four variants of MCL, MCL+k achieves the fastest convergence speed and the smallest test error, followed by MCL+random, MCL( = 0) and MCL( = 0,  = 0) (the only exception is the test error on News20 between the last two variants). This indicates that the diversity introduced by submodular regularization does bring improvement, and changing both hardness and diversity leads to better performance. The combination of MCL and random curriculum speedups the convergence, but still cannot outperform MCL+k. We can obtain similar ranking on sample complexity, but the differences among variants are small.

Number of distinct training samples Number of distinct training samples Number of distinct training samples

11000 10000 9000 8000 7000 6000 50000

20000

40000

60000

Number of training batches (batch size = 64)

SPL: 25,0.1 SPLD: 25,0.4 MCL+random: 3,12 MCL:  = 0,  = 0 MCL:  = 0 MCL+k:  = 8
80000

50000

SPL: 15,0.2

SPLD: 15,0.1

MCL+random: 4,48

MCL:  = 0,  = 0

MCL:  = 0

40000

MCL+k:  = 8

30000

20000

10000

0 102

103 104 Number of training batches (batch size = 128)

50000

SPL: 15,0.1

SPLD: 25,0.01

MCL+random: 3,16

MCL:  = 0,  = 0

MCL:  = 0

MCL+k:  = 2

40000

30000

20000

10000 105 0 5000 10000 15000 20000 25000 30000 35000 40000
Number of training batches (batch size = 50)

Figure 4: Number of distinct training samples vs. number of training batches for News20 (left), CIFAR10 (middle) and MNIST(right) (grey curves represents 10 random trials of SGD).

Figure 4 shows how the number of distinct training samples changes as training proceeds. It reflects the trade-off between "training on more new samples" vs. "training on fewer samples more often." MCL and its variants usually require much fewer labeled samples than SGD but more than SPL and SPLD. Considering their advantages on the smaller error, better sample complexity, and faster convergence, MCL achieves a promising trade-off.

REFERENCES
Naoki Abe and Hiroshi Mamitsuka. Query learning strategies using boosting and bagging. In ICML, pp. 1­9, 1998.
E. L. Allgower and Kurt Georg. Introduction to Numerical Continuation Methods. Society for Industrial and Applied Mathematics, 2003.
Anonymous Anonymous. Supplementary material for minimax curriculum learning. In Submitted to ICLR, 2018.

9

Under review as a conference paper at ICLR 2018
Sumit Basu and Janara Christensen. Teaching classification boundaries to humans. In AAAI, pp. 109­115, 2013.
Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse m-best solutions in markov random fields. In ECCV, pp. 1­16, 2012.
Mokhtar S. Bazaraa, Hanif D. Sherali, and C. M. Shetty. Nonlinear programming - theory and algorithms (2. ed.). Wiley, 1993.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798­1828, 2013.
Yoshua Bengio. Evolving Culture Versus Local Minima, pp. 109­138. Springer Berlin Heidelberg, 2014.
Yoshua Bengio, Je´ro^me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, pp. 41­48, 2009.
Ellen Bialystok, Fergus I. M. Craik, and Gigi Luk. Bilingualism: consequences for mind and brain. Trends in Cognitive Sciences, 16(4):240­250, 2012.
Michele Conforti and Gerard Cornuejols. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics, 7(3):251­274, 1984.
Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In AAAI, pp. 746­751, 2005.
Ido Dagan and Sean P. Engelson. Committee-based sampling for training probabilistic classifiers. In ICML, pp. 150­157, 1995.
Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In ICML, pp. 208­215, 2008.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
Farzan Farnia and David Tse. A minimax approach to supervised learning. In NIPS, pp. 4240­4248, 2016.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119­139, 1997.
Satoru Fujishige. Submodular functions and optimization. Annals of discrete mathematics. Elsevier, 2005.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal point processes. In NIPS, pp. 2735­2743, 2012.
Rishabh Iyer and Jeff Bilmes. Submodular point processes with applications in machine learning. In AISTATS, May 2015.
Rishabh Iyer, Stefanie Jegelka, and Jeff A. Bilmes. Fast semidifferential-based submodular function optimization. In ICML, 2013.
Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander G. Hauptmann. Self-paced learning with diversity. In NIPS, pp. 2078­2086, 2014.
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G. Hauptmann. Self-paced curriculum learning. In AAAI, pp. 2694­2700, 2015.
Faisal Khan, Bilge Mutlu, and Xiaojin Zhu. How do humans teach: On curriculum learning and teaching dimension. In NIPS, pp. 1449­1457, 2011a.
10

Under review as a conference paper at ICLR 2018
Faisal Khan, Xiaojin (Jerry) Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning and teaching dimension. In NIPS, pp. 1449­1457, 2011b.
A´ gnes Melinda Kova´cs and Jacques Mehler. Flexible learning of multiple speech structures in bilingual infants. Science, 325(5940):611­612, 2009.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, pp. 1189­1197, 2010.
Gert R.G. Lanckriet, Laurent El Ghaoui, Chiranjib Bhattacharyya, and Michael I. Jordan. A robust minimax approach to classification. Journal of Machine Learning Research (JMLR), 3:555­582, 2003.
Ken Lang. Newsweeder: Learning to filter netnews. In ICML, pp. 331­339, 1995.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie Glance. Cost-effective outbreak detection in networks. In SIGKDD, pp. 420­429, 2007.
Ping Li, Jennifer Legault, and Kaitlyn A. Litcofsky. Neuroplasticity as a function of second language learning: Anatomical changes in the human brain. Cortex, 58:301­324, 2014.
Andrea Mechelli, Jenny T. Crinion, Uta Noppeney, John O'Doherty, John Ashburner, Richard S. Frackowiak, and Cathy J. Price. Neurolinguistics: Structural plasticity in the bilingual brain. Nature, 431(7010):757­757, 2004.
Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27, pp. 234­243. Springer Berlin Heidelberg, 1978.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondra´k, and Andreas Krause. Lazier than lazy greedy. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 1812­1818, 2015.
Douglas C. Montgomery. Design and Analysis of Experiments. John Wiley & Sons, 2006.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions?I. Mathematical Programming, 14(1):265­294, 1978.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, 2004.
Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1): 127­152, 2005.
Kaustubh R Patil, Xiaojin Zhu, L ukasz Kopec´, and Bradley C Love. Optimal teaching for limitedcapacity human learners. In NIPS, pp. 2465­2473, 2014.
Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets. In NIPS, pp. 2645­2653, 2014.
Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197­227, 1990.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for information extraction. In CAIDA, pp. 309­318, 2001.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin, Madison, 2010.
H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In COLT, pp. 287­294, 1992.
11

Under review as a conference paper at ICLR 2018
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby Steps: How "Less is More" in unsupervised dependency parsing. In NIPS 2009 Workshop on Grammar Induction, Representation of Language and Language Learning, 2009.
James Steven Supancic III and Deva Ramanan. Self-paced learning for long-term tracking. In CVPR, pp. 2379­2386, 2013.
Kevin Tang, Vignesh Ramanathan, Li Fei-fei, and Daphne Koller. Shifting weights: Adapting object detectors from image to video. In NIPS, pp. 638­646, 2012a.
Ye Tang, Yu-Bin Yang, and Yang Gao. Self-paced dictionary learning for image classification. In MM, pp. 833­836, 2012b.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Fast multi-stage submodular maximization. In ICML, 2014a. Kai Wei, Yuzong Liu, Katrin Kirchhoff, Chris D. Bartels, and Jeff A. Bilmes. Submodular subset
selection for large-scale speech training data. In IEEE International Conference on Acoustics, Speech and Signal Processing, (ICASSP) 2014, pp. 3311­3315, 2014b. Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In ICML, 2015. Tianyi Zhou, Hua Ouyang, Jeff Bilmes, Yi Chang, and Carlos Guestrin. Scaling submodular maximization via pruned submodularity graphs. In AISTATS, 2017. Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward optimal education. In AAAI, pp. 4083­4087, 2015.
12

Under review as a conference paper at ICLR 2018

4 APPENDIX

4.1 PROOF OF PROPOSITION 1

Proof. Let g(x) = maxi gi(x), where gi(x) is -strongly convex for any i. According to a definition of strongly convex function given in Theorem 2.1.8 of (Nesterov, 2004), let   [0, 1], we have

gi(x

+

(1

-

)y)



gi(x)

+

(1

-

)gi(y)

-

 2

(1

-

)

x-y

22, i.

The following proves that g(x) is also -strongly convex:

g(x + (1 - )y) = max gi(x + (1 - )y)
i

 max [gi(x) + (1 - )gi(y)] -
i

 (1 - ) 2

x-y

2 2



max
i

gi(x)

+

max(1
i

-

)gi(y)

-

 2

(1

-

)

x-y

2 2

=g(x) + (1 - )g(y) -

 (1 - ) 2

x-y

22.

4.2 SUBMODULAR MAXIMIZATION USED IN OUR IMPLEMENTATION

In our implementation, we use two strategies to accelerate the computation: 1) reduce the size of ground set by pruning methods before running any submodular maximization algorithm, we use (Wei et al., 2014a) here for its simplicity but running (Zhou et al., 2017) afterwards can reduce additional elements; 2) update A by a subgradient ascent step (which costs |V | function evaluations) of majorization-minimization (MM) scheme (Iyer et al., 2013) when a targeted approximation factor  has already been achieved on A^ from the previous iteration, otherwise the lazy greedy algorithm is applied.

Algorithm 2 SUBMODUARMAX(G, k, A^,   [0, 1))

1: input: G(·), k, A^,  2: output: A^ 3: Reduce ground set V : arrange V non-increasingly in terms of G(i|V \i) in a permutation  where
(k) is the kth element, V  {i  V |G(i)  G((k)|V \(k))}; 4: Compute upper bound to maximum of Eq. (5):

 = max

L yi, f (xi, wt) + F (i)

AV,|A|k

iA

5: if G(A^)   ·  then

6: Permutation  of V : the first k elements Sk = A^ with random order, the rest elements in 

with non-decreasing G(i|V \i)/G(i);

7: Define modular function hA^(A)

iA hA^(i) and compute hA^((i)) = G(Si) - G(Si-1);

8: Compute lower bound L(A) of G(A):

L(A)
9: A^  argmaxAV ,|A|k L(A); 10: else 11: A^  LazyGreedy(G, V , k); 12: end if

G(A^) + hA^(A) - hA^(A^)

The complete algorithm is given in Algorithm 2. Step 3 denotes the pruning procedure given in (Wei et al., 2014a). Step 4 compute an upper bound  for maxAV,|A|k G(A) due to submodularity. Step 5-12 performs submodular maximization: if the targeted approximation factor  is already achieved on A^ we run subgradient ascent to further improve it, otherwise a lazy greedy algorithm is applied as in Step 11. Note the modular maximization in Step 4 and Step 9 simply needs to find the top k
elements from V with the largest objective value. Step 6-9 is identical to one step of MM scheme (Iyer et al., 2013) except Step 6 which arranges the elements V \A^ in a non-decreasing order of

13

Under review as a conference paper at ICLR 2018

G(i|V \i)/G(i). The set of the first i elements in permutation  is denoted by Si in Step 7. Step 6 places fewer elements before the ones with small G(i|V \i)/G(i) in , and intuitively tends to

reduce the gap between L(A) and G(A) for any subset A. To see this, let Ai-1 {(j)  A|j < i} for any i  A, the gap we want to reduce is

G(A) - L(A) =

(i)A G((i)) ·

G( (i)|Ai-1 ) G((i))

-

hA^ ((i)) G((i))

,

(19)

where the second term in the brackets is

hA^((i))

=

G((i)|Si-1)



G((i)|V

\(i)) .

G((i))

G((i))

G((i))

(20)

The elements (i) with small rank i associates with small Si-1, which often results in (but does not guarantee) small hA^((i))/G((i)) due to submodularity. According to Step 6, the elements

with large rank in  has large G((i)|V \(i))/G((i)), the lower bound for hA^((i))/G((i)) in Eq. (20). Together they indicate large hA^((i))/G((i)) and thus a small gap in Eq. (19).

It has been proved in (Iyer et al., 2013) that Step 6-9 produces a non-decreasing G(A^), i.e., if A^+ is

the optimal A^ achieved in Line 9, we have

G(A^+)  L(A^+)  L(A^) = G(A^).

(21)

The first inequality is due to that L(·) is the lower bound of G(A); the second inequality is due to the optimality of A^+; the equality is due to Sk = A^.
Lemma 1. Algorithm 2 outputs a solution A^ such that G(A^)  min{, LG} · maxAV,|A|k G(A), where LG is the approximation factor of lazy greedy.

Proof. Firstly, let A denote the optimal solution to Eq. (5):

A argmax L yi, f (xi, wt) + F (A).
AV,|A|k iA
We prove that  in Step 4 is an upper bound to G(A) as follows.

(22)

 = 

iA [L (yi, f (xi, wt)) + F (i)]

iA iA

L L

(yi, (yi,

f f

(xi, (xi,

wt)) wt))

+ +

 F

(AiA).

F

(i)

(23)

The first inequality is due to the fact that  is the maximum of the objective in Step 4; the last

inequality is due to submodularity, which guarantees F (i)  F (i|B) for any B  V .

When G(A^)   ·  (Step 5), as we discussed before, the subgradient ascent in Step 6-9 does not decrease the objective, so we have G(A^)   · maxAV,|A|k G(A) for A^ obtained in Step 9.
Otherwise, we run lazy greedy on the reduced ground set V . According to Lemma 1 in (Wei et al., 2014a), the ground set reduction in Step 3 does not change the objective value achieved by lazy greedy. So we have G(A^)  LG · maxAV,|A|k G(A) for A^ obtained in Step 11. This completes the proof.

Algorithm 2 simply combines three proposed submodular maximization techniques, but effectively accelerates the update of A^ in practice. Firstly, as  decreasing, G(A) turns to be more modular, and G(i|V \i) becomes closer to G(i). So the pruning in Step 3 becomes similar to sorting, and rules out more elements from V . Secondly, G(A) changes smoothly with wt between two iterations within Algorithm 1. When the learning rate  is small, A^ from the previous iteration can still achieve a large approximation factor, in which case running lazy greedy is not necessary, and Algorithm 2 thus uses faster subgradient ascent to update A^.
This mechanism of setting targeted approximation factor  provides a trade-off between  and computational cost: if  is large, we can obtain large approximation factor but the computation might be expensive since lazy greedy is used more frequently; if  is small, single subgradient ascent will be used more often and the computational cost reduces, but the approximation factor also decrease. Note if we set  = 1, Algorithm 2 is forced to run lazy greedy to update A^.
In experiments, we set  = 0.5. Though  = 1 might lead to smaller approximation error, empirically  = 0.5 often results in less computation for Algorithm 2 while maintaining promising performance.

14

Under review as a conference paper at ICLR 2018

Dataset
#Training #Test #Feature #Class

News20
11314 7532 129791 20

MNIST
50000 10000 28 × 28
10

CIFAR10
50000 10000 32 × 32
10

Table 3: Information of datasets.

Dataset News20 MNIST CIFAR10

p #cluster  initial k initial  initial 

50 200 0.05
4 6 × 10-6
3.5

50 1000 0.05
4 1 × 10-6
0.02

20 1000 0.05
4 1 × 10-7
0.01

Table 4: Parameters of MCL (Algorithm 1) and its variants for different datasets.

90 50

SPL: 25,0.1

SPL: 25,0.1

80

SPLD: 25,0.4 MCL+random: 3,12

SPLD: 25,0.4 MCL+random: 3,12

70

MCL:  = 0,  = 0 MCL:  = 0

40

MCL:  = 0,  = 0 MCL:  = 0

MCL+k:  = 8

MCL+k:  = 8

60

30 50

Training error rate (%)

Training error rate (%)

40 20
30

20 10 10

50000

6000

7000

8000

9000

10000

11000

Number of distinct training samples

00

20000

40000

60000

Number of training batches (batch size = 64)

80000

Figure 5: Training error rate (%) vs. number of distinct training sample (left) and number of training

batches (right) on 20newsgroups (grey curves represents 10 random trials of SGD).

3.0 3.0

SPL: 25,0.1

SPLD: 25,0.4

MCL+random: 3,12

2.5

MCL:  = 0,  = 0

2.5

MCL:  = 0

MCL+k:  = 8

SPL: 25,0.1 SPLD: 25,0.4 MCL+random: 3,12 MCL:  = 0,  = 0 MCL:  = 0 MCL+k:  = 8

2.0 2.0

Training loss

Training loss

1.5 1.5

1.0 1.0

0.5 0.5

0.50000

6000

7000

8000

9000

10000

11000

Number of distinct training samples

0.00

20000

40000

60000

Number of training batches (batch size = 64)

80000

Figure 6: Training loss vs. number of distinct training sample (left) and number of training batches (right) on 20newsgroups (grey curves represents 10 random trials of SGD).

3.0 3.0

SPL: 25,0.1

SPL: 25,0.1

SPLD: 25,0.4

SPLD: 25,0.4

MCL+random: 3,12

MCL+random: 3,12

2.5

MCL:  = 0,  = 0

2.5

MCL:  = 0,  = 0

MCL:  = 0

MCL:  = 0

MCL+k:  = 8

MCL+k:  = 8

2.0 2.0

Test loss

Test loss

1.5 1.5

1.0 1.0

0.5 0.5

0.50000

6000

7000

8000

9000

10000

11000

Number of distinct training samples

0.00

20000

40000

60000

Number of training batches (batch size = 64)

80000

Figure 7: Test loss vs. number of distinct training sample (left) and number of training batches (right) on 20newsgroups (grey curves represents 10 random trials of SGD).

15

Under review as a conference paper at ICLR 2018

3.0 3.0

SPL: 15,0.2

SPL: 15,0.2

SPLD: 15,0.1

SPLD: 15,0.1

MCL+random: 4,48

MCL+random: 4,48

2.5

MCL:  = 0,  = 0

2.5

MCL:  = 0,  = 0

MCL:  = 0

MCL:  = 0

MCL+k:  = 8

MCL+k:  = 8

2.0 2.0

Training loss

Training loss

1.5 1.5

1.0 1.0

0.5 0.5

0.0 10000

20000

30000

Number of distinct training samples

40000

50000

0.00

10000

20000

30000

40000

50000

60000

70000

Number of training batches (batch size = 128)

Figure 8: Training loss vs. number of distinct training sample (left) and number of training batches (right) on CIFAR10 (grey curves represents 10 random trials of SGD).

3.0 3.0

SPL: 15,0.2

SPL: 15,0.2

SPLD: 15,0.1

SPLD: 15,0.1

MCL+random: 4,48

MCL+random: 4,48

2.5

MCL:  = 0,  = 0

2.5

MCL:  = 0,  = 0

MCL:  = 0

MCL:  = 0

MCL+k:  = 8

MCL+k:  = 8

2.0 2.0

Test loss

Test loss

1.5 1.5

1.0 1.0

0.5 0.5

0.0 10000

20000

30000

Number of distinct training samples

40000

50000

0.00

10000

20000

30000

40000

50000

60000

70000

Number of training batches (batch size = 128)

Figure 9: Test loss vs. number of distinct training sample (left) and number of training batches (right) on CIFAR10 (grey curves represents 10 random trials of SGD).

20

SPL: 15,0.1

SPLD: 25,0.01

MCL+random: 3,16 MCL:  = 0,  = 0

15

MCL:  = 0 MCL+k:  = 2

SPL: 15,0.1 SPLD: 25,0.01 MCL+random: 3,16 MCL:  = 0,  = 0 MCL:  = 0 MCL+k:  = 2

101 10

Training error rate (%)

Training error rate (%)

5 100

0 10000

20000

30000 Number of distinct training samples

40000

50000

0

5000

10000

15000

20000

25000

30000

Number of training batches (batch size = 50)

Figure 10: Training error rate (%) vs. number of distinct training sample (left) and number of training batches (right) on MNIST (grey curves represents 10 random trials of SGD).

16

