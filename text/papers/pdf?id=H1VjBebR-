Under review as a conference paper at ICLR 2018
THE ROLE OF MINIMAL COMPLEXITY FUNCTIONS IN UNSUPERVISED LEARNING OF SEMANTIC MAPPINGS
Anonymous authors Paper under double-blind review
ABSTRACT
We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem. We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture. Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.
1 INTRODUCTION
Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples. For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN (Zhu et al., 2017) creates the analog zebra image for a new image of a horse and vice versa.
These recent methods employ two types of constraints. First, when mapping from one domain to another, the output has to be indistinguishable from the samples of the new domain. This is enforced using GANs (Goodfellow et al., 2014) and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa. The second type of constraint enforces that for every single sample, transforming it to the other domain and back (by a composition of the mappings in the two directions) results in the original sample. This is enforced for each training sample from either domain: every training image of a horse (zebra), which is mapped to a zebra (horse) image and then back to the source domain, should be as similar as possible to the original input image.
In another example, taken from DiscoGAN (Kim et al., 2017), a function is learned to map a handbag to a shoe of a similar style. One may wonder why striped bags are not mapped, for example, to shoes with a checkerboard pattern. If every striped pattern in either domain is mapped to a checkerboard pattern in the other and vice-versa, then both the distribution constraints and the circularity constraints might hold. The former could hold since both striped and checkerboard patterned objects would be generated. Circularity could hold since, for example, a striped object would be mapped to a checkerboard object in the other domain and then back to the original striped object.
One may claim that the distribution of striped bags is similar to those of striped shoes and that the distribution of checkerboard patterns is also the same in both domains. In this case, the alignment follows from fitting the shapes of the distributions. This explanation is unlikely, since no effort is
1

Under review as a conference paper at ICLR 2018

being made to create handbags and shoes that have the same distributions of these properties, as well as many other properties.
Our work is dedicated to the alternative hypothesis that the target mapping is implicitly defined by being approximated by the lowest-complexity mapping that has a low discrepancy between the mapped samples and the target distribution, i.e., the property that even a good discriminator cannot distinguish between the generated samples and the target ones. In Sec. 2 we explore the inherent ambiguity of cross domain mapping. In Sec. 3, we present the hypothesis and two verifiable predictions, as well as a new unsupervised mapping algorithm. In Sec. 4, we show that the number of minimal complexity mappings is expected to be small. Sec. 5 verifies the various predictions. Some context to our work, including classical ideas such as Occam's Razor, MDL, and Kolmogorov complexity are discussed in Sec. 6.

2 THE UNSUPERVISED ALIGNMENT PROBLEM

The learning algorithm is provided with only two unlabeled datasets: one includes i.i.d samples from the first distribution and the second includes i.i.d samples from the other distribution (all notations are listed in Appendix B, Tab. 5).

xi  XA for i = 1 . . . m where xi i.i.d DA and XA denotes the space of domain A = (XA, DA)
xj  XB for j = 1 . . . n where xj i.i.d DB and XB denotes the space of domain B = (XB, DB) (1)

To semantically tie the two distributions together, a generative view can be taken. This view is well aligned with the success of GAN-based image generation, e.g., (Radford et al., 2015), in mapping random input vectors into realistic-looking images. Let z  X be a random vector that is distributed according to the distribution DZ and which we employ to denote the semantic essence of samples in XA and XB. We denote DA = yA  DZ and DB = yB  DZ ,
where the functions yA : X  XA and yB : X  XB, and f  D denotes the distribution of f (x), where x  D. Following the circularity-based methods (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017), we assume that both yA and yB are invertible.

The assumption of invertibility is further justified by the recent success of supervised pre-image computation methods (Dosovitskiy & Brox, 2016). In unsupervised learning, given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision (Chen et al., 2016). However, if the target function between domains A and B is not invertible, because for each member of A there are a few possible members of B (or vice versa), we can add a stochastic component to A that is responsible for choosing which member in B to take, given a member of A. For example, if A is a space of handbag images and B is a space of shoes, such that for every handbag, there are a few analogous shoes, then a stochastic variable can be added such that given a handbad, one shoe is selected among the different analog shoes.
We denote by yAB = yB  yA-1, the function that maps the first domain to the second domain. It is semantic in the sense that it goes through the shared semantic space X . The goal of the learner is to fit a function h  H, for some hypothesis class H that is closest to yAB,

inf
hH

RDA

[h,

yAB

],

(2)

where RD[f1, f2] = ExD (f1(x), f2(x)), for a loss function : R × R  R and a distribution D.

It is not clear that such fitting is possible without further information. Assume, for example, that there is a natural order on the samples in XB. A mapping that transforms an input sample x  XA to the sample that is next in order to yAB(x), could be just as feasible. More generally, one can permute the samples in XA by some function  that replaces each sample with another sample that has a similar likelihood (see Def. 1 below) and learn h that satisfies h = yAB  . We call this difficulty "the
alignment problem" and our work is dedicated to understanding the plausibility of learning despite
this problem.

In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed. Circularity requires the recovery of both yAB and yBA = yA  yB-1

2

Under review as a conference paper at ICLR 2018

simultaneously. Namely, functions h and h are learned jointly by minimizing the risk:

h,hinf HdiscC(h  DA, DB) + discC(h  DB, DA) + RDA [h  h, IdA] + RDB [h  h , IdB]

(3)

where discC(D1, D2) = supc1,c2C |RD1 [c1, c2] - RD2 [c1, c2]| denotes the discrepancy between distributions D1 and D2 that is implemented with a GAN (Ganin et al., 2016).

The first term in Eq. 3 ensures that the samples generated by mapping domain A to domain B follow the distribution of samples in domain B. The second term is the analog term for the mapping in the other direction. The last two terms ensure that mapping a sample from one domain to the second and
back, results in the original sample.

While the circularity constraints, expressed as the last two terms in Eq. 3, are elegant and do not

require additional supervision, for every invertible permutation  of the samples in domain B (not

to be confused with a permutation of the vector elements of the representation of samples in B) we

have (h  -1)  (  h) = h  h  IdA, and
(  h)  (h  -1) =   (hh )  -1    IdB  -1 = IdB.

(4)

Therefore, every circularity preserving pair h and h gives rise to many possible solutions of the form h~ = h   and h~ = -1  h . If  happens to satisfy DB(x)  DB((x)), then the discrepancy terms in Eq. 3 also remain largely unchanged. Circularity by itself cannot, therefore, explain the
recent success of unsupervised mapping.

3 THE SIMPLICITY HYPOTHESIS
Despite the availability of a large number of alternative hypotheses h that satisfy the constraints of Eq. 3, the methods of Xia et al. (2016); Kim et al. (2017); Zhu et al. (2017); Yi et al. (2017) enjoy empirical success, Why?
Our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic function. We further hypothesize that when performing research in unsupervised mapping, goldilock architectures are selected. These architectures are complex enough to allow small discrepancies but not complex enough to support mappings that are not minimal in complexity. By doing so, one of the minimal-complexity low-discrepancy mappings is learned.

3.1 AN ILLUSTRATIVE EXAMPLE

In order to illustrate our hypothesis, we present a very simple toy example, depicted in Fig. 1. Consider the domain A of uniformly distributed points (x1, x2)  R2, where 0  x1 < 1 and
x2 = 0.5. Let B be a similar domain, except x2 = 2. We are interested in learning the mapping yA2DB((x1, 0.5) ) = (x1, 2) . We note that there are infinitely many mappings from domain A to B that satisfy the constraints of Eq. 3.

However, when we learn the mapping using a neural network with one hidden layer of size 2, and

Leaky ReLU a(W x + b),

activations1 (Maas for W  R2×2,b 

eRt 2ala.,n2d0w13h)e,ryeA2DBa

is is

one of only applied per

two options. In this case h(x) = coordinate. The only admissible

solutions are of the form Wb =

1 -2b1 0 4 - 2b2

or Wb =

-1 1 - 2b1 0 4 - 2b2

and b = (b1, b2) ,

which are identical, for every b, to yA2DB or to an alternative yA2DB ((x1, 0.5) ) = (1 - x1, 2) . Exactly the same situation holds for any pair of line segments in Rd+.

Therefore, by restricting the hypothesis space of h, we eliminate all alternative solutions, except two. These two are exactly the two mappings that would commonly be considered "more semantic" than any other mapping, and can be expressed as the simplest possible mapping through a shared one dimensional space. While this is an extreme example, we believe that the principle is general since

1a(x) = Ind[x < 0]ax + Ind[x  0]x, for the indicator function Ind[q] which maps a true value to one, zero otherwise.

3

Under review as a conference paper at ICLR 2018

Figure 1: An illustrative example where the two domains are line segments in R2. There are infinitely many mappings that preserve the uniform distribution on the two segments. However, only two stand
out as "semantic". These are exactly the two mappings that can be captured by a neural network with only two hidden neurons and Leaky ReLU activations, i.e., by a function h(x) = a(W x + b), for a weight matrix W and the bias vector b.

limiting the complexity of the admissible solutions eliminates the solutions that are derived from yAB by permuting the samples in the space XA, because such mixing requires added complexity.

3.2 A COMPLEXITY MEASURE FOR FUNCTIONS

In this work, we focus on functions of the form

f := F [Wn+1, ..., W1] = Wn+1    ...    W2    W1

(5)

here, W1, ..., Wn+1 are invertible linear transformations from RM to itself. In addition,  is a non-linear element-wise activation function. We will mainly focus on  that is Leaky ReLU with parameter 0 < a = 1. In addition, for any function f , we define the complexity of f , denoted by C(f ) as the minimal number n such that there are invertible linear transformations W1, ..., Wn+1 that satisfy f = F [Wn+1, ..., W1].
Our function complexity framework, therefore, measures the complexity of a function as the depth of a neural network which implements it, or the shallowest network, if there are multiple such networks. In other words, we use the number of layers of a network as a proxy for the Kolmogorov complexity of functions, using layers in lieu of the primitives of the universal Turing machines, which is natural for studying functions that can be computed by feedforward neural networks.
Note that capacity is typically controlled by means of norm regularization, which is optimized during training. Here, the architecture is bounded to a certain number of layers. This measure of complexity is intuitive and provides a clear and stable stratification of functions.
Norm capacity (for norms larger than zero) are not effective in comparing functions of different architectures. In Sec. 5, we demonstrate that the L1 and L2 norms of the desired mapping are within the range of norms that are obtained when employing bigger or smaller architectures. Other ways to define the complexity of functions, such as the VC-dimension (Vapnik & Chervonenkis, 1971b) and Rademacher complexity (Bartlett & Mendelson, 2003), are not suitable for measuring the complexity of individual functions, since their natural application is in measuring the capacity of classes of functions.

4

Under review as a conference paper at ICLR 2018

3.3 CONSEQUENCES OF THE SIMPLICITY HYPOTHESIS
The simplicity hypothesis leads to concrete predictions, which are verified in Sec. 5. The first one states that in contrast to the current common wisdom, one can learn a semantically aligned mapping between two spaces without any matching samples and even without circularity.
Prediction 1. When learning with a small enough network in an unsupervised way a mapping between domains that share common characteristics, the GAN constraint in the target domain is sufficient to obtain a semantically aligned mapping.
The strongest clue that helps identify the alignment of the semantic mapping from the other mappings is the suitable complexity of the network that is learned. A network with a complexity that is too low cannot replicate the target distribution, when taking inputs in the source domain (high discrepancy). A network that has a complexity that is too high, would not learn the minimal complexity mapping, since it could be distracted by other alignment solutions.
We believe that the success of the recent methods results from selecting the architecture used in an appropriate way. For example, DiscoGAN (Kim et al., 2017) employs either eight or ten layers, depending on the dataset. We make the following prediction:
Prediction 2. When learning in an unsupervised way a mapping between domains, the complexity of the network needs to be carefully adjusted.
This prediction is also surprising, since in supervised learning, extra depth is not as detrimental, if at all. As far as we know, this is the first time that this clear distinction between supervised and unsupervised learning is made 2.

3.4 ALIGNMENT WITH NON-MINIMAL ARCHITECTURES

If the simplicity hypothesis is correct, then in order to capture the target alignment, one would need to learn with the minimal complexity architecture that supports a small discrepancy. However, deeper architectures can lead to even smaller discrepancies and to better outcomes.

In order to enjoy both the alignment provided by our hypothesis and the improved output quality, we propose to find a function h of a non-minimal complexity k2 that minimizes the following objective function

min
h s.t C(h)=k2

disc(h



DA,

DB

)

+



g

s.t

inf
C (g )=k1

RDA

[h,

g]

(6)

where k1 is the minimal complexity for mapping with low discrepancy between domain A and domain B. In other words, we suggest to find a function h that is both a high complexity mapping
from domain A to B and is close to a function of low complexity that has low discrepancy.

There are alternative ways to implement an algorithm that minimizes the objective function presented
in Eq. 6. Assuming, based on this equation, that for h that minimizes the objective function, the corresponding g = arg inf RDA [h, g] has a (relatively) small discrepancy, leads to a two-step
g s.t C(g)=k1
algorithm. The algorithm first finds a function g that has small complexity and small discrepancy
and then finds h of a larger complexity that is close to g. This is implemented in Alg. 1. Note that
in the first step, k1 is being estimated, for example, by gradually increasing its value, until g with a discrepancy lower than a threshold 0 is found. We suggest to use a liberal threshold, since the goal of the network g is to provide alignment and not the lowest possible discrepancy.

4 COUNTING MINIMAL COMPLEXITY MAPPINGS
Recall, from Sec. 2, that disc is the discrepancy distance, which is based on the optimal discriminator. Also discussed were the functions , that switches between members in the domain B that have similar probabilities. These are defined using the discrepancy distance as follows (simplified version; the definitions and results of this section are stated more broadly in Appendix A):
2The MDL literature was developed when people believed that small hypothesis classes are desired for both supervised and unsupervised learning.

5

Under review as a conference paper at ICLR 2018

Algorithm 1 Complexity Based Regularization Alignment
Require: Unlabeled training sets SA i.i.d DAm and SB  DBn , a desired complexity k2, and a trade-off parameter 
1: Identify a complexity k1, which leads to a small discrepancy min disc(g  DA, DB).
g s.t: C(g)=k1
2: Train g of complexity k1 to minimize disc(g  DA, DB). 3: Train h of complexity k2 to minimize disc(h  DA, DB) + RDA [h, g].

Definition 1 (Density preserving mapping). Let X = (X , DX ) be a domain. A 0-density preserving mapping over X (or an 0-DPM for short) is a function  such that

disc(  DX , DX )  0

(7)

We denote the set of all 0-DPMs of complexity k by DPM 0 (X; k)  disc(  DX , DX )  0 and C() = k .

:=

Below, we define a similarity relation between functions that reflects whether the two are similar. In this way, we are able to bound the number of different (non-similar) minimal complexity mappings by the number of different DPMs.
Definition 2. Let D be a distribution. We denote f D g, if C(f ) = C(g) and there are mini-
0
mal decompositions: f = F [Wn+1, ..., W1] and g = F [Vn+1, ..., V1] such that: i  [n + 1] : disc(F [Wi, ..., W1]  D, F [Vi, ..., V1]  D)  0.

Put differently, two functions of the same complexity have this relation, if for every step of their processing, the activations of the matching functions are similar.
The defined relation is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions have the closeness property. We count the number of functions as the minimal number of subsets required in order to cover the entire space. This quantity is denoted by N(U, U ) where U is the set and U is the closeness relation. The formal presentation is in Def. 9, which slightly generalizes the notion of covering numbers (Anthony & Bartlett, 2009).
Informally, the following theorem states that the number of minimal low-discrepancy mappings is upper bounded by both the number of DPMs of size 2CA0,B over DA and over DB. This result is useful, since DPMs are expected to be rare in real-world domains. When imagining mapping a space to itself, in a way that preserves the distribution, one first considers symmetries. Near-perfect symmetries are rare in natural domains, and when these occur, e.g., (Kim et al., 2017), they form wellunderstood ambiguities. Another option that can be considered is that of replacing specific samples in domain B with other samples of the same probability. However, these very local discontinuous mappings are of very high complexity, since this complexity is required for reducing the modeling error for discontinuous functions. One can also consider replacing larger sub-domains with other sub-domains such that the distribution is preserved. This could be possible, for example, if the distribution within the sub-domains is almost uniform (unlikely), or if it is estimated inaccurately due to the limitations of the training set.
We, therefore, make the following prediction.
Prediction 3. The number of DPMs of low complexity is small.

Given two domains A and B, there is a certain complexity CA0,B, which is the minimal complexity of the networks needed in order to achieve discrepancy smaller than 0 for mapping the distribution DA to the distribution DB. The set of minimal complexity mappings, i.e., mappings of complexity CA0,B that achieve 0 discrepancy is denoted by H 0 (A, B) :=
h C(h)  CA0,B and disc(h  DA, DB)  0 . The following theorem shows that the cover-
ing number of this set is similar to the covering number of the DPMs. Therefore, if prediction 3 above holds, the number of minimal low-discrepancy mappings is small.
Theorem 1 (Informal). Let  be a Leaky ReLU with parameter 0 < a = 1 and assume identifiability. Let 0, 1 and 2 < 1 be three positive constants and A = (XA, DA) and B = (XB, DB) are two

6

Under review as a conference paper at ICLR 2018

domains. Then,



N

H 0 (A, B), DA
1

N   min
N 

DPM 0 DPM 0

A; 2CA0,B B; 2CA0,B

, DA
2
, DB

2

(8)

Proof. See Appendix D.
The theorem assumes identifiability. In the context of neural networks, the general question of uniqueness up to invariants, also known as identifiability, is an open question. Several authors have made progress in this area for different neural network architectures. The most notable work has been done by Fefferman & Markel (1993) that proves identifiability for  = tanh. Furthermore, the representation is unique up to some invariants. Other works (Williamson & Helmke, 1995; F. Albertini & Maillot, 1993; Kurková & Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions. Similarly, in Lem. 3 in the Appendix, we show that identifiability holds for Leaky ReLU networks with one hidden layer.

5 EXPERIMENTS
The first group of experiments is dedicated to test the validity of the three predictions made, in order to give further support to the simplicity hypothesis. Next, we evaluate the success of the proposed algorithm in comparison to the DiscoGAN method of Kim et al. (2017).
We chose to experiment with the DiscoGAN architecture since it focuses on semantic tasks that contain a lesser component of texture or style transfer. The CycleGAN architecture of Zhu et al. (2017) inherits much from the style transfer architecture of Pix2Pix Isola et al. (2017), and the discrepancy term is based on a patch-based analysis, which introduces local constraints that could mask the added freedom introduced by adding layers. In addition, the U-net architecture of Ronneberger et al. (2015) used by Isola et al. (2017) deviates from the connectivity pattern of our model.
Experiments in this architecture and with the architecture of DualGAN (Yi et al., 2017), which focuses on tasks similar to CycleGAN, and shares many of the architectural choices, including U-nets and the use of patches, are left for future work.
5.1 EMPIRICAL VALIDATION OF THE PREDICTIONS
Prediction 1 states that since the unsupervised mapping methods are aimed at learning minimal complexity low discrepancy functions, GANs are sufficient. In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA  XB, based only on the GAN constraint on B, is presented as a failing baseline. In (Yi et al., 2017), among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful. However, this goes unnoticed.
In order to validate the prediction that a purely GAN based solution is viable, we conducted a series of experiments using the DiscoGAN architecture and GAN loss only. We consider image domains A and B, where XA = XB = R3×64×64.
In DiscoGAN, the generator is build of: (i) an encoder consisting of convolutional layers with 4 × 4 filters followed by Leaky ReLU activation units and (ii) a decoder consisting of deconvolutional layers with 4 × 4 filters followed by a ReLU activation units. Sigmoid is used for the output layer. Between four to five convolutional/deconvolutional layers are used, depending on the domains used in A and B (we match the published code architecture per dataset). The discriminator is similar to the encoder, but has an additional convolutional layer as the first layer and a sigmoid output unit.
The first set of experiments considers the CelebA face dataset. Transformations are learned between the subset of images labeled as male and those labeled as female, as well as from blond to black hair and eyeglasses to no eyeglasses. The results are shown in Fig. 2, 3, and 4, (resp.). It is evident that the output image is highly related to the input images.
In the case of mapping handbags to shoes, as seen in Fig. 5, the GAN does not provide a meaningful solution. However, in the case of edges to shoes and vice versa (Fig. 6), the GAN solution is successful.

7

Under review as a conference paper at ICLR 2018
In Prediction 2, we predict that the selection of the right number of layers is crucial in unsupervised learning. Using fewer layers than needed, will not support the modeling of the target alignment between the domains. In contrast, adding superfluous layers would mean that more and more alternative mappings obscure the target transformation.
In (Kim et al., 2017), 8 or 10 layers are employed (counting both convolution and deconvolution) depending on the experiment. In our experiment, we vary the number of layers and inspect the influence on the results.
These experiments were done on the CelebA gender conversion task, where 8 layers are employed in the experiments of (Kim et al., 2017). Using the public implementation and adding and removing layers, we obtain the results in Fig. 7­ 12. Note that since the encoder and the decoder parts of the learned network are symmetrical, the number of layers is always even. As can be seen, changing the number of layers has a dramatic effect on the results. The best results are obtained at 6 or 8 layers with 6 having the best alignment and 8 having better discrepancy. The results degrade quickly, as one deviates from the optimal value. Using fewer layers, the GAN fails to produce images of the desired class. Adding layers, the semantic alignment is lost, just as expected.
Note that Kim et al. (2017) have preferred low discrepancy over alignment in their choice. In other words, the selected architecture of size k = 8 presents acceptable images at the price of lower alignment compared to an architecture of size k - 2. This is probably a result of ambiguity that is already present at the size k architecture. On the other hand, the smaller architecture of size k - 2 does not produce images of extremely low discrepancy, and there is no architecture that benefits both, an extremely low discrepancy and high alignment. This is observed for example in Fig. 7 where females are translated to males. For 4 layers the discrepancy is too low and the mapping fails to produce images of males. For 6 layers, the discrepancy is relatively low and the alignment is at its highest. For 8 layers, the discrepancy is at its lowest value, nevertheless, the alignment is worse.
While our discrete notion of complexity seems to be highly related to the quality of the results, the norm of the weights do not seem to point to a clear architecture, as shown in Tab. 2(a). Since the table compares the norms of architectures of different sizes, we also approximated the functions using networks of a fixed depth k = 18 and then measured the norm. These results are presented in Tab. 2(b). In both cases, the optimal depth, which is 6 or 8, does not appear to have a be an optimum in any of the measurements.
Prediction 3 states that there are only a handful of DPMs, except for the identity function. In order to verify it, we trained a DiscoGAN from a distribution A to itself with an added loss of the form - xA |x - h(x)|. In our experiment, testing network complexities from 2 to 12, we could not find a DPM, see Fig. 15 and Tab. 3. For lower complexities, the identity was learned despite the added loss. For higher complexities, the network learned the identity while changing the background color. For even higher complexities, other mapping emerged. However, these mappings did not satisfy the circularity constraint, and are unlikely to be DPMs.
5.2 RESULTS FOR ALG. 1
The goal of Alg. 1 is to find a well-aligned solution with higher complexity than the minimal solution and potentially smaller discrepancy. It has two stages. In the first one, k1, which is the minimal complexity that leads to a low discrepancy, is identified. This follows a set of experiments that are similar to the one that is captured, for example, by Fig. 2. To demonstrate robustness, we select a single value of k1 across all experiments. Specifically, we use k1 = 6, which, as discussed above, typically leads to a low (but not very low) discrepancy, while the alignment is still unambiguous.
Once g is trained, we proceed to the next step of optimizing a second network of complexity k2. Note that while the first function (g) uses the complete DiscoGAN architecture, the second network (h) only employs a one-directional mapping, since alignment is obtained by g. Figs. 20­ 28 depict the obtained results, for a varying number of layers. First, the result obtained by the DiscoGAN method with k1 is displayed. The results of applying Alg. 1 are then displayed for a varying k2.
As can be seen, our algorithm leads to more sophisticated mappings. Kim et al. (2017) have noted that their solutions are, at many times, related to texture or style transfer and, for example, geometric transformations are not well captured. The new method is able to better capture such complex transformations. Consider the case of mapping male to female in Fig. 19, first row. A man with a
8

Under review as a conference paper at ICLR 2018
beard is mapped to a female image. While for g the beard is still somewhat present, it is not so for h with k2 > k1. On the female to male mappings in Fig. 20 it is evident in most mappings that g produces a more blurred image, while h is more coherent for k2 > k1. Another example is in the blond to black hair mapping in Fig. 21. In the 5th row, the style transfer nature of g is evident, since it maps a red object behind the head together with the whole blond hair, producing an unrealistic black hair. h of complexity k2 = 8 is able to separate that object from the hair, and in k2 > 8 it produces realistic looking black hair. This kind of transformation requires more than a simple style transfer. On the edges to shoes and edges to handbags mappings of Fig. 25 and Fig. 27, while the general structure is clearly present, it is significantly sharpened by mapping h with k2 > k1.
For the face datasets, we also employ face descriptors in order to learn whether the mapping is semantic. Namely, we can check if the identity is preserved post mapping by comparing the VGG face descriptors of Parkhi et al. (2015). One can assume that two images that match will have many similar features and so the VGG representation will be similar. The cosine similarities are used, as is commonly done.
In addition, we train a linear classifier in the space of the VGG face descriptors in order to distinguish between Male/Female, Eyeglasses/No-eyeglasses, and Blond/Black. This way, we can check, beyond discrepancy, that the mapping indeed transforms between the domains. The training samples in domains A and B are used to train this classifier, which is then applied to a set of test images before and after mapping, measuring the accuracy. The higher the accuracy, the better the separation.
Tab. 4 presents the results for both the k1 layers network g, alternative networks g of higher complexity (shown as baseline only), and the network h trained using Alg. 1. We expect the alignment of g to be best at complexity k1, and worse due to the loss of discrepancy for alternative network g with complexity k > k1. We expect this loss of alignment to be resolved for networks h trained with Alg. 1.
In the experiments of black to blond hair and blond to black hair mappings, we note that h with k2 = 8 has the best descriptor similarity, and very good separation accuracy and discrepancy. Higher values of k2 are best in terms of separation accuracy and discrepancy, but lose somewhat in descriptor similarity. A similar situation occurs for male to female and female to male mappings and in eyeglasses to non-eyeglasses, where k2 = 8 results in the best similarity score and higher values of k2 result in better separation accuracy and discrepancy.
It is interesting to note, that the distance between g and h is also minimal for k2 = 8. Perhaps, with more effective optimization, higher complexities could also maintain similarity, while delivering lower discrepancies.
6 DISCUSSION
Our stratified complexity model is related to structural risk minimization by Vapnik & Chervonenkis (1971a), which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity. In our stratification, which is based on the number of layers, the complexity classes are not necessarily nested.
We point to a key difference between supervised learning and unsupervised learning. While in the former, deeper networks, which can learn even random labels, work well (Zhang et al., 2017), unsupervised learning requires a careful control of the network capacity. This realization, which echoes the application of MDL for model selection in unsupervised learning (Zemel, 1994), was overshadowed by the overgeneralized belief that deeper networks lead to higher accuracy.
The limitations of unsupervised based learning that are due to symmetry, are also a part of our model. For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in (Kim et al., 2017), is similar in nature to the mapping of x to 1 - x in the simple example given in Sec. 3.1. Such symmetries occur when we can divide yAB into two functions yAB = y2  y1 such that a function W is a linear mapping and also a DPM of y1  DA and, therefore, DB  y2  W  y1.
While we focus on unsupervised learning, the emergence of semantics when learning with a restricted capacity is widely applicable, such as with autoencoders, transfer learning, semi-supervised learning and elsewhere. As an extreme example, Sutskever et al. (2015) present empirical evidence that a meaningful mapper can be learned, even from very few examples, if the network trained is kept small.
9

Under review as a conference paper at ICLR 2018
7 CONCLUSION
The recent success in mapping between two domains in an unsupervised way and without any existing knowledge, other than network hyperparameters, is nothing less than extraordinary and has far reaching consequences. As far as we know, nothing in the existing machine learning or cognitive science literature suggests that this would be possible.
We provide an intuitive definition of function complexity and employ it in order to identify minimal complexity mappings, which we conjecture play a pivotal role in this success. If our hypothesis is correct, simply by training networks that are not too complex, the target mapping stands out from all other alternative mappings.
Our analysis leads directly to a new unsupervised cross domain mapping algorithm that is able to avoid the ambiguity of such mapping, yet enjoy the expressiveness of deep neural networks. The experiments demonstrate that the analogies become richer in details and more complex, while maintaining the alignment.
We show that the number of low-discrepancy mappings that are of low-complexity is expected to be small. Our main proof is based on the assumption of identifiability, which constitutes an open question. We hope that there would be a renewed interest in this problem, which has been open for decades for networks with more than a single hidden layer and is unexplored for modern activation functions.
REFERENCES
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, New York, NY, USA, 1st edition, 2009. ISBN 052111862X, 9780521118620.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463­482, March 2003.
Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS. 2016.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
E.D. Sontag F. Albertini and V. Maillot. Uniqueness of weights for neural networks. In R. Mammone, editor, Artificial Neural Networks for Speech and Vision, 1993.
Charles Fefferman and Scott Markel. Recovering a feed-forward net from its output. In NIPS, 1993.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):2096­2030, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680. 2014.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.
Vera Kurková and Paul C. Kainen. Comparing fixed and variable-width gaussian networks. Neural Networks, 2014.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.
10

Under review as a conference paper at ICLR 2018
O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In British Machine Vision Conference, 2015.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), volume 9351 of LNCS, pp. 234­241. Springer, 2015. (available on arXiv:1505.04597 [cs.CV]).
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, New York, NY, USA, 2014. ISBN 1107057132, 9781107057135.
Héctor J. Sussmann. Uniqueness of the weights for minimal feedforward nets with a given inputoutput map. Neural Networks, 1992.
Ilya Sutskever, Rafal Józefowicz, Karol Gregor, Danilo Jimenez Rezende, Timothy P. Lillicrap, and Oriol Vinyals. Towards principled unsupervised learning. arXiv preprint arXiv:1511.06440, 2015.
V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probab. and its Applications, 16(2):264­280, 1971a.
V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264­280, 1971b.
Robert C. Williamson and Uwe Helmke. Existence and uniqueness results for neural network approximations. IEEE Trans. Neural Networks, 6(1):2­13, 1995.
Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual learning for machine translation. arXiv preprint arXiv:1611.00179, 2016.
Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint arXiv:1704.02510, 2017.
Richard S Zemel. A minimum description length framework for unsupervised learning. University of Toronto, 1994.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networkss. arXiv preprint arXiv:1703.10593, 2017.
11

Under review as a conference paper at ICLR 2018

Table 1: Comparing the VGG descriptor similarity, separation accuracy and discrepancy for varying
complexity k k = 4 k = 6 k = 8 k = 10 k = 12 k = 14

Male to Female

Discrepancy 0.527 0.203 0.091 Similarity 0.301 0.269 0.103 Separation 0.938 0.932 0.940

0.094 0.106 0.940

0.083 0.089 0.940

0.086 0.100 0.938

Female to Male

Discrepancy 0.882 0.122 0.150 Similarity 0.303 0.260 0.110 Separation 0.798 0.865 0.860

0.075 0.105 0.87

0.076 0.093 0.857

0.091 0.100 0.866

Blond to Black Hair

Discrepancy 0.467 0.214 0.092 Similarity 0.365 0.287 0.240 Separation 0.903 0.925 0.922

0.097 0.106 0.917

0.094 0.091 0.922

0.081 0.0870 0.923

Black to Blond Hair

Discrepancy 0.663 0.264 0.073 Similarity 0.337 0.270 0.240 Separation 0.941 0.941 0.911

0.094 0.106 0.916

0.084 0.087 0.915

0.076 0.085 0.917

Eyeglasses to Non-Eyeglasses Discrepancy 0.323 0.159 0.071 Similarity 0.470 0.391 0.347 Separation 0.786 0.785 0.828

0.082 0.114 0.843

0.083 0.125 0.849

0.081 0.146 0.828

Non Eyeglasses to Eyeglasses Discrepancy 0.577 0.518 0.236 Similarity 0.452 0.373 0.364 Septation 0.748 0.749 0.766

0.263 0.105 0.848

0.093 0.108 0.832

0.085 0.127 0.840

Table 2: (a) Norms of the various mappings h for mapping Males to Females using the DiscoGAN architecture. (b) Norms of 18-layer networks that approximates the mappings obtained with a varying
number of layers.

A to B B to A

Norm
L1 norm Average L1 norm per layer L2 norm Average L2 norm per layer
L1 norm Average L1 norm per layer L2 norm Average L2 norm per layer

------­ Number of layers -------- 4 6 8 10 12

6382 1064 18.25 7.084

23530 2353 29.24 8.353

36920 2637 28.44 7.154

44670 2482 31.72 6.708

71930 3270 36.57 7.009

6311 1052 18.36 7.161

21240 2124 26.79 7.757

31090 2221 25.85 6.552

37380 2077 28.36 6.058

64500 2932 34.99 6.771

(a)

A to B B to A

Norm
L1 norm Average L1 norm per layer L2 norm Average L2 norm per layer
L1 norm Average L1 norm per layer L2 norm Average L2 norm per layer

------­ Number of layers -------- 4 6 8 10 12

317200 9329 528.1 3.031

228700 6726 401.7 2.284

356500 10485 559.6 3.242

247200 7271 410.1 2.257

164200 4829 346.8 1.890

316900 9323 523.2 3.003

194500 5719 375.9 2.029

353900 10410 555.7 3.210

171500 5045 346.5 1.921

228900 6733 373.3 2.289

(b)

12

Under review as a conference paper at ICLR 2018

Table 3: Seeking DPMs: the distance from the identity and the discrepancy (GAN loss) for various numbers of layers, where training a DiscoGAN from a dataset to itself.

Dataset Males Females Handbags Shoes Edges of handbags Edges of shoes

loss xA |x - h(x)| Discrepancy xA |x - h(x)| Discrepancy xA |x - h(x)| Discrepancy xA |x - h(x)| Discrepancy xA |x - h(x)| Discrepancy xA |x - h(x)| Discrepancy

------- Number of layers: ------4 6 8 10 12 14
0.09 0.42 0.45 0.45 0.45 0.45 0.37 0.60 0.27 0.20 0.17 0.10
0.06 0.36 0.43 0.42 0.44 0.45 0.32 0.40 0.15 0.11 0.11 0.11
0.10 0.28 0.37 0.37 0.38 0.37 0.13 0.28 0.24 0.14 0.15 0.20
0.06 0.15 0.29 0.30 0.30 0.30 0.15 0.28 0.20 0.15 0.10 0.10
0.28 0.55 0.51 0.52 0.50 0.49 0.18 0.28 0.58 0.47 0.40 0.35
0.23 0.50 0.59 0.55 0.49 0.43 0.17 0.21 0.65 0.46 0.45 0.45

13

Under review as a conference paper at ICLR 2018

(a) Input

(b) Output

(Male to female)

(Female to male)

Figure 2: Results for celebA Male to Female transfer (a) Input (b) The mapping obtained by the GAN loss without additional losses.

(a) Input
(b) Output

(Blond to black hair)

(Black to blond hair)

Figure 3: Same as Fig. 2 for black to blond hair conversion.

(a) Input
(b) Output

(With to without eyeglasses)

(Without to with eyeglasses)

Figure 4: Same as Fig. 2 for eyeglasses to no eyeglasses conversion.

(a) Input

(b) Output

(Handbags to shoes)

(Shoes to handbags)

Figure 5: Same as Fig. 2 for handbag to shoes and shoes to handbag mapping.

(a) Input

(b) Output

(Edges to shoes)

(Shoes to edges)

Figure 6: Same as Fig. 2 for edges to shoes and shoes to edges conversion.

14

Under review as a conference paper at ICLR 2018

Table 4: Results for Alg. 1 for different datasets. VGG Similarity is given in the first column. The
second column gives the separation value using the linear classifier. In the third column, we measure the discrepancy of the mapping. The last column provides the distance of h to g, where applicable.

Dataset

f Complexity Descriptor Separation Discrepancy

Distance

Similarity Accuracy disc(f  DA, DB) RDA [h, g]

Male to Female

g g g g h h h

k1 = 6 k=8
k = 10
k = 12
k2 = 8 k2 = 10 k2 = 12

0.267 0.280 0.106 0.089 0.316 0.204 0.197

0.928 0.938 0.940 0.940 0.933 0.937 0.941

0.230 0.077 0.094 0.083 0.087 0.109 0.127

0.054 0.075 0.077

Female to Male

g g g g h h h

k1 = 6 k=8
k = 10
k = 12
k2 = 8 k2 = 10 k2 = 12

0.268 0.260 0.105 0.093 0.304 0.215 0.214

0.848 0.848 0.870 0.857 0.878 0.884 0.883

0.310 0.107 0.075 0.076 0.107 0.082 0.073

0.056 0.083 0.082

Blond to Black Hair

g k1 = 6 g k=8
g k = 10
g k = 12
h k2 = 8 h k2 = 10 h k2 = 12

0.287 0.24 0.106 0.091 0.293 0.197 0.199

0.925 0.922 0.917 0.922 0.926 0.926 0.928

0.214 0.092 0.097 0.094 0.136 0.225 0.092

0.152 0.161 0.161

Black to Blond Hair

g k1 = 6 g k=8
g k = 10
g k = 12
h k2 = 8 h k2 = 10 h k2 = 12

0.270 0.24 0.106 0.087 0.287 0.179 0.180

0.941 0.911 0.916 0.915 0.938 0.946 0.952

0.264 0.073 0.094 0.084 0.077 0.165 0.168

0.146 0.149 0.152

Eyeglasses to Non-Eyeglasses

g g g g h h h

k1 = 6 k=8
k = 10
k = 12
k2 = 8 k2 = 10 k2 = 12

0.391 0.347 0.114 0.125 0.391 0.283 0.274

0.785 0.828 0.843 0.849 0.786 0.847 0.860

0.159 0.071 0.082 0.083 0.097 0.180 0.148

0.058 0.083 0.081

Non-Eyeglasses to Eyeglasses

g g g g h h h

k1 = 6 k=8
k = 10
k = 12
k2 = 8 k2 = 10 k2 = 12

0.373 0.364 0.105 0.108 0.389 0.272 0.282

0.749 0.766 0.848 0.832 0.780 0.807 0.803

0.518 0.236 0.263 0.093 0.300 0.370 0.409

0.063 0.083 0.081

15

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: ---------------- 4 6 8 10 12

Figure 7: Results for celebA Male to Female transfer for networks with different number of layers. 16

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: ---------------- 4 6 8 10 12

Figure 8: Results for celebA Female to Male transfer for networks with different number of layers. The case of 4 layers failed to produce acceptable results.
17

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: -------------- 4 6 8 10 12

1
Figure 9: Results for celebA Blond to Black Hair transfer for networks with different number of layers.
18

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: --------------4 6 8 10 12

1
Figure 10: Results for celebA Black Hair to Blond transfer for networks with different number of layers.
19

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: --------------4 6 8 10 12

1
Figure 11: Results for celebA Eyeglasses to Non-Eyeglasses transfer for networks with different number of layers.
20

Under review as a conference paper at ICLR 2018

Input

---------------- Number of layers: --------------4 6 8 10 12

1
Figure 12: Results for celebA Non-Eyeglasses to Eyeglasses transfer for networks with different number of layers.
21

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 13: Results for mapping Males to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
22

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 14: Results for mapping the Females to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
23

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 15: Results for mapping shoe edges to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
24

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 16: Results for mapping handbag edges to itself (B=A), using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
25

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 17: Results for mapping handbags to itself (B=A), using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
26

Under review as a conference paper at ICLR 2018

Input op h

---------------- Number of layers: ---------------- 4 6 8 10 12 14

h h

h

h h

h

h h

h

h h

h

h h

h

h h

Figure 18: Results for mapping shoes to itself (B=A) using a DiscoGAN architecture and enforcing that the mapping is not the identity mapping. The odd rows present the learned mapping h, and the even rows present the full cycle h  h.
27

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------­ h of complexity: --------­

k2 = 8

k2 = 10

k2 = 12

Figure 19: Results for Alg. 1 on Male2Female dataset for mapping male to female. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
28

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------­ h of complexity: --------­

k2 = 8

k2 = 10

k2 = 12

Figure 20: Results for Alg. 1 on Male2Female dataset for mapping female to male. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
29

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------- h of complexity: ---------

k2 = 8

k2 = 10

k2 = 12

Figure 21: Results for Alg. 1 on celebA dataset for mapping blond to black. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
30

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------­ h of complexity: --------­

k2 = 8

k2 = 10

k2 = 12

Figure 22: Results for Alg. 1 on celebA dataset for mapping black to blond. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
31

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------­ h of complexity: --------­

k2 = 8

k2 = 10

k2 = 12

Figure 23: Results for Alg. 1 on Eyeglasses dataset for mapping eyeglasses to no eyeglasses. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
32

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

--------­ h of complexity: --------­

k2 = 8

k2 = 10

k2 = 12

Figure 24: Results for Alg. 1 on Eyeglasses dataset for mapping no eyeglasses to eyeglasses. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
33

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

----------- h of complexity: -----------

k2 = 8

k2 = 10

k2 = 12

k2 = 14

Figure 25: Results for Alg. 1 on Edges2Handbags dataset for mapping edges to handbags. Shown is a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
34

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

----------------­ h of complexity: ----------------­

k2 = 8

k2 = 10

k2 = 12

k2 = 14

Figure 26: Results for Alg. 1 on Edges2Handbags dataset for mapping handbags to edges. Shown are a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
35

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

----------- h of complexity: -----------

k2 = 8

k2 = 10

k2 = 12

k2 = 14

Figure 27: Results for Alg. 1 on Edges2Shoes dataset for mapping edges to shoes. Shown are a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
36

Under review as a conference paper at ICLR 2018

Input

g of k1 = 6

----------------­ h of complexity: ----------------­

k2 = 8

k2 = 10

k2 = 12

k2 = 14

Figure 28: Results for Alg. 1 on Edges2Shoes dataset for mapping shoes to edges. Shown are a minimal complexity mapping g that has low discrepancy, and various mappings h obtained by the method.
37

Under review as a conference paper at ICLR 2018

A A GENERALIZED AND FORMAL STATEMENT OF THE RESULTS

For brevity, we have not presented our results in the most general way. For example, in Def. 1, we did not bound the complexity of the discriminators. For the same reason, some of our terms were described and not yet formally defined.

A.1 A COMPLEXITY MEASURE FOR FUNCTIONS

In order to model the composition of neural networks, we define a complexity measurement that assigns a value based on the number of simple functions that make up a complex function.
Definition 3 (Stratified complexity model (SCM)). A stratified complexity model N := SCM[C] is a hypothesis class of functions p : RM  RM specified by a set of functions C. Every function p in N has an appropriate decomposition:

· N=

 n=0

Cn

(where,

Cn

=

{pn



...



p1|p1,

..., pn



C}

and

C0

=

{Id}).

· Every function in C is invertible.

Informally, a SCM partitions a set of invertible functions into disjoint complexity classes,

C0 := p  N  n  N, q  Cn : p  q, q  p, p-1  q, q  p-1  Cn

n-1

Cn := Cn \

Ci  C0

i=0

(9)

When considering simple functions pi that are layers in a neural network, each complexity class

contains the functions that are implemented by networks of n hidden layers. In addition, we denote

the complexity of a function p:

C(p) := arg {p  Cn}

(10)

nN{0}

If the complexity of a function p equals n, then any appropriate decomposition p = pn  ...  p1 will be called a minimal decomposition of p. According to this measurement, the complexity of a function
p is determined by the minimal number of primitive functions required in order to represent it.

In this work, we focus our attention on SCMs that represent the architectures of fully connected neural networks with layers of a fixed size, i.e.,

Definition 4 (NN-SCM). A NN-SCM is a SCM N = SCM[C] that satisfies the following conditions:

· C = W2    W1 W1, W2  RM×M and W1, W2 are invertible . Here, W1, W2 denote both linear transformations and the associated matrix forms.
·  is a non-linear element-wise activation function.

For brevity, we denote N := SCM[] to refer to a NN-SCM with the activation function .

The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh
are also used). In the NN-SCM framework, to specify the function obtained by a decomposition Wn    Wn-1    ...    W1 we simply write:

F [Wn, ..., W1] := Wn    Wn-1    ...    W1

(11)

It is useful to characterize the effect of inversion on the complexity of functions, since, for example, we consider both h =   h and h = -1  h . The following lemma states that, in the case of NN-SCM with  that is the Leaky ReLU, the complexity of the inverse function is the same as that of the original function.
Lemma 1. Let N = SCM[] be a NN-SCM with  that is the Leaky ReLU with parameter 0 < a = 1. Then, for any u  N , C(u-1) = C(u).

38

Under review as a conference paper at ICLR 2018

Proof. First, we denote C (p) the minimal number n such that there are invertible linear mappings
W1, ..., Wn+1 such that p = F [Wn+1, ..., W1] (if p = Id then C (p) = 0). This complexity measure is similar to the complexity measure C. For a function p such that C(p) = 0, we have,
C(p) = C (p). Nevertheless, for p such that C(p) = 0, it is not necessarily true that C (p) = 0.
For example, if p = Id is an invertible linear mapping, we have, C(p) = 0 and C (p) = 2. Let p = F [W2, W1] = W2    W1 be any function such that C(p) = 1. We consider that:

-1 = -Id    -Id/a = F [-Id, -Id/a]

(12)

Therefore,

F [W2, W1]-1 = -W1-1    -W2-1/a = F [-W1-1, -W2-1/a]

(13)

In particular, C linear mapping

(p-1)  1. If C (p-1) = 0, then, - in contradiction. Thus, C (p-1)

Id =

= 1.

-W1-1







-W2-1/a

and,

therefore,



is

a

Next, we would like to show that for any u  N , C (u-1) = C (u). Let u such that C (u) = 0. Then, u = u-1 = Id and therefore, C (u-1) = 0. Let u = F [Wn+1, ..., W1] be a function such that C (u) = n > 0. Then,

u = F [Wn+1, Wn]  F [Id, Wn-1]  ...  F [Id, W1]

(14)

In particular,

u-1 = F [Id, W1]-1  ...  F [Id, Wn-1]-1  F [Wn+1, Wn]-1

(15)

or, Therefore, by Lem. 7,

u-1 = F [-W1-1, W2-1/a, ..., Wn-1/a, -Wn-+11/a]

(16)

C (u-1)  C (F [Id, W1]-1) + ... + C (F [Id, Wn-1]-1) + C (F [Wn+1, Wn]-1) = n (17)

On the other hand, if v = u-1, n = C (u) = C (v-1)  C (v) = C (u-1)  n and C (u-1) = C (u). Finally, we would like to show that for every u  N , we have: C(u-1) = C(u). If C(u) = 0, then, by Lem. 11, C(u-1) = 0. On the other hand, if C(u) = 0, then, by Lem. 11, C(u-1) = 0 and by the above: C(u) = C (u) = C (u-1) = C(u).

A.2 MINIMAL COMPLEXITY MAPPINGS

Based on our simplicity hypothesis, we present a definition of a minimal complexity mapping that is both intuitive and well-defined in concrete complexity terms. Given two distributions DA and DB, a minimal complexity mapping f : XA  XB between domains A and B is a mapping that has minimal complexity among the functions h : XA  XB that satisfy h  DA  DB.
Consider, again, the example of a line segment in RM (Sec. 3.1) and the semantic space of the interval, [0, 1]  R. The two linear mappings, which map either segment ends to 0 and the other to 1 are minimal, when using f that are ReLU based neural networks. Other mappings to this segment are possible, simply by permuting points on the segment in RM . However, these alternative mappings have higher complexity, since the two mappings above are the only ones with the minimal possible complexity.
In order to measure the distance between h  DA and DB, we use the discrepancy distance, discD. In this work, we focus on classes of discriminators D of the form Dm := {u|C(u)  m} for some m  N. In addition, for simplicity, we will write discm := discDm .

Definition 5 (Minimal complexity mappings). Let N = SCM[C]. Let A = (XA, DA) and B = (XB, DB) be two domains. We define the (m, 0)-minimal complexity between A and B as:

CAm,,B0

:=

min {h
iN{0}

s.t

C (h)

=

i

and

discm(h



DA, DB)



0}

(18)

The set of (m, 0)-minimal complexity mappings between A and B is:

H 0 (A, B; m) := h C(h)  CAm,,B0 and discm(h  DA, DB)  0

(19)

39

Under review as a conference paper at ICLR 2018

We note that for any fixed 0 > 0, the sequence {CAm,,B0 }m=0 is monotonically increasing as m tends to infinity. In addition, we assume that for every two distributions of interest, DI and DJ , and an error rate 0 > 0, there is a function h of finite complexity such that disc(h  DI , DJ )  0. Therefore, the sequence {CAm,,B0 }m=0 is upper bounded by C(h) for all m  N  {0}. In particular, there is a minimal value m0 > 0 such that CAm,,B0 = CAm,0B, 0 for all m  m0. We denote: EA0,B := m0 and CA0,B := CAm,0B, 0 . For simplicity, sometimes we will assume that m = . In this case, we will write H 0 (A, B) := H 0 (A, B; ).
A.3 IDENTIFIABILITY
Every neural network implementation gives rise to many alternative implementations by performing simple operations, such as permuting the units of any hidden layer, and then permuting back as part of the linear mapping in the next layer. Therefore, it is first required to identify and address the set of transformations that could be inconsequential to the function which the network computes.
Definition 6 (Invariant set). Let N = SCM[] be a NN-SCM. The invariant set Invariant(N ) is the set of all  : RM  RM that satisfy the following conditions:

·  : RM  RM is an invertible linear transformation. ·    =   .

Functions in Invariant(N ) are called invariants or invariant functions.

For example, for neural networks with the tanh activation function, the set of invariant functions contains the linear transformations that take vectors, permute them and multiply each coordinate by ±1. Formally, each  = [ 1 · e(1), ..., M · e(M)] where ei is the i'th standard basis vector,  is a permutation over [M ] and i  {±1} (Fefferman & Markel, 1993).
In the following lemma, we characterize the set of all invariant functions for  that is Leaky ReLU with parameter 0 < a = 1.
Lemma 2. Let N = SCM[] with  be Leaky ReLU with parameter 0 < a = 1. Then,

Invariant(N ) =





M ×M
R

 = [c1 · e(1), ..., cM · e(M)], where i  [M ] : ci > 0 and   SymM

(20)

Here, ei denotes the i'th standard basis vector in RM and SymM is the set of permutations of [M ].

Proof. Let  be an invertible linear mapping satisfying    =   . We consider that for all

i  [M ] and vector x; ( i, x ) = i, (x) , where i is the i'th row of  and i,j is the (i, j) entry

of  . For x = ej, we have:

i,j = (i,j )

(21)

For x = -ej, we have:

-ai,j = (-i,j )

(22)

If i,j < 0, then the first equation leads to contradiction. Otherwise, the equations are both satisfied.

Finally, for x = ej - ek, we have:

i,j - ai,k = (i,j - i,k)

(23)

If i,j -i,k = 0, then, i,j -ai,k = 0 and since a = 1, 0, we have, i,j = i,k = 0. If i,j -i,k  0, then, i,j - i,k = i,j - ai,k that gives i,k = 0. If i,j - i,k  0, then, a(i,j - i,k) = i,j - ai,k that yields i,j = 0. Therefore, for each i  [M ] there is at most one entry i,j that is not 0. If for all j  [M ], i,j = 0, then the mapping  is not invertible, in contradiction. Therefore, for each i  [M ] there is exactly one entry i,j > 0 (it is non-negative as shown above). Finally, if there are i1 = i2 such that i1,j, i2,j = 0 then the matrix is invertible. Therefore,  is a member of the set defined in Eq. 20. In addition, it is easy to see that every member of the noted set satisfies the conditions of the
invariant set. Thus, we obtain the desired equation.

40

Under review as a conference paper at ICLR 2018

Our analysis is made much simpler, if every function has one invariant representation up to a sequence of manipulations using invariant functions that do not change the essence of the processing at each layer.

Assumption 1 (Identifiability). Let N = SCM[] with  that is Leaky ReLU with parameter
0 < a = 1. Then, every function p  N is identifiable (with respect to Invariant(N )), i.e., for
any two minimal decompositions, p = F [Wn+1, ..., W1] = F [Vn+1, ..., V1], there are invariants 1, ..., n  Invariant(N ) such that:

V1 = 1  W1, i = 2, ..., n : Vi = i  Wi  i--11 and Vn+1 = Wn+1  n-1

(24)

Uniqueness up to invariants, also known as identifiability, forms an open question. Fefferman & Markel (1993) proved identifiability for the tanh activation function. Other works (Williamson & Helmke, 1995; F. Albertini & Maillot, 1993; Kurková & Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various classical activation functions. In the following lemma, we show that identifiability holds for Leaky ReLU networks with only one hidden layer.
Lemma 3. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Any function p such that C(p) = 1 is identifiable, i.e, if p = F [W2, W1] = F [V2, V1], then, W1 =   V1 and W2 = V2   -1 for some   Invariant(N ).

Proof. An alternative representation of the equation is: (  W1  V1-1) = (W2-1  V2  )
We would like to prove that if   U = V   then V = U . We have:

(25)

  U (x) = V  (x)

(26)

In particular, if vi is the i'th row of V (similarly ui) and x = ej:

(ui,j ) = ( ui, ej ) = vi, (ej ) = vi,j

(27)

where vi,j is the (i, j) entry of V (similarly ui,j). Similarly, for x = -ej:

(-ui,j) = ( ui, -ej ) = vi, (-ej ) = -avi,j

(28)

If ui,j is negative, we obtain: aui,j = vi,j (the first equation) and -ui,j = -avi,j (the second equation) that yields a = 1 in contradiction. Therefore, ui,j  0 and ui,j = vi,j (the second
equation).

We conclude that W1  V1-1 = W2-1  V2 :=  . Finally, since (  W1  V1-1) = (W2-1  V2  ) we have    =    and  is invertible linear mapping. Differently said, W1 =   V1 and W2 = V2   -1 such that   Invariant(N ).

As far as we know, there are no other results continuing the identifiability line of work for activation

functions such as Leaky ReLU. Uniqueness, which is stronger than identifiability, since it means

that even multiple representations with different number of layers do not exist, does not hold for these activation functions. To see this, note that for every M × M invertible linear mapping W , the

following holds:

U    W = U    W    -Id    -Id/a

(29)

where  is the Leaky ReLU activation function with parameter a. We conjecture that for networks with Leaky ReLU activations identifiability holds, or at least for networks with a fixed number of neurons per layer. In addition to identifiability, we make the following assumption, which states that almost all mappings are non-degenerate.

Assumption 2. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Assume that the set of (W1, ..., Wn+1)  RM×M×m such that C(F [Wn+1, ..., W1]) = n is dense in RM×M×m.

41

Under review as a conference paper at ICLR 2018

A.4 COUNTING MINIMAL COMPLEXITY MAPPINGS

In the unsupervised alignment problem, the algorithms are provided with only two unmatched datasets of samples from the domains A and B and the task is to learn a well-aligned function between them. Since we hypothesize that the alignment of the target mapping is typically captured by the lowest complexity low-discrepancy mapping, we develop the machinery needed in order to show that such mappings are rare.
Recall that discm is the discrepancy distance for discriminators of complexity up to m. In Sec. 2, we have discussed the functions  which replaces between members in the domain B that have similar probabilities. Formally, these are defined using the discrepancy distance.

Definition 7 (Density preserving mapping). Let N = SCM[C] and X = (X , DX ) a domain. A (m, 0)-density preserving mapping over X (or an (m, 0)-DPM for short) is a function  such that

discm(  DX , DX )  0

(30)

We denote the set of all (m, 0)-DPMs of complexity k by DPM 0 (X; m, k) :=  discm(  DX , DX )  0 and C() = k .

We would like to bound the number of mappings that are both low-discrepancy and low-complexity by the number of DPMs. We consider that there are infinitely many DPMs. For example, if we slightly perturb the weights of a minimal representation of a DPM, , we obtain a new DPM. Therefore, we define a similarity relation between functions that reflects whether the two are similar. In this way, we are able to bound the number of different (non-similar) minimal-complexity mappings by the number of different DPMs.
Definition 8 (Closeness between pairs of distributions or functions). Let N = SCM[].

·

We denote D1


m, 0

D2



discm(D1, D2) 

0.

· We denote f D g, if C(f ) = C(g) =: n and there are minimal decompositions: f =
m, 0
F [Wn+1, ..., W1] and g = F [Vn+1, ..., V1] such that i  [n + 1] : F [Wi, ..., W1]  D 
m, 0
F [Vi, ..., V1]  D.

The defined relation is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions are similar. We count the number of functions up to the similarity as the minimal number of subsets required in order to cover the entire space. This idea is presented in Def. 9, which slightly generalizes the notion of covering numbers (Anthony & Bartlett, 2009).
Definition 9 (Covering number). Let (U, U ) be a set and a reflexive and symmetric relation. A covering of (U , U ), is a tuple (U , U ) such that: U is an equivalence relation and u1 U u2 = u1 U u2. The covering number of (U , U ), denoted by N(U , U ), is:
min U / U s.t: the minimum is taken over (U , U ) that is a covering of (U , U ) (31)
Here, U / U is the quotient set of U by U .

Thm. 1 below states that the number of low discrepancy mappings of complexity CA0,B is upper bounded by the number of DPMs of size 2CA0,B. By prediction 3, the number of such DPMs is small. The theorem employs the following weak assumption. In Lem. 19, we prove that this assumption holds for the case of a continuous risk if the discriminators have bounded weights.
Assumption 3. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. For every m > 0 (possibly ) and n > 0, the function discm(F [Wn, ..., W1]  D1, D2) is continuous as a function of the weights of W1, ..., Wn  RM×M .

42

Under review as a conference paper at ICLR 2018

Theorem 1. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Assume Assumptions 1, 2 and 3. Let 0, 1 and 2 be three constants such that 0 < 1/4 and 2 < 1 - 4 0 be three positive constants and A = (XA, DA) and B = (XB, DB) are two domains. Then,



N

H 0 (A, B), DA
1

N   lim min 0 N 

DPM2 0+ DPM2 0+

A; 2CA0,B , DA 2
B; 2CA0,B , DB

2

(32)

Proof. See Sec. D.

43

Under review as a conference paper at ICLR 2018

B SUMMARY OF NOTATION
Tab. 5 lists the symbols used in our work.

Table 5: Summary of Notation

Symbol
X XA, XB DA, DB A, B yA, yB
DZ yAB , yBA

RD[f1, f2] discD(D1, D2)

 C

F [Wn, ..., W1] N = SCM[C]
N = SCM[]
C (p)
Invariant(N )

Dm discm, disc CAm,,B0 CA0,B , EA0,B H 0 (A, B; m)

H 0 (A, B) S1  S2

D1


m,

D2, D1  D2

f D g, f D g
m,

N(U , U )

X : x

Explanation
A feature space The sample spaces of A and B (resp.) Distributions over XA and XB (resp.) Two domains; Specified by (XA, DA) and (XB, DB) (resp.) Functions from the feature space to the domains, yA : X  XA and yB : X  XB A distribution over a feature space X yAB = yB  yA-1 and yBA = yA  yB-1 Loss function : R × R  R The risk function RD[f1, f2] = ExD (f1(x), f2(x)) where is a loss function and D is a distribution The discrepancy between two distributions D1 and D2, i.e, discD(D1, D2) = supc1,c2D |RD1 [c1, c2] - RD2 [c1, c2]| A non-linear element-wise activation function A class of functions; in most cases C = {W2    W1 | W1, W2  RM×M are invertible linear transformation} F [Wn, ..., W1] = Wn    Wn    ...    W2    W1 A SCM specified by a class of functions C (see Def. 3) A NN-SCM specified by the activation function  (see Def. 4) The complexity of a function p (see Eqs. 9, 10) The invariant set of N (see Def. 6)
An invariant function (see Def. 6) Dm = {u|C(u)  m} discm := discDm and disc := discD The (m, 0)-minimal complexity between A and B (see Def. 5) CA0,B = maxm1 CAm,,B0 and EA0,B = arg minm[CAm,,B0 = CA0,B ] The set of ( 0, m)-minimal complexity mappings between A and B (see Def. 5) H 0 (A, B) = H 0 (A, B; ) A composition of sets, S1  S2 = {s1  s2|s1  S1 and s2  S2} discm(D1, D2)  and disc(D1, D2)  (see Def. 8)
discm(f  D, g  D)  and disc(f  D, g  D)  (see Def. 8)
The covering number of U with respect to relation U on U (see Def.9) x is assigned to X

44

Under review as a conference paper at ICLR 2018

C LEMMAS

In this section, we prove various lemmas that are used in the proof of Thm. 1. In Sec. C.1 we present the assumptions taken in various lemmas in the appendix. In Sec. C.2 we prove useful inequalities involving the discrepancy distance. Sec. C.3 provides lemmas concerning the defined complexity measure and invariant functions. The lemmas in Sec. C.4 concern the properties of inverse functions.

C.1 ASSUMPTIONS

We list the assumptions employed in our proofs. Assumptions 1 and 2 were already presented and are heavily used. Assumptions 3 and its relaxation 4 are mild assumptions that were taken for convenience.

Assumption 1 (Identifiability). Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Then, every function p  N is identifiable (with respect to Invariant(N )), i.e., for any two minimal decompositions, p = F [Wn+1, ..., W1] = F [Vn+1, ..., V1], there are invariants 1, ..., n  Invariant(N ) such that:

V1 = 1  W1, i = 2, ..., n : Vi = i  Wi  i--11 and Vn+1 = Wn+1  n-1

(24)

Assumption 2. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Assume that the set of (W1, ..., Wn+1)  RM×M×m such that C(F [Wn+1, ..., W1]) = n is dense in RM×M×m.

Assumption 3. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. For every m > 0 (possibly ) and n > 0, the function discm(F [Wn, ..., W1]  D1, D2) is continuous as a function of the weights of W1, ..., Wn  RM×M .

In the case that the norm of the discriminator is bounded, Lem 19, it follows from the following assumption, which is well-justified, (cf. Shalev-Shwartz & Ben-David (2014), page 162, Eq.14.13).
Assumption 4. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. For all m > 0, the function RD [F [Vm, ..., V1], F [Wm, ..., W1]] is continuous as a function of Vm, ..., V1, Wm, ..., W1.

C.2 PROPERTIES OF DISCREPANCIES

Lemma 4. Let D1 and D2 be two classes of functions and D1, D2 two distributions. Assume that

D1  {p}  D2, then,

discD1 (p  D1, p  D2)  discD2 (D1, D2)

(33)

In particular, if m  k + C(p), then,

disck(p  D1, p  D2)  discm(D1, D2)

(34)

Proof. By the definition of discrepancy:

discD1 (p  D1, p  D2) = sup RpD1 [c1, c2] - RpD2 [c1, c2]
c1 ,c2 D1
= sup RD1 [c1  p, c2  p] - RD2 [c1  p, c2  p]
c1 ,c2 D1

(35)

Since D1  {p}  D2 we have:

discD1 (p  D1, p  D2) = sup RD1 [c1  p, c2  p] - RD2 [c1  p, c2  p]
c1 ,c2 D1
 sup RD1 [u1, u2] - RD2 [u1, u2] = discD2 (D1, D2)
u1 ,u2 D2
The second inequality is a special case for D1 = Dk and D2 = Dm. Lemma 5. Let A = (X1, D1) and B = (X2, D2) be two domains and DZ a distribution.

(36)

45

Under review as a conference paper at ICLR 2018

1. Assume that m  k + C(p). Then, disck(p  D1, D3)  discm(D1, D2) + disck(p  D2, D3)
2. Let y1, y2 and y = y2  y1-1 be three functions and m  k + C(y2). Then, disck(y  D1, D2)  discm(DZ , y1-1  D1) + disck(y2  DZ , D2)
3. Let h be any function and m  k + C(h-1). Then, disck(D1, h-1  D2)  discm(h  D1, D2)
Proof. 1. Follows from Lem. 4, since m  k + C(p), we have: disck(p  D1, p  D2)  discm(D1, D2)
Therefore, by the triangle inequality, disck(p  D1, D3)  disck(p  D1, p  D2) + disck(p  D2, D3)  discm(D1, D2) + disck(p  D2, D3)
2. We use Lem. 4 with p : y2, D1 : Dk, and D2 : Dm and Dk  {y2}  D2:

(37) (38) (39) (40) (41)

disck(y2  DZ , y  D1) = disck(y2  DZ , y2  y1-1  D1)  discm(DZ , y1-1  D1) Therefore, by the triangle inequality,
disck(y  D1, D2)  disck(y2  DZ , D2) + disck(y2  DZ , y  D1)  disck(y2  DZ , D2) + discm(DZ , y1-1  D1)
3. Follows immediately from Lem. 4 for p : h-1 and Dk  {h-1}  Dm.

(42) (43)

C.3 PROPERTIES OF THE COMPLEXITY MEASURE AND INVARIANTS
Lemma 6. Let N = SCM[C]. In addition, let u, v be any two functions. Then, max{C(u) - C(v-1), C(v) - C(u-1)}  C(u  v)  C(u) + C(v)

(44)

Proof. We begin with the case C(v) = 0. In this case, C(u  v) = C(u) = C(u) + C(v). By definition, C(v) = 0 implies that C(v-1) = 0 and C(u  v) = C(u) - C(v-1). Finally, C(u) - C(v-1) = C(u) = C(u  v). The case C(u) = 0 is analogous. Next, we assume that
C(u) = n > 0 and C(v) = m > 0. Let u = un  ...  u1 and v = vm  ...  v1 be minimal decompositions of u and v (resp.). Therefore, we can represent, u  v = un  ...  u1  vm  ...  v1. In particular, C(u  v)  n + m = C(u) + C(v).

The lower bound follows immediately from the upper bound: C(u) = C(u  v  v-1)  C(u  v) + C(v-1) = C(u) - C(v-1)  C(u  v)
By similar considerations, we also have: C(v) - C(u-1)  C(u  v).

(45)

For a given function u  N = SCM[C], we define, C (u) = argn{u  Cn}
Lemma 7. Let N = SCM[C]. In addition, let u, v be any two functions. Then, max{C (u) - C (v-1), C (v) - C (u-1)}  C (u  v)  C (u) + C (v)

(46) (47)

Proof. We begin by proving the upper bound. We assume C (u) = n and C (v) = m. Let
u = un  ...  u1 and v = vm  ...  v1 be minimal decompositions of u and v (resp.). Therefore, we can represent, u  v = un  ...  u1  vm  ...  v1. In particular, C (u  v)  n + m = C (u) + C (v).
The lower bound follows immediately from the upper bound:

C (u) = C (u  v  v-1)  C (u  v) + C (v-1) = C (u) - C (v-1)  C (u  v) (48) By similar considerations, C (v) - C (u-1)  C (u  v).

46

Under review as a conference paper at ICLR 2018

Lemma 8. Invariant(N ) is closed under inverse and composition, i.e,   Invariant(N )   -1  Invariant(N )
And, 1, 2  Invariant(N ) = 1 · 2  Invariant(N )

(49) (50)

Proof. Inverse: Let   Invariant(N ). Then, by definition,  is an invertible linear mapping and    =    . In particular,  -1 is also an invertible linear mapping and  -1   =    -1. Thus,  -1  Invariant(N ).
Composition: Let 1, 2  Invariant(N ). Then, i is an invertible linear mapping and i   =   i for i = 1, 2. In particular, 1  2 is also an invertible linear mapping and 1  2   = 1    2 =   1  2. Thus, 1  2  Invariant(N ).
Lemma 9. Let N = SCM[] with  that is Leaky ReLU with 0 < a = 1. Assume that p obeys identifiability, i.e., that Assumption 1 holds. Then, for any two minimal decompositions p = F [Wn+1, ..., W1] = F [Vn+1, ..., V1], we have:

i  [n + 1] :F [Wi, ..., W1]  F [Vi, ..., V1]-1  Invariant(N ) and F [Wn+1, ..., Wi]  F [Vn+1, ..., Vi]-1  Invariant(N )

(51)

Proof. We prove that F [Wi, ..., W1]  F [Vi, ..., V1]-1  Invariant(N ). If i = n + 1, then, F [Wi, ..., W1]  F [Vi, ..., V1]-1 = Id  Invariant(N ). Otherwise, by minimal identifiability,

V1 = 1  W1, i = 2, ..., n : Vi = i  Wi  i--11 and Vn+1 = Wn+1  n-1

(52)

In addition,

F [Wi, ..., W1] = Wi    Wi-1  ...    W1 F [Vi, ..., V1] = (i  Wi  i--11)    (i-1  Wi-1  i--12)  ...    (1  W1)
Since each for all k  [i], k commutes with , we have,

(53)

F [Vi, ..., V1] = i  F [Wi, ..., W1]

(54)

and F [Wi, ..., W1]  F [Vi, ..., V1]-1 = i-1  Invariant(N )
By similar considerations, F [Wn+1, ..., Wi]  F [Vn+1, ..., Vi]-1  Invariant(N ).

(55)

Lemma 10. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Then, every invertible linear mapping W is a member of C0.

Proof. Let p  Cn. Then, p = F [Wn+1, ..., W1], for invertible linear mappings W1, ..., Wn+1. In particular, W  p = F [W · Wn+1, ..., W1]  Cn, p  W = F [Wn+1, ..., W1 · W ]  Cn and similarly, W -1  p, p  W -1  Cn. Therefore, W  C0.

Lemma 11. C0 is closed under inverse and composition, i.e, u  C0  u-1  C0

(56)

and,

u1, u2  C0 = u1  u2  C0

(57)

Proof. Inverse: By definition, u  C0 iff for all n  N and q  Cn, we have: u  q, q  u, u-1  q, q  u-1  Cn iff u-1  C0.

Decomposition: u-1 1  u2-1  f, f

Let f  u1-1

 Cn.  u-2 1,

Then, f  u2

g 

= u1

u1 

f Cn.



Cn

and

u2

 u1

f

=

u2

g



Cn.

Similarly,

47

Under review as a conference paper at ICLR 2018

C.4 PROPERTIES OF INVERSES

Lemma 12. Let N = SCM[] where  is the Leaky ReLU activation function, with parameter 0 < a = 1. Let f = F [Wn+1, ..., W1] be a minimal decomposition. Then, for all i  [n], we have:

F [Wi-+11/a, ..., Wn-1/a, -Wn-+11/a]  f = -1/a ·   F [Wi, ..., W1]

(58)

Proof. We prove this statement by induction on i from i = n backwards to i = 1. Case i = n: Then, F [Wi-+11/a, ..., -Wn-+11/a] = F [-Wn-+11/a] = -Wn-+11/a. In addition,
F [-Wn-+11/a]  f = -1/a ·   Wn    Wn-1    ...    W1 = -1/a ·   F [Wn, ..., W1] = -1/a · F [Id, Wn, ..., W1]

(59)

Induction hypothesis: We assume that: F [Wi-+11/a, ..., Wn-1/a, -Wn-+11/a]  f = -1/a · F [Id, Wi, ..., W1]

(60)

Case i - 1: We consider that by the induction hypothesis:

F [Wi-1/a, ..., Wn-1/a, -Wn-+11/a]  f = Wi-1/a  F [Id, Wi-+11/a, ..., Wn-1/a, -Wn-+11/a]  f = Wi-1/a    F [Wi-+11/a, ..., Wn-1/a, -Wn-+11/a]  f = Wi-1/a    -1/a    F [Wi, ..., W1] = -Wi-1/a  F [Wi, ..., W1] = -1/a · F [Id, Wi-1, ..., W1] (61)
Finally, we conclude that:

F [Wi-+11/a, ..., Wn-1/a, -Wn-+11/a]  f = -1/a ·   F [Wi, ..., W1]

(62)

Lemma 13. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Then, for all 0 > 0, we have CB0,A = CA0,B.

Proof. Let k  EA0,B, EB0,A and m  k + CB0,A. We take y  H 0 (A, B; m). Then, C(y) = CA0,B. In addition, discm(y  DA, DB)  0. By the third part of Lem. 5, for h : y, we have:

disck(y-1  DB, DA)  discm(y  DA, DB)  0

(63)

In particular, CB0,A  C(y-1). In addition, by Lem. 1, C(y-1) = C(y). Therefore, CB0,A  CA0,B. By symmetric arguments (switching between A and B) we also have the opposite side and thus,
CB0,A = CA0,B .

D PROOF OF THM. 1 AND ITS GENERALIZATION THM. 2

D.1 COVERING NUMBERS

Definition 10 (Set embedding). Let (U , U ) and (V, V ) be two tuples of sets and symmetric and reflexive relations on them (resp.). A function G : U  V is an embedding of (U , U ) in (V, V ) and we denote (U , U ) (V, V ) if:

u1, u2  U : G(u1) V G(u2) = u1 U u2

(64)

Lemma 14. Let (U , U ) and (V, V ) be two tuples of sets and reflexive and symmetric relations on them (resp.). If (U , U ) (V, V ) then N(U , U )  N(V, V ).

48

Under review as a conference paper at ICLR 2018

Proof. Assume that (U , U ) (V, V ). Then, by definition, there is an embedding function G : U  V such that:

u1, u2  U : G(u1) V G(u2) = u1 U u2

(65)

Let (V, V ) be a covering of (V, V ). We define a covering (U , U ) of (U , U ) as follows:

u1 U u2  G(u1) V G(u2)

(66)

Part 1: We would like to prove that (U , U ) is a covering of (U , U ). It is easy to see that U is an equivalence relation since V is an equivalence relation. Next, we would like to prove that u1 U u2 = u1 U u2. By the definition of U :

u1 U u2 = G(u1) V G(u2)

(67)

In addition, since (V, V ) is a covering of (V, V ):

G(u1) V G(u2) = G(u1) V G(u2)

(68)

Finally, since G is an embedding:

G(u1) V G(u2) = u1 U u2

(69)

We conclude:

u1 U u2 = u1 U u2

Therefore, (U , U ) is indeed a covering of (U , U ).

(70)

Part 2: We would like to prove that |U / U |  |V/ V |. Let u1, u2  U such that u1 U u2. Then, by definition of U we have: G(u1) V G(u2). Therefore, if we take u1, ..., un  U representations of n different equivalence classes in (U , U ) then, G(u1), ..., G(un)  V are n representations of n different equivalence classes in (V, V ). In particular, |U / U |  |V/ V |. Therefore, the covering number of (U , U ) is at most the covering number of (V, V ).

Lemma 15. Let (U , 1) and (U , 2) be two coverings of (U , U ). Then, (U 2, 1 × 2) is a covering of (U 2, U2 ). Where U 2 = U × U and the relation 2U is defined as follows:

(a, b) 2U (c, d)  a U c and b U d and 1 × 2 is defined as:
(a, b) 1 × 2 (c, d)  a 1 c and b 2 d

(71) (72)

Proof. We have to prove that 1 × 2 is an equivalence relation and that (u1, u2) 1 × 2 (v1, v2) = (u1, u2) 2U (v1, v2).

Reflexivity:

(u1, u2) 1 × 2 (u1, u2)  u1 1 u1 and u2 1 u2

(73)

The RHS is true since 1 and 2 are reflexive relations.

Symmetry:

(u1, u2) 1 × 2 (v1, v2)  u1 1 v1 and u2 2 v2

Since 1 and 2 are symmetric, we have:

(74)

u1 1 v1 and u2 2 v2  v1 1 u1 and v2 2 u2

(75)

In addition, Therefore,

(v1, v2) 1 × 2 (u1, u2)  v1 1 u1 and v2 2 u2 (u1, u2) 1 × 2 (v1, v2)  (v1, v2) 1 × 2 (u1, u2)

(76) (77)

Transitivity: follows from similar arguments.

Covering:

(u1, u2) 1 × 2 (v1, v2)  u1 1 v1 and u2 2 v2

(78)

49

Under review as a conference paper at ICLR 2018

Since (U , i) is a covering of (U , U ), for i = 1, 2, we have:

u1 1 v1 and u2 2 v2 = u1 U v1 and u2 U v2 By the definition of 2U we have:
u1 U v1 and u2 U v2  (u1, u2) U (v1, v2)

Therefore,

(u1, u2) 1 × 2 (v1, v2) = (u1, u2) 2U (v1, v2)

(79) (80) (81)

Lemma 16. Let (U, U ) be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then,

N(U 2, U2 )  N(U , U )2

(82)

Proof. Let U be an equivalence relation such that (U , U ) is a covering of (U , U ). By Lem. 15, (U 2, 2U ) is a covering of (U 2, U2 ). In addition,

|U 2/ 2U | = |U / U |2

(83)

Thus, for every covering (U , U ) of (U , U ), particular, N(U 2, 2U )  N(U , U )2.

there

is

a

covering

of

(U 2, 2U )

of

size

|U /

U

|2.

In

Lemma 17. Let (U, U ) be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then,

N(U , U )  N(U 2, 2U )

(84)

Proof. We define an embedding, because,

embedding F (u) 2U F

from (U, (v) =

U ) to (U (u, u) 2U

2, U2 ) (v, v)

as follows F (u) = u U v.

=

(u, u).

This

is

an

Lemma 18. Let (U , U ) and (V, V ) be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that U  V and U := (V ) U , i.e,

u, v  U : u U v  u V v

(85)

Then,

N(U , U )  N(V, V )

(86)

Proof. Let (V, V ) be a covering of (V, V ). Then, it is easy to see that (U , U ) is a covering
of (U , U ), where U := (V ) U . In addition, we have: |U / U |  |V/ V |. Thus, for every covering of (V, V ), we can find a smaller covering for (U , U ). In particular, N(U , U )  N(V, V ).

D.2 PERTURBATIONS AND DISCREPANCY
Thm. 1 employs assumption 3. In Lem. 19 we prove that this assumption holds for the case of a continuous risk (assumption 4) if the discriminators have bounded weights.
Assumption 3. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. For every m > 0 (possibly ) and n > 0, the function discm(F [Wn, ..., W1]  D1, D2) is continuous as a function of the weights of W1, ..., Wn  RM×M .
Assumption 4. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. For all m > 0, the function RD [F [Vm, ..., V1], F [Wm, ..., W1]] is continuous as a function of Vm, ..., V1, Wm, ..., W1.
Lemma 19. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1 and assume Assumption 4 for D : D1. Let discm,E := discCm,E for

Cm,E = F [Wm, ..., W1] i  [m] : ||Wi||  E

(87)

Then, for all m > 0, n > 0 and E > 0, the function discm,E(F [Wn, ..., W1]  D1, D2) is continuous as a function of Wn, ..., W1.

50

Under review as a conference paper at ICLR 2018

Proof. Let Wn, ..., W1 and Wnk, ..., W1k be any invertible matrices in RM×M such that for all i  [n], Wik  Wi. We denote GE = W  RM×M ||W ||  E . By the triangle inequality,

discm,E(D1, D2)  discm,E(D1, D3) + discm,E(D3, D2) = discm,E(D1, D2) - discm,E(D3, D2)  discm,E(D1, D3)

(88)

Similarly,
therefore, In particular,

discm,E(D3, D2)  discm,E(D1, D3) + discm,E(D1, D2) = discm,E(D3, D2) - discm,E(D1, D2)  discm,E(D1, D3)
|discm,E(D3, D2) - discm,E(D1, D2)|  discm,E(D1, D3)

(89) (90)

discm,E (F [Wn, ..., W1]  D1, D2) - discm,E (F [Wnk, ..., W1k]  D1, D2)

discm,E(F [Wn, ..., W1]  D1, F [Wnk, ..., W1k]  D1)

 sup RD1 [c1  F [Wn, ..., W1], c2  F [Wn, ..., W1]] - RD1 [c1  F [Wnk, ..., W1k], c2  F [Wnk, ..., W1k]] c1 ,c2 Cm,E

 sup

RD1 [F [Vm, ..., V1]  F [Wnk, ..., W1k], F [Um, ..., U1]  F [Wn, ..., W1]]

V1 ,..,Vm ,U1 ,...,Um GE

- RD1 [F [Vm, ..., V1]  F [Wnk, ..., W1k], F [Um, ..., U1]  F [Wnk, ..., W1k]]

 sup

RD1 [F [Vm, ..., V2, V1 · Wn, Wn-1, ..., W1], F [Um, ..., U2, U1 · Wn, Wn-1, ..., W1]]

V1 ,..,Vm ,U1 ,...,Um GE

- RD1 F [Vm, ..., V2, V1 · Wnk, Wnk-1, ..., W1k], F [Um, ..., U2, U1 · Wnk, Wnk-1, ..., W1k]
(91) Assume by contradiction that the last expression does not converge to 0. Therefore, there is a sequence (V1k, ..., Vmk , U1k, ..., Umk ) such that V1k, .., Vmk , U1k, ..., Umk  GE and

Qk = RD1 F [Vmk , ..., V2k, V1k · Wn, Wn-1, ..., W1], F [Umk , ..., U2k, U1k · Wn, Wn-1, ..., W1]

- RD1 F [Vmk , ..., V2k, V1k · Wnk, Wnk-1, ..., W1k], F [Umk , ..., U2k, U1k · Wnk, Wnk-1, ..., W1k]  0
(92) In particular, there is some > 0 and an increasing sequence {kj}j=1  N such that Qkj > for all j  N. With no loss of generality, we can assume that kj = j (otherwise, we replace the original sequence with the new one). Since (V1kj , ..., Vmkj , U1kj , ..., Umkj )  G2Em and G2Em is compact in RM×M×2m, by the Bolzano-Weierstrass theorem, it has a converging subsequence. With no loss of generality, we can assume that (V1kj , ..., Vmkj , U1kj , ..., Umkj ) converges (otherwise, we replace it with a converging sub-sequence):

In particular,

(V1kj , ..., Vmkj , U1kj , ..., Umkj )  (V1, ..., Vm, U1, ..., Um)  G2Em

(93)

(Vmkj , ..., V2kj , V1kj · Wnkj , Wnk-j 1, ..., W1kj )  (Vm, ..., V2, V1 · Wn, Wn-1, ..., W1) (Umkj , ..., U2kj , U1kj · Wnkj , Wnk-j 1, ..., W1kj )  (Um, ..., U2, U1 · Wn, Wn-1, ..., W1)

(94)

By Assumption 4, the function RD1 [F [Xm+n, ..., X1], F [Ym+n, ..., Y1]] is continuous. Therefore,

RD1 F [Vmkj , ..., V2kj , V1kj · Wn, Wn-1, ..., W1], F [Umkj , ..., U2kj , U1kj · Wn, Wn-1, ..., W1] - RD1 [F [Vm, ..., V2, V1 · Wn, Wn-1, ..., W1], F [Um, ..., U2, U1 · Wn, Wn-1, ..., W1]]  0

(95)

and,

RD1 [F [Vm, ..., V2, V1 · Wn, Wn-1, ..., W1], F [Um, ..., U2, U1 · Wn, Wn-1, ..., W1]]
- RD1 F [Vmkj , ..., V2kj , V1kj · Wnkj , Wnk-j 1, ..., W1kj ], F [Umkj , ..., U2kj , U1kj · Wnkj , Wnk-j 1, ..., W1kj ]  0 (96)

51

Under review as a conference paper at ICLR 2018

Therefore, by the triangle inequality,

Qkj = RD1 F [Vmkj , ..., V2kj , V1kj · Wn, Wn-1, ..., W1], F [Umkj , ..., U2kj , U1kj · Wn, Wn-1, ..., W1]
- RD1 F [Vmkj , ..., V2kj , V1kj · Wnkj , Wnk-j 1, ..., W1kj ], F [Umkj , ...U2kj , U1kj · Wnkj , Wnk-j 1, ..., W1kj ] (97)
in contradiction. Thus, we conclude that:

0

lim
k

discm,E(F [Wn, ..., W1]  D1, D2) - discm,E(F [Wnk, ..., W1k]  D1, D2)

=0

(98)

Lemma 20. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. In addition,

let f = F sumptions

[Wn+1, ..., W1] 2 and 3. Then,

and g = there are

F [Vn+1, functions

.f.¯.,=V1F]

be two minimal decompositions. [W¯ n+1, ..., W¯ 1] and g¯ = F [V¯n+1,

Assume As..., V¯1] such

that:

· C(f¯  g) = 2n. · j  [n + 1] : discm(F [W¯ j, ..., W¯ 1]  D, F [Wj, ..., W1]  D)  . · j  [n + 1] : discm(F [V¯j, ..., V¯1]  D, F [Vj, ..., V1]  D)  .

Proof.
 > 0, all j 

We consider that there are f¯ = F [n + 1]: ||W¯ j -

[fWW¯jng|+|,=1|,|V¯.F.j.[,-WW¯nV1+]j |1a|,n.d..,g¯W.=B2 ,yFWA[V1¯sn·s+Vun1m,+p.1.t.,i,oVV¯nn1.,]3.s.,.u,fcoVhr1 ]te.haBacthyCA(s>f¯su0m,g¯pt)hti=eorne22in,s faaonrsdmeafacolhrl

enough ||W¯ j -

>0 Wj ||,

|s|uV¯cjh-thVajt:||f¯=,Fw[We¯hna+v1e,:...,

W¯ 1]

and

g¯

=

F

[V¯n+1,

...,

V¯1]

such

that

for

all

j



[n

+

1]:

· j  [n + 1] : discm(F [W¯ j, ..., W¯ 1]  D, F [Wj, ..., W1]  D)  . · j  [n + 1] : discm(F [V¯j, ..., V¯1]  D, F [Vj, ..., V1]  D)  .

In particular, for any > 0, there are functions f¯ = F [W¯ n+1, ..., W¯ 1] and g¯ = F [V¯n+1, ..., V¯1] with the desired properties.

Lemma 21. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Assume

Assumption 1. Let f

D
m,

g. Then, for every minimal decomposition f

= F [Wn+1, ..., W1] there is a

minimal decomposition g = F [Vn+1, ..., V1 ] such that:

i



[n

+

1]

:

F

[Wi

,

...,

W1]



D


m,

F [Vi , ..., V1 ]  D

(99)

Proof. Since f

D
m,

g there are minimal decompositions f

= F [Wn+1, ..., W1] and g =

F [Vn+1, ..., V1] such that:

i



[n

+

1]

:

F

[Wi,

...,

W1]



D


m,

F [Vi, ..., V1]  D

(100)

By Assumption 1, W1 = 1  W1, for all i = 2, ..., n: Wi = i  Wi  i--11 and Wn+1 = Wn+1  n-1. Therefore, we define a minimal decomposition for g as follows: g = F [Vn+1, ..., V1 ] such that V1 = 1  V1, for all i = 2, ..., n: Vi = i  Vi  i--11 and Vn+1 = Vn+1  n-1. This is a minimal decomposition of g, since each invariant function is an invertible linear mapping and commutes with
. By Lem. 9 we have:

i  [n] : F [Wi , ..., W1] = i  F [Wi, ..., W1] and F [Vi , ..., V1 ] = i  F [Vi, ..., V1] Therefore, by Lem. 4, since C(i) = 0, we have:

(101)

i  [n] :discm(F [Wi , ..., W1]  D, F [Vi , ..., V1 ]  D)  discm(i  F [Wi, ..., W1]  D, i  F [Vi, ..., V1]  D) 

(102)

52

Under review as a conference paper at ICLR 2018

Alternatively,

i



[n]

:

F

[Wi

,

...,

W1]



D


m,

F [Vi , ..., V1 ]  D

(103)

Since F [Wn+1, ..., W1] = f = F [Wn+1, ..., W1] and F [Vn+1, ..., V1 ] = g = F [Vn+1, ..., V1] we

also

have

F

[Wn+1,

...,

W1]



D


m,

F [Vn+1, ..., V1 ]  D.

Lemma 22. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. We have:

f

DA


f

, f¯ DA

f

and f¯

DA

f

= f¯

DA


f¯

k, 1

k,

k,

k, 1-2

(104)

Proof. Assume by contradiction that f¯ DA f¯ . Then, there are decompositions f¯ =
k, 1-2
F [W¯ n+1, ..., W¯ 1] and f¯ = F [W¯ n+1, ..., W¯ 1] such that:

j  [n + 1] : disck(F [W¯ j, ..., W¯ 1]  DA, F [W¯ j, ..., W¯ 1]  DA)  1 - 2

(105)

By Lem. 21, since f¯ DA f and f¯ DA f , there are minimal decompositions f = F [Wn+1, ..., W1]
k, k,
and f = F [Wn+1, ..., W1] such that:

j  [n + 1] :disck(F [Wj, ..., W1]  DA, F [W¯ j, ..., W¯ 1]  DA)  disck(F [Wj, ..., W1]  DA, F [W¯ j, ..., W¯ 1]  DA) 

(106)

DA
Since f  f , there is an index i  [n + 1] such that:
k, 1

disck(F [Wi, ..., W1]  DA, F [Wi , ..., W1]  DA) > 1 Therefore, by the triangle inequality, we arrive to a contradiction:

(107)

disck(F [Wi, ..., W1]  DA, F [Wi , ..., W1]  DA) disck(F [W¯ i, ..., W¯ 1]  DA, F [Wi , ..., W1]  DA)
+ disck(F [W¯ i, ..., W¯ 1]  DA, F [Wi, ..., W1]  DA)
disck(F [W¯ i, ..., W¯ 1]  DA, F [W¯ i , ..., W¯ 1]  DA) + disck(F [W¯ i, ..., W¯ 1]  DA, F [Wi, ..., W1]  DA)
+ disck(F [W¯ i , ..., W¯ 1]  DA, F [Wi , ..., W1]  DA) ( 1 - 2 ) + + = 1

(108)

Lemma 23. Let N = SCM[] with  that is a Leaky ReLU with parameter 0 < a = 1. Let A = (XA, DA) and B = (XB, DB) are two domains. We have:

N

H 0 (A, B),

DA
1+2 0

 N H 0 (B, A), DB
1

(109)

Proof. We would like to show that the function G(h) = h-1 is an embedding of

H 0 (A, B),

DA
1+2 0

into

H 0 (B, A), DB . First, we consider that if h  H 0 (A, B), then,
1

disc(G(h)  DB, DA) = disc(h-1  DA, DB)  disc(h  DA, DB)  0

(110)

and by Lem. 1 and Lem. 13, C(G(h)) = C(h-1) = C(h) = CA0,B = CB0,A. Therefore, G(h) 

H 0 (B, A). Next, we would like to prove that for all h1, h2  H 0 (A, B): G(h1) DB G(h2) =
1

h1

DA
1 +2

0

h2.

53

Under review as a conference paper at ICLR 2018

Let h1, h2  H 0 (A, B) such that G(h1) DB G(h2). Then, there are minimal decompositions
1
G(h1) = F [Wn+1, ..., W1] and G(h2) = F [Vn+1, ..., V1] such that:

i  [n] : disc(F [Wi, ..., W1]  DA, F [Vi, ..., V1]  DA)  1 and: disc(G(h1)  DA, G(h2)  DA)  1

(111)

We consider that by Lem. 1, G(h1) = F [-W1-1, W2-1/a, ..., Wn-1/a, -Wn-+11/a] and G(h2) = F [-V1-1, V2-1/a, ..., Vn-1/a, -Vn-+11/a] are minimal decompositions. In addition, by Lem. 12, we have:

i  [n] :F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  G(h1) = -1/a ·   F [Wi, ..., W1] F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  G(h2) = -1/a ·   F [Vi, ..., V1]

(112)

By the first item of Lem. 5, for D1 := DB, D2 := h1 DA, D3 := F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a] DB and p := F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a],

disc(F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  DB, F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  DB)

disc(F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  h1  DA, F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  DB) + disc(h1  DA, DB)

(113)

disc(F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  h1  DA, F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  DB) + 0

Similarly (by the first item of Lem. 5), we have:

disc(F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  DB, F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  DB)

disc(F [Wi-+11/a, ...Wn-1/a, -Wn-+11/a]  h1  DA, F [Vi-+11/a, ...Vn-1/a, -Vn-+11/a]  h2  DA) + 2 0

=disc(-1/a ·   F [Wi, ..., W1]  DA, -1/a ·   F [Vi, ..., V1]  DA) + 2 0

disc(F [Wi, ..., W1]  DA, F [Vi, ..., V1]  DA) + 2 0  1 + 2 0

Therefore, we conclude that h1

DA
1 +2

0

h2.

(114)

D.3 PROOF OF THM. 1

Theorem 2. Let N = SCM[] with  that is a Leaky ReLU with parameter 0 < a = 1. Assume
Assumptions 1, 2 and 3. Let 0, 1 and 2 such that 0 < 1/2 and 2 < 1 - 2 0 be three positive constants and A = (XA, DA) and B = (XB, DB) are two domains. Assume that m  k+2CA0,B +2.
Then,

N

H

0 (A, B;

m),

DA
k, 1

 lim N
0

DPM2 0+

B; k, 2CA0,B

, DB
m, 2

(115)

Proof. Let be any positive constant such that: < min{( 1 - 2 0 - 2)/4, 2/2}. For such , we have 2 0  1 - 4 and 2  1 - 2 0 - 4 . In addition, let t := k + CA0,B + 1. We would like to find an embedding mapping:

G : (H 0 (A, B; m))2  DPM2 0+ B; k, 2CA0,B + 2

(116)

Part 1: In this part, we show how to construct G. Let (f, g)  (H 0 (A, B; m))2. We denote:

f= Lem.

F [Wn+1, ..., 20, there are

Wfun1c] taionndsgf¯==

F F

[[VW¯nn++11, ,.....,.,VW1¯]1m] ainnidmg¯al=dFec[oV¯mn+po1,si.t..i,oVn¯1s

of f and g ] such that:

(resp.).

By

· C(f¯  g¯-1) = 2n. · j  [n + 1] : discm(F [W¯ j, ..., W¯ 1]  DA, F [Wj, ..., W1]  DA)  . · j  [n + 1] : discm(F [V¯j, ..., V¯1]  DA, F [Vj, ..., V1]  DA)  .

54

Under review as a conference paper at ICLR 2018

We define G(f, g) = f¯  g¯-1.

Part 2: In this part, we show that:

(f, g)  (H 0 (A, B; m))2 = G(f, g)  DPM2 0+2 DB; k, 2CA0,B

(117)

By Part 1, C(f¯ g¯-1) = 2n = 2CA0,B. In addition, by the first item of Lem. 5, for D1 : g¯-1  DB, D2 : DA, D3 : DB, p : f¯, t  k + CA0,B we have:

disck(f¯  g¯-1  DB, DB)  disct(f¯  DA, DB) + disct(g¯-1  DB, DA)

(118)

Since f  H 0 (A, B; m): disct(f¯  DA, DB)  discm(f  DA, DB) + discm(f¯  DA, f  DA)  0 +

(119)

In addition, by the third item of Lem. 5, for h : g¯ and m  t + CA0,B  t + C(g¯-1), we have:

disct(g¯-1  DB, DA)  discm(g¯  DA, DB)  discm(g  DA, DB) + discm(g  DA, g¯  DA)  0 +

(120)

Finally, disck(f¯  g¯-1  DB, DB)  2 0 + 2 and we conclude that:

G(f, g)  DPM2 0+2 B; k, 2CA0,B

(121)

Part 3: In this part, we show that G is an embedding. It requires showing that

2

G(f, g) DB G(f , g ) = (f, g) DA (f , g )

m, 2

k, 1

(122)

Assume by contradiction that G(f, g)

DB

G(f

, g ) and that (f, g)

DA


(f

, g ). Then, we have

m, 2

k, 1

DA

DA

f  f or g  g

k, 1

k, 1

We denote G(f, g) = f¯  g¯-1 and G(f , g ) = f¯  (g¯ )-1 (see Part 1).

(123)

Assume that f

DA


f

: By Lem. 22, f¯

DA


f¯ . In particular, for every two decompositions:

k, 1

k, 1-2

f¯ = F [W¯ n+1, ..., W¯ 1] and f¯ = F [W¯ n+1, ..., W¯ 1]

(124)

there is an index i  [n + 1] such that:

disck(F [W¯ i, ..., W¯ 1]  DA, F [W¯ i , ..., W¯ 1]  DA) > 1 - 2

(125)

The option i = n + 1 is not a possibility, since:

disck(f¯  DA, f¯  DA) disck(f  DA, DB) + disck(f¯  DA, f  DA) + disck(DB, f  DA) + disck(f¯  DA, f  DA)

(126)

2 0 + 2  1 - 2

By the first item of Lem. 5, for D1 : DA, D2 : g¯-1  DB, D3 : p : F [W¯ i, ..., W¯ 1] and t  k + CA0,B  k + C(F [W¯ i, ..., W¯ 1]), we have:

F [W¯ i , ..., W¯ 1]

 DA,

disck(F [W¯ i, ..., W¯ 1]  DA, F [W¯ i , ..., W¯ 1]  DA)

 disct(F [W¯ i, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  DA) + disct(g¯-1  DB, DA)

(127)

 disct(F [W¯ i, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  DA) + 0

Again, by the first item of Lem. 5, for D1 : DA, D2 : (g )-1  DB, D3 : F [W¯ i, ..., W¯ 1]  g-1  DB, p : F [W¯ i , ..., W¯ 1] and m  t + CA0,B  t + C(F [W¯ i , ..., W¯ 1]), we have:

disct(F [W¯ i, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  DA)

discm(F [W¯ i, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  (g¯ )-1  DB) + discm((g¯ )-1  DB, DA) (128)

discm(F [W¯ i, ..., W¯ 1]  g-1  DB, F [W¯ i , ..., W¯ 1]  (g )-1  DB) + 0

55

Under review as a conference paper at ICLR 2018

Therefore, we conclude that:

1 - 2 0 - 2 < discm(F [W¯ i, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  (g¯ )-1  DB)

(129)

Alternatively, for any minimal decompositions f¯ F [W¯ n+1, ..., W¯ 1]  (g¯ )-1 there are right partial

fug¯n-ct1io=nsFF[[WW¯¯ni+, .1..,,.W..¯, 1W¯] 1]g¯-g¯1-a1ndanFd[fW¯¯ i,

(g¯ )-1 = ..., W¯ 1] 

(g¯ )-1 such that:

1 - 2 0 - 2 < discm(F [W¯ n+1, ..., W¯ 1]  g¯-1  DB, F [W¯ i , ..., W¯ 1]  (g¯ )-1  DB) (130)

in contradiction to F (f, g) DB F (f , g ).
m, 2

DA

DA

Assume that g  g : By Lem. 22, g¯  g¯ . Let

k, 1

k, 1-2

g¯-1 = F [-V¯1, V¯2-1/a, ..., V¯n-1/a, -V¯n-+11/a] and (g¯ )-1 = F [-(V¯1 )-1, (V¯2 )-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]

(131)

be any two minimal decompositions of g¯-1 and (g¯ )-1 (resp.). Then, by Lem. 12, there are minimal decompositions g¯ = F [V¯n+1, ..., V¯1] and g¯ = F [V¯n+1, ..., V¯1 ] such that:

j  [n] :F [V¯j-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  g¯  DA = -1/a ·   F [V¯j , ..., V¯1]  DA
and: F [(V¯j+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA = -1/a ·   F [V¯j , ..., V¯1 ]  DA (132)
DA
Since g¯  g¯ , there is an index i  [n + 1] such that:
k, 1-2

disck(F [V¯i, ..., V¯1]  DA, F [V¯i , ..., V¯1 ]  DA) > 1 - 2

(133)

The case i = n + 1 is not a possibility, similarly to Eq. 126. Therefore, there is i  [n] such that Eq. 133 holds. In addition,

disck+1(-1/a ·   F [V¯i, ..., V¯1]  DA, -1/a ·   F [V¯i , ..., V¯1 ]  DA)

=disck+1 F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  g¯  DA,

(134)

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA

By Lem. 4, for p : -1/a ·  of complexity 1 we have:

1 - 2 < disck(F [V¯i, ..., V¯1]  DA, F [V¯i , ..., V¯1 ]  DA)  disck+1(-1/a ·   F [V¯i, ..., V¯1]  DA, -1/a ·   F [V¯i , ..., V¯1 ]  DA)

(135)

In addition, by Lem. 5, for D1 : g¯  DA, D2 : DB, D3 : F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA, t  (k + 1) + CA0,B  (k + 1) + C(F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]), we have:

disck+1 F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  g¯  DA,

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA

 disct F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB, F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA + disct(g¯  DA, DB)

(136)

 disct F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB,

F [(Vi+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA + 0 +

56

Under review as a conference paper at ICLR 2018

Again, by Lem. 5, for D1 : g¯  DA, D2 : DB, D3 : F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB, m  t + CA0,B  t + C(F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]), we have:
disct F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB,

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA

 discm F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB, F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  DB + discm(g¯  DA, DB)

(137)

 discm F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB,

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  DB + 0 +

Finally,

1 - 2 < disck(-1/a ·   F [V¯i, ..., V¯1]  DA, -1/a ·   F [V¯i , ..., V¯1 ]  DA)  disct F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB,

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  g¯  DA + 0 +  discm F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB,

(138)

F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  DB + 2 0 + 2

In particular,

2  1 - 2 0 - 4 < discm F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a]  DB, F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]  DB

(139)

Alternatively, for any minimal decompositions
f¯  g¯-1 = F [W¯ n+1, ..., W¯ 2, -W¯ 1 · V¯1, V¯2-1/a, ..., V¯n-1/a, -V¯n-+11/a] and f¯  (g¯ )-1 = F [W¯ n+1, ..., W¯ 2, -W¯ 1 · (V¯1 )-1, (V¯2 )-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]
there are right partial functions F [V¯i-+11/a, ..., V¯n-1/a, -V¯n-+11/a] and F [(V¯i+1)-1/a, ..., (V¯n)-1/a, -(V¯n+1)-1/a]

(140) (141)

such that Eq. 139 holds, in contradiction to F (f, g) DB F (f , g ).
m, 2
Part 3: Finally, by Lem. 17 and Lem. 14,

2

N

H

0 (A, B; m),

DA
k, 1

N

(H 0 (A, B; m))2,

DA
k, 1

 N DPM2 0+2

B; k, 2CA0,B

, DB
m, 2

Alternatively, for all 0, 1, 2, such that < min{( 1 - 2 0 - 2)/4, 2/2},

N

H

0 (A,

B;

m),

DA
k, 1

 N DPM2 0+2

B; k, 2CA0,B

, DB
m, 2

(142) (143)

57

Under review as a conference paper at ICLR 2018

In particular, we can replace with /2 in the inequality. By Lem. 18, the function q =

N DPM2 0+

B; k, 2CA0,B

, DB
m, 2

is monotonically decreasing as

tends to 0 and is lower

bounded by N

DPM2 0

B; k, 2CA0,B

, DB
m, 2

. Therefore, by the monotone convergence theorem,

the limit lim 0 q exists and upper bounds N

H

0

(A,

B

;

m),

DA
k, 1

.

Theorem 1. Let N = SCM[] with  that is Leaky ReLU with parameter 0 < a = 1. Assume Assumptions 1, 2 and 3. Let 0, 1 and 2 be three constants such that 0 < 1/4 and 2 < 1 - 4 0 be three positive constants and A = (XA, DA) and B = (XB, DB) are two domains. Then,



N

H 0 (A, B), DA
1

N   lim min 0 N 

DPM2 0+ DPM2 0+

A; 2CA0,B , DA 2
B; 2CA0,B , DB

2

(32)

Proof. By Lem. 2, with m = k = , we have:

N H 0 (A, B), DA
1

 lim N
0

DPM2 0+

B; 2CA0,B , DB 2

Similarly,

N

H 0 (B, A),

DB
1-2 0

 lim N
0

DPM2 0+

B; 2CA0,B , DB 2

By Lem. 23,

N H 0 (A, B), DA
1

N

H

0 (B, A),

DB
1-2 0

Since the limits in the RHS of Eqs. 144 and 145 are limits of positive integers, we have:



N

H 0 (A, B), DA
1

lim 

0

N

 min

lim 

0

N

DPM2 0+ DPM2 0+

B; 2CA0,B , DB 2
A; 2CA0,B , DA

2


N   lim min 0 N 

DPM2 0+ DPM2 0+

B; 2CA0,B A; 2CA0,B

, DB
2
, DA
2

(144) (145) (146)
(147)

58

