Under review as a conference paper at ICLR 2018
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION
Anonymous authors Paper under double-blind review
ABSTRACT
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt­Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.
1 INTRODUCTION
Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, some internet companies may be interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal.
Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015; Zhang, 2003; Cochrane, 2005; Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt­Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012; Scott & Varian, 2014; Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS) (Hyndman et al., 2008) decomposes times series into error, trend, seasonal that change over time. In Internet industry, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015; Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst.
However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as
· Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
· Change points. A market intervention, such as a new product launch or the onset of an ads campaign, may lead to the level change of the original series.
1

Under review as a conference paper at ICLR 2018

Time series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary.
In this paper, we develop a state space time series forecasting model in the Bayesian framework, jointly detect anomaly and change points. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals for time series forecasting. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points. To summarize, our work has the following contributions.
· Our proposed method outperforms the state-of-the-art methods in time series forecasting, especially when there exist change points and anomalies. By our method, we are able to obtain distributions and intervals for forecasting.
· Along with time series forecasting, our proposed method automatically detects change points and anomalies, and it achieves high accuracy and low false discovery rate on both tasks, outperforming some popular change point and anomaly detection methods.
· Our method is flexible to capture the structure of time series under various scenarios. By default, it takes the trend, seasonality, change points and anomalies into consideration, but it can be easily modified to study time series without some components, for example, time series without seasonality. Thus, our method can be applied to many settings in practice.

2 MODEL

State space time series model has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model.

Observed Data

Trend

Seasonality

Change Points

Anomaly Points

Residual

-5 -4 -3 -2 -1 0 1 2 -5 -4 -3 -2 -1 0 1 2 -5 -4 -3 -2 -1 0 1 2 -5 -4 -3 -2 -1 0 1 2 -5 -4 -3 -2 -1 0 1 2 -5 -4 -3 -2 -1 0 1 2

0 100 200 300 400 500

0 100 200 300 400 500

0 100 200 300 400 500

0 100 200 300 400 500

0 100 200 300 400 500

0 100 200 300 400 500

Figure 1: Demostration of Decompositions.

Let y = (y1, y2, . . . , yn) be a sequence of time series observations with length n. The ultimate goal is to forecast (yn+1, yn+2, . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with
time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the top left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.

2

Under review as a conference paper at ICLR 2018

As the classical state space model, we have observation equation and transition equations to model
y and hidden variables. We use µ = (µ1, µ2, . . . , µn) to model trend, and use  = (1, 2, . . . , n) to model seasonality. We use a binary vector za = (z1a, z2a, . . . , zna) to indicate anomaly points. Then we have

Observation equation: yt = µt + t +

t, if zta = 0 ot, if zta = 1

.

(1)

The deviation between the observation yt and its "mean" µt + t is modeled by t and ot, depending on the value of zta. If zta = 1, then yt is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1, 2, . . . , n), the anomaly is captured by o = (o1, o2, . . . , on) which has relative
large magnitude.

The hidden state variable µ and  have intrinsic structures. There are two transition equations, for trend and seasonality separately

Transition Equations:

Trend:

µt = µt-1 + t-1 +

ut, if ztc = 0 rt, if ztc = 1

t = t-1 + vt,

S-1

Seasonality: t = - t-s + wt.

s=1

,

(2) (3)

In Equation (2),  = (1, 2, . . . , n) can be viewed as the "slope" of the trend, measuring how fast
the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector zc = (z1c, z2c, . . . , znc ). If ztc = 1, it means the t-th point is a change point, with µt differs from µt-1 + t-1 (which can be interpreted as the "momentum" from the previous status )
by rt; otherwise it is not a change point and they differ by ut. We model the change points in a way
such that r = (r1, r2, . . . , rn) have larger magnitude compared u = (u1, u2, . . . , un). The "slope"
part  also has its own noise v = (v1, v2, . . . , vn).

A first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation (2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).

The seasonality component is presented in Equation (3). Here S is the length of one season and w = (w1, w2, . . . , wn) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.

The observation equation and transition equations together (i.e., Equation (1,2,3)) together define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.

3 BAYESIAN FRAMEWORK
Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014); Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed
{ t}tn=1 iid N (0, 2), {ot}tn=1 iid N (0, o2), {ut}nt=1 iid N (0, u2 ), {rt}nt=1 iid N (0, r2), {vt}nt=1 iid N (0, v2), {wt}tn=1 iid N (0, w2 ), where  , o, u, r, v, w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator za and change point indicator zc to the model them as Bernoulli random variables
{zta}nt=1 iid Ber(pa), {ztc}nt=1 iid Ber(pc),

3

Under review as a conference paper at ICLR 2018

Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.

where pa, pc are probabilities for each point to be an anomaly or change point.
For simplicity, we denote t = (µt, t, t, t-1, . . . , t-(S-2)) to include the main hidden variables (except zta and ztc) in the transition equations. All the t are well defined and can be generated from the previous status, except 1. We denote a1 to be the parameter for 1, which can be interpreted as the "mean" for 1.
With Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former "parameters" and the latter "latent variable", as listed in Table 1.

Category Latent Variable
Parameter

Table 1: Two Categories for Hidden Variables

Hidden Variable  = (1, 2, . . . , n) z = (za, zc)
a1 p = (pa, pc)  = ( , o, u, r, v, w)

Definition Trend and seasonality Anomaly and change points The "mean" for the initial trend and seasonality Probabilities for each point to be anomaly or change point Standard deviation

The discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function can be written down explicitly as

La1,p, (y, , z)

(4)

= g(yt - µt - t,  ) × g(yt - µt - t, o) × g(µt - µt-1 - t-1, u)

{t:zta =0}

{t:zta =1}

{t:ztc =0}

n

n S-1

n

×

g(µt - µt-1 - t-1, r) × g(t - t-1, v) × g(-

t-s, v ) × (pa)zta (1 - pa)1-zta (pc)ztc (1 - pc)1-ztc ,

{t:ztc =1}

t=1

t=1

s=1

i=1

where g(x1, x2)

=

 1 exp
2x2

-x21/(2x22)

is the density function for normal distribution

with mean x1 and standard deviation x2. Here we slightly abuse the notation by using

µ0, 0, 0, -1, . . . , 2-S, which are actually the corresponding coordinates of a1.

4

Under review as a conference paper at ICLR 2018
As long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a1, p, , we are able to generate time series. We present the generative procedure as follows.
Algorithm 1: Generative Procedure Input: Parameters a1,  = ( , o, u, r, v, w) and pa, pc, length of time series to generate m Output: Time series y = (y1, y2, . . . , ym) 1 Generate the indexes where anomalies or change points occur
{zta}tn=1 iid Ber(pa), {ztc}tn=1 iid Ber(pc); 2 Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and
standard deviation  , o, u, r, v, w respectively; 3 Generate {t}tm=1 sequentially by the transition functions in Equation (2) and (3); 4 Generate time series {yt}mt=1 by the observation function in Equation (1).
4 INFERENCE
This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.
4.1 UPDATES ON TREND AND SEASONALITY
In this section, we focus on updating  assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution pa1,p,(|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called "fake-path" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation. Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(t|y1, y2, . . . , yt); while Kalman smoothing distribute information backwards to achieve E(t|y). However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(t|y)}nt=1, instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the "fake-path" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(t|y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(t|y)}tn=1 to obtain a sample from p(|y). This trick involves three steps. Note that all the other hidden variables z, p,  are given.
1. Pick some vector a~1, and generate a sequence of time series y~ from it by Algorithm 1. In this way, we also observe ~.
2. Obtain {E(~t|y~)}tn=1 from y~ by Kalman filter and Kalman smoothing. 3. We use {~t - E(~t|y~) + E(t|y)}tn=1 as our sampling from the conditional distribution.
4.2 CHANGE POINT AND ANOMALY DETECTION
In this section, we update z by Gibbs sampler, assuming , a1, p,  are all given and fixed. We need to obtain the conditional distribution pa1,p,(z|y, ). Note that in the graphical model described in Section 2, {zta}tn=1 and {ztc}in=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution pa1,p,(z|y, ) can also be decomposed into product of Bernoulli
5

Under review as a conference paper at ICLR 2018

density functions. In other words, conditioned on y, , {zta}nt=1 and {ztc}in=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the
calculation point by point. For example, for the anomaly detection for the t-th point, we have

zta = 0 : yt - µt - t  N (0, 2) zta = 1 : yt - µt - t  N (0, o2).

And the prior on zta is P(zta = 1) = pa and P(zta = 0) = p1. Let pat = P(zta = 1|y, ). Directly calculation leads to

pta

=

1-pa 

exp

pa o

exp

- (yt-µt-t)2
2o2

- (yt-µt-t)2
22

+

pa o

exp

- (yt-µt-t)2
2o2

.

(5)

This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let pct = P(ztc = 1|y, ), and we have

ptc =

1-pc u

exp

exp -pc (µt-µt-1-t-1)2
r 2r2

- + exp -(µt-µt-1-t-1)2
2u2

pc r

(µt -µt-1 -t-1 )2 2r2

.

(6)

As mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, . Thus, for Gibbs sampler, we can generate z by sampling independently with

{zta}nt=1  Ber(pta), {ztc}nt=1  Ber(ptc).

For change point detection here, we have an additional segment control step. After obtaining {ztc}tn=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as l). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, -1, -1, 1, 1, 1, 1) and the minimum time series parameter l = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter l is the length of seasonality, i.e., l = S.

Algorithm 2: Segment control on change points

Input: change point binary vector zc,trend µ, standard deviation for outliers r, change point

minimum segment l

Output: change point binary vector zc

1 Denote t1 < t2 < . . . to be all the indexes such that ztci = 1;

while there exists i such that |ti+1 - ti| < l do

2 Check if |µti-1 - µti+1+1|  r/2. If so, exclude both them from change points by setting

ztci

=

zc
ti+1

=

0.

Otherwise,

randomly

exclude

one

of

them

by

setting

the

corresponding

coordinate in zc to be 0;

3 Update all the indexes of change points in zc.

end

4.3 INITIALIZATION AND UPDATES ON PARAMETERS The parameters , a1 and p need both initialization and update. We have different initializations and update schemes for each of them.
6

Under review as a conference paper at ICLR 2018

For all the standard deviations, once we obtain  and z, we update them by taking the empirical
standard deviation correspondingly. For  and , the calculation is straightforward as they only involve  and  respectively. For  , o, u and r, it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:

=

{t:zta =0}

(yt - µt |{t : zta

- =

t )2 0}|

,

o

=

{t:zta =1}

(yt - µt |{t : zta

- =

t )2 1}|

,

u

=

{t:ztc =0}

(µt

- µt-1 |{t : ztc

- t-1)2 = 0}|

,

(7)

r =

{t:ztc =1}

(µt

- µt-1 |{t : ztc

- =

t-1 )2 0}|

,



=

1n

n

(t
t=1

- t-1)2, 

=

1 n S-1

n

(
t=1 s=0

t-s )2 .

(8)

Note that in some iterations, when there is no change point or anomaly detected in z, then the
updates above for o, r are not well-defined. In those cases, we simply let them remain the same. To initialize , we let them all equal to the standard deviation of y.

For a1, we initialize it by letting its first coordinate to be equal to the average of y1, y2, . . . , yS, and all the remaining coordinates to be equal to 0. Since a1 can be interpreted as the mean vector of 1, in this way the trend is initialized to be matched up with average of the first season, and the slope
and seasonality are initialized to be equal to 0. We update a1 by using information of . We let the first two coordinates (trend and slope) of a1 to be equal to those of 1, and we let the remaining coordinates (seasonality) of a1 to be equal to those of S+1. The reason why we do not let a1 to be equal to 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a1 as 0, it will remain 0 if we let a1 equals 1 entirely (due to the mechanism how we update 1 as described in Section 4.1. We can avoid such trouble via using S+1.

For p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that za or zc may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then pa or pc might be 0, and it may get stuck in 0 in the remaining iterations.

5 FORECASTING
Once we infer all the latent variables , z and tune all the parameters p, a1, , we are able to forecast the future time series yfuture. From the graphical model described in Section 3, the future forecasting only involves n instead of the whole . Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given n and  we can use our predictive procedures (i.e., Algorithm 1) to generate future time series yfuture. We can further integral out n to have the posterior predictive distribution as p(yfuture|y).
The forecasting on future time series is not deterministic. There are two sources for the randomness in yfuture. One comes from the inference of n (and also ) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t, ut, vt and wt. Thus, the predictive density function p(yfuture|y, n) will lead to different path even with fixed  and n. In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.
The average of multiple forecasting paths (denoted as y¯future), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend µn, slope n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity of y¯future, in both mean and standard deviation.
Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {yn(1+)j }mj=1, {yn(2+)j }mj=1, . . . , {yn(N+)j }mj=1

7

Under review as a conference paper at ICLR 2018

to be the future paths. Define y¯future = (y¯n+1, y¯n+2, . . . , y¯n+m) to be the average such that

1 y¯n+j = N

N

yn(i+) j .

i=1

Then for all j = 1, 2, . . . , N , we have y¯n+j as a normal distribution with mean and variance as

E[y¯n+j ] = µn + jn + n-S+(j mod S)

1 Var [y¯n+j] = N

j(j + 1)v2/2 + j(u2 + w2 ) + 2

.

Consequently, for all j = 1, 2, . . . , m, E[y¯n+j] is in a linear form with respect to j, and the standard deviation of y¯n+j also takes a approximately linear form with respect to j.

Proof. Recall that n,  are given and fixed, and we assume there is no change point or anomaly

in the future time series. The Equation (2) leads to n+j = n +

j l=1

vn+l,

which

implies

that

jj
µn+j = µn + jn + (j + 1 - l)vn+l + um+l.
l=1 l=1

For the seasonality part, simple linear algebra together with Equation 3 leads to n+j =

n-S+(j mod S) +

j l=1

wn+l.

Thus,

1N y¯n+j = N

j jj

µn + jn + n-S+(j mod S) + (j + 1 - l)vn(i+) l +

u(mi)+l +

wn(i+) l +

(i) n+j

.

i=1 l=1 l=1 l=1

Due to the independence and Gaussian distribution of all the noises, y¯n+j is also normally distributed and its means and variance can be calculated accordingly.

6 ALGORITHM
Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3.
It is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting pc = 0, we assume there is no change point in the time series. By letting pa = 0, we assume there is no anomaly point in the time series. If both pc and pa are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.

7 SIMULATION
In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for  and a1. The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.
When generating, we let the time series have weekly seasonality with S = 7. For  we have  = 0.1, u = 0.1, v = 0.0004, w = 0.01, r = 1, o = 4. For 1 we have value for µ as 20, value for  as 0, and value for seasonality as (1, 2, 4, -1, -3, -2)/10. For p we have pc = 4/350 and pa = 10/350. Despite that, to make sure that at least one change point is in existence, we force z3c30 = 1 and r330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force zic = zia = 0, 351  i  500 so there exists no change point or anomaly point in the testing part.

8

Under review as a conference paper at ICLR 2018
Algorithm 3: Proposed Algorithm Input: Observed time series y = (y1, y2, . . . , yn), seasonality length S, length of time series for
forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection zc, anomaly points za, forecasting result
yfuture = (yn+1, yn+1, . . . , yn+m) and its distribution or predictive intervals
Part I: Initialization; 1 Initialize  , o, u, r, v, w all with the empirical standard deviation of y; 2 Initialize a1 such that its first coordinate equals to the average of (y1, y2, . . . , yS) and all the
remaining S coordinates with 0; 3 Initialize pa and pc by 1/n. Then generate za and zc as independent Bernoulli random variables
with success probability pa and pc respectively;
Part II: Inference; while the likelihood function La1,p,(y, , z) not converges do 4 Infer  by Kalman filter, Kalman smoothing and "fake-path" trick described in Section 4.1; 5 Update za and zc by sampling from
{zta}nt=1  Ber(pta), {ztc}nt=1  Ber(pct ), where the success probability {pta}tn=1 and {pct }nt=1 are defined in Equation (5) and (6); 6 Segment control on zc by Algorithm 2; 7 Update  by Equation (7) to (8); 8 Update a1 such that its first two coordinates equal to the those of 1 and the remaining (S - 1) coordinates equals to those of S+1; 9 Calculate the likelihood function La1,p,(y, , z) given in Equation (4); end
Part III: Forecasting; 10 With an and , use the generate procedure in Algorithm 1 to generate future time series yfuture
with length m. Repeat the generative procedure to obtain multiple future paths yf(u1tu)re, yf(u2tu)re, . . . , yf(uNtur)e; 11 Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.
The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting.
0 100 200 300 400 500 0 100 200 300 400 500
Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
9

14 18 22 14 18 22

Under review as a conference paper at ICLR 2018

We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies. For time series forecasting, we compare our method against Bayesian Structural Time Series (BSTS) (Scott & Varian, 2014; Brodersen et al., 2015)), Seasonal Decomposition of Time Series by Loess (STL) (Cleveland et al., 1990)), Seasonal ARIMA (Box et al., 2015), Holt­Winters (Holt, 2004), Exponential Smoothing State Space Model (ETS) (Hyndman et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x1, x2, . . . , xn be the true value and x^1, x^2, . . . , x^n be the estimation or predictive values. Then we have

1 MAPE =

n

|xi - x^i| , MSE =

n
i=1

xi

1 n

n

(xi

- x^i)2, MAE

=

1 n

n

|xi - x^i|.

i=1

i=1

The comparison of our proposed algorithm and the aforementioned algorithms are included below
in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting pc = 0 or pa = 0. We also run proposed algorithm on the synthetic data with pc = 0 (no change point), or pa = 0 (no anomaly point), or pc = pa = 0 (no change and anomaly point), for the purpose of numeric comparison.

Table 2: Comparison of methodologies on Forecasting

Methods Proposed Proposed (pa = 0) Proposed (pc = 0) Proposed (pa = 0, pc = 0) BSTS STL ARIMA Holt­Winters ETS Prophet

MAPE
0.041 ± 0.027 0.069 ± 0.068 0.065 ± 0.058 0.084 ± 0.079
0.162 ± 0.110 0.047 ± 0.039 0.076 ± 0.050 0.093 ± 0.082 0.054 ± 0.042 0.082 ± 0.055

MSE
1.03 ± 0.59 1.71 ± 1.61 1.67 ± 1.53 2.15 ± 2.00
4.10 ± 2.81 1.18 ± 1.06 1.88 ± 1.38 2.35 ± 2.06 1.37 ± 1.05 2.06 ± 1.33

MAE
0.89 ± 0.53 1.49 ± 1.44 1.43 ± 1.35 1.87 ± 1.77
3.59 ± 2.48 1.03 ± 0.95 1.71 ± 1.24 2.05 ± 1.84 1.19 ± 0.94 1.78 ± 1.16

From Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.

We also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z1, z2, . . . , zn) be the true binary vector for change points or anomalies, and (z^1, z^2, . . . , z^n) are the estimated ones. Then

TPR

=

|{i

: zi |{i

= 1, z^i = : zi = 1}|

1}| ,

FP = |{i : zi = 0, z^i = 1}|.

From the definition, we can see high TPR and low FP means the algorithm has better performance in detection.

The comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.

In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017),

10

Under review as a conference paper at ICLR 2018

Table 3: Comparison of Change Point Detection

Mehtods Proposed Proposed (pa = 0) BCP CP Breakout

TPR 0.41 ± 0.26 0.14 ± 0.21 0.58 ± 0.22 0.29 ± 0.22 0.01 ± 0.04

FP 0.34 ± 0.57 0.26 ± 0.60 29.84 ± 8.13 1.71 ± 1.15 0.53 ± 0.86

Table 4: Comparison of Anomaly Detection

Mehtods Proposed Proposed (pc = 0) AnomalyDetection RAD tsoutlier

TPR 0.88 ± 0.12 0.87 ± 0.12 0.32 ± 0.19 0.88 ± 0.11 0.81 ± 0.14

FP 0.58 ± 0.96 2.56 ± 1.49 1.03 ± 1.94 19.33 ± 3.58 4.76 ± 4.29

RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.
8 REAL DATA ANALYSIS
In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.
8.1 WELL-LOG DATA
This dataset (Fearnhead & Clifford, 2003; JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 1. It has 4050 points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance.

120000

120000

80000

80000

0

1000

2000

3000

4000

0

1000

2000

3000

4000

Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).

From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting pa = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other.
In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.

1http://hips.seas.harvard.edu/files/well-log-2.dat

11

Under review as a conference paper at ICLR 2018

Table 5: Comparison of Forecasting in Well-log Data

Methods Proposed Proposed (pa = 0) Proposed (pc = 0) Proposed (pa = 0, pc = 0) BSTS ARIMA ETS Prophet

MAPE 0.031 0.029 0.033 0.038 0.250 0.084 0.037 0.159

MSE 5296 5252 5434 5703 32030 10480 6071 19530

MAE 3120 2957 3409 3908 27210 8738 3860 17480

8.2 INTERNET TRAFFIC DATA
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5). It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm.

0.4 0.8 1.2 1.6 0.4 0.8 1.2 1.6

0 200 400 600 800 1000

0 200 400 600 800 1000

Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).

We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.

Table 6: Comparison of Forecasting in Internet traffic data

Methods Proposed Proposed (pa = 0) Proposed (pc = 0) Proposed (pa = 0, pc = 0) BSTS STL ARIMA Holt­Winters ETS Prophet

MAPE 0.0837 0.0838 0.0934 0.0934 0.2756 0.1014 0.1409 0.2495 0.0893 0.1015

MSE 0.1216 0.1215 0.1332 0.1366 0.3087 0.1258 0.1653 0.2739 0.1199 0.1405

MAE 0.08414 0.08320 0.09296 0.09223 0.27960 0.09910 0.12580 0.25270 0.09362 0.11450

From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.

12

Under review as a conference paper at ICLR 2018
9 CONCLUSION
We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.
REFERENCES
Daniel Barry and John A Hartigan. A bayesian analysis for change point problems. Journal of the American Statistical Association, 88(421):309­319, 1993.
George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015.
Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L Scott, et al. Inferring causal impact using bayesian structural time-series models. The Annals of Applied Statistics, 9 (1):247­274, 2015.
Chung Chen and Lon-Mu Liu. Joint estimation of model parameters and outlier effects in time series. Journal of the American Statistical Association, 88(421):284­297, 1993.
Robert B Cleveland, William S Cleveland, and Irma Terpenning. Stl: A seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6(1):3, 1990.
John H Cochrane. Time series for macroeconomics and finance. 2005.
James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38. OUP Oxford, 2012.
Paul Fearnhead and Peter Clifford. On-line inference for hidden markov models via particle filters. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(4):887­899, 2003.
Keith W Hipel and A Ian McLeod. Time series modelling of water resources and environmental systems, volume 45. Elsevier, 1994.
Charles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages. International journal of forecasting, 20(1):5­10, 2004.
Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential smoothing: the state space approach. Springer Science & Business Media, 2008.
OR JK and F WJ. Numerical bayesian methods applied to signal processing, 1996.
Rebecca Killick and Idris Eckley. changepoint: An r package for changepoint analysis. Journal of Statistical Software, 58(3):1­19, 2014.
Netflix. Rad: Time series anomaly detection. 2017.
Steven L Scott and Hal R Varian. Predicting the present with bayesian structural time series. International Journal of Mathematical Modelling and Numerical Optimisation, 5(1-2):4­23, 2014.
S. J. Taylor and Letham. Prophet: forecasting at scale. 2017.
Twitter. Anomalydetection: Anomaly detection with r. 2017.
twitter. Breakout detection via robust e-statistics. 2017.
Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management science, 6(3):324­342, 1960.
G Peter Zhang. Time series forecasting using a hybrid arima and neural network model. Neurocomputing, 50:159­175, 2003.
13

