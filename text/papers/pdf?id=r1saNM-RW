Under review as a conference paper at ICLR 2018
SMALL CORESETS TO REPRESENT LARGE TRAINING DATA FOR SUPPORT VECTOR MACHINES
Anonymous authors Paper under double-blind review
ABSTRACT
Despite their popularity, even efficient implementations of Support Vector Machines (SVMs) have proven to be computationally expensive to train at a largescale, especially in streaming settings. In this paper, we propose a coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training. A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set. We provide lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data. Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem. We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.
1 INTRODUCTION
Popular machine learning algorithms are computationally expensive, or worse yet, intractable to train on Big Data. The notion of using coresets (Feldman & Langberg, 2011; Braverman et al., 2016; Bachem et al., 2017), small weighted subsets of the input points that provably approximate the original data set, has shown promise in accelerating machine learning algorithms, such as kmeans clustering (Feldman & Langberg, 2011), mixture model training (Feldman et al., 2011; Lucic et al., 2017), and logistic regression (Huggins et al., 2016).
Coresets provide a compact representation of the structure of static and streaming data, with provable approximation guarantees with respect to specific algorithms. For instance, a data set consisting of K clusters would yield a coreset of size K, with each cluster represented by one coreset point. Even if the data has no structure (e.g., uniformly distributed), coresets will correctly down sample the data to within prescribed error bounds. For domains where the data has structure, the coreset representation has the potential to greatly and effectively reduce the time required to manually label data for training and the computation time for training, while at the same time providing a mechanism of supporting machine learning systems for applications with streaming data.
Coresets are constructed by approximating the relative importance of each data point in the original data set to define a sampling distribution and sampling sufficiently many points in accordance with this distribution. This construction scheme suggests that beyond providing a means of conducting provably fast and accurate inference, coresets also serve as efficient representations of the full data set and may be used to automate laborious representation tasks, such as automatically generating semantic video representations or detecting outliers in data (Lucic et al., 2016).
The representative power and provable guarantees provided by coresets also motivate their use in training of one of the most popular algorithms for classification and regression analysis: Support Vector Machines (SVMs). Despite their popularity, SVMs are computationally expensive to train on massive data sets, which has proven to be computationally problematic with the rising availability of Big Data. In this paper, we present a novel coreset construction algorithm for efficient, large-scale Support Vector Machine training.
In particular, this paper contributes the following:
1

Under review as a conference paper at ICLR 2018
1. A practical coreset construction algorithm for accelerating SVM training based on an efficient importance evaluation scheme for approximating importance of each point.
2. An analysis proving lower bounds on the number of samples required by any coreset construction algorithm to approximately represent the data.
3. An analysis proving the efficiency and theoretical guarantees of our algorithm and characterizing the family of data sets for which applications of coresets are most suited.
4. Evaluations against synthetic and real-world data sets that demonstrate the practical effectiveness of our algorithm for large-scale SVM training.
2 RELATED WORK
Training a canonical Support Vector Machine (SVM) requires O(n3) time and O(n2) space where n is the number of training points (Tsang et al., 2005). Work by Tsang et al. (2005) introduced Core Vector Machines (CVMs) that reformulated the SVM problem as the Minimum Enclosing Ball (MEB) problem and used existing coreset methods for MEB to compress the data. The authors proposed a method that generates a (1 + )2 approximation to the two-class L2-SVM in O(n/2 + 1/4) time, when certain assumptions about the kernel used are satisfied. However, CVM's accuracy and convergence properties have been noted to be at times inferior to the performance of existing SVM implementations (Loosli & Canu, 2007). Similar geometric approaches including extensions to the MEB formulation, those based on convex hulls, and extreme points, among others, were also investigated by Rai et al. (2009); Agarwal & Sharathkumar (2010); Har-Peled et al. (2007); Nandan et al. (2014).
Since the SVM problem is inherently a quadratic optimization problem, prior work has investigated approximations to the quadratic programming problem using the Frank-Wolfe algorithm or Gilbert's algorithm (Clarkson, 2010). Another line of research has been in reducing the problem of polytope distance to solve the SVM problem (Ga¨rtner & Jaggi, 2009). The authors establish lower and upper bounds for the polytope distance problem and use Gilbert's algorithm to train an SVM in linear time.
A variety of prior approaches were based on randomized algorithms with the property that they generated accurate approximations with high probability. Most notable are the works of Clarkson et al. (2012) Hazan et al. (2011). Hazan et al. (2011) used a primal-dual approach combined with Stochastic Gradient Descent (SGD) in order to train linear SVMs in sub-linear time. They proposed the SVM-SIMBA approach and proved that it generates an -approximate solution with probability at least 1/2 to the SVM problem that uses hinge loss as the objective function. The key idea in their method is to access single features of the training vectors rather than the entire vectors themselves. Their method is nondeterministic and returns the correct -approximation with probability greater than a constant probability, similar to the probabilistic guarantees of coresets.
Clarkson et al. (2012) present sub-linear-time (in the size of the input) approximation algorithms for some optimization problems such as training linear classifiers (e.g., perceptron) and finding MEB. They introduce a technique that is originally applied to the perceptron algorithm, but extend it to the related problems of MEB and SVM in the hard margin or L2-SVM formulations. Pegasos ShalevShwartz et al. (2011) employs primal estimated Subgradient Descent independent of the input size. These works offer similar probabilistic guarantees as do coresets and have been noted to perform well empirically; however, unlike coresets, SGD-based approaches cannot be trivially extended to streaming settings.
Joachims presents an alternative approach to training SVMs in linear time based on the cutting plane method that hinges on an alternative formulation of the SVM optimization problem Joachims (2006). He shows that the Cutting-Plane algorithm can be leveraged to train SVMs in O(sn) time for classification and O(sn log n) time for ordinal regression where s is the average number of nonzero features. However, this approach does not trivially extend to SVMs with kernels and is not sublinear with respect to the number of points n.
Har-Peled et al. (2007) constructed coresets to approximate the maximum margin separation, i.e., a hyperplane that separates all of the input data with margin larger than (1 - ), where  is the best achievable margin (Har-Peled et al., 2007). They studied the running time of a simple-to-implement coreset algorithm for binary and structured-output classification, and noise-tolerant learning in the agnostic setting.
2

Under review as a conference paper at ICLR 2018

3 PROBLEM DEFINITION

3.1 SOFT-MARGIN SVM

We assume that we are given a set of weighted training points P = {(xi, yi)}in=1 with the corresponding weight function u : P  R0, such that for every i  [n], xi  Rd, yi  {-1, 1}, and u(pi) corresponds to the point's weight. For simplicity, we assume that the bias term is embedded into the feature space by defining x~i = (xi, 1) for each point and w~ = (w, 1) for each query. Thus, we henceforth assume that we are dealing with d + 1 dimensional points and refer to w~ and x~i as just w and xi respectively. Under this setting, the hinge loss of a point pi = (xi, yi) with respect to the separating hyperplane w is defined as h(pi, w) = [1 - yi xi, w ]+, where [x]+ = max{0, x}.
For any subset of points, P  P, define H(P , w) = pP u(p)h(p, w) as the sum of the hinge losses and u(P ) = pP u(p) as the sum of the weights of points in set P . To clearly depict the contribution of each point to the objective value of the SVM problem, we present the SVM objective function, f (P, w), as the sum of per-point objective function evaluations, which we formally define below.
Definition 1 (Per-point Objective Function). Define the function f : (Rd+1 × {-1, 1}) × Rd+1  R0 such that for every i  [n]

f (pi, w) =

w1:d

2 2

2u(P )

+

Ch(pi, w),

w1:d denotes the entries 1 : d of w, h(pi, w) is the corresponding hinge loss and C  [0, 1] is the regularization parameter.

Definition 2 (Soft-margin SVM Problem). Given a set of d + 1 dimensional weighted points P with weight function u : P  R0 the primal of the SVM problem is expressed by the following quadratic program

min f ((P, u), w),
wQ

(1)

where f is the evaluation of the weighted point set P with weight function u : P  R0,

f ((P, u), w) =

u(pi)f (pi, w) =

w1:d

2 2

+ CH(P, w).

2

i[n]

(2)

When the set of points P and the corresponding weight function u are clear from context, we will henceforth denote f ((P, u), w) by f (P, w) for notational convenience.

3.2 CORESETS

Coresets can be seen as a compact representation of the full data set that approximate the SVM cost function (2) uniformly over all queries w  Q. Thus, rather than introducing an entirely new algorithm for solving the SVM problem, our approach is to reduce the runtime of standard SVM algorithms by compressing the size of the input points from n to a compact set of size polylogarithmic in n.
Definition 3 ((, )-coreset). Let   (0, 1/2) and P  Rd+1 be a set of n weighted points with weight function u : P  R0. The weighted subset S  P with corresponding weight function v : S  R0 is an (, )-coreset if for any query w  Q, the pair (S, v) satisfies the coresetproperty

|f ((P, u), w) - f ((S, v), w)|  f ((P, u), w) (Coreset Property),

(3)

with probability at least 1 - .

Our overarching goal is to efficiently construct an -coreset S  P such that the cardinality of S is sufficiently small, i.e., sublinear in the number of data points, n.

3

Under review as a conference paper at ICLR 2018

4 METHOD

4.1 METHOD OVERVIEW
Our coreset construction scheme is based on the unified framework of Feldman & Langberg (2011); Braverman et al. (2016) and is shown as Alg 1. The crux of our algorithm lies in generating the importance sampling distribution via efficiently computable upper bounds (proved in Sec. 5) on the importance of each point (Lines 1-6). Sufficiently many points are then sampled from this distribution and each point is given a weight that is inversely proportional to its sample probability (Lines 7-9). The number of points required for an (, )-coreset is a function of the desired accuracy , failure probability , and complexity of the data set (t from Theorem 9). Under mild assumptions on the problem at hand (see Sec. 5.2), the required sample size is polylogarithmic in n.
Intuitively, our algorithm can be seen as an importance sampling procedure that first generates a judicious sampling distribution based on the structure of the input points and samples sufficiently many points from the original data set. The resulting weighted set of points, (S, v), serves as an unbiased estimator for f (P, w) for any query w  Q, i.e., E[f ((S, v), w)] = f (P, w). Although sampling points uniformly with appropriate weights can also generate such an unbiased estimator, it turns out that the variance of this estimation is minimized if the points are sampled according to the distribution defined by the ratio between each point's sensitivity and the sum of sensitivities, i.e., (pi)/t on Line 9 Bachem et al. (2017).

4.2 CHICKEN AND THE EGG PHENOMENA

Coresets are intended to provide efficient and provable approximations to the optimal SVM solu-

tion, however, the very first line of our algorithm entails computing the optimal solution to the SVM

problem. This seemingly eerie phenomenon is explained by the merge-and-reduce technique that

ensures that our coreset algorithm is only run against small partitions of the original data set Braver-

man et al. (2016). The merge-and-reduce approach leverages the fact that coresets are composable

and reduces the coreset construction problem for a (large) set of n points into the problem of com-

puting

coresets

for

n |S|

,

where

2|S|

is

the

minimum

size

of

input

set

that

can

be

reduced

to

half

using

Alg 1 Braverman et al. (2016). Assuming that the sufficient conditions for obtaining polylogarithmic

size coresets implied by Theorem 9 hold, the overall time required for coreset construction is nearly

linear in n, O,(n).

4.3 EXTENSIONS
We briefly remark on a straightforward extension that can be made to our algorithm to accelerate performance and applicability. First, we remark that the computation of the optimal solution to the SVM problem in line 1 can be replaced by an efficient gradient-based method, such as Pegasos ShalevShwartz et al. (2011), to compute an approximately ~ optimal solution in O~ (dnC/~), which is particularly suited to scenarios with C small. Without too much effort, the proof of Lemma 7 can be extended to obtain a similar expression for the sensitivity bound, as a function of  , when a sufficiently close approximation to the optimal solution is available.

5 ANALYSIS

In this section, we prove upper and lower bounds on the sensitivity of a point in terms of the complexity of the given data set. Our main result is Theorem 9, which establishes sufficient conditions for the existence of small coresets depending on the properties of the data. Our theoretical results also highlight the influence of the regularization parameter, C, in the size of the coreset.

Definition 4 (Sensitivity Braverman et al. (2016)). The sensitivity of an arbitrary point p  P, p = (x, y) is defined as

s(p) = sup
wQ

u(p)f (p, w) ,
qP u(q)f (q, w)

(4)

where u : P  R0 is the weight function as before.

4

Under review as a conference paper at ICLR 2018

Algorithm 1: CORESET(P, u, , )

Input: Output:

A set of training points P  Rd containing n points, a weight function u : P  R0, an error parameter   (0, 1), and failure probability   (0, 1).
An -coreset (S, v) for the query space F with probability at least 1 - .

// Compute the optimal solution using an Interior Point Method. 1 w  InteriorPointMethod(P, C) 2 Kyi  u(Pyci )/(2u(P) · u(Pyi ) · maxj[n] xj 2) for each yi  {-1, 1};
// Compute an upper bound for the sensitivity of each point

according to Eqn.(5).

3 for i  [n] do

4

p



(p¯yi

- pi)

where

p¯yi

=

1 u(Pyi )

qPyi u(q) q

5

(pi)



u(pi ) u(Pyi )

+

C u(pi ) 2f (P,w)

w, p

-

Kyi C

2
+ 2f (P, w)

p

2 2

+

w, p

-

Kyi C

6 t  i[n] (pi) 7 Let

m

t2 2 (d + log(1/))

8 (K1, . . . , Kn)  Multinomial (m, i = (pi)/t i  [n])

9 S  {pi  P : Ki > 0}

// Compute the weights v : S  R0 for every point pi  S.

10 for i  [n] do

11

v(pi)



tKi u(pi )  (pi )|S |

12 return (S, v)

5.1 SENSITIVITY LOWER BOUND

We first prove the existence of a hard point set for which the sum of sensitivities is approximately (nC), ignoring d factors, which suggests that if the regularization parameter is too large, then the required number of samples for property (3) to hold is (n).

Lemma 5. There exists a set of n points P  Rd+1 such that the sensitivity of each point pi is

bounded

below

by

(

d2 +nC n(C +d2 )

)

and

the

sum

of

sensitivities

is

bounded

below

by



d2 +nC C +d2

.

The same hard point set from Lemma 5 can be used to also prove a bound that is nearly exponential in the dimension, d.

Corollary 6.

There exists a set of n points P




Rd+1

such that the total sensitivity is bounded

below by 

d2+C(2d/ d)
d2 +C

.

We next prove upper bounds on the sensitivity of each data point with respect to the complexity of the input data. Despite the non-existence results established above, our upper bounds shed light into the class of data sets for which coresets of sufficiently small size exist, and thus have potential to significantly speed up SVM training.

5.2 SENSITIVITY UPPER BOUND

For Pyci

any arbitrary = P \ Pyi be

pitosinctomp p=lem(xeni,t,yai)ndlePt w, ledtePnoyite

 P denote the set the optimal solution

of to

points of label yi, let the SVM problem (2).

Also, let R = maxi[n] x 2 denote the maximum 2 norm of any point.

5

Under review as a conference paper at ICLR 2018

Lemma 7. The sensitivity of any point pi  P is bounded above by



s(pi)



u(pi) u(Pyi )

+

C u(pi ) 2f (P, w)





w, p

- Kyi C

2
+ 2f (P, w)

p

2 2

+

w, p

- Kyi  C

= (pi), where p = p¯yi - pi and Kyi = u(Pyci )/(2u(P) · u(Pyi ) · maxj[n] xj 2).

(5)

Let P+ = P1  P and P- = P \ P1 denote the set of points with positive and negative labels respectively. Let p¯+ and p¯- denote the weighted mean of the positive and labeled points respectively, and for any pi  P+ let p+i = p¯+ - pi and p-i = p¯- - pi. Lemma 8. The sum of sensitivities over all points P is bounded by

S(P)  2 + C (Var(P+) + Var(P-)) = t, f (P, w)

(6)

where f (P, w) is the optimal value of the SVM problem, and Var(P+) and Var(P-) denote the total deviation of positive and negative labeled points from their corresponding label-specific mean

Var(P+) =

u(pi)

p+i

=
2

u(pi) p¯+ - pi 2

pi P+

pi P1

Var (P-) =

u(pi)

p-i

=
2

u(pi) p¯- - pi 2 .

pi P-

pi P-

Theorem 9. Given any   (0, 1/2),   (0, 1) and a weighted data set P with corresponding

weight function u, with probability greater than 1 - , Algorithm 1 generates an -coreset, i.e., a

weighted set (S, v), of size

|S|  

t2 2 d + log(1/)

,

in O(n3d) time, where d = O(d) corresponds to the dimension of the query space (P, u, Q, f ) and t is the upper bound on the sum of sensitivities from Lemma 8,

t = 2 + C (Var(P+) + Var(P-)) . f (P, w)

For any subset T  P, let wT denote the optimal separating hyperplane with respect to the set of points in T . The following corollary immediately follows from Theorem 9 and implies that SVM training on an (, )-coreset, (S, v), to obtain wS yields an approximate solution to the objective value of the optimal solution with respect to all data points P.
Corollary 10. Given any   (0, 1/2),   (0, 1) and a weighted data set (P, u), the weighted set of points (S, v) generated by Alg. 1 satisfies
f ((P, u), wP )  f ((S, v), wS )  (1 + 4)f ((P, u), wP ), with probability greater than 1 - .

Proof. Since (S, v) is an (, )-coreset for (P, u) with probability at least 1 - , we have

f ((P, u), wP )



f ((P, u), wS )



f ((S, v), wS ) 1-



(1

+

)f ((P, u), wP ) 1-

 (1 + 4)f ((P, u), wP ).

Sufficient Conditions Theorem 9 immediately implies that, for reasonable  and , coresets of polylogarithmic (in n) size can be obtained if d = O(polylog(n)), which is usually the case in our
target applications, and if

C (Var(P+) + Var(P-)) = O(polylog(n)). f (P, w)

For example, a value of C



log n n

for the regularization parameter C

satisfies the sufficient condition

for all data sets with normalized points.

6

Under review as a conference paper at ICLR 2018

Interpretation of Bounds Our approximation of the sensitivity of a point pi  P, i.e., its relative importance, is a function of the following highly intuitive variables.
1. Relative weight with respect to the weights of points of the same label (u(pi)/u(Pyi )): the sensitivity increases as this ratio increases.
2. Distance to the label-specific mean point p¯yi - pi 2 : points that are considered outliers with respect to the label-specific cluster are assigned higher importance.
3. Distance to the optimal hyperplane ( w, p ): importance increases as distance of the difference vector p = p¯yi - pi to the optimal hyperplane increases.
Note that the sum of sensitivities, which dictate how many samples are necessary to obtain an (, )coreset and in a sense measure the difficulty of the problem, increases monotonically with the sum of distances of the points from their label-specific means.

6 RESULTS

6.1 EXPERIMENTAL SETUP
We evaluate the performance of our coreset construction algorithm against synthetic and real-world, publicly available data sets Lichman (2013). We compare the effectiveness of our method to uniform subsampling on a wide variety of data sets and also to Pegasos, one of the most popular StochasticGradient Descent based algorithm for SVM training (Shalev-Shwartz et al., 2011). For each data set of size N , we selected a set of M = 10 subsample sizes S1, . . . , SM  [N ] and ran each coreset construction algorithm to construct and evaluate the accuracy subsamples sizes S1, . . . , SM. The results were averaged across 100 trials for each subsample size. Our results of relative error and sampling variance are shown as Figures 1 and 2. The computation time required for each sample size and approach can be found in the Appendix (Fig. 3).
Our experiments were implemented in Python and performed on a 3.2GHz i7-6900K (8 cores total) machine with 16GB RAM. We considered the following data sets for evaluation.

Pathological (Synthetic) -- 1000 points in two dimensional space describing two clusters distant from each other of different labels, as well as two points of different labels which are close to each other. We remark that uniform sampling performs particularly poorly against this data set, due to the presence of outliers.

Synthetic -- The Synthetic and Synthetic100K are datasets containing of 6000, 10000, each consisting of 3, 4 dimensions respectively. The datasets describe two blocks of mirrored nested rings of points, each of different labels such that Gaussian noise has been added to them.

HTRU1 -- 17898 radio emissions of Pulsar (rare type of Neutron star) each consisting of 9 features.

CreditCard2 -- 30, 000 client entries each consisting of 24 features that include education, age, and gender among other factors.

Skin3 -- 245, 057 random samples of B,G,R from face images consisting of 4 dimensions.

Evaluation We computed the relative error of the sampling-based algorithms with respect to the

cost of the optimal solution to the subsample, f ((S, v), wS ).

the SVM We also

problem, f (P, wP ) and the approximate evaluated the variance of the estimators

cost generated by for the sampling-

based approaches and observed that the variances of the estimates generated by our coreset were

lower than those of uniform subsampling.

1https://archive.ics.uci.edu/ml/datasets/HTRU2 2https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients 3https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation/

7

Under review as a conference paper at ICLR 2018
Figure 1: The relative error of query evaluations with respect uniform and coreset subsamples for the 5 data sets. Bottom right) Comparison of the efficiency of our method and that of Pegasos.
Figure 2: The estimator variance of query evaluations. We note that due to the use of a judicious sampling distribution based on the points' sensitivities, the variance of our coreset estimator is lower than that of uniform sampling for all data sets.
7 CONCLUSION
We presented an efficient coreset construction algorithm for generating compact representations of the input data points that provide provably accurate inference. We presented both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of input data complexity and established sufficient conditions for the existence of compact representations. Our experimental results demonstrate the effectiveness of our approach in speeding up SVM training when compared to uniform sub-sampling The method presented in this paper is also applicable to streaming settings, using the merge-andreduce technique from coresets literature Braverman et al. (2016).We conjecture that our coreset construction method can be extended to significantly speed up SVM training for nonlinear kernels as well as other popular machine learning algorithms, such as deep learning.
8

Under review as a conference paper at ICLR 2018

8 APPENDIX
Definition 11 (Query Space). Let P be the set of input points, let Q denote the set of possible margins over which the SVM optimization is performed over, and let f be the SVM objective function given by Definition 2. Then, the tuple F = (P, Q, f ) is called the query space.

8.1 PROOF OF LEMMA 5

Proof. Following Yang et al. (2017) we define the set of n points P  Rd+1, each point p  P with weight u(p) = 1, such that for each i  [n], among the first d entries of pi, exactly d/2 entries are equivalent to :

R+1 = ,
d

where R is the normalization factor (typically R = 1), the remaining d/2 entries among the first d are set to 0, and pi(d+1) = yi as before. For each i  [n], define the set of non-zero entries of pi as the set
Bi = {j  [d + 1] : pij = 0}.

Now, for bounding the sensitivity of point pi, consider the normal to the margin wi with entries defined as

j  [d + 1] wij =

0 if j  Bi, 1/ otherwise.

Note that for R = 1, ||wi||2 = (d/2)(1/2) = (d/2)d/(R + 1) = d2/4. We also have that
h(wi, pi) = 1 since pi · wi = lBi pijwij = (d/2)(0) = 0. To bound the sum of hinge losses contributed by other points j  [n], j = i note that Bi \ Bj = , thus:

wi, pj =

wilpjl  (1/) = 1,

lBi \Bj

which implies that h(wi, pj) = 0. Thus, it follows that H(wi) = j[n] h(wi, pj) = 1. Putting it all together, we have for the sensitivity of any arbitrary i  [n]:

s(pi) = sup
w

f (pi, w) j[n] f (w, pj )



d2 8n

+

C

h(wi, pi)

||wi ||2 2

+C

=

d2 8n d2 8

+ +

C C

.

Moreover, we have for the sum of sensitivities that

s(pi) 
i[n]

d2 8

+ nC

d2 8

+C

=

d2 + nC d2 + C

8.2 PROOF OF COROLLARY 6

Proof. Consider the set of points P from the proof of Lemma 5 and note that

n=

d

 2d(1 - 1/d)  2d

 = (2d/ d).

d/2

(d/2)

d

8.3 PROOF OF LEMMA 7 Proof. Consider any arbitrary point pi  P and let p = xiyi for brevity when the point pi = (xi, yi) is clear from the context. We proceed to bound s(pi)/u(pi) by first leveraging the Lipschitz
9

Under review as a conference paper at ICLR 2018

continuity of hinge loss and convexity of f

s(pi) u(pi)

=

sup
wQ(P )

f (pi, w) f (P, w)

 sup f (p¯yi , w) + C [ w, p¯yi - p ]+

wQ

f (P, w)

Lipschitz Continuity

 sup
wQ

qPyi u(q)f (q, w) + C [ w, p¯yi - p ]+

u(Pyi )f (P, w)

f (P, w)

= sup f (Pyi , w) + f (Pyci , w) - f (Pyci , w) + C [ w, p¯yi - p ]+

wQ

u(Pyi )f (P, w)

f (P, w)

= 1 + sup C [ w, p¯yi - p ]+ - f (Pyci , w)

u(Pyi ) wQ

f (P, w)

u(Pyi )f (P, w)

 1 + sup C [ w, p¯yi - p ]+ - u(Pyci )/(2n · u(Pyi )) ,

u(Pyi ) wQ

f (P, w)

Jensen's Inequality (7)

where the last inequality follows from the fact that for any w  Q, w 2  1/R and thus,

f (Pyci , w) 2u(Pyi )



w

2 2

u(Pyci

)

2u(P)u(Pyi )



u(Pyci )/(2u(P)

·

u(Pyi )

·

max
j[n]

xj

2).

Now consider the expression involving the supremum from (7) and let

Kyi

=

u(Pyci )/(2u(P)

·

u(Pyi )

·

max
j[n]

xj

2)

(8)

be the constant independent of w in the numerator, let p = p¯yi - p, and let w = w - w for notational convenience. We observe that by -strong convexity of the SVM objective function for  = 1, we have the inequality f (P, w) +  w - w 2 /2  f (P, w) for any w  Q. Thus, the
expression containing the supremum from (7) is bounded by

sup C [ w, p¯yi - p ]+ - Kyi = sup C [ w - w, p¯yi - p + w, p¯yi - p ]+ - Kyi

wQ

f (P, w)

wQ

f (P, w)



sup
w Q

C

[

w, p + w, p f (P, w) +  w

]+ -

2 2

/2

Kyi



sup
w Q

[

w, p c w

+ a]+ +

2 2

+

d

b ,

(9)

where a = w, p , b = -Kyi /C, c = 1/(2C), and d = f (P, w)/C are constants independent

of w. Analytical optimization based on the gradient of the term above yields the optimal solution,

w

=

p 2cz

,

where

z=

w , p + a + b

c

w

2 2

+

d

(10)

We resolve the interdependency on w by observing that

w , p =

p 2cz

,

p

=

p

2 2

2cz

and

w

2 2

=

w , w

=

p (2cz

2 2
)2

,

which implies that

w

2 2

=

w , p /(2cz). Putting it all together, we obtain

z

(a + b)2 +

d

p c

2 2

+a+b

2d

C = 2f (P, w)

( w, p

- Kyi /C)2 + 2f (P, w)

p

2 2

+

w, p

- Kyi /C

,

+

and the sensitivity bound follows.

10

Under review as a conference paper at ICLR 2018

8.4 PROOF OF LEMMA 8

Proof. Let P+ = P1  P and P- = P \P1 denote the set of points with positive and negative labels respectively and let K+ and K- denote the corresponding constants defined by (8). Let p¯+ and p¯- denote the weighted mean of the positive and labeled points respectively, and for any pi  P+ let p+i = p¯+ - pi and p-i = p¯- - pi.
Since the sensitivity can be decomposed into sum over the two disjoint sets, i.e., S(P) = pP s(p) = pP1 s(p) + pP- s(p) = S(P1) + S(P-), we consider first bounding S(P1).
Invoking Lemma 7 yields

S(P1)  1 +

C u(pi ) 2f (P, w)

pi P+

w, p+i

- K+/C 2 + 2f (P, w)

p+i

2+
2

w, p+i

- K+/C

1+

C u(pi ) 2f (P, w)

pi P+

w, p+i

- K+/C 2 + 2f (P, w)

p+i

2 2

C

 1 + f (P, w)

u(pi)

pi P+

p+i 2

f (P, w)

=1+

C f (P, w) piP+ u(pi)

p+i

,
2

where the second equality follows by the definition of K+ and the fact that

u(pi) w, p+i =

u(pi) w, p¯+ -

u(pi) w, pi

pi P+

pi P+

pi P+

= w, p¯+

u(pi) -

u(pi) w, pi = 0,

pi P+

pi P+

and the third by noting that ( w, p+i

- K+/C)2  2f (P, w)

p+i

2 since by Cauchy-Schwarz
2

and definition of f (P, w) we have

w, p+i

- K+/C 

w 2

p+i


2

2f (P, w)

p+i

.
2

Using the same argument as above, an analogous bound holds for S(P-), thus we have

S(P)  2 +



C

f

(P ,

w)


pi P1

u(pi)

p+i

+
2

u(pi)

pi P-

p-i

2

= 2 + C (Var(P+) + Var(P-)) = t. f (P, w)

8.5 PROOF OF THEOREM 9

Proof. By Lemma 8 and Theorem 2.3 of Bachem et al. (2017) we have that the coreset constructed by our algorithm is an -coreset with probability at least 1 -  if

|S|  

t2 2 (d + log(1/))

,

where we used the fact that the VC dimension of a separating hyperplane in the case of a linear kernel is bounded dim(F)  d + 1 = O(d) Vapnik & Vapnik (1998). Moreover, note that the computation
time of our algorithm is dominated by computing the optimal solution of the SVM problem using interior-point Method which takes O(d3L) = O(n3d) time (Nesterov & Nemirovskii, 1994), where
L is the bit length of the input data.

11

Under review as a conference paper at ICLR 2018
Figure 3: Computation + Training Time vs. Sample Size
REFERENCES
Pankaj K Agarwal and R Sharathkumar. Streaming algorithms for extent problems in high dimensions. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 1481­1489. Society for Industrial and Applied Mathematics, 2010.
Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint arXiv:1703.06476, 2017.
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset constructions. arXiv preprint arXiv:1612.00889, 2016.
Kenneth L Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):63, 2010.
Kenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine learning. Journal of the ACM (JACM), 59(5):23, 2012.
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 569­578. ACM, 2011.
Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. In Advances in neural information processing systems, pp. 2142­2150, 2011.
Bernd Ga¨rtner and Martin Jaggi. Coresets for polytope distance. In Proceedings of the twenty-fifth annual symposium on Computational geometry, pp. 33­42. ACM, 2009.
Sariel Har-Peled, Dan Roth, and Dav Zimak. Maximum margin coresets for active and noise tolerant learning. In IJCAI, pp. 836­841, 2007.
Elad Hazan, Tomer Koren, and Nati Srebro. Beating sgd: Learning svms in sublinear time. In Advances in Neural Information Processing Systems, pp. 1233­1241, 2011.
Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic regression. arXiv preprint arXiv:1605.06423, 2016. 12

Under review as a conference paper at ICLR 2018
Thorsten Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 217­226. ACM, 2006.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ ml.
GaA~ G¸ lle Loosli and StA~ S phane Canu. Comments on the"core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research, 8(Feb):291­301, 2007.
Mario Lucic, Olivier Bachem, and Andreas Krause. Linear-time outlier detection via sensitivity. arXiv preprint arXiv:1605.00519, 2016.
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training mixture models at scale via coresets. arXiv preprint arXiv:1703.08110, 2017.
Manu Nandan, Pramod P Khargonekar, and Sachin S Talathi. Fast svm training using approximate extreme points. Journal of Machine Learning Research, 15(1):59­98, 2014.
Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM, 1994.
Piyush Rai, Hal Daume´ III, and Suresh Venkatasubramanian. Streamed learning: one-pass svms. arXiv preprint arXiv:0908.0572, 2009.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming, 127(1):3­30, 2011.
Ivor W Tsang, James T Kwok, and Pak-Ming Cheung. Core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research, 6(Apr):363­392, 2005.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998.
Jiyan Yang, Yin-Lam Chow, Christopher Re´, and Michael W Mahoney. Weighted sgd for \ell p regression with randomized preconditioning. arXiv preprint arXiv:1502.03571, 2017.
13

