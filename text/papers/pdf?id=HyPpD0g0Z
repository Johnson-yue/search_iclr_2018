Under review as a conference paper at ICLR 2018
GUARDING AGAINST ADVERSARIAL DOMAIN SHIFTS WITH COUNTERFACTUAL REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
When training a deep neural network for supervised image classification, one can broadly distinguish between two types of latent features of images that will drive the classification: (i) `immutable' or `core' features that are really inherent to the object in question and do not change substantially from one instance of the object to another and (ii) `mutable' or `style' features such as position, rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. The distribution of the style features can change in the future. While transfer learning and domain adaptation would try to adapt to a shift in the distribution(s), we here want to protect against future adversarial domain shifts, arising through changing style features, by ideally not using the mutable style features altogether. There are two broad scenarios and we show how exploiting grouping information in the data helps in both. (a) If the style features are known explicitly (such as translation, rotation, etc.) one usually proceeds by using data augmentation. By exploiting the grouping information about which original image an augmented sample belongs to, we can reduce the sample size required to achieve invariance to the style feature in question. (b) Sometimes the style features are not known explicitly but we still have information about samples that belong to the same underlying object (such as pictures of the same person in different circumstances). By constraining the classification to give the same forecast for all instances that belong to the same object, we show how using this grouping information leads to invariance to such implicit style features and helps to protect against adversarial domain shifts. We provide a causal framework for the problem and treat groups of instances of the same object as counterfactuals under different interventions on the mutable style features. We show links to questions of interpretability, fairness, transfer learning and adversarial examples.
1 INTRODUCTION
Deep neural networks (DNNs) have achieved outstanding performance on prediction tasks like visual object and speech recognition (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015). Their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand. However, in some settings DNNs exploit indirect dependencies that are very different from the features human cognition is based on.
Problems arise when the learned representations rely on dependencies that vanish in test distributions (e.g. see Csurka (2017) and references therein). Such domain shifts can be caused by changing conditions, e.g. color, background or location changes arising when deploying the machine learning (ML) system in production. Predictive performance is then likely to degrade. For instance, the "Russian tank legend" is an example where the training data was subject to sampling biases that were not replicated in the real world. Concretely, the story relates how a machine learning system was trained to distinguish between Russian and American tanks from photos. The accuracy was very high but only due to the fact that all images of Russian tanks were of bad quality while the photos of
1

Under review as a conference paper at ICLR 2018
American tanks were not. The system learned to discriminate between images of different qualities but would have failed badly in practice (Emspak, 2016)1.
Hidden confounding factors like in the example above between image quality and the origin of the tank give rise to indirect associations. These are arguably one reason why deep learning requires large sample sizes as large sample sizes tend to ensure that the effect of the confounding factors averages out (although a large sample size is clearly not per se a guarantee that the confounding effect will become weaker). A large sample size is also required if one is trying to achieve invariance to known factors like translation, point of view, and rotation by using data augmentation. Another related example where human and artificial cognition deviate strongly are adversarial examples-- imperceptibly but intentionally perturbed inputs that are misclassified by a ML model (Szegedy et al., 2014; Goodfellow et al., 2015). Adversarial examples do not fool humans and in general we only need to see one rotated example of the same object to achieve invariance to rotations in our perception. Our starting point is the question whether we can in a simple way mimic the human ability to learn desired invariances from a few instances of the same object and whether we can better align the features DNNs exploit with human cognition.
Considerations of fairness and discrimination might be another reason why we are interested in controlling that certain characteristics of the input data are not included in the learned representations and thus have no impact on the resulting decisions (Barocas & Selbst, 2016; Kilbertus et al., 2017). Unfortunately, existing biases in datasets used for training ML algorithms tend to be replicated in the estimated models (Bolukbasi et al., 2016). For instance, in June 2015 Google's photo app tagged two non-white people as "gorillas"--most likely because the training examples for "people" were mainly photos of white persons, making "color" predictive for the class label (Crawford, 2016; Emspak, 2016). A human would not make the same mistake after only seeing one instance of a non-white person.
Addressing the issues outlined above, we propose counterfactual regularization (CORE) to control what latent features an estimator extracts from the input data. Conceptually, we take a causal view of the data generating process and categorize the latent data generating factors into immutable core and mutable style features. The former are the ones a classifier should use as they pertain to the target of interest in a stable and coherent fashion. On the contrary, the style features should not be relied upon as they can either be easily manipulated and thus change in test distributions, or they encode information that should have no influence on decisions. CORE yields an estimator which is invariant to factors of variation corresponding to style features. Consequently, it is robust with respect to adversarial domain shifts, arising through arbitrarily strong interventions on the style features. CORE relies on the fact that for certain datasets we can observe "counterfactuals" in the sense that we observe the same object under different conditions. Rather than pooling over all examples, CORE exploits knowledge about this grouping, i.e. that a number of instances relate to the same object.
The remainder of this manuscript is structured as follows: §2 starts with two motivating examples, showing how CORE can reduce the need for data augmentation and help predictive performance in small sample size settings. In §3 we review related work and in §4 we formally introduce counterfactual regularization, along with the CORE estimator and theoretical insights for the logistic regression setting. In §5 we further evaluate the performance of CORE in a variety of experiments.
2 TWO MOTIVATING EXAMPLES
2.1 GROUPING PHOTOS OF THE SAME PERSON: BETTER PREDICTIVE PERFORMANCE
The CelebA dataset (Liu et al., 2015) contains face images of celebrities. We consider the task of classifying whether a person wears glasses. Several photos of the same person are available. We use this grouping information and constrain the classification to yield the same prediction for all images belonging to the same person and sharing the same class label. We call the additional instances of the same person counterfactual (CF) observations. Figure 1a shows examples from the training set. The standard approach would be to pool all examples. The only additional information we exploit is that some observations can be grouped. We include n = 10 identities in the training set, resulting
1A different version of this story can be found in Yudkowsky (2008).
2

Under review as a conference paper at ICLR 2018

(a) Grouping by identity.

(b) Grouping by original image.

Figure 1: Examples from a) the subsampled CelebA dataset and b) the augmented MNIST dataset. Connected images are counterfactual examples. The comparison is a training of exactly the same network architecture that does not make use of the grouping information. In a) exploiting the grouping information reduces the test error by 32% compared to pooling over all samples. In b) the test error on rotated digits is reduced by 50%.
in a total sample size m = 321 as there are approximately 30 images of each person2. Exploiting the group structure reduces the average test error from 24.76% to 16.89%, i.e. by approx. 32%, compared to the estimator which just pools all images3.
2.2 GROUPING AUGMENTED IMAGES BY ORIGINAL: MORE SAMPLE EFFICIENT
A different use case of CORE is to make data augmentation more efficient in terms of the required samples. In data augmentation, one creates additional samples by modifying the original inputs, e.g. by rotating, translating, or flipping the images. In other words, additional samples are generated by interventions on style features. Using this augmented data set for training results in invariance of the estimator with respect to the transformations (style features) of interest. For CORE we can use the grouping information that the original and the augmented samples belong to the same object. This enforces the invariance with respect to the style features more strongly compared to normal data augmentation which just pools all samples. We assess this for the style feature "rotation" on MNIST (LeCun & Cortes, 2010) and only include c = 100 augmented training examples for n = 10000 original samples, resulting in a total sample size of m = 10100. The degree of the rotations is sampled uniformly at random from [35, 70]. Figure 1b shows examples from the training set. By using CORE the average test error on rotated examples is reduced from 32.86% to 16.33%, around half its original value4.
3 RELATED WORK
Causal modeling has related aims to the setting of transfer learning and guarding against adversarial domain shifts. Specifically, causal models have the defining advantage that the predictions will be valid even under arbitrarily large interventions on all predictor variables (Haavelmo, 1944; Aldrich, 1989; Pearl, 2009; Scho¨lkopf et al., 2012; Peters et al., 2016). There are two difficulties in transferring these results to the setting of guarding against adversarial domain changes in image classification. The first hurdle is that the classification task is typically anti-causal since the image we use as a predictor is a descendant of the true class of the object we are interested in rather than the other way around. The second challenge is that we do not want to guard against arbitrary interventions on any or all variables but only would like to guard against a shift of the style features. It is hence not immediately obvious how standard causal inference can be used to guard against large domain shifts.
Recently, various approaches have been proposed that leverage causal motivations for deep learning or use deep learning for causal inference. In all of the following methods, the goals and the settings are different from ours. Specifically, the setting of anti-causal prediction and non-ancestral interventions on style variables is not considered. Various approaches focus on cause-effect inference where
2Additional results for n ranging from 10 to 160 can be found in Figure C.7b. 3Details on the architecture can be found in Table C.1. Using ImageNet pre-trained features from Inception V3 does not yield lower error rates. 4Additional results for n  {1000, 10000} and c ranging from 100 to 5000 can be found in Figure C.8.
3

Under review as a conference paper at ICLR 2018

the goal is to find the causal relation between two random variables, X and Y (Lopez-Paz et al., 2017; Lopez-Paz & Oquab, 2017; Goudet et al., 2017). Lopez-Paz et al. (2017) propose the Neural Causation Coefficient (NCC) to estimate the probability of X causing Y and apply it to finding the causal relations between image features. Specifically, the NCC is used to distinguish between features of objects and features of the objects' contexts. Lopez-Paz & Oquab (2017) note the similarity between structural equation modeling and CGANs (Mirza & Osindero, 2014). One CGAN is fitted in the direction X  Y and another one is fitted for Y  X. Based on a two-sample test statistic, the estimated causal direction is returned. Goudet et al. (2017) use generative neural networks for cause-effect inference, to identify v-structures and to orient the edges of a given graph skeleton. Bahadori et al. (2017) devise a regularizer that combines an 1 penalty with weights corresponding to the estimated probability of the respective feature being causal for the target. The latter estimates are obtained by causality detection networks or scores such as estimated by the NCC. Besserve et al. (2017) draw connections between GANs and causal generative models, using a group theoretic framework. Kocaoglu et al. (2017) propose causal implicit generative models to sample from conditional as well as interventional distributions, using a conditional GAN architecture (CausalGAN). The generator structure needs to inherit its neural connections from the causal graph, i.e. the causal graph structure must be known. Louizos et al. (2017) propose the use of deep latent variable models and proxy variables to estimate individual treatment effects.
Kilbertus et al. (2017) exploit causal reasoning to characterize fairness considerations in machine learning. Distinguishing between the protected attribute and its proxies, they derive causal nondiscrimination criteria. The resulting algorithms avoiding proxy discrimination require predictors to be constant as a function of the proxy variables in the causal graph, thereby bearing some structural similarity to our style features.
Distinguishing between core and style features can be seen as some form of disentangling factors of variation. While estimating disentangled factors of variation has gathered a lot of interested in the context of generative modeling (Higgins et al., 2017; Chen et al., 2016; Bouchacourt et al., 2017), we categorize factors of variation by their relation to the object of interest and solve a classification task directly without estimating the latent factors explicitly as in a generative framework. As in CORE, Bouchacourt et al. (2017) exploit grouped observations. In a variational autoencoder framework, they aim to separate style and content--they assume that samples within a group share a common but unknown value for one of the factors of variation while the style can differ.

4 COUNTERFACTUAL REGULARIZATION
We first describe the standard notation for classification before developing a causal graph that allows us to compare the setting of adversarial domain shifts to transfer learning, domain adaptation and adversarial examples.

4.1 NOTATION FOR STANDARD CLASSIFICATION

Let Y  Y be a target of interest. Typically Y = R for regression or Y = {1, . . . , K} in classification with K classes. Let X  Rp be a predictor, for example the p pixels of an image. The prediction y^ for y, given X = x, is of the form f(x) for a suitable function f with parameters   Rd, where the parameters  correspond to the weights in a DNN. For regression, f(x)  R, whereas for classification f(x) corresponds to the conditional probability distribution of Y  {1, . . . , K}. Let be a suitable loss that maps y and y^ = f(x) to R+. A standard goal is to minimize the expected loss or risk
L() = E (Y, f(X)) .

Let (xi, yi) for i = 1, . . . , n be the samples that constitute the training data and y^i = f(xi)

the prediction for yi. A standard approach to parameter estimation is empirical risk minimization,

where Ln()

we =

choose the weights or parameters

1 n

n i=1

(yi, f(xi)).

as

^

=

argmin

Ln(),

with

the

empirical

loss

given

by

4.2 CAUSAL GRAPH

4

Under review as a conference paper at ICLR 2018

latent H

We describe in the following the assumed underlying

causal graph. The prediction is assumed to be anti-

causal, that is the predictors X that we use for Y^ are non-

interventions 

ancestral to Y . In other words, the class label is causal

for the image and not the other way around. The causal

class Y

effect from the class label Y on the image X is mediated

via two types of latent variables: the so-called core fea-

tures C and the style features S. The distinguishing factor between the two is that external interventions  are possi- core C

style S()

ble on the style features but not on the core features. The

class label and the mediating core and style features can

also be confounded by a latent variable H. The full structural model for (H, Y, C, S, , X) is shown in Figure 2.

image X() f Y^ (X())

Importantly, S is non-ancestral for the class Y , that is an

intervention on S will not change the class label. The Figure 2: Data generating process for the

style variable can include point of view, image quality, considered model.

resolution, rotations, color changes, body posture, move-

ment etc. and will in general be context-dependent. The type of features we regard as style and

which ones we regard as core features can conceivably change depending on the circumstances--

for instance, is the color "gray" an integral part of the object "elephant" or can it be changed so

that a colored elephant is still considered to be an elephant? The intervention variable  influences

both the latent style S, as well as the image X via the mediating factor S(). In potential outcome

notation, we let S( = ) be the style under intervention  = , X( = ) the image under the

same intervention and f(X( = )) is finally the prediction under this intervention. For a formal justification of using a causal graph and potential outcome notation simultaneously see Richardson

& Robins (2013).

4.3 DOMAIN ADAPTATION, ADVERSARIAL EXAMPLES AND ADVERSARIAL DOMAIN SHIFTS

In this work, we are interested in guarding against adversarial domain shifts. We use the causal graph to explain the related but not identical goals of domain adaptation, transfer learning and guarding against adversarial examples.

(i) Domain adaptation and transfer learning. Assume we have J different environments, each with a new distribution Fj for the interventions . The shift of Fj for different environments j = 1, . . . , J causes a shift in both the distribution of X and in the conditional distribution Y |X. If we consider domain adaptation and transfer learning together, their goal is generally to give the best possible prediction Y^j(x) in each environment j = 1, . . . , J.
(ii) Standard adversarial examples. The setting of adversarial examples in the sense of Szegedy et al. (2014) and Goodfellow et al. (2015) can also be described by the causal graph above by using S() =  and identifying S with pixel-by-pixel additive effects that are added to the image X. The magnitude of the intervention  is then typically assumed to be within an -ball in q-norm around the origin, with q =  or q = 2 for example. If the input dimension is large many imperceptible changes in the coordinates of X can cause a large change in the output, leading to a misclassification of the sample. The goal is to devise a classification in this graph that minimizes the adversarial loss.

E max

Y, f X() ,

Rq :  q 

(1)

where X() is the image under the intervention  and Y^ = f(X()) is the estimated conditional distribution of Y , given the image under the chosen interventions.

(iii) Adversarial domain shifts. Here we are interested in arbitrarily strong interventions   Rq on the style features S, which are not known explicitly in general. Analogously to (1), the

adversarial loss under arbitrarily large style interventions is

Ladv() = E

max
Rq

Y, f X()

.

(2)

In contrast to (1) the interventions can be arbitrarily strong but we assume that the style features

S can only change certain aspects of the image, while other aspects of the image (mediated by

the core features) cannot be changed.

5

Under review as a conference paper at ICLR 2018

4.4 COUNTERFACTUAL OBSERVATIONS / GROUPING

The classical problem of causal inference is that we can never observe a counterfactual. For instance,

we can only see the health outcome if we take a medicine or not but we can never see both. In

contrast, for the applications of image analysis, counterfactuals are conceivable5 we can see the

same object under different conditions and know that it is still the same object. Assume we have

again n samples but for each sample i  {1, . . . , n} we have mi  1 counterfactuals under different

(but unobserved) values of . The interventions can be shift- or do-interventions and also have

nonlinear effects. of counterfactual

Let m = i observations.

mi denote the total number Denote the j-th observation

of of

samples and sample i, by

c= xi,j

m 

- n, Rp.

the

number

4.4.1 STANDARD APPROACH: POOLED ESTIMATOR

The standard approach is to simply pool over all available observations, ignoring any grouping information that might be available. The pooled estimator thus treats all examples identically by summing over the loss as

^pool =

argmin

1 m

n

mi

i=1 j=1

yi, f(xi,j ) .

The adversarial loss of the pooled estimator will in general be infinite; see §4.6 for a concrete
example. Using Figure 2, one can show that the pooled estimator will work well in terms of the adversarial loss Ladv if both (i) Y  X|C and (ii) Y  C|S. The first condition (i) implies that if the estimator learns to extract C from the image X, there is no further information in X that explains Y and, therefore, the direction corresponding to S does not need to be used for predicting Y . The second condition (ii) is fulfilled if the relations between Y , C, and S are not deterministic. Intuitively, it ensures that S cannot replace C in the first condition. From (i) and (ii), we see that the pooled estimator will work well in terms of the adversarial loss Ladv if (a) the edge from S to X is absent or if (b) both the edge from H to S and the edge from Y to S are absent (cf. Figure 2).

4.5 CORE ESTIMATOR

In order to minimize the adversarial loss (2) we have to ensure f(x()) is as constant as possible as a function of  for all x  Rp. Let I be the invariant parameter space
I := { : f(x()) is constant as function of  for all x  Rp}.
For all   I, the adversarial loss (2) is identical to the loss under no interventions at all. More precisely, let X be a shorthand notation for X( = 0), the images in absence of external interventions:

if   I, then

E max
Rq

Y, f X()

The optimal predictor in the invariant space I is

= E Y, f X .

 = argmin E (Y, f(X)) such that   I.

(3)

If f is only a function of the core features C, then   I. The challenge is that the core features are not directly observable and we have to infer the invariant space I from data. To get an approximation
to the optimal invariant parameter vector (3), we use empirical risk minimization:

^core

=

argmin

1 n

n

mi

yi, f(xi,j) such that   In,

i=1 j=1

(4)

where the first part is the empirical version of the expectation in (3). The unknown invariant parameters space I is approximated by an empirically invariant space In, defined as
n
In := { : i2()   },
i=1

5If we take the word counterfactual literally they cannot exist as they are counter-to-fact, but if we understand as counterfactual the image X under different interventions  while keeping all other noise variables constant, then counterfactuals are possible as we can often keep everything constant about an object but change the style variables like translation, rotation, movement, or image quality.

6

Under review as a conference paper at ICLR 2018

(a) Examples from the training set.

(c) Misclassification rates.

MISCLASS. RATE (IN %)

50

(b) Examples of misclassified observations.

y  adult P^core(adult) = 0.99 P^pool(adult) = 0.01

y  child P^core(child) = 0.56 P^pool(child) = 0.33

y  adult P^core(adult) = 0.18 P^pool(adult) = 0.03

40 30 20 10

Method
CORE pooled

0
Train. set Test set 1 Test set 2 Test set 3
Dataset
Figure 3: a) Examples from the stickmen training set. The first three images from the left have y  child; the remaining three images have y  adult. Connected images are counterfactual examples. b) Misclassified observations from test set 2. c) Misclassification rates for c = 50. Results for c  {20, 500, 2000} can be found in Figure C.10.

where i2() is the variance of f(xi,j) when varying j = 1, . . . , mi for a fixed value of i and   0 is a regularization constant. Setting  = 0 is equivalent to demanding that the estimated predictions
for the class labels are identical across all mi counterfactuals of image i, while slightly larger values of  allow for some small degree of variations. For all values   0 the true invariant space I is a
subset of the empirically invariant subspace In, that is I  In. Under the right assumptions we get In = I for n  . We return to this question in §4.6. One can equally use the Lagrangian form of the constrained optimization in (4), with a penalty parameter  instead of a constraint  . We show in
§C.1 that the outcome does not depend strongly on the chosen value of the constraint or the penalty.

4.6 THEORETICAL RESULTS
In §A we analyze the adversarial loss, defined in Eq. (2), for the pooled and the CORE estimator in a one-layer network for binary classification (logistic regression). Here, we briefly sketch the result while all details are given in §A. Assume the structural equation for the image X  Rp is linear in the style features S  Rq (with generally p q) and we use logistic regression to predict a class label Y  {-1, 1}. Under suitable assumptions (cf. Assumption 1), the pooled estimator has infinite adversarial loss while the adversarial loss of the CORE estimator converges to the optimal adversarial loss as n  .

5 EXPERIMENTS
We perform an array of different experiments: in §5.1 and §5.2 we study how CORE can handle confounded training data sets and changing style features in test distributions. For the assessment we explicitly control the level of confounding. In §5.3, we consider classifying elephants and horses where S  color. In §B, we include two additional experiments: in the first one, Y  gender and S  wearing glasses; in the second one, Y  wearing glasses and S  brightness. Additional experimental results for the settings introduced in §2 can be found in §C.2 and §C.3. A TensorFlow (Abadi et al., 2015) implementation of CORE will be made available as well as further code necessary to reproduce the experiments. In addition to the details provided below, information on the employed architectures can be found in §C.7. An open question is how to set the value of the tuning parameter  or the penalty  in Lagrangian form. We show in §C.1 that performance is typically not very sensitive to the choice of .
5.1 STICKMEN IMAGE-BASED AGE CLASSIFICATION
In this example we consider synthetically generated stickmen images (cf. Figure 3a). The target of interest is Y  {adult, child} and C  height. The class Y is causal for height and height cannot be easily intervened on, so we consider it to be a core feature--it is a robust predictor for differentiating between children and adults. Additionally, there is a dependence between age and S  movement in the training dataset which arises through the hidden common cause H  place of observation. The data generating process is illustrated in Figure C.9. For instance, the images of children might mostly

7

Under review as a conference paper at ICLR 2018

(a) CF setting 1, µ = 30

(c) Misclassification rates.

70

MISCLASS. RATE (IN %)

60

(b) Examples of misclassified observations.

y  no glasses P^core(no gl.) = 0.96
P^pool(no gl.) = 0

y  glasses P^core(gl.) = 0.62 P^pool(gl.) = 0.02

y  glasses P^core(gl.) = 0.99 P^pool(gl.) = 0.09

50 40 30 20 10

Method
CORE pooled

0
Tr Te1 Te2 Te3 Te4
Dataset
Figure 4: a) Examples from the CelebA image quality dataset. The first three images from the left have y  no glasses; the remaining three images have y  glasses. Connected images are counterfactual examples. b) Misclassified examples from the test sets. c) Misclassification rates for µ = 30 and c = 5000. Results for different counterfactual settings and µ  {30, 40, 50} can be found in Figure C.12.

show children playing while the images of adults typically show them in more "static" postures. If the learned model exploits this dependence for predicting Y , it will fail when presented images of, say, dancing adults.
Figure 3a shows examples from the training set where large movements are associated with children and small movements are associated with adults. Test set 1 follows the same distribution. In test sets 2 and 3 S is intervened on such that the edge from H to S is removed and the dependence between Y and S vanishes. In test sets 2 and 3 large movements are associated with both children and adults, while the movements are heavier in test set 3 than in test set 2. Figure C.10 shows examples from all test sets. Figure 3c shows misclassification rates for CORE and the pooled estimator for c = 50 with a total sample size of m = 20000. For as few as 50 counterfactual observations, CORE succeeds in achieving good predictive performance on test sets 2 and 3 where the pooled estimator fails (test errors > 40%). These results suggest that the learned representation of the pooled estimator uses movement as a predictor for age while CORE does not use this feature due to the counterfactual regularization. Importantly, including more counterfactual examples would not improve the performance of the pooled estimator as these would be subject to the same bias and hence also predominantly have examples of heavily moving children and "static" adults (also see Figure C.10 which shows results for c  {20, 500, 2000}).

5.2 EYEGLASSES DETECTION: IMAGE QUALITY INTERVENTION
As in §2.1, we use the CelebA dataset and consider the problem of classifying whether the person in the image is wearing eyeglasses. Here, S is the quality of the image which differs conditional on Y 6--if the image shows a person wearing glasses, the image quality tends to be lower. This setting mimics the confounding that occurred in the Russian tank legend (cf. §1). The strength of the image quality intervention is governed by sampling the new image quality as a percentage of the original image's quality from a Gaussian distribution N (µ = 30,  = 10). Images of people without glasses are not changed. Thus, we only have counterfacutual observations for Y  glasses. Figure 4a shows examples from the training set. Here, we use as the counterfactual observation the same image but with a newly sampled image quality value from N (30, 10). We call using the same image as a counterfactual "CF setting 1". Two alternatives for constructing counterfactual observations for this setting are discussed in §B.2.1. Here, c = 5000 and m = 20000.
Figure 4c shows misclassification rates for CORE and the pooled estimator on different test sets. Examples from all test sets can be found in Figure C.11. Test set 1 follows the same distribution as the training set. In test set 2 the class of the quality intervention is reversed, i.e. the quality of images showing people without glasses tends to be lower. In test set 3 all images are left unchanged and in test set 4 the quality of all images is decreased. First, we notice that the pooled estimator performs better than CORE on test set 1. This can be explained by the fact that it can exploit the predictive information contained in an image's quality while CORE is restricted not to do so. Second, we
6In §B.2 we additionally look at the case where the brightness distribution differs conditional on Y .

8

Under review as a conference paper at ICLR 2018

(a) Training examples.

(c) Misclassification rates.

MISCLASS. RATE (IN %)

50

(b) Examples of misclassified observations.

y  horse

y  horse

P^core(horse) = 0.72 P^core(horse) = 1.00

P^pool(horse) = 0.01 P^pool(horse) = 0.01

y  elephant P^core(ele.) = 0.95 P^pool(ele.) = 0.00

40 30 20 10

Method
CORE pooled

0
Tr Te1 Te2 Te3 Te4
Dataset
Figure 5: a) Examples from the subsampled and augmented AwA2 dataset. The first three images from the left shows horses, the remaining three images show elephants. Connected images are counterfactual examples. b) Misclassified examples from the test sets. c) Misclassification rates.

observe that the pooled estimator does not perform well on test sets 2­4 as its learned representation seems to use the image's quality as a predictor for the target. In contrast, the predictive performance of CORE is hardly affected by the changing image quality distributions. More experimental details are provided in §C.5. Results for quality interventions of different strengths (µ  {30, 40, 50}) are shown in Figure C.12.

5.3 ELMER THE ELEPHANT
In this example, we want to assess whether invariance with respect to S  color can be achieved. In the children's book "Elmer the elephant"7 one instance of a colored elephant suffices to recognize it as being an elephant, making the color "gray" no longer an integral part of the object "elephant". Motivated by this process of concept formation, we would like to assess whether CORE can exclude "color" from its learned representation by including a few counterfactuals of different color.
We work with the "Animals with attributes 2" (AwA2) dataset (Xian et al., 2017) and consider classifying images of horses and elephants. The data generating process is illustrated in Figure C.14. We include counterfactual examples by adding grayscale images for c = 250 images of elephants, i.e. counterfactuals are only available for one class and the shift in color is quite subtle. The total sample size is 1850.
Figure 5a shows examples from the training set and Figure 5c shows misclassification rates for CORE and the pooled estimator on different test sets. Examples from all test sets can be found in Figure C.13. Test set 1 contains original, colored images only. In test set 2 images of horses are in grayscale and the colorspace of elephant images is modified, effectively changing the color gray to red-brown. Test set 3 contains grayscale images only and in test set 4 the colorspace of all images is shifted towards red. The details are given in §C.6 . We observe that the pooled estimator does not perform well on test sets 2 and 3 as its learned representation seems to exploit the fact that "gray" is predictive for the target in the training set. Using this information helps its predictive accuracy on test set 1. In contrast, the predictive performance of CORE is hardly affected by the changing color distributions.
It is noteworthy that a colored elephant can be recognized as an elephant by adding a few examples of a grayscale elephant to the very lightly colored pictures of natural elephants. If we just pool over these examples, there is still a strong bias that elephants are gray. The CORE estimator, in contrast, demands invariance of the prediction for instances of the same elephant and we can learn color invariance with a few added grayscale images.
While a thorough analysis in terms of fairness considerations is beyond the scope of this work, we would like to draw the following connection. If "color" was a protected attribute or a proxy for one, CORE would satisfy fairness in the sense that it would not include it in its learned representation. In contrast, there is no way to avoid that the pooled estimator extracts and uses "color" for its decisions.
7https://en.wikipedia.org/wiki/Elmer_the_Patchwork_Elephant

9

Under review as a conference paper at ICLR 2018
6 CONCLUSION
Distinguishing the latent features in an image into core and style features, we have proposed counterfactual regularization (CORE) to achieve robustness with respect to arbitrarily large interventions on the mutable style features. The main idea of the CORE estimator is to exploit the fact that we often have instances of the same object in the training data. By demanding invariance of the classifier amongst a group of instances that relate to the same object, we can achieve invariance of the classification performance with respect to adversarial interventions on style features such as image quality, fashion type, color, or body posture. The training also works despite sampling biases in the data. There are two main applications areas. If the style features are known explicitly, we can achieve the same classification performance as standard data augmentation approaches but using fewer instances which, on top, do not have to be carefully balanced in the training data. Perhaps more interestingly, if the style features are unknown, the regularization of CORE avoids usage of them automatically by penalizing features that vary strongly between different instances of the same object in the training data. An interesting line of work would be to use larger models such as Inception or large ResNet architectures (Szegedy et al., 2015; He et al., 2016). These models have been trained to be invariant to an array of explicitly defined style features. In §B.1 we include results which show that using Inception V3 features does not guard against interventions on more implicit style features. We would thus like to assess what benefits CORE can bring for training Inception-style models end-to-end, both in terms of sample efficiency and in terms of generalization performance. While we showed some examples where the necessary grouping information is available, an interesting possible future direction would be to use video data since objects display temporal constancy and the temporal information can hence be used for grouping and counterfactual regularization. Potentially an analogous approach could also help to debias word embeddings.
10

Under review as a conference paper at ICLR 2018

REFERENCES

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane´, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vie´gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https: //www.tensorflow.org/. Software available from tensorflow.org.

J. Aldrich. Autonomy. Oxford Economic Papers, 41:15­34, 1989.

M. T. Bahadori, K. Chalupka, E. Choi, R. Chen, W. F. Stewart, and J. Sun. Causal regularization. ArXiv e-prints, 2017. URL http://arxiv.org/abs/1702.02604.

S. Barocas and A. D. Selbst. Big Data's Disparate Impact. 104 California Law Review 671, 2016.

M. Besserve, N. Shajarisales, B. Scho¨lkopf, and D. Janzing. Group invariance principles for causal generative models. ArXiv e-prints, 2017. URL http://arxiv.org/abs/1705.02212.

T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems 29. 2016.

D. Bouchacourt, R. Tomioka, and S. Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. ArXiv e-prints, 2017. URL http: //arxiv.org/abs/1705.08841.

X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. In Advances in Neural Information Processing Systems 29. 2016.

K. Crawford. Artificial intelligence's white guy problem. The New York Times, June 25 2016, 2016. URL https://www.nytimes.com/2016/06/26/opinion/sunday/ artificial-intelligences-white-guy-problem.html.

G. Csurka. A comprehensive survey on domain adaptation for visual applications. In Domain Adaptation in Computer Vision Applications., pp. 1­35. 2017.

J. Emspak. How a machine learns prejudice. Scientific American, December 29

2016, 2016.

URL https://www.scientificamerican.com/article/

how-a-machine-learns-prejudice/.

I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.

O. Goudet, D. Kalainathan, P. Caillou, D. Lopez-Paz, I. Guyon, M. Sebag, A. Tritas, and P. Tubaro. Learning Functional Causal Models with Generative Neural Networks. ArXiv e-prints, 2017. URL https://arxiv.org/abs/1709.05321.

T. Haavelmo. The probability approach in econometrics. Econometrica, 12:S1­S115 (supplement), 1944.

K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ICCV, 2015.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.

I. Higgins, L. Matthey, A. Pal, C. Burges, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. International Conference on Learning Representations, 2017.

N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Scho¨lkopf. Avoiding discrimination through causal reasoning. Advances in Neural Information Processing Systems, 2017.

11

Under review as a conference paper at ICLR 2018
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015. M. Kocaoglu, C. Snyder, A. G. Dimakis, and S. Vishwanath. CausalGAN: Learning Causal Implicit
Generative Models with Adversarial Training. ArXiv e-prints, 2017. URL https://arxiv. org/abs/1709.02023. A. Krizhevsky, I. Sutskever, and G. E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25. 2012. Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun. com/exdb/mnist/. Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. D. Lopez-Paz and M. Oquab. Revisiting Classifier Two-Sample Tests. International Conference on Learning Representations (ICLR), 2017. D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scho¨lkopf, and L. Bottou. Discovering causal signals in images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017), 2017. C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference with deep latent-variable models. Advances in Neural Information Processing Systems, 2017. M. Mirza and S. Osindero. Conditional Generative Adversarial Nets. ArXiv e-prints, 2014. URL https://arxiv.org/abs/1411.1784. J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, USA, 2nd edition, 2009. J. Peters, P. Bu¨hlmann, and N. Meinshausen. Causal inference using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, Series B (with discussion), to appear, 2016. T. Richardson and J. M. Robins. Single world intervention graphs (SWIGs): A unification of the counterfactual and graphical approaches to causality. Center for the Statistics and the Social Sciences, University of Washington Series. Working Paper 128, 30 April 2013, 2013. B. Scho¨lkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), pp. 1255­1262, 2012. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Computer Vision and Pattern Recognition (CVPR), 2015. Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-shot learning - A comprehensive evaluation of the good, the bad and the ugly. ArXiv e-prints, 2017. URL http://arxiv.org/abs/ 1707.00600. E. Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. Global catastrophic risks, 1, 2008.
12

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY MATERIAL

A LOGISTIC REGRESSION

Assume the structural equation for the image X  Rp is linear in the style features S  Rq (with generally p q) and we use logistic regression to predict a class label Y  {-1, 1}. Let the interventions   Rq act additively on the style features S (this is only for notational convenience) and let the style features act in a linear way on the image X via a matrix G  Rp×q (this is an important assumption without which results are more involved). The core features are in C  Rr, where in general r  p but this is not important for the following. For independent Y , S, C , X in R, Rq, Rr, Rp respectively with positive density on their support and continuously differentiable functions ky, ks, kc, kx,
Y  ky(H, Y ) C  kc(H, Y, C ) S  kS(H, Y, S) +  X  kX (C, X ) + GS.
Of these, Y and X are observed whereas H, C, , S and the noise variables are latent.
We assume a logistic regression as a prediction of Y from the image data X:
exp(xt) f(x) := 1 + exp(xt) .
Given training data with m samples, we estimate  with ^ and use here a logistic loss (yi, xi) = log(1 + exp(-yi(xit))) for training and testing. Some interesting expected losses on test data include
L() = E Y, f(X))

Ladv() = E

max
Rq

Y, f(X())

,

where the X in the first loss is a shorthand notation for X( = 0), that is the images in absence of interventions on the style variables. The first loss is thus a standard logistic loss in absence of adversarial interventions. The second loss is the loss under adversarial style or domain interventions as we allow arbitrarily large interventions on S here. The corresponding benchmarks are

L

=

min L(),


and

Ladv

=

min Ladv().


The formulation of Theorem 1 relies on the following assumptions. Assumption 1. We require the following conditions to hold:
(A1) Assume  is sampled from a distribution for training data in Rq with positive density on an -ball in 2-norm around the origin for some > 0.
(A2) Assume the matrix G has full rank q.
(A3) Assume c  q, that is the number c = m - n of counterfactual examples in the samples is at least as large as the dimension of the style variables.
Theorem 1. Under Assumption 1, with probability 1 with respect to the training data of independently sampled (xi, yi) for i = 1, . . . , n, the pooled estimator has infinite adversarial loss
Ladv(^pool) = . For the CORE estimator, for n  ,
Ladv(^core) p Ladv.

An equivalent results can be derived for misclassification loss instead of logistic loss (with infinity replaced by 1).

13

Under review as a conference paper at ICLR 2018

Proof. First part. To show the first part, namely that with probability 1,

Ladv(^pool) = ,

we need to show that Gt^pool = 0 with probability 1. The reason is as follows: if Gt = 0, then Ladv() =  as we can then find a v  Rq such that  := tGv = 0. Setting  = v for   R, we get x()t = x( = 0)t + . Hence log(1 + exp(-x()t))   for either    or
  -.

To show that Gt^pool = 0 with probability 1, let ^ be the oracle estimator that is constrained to be orthogonal to the column space of G:

^

=

argmin:Gt=0

Ln()

with

Ln()

:=

1 n

n

(yi, f(xi(i))).

i=1

(5)

We show Gt^pool = 0 by contradiction. Assume that Gt^pool = 0. If this is indeed the case, then
the constraint Gt = 0 in (5) becomes non-active and we have ^pool = ^. This would imply that taking the directional derivative of the training loss with respect to any   Rp in the column space of G should vanish at the solution ^. Define ri() := (yi + 1)/2 - f^ . For all i = 1, . . . , n we have ri = 0. The derivative g() of Ln() in direction of  is proportional to

1 g() =
n

n

p
ri(^)(

xi,k k ),

i=1 k=1

(6)

where xi,k is the k-th variable in sample xi. Let xi(0) for i = 1, . . . , n be the counterfactual training data in absence of any interventions ( = 0). Since the interventions only have an effect

on the column space of G in X, the oracle estimator ^ is identical under the true training data and the counterfactual training data x(0). Hence, for any  in Rp, the derivative g() in (6) can also be

written as

1 g() =
n

n

ri(^)(

xi,k (0)k ).

i=1 k

(7)

Taking the difference between (6) and (7),

1 n

n

ri(^)(

(xi,k - xi,k(0))k) = 0.

i=1 k

(8)

Now, by the model assumptions, xi - xi(0) = Gi. Since  is in the column-space of G, there exists u  Rq such that  = Gu. then (8) can be written as

1 n

n

ri(^)itGtGu = 0

i=1

(9)

From (A2) we have that the eigenvalues of GtG are all positive. Also ri(^) is not a function of the interventions i for i = 1, . . . , n since, as already argued above, the estimator ^ is identical whether trained on the original data xi for i = 1, . . . , n or on the counterfactual data xi(0) for i = 1, . . . , n. If we condition on (xi(0), yi) for i = 1, . . . , n (that is everything except for the random i, i = 1, . . . , n), then the interventions i are by (A1) drawn from a continuous distribution. Hence the left hand side of (9) has a continuous distribution, and the probability of the left hand side
of (9) being not identically 0 is 1. This completes the proof of the first part by contradiction.

Second part. For the second part, we first show that with probability 1, ^core = ^ with ^ defined as in (5). Note that the invariant space is for this model the linear subspace I = { : Gt = 0}.
Note that by their respective definitions,

^

=

argmin

1 n

n

(yi, f(xi(i))) such that   I,

i=1

^core

=

argmin

1 n

n

(yi, f(xi(i))) such that   In.

i=1

14

Under review as a conference paper at ICLR 2018

By (A2) and (A3), with probability 1, In = { : Gt = 0} since the number of counterfactuals examples is equal to or exceeds the rank q of G and S has a linear influence on X. Hence with probability 1, we have I = In and hence ^core = ^. We thus need to show that

Ladv(^) p Ladv.

(10)

Since ^ is in I, we have (y, x()) = (y, x(0)), where x(0) are the previously discussed counterfactual data in the absence of interventions. Hence

^

=

argmin

1 n

n

(yi, f(xi(0))) such that   I,

i=1

(11)

that is the estimator is unchanged if we use the data without interventions (i = 0) as training data. Define the population-optimal vector as

 = argmin E

max


(Y, f(X()))

such that   I,

which can for the same reason be written as

 = argmin E (Y, f(X( = 0))) such that   I.

(12)

Hence (11) and (12) can be written as

^

=

argmin:I

L(n0)()

where

L(n0)()

:=

1 n

n

(yi, f(xi(0)))

i=1

 = argmin:I L(0)() where L(0)() := E[ (Y, f(X( = 0)))].

Comparing (11) and (12), by uniform convergence of Ln(0) to the population loss L(0), we have L(0)(^) p L(0)().

By definition of I and  we have Ladv = Ladv() = L(0)(). As ^ is in I, we also have Ladv(^) = L(0)(^). Since, from above, L(0)(^) p L(0)(), this also implies Ladv(^) p
Ladv() = Ladv. This completes the proof, using the previous fact that ^core = ^ with probability 1 under (A3).

B ADDITIONAL EXPERIMENTS
B.1 GENDER CLASSIFICATION
We work with the CelebA dataset (Liu et al., 2015) and consider the problem of classifying whether the person in the image is male or female. We create a confounding by including mostly images of men wearing glasses while the images of women do not include photos of women with glasses. As counterfactuals, we use an image of the same person without glasses if the person is male and with glasses if the person is female. We call using an image of the same person as counterfactual "CF setting 2". Examples from the training and test sets are shown in Figure B.2. Test set 1 follows the same distribution as the training set. In test set 2 the association between gender and glasses is flipped: women always wear glasses while men never wear glasses.
In this example, we would like to assess whether the results will differ when (a) training a fourlayer CNN (as detailed in Table C.1) end-to-end versus (b) using Inception V3 features and merely retraining the softmax layer. Figure B.2 shows the results for varying numbers of m and c--in the left column for training a four-layer CNN; in the right column for using Inception V3 features. Overall, we see the same trends: As c increases, the performance difference between CORE and the pooled estimator becomes smaller. This is due to the fact that S is binary in this example and, therefore, including counterfactual examples corresponds to data augmentation. Interestingly, the pooled estimator performs worse on test set 2 as m becomes larger. It thus seems to exploit S to a larger extent as m grows.
15

Under review as a conference paper at ICLR 2018

(a) Training examples.

(b) Examples from test set 1.

(c) Examples from test set 2.

Figure B.1: Examples from the CelebA gender dataset.

(a) m = 5000, 4-layer CNN

(b) m = 5000, Inception V3

Method CORE pooled

Method CORE pooled

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

(c) m M=et1h0o0d00, C4O-lRaEyer pCoNoleNd

(d) m M=et1h0o0d00, CInOcReEptiopnooVled3

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

(e) m M=et1h7o0d00, C4O-lRaEyer pCoNoleNd

(f) m M=e1th7o0d00, CInOcReEptiopnooVled3

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

MISCLASS. RATE (IN %)

c: 100
50 45 40 35 30 25 20 15 10
5 0
Tr Te1 Te2

c: 250

c: 500

Tr Te1 Te2 Tr Te1 Te2
Dataset

c: 1000 Tr Te1 Te2

Figure B.2: Misclassification rates for the CelebA gender datasets with varying numbers for m and c. The left column shows results for training a four-layer CNN (cf. Table C.1) end-to-end, the right column shows results for using Inception V3 features and retraining the softmax layer.

16

Under review as a conference paper at ICLR 2018

(a) Training examples (CF setting 1,  = 20).

(c) Misclassification rates.
40

MISCLASS. RATE (IN %)

(b) Examples of misclassified observations.

y  glasses P^core(gl.) = 1.00 P^pool(gl.) = 0.21

y  no glasses P^core(no gl.) = 0.84 P^pool(no gl.) = 0.13

y  glasses P^core(gl.) = 0.90 P^pool(gl.) = 0.14

30 20 10

Method
CORE pooled

0
Tr Te1 Te2 Te3 Te4
Dataset
Figure B.3: a) Examples from the CelebA brightness dataset. The first three images from the left have y  no glasses; the remaining three images have y  glasses. Connected images are counterfactual examples. b) Misclassified examples from the test sets. c) Misclassification rates for  = 20 and c = 2000. Results for different counterfactual settings,   {5, 10, 20} and c  {200, 5000} can be found in Figure B.5.

B.2 EYEGLASSES DETECTION: BRIGHTNESS INTERVENTION
As in §5.2 we work with the CelebA dataset and consider the problem of classifying whether the person in the image is wearing eyeglasses. Here we analyze a confounded setting that could arise as follows. Say the hidden common cause of Y and S, H indicates whether the image was taken outdoors or indoors. If it was taken outdoors, then the person wears glasses and the image tends to be brighter. If the image was taken indoors, then the person does not wear glasses and the image tends to be darker. In other words, S  brightness and the structure of the data generating process is equivalent to the one shown in Figure C.9. Figure B.3a shows examples from the training set. Here, we use as the counterfactual observation the same image (CF setting 1) but with a different brightness. Two alternatives for constructing counterfactual observations in this setting are discussed in §B.2.1. We use c = 2000 and m = 20000.
For the brightness intervention, we sample the value for the magnitude of the brightness increase resp. decrease from an exponential distribution with mean  = 20. Specifically, we use ImageMagick8 to modify the brightness of each image. In the training set and test set 1, we sample the brightness value as bi,j = 100 + yiei,j where ei,j  Exp(-1) and yi  {-1, 1}. yi = 1 corresponds to yi  glasses. We then apply the command convert -modulate b ij,100,100 input.jpg output.jpg to the image. Importantly, since we sample from an exponential distribution, the brightness interventions are quite subtle in many cases as can be seen in Figure B.3a.
Figure B.3c shows misclassification rates for CORE and the pooled estimator on different test sets. Examples from all test sets can be found in Figure B.4. Test set 1 follows the same distribution as the training set. In test set 2 the sign of the brightness intervention is reversed, i.e. images of people with glasses tend to be darker; images of people without glasses tend to be brighter. In test set 3 all images are left unchanged and in test set 4 the brightness of all images is increased. First, we notice that the pooled estimator performs better than CORE on test set 1. This can be explained by the fact that it can exploit the predictive information contained in the brightness of an image while CORE is restricted not to do so. Second, we observe that the pooled estimator does not perform well on test sets 2 and 4 as its learned representation seems to use the image's brightness as a predictor for the response which fails when the brightness distribution in the test set differs significantly from the training set. In contrast, the predictive performance of CORE is hardly affected by the changing brightness distributions. Results for   {5, 10, 20} and c  {200, 5000} can be found in Figure B.5.
B.2.1 COUNTERFACTUAL SETTINGS 2 AND 3
Above we used the same image to create a counterfactual observation by sampling a different value for the brightness intervention. A plausible alternative is to use a different image of the same person as counterfactual. We call this "CF setting 2". For comparison, we also evaluate using an image of
8https://www.imagemagick.org

17

Under review as a conference paper at ICLR 2018

(a) CF setting 1,  = 5

(b) CF setting 1,  = 10

(c) CF setting 1,  = 20

(d) CF setting 2,  = 5

(e) CF setting 2,  = 10

(f) CF setting 2,  = 20

(g) CF setting 3,  = 5

(h) CF setting 3,  = 10

(i) CF setting 3,  = 20

Figure B.4: Examples from the CelebA brightness datasets, counterfactual settings 1­3 with   {5, 10, 20}. In all rows, the first three images from the left have y  no glasses; the remaining three images have y  glasses. Connected images are counterfactual examples. In panels (a)­(c), row 1 shows examples from the training set, rows 2­4 contain examples from test sets 2­4, respectively. Panels (d)­(i) show examples from the respective training sets.
a different person as counterfactual as a baseline ("CF setting 3"). Examples from the training sets using CF setting 2 and 3 can be found in Figure B.4.
Results for all counterfactual settings,   {5, 10, 20} and c  {200, 5000} can be found in Figure B.5. We see that using counterfactual setting 1 works best since we could explicitly control that only S  brightness varies between counterfactual examples. In counterfactual setting 2, different images of the same person can vary in many factors, making it more challenging to isolate brightness as the factor to be invariant against. Lastly, we see that even grouping images of different persons can still help predictive performance to some degree.
C EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS FOR EXPERIMENTS INTRODUCED IN §2 AND §5
C.1 CHOOSING THE TUNING PARAMETER 
An open question is how to set the value of the tuning parameter  in Eq. (4) or the penalty  in the Lagrangian form. Figure C.6 shows the misclassification rates of CORE on the subsampled and augmented AwA2 dataset as a function of the penalty . We see that performance is not very sensitive to the choice of .
C.2 GROUPING PHOTOS OF THE SAME PERSON: BETTER PREDICTIVE PERFORMANCE
Here, we show further results for the experiment introduced in §2.1. We vary the number of identities included in the training data set n  {10, 20, 40, 80, 160}. This results in total sample sizes m ranging from 321 for n = 10 to 4386 for n = 160, implying that the average number of counterfactual observations per person varies between 27 and 32. Figure C.7b shows the misclassification rates for the test set which consists of 5000 examples. We see that CORE helps predictive performance compared to the estimator which just pools all images, notably when n is very small. It thus successfully mitigates the effect of potential confounders arising due to small sample sizes. As n and m increase the performance of CORE and the pooled estimator become comparable--the larger
18

Under review as a conference paper at ICLR 2018

(a) CF setting 1, c = 200

(b) CF setting 1, c = 2000

Method CORE pooled

Method CORE pooled

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

(c) CMFestheottding 2C,OcRE= 20po0o0led

(d) CMFestheottding 2C,OcRE= 5p0o0o0led

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

(e) CMFestheottding 3C,OcRE= 20po0o0led

(f) CMFestehtotding 3C,OcRE= 50po0o0led

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

MISCLASS. RATE (IN %)

mean: 5 40

mean: 10

mean: 20

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

Figure B.5: Misclassification rates for the CelebA brightness datasets, counterfactual settings 1­3 with c  {200, 2000, 5000} and the mean of the exponential distribution   {5, 10, 20}.

(a) Test set 1.

(b) Test set 2.

(c) Test set 3.

(d) Test set 4.

MISCLASS. RATE (IN %)

MISCLASS. RATE (IN %)

MISCLASS. RATE (IN %)

MISCLASS. RATE (IN %)

30 30 30 30

20

Me2t0hod

Me2t0hod

Me2t0hod

Method

CORE

CORE

CORE

CORE

10 10 10 10

0 10 25 50 75 100 125
Lambda

0 10 25 50 75 100 125
Lambda

0 10 25 50 75 100 125
Lambda

0 10 25 50 75 100 125
Lambda

Figure C.6: Misclassification rates of CORE on the subsampled and augmented AwA2 dataset as a function of the penalty . The outcome does not depend strongly on the chosen value.

19

Under review as a conference paper at ICLR 2018

(a) Training examples, grouped by identity.

(b) Misclassification rates.

n: 10 n: 20 n: 40 n: 80 n: 160

MISCLASS. RATE (IN %)

25

20
Method
15 CORE
10 pooled

5

0 Tr Te1

Tr Te1 Tr Te1 Tr Te1
Dataset

Tr Te1

Figure C.7: a) Examples from the subsampled CelebA dataset. In each row, the first three images from the left have y  no glasses; the remaining three images have y  glasses. Connected images are counterfactual
examples. b) Misclassification rates for different numbers of identities, included in the training data.

(a) n = 1000

(b) n = 10000

Method CORE pooled

Method CORE pooled

MISCLASS. RATE (IN %)

MISCLASS. RATE (IN %)

c: 100 40

c: 200

c: 500 c: 1000

c: 100 40

c: 200

c: 500

c: 1000 c: 2000 c: 5000

30 30

20 20

10 10

00

Tr Te1Te2 Tr Te1Te2 Tr Te1Te2 Tr Te1Te2
Dataset

Tr Te1Te2 Tr Te1Te2 Tr Te1Te2 Tr Te1Te2 Tr Te1Te2 Tr Te1Te2
Dataset

Figure C.8: Data augmentation setting: Misclassification rates for MNIST and S  rotation. In test set 1 all digits are rotated by a degree randomly sampled from [35, 70]. Test set 2 is the usual MNIST test set.

sample sizes ensure that fewer confounding factors are present in the training data and exploited by the pooled estimator.
C.3 GROUPING AUGMENTED IMAGES BY ORIGINAL: MORE SAMPLE EFFICIENT
Here, we show further results for the experiment introduced in §2.2. We vary the number of augmented training examples c from 100 to 5000 for n = 10000 and c  {100, 200, 500, 1000} for n = 1000. The degree of the rotations is sampled uniformly at random from [35, 70]. Figure C.8 shows the misclassification rates. Test set 1 contains rotated digits only, test set 2 is the usual MNIST test set. We see that the misclassification rates of CORE are always lower on test set 1, showing that it makes data augmentation more efficient. For n = 1000, it even turns out to be beneficial for performance on test set 2.
20

Under review as a conference paper at ICLR 2018

(a) Examples from test sets 1­3.

(b) Misclassification rates.

c: 20

c: 500

c: 2000

MISCLASS. RATE (IN %)

50

40
Method
30 CORE
20 pooled

10

0 Tr Te1 Te2 Te3

Tr Te1 Te2 Te3
Dataset

Tr Te1 Te2 Te3

Figure C.10: a) Examples from the stickmen test set 1 (row 1), test set 2 (row 2) and test sets 3 (row 3). In each row, the first three images from the left have y  child; the remaining three images have y  adult. Connected images are counterfactual examples. b) Misclassification rates for different numbers of counterfactual examples.

C.4 STICKMEN IMAGE-BASED AGE CLASSIFICATION place of observation H

adult/child Y

interventions 

height C

movement S()

image X() f Y^ (X())
Figure C.9: Data generating process for the stickmen example.
Here, we show further results for the experiment introduced in §5.1. Figure C.10b shows results for different numbers of counterfactual examples. For c = 20 the misclassification rate of CORE estimator has a large variance. For c  {50, 500, 2000}, the CORE estimator shows similar results. Its performance is thus not sensitive to the number of counterfactual examples, once there are sufficiently many counterfactual observations in the training set. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3 as it seems to use "movement" as a predictor for "age".
C.5 EYEGLASSES DETECTION: IMAGE QUALITY INTERVENTION
Here, we show further results for the experiment introduced in §5.2. Specifically, we consider interventions of different strengths by varying the mean of the quality intervention in µ  {30, 40, 50}. As in §B.2, we use ImageMagick, this time to modify the image quality. In the training set and in test set 1, we sample the image quality value as qi,j  N (µ,  = 10) and apply the command convert -quality q ij input.jpg output.jpg if yi  glasses. If yi  no glasses, the image is not modified. In test set 2, the above command is applied if yi  no glasses while images with yi  glasses are not changed. In test set 3 all images are left unchanged and in test set 4 the command is applied to all images, i.e. the quality of all images is reduced.
We run experiments for counterfactual settings 1­3 and for c = 5000. Figure C.11 shows examples from the respective training and test sets and Figure C.12 shows the corresponding misclassification rates. Again, we observe that counterfactual setting 1 works best while there are only small differ-
21

Under review as a conference paper at ICLR 2018

(a) CF setting 1, µ = 50

(b) CF setting 1, µ = 40

(c) CF setting 1, µ = 30

(d) CF setting 2, µ = 50

(e) CF setting 2, µ = 40

(f) CF setting 2, µ = 30

(g) CF setting 3, µ = 50

(h) CF setting 3, µ = 40

(i) CF setting 3, µ = 30

Figure C.11: Examples from the CelebA image quality datasets, counterfactual settings 1­3 with µ  {30, 40, 50}. In all rows, the first three images from the left have y  no glasses; the remaining three images have y  glasses. Connected images are counterfactual examples. In panels (a)­(c), row 1 shows examples from the training set, rows 2­4 contain examples from test sets 2­4, respectively. Panels (d)­(i) show examples from the respective training sets.

(a) CF setting 1

(b) CF setting 2

(c) CF setting 3

MISCLASS. RATE (IN %) MISCLASS. RATE (IN %) MISCLASS. RATE (IN %)

Method CORE pooled

mean: 30 70

mean: 40

mean: 50

60

50

40

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

Method CORE pooled

mean: 30 70

mean: 40

mean: 50

60

50

40

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

Method CORE pooled

mean: 30 70

mean: 40

mean: 50

60

50

40

30

20

10

0 Tr Te1 Te2 Te3 Te4

Tr Te1 Te2 Te3 Te4
Dataset

Tr Te1 Te2 Te3 Te4

Figure C.12: Misclassification rates for the CelebA image quality datasets, counterfactual settings 1­3 with c = 5000 and the mean of the Gaussian distribution µ  {30, 40, 50}.

ences in predictive performance between counterfactual settings 2 and 3. Interestingly, there is a large performance difference between µ = 40 and µ = 50 for the pooled estimator. Possibly, with µ = 50 the image quality is not sufficiently predictive for the target.
C.6 ELMER THE ELEPHANT
The color interventions for the experiment introduced in §5.3 are created as follows. In the training set, if yi  elephant we apply the following ImageMagick command only for the counterfactual examples convert -modulate 100,0,100 input.jpg output.jpg, producing a grayscale image. In test set 1, all images are left unchanged. In test set 2, the above command is applied if yi  horse; if yi  elephant we sample ci,j  N (µ = 20,  = 1) and apply convert -modulate 100,100,100-c ij input.jpg output.jpg to the image. In test set 4, the
22

Under review as a conference paper at ICLR 2018

Figure C.13: Examples from the subsampled and augmented AwA2 dataset. Row 1 shows examples from the training set, rows 2­5 show examples from test sets 1­4, respectively.
latter command is applied to all images. It rotates the colors of the image, in a cyclic manner9. In test set 3, all images are changed to grayscale.
observation place H

animal Y

interventions 

C color S()
image X() f Y^ (X())
Figure C.14: Data generating process for the Elmer the elephant example.
C.7 NETWORK ARCHITECTURES
We implemented the considered models in TensorFlow (Abadi et al., 2015). The model architectures used are detailed in Table C.1. CORE and the pooled estimator thus use the same network architecture and training procedure; merely the loss function differs by the counterfactual regularization term. In all experiments we use the Adam optimizer (Kingma & Ba, 2015). All experimental results are based on training the respective model five times (using the same data) to assess the variance due to the randomness in the training procedure. In each epoch of the training, the training data xi,·, i = 1, . . . , n is randomly shuffled, keeping the counterfactual observations xi,j, j = 1, . . . , mi together to ensure that mini batches will contain counterfactual observations. In all experiments the mini batch size is set to 120. For small c this implies that not all mini batches contain counterfactual observations, making the optimization more challenging.

9For more details, see http://www.imagemagick.org/Usage/color_mods/#color_mods. 23

Under review as a conference paper at ICLR 2018

Dataset MNIST
Stickmen
CelebA (all experiments using CelebA) AwA2

Optimizer Adam
Adam
Adam
Adam

Input CNN
Input CNN
Input CNN
Input CNN

Architecture
28 × 28 × 1 Conv 5 × 5 × 16, 5 × 5 × 32 (same padding, strides = 2, ReLu activation), fully connected, softmax layer
64 × 64 × 1 Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × 128 (same padding, strides = 2, leaky ReLu activation), fully connected, softmax layer
64 × 48 × 3 Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × 128 (same padding, strides = 2, leaky ReLu activation), fully connected, softmax layer
32 × 32 × 3 Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × 128 (same padding, strides = 2, leaky ReLu activation), fully connected, softmax layer

Table C.1: Details of the model architectures used.

24

