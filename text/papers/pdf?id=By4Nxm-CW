Under review as a conference paper at ICLR 2018
DO CONVOLUTIONAL NEURAL NETWORKS ACT AS COMPOSITIONAL NEAREST NEIGHBORS?
Anonymous authors Paper under double-blind review

Input

Convolutional Neural Networks
(a) Labels-to-Facades

Compositional Nearest Neighbors

Input

Convolutional

Compositional

Neural Networks Nearest Neighbors

(b) Facades-to-Labels

Input

Convolutional Neural Networks

Compositional Nearest Neighbors

Input

(c) Images-to-Labels

Convolutional

Compositional

Neural Networks Nearest Neighbors

Figure 1: We propose a non-parametric method to explain and modify the behavior of convolutional networks, including those that classify pixels and those that generate images. For example, given the label mask on the top-left, why does a network generate strange gray border artifacts? Given the image on the bottom-right, how is a network able to segment out the barely-visible lamppost? We show that the output of such networks can be explained through a simple compositional nearestneighbor operation. On the right of every image triple, we show the output obtained by simply (1) matching input patches to those in the training set and (2) returning a corresponding output label. This means that the output is created by cutting-and-pasting (composing) patches of training images. Such an operation can readily explain the strange behavior - the border artifact and lamppost were matched to such output labels in the training set! Importantly, non-parametric matching does not require any complex feedforward operations given a suitable embedding for comparing patches. We provide evidence that networks implicitly learn such embeddings at all layers of a network. Such a perspective allows one to "explain" errors and modify the biases of a network by changing the set of image patches used for non-parametric matching.

ABSTRACT

We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and imageto-image translation show qualitative and quantitative evidence supporting this hypothesis.

1

Under review as a conference paper at ICLR 2018
1 INTRODUCTION
Convolutional neural networks (CNNs) have revolutionized computer vision, producing impressive results for discriminative tasks such as image classification and semantic segmentation. More recently, they have also produced startlingly impressive results for image generation through generative models. However, in both cases, such feedforward networks largely operate as "black boxes". As a community, we are still not able to succinctly state why and how such feedforward functions generate a particular output from a given input. If a network fails on a particular input, why? How will a network behave on never-before-seen data? To answer such questions, there is a renewed interest in so-called explainable AI1. The central goal in this (re)invigorated space is the development of machine learning systems that are designed to be more interpretable and explanatory.
Interpreting networks: Rather than redesigning a network to be more interpretable, a complementary approach is to post-hoc add interpretability to an existing network. Deep hierarchical features dramatically outperform manually-designed hierarchies (typically based upon edges, parts, objects (Marr, 1982)), so it would be valuable to understand what non-obvious hierarchical concepts are being learned by such high-performing deep networks. What did classic visual hierarchies fail to capture? An illustrative example of such reasoning might be post-hoc interpretation of the AlphaGo network (Silver et al., 2016); while an automated Go player itself is certainly interesting, what maybe more so is understanding the novel game heuristics (unknown to current experts) learned by the network.
Spatial prediction: In this work, we focus on the class of CNNs designed to make predictions at each image pixel. Many problems in computer vision can be cast in this framework, from semantic segmentation to depth estimation to image synthesis/translation. We explore a simple hypothesis for explaining the behaviour of such networks: they operate by cutting-and-pasting (composing) image patches found in training data. Consider Fig. 1, where we visualize the output of Isola et al. (2016)'s translation network trained to synthesize images of building facades from label masks. Why does the network generate the strange diagonal gray edge at the top? To answer this question, we visualize image pixels extracted from the closest-matching nearest-neighbor (NN) patches found in the training data. Remarkably, NN-synthesis looks quite similar to the CNN output, providing a clear explanation for the synthesized corner artifact.
Compositional embeddings: Our central thesis is consistent with recent work on network memorization (Zhang et al., 2016), but notably, naive memorization fails to explain how/why networks generalize to never-before-seen data. We explain the latter through composition: the synthesized output in Fig. 1 consists of windows copied from different training images. But crucially, these windows are globally consistent in that they have similar aesthetic styles. This requires a carefullytuned metric for comparing patches that can encode such global knowledge. But where do we obtain such a metric? Devlin et al. (2015a) demonstrate that the penultimate ("FC7") layer of classification networks learn semantic embeddings that can be used to compare images with surprising accuracy. We apply this observation to spatial prediction networks, to learn semantic embeddings of image patches. Interestingly, we show that such an interpretation also holds for the internal layers of a network- e.g., one can "explain" the activations in a layer by matching to training patches using embeddings computed from the previous layer. This may suggest a novel perspective of CNNs as learning compositions of embeddings.
Correspondence and bias: Beyond being a mechanism for interpretation, we demonstrate that compositional NN matching is a viable algorithm that can rival the accuracy of highly-tuned CNNs. Although slower than a feedforward net, compositional matching is attractive in two respects: (1) It provides spatial correspondences between pixels in the predicted output and pixels in the training set. This may be useful in some applications such as label transfer (Liu et al., 2011). (2) Implicit biases of the network can be explicitly manipulated by changing the set of images used for matching ­ it need not be the same set used for training the network. As an illustrative example, we can force a pre-trained translation network to predict European or American building facades by restricting the set of images used for matching. Such a manipulation may, for example, be used to modify a biased face recognition network to process genders and races in a more egalitarian fashion.
1http://www.darpa.mil/program/explainable-artificial-intelligence
2

Under review as a conference paper at ICLR 2018
Contribution: We introduce compositional NN embeddings for interpreting discriminative and generative CNNs. Specifically, we demonstrate that CNNs appear to work by memorizing image patches from training data, and then composing them into new configurations. To make compositional matching viable, CNNs learn an embedding of image patches that captures both global and local semantics. We validate our hypothesis on state-of-the-art networks for image translation and semantic image segmentation. Finally, we also show evidence that compositional matching can be used to generate spatial correspondences and manipulate the implicitly-learned biases of a network.
2 RELATED WORK
We broadly classify networks for spatial prediction into two categories: (1) discriminative prediction, where one is seeking to infer high-level semantic information from RGB values and (2) generative synthesis, where the intent is to synthesize a new image from a given input "prior". There is a broad literature for each of these tasks, and here we discuss the ones most relevant to ours.
Discriminative Models: An influential formulation for state-of-the-art spatial prediction tasks is that of fully convolutional networks (Long et al., 2015). These have been used for pixel prediction problems such as semantic segmentation (Long et al., 2015; Hariharan et al., 2015; Ronneberger et al., 2015; Bansal et al., 2017a; Chen et al., 2016), depth/surface-normal estimation (Bansal et al., 2016; Eigen & Fergus, 2015), or low-level edge detection (Xie & Tu, 2015; Bansal et al., 2017a). A substantial progress has been made to improve the performance by employing deeper architectures (He et al., 2015), or increasing the capacity of the models (Bansal et al., 2017a), or utilizing skip connections, or intermediate supervision (Xie & Tu, 2015). However, we do not precisely know what these models are actually capturing to do pixel-level prediction. In the race for better performance, the interpretability of these models has been typically ignored. In this work, we take a first step to analyze the prediction space of these discriminative models. In this work, we focus on analyzing encoder-decoder architectures for spatial classification Ronneberger et al. (2015).
Generative Models: Goodfellow et al. (2014) proposed a two-player minmax formulation where a generator G synthesized an image from random noise z, and a discriminator (D) is used to distinguish the generated images from the real images. While this Generative Adversarial Network (GAN) formulation was originally proposed to synthesize an image from random noise vectors z, this formulation could also be used synthesize new images from other priors such as a low resolution image or label mask by treating z as an explicit input to be conditioned upon. This conditional image synthesis via generative adversarial formulation has been well utilized by multiple follow-up works to synthesize a new image conditioned on a low-resolution image (Denton et al., 2015), class labels (Radford et al., 2015), and other inputs (Isola et al., 2016; Zhu et al., 2017). While the quality of synthesis from different inputs has rapidly improved in recent history, interpretation of GANs has been relatively unexplored. In this work, we examine the influential Pix-to-Pix network Isola et al. (2016) and demonstrate an intuitive non-parametric representation for explaining its impressive results.
Interpretability & Nearest Neighbors: There is a substantial body of work (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2015; Zhou et al., 2014; Bau et al., 2017) on interpreting general convolutional neural networks (CNNs). The earlier work of (Zeiler & Fergus, 2014) presented an approach to understand and visualize the functioning of intermediate layers of CNN. Mahendran & Vedaldi (2015) proposed to invert deep features to visualize what is learnt by CNNs, similar to inverting HOG features to understand object detection (Vondrick et al., 2013). Zhou et al. (2014) demonstrated that object detectors automatically pops up while learning the representation for scene categories. Krishnan & Ramanan (2016) explored interactive modifcation of a pre-trained network to learn novel concepts, and recently (Bau et al., 2017) proposed to quantify interpretability by measuring scene semantics such as objects, parts, texture, material etc. Despite this, understanding the space of pixel-level CNNs is not well studied. The recent work of PixelNN (Bansal et al., 2017b) focuses on high-quality image synthesis by making use of a two-stage matching process that begins by feedforward CNN processing and ends with a nonparametric matching of high-frequency detail. We differ in our focus on interpretability rather than image synthesis, our examination of networks for both discriminative classification and generative synthesis, and our simpler single-stage matching process that does not require feedforward processing.
3

Under review as a conference paper at ICLR 2018

Compositionality & CNNs: The design of part-based models (Crandall et al., 2005; Felzenszwalb et al., 2008), pictorial structures or spring-like connections (Fischler & Elschlager, 1973; Felzenszwalb & Huttenlocher, 2005), star-constellation models (Weber et al., 2000; Fergus et al., 2003), and the recent works using CNNs share a common theme of compositionality. While the earlier works explicitly enforce the idea of composing different parts for object recognition in the algorithmic formulation, there have been suggestions that CNNs also take a compositional approach (Zeiler & Fergus, 2014; Krishnan & Ramanan, 2016; Bau et al., 2017). In this work, we explicitly use a nearest neighbors pipeline to strongly suggest that Convolutional NeuralNetworks behave as Compositional Nearest Neighbors!

3 COMPOSITIONAL NEAREST NEIGHBORS

We now introduce our method to interpret various fully convolutional networks designed for pixellevel tasks.

Global Nearest-Neighbor Embeddings: Our starting point is the observation that classification networks can be interpreted as linear classifiers defined on nonlinear features extracted from the penultimate layer (e.g., "FC7" features) of the network. We formalize this perspective with the following notation:

Label(x) = k where k = argmax wk · (x),
k{1...K }

[K-way classification]

(1)

where (x)  RN corresponds to the penultimate FC7 features computed from input image x. Typically, the parameters of the linear classifier {wy} and those of the feature encoder (·) are trained on large-scale supervised datasets:

D = {(xn, yn)} [Training database]

(2)

Devlin et al. (2015b) make the observation that penultimate features (x) can be interpreted as an embedding in RN . By extracting such embeddings for training images xn, Devlin et al. (2015b) build a nonparametric nearest-neighbor (NN) predictor for complex tasks such as image captioning.
We write this as follows:

Label(x) = yn where n = argmin Dist (x), (xn)
n

[NN classification] (3)

Importantly, the above NN classifier performs quite well even when feature encoders (·) are trained for classification rather than as an explicit embedding. In some sense, deep nets seem to implicitly learn embeddings upon which simple linear classifiers (or regressors) operate. We argue that such a NN perspective is useful in interpreting the predicted classification since the corresponding training example can be seen as a visual "explanation" of the prediction - e.g., the predicted label for x is "dog" because x looks similar to training image xn.

Pixel-wise Nearest-Neighbor Embeddings: We now extend the above insight to pixel-prediction networks that return back a prediction for each pixel i in an image:

Labeli(x) = k where k = argmax wk · i(x)
k{1...K }

[K-way pixel classification] (4)

where we write Labeli(·) for the label of the ith pixel and i(·) for its corresponding feature vector. Because we will also examine pixel-level prediction networks trained to output a continuous value,
write out the formulation for pixel-level regression:

Predicti(x) = W i(x) where W  RM×N [Pixel regression]

(5)

For concreteness, consider a Pix2Pix (Isola et al., 2016) network trained to regress RGB values at
each pixel location. These predictions are obtained by convolving features from the penultimate layer with filters of size 4 × 4 × 128. In this case, the three filters that generate R,G, and B values can be written as a matrix W of size M × N , where M = 3 and N = 4  4  128 = 2048. Analogously, i(x) corresponds to N dimensional features extracted by reshaping local 4 × 4 convolutional neighborhoods of features from the penultimate feature map (of size H × W × 128). We
now can perform nearest-neighbor regression to output pixel values:

Predicti(x) = yn,m where (n, m) = argmin Dist i(x), m(xn)
n,m

[NN pixel regression]

4

Under review as a conference paper at ICLR 2018

Input: Label

Nearest Neighbor Search

Input: Image

Nearest Neighbor Search

Output: Image

Label-Image Pair in Training Set

Output: Label

Image-Label Pair in Training Set

Figure 2: Overview of Pipeline: Given an input label/image on the top-left, our approach extracts an embedding for each pixel. We visualize two pixels with yellow and white dot. The embedding captures global context, which we visualize with the surrounding rectangular box. We then find the closest matching patches in the training set (with a nearest neighbor search), and then report back the corresponding pixel labels to generate the final output (bottom-left). We visualize an example for label-to-image synthesis on the left, and image-to-label prediction on the right.

Pix-to-Pix

NN-Embedding

Pix-to-Pix

NN-Embedding

Pix-to-Pix

NN-Embedding

Figure 3: Results on interior layers, the layers we evaluate here is decoder4 to decoder3 from Pixto-Pix architecture. Here, each image represents a feature map of 3 continuous channels from the whole 256 channels. This results indicate that our embedding interpretation can be applied to each layer and a more generalized deep network.

where yn,m refers to the mth pixel from the nth training image. Importantly, pixel-level nearest neighbors reveals spatial correspondences for each output pixel. We demonstrate that these can be used to provide an intuitive explanation of pixel outputs, including an explanation of errors that otherwise seem quite mysterious (see Fig.1 and Fig. 6).
Nearest-Neighbor Embeddings of Interior Layers: We now extend our embedding interpretation of neural nets to internal layers. Recall that activations a at a spatial position i and layer j can be computed using thresholded linear functions of features (activations) from the previous layer:

aij(x) = max 0, W ij(x) ,

where aij  RM , ij  RN , W  RM×N

(6)

where we write aij for the vector of activations corresponding to the ith pixel position from layer j, possibly computed with bilinear interpolation (Long et al., 2015). We write ij for the local convolutional neighborhood of features (activations) from the previous layer that are linearly com-
bined with bank of linear filters W to produce aij. For concreteness, consider layer 6 of Pix2Pix network. Here, ij  RN where N = 4  4  1024 = 16384. We similarly posit that one can produce approximate activations by nearest neighbors. Specifically, let us run Pix2Pix on the set of

5

Under review as a conference paper at ICLR 2018

Input

Pix-to-Pix NN-Embedding Ground Truth

Input

SegNet NN-Embedding Ground Truth

Figure 4: Semantic Segmentation: The results of semantic segmentation on cityscape and CamVid dataset. This result suggests the following observations. First, the difference between images generated by generative networks (Pix-to-Pix and SegNet columns) and NN embedding is surprisingly small. Thus, our method can perfectly interpret discriminative deep networks on pixel-level classification. Second, we can notice some noise edges with a high gradient (see columns 5-8). This phenomenon can also be used to understand the difficulty of image segmentation task: borders with high gradients are usually hard to classify due to the ambiguous patches in training set.

training images, and construct a dataset of training patches with features ij(xn) and corresponding activation vectors aij(xn). We can then predict activation maps for a query image x with NN:
aij(x) = am,j(xn ) where (m, n) = argmin Dist ij(x), mj(xn) [NN activations]
m,n

We show that such an approach actually produces reasonable activations, which suggests one novel
interpretation of neural networks as learning hierarchies of embeddings (Fig. 3). An important special case is the bottleneck feature, which is computed from an activation map of size 1 × 1 × 512. In this case, we posit that the corresponding feature ij(x)  R512 is a good global descriptor of image x. In our experiments, we found that such a global descriptor can be used to prune the
NN pixel search, significantly speeding up run-time performance (e.g., we first prune the training
database to a shortlist of images with similar bottleneck features, and then search through these
images for similar patches).

Bias modification: Finally, our results suggest that the matching database from Eq.(2) serves as
an explicit "associative memory" of a network (Carpenter, 1989). We can explicitly modify the memory by changing the dataset of training images {xn}, labels {yn}, or both. We experiment with various modifications in our experimental results. One modification that consistently produced
smoother visual results was to refine the training labels to those predicted by a network:

xn, yn  { xn, CN N (xn)

(7)

Such as procedure for "self-supervised" learning is sometimes used when training labels are known to be noisy. From an associative network perspective, we posit that such labels represent a more faithful representation of its internal memory. Unless otherwise specified, all results make use of the above matching database.

4 EXPERIMENTS
We now present experimental results for discriminative networks trained for semantic segmentation, as well as generative networks trained for image synthesis.
Networks: We use SegNet (Badrinarayanan et al., 2017), a recent state-of-the-art network for image segmentation, and Pix2Pix (Isola et al., 2016), a state-of-the-art network for conditional image

6

Under review as a conference paper at ICLR 2018

Input

Pix-to-Pix

NN-Embedding Original Image

Input

Pix-to-Pix NN-Embedding Original Image

Figure 5: Architectural Labels to Facades: This figure shows the results for image synthesis. The results suggest that our approach can also explain the results from Pix-to-Pix. We conclude this because our approach reproduces color and the structure of the Pix-to-Pix output, including a few artifacts (e.g., the image cuts in the 6th and 7th columns).

synthesis and translation. We evaluate our findings for multiple datasets/tasks on which the original networks were trained. These include tasks such as synthesizing facades from architectural labels and vice versa, predicting segmentation class labels from urban RGB images, and synthesizing google maps from aerial/satellite views.
Semantic Segmentation: We use the CityScape (Cordts et al., 2016) and CamVid (Brostow et al., 2008b;a) dataset for the task of semantic segmentation. Both datasets are annotated with semantic class labels for outdoor images collected from a car driving on the road. We use SegNet (Badrinarayanan et al., 2017) for CamVid sequences, and Pix-to-Pix (Isola et al., 2016) for CityScape dataset. Figure 4 qualitatively shows results for all datasets.
Architectural Labels to Facades: We followed Isola et al. (2016) for this setting that use the annotations from (Radim Tylecek, 2013). There are 400 training images in this dataset, and 100 images in validation set. We use the same dataset to generate architectural labels from images of facades. Pix-to-Pix (Isola et al., 2016) models are trained using 400 images in training set for both labelsto-facades and vice versa. Figure 5 qualitatively show examples for synthesizing real-world images using pixel-wise nearest neighbor embedding. We observe that NN-embedding perfectly explains the generation of deformed edges, and how the generative architecture are eventually memorizing patches from training set.
Satellite to Maps: This dataset contains 1096 training images and 1000 images in test set scraped from Google Maps. We use the same settings as set up by Isola et al. (2016) for this task. Figure 5 qualitatively shows in its third row how google maps are synthesized from satellite. We can observe that our synthesis is nearly identical to the image generated by the network.

Approach

Facades-to-Labels CityScape CamVid

Baseline CNN CompNN

0.441 0.480

0.738 0.724

0.786 0.798

Table 1: Quantitative Evaluation: We compare our compositional nearest neighbor approach to baseline networks trained on the same training/matching database. We specifically compare to (Isola et al., 2016) for Facades and CitiScape, and compare with (Badrinarayanan et al., 2017) for CamVid. Our results are quite competitive, and sometimes even outperform the baseline network.

7

Under review as a conference paper at ICLR 2018

Labels

Pix-to-Pix

Controllable Synthesis via Nearest Neighbors

Figure 6: Bias modification: Given the same label input, we show different results obtained by matching to different databases. By modifying the database to include specific buildings from specific locations, one can introduce and remove implicit biases in the original network (e.g., generate "european" facades versus "american").

Quantitative Evaluation: We now present the quantitative analysis of our pixel-wise nearest neighbor approach with an end-to-end pipeline. We report classification accuracy of ground truth labels compared to the predicted labels for the task of semantic segmentation. Our results are competitive, and sometimes even outperform the baseline network.
5 DISCUSSION
In this paper, we have presented a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of convolutional neural networks for spatial prediction tasks. Our analysis suggests that CNNs behave as compositional nearest neighbor operators over a training set of patch-label pairs that act as an associative memory. But beyond simply memorizing, CNNs can generalize to novel data by composing together local patches from different training instances. Our analysis allows for example-based explanations of network behaviour, as well as explicit modulation of the implicit biases learned by the network. We hope our framework allows for further analysis of convolutional networks from a non-parametric perspective.
REFERENCES
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for scene segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.
Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr Revisited: 2D-3D model alignment via surface normal prediction. In CVPR, 2016.
Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, and Deva Ramanan. PixelNet: Representation of the pixels, by the pixels, and for the pixels. arXiv:1702.06506, 2017a.
Aayush Bansal, Yaser Sheikh, and Deva Ramanan. PixelNN: Example-based image synthesis. CoRR, abs/1708.05349, 2017b.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. CoRR, abs/1704.05796, 2017.
Gabriel J. Brostow, Julien Fauqueur, and Roberto Cipolla. Semantic object classes in video: A high-definition ground truth database. Pattern Recognition Letters, xx(x):xx­xx, 2008a.
Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition using structure from motion point clouds. In ECCV (1), pp. 44­57, 2008b.
Gail A Carpenter. Neural network models for pattern recognition and associative memory. Neural networks, 2 (4):243­257, 1989.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. CoRR, abs/1606.00915, 2016.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

8

Under review as a conference paper at ICLR 2018
David Crandall, Pedro Felzenszwalb, and Daniel Huttenlocher. Spatial priors for part-based recognition using statistical models. In CVPR, 2005.
Emily L. Denton, Soumith Chintala, Arthur Szlam, and Robert Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. CoRR, abs/1506.05751, 2015.
Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, and C Lawrence Zitnick. Exploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467, 2015a.
Jacob Devlin, Saurabh Gupta, Ross B. Girshick, Margaret Mitchell, and C. Lawrence Zitnick. Exploring nearest neighbor approaches for image captioning. CoRR, abs/1505.04467, 2015b.
David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV, 2015.
Pedro Felzenszwalb, David McAllester, and Deva Ramanan. A discriminatively trained, multiscale, deformable part model. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.
Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Pictorial structures for object recognition. International Journal of Computer Vision, 2005.
R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. In In CVPR, 2003.
M. A. Fischler and R. A. Elschlager. The representation and matching of pictorial structures. IEEE Trans. Comput., 22(1), January 1973.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661, 2014.
Bharath Hariharan, Pablo Arbela´ez, Ross Girshick, and Jitendra Malik. Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arxiv, 2016.
Vivek Krishnan and Deva Ramanan. Tinkering under the hood: Interactive zero-shot learning with net surgery. CoRR, abs/1612.04901, 2016.
Ce Liu, Jenny Yuen, and Antonio Torralba. Nonparametric scene parsing via label transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(12):2368­2382, 2011.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional models for semantic segmentation. In CVPR, 2015.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
David Marr. Vision: A computational investigation into the human representation and processing of visual information. 1982.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015.
Radim S a´ra Radim Tylecek. Spatial pattern templates for recognition of objects with regular structure. In Proc. GCPR, Saarbrucken, Germany, 2013.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles: Visualizing Object Detection Features. ICCV, 2013.
9

Under review as a conference paper at ICLR 2018 Markus Weber, Max Welling, and Pietro Perona. Towards automatic discovery of object categories. In CVPR.
IEEE, 2000. Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In ICCV, 2015. Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, pp.
818­833, 2014. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Bolei Zhou, Aditya Khosla, A` gata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in
deep scene cnns. In ICLR, 2014. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017.
10

