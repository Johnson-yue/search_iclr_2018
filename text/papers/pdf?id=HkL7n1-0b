Under review as a conference paper at ICLR 2018
WASSERSTEIN AUTO-ENCODERS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose the Wasserstein Auto-Encoder (WAE)--a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) (Kingma & Welling, 2014). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE) (Makhzani et al., 2016). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.
1 INTRODUCTION
The field of representation learning was initially driven by supervised approaches, with impressive results using large labelled datasets. Unsupervised generative modeling, in contrast, used to be a domain governed by probabilistic approaches focusing on low-dimensional data. Recent years have seen a convergence of those two approaches. In the new field that formed at the intersection, variational auto-encoders (VAEs) (Kingma & Welling, 2014) constitute one well-established approach, theoretically elegant yet with the drawback that they tend to generate blurry samples when applied to natural images. In contrast, generative adversarial networks (GANs) (Goodfellow et al., 2014) turned out to be more impressive in terms of the visual quality of images sampled from the model, but come without an encoder, have been reported harder to train, and suffer from the "mode collapse" problem where the resulting model is unable to capture all the variability in the true data distribution. There has been a flurry of activity in assaying numerous configurations of GANs as well as combinations of VAEs and GANs. A unifying framework combining the best of GANs and VAEs in a principled way is yet to be discovered.
Following Arjovsky et al. (2017), we approach generative modeling from the optimal transport (OT) point of view. The OT cost (Villani, 2003) is a way to measure a distance between probability distributions and provides a much weaker topology than many others, including f -divergences associated with the original GAN algorithms (Nowozin et al., 2016). This is particularly important in applications, where data is usually supported on low dimensional manifolds in the input space X . As a result, stronger notions of distances (such as f -divergences, which capture the density ratio between distributions) often max out, providing no useful gradients for training. In contrast, OT was claimed to have a nicer behaviour (Arjovsky et al., 2017; Gulrajani et al., 2017) although it requires, in its GAN-like implementation, the addition of a constraint or a regularization term into the objective.
In this work we aim at minimizing OT Wc(PX , PG) between the true (but unknown) data distribution PX and a latent variable model PG specified by the prior distribution PZ of latent codes Z  Z and the generative model PG(X|Z) of the data points X  X given Z. Our main contributions are listed below (cf. also Figure 1):
· A new family of regularized auto-encoders (Algorithms 1, 2 and Eq. 4), which we call Wasserstein Auto-Encoders (WAE), that minimize the optimal transport Wc(PX , PG) for any cost function c. Similarly to VAE, the objective of WAE is composed of two terms: the c-reconstruction cost and a regularizer DZ (PZ , QZ ) penalizing a discrepancy between two distributions in Z: PZ and a distribution of encoded data points, i.e. QZ := EPX [Q(Z|X)].
1

Under review as a conference paper at ICLR 2018

PG (X |Z )

(a) VAE PZ Z
QVAE (Z |X )

X

(b) WAE

PZ PG (X |Z )

QZ Z QWAE (Z |X )

X

VAE reconstruction

WAE reconstruction

Figure 1: Both VAE and WAE minimize two terms: the reconstruction cost and the regularizer penalizing discrepancy between PZ and distribution induced by the encoder Q. VAE forces Q(Z|X = x) to match PZ for all the different input examples x drawn from PX . This is illustrated on picture (a), where every single red ball is forced to match PZ depicted as the white shape. Red balls start intersecting, which leads to problems with reconstruction. In contrast, WAE forces the continuous mixture QZ := Q(Z|X)dPX to match PZ , as depicted with the green ball in picture (b). As a result latent codes of different examples get a chance to stay far away from each other, promoting a
better reconstruction.

When c is the squared cost and DZ is the GAN objective, WAE coincides with adversarial auto-encoders of Makhzani et al. (2016).
· Empirical evaluation of WAE on MNIST and CelebA datasets with squared cost c(x, y) = x - y 22. Our experiments show that WAE keeps the good properties of VAEs (stable
training, encoder-decoder architecture, and a nice latent manifold structure) while generating samples of better quality, approaching those of GANs.
· We propose and examine two different regularizers DZ (PZ , QZ ). One is based on GANs and adversarial training in the latent space Z. The other uses the maximum mean discrepancy, which is known to perform well when matching high-dimensional standard normal distributions PZ (Gretton et al., 2012). Importantly, the second option leads to a fully adversary-free min-min optimization problem.
· Finally, the theoretical considerations used to derive the WAE objective might be interesting in their own right. We prove in particular (Theorem 1) that in the case of generative models, the primal form of Wc(PX , PG) is equivalent to a problem involving the optimization of a probabilistic encoder Q(Z|X) .
The paper is structured as follows. In Section 2 we derive a novel auto-encoder formulation for OT between PX and the latent variable model PG. Relaxing the resulting constrained optimization problem we arrive at an objective of Wasserstein auto-encoders. We propose two different regularizers, leading to WAE-GAN and WAE-MMD algorithms. Section 3 discusses the related work. We present the experimental results in Section 4 and conclude by pointing out some promising directions for future work.
2 PROPOSED METHOD
Our new method minimizes the optimal transport cost Wc(PX , PG) based on the novel auto-encoder formulation derived in Theorem 1. In the resulting optimization problem the decoder tries to accurately reconstruct the encoded training examples as measured by the cost function c. The encoder tries to simultaneously achieve two conflicting goals: it tries to match the encoded distribution of training examples QZ := EPX [Q(Z|X)] to the prior PZ as measured by any specified divergence DZ (QZ , PZ ), while making sure that the latent codes provided to the decoder are informative enough to reconstruct the encoded training examples. This is schematically depicted on Fig. 1.
2

Under review as a conference paper at ICLR 2018

2.1 PRELIMINARIES AND NOTATIONS

We use calligraphic letters (i.e. X ) for sets, capital letters (i.e. X) for random variables, and

lower case letters (i.e. x) for their values. We denote probability distributions with capital letters

(i.e. P (X)) and corresponding densities with lower case letters (i.e. p(x)). In this work we will

consider several measures of discrepancy between probability distributions PX and PG. The class

of f -divergences (Liese & Miescke, 2008) is defined by Df (PX PG) :=

f

pX (x) pG (x)

pG(x)dx,

where f : (0, )  R is any convex function satisfying f (1) = 0. Classical examples include the

Kullback-Leibler DKL and Jensen-Shannon DJS divergences.

2.2 OPTIMAL TRANSPORT AND ITS DUAL FORMULATIONS

A rich class of divergences between probability distributions is induced by the optimal transport (OT) problem (Villani, 2003). Kantorovich's formulation of the problem is given by

Wc(PX , PG) := inf E(X,Y )[c(X, Y )] ,
P(XPX ,Y PG)

(1)

where c(x, y) : X × X  R+ is any measurable cost function and P(X  PX , Y  PG) is a set of all joint distributions of (X, Y ) with marginals PX and PG respectively. A particularly interesting case is when (X , d) is a metric space and c(x, y) = dp(x, y) for p  1. In this case Wp, the p-th root of Wc, is called the p-Wasserstein distance.
When c(x, y) = d(x, y) the following Kantorovich-Rubinstein duality holds1:

W1(PX , PG) = sup EXPX [f (X)] - EY PG [f (Y )],
f FL
where FL is the class of all bounded 1-Lipschitz functions on (X , d).

(2)

2.3 APPLICATION TO GENERATIVE MODELS: WASSERSTEIN AUTO-ENCODERS
One way to look at modern generative models like VAEs and GANs is to postulate that they are trying to minimize certain discrepancy measures between the data distribution PX and the model PG. Unfortunately, most of the standard divergences known in the literature, including those listed above, are hard or even impossible to compute, especially when PX is unknown and PG is parametrized by deep neural networks. Previous research provides several tricks to address this issue.
In case of minimizing the KL-divergence DKL(PX , PG), or equivalently maximizing the marginal log-likelihood EPX [log pG(X)], the famous variational lower bound provides a theoretically grounded framework successfully employed by VAEs (Kingma & Welling, 2014; Mescheder et al., 2017). More generally, if the goal is to minimize the f -divergence Df (PX , PG) (with one example being DKL), one can resort to its dual formulation and make use of f -GANs and the adversarial training (Nowozin et al., 2016). Finally, OT cost Wc(PX , PG) is yet another option, which can be, thanks to the celebrated Kantorovich-Rubinstein duality (2), expressed as an adversarial objective as implemented by the Wasserstein-GAN (Arjovsky et al., 2017). We include an extended review of all these methods in Supplementary A.
In this work we will focus on latent variable models PG defined by a two-step procedure, where first a code Z is sampled from a fixed distribution PZ on a latent space Z and then Z is mapped to the image X  X = Rd with a (possibly random) transformation. This results in a density of the form

pG(x) := pG(x|z)pz(z)dz, x  X ,
Z

(3)

assuming all involved densities are properly defined. For simplicity we will focus on non-random
decoders, i.e. generative models PG(X|Z) deterministically mapping Z to X = G(Z) for a given map G : Z  X . In Supplementary B we present similar results for random decoders.

It turns out that under this model, the OT cost takes a simpler form as the transportation plan factors through the map G: instead of finding a coupling  in (1) between two random variables living in

1Note that the same symbol is used for Wp and Wc, but only p is a number and thus the above W1 refers to the 1-Wasserstein distance.

3

Under review as a conference paper at ICLR 2018

the X space, one distributed according to PX and the other one according to PG, it is sufficient to find a conditional distribution Q(Z|X) such that its Z marginal QZ (Z) := EXPX [Q(Z|X)] is identical to the prior distribution PZ. This is the content of our main theorem below.

Theorem 1 For any function G : Z  X we have

inf E(X,Y ) c X, Y
P(XPX ,Y PG)

=

Q:

inf
QZ =PZ

EPX

EQ(Z |X )

c X, G(Z)

,

where QZ is the marginal distribution of Z when X  PX and Z  Q(Z|X).

Proof The proof is reported in Supplementary B.

This result allows us to optimize over random encoders Q(Z|X) instead of optimizing over all couplings between X and Y . Of course, both problems are still constrained. In order to implement a numerical solution we relax the constraints on QZ by adding a penalty to the objective. This finally leads us to the WAE objective:

DWAE(PX ,

PG)

:=

inf
Q(Z |X )Q

EPX

EQ(Z |X )

c

X, G(Z)

+  · DZ (QZ , PZ ),

(4)

where Q is any nonparametric set of probabilistic encoders, DZ is an arbitrary divergence between QZ and PZ, and  > 0 is a hyperparameter. Similarly to VAE, we propose to use deep neural networks to parametrize both encoders Q and decoders G. Note that as opposed to VAEs, the WAE
formulation allows for non-random encoders deterministically mapping inputs to their latent codes.

We propose two different penalties DZ (QZ , PZ ):
GAN-based DZ . The first option is to choose DZ (QZ , PZ ) = DJS(QZ , PZ ) and use the adversarial training to estimate it. Specifically, we introduce an adversary (discriminator) in the latent space Z trying to separate2 "true" points sampled from PZ and "fake" ones sampled from QZ (Goodfellow et al., 2014). This results in the WAE-GAN described in Algorithm 1. Even though WAE-GAN falls back to the min-max problem, we move the adversary from the input (pixel) space X to the latent space Z. On top of that, PZ may have a nice shape with a single mode (for a Gaussian prior), in which case the task should be easier than matching an unknown, complex, and possibly multi-modal distributions as usually done in GANs. This is also a reason for our second penalty:

MMD-based DZ. For a positive-definite reproducing kernel k : Z × Z  R the following expression is called the maximum mean discrepancy (MMD):

MMDk(PZ , QZ ) =

k(z, ·)dPZ (z) -
Z

k(z, ·)dQZ (z)
Z

Hk ,

where Hk is the RKHS of real-valued functions mapping Z to R. If k is characteristic then MMDk defines a metric and can be used as a divergence measure. We propose to use DZ (PZ , QZ ) = MMDk(PZ , QZ ). Fortunately, MMD has an unbiased U-statistic estimator, which can be used in conjunction with stochastic gradient descent (SGD) methods. This results in the WAE-MMD
described in Algorithm 2. It is well known that the maximum mean discrepancy performs well
when matching high-dimensional standard normal distributions (Gretton et al., 2012) so we expect
this penalty to work especially well working with the Gaussian prior PZ.

3 RELATED WORK
Literature on auto-encoders Classical unregularized auto-encoders minimize only the reconstruction cost. This results in different training points being encoded into non-overlapping zones chaotically scattered all across the Z space with "holes" in between where the decoder mapping PG(X|Z) has never been trained. Overall, the encoder Q(Z|X) trained in this way does not provide a useful representation and sampling from the latent space Z becomes hard (Bengio et al., 2013).
Variational auto-encoders (Kingma & Welling, 2014) minimize a variational bound on the KL-divergence DKL(PX , PG) which is composed of the reconstruction cost plus
2We noticed that the famous "log trick" (also called "non saturating loss") proposed by Goodfellow et al. (2014) leads to better results.

4

Under review as a conference paper at ICLR 2018

EPX [DKL(Q(Z|X), PZ )] which captures how distinct the image by the encoder of each training example is from the prior PZ, which is not guaranteeing that the overall encoded distribution EPX [Q(Z|X)] matches PZ like WAE does. Also, VAEs require non-degenerate Gaussian encoders and random decoders for which log pG(x|z) can be computed and differentiated with respect to the
parameters. Later Mescheder et al. (2017) proposed a way to use VAE with non-Gaussian encoders.
WAE minimizes OT Wc(PX , PG) and allows both probabilistic and deterministic encoder-decoder
pairs of any kind.

ALGORITHM 1 Wasserstein Auto-Encoder ALGORITHM 2 Wasserstein Auto-Encoder

with GAN-based penalty (WAE-GAN).

with MMD-based penalty (WAE-MMD).

Require: Regularization coefficient  > 0. Initialize the parameters of the encoder Q, decoder G, and latent discriminator D . while (, ) not converged do Sample {x1, . . . , xn} from the training set Sample {z1, . . . , zn} from the prior PZ Sample z~i from Q(Z|xi) for i = 1, . . . , n Update D by ascending:

 n

n

log D (zi) + log 1 - D (z~i)

i=1

Update Q and G by descending:

1 n

n

c xi, G(z~i)

-  · log D (z~i)

i=1

Require: Regularization coefficient  > 0, characteristic positive-definite kernel k. Initialize the parameters of the encoder Q, decoder G, and latent discriminator D . while (, ) not converged do Sample {x1, . . . , xn} from the training set Sample {z1, . . . , zn} from the prior PZ Sample z~i from Q(Z|xi) for i = 1, . . . , n Update Q and G by descending:

1n n c xi, G(z~i)
i=1

+

 n(n -

1)

k(z , zj) + k(z~ , z~j) - 2k(z , z~j)

=j

end while

end while

When used with c(x, y) =

x-y

2 2

WAE-GAN is equivalent to adversarial auto-encoders (AAE)

proposed by Makhzani et al. (2016). Our theory thus suggests that AAEs minimize the 2-Wasserstein

distance between PX and PG. This provides the first theoretical justification for AAEs known to

the authors. WAE generalizes AAE in two ways: first, it can use any cost function c in the input

space X ; second, it can use any discrepancy measure DZ in the latent space Z (for instance MMD),

not necessarily the adversarial one of WAE-GAN.

Literature on OT Genevay et al. (2016) address computing the OT cost in large scale using SGD and sampling. They approach this task either through the dual formulation, or via a regularized version of the primal. They do not discuss any implications for generative modeling. Our approach is based on the primal form of OT, we arrive at regularizers which are very different, and our main focus is on generative modeling.

The WGAN (Arjovsky et al., 2017) minimizes the 1-Wasserstein distance W1(PX , PG) for generative modeling. The authors approach this task from the dual form. Their algorithm comes with-
out an encoder and can not be readily applied to any other cost Wc, because the neat form of the Kantorovich-Rubinstein duality (2) holds only for W1. WAE approaches the same problem from the primal form, can be applied for any cost function c, and comes naturally with an encoder.

In order to compute the values (1) or (2) of OT we need to handle non-trivial constraints, either
on the coupling distribution  or on the function f being considered. Various approaches have
been proposed in the literature to circumvent this difficulty. For W1 Arjovsky et al. (2017) tried to implement the constraint in the dual formulation (2) by clipping the weights of the neural network f .
Later Gulrajani et al. (2017) proposed to relax the same constraint by penalizing the objective of (2) with a term  · E ( f (X) - 1)2 which should not be greater than 1 if f  FL. In a more general OT setting of Wc Cuturi (2013) proposed to penalize the objective of (1) with the KLdivergence  · DKL(, P  Q) between the coupling distribution and the product of marginals. Genevay et al. (2016) showed that this entropic regularization drops the constraints on functions in
the dual formulation as opposed to (2). Finally, in the context of unbalanced optimal transport it

5

Under review as a conference paper at ICLR 2018

has been proposed to relax the constraint in (1) by regularizing the objective with  · Df (X , P ) +
Df (Y , Q) (Chizat et al., 2015; Liero et al., 2015), where X and Y are marginals of . In this paper we propose to relax OT in a way similar to the unbalanced optimal transport, i.e. by adding additional divergences to the objective. However, we show that in the particular context of generative modeling, only one extra divergence is necessary.

VAE

WAE-MMD

WAE-GAN

Test interpolations

Test reconstructions

Random samples

Figure 2: VAE (left column), WAE-MMD (middle column), and WAE-GAN (right column) trained on MNIST dataset. In "test reconstructions" odd rows correspond to the real test points.
Literature on GANs Many of the GAN variations (including f -GAN and WGAN) come without an encoder. Often it may be desirable to reconstruct the latent codes and use the learned manifold, in which cases these models are not applicable.
There have been many other approaches trying to blend the adversarial training of GANs with autoencoder architectures (Zhao et al., 2017; Dumoulin et al., 2017; Ulyanov et al., 2017; Berthelot et al., 2017). The approach proposed by Ulyanov et al. (2017) is perhaps the most relevant to our work. The authors use the discrepancy between QZ and the distribution EZ PZ [Q Z|G(Z ) ] of auto-encoded noise vectors as the objective for the max-min game between the encoder and decoder respectively. While the authors showed that the saddle points correspond to PX = PG, they admit that encoders and decoders trained in this way have no incentive to be reciprocal. As a workaround they propose to include an additional reconstruction term to the objective. WAE does not necessarily lead to a min-max game, uses a different penalty, and has a clear theoretical foundation.
Several works used reproducing kernels in context of GANs. Li et al. (2015); Dziugaite et al. (2015) use MMD with a fixed kernel k to match PX and PG directly in the input space X . These methods have been criticised to require larger mini-batches during training: estimating MMDk(PX , PG) requires number of samples roughly proportional to the dimensionality of the input space X (Reddi
6

Under review as a conference paper at ICLR 2018

et al., 2015) which is typically larger than 103. Li et al. (2017) take a similar approach but further
train k adversarially so as to arrive at a meaningful loss function. WAE-MMD uses MMD to match QZ to the prior PZ in the latent space Z. Typically Z has no more than 100 dimensions and PZ is Gaussian, which allows us to use regular mini-batch sizes to accurately estimate MMD.

VAE

WAE-MMD

WAE-GAN

Test interpolations

Test reconstructions

Random samples

Figure 3: VAE (left column), WAE-MMD (middle column), and WAE-GAN (right column) trained on CelebA dataset. In "test reconstructions" odd rows correspond to the real test points.

4 EXPERIMENTS

In this section we empirically evaluate the proposed WAE model. We would like to test if WAE can simultaneously achieve (i) accurate reconstructions of data points, (ii) reasonable geometry of the latent manifold, and (iii) random samples of good (visual) quality. Importantly, the model should generalize well: requirements (i) and (ii) should be met on both training and test data. We trained WAE-GAN and WAE-MMD (Algorithms 1 and 2) on two real-world datasets: MNIST (LeCun et al., 1998) consisting of 70k images and CelebA (Liu et al., 2015) containing roughly 203k images.

Experimental setup In all reported experiments we used Euclidian latent spaces Z = Rdz

for various dz depending on the complexity of the dataset, isotropic Gaussian prior distributions

PZ (Z) = N (Z; 0, z2 · Id) over Z, and a squared cost function c(x, y) =

x-y

2 2

for

data

points

x, y  X = Rdx . We used deterministic encoder-decoder pairs, Adam (Kingma & Lei, 2014) with

1 = 0.5, 2 = 0.999, and convolutional deep neural network architectures for encoder mapping Q : X  Z and decoder mapping G : Z  X similar to the DCGAN ones reported by Radford
et al. (2016) with batch normalization (Ioffe & Szegedy, 2015). We tried various values of  and

noticed that  = 10 seems to work good across all datasets we considered. All reported experiments

use this value.

7

Under review as a conference paper at ICLR 2018

Since we are using deterministic encoders, choosing dz larger than intrinsic dimensionality of the dataset would force the encoded distribution QZ to live on a manifold in Z. This would make matching QZ to PZ impossible if PZ is Gaussian and may lead to numerical instabilities. We use dz = 8 for MNIST and dz = 64 for CelebA which seems to work reasonably well.

We also report results of VAEs. VAEs used the same latent spaces as discussed above and standard Gaussian priors PZ = N (0, Id). We used Gaussian encoders Q(Z|X) = N Z; Q(X), (X) with mean Q and diagonal covariance . For MNIST we used Bernoulli decoders parametrized by G and for CelebA the Gaussian decoders PG(X|Z) = N Z; G(X), G2 · Id with mean G(Z). Functions Q, , and G were parametrized by deep nets of the same architectures as used in WAE.

WAE-GAN and WAE-MMD specifics In WAE-GAN we used discriminator D composed of sev-

eral fully connected layers with ReLu. We tried WAE-MMD with the RBF kernel but observed that

it fails to penalize the outliers of QZ because of the quick tail decay. If the codes z~ = Q(x) for

some of the training points x  X end up far away from the support of PZ (which may happen in

the

early

stages

of

training)

the

corresponding

terms

in

the

U-statistic

k(z, z~)

=

e-

z~-z

2 2

/k2

will

quickly approach zero and provide no gradient for those outliers. This could be avoided by choosing

the kernel bandwidth k2 in a data-dependent manner, however in this case per-minibatch U-statistic would not provide an unbiased estimate for the gradient. Instead, we used the inverse multiquadrat-

ics kernel k(x, y) = C/(C +

x-y

2 2

)

which

is

also

characteristic

and

has

much

heavier

tails.

In

all experiments we used C = 2dzz2, which is the expected squared distance between two multi-

variate Gaussian vectors drawn from PZ. This significantly improved the performance compared to

the RBF kernel (even the one with k2 = 2dzz2). Trained models are presented in Figures 2 and 3.

Further details are presented in Supplementary E.

Random samples are generated by sampling PZ and decoding the resulting noise vectors z into G(z). As expected, in our experiments we observed that for both WAE-GAN and WAE-MMD the quality of samples strongly depends on how accurately QZ matches PZ. To see this, notice that while training the decoder function G is presented only with encoded versions Q(X) of the data points X  PX . Indeed, the decoder is trained on samples from QZ and thus there is no reason to expect good results when feeding it with samples from PZ. In our experiments we noticed that even slight differences between QZ and PZ may affect the quality of samples. In some cases WAE-GAN seems to lead to a better matching and generates better samples than WAE-MMD. However, due to adversarial training
WAE-GAN is highly unstable, while WAE-MMD has a very stable training much like VAE.

In order to quantitatively assess the quality of the generated images, we use the Fre´chet Inception Distance introduced by Heusel et al. (2017) and report the results on CelebA in Table 1. These results confirm that the sampled images from WAE are of better quality than from VAE, and WAE-GAN gets a slightly better score than WAEMMD, which correlates with visual inspection of the images.

Algorithm
VAE WAE-MMD WAE-GAN

FID
98 55 42

Table 1: FID scores for samples on CelebA (smaller is better).

Test reconstructions and interpolations. We take random points x from the held out test set
and report their auto-encoded versions G(Q(x)). Next, pairs (x, y) of different data points are sampled randomly from the held out test set and encoded: zx = Q(x), zy = Q(y). We linearly interpolate between zx and zy with equally-sized steps in the latent space and show decoded images.

5 CONCLUSION
Using the optimal transport cost, we have derived Wasserstein auto-encoders--a new family of algorithms for building generative models. We discussed their relations to other probabilistic modeling techniques. We conducted experiments using two particular implementations of the proposed method, showing that in comparison to VAEs, the images sampled from the trained WAE models are of better quality, without compromising the stability of training and the quality of reconstruction. Future work will include further exploration of the criteria for matching the encoded distribution QZ to the prior distribution PZ, assaying the possibility of adversarially training the cost function c in the input space X , and a theoretical analysis of the dual formulations for WAE-GAN and WAE-MMD.

8

Under review as a conference paper at ICLR 2018
REFERENCES
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN, 2017.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, 35, 2013.
D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial networks, 2017.
Lenaic Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Unbalanced optimal transport: geometry and kantorovich formulation. arXiv preprint arXiv:1508.05216, 2015.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pp. 2292­2300, 2013.
V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. In ICLR, 2017.
G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015.
A. Genevay, M. Cuturi, G. Peyre´, and F. R. Bach. Stochastic optimization for large-scale optimal transport. In Advances in Neural Information Processing Systems, pp. 3432­3440, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014.
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scho¨lkopf, and A. J. Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723­773, 2012.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Domoulin, and A. Courville. Improved training of wasserstein GANs, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gu¨nter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015.
D. P. Kingma and J. Lei. Adam: A method for stochastic optimization, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, volume 86(11), pp. 2278­2324, 1998.
C. L. Li, W. C. Chang, Y. Cheng, Y. Yang, and B. Poczos. Mmd gan: Towards deeper understanding of moment matching network, 2017.
Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In ICML, 2015.
Matthias Liero, Alexander Mielke, and Giuseppe Savare´. Optimal entropy-transport problems and a new hellinger-kantorovich distance between positive measures. arXiv preprint arXiv:1508.07941, 2015.
F. Liese and K.-J. Miescke. Statistical Decision Theory. Springer, 2008.
J. Lin. Divergence measures based on the shannon entropy. Information Theory, 37, 1991.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
A. Makhzani, J. Shlens, N. Jaitly, and I. Goodfellow. Adversarial autoencoders. In ICLR, 2016.
9

Under review as a conference paper at ICLR 2018
L. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.
B. Poole, A. Alemi, J. Sohl-Dickstein, and A. Angelova. Improved generator objectives for GANs, 2016.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
R. Reddi, A. Ramdas, A. Singh, B. Poczos, and L. Wasserman. On the high-dimensional power of a linear-time two sample test under mean-shift alternatives. In AISTATS, 2015.
A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, NY, 2008. D. Ulyanov, A. Vedaldi, and V. Lempitsky. It takes (only) two: Adversarial generator-encoder
networks, 2017. C. Villani. Topics in Optimal Transportation. AMS Graduate Studies in Mathematics, 2003. J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2017.
10

Under review as a conference paper at ICLR 2018

A IMPLICIT GENERATIVE MODELS: A SHORT TOUR OF GANS AND VAES

Even though GANs and VAEs are quite different--both in terms of the conceptual frameworks and empirical performance--they share important features: (a) both can be trained by sampling from the model PG without knowing an analytical form of its density and (b) both can be scaled up with SGD. As a result, it becomes possible to use highly flexible implicit models PG defined by a twostep procedure, where first a code Z is sampled from a fixed distribution PZ on a latent space Z and then Z is mapped to the image G(Z)  X = Rd with a (possibly random) transformation G : Z  X . This results in latent variable models PG of the form (3).
These models are indeed easy to sample and, provided G can be differentiated analytically with respect to its parameters, PG can be trained with SGD. The field is growing rapidly and numerous variations of VAEs and GANs are available in the literature. Next we introduce and compare several of them.
The original generative adversarial network (GAN) Goodfellow et al. (2014) approach minimizes

DGAN(PX , PG) = sup EXPX [log T (X)] + EZPZ log 1 - T (G(Z))
T T

(5)

with respect to a deterministic decoder G : Z  X , where T is any non-parametric class of choice.

It is known that DGAN(PX , PG)  2 · DJS(PX , PG) - log(4) and the inequality turns into identity in the nonparametric limit, that is when the class T becomes rich enough to represent all functions

mapping X to (0, 1). Hence, GANs are minimizing a lower bound on the JS-divergence. How-

ever, GANs are not only linked to the JS-divergence: the f -GAN approach Nowozin et al. (2016)

showed that a slight modification Df,GAN of the objective (5) allows to lower bound any desired f -divergence in a similar way. In practice, both decoder G and discriminator T are trained in al-

ternating SGD steps. Stopping criteria as well as adequate evaluation of the trained GAN models

remain open questions.

Recently, the authors of Arjovsky et al. (2017) argued that the 1-Wasserstein distance W1, which is known to induce a much weaker topology than DJS, may be better suited for generative modeling. When PX and PG are supported on largely disjoint low-dimensional manifolds (which may be the case in applications), DKL, DJS, and other strong distances between PX and PG max out and no longer provide useful gradients for PG. This "vanishing gradient" problem necessitates complicated scheduling between the G/T updates. In contrast, W1 is still sensible in these cases and provides stable gradients. The Wasserstein GAN (WGAN) minimizes

DWGAN(PX , PG) = sup EXPX [T (X)] - EZPZ T (G(Z)) ,
T W
where W is any subset of 1-Lipschitz functions on X . It follows from (2) that DWGAN(PX , PG)  W1(PX , PG) and thus WGAN is minimizing a lower bound on the 1-Wasserstein distance.

Variational auto-encoders (VAE) Kingma & Welling (2014) utilize models PG of the form (3) and minimize

DVAE(PX ,

PG)

=

inf
Q(Z |X )Q

EPX

DKL Q(Z|X), PZ

- EQ(Z|X)[log pG(X|Z)]

(6)

with respect to a random decoder mapping PG(X|Z). The conditional distribution PG(X|Z) is often parametrized by a deep net G and can have any form as long as its density pG(x|z)
can be computed and differentiated with respect to the parameters of G. A typical choice is to use Gaussians PG(X|Z) = N (X; G(Z), 2 · I). If Q is the set of all conditional probability distributions Q(Z|X), the objective of VAE coincides with the negative marginal log-likelihood
DVAE(PX , PG) = -EPX [log PG(X)]. However, in order to make the DKL term of (6) tractable in closed form, the original implementation of VAE uses a standard normal PZ and restricts Q to a class of Gaussian distributions Q(Z|X) = N Z; µ(X), (X) with mean µ and diagonal covari-
ance  parametrized by deep nets. As a consequence, VAE is minimizing an upper bound on the
negative log-likelihood or, equivalently, on the KL-divergence DKL(PX , PG). Further details can
be found in Section D.

One possible way to reduce the gap between the true negative log-likelihood and the upper bound
provided by DVAE is to enlarge the class Q. Adversarial variational Bayes (AVB) Mescheder et al. (2017) follows this argument by employing the idea of GANs. Given any point x  X ,

11

Under review as a conference paper at ICLR 2018

a noise  N (0, 1), and any fixed transformation e : X × R  Z, a random variable e(x, ) implicitly defines one particular conditional distribution Qe(Z|X = x). AVB allows Q to contain
all such distributions for different choices of e, replaces the intractable term DKL Qe(Z|X), PZ in (6) by the adversarial approximation Df,GAN corresponding to the KL-divergence, and proposes to minimize3

DAVB(PX ,

PG)

=

inf
Qe (Z |X )Q

EPX

Df,GAN Qe(Z|X), PZ

- EQe(Z|X)[log pG(X|Z)] .

(7)

The DKL term in (6) may be viewed as a regularizer. Indeed, VAE reduces to the classical unregularized auto-encoder if this term is dropped, minimizing the reconstruction cost of the encoder-decoder pair Q(Z|X), PG(X|Z). This often results in different training points being encoded into nonoverlapping zones chaotically scattered all across the Z space with "holes" in between where the decoder mapping PG(X|Z) has never been trained. Overall, the encoder Q(Z|X) trained in this way does not provide a useful representation and sampling from the latent space Z becomes hard
Bengio et al. (2013).

Adversarial auto-encoders (AAE) Makhzani et al. (2016) replace the DKL term in (6) with another regularizer:

DAAE(PX ,

PG)

=

inf
Q(Z |X )Q

DGAN(QZ ,

PZ )

-

EPX

EQ(Z|X)[log

pG (X |Z )],

(8)

where QZ is the marginal distribution of Z when first X is sampled from PX and then Z is sampled from Q(Z|X), also known as the aggregated posterior Makhzani et al. (2016). Similarly
to AVB, there is no clear link to log-likelihood, as DAAE  DAVB (see Appendix D). The authors of Makhzani et al. (2016) argue that matching QZ to PZ in this way ensures that there are no "holes" left in the latent space Z and PG(X|Z) generates reasonable samples whenever Z  PZ. They also report an equally good performance of different types of conditional distributions Q(Z|X), includ-
ing Gaussians as used in VAEs, implicit models Qe as used in AVB, and deterministic encoder mappings, i.e. Q(Z|X) = µ(X) with µ : X  Z.

B PROOF OF THEOREM 1 AND FURTHER DETAILS

We will consider certain sets of joint probability distributions of three random variables (X, Y, Z)  X × X × Z. The reader may wish to think of X as true images, Y as images sampled from the
model, and Z as latent codes. We denote by PG,Z (Y, Z) a joint distribution of a variable pair (Y, Z), where Z is first sampled from PZ and next Y from PG(Y |Z). Note that PG defined in (3) and used throughout this work is the marginal distribution of Y when (Y, Z)  PG,Z .

In the optimal transport problem (1), we consider joint distributions (X, Y ) which are called cou-
plings between values of X and Y . Because of the marginal constraint, we can write (X, Y ) = (Y |X)PX (X) and we can consider (Y |X) as a non-deterministic mapping from X to Y . Theorem 1. shows how to factor this mapping through Z, i.e., decompose it into an encoding distribution Q(Z|X) and the generating distribution PG(Y |Z).

As in Section 2.2, P(X  PX , Y  PG) denotes the set of all joint distributions of (X, Y ) with marginals PX , PG, and likewise for P(X  PX , Z  PZ ). The set of all joint distributions of (X, Y, Z) such that X  PX , (Y, Z)  PG,Z , and (Y  X)|Z will be denoted by PX,Y,Z . Finally, we denote by PX,Y and PX,Z the sets of marginals on (X, Y ) and (X, Z) (respectively) induced by distributions in PX,Y,Z . Note that P(PX , PG), PX,Y,Z , and PX,Y depend on the choice of conditional distributions PG(Y |Z), while PX,Z does not. In fact, it is easy to check that PX,Z = P(X  PX , Z  PZ ). From the definitions it is clear that PX,Y  P(PX , PG) and we immediately
get the following upper bound:

Wc(PX , PG)



Wc(PX , PG)

:=

inf
P PX,Y

E(X,Y )P

[c(X, Y

)] .

(9)

If PG(Y |Z) are Dirac measures (i.e., Y = G(Z)), it turns out that PX,Y = P(PX , PG):

3The authors of AVB Mescheder et al. (2017) note that using f -GAN as described above actually results in "unstable training". Instead, following the approach of Poole et al. (2016), they use a trained discriminator T  resulting from the DGAN objective (5) to approximate the ratio of densities and then directly estimate the KL divergence f p(x)/q(x) q(x)dx.

12

Under review as a conference paper at ICLR 2018

Lemma 1 PX,Y  P(PX , PG) with identity if4 PG(Y |Z = z) are Dirac for all z  Z.
Proof The first assertion is obvious. To prove the identity, note that when Y is a deterministic function of Z, for any A in the sigma-algebra induced by Y we have E 1[Y A]|X, Z = E 1[Y A]|Z . This implies (Y  X)|Z and concludes the proof.

We are now in place to prove Theorem 1. Lemma 1 obviously leads to

Wc(PX , PG) = Wc(PX , PG). The tower rule of expectation, and the conditional independence property of PX,Y,Z implies

Wc(PX , PG)

=

inf
P PX,Y,Z

E(X,Y,Z)P

[c(X,

Y

)]

=

inf
P PX,Y,Z

EPZ

EXP (X|Z)EY

P (Y

|Z)[c(X,

Y

)]

=

inf
P PX,Y,Z

EPZ EXP (X|Z)

c

X, G(Z)

=

inf
P PX,Z

E(X,Z)P

c

X, G(Z)

.

It remains to notice that PX,Z = P(X  PX , Z  PZ ) as stated earlier.

B.1 RANDOM DECODERS PG(Y |Z)

If the decoders are non-deterministic, Lemma 1 provides only the inclusion of sets PX,Y  P(PX , PG) and we get the following upper bound on the OT:

Corollary 1 Let X = Rd and assume the conditional distributions PG(Y |Z = z) have mean

values G(z)  Rd and marginal

c(x, y) =

x-y

2 2

.

Then

variances

12,

.

.

.,

d2



0

for

all

z



Z,

where

G:

Z



X.

Take

d

Wc(PX , PG)



Wc(PX , PG)

=

i=1

i2

+

inf E(X,Z)P
P P(XPX ,ZPZ )

X - G(Z) 2 .

(10)

Proof First inequality follows from (9). For the identity we proceed similarly to the proof of Theorem 1 and write

Wc(PX ,

PG)

=

inf
P PX,Y,Z

EPZ

EX P

(X |Z ) EY

P (Y

|Z )

X-Y 2 .

(11)

Note that

EY P (Y |Z) X - Y 2 = EY P (Y |Z) X - G(Z) + G(Z) - Y 2 = X - G(Z) 2 + EY P (Y |Z) X - G(Z), G(Z) - Y + EY P (Y |Z) G(Z) - Y 2
d
= X - G(Z) 2 + i2.
i=1

Together with (11) and the fact that PX,Z = P(X  PX , Z  PZ ) this concludes the proof.

C RELATIONS OF WAE TO AAE, VAES, AND GANS
In Section C.1, we compare minimizing the optimal transport cost Wc, the upper bound Wc, and its relaxed version DWAE to VAE, AVB, and AAE in the special case when c(x, y) = x - y 2 and PG(Y |Z) = N (Y ; G(Z), 2 · I). We show that in this case the solutions of VAE and AVB both
4We conjecture that this is also a necessary condition. The necessity is not used in the paper.
13

Under review as a conference paper at ICLR 2018

depend on 2, while the minimizer G of Wc(PX , PG) does not depend on 2, and is the same as the minimizer of Wc(PX , PG) for 2 = 0. We also briefly discuss the role of these conclusions in explaining the well-known blurriness of VAE outputs. Section C.2 shows that when c(x, y) = x - y , WAE and WGAN approach primal and dual forms respectively of the same optimization problem. Finally, we discuss a difference in behaviour between the two algorithms caused by this duality.
We refer the reader to Supplementary A for the detailed overview of all these methods.

C.1 THE 2-WASSERSTEIN DISTANCE: RELATION TO VAE, AVB, AND AAE
Consider the squared Euclidean cost function c(x, y) = x - y 2, for which Wc is the squared 2Wasserstein distance W22. The goal of this section is to compare the minimization of W2(PX , PG) to other generative modeling approaches. Let us focus our attention on generative distributions PG(Y |Z) typically used in VAE, AVB, and AAE, i.e., Gaussians PG(Y |Z) = N Y ; G(Z), 2·Id . In order to verify the differentiability of log pG(x|z) all three methods require 2 > 0 and have problems handling the case of deterministic decoders (2 = 0). To emphasize the role of the variance 2 we will denote the resulting latent variable model PG.
Relation to VAE and AVB The analysis of Supplementary B shows that the value of W2(PX , PG) is upper bounded by Wc(PX , PG) of the form (10) and the two coincide when 2 = 0. Next we summarize properties of solutions G minimizing these two values W2 and Wc:
Proposition 1 Let X = Rd and assume c(x, y) = x - y 2, PG(Y |Z) = N Y ; G(Z), 2·I with any function G : X  R. If 2 > 0 then the functions G and G minimizing Wc(PX , PG) and Wc(PX , PG) respectively are different: G depends on 2, while G does not. The function G is also a minimizer of Wc(PX , PG0 ).
In order to prove this proposition we will need the following simple result, which is basically saying that the variance of a sum of two independent random variables is a sum of the variances:
Lemma 2 Under conditions of Proposition 1, assume Y  PG. Then Var[Y ] = 2 + VarZPZ [G(Z)].
Proof First of all, using (3) we have

E[Y ] := y pG(y|z)pZ (z)dzdy =

RZ

Z

Then

y pG(y|z)dy pZ (z)dz = EZPZ [G(Z)].
R

Var[Y ] := (y - E[G(Z)])2 pG(y|z)pZ (z)dzdy
RZ

= (y - G(z))2 pG(y|z)pZ (z)dzdy + (G(z) - E[G(Z)])2 pG(y|z)pZ (z)dzdy

RZ

RZ

= 2 + VarZPZ [G(Z)].

Next we turn to the proof of Proposition 1: Proof Corollary 1 shows that G does not depend on the variance 2. When 2 = 0 the distribution PG(Y |Z) turns into Dirac. In this case we combine Theorem 1 and Corollary 1 to conclude that G also minimizes Wc(PX , PG0 ). Next we prove that when 2 > 0 the function G minimizing Wc(PX , PG) depends on 2. The proof is based on the following example: X = Z = R, PX = N (0, 1), PZ = N (0, 1), and 0 < 2 < 1. Note that by setting G(z) = c · z for any c > 0 we ensure that PG is the Gaussian distribution, because a convolution of two Gaussians is also Gaussian. In particular if we take G(z) = 1 - 2 · z. Lemma 2 implies that PG is the standard normal Gaussian N (0, 1). In
14

Under review as a conference paper at ICLR 2018

other words, we obtain the global minimum Wc(PX , PG ) = 0 and G clearly depends on 2.

For the purpose of generative modeling, the noise 2 > 0 is often not desirable, and it is common practice to sample from the trained model G by simply returning G(Z) for Z  PZ without adding noise to the output. This leads to a mismatch between inference and training. Furthermore, VAE, AVB, and other similar variational methods implicitly use 2 as a factor to balance the 2 reconstruction cost and the KL-regularizer.
In contrast, Proposition 1 shows that for the same Gaussian models with any given 2  0 we can minimize Wc(PX , PG) and the solution G will be indeed the one resulting in the smallest 2Wasserstein distance between PX and the noiseless implicit model G(Z), Z  PZ used in practice.

Blurriness of VAE and AVB We next add to the discussion regarding the blurriness commonly
attributed to VAE samples. Our argument shows that VAE, AVB, and other methods based on the marginal log-likelihood necessarily lead to an averaging in the input space if PG(Y |Z) are Gaussian.

First we notice that in the VAE and AVB objectives, for any fixed encoder Q(Z|X), the decoder

is minimizing the expected 2-reconstruction cost EPX EQ(Z|X) X - G(Z) 2 with respect to G.

The optimal solution G Hence, as soon as supp

is of Pz is

the form G(z) = non-singleton, the

oEpPtzim[Xal],dwechoedreerPGz(Xwi)llenPdXu(pXav)Qer(aZgin=g

z |X ). points

in the input space. In particular this will happen whenever there are two points x1, x2 in supp PX

such that supp Q(Z|X = x1) and supp Q(Z|X = x2) overlap.

This overlap necessarily happens in VAEs, which use Gaussian encoders Q(Z|X) supported on the entire Z. When probabilistic encoders Q are allowed to be flexible enough, as in AVB, for any fixed PG(Y |Z) the optimal Q will try to invert the decoder (see Appendix D) and take the form

Q (Z |X )



PG (Z |X )

:=

PG(X|Z)PZ (Z) . PG (X )

(12)

This approximation becomes exact in the nonparametric limit of Q. When PG(Y |Z) is Gaussian we have pG(y|z) > 0 for all y  X and z  Z, showing that supp Q(Z|X = x) = supp PZ for all x  X . This will again lead to the overlap of encoders if supp PZ = Z. In contrast, the optimal
encoders of WAE do not necessarily overlap, as they are not inverting the decoders.

The common belief today is that the blurriness of VAEs is caused by the 2 reconstruction cost, or equivalently by the Gaussian form of decoders PG(Y |Z). We argue that it is instead caused by the
combination of (a) Gaussian decoders and (b) the objective (KL-divergence) being minimized.

C.2 THE 1-WASSERSTEIN DISTANCE: RELATION TO WGAN

We have shown that the DWAE criterion leads to a generalized version of the AAE algorithm and can be seen as a relaxation of the optimal transport cost Wc. In particular, if we choose c to be the Euclidean distance c(x, y) = x - y , we get a primal formulation of W1. This is the same criterion
that WGAN aims to minimize in the dual formulation (see Eq. 2). As a result of Theorem 1, we have

W1(PX ,

PG)

= inf EX
Q:QZ =PZ

PX

,Z Q(Z

|X

)

[

X - G(Z)

] = sup EPX [f (X)] - EPZ [f (G(Z))] .
f FL

This means we can now approach the problem of optimizing W1 in two distinct ways, taking gradient steps either in the primal or in the dual forms. Denote by Q the optimal encoder in the primal and f  the optimal witness function in the dual. By the envelope theorem, gradients of W1 with respect to G can be computed by taking a gradient of the criteria evaluated at the optimal points Q or f .

Despite the theoretical equivalence of both approaches, practical considerations lead to different behaviours and to potentially poor approximations of the real gradients. For example, in the dual formulation, one usually restricts the witness functions to be smooth, while in the primal formulation, the constraint on Q is only approximately enforced. We will study the effect of these approximations.

Imperfect gradients in the dual (i.e., for WGAN) We show that (i) if the true optimum f  is not reached exactly (no matter how close), the effect on the gradient in the dual formulation can

15

Under review as a conference paper at ICLR 2018

be arbitrarily large, and (ii) this also holds when the optimization is performed only in a restricted

class of smooth functions. We write the criterion to be optimized as JD(f ) := EPX [f (X)] -

EPZ [f (G(Z))] and denote its 1-Lipschitz functions FL on X

gradient with respect to G by JD(f ). Let H be a subset containing smooth functions with bounded Hessian. Denote

of by

the fH

the minimizer of JD in H. A(f, f ) := cos (JD(f ), JD(f )) will denote the cosine of the angle

between the gradients of the criterion at different functions.

Proposition 2 There exists a constant C > 0 such that for any > 0, one can construct distributions PX , PG and pick witness functions f  FL and h  H that are -optimal |JD(f ) - JD(f )|  , |JD(h ) - JD(h)|  , but which give (at some point z  Z) gradients whose direction is at least C-wrong: A(f , f )  1 - C, A(h0, h)  1 - C, and A(h , h)  0.

Proof We give a sketch of the proof. Consider discrete distributions PX supported on two points

{x0, x1}, and PZ supported on {0, 1} and let y0 = G(0), y1 = G(1) (y0 = y1). Given an optimal

f , one can modify locally it around y0 without changing its Lipschitz constant such that the

obtained f is an -approximation of f  whose gradients at y0 and y1 point in directions arbitrarily

different from those of f . For smooth functions, by moving y0 and y1 away from the segment

[x0, x1] but close to each other y0 - y1  K , the gradients of f  will point in directions roughly

opposite but the constraint on the Hessian will force the gradients of fF,0 at y0 and y1 to be very

close. y1 are

Finally, putting y0, y1 on the segment exactly opposite, while taking fF, (y)

[x0, x1], = fF (y

one can get an fF + ), we can swap

whose gradients at y0 and the direction at one of the

points while changing the criterion by less than .

Imperfect posterior in the primal (i.e., for WAE) In the primal formulation, when the constraint
is violated, that is the aggregated posterior QZ is not matching PZ, there can be two kinds of negative effects: (i) the gradient of the criterion is only computed on a (possibly small) subset of the latent space reached by QZ; (ii) several input points could be mapped by Q(Z|X) to the same latent code z, thus giving gradients that encourage G(z) to be the average/median of several inputs (hence
encouraging a blurriness).

D FURTHER DETAILS ON VAES AND GANS

VAE, KL-divergence and a marginal log-likelihood For models PG of the form (3) and any conditional distribution Q(Z|X) it can be easily verified that

-EPX [log PG(X)] = - EPX DKL Q(Z|X), PG(Z|X) + EPX DKL Q(Z|X), PZ - EQ(Z|X)[log pG(X|Z)] .

(13)

Here the conditional distribution PG(Z|X) is induced by a joint distribution PG,Z (X, Z), which is in turn specified by the 2-step latent variable procedure: (a) sample Z from PZ, (b) sample X from PG(X|Z). Note that the first term on the r.h.s. of (13) is always non-positive, while the l.h.s. does not depend on Q. This shows that if conditional distributions Q are not restricted then

-EPX

[log

PG (X )]

=

inf
Q

EPX

DKL Q(Z|X), PZ

- EQ(Z|X)[log pG(X|Z)]

,

where the infimum is achieved for Q(Z|X) = PG(Z|X). However, for any restricted class Q of conditional distributions Q(Z|X) we only have

- EPX [log PG(X)]

=

inf
Q

-EPX

DKL

Q(Z|X), PG(Z|X)

+EPX DKL Q(Z|X), PZ -EQ(Z|X)[log pG(X|Z)]



inf
QQ

EPX

DKL Q(Z|X), PZ

- EQ(Z|X)[log pG(X|Z)]

= DVAE(PX , PG),

where the inequality accounts for the fact that Q(Z|X) might be not flexible enough to match P (Z|X) for all values of X.

16

Under review as a conference paper at ICLR 2018

Relation between AAE, AVB, and VAE

Proposition 3 For any distributions PX and PG: DAAE(PX , PG)  DAVB(PX , PG).

Proof By Jensen's inequality and the joint convexity of DGAN we have

DAAE(PX , PG) = inf DGAN
Q(Z |X )Q

X Q(Z|x)pX (x)dx, PZ - EPX EQ(Z|X)[log pG(X|Z)]



inf
Q(Z |X )Q

EPX

DGAN (Q(Z|X)pX (x)dx, PZ ) - EQ(Z|X)[log pG(X|Z)]

= DAVB(PX , PG).

Under certain assumptions it is also possible to link DAAE to DVAE: Proposition 4 Assume DKL(Q(Z|X), PZ )  1/4 for all Q  Q with PX -probability 1. Then
DAAE(PX , PG)  DVAE(PX , PG).

Proof We already mentioned that DGAN(P, Q)  2 · DJS(P, Q) - log(4) for any distributions

P

and Q.

Furthermore, DJS(P, Q)



1 2

DTV

(P,

Q)

(Lin,

1991,

Theorem

3)

and

DTV(P,

Q)



DKL(P, Q) (Tsybakov, 2008, Eq. 2.20), which leads to

1 DJS(P, Q)  2 DKL(P, Q).

Together with the joint convexity of DJS and Jensen's inequality this implies

DAAE(PX , PG) := inf DGAN
Q(Z |X )Q

X Q(Z|x)pX (x)dx, PZ - EPX EQ(Z|X)[log pG(X|Z)]

 inf DJS
Q(Z |X )Q

X Q(Z|x)pX (x)dx, PZ - EPX EQ(Z|X)[log pG(X|Z)]



inf
Q(Z |X )Q

EPX

DJS (Q(Z|X), PZ ) - EQ(Z|X)[log pG(X|Z)]

1



inf
Q(Z |X )Q

EPX

2

DKL (Q(Z|X), PZ ) - EQ(Z|X)[log pG(X|Z)]



inf
Q(Z |X )Q

EPX

DKL (Q(Z|X), PZ ) - EQ(Z|X)[log pG(X|Z)]

= DVAE(PX , PG).

E FURTHER DETAILS ON EXPERIMENTS
MNIST: We use mini-batches of size 100, z2 = 1, and 4x4 convolutional filters. The reported models were trained for 100 epochs. We used  = 10-3 for Adam in the beginning, decreased it to 5 × 10-4 after 30 epochs, and to 10-4 after first 50 epochs. CelebA: We pre-processed CelebA images by first taking a 140x140 center crops and then resizing to the 64x64 resolution. We used mini-batches of size 100 and trained the models for various number of epochs (up to 250). All reported WAE models were trained for 55 epochs and VAE for 35 epochs. Initial learning rate of Adam was set to  = 10-4 as often recommended in the literature, decreased it to 5 × 10-5 after 30 epochs, and to 10-5 after first 50 epochs..
17

