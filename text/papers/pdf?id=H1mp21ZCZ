Under review as a conference paper at ICLR 2018
NUCLEAR p-NORMS FOR LARGE TENSOR COMPLETION
Anonymous authors Paper under double-blind review
ABSTRACT
We present algorithms for tensor completion using regularizers based on tensor nuclear p-norms. For the particular case of the nuclear -norm, we generalize to higher-order tensors the theoretical guarantees of the max-norm for matrix completion. From a practical perspective, we present two algorithms based on stochastic gradients to regularize the canonical decomposition of tensors, and show on large-scale benchmark datasets for knowledge base completion that (a) contrary to what suggested prior results in the literature, the canonical decomposition of tensors can achieve state-of-the-art level performance on FB15K and WN, and (b) our new regularizations reach or outperform the state-of-the-art on task where the canonical decomposition alone is not reaching it. In particular, we provide evidence that the nuclear 3-norm can replace the structures and/or regularization terms of existing link prediction models, and leads to better performance.
1 INTRODUCTION
The task of tensor completion is the extension of matrix completion to higher-order tensors which is one of the main tools for collaborative filtering, and, more generally, recommendation (Koren et al., 2009). Higher-order tensors capture multiple types of interactions between entities, and have been extensively used for link prediction or representation learning of relational databases such as knowledge bases (Nickel et al., 2016; Nguyen, 2017).
Matrix completion has been studied extensively in the past decade, and effective regularization techniques with strong statistical guarantees have been proved, such as the (weighted) nuclear-(or trace)-norm (Srebro et al., 2005; Candès & Recht, 2009; Foygel et al., 2011) or the max-norm (Srebro et al., 2005; Cai et al., 2016). These lead to convex optimization problems that can be solved directly (Cai et al., 2010; Jaggi et al., 2010), or to non-convex approximations based on stochastic or proximal gradients that scale better (Rennie & Srebro, 2005; Koren et al., 2009; Lee et al., 2010).
Unfortunately, these algorithms do not extend naturally to tensors. For tensors, several notions of rank exist and the canonical rank, which is the smallest number of rank one tensor whose sum is the original tensor is NP-hard to compute (see e.g. Kolda & Bader, and references therein for a comprehensive review on tensor decompositions). Furthermore, in the context of matrix completion, a regularization of the Frobenius norm of the factors leads naturally to the nuclear norm, which is not the case for tensors. Given the difficulties associated with the tensor canonical rank, this has led several authors to propose tensor norms based on trace norms of matricizations (Tomioka et al., 2010; Gandy et al., 2011; Wimalawarne et al., 2014; Romera-Paredes et al.). However, the resulting algorithms do not scale to very large tensors.
We focus on norm-based regularization schemes for tensors, based on the so-called nuclear p-norms of tensors (Friedland & Lim, 2014), whose unit balls are the convex hull of rank-1 tensors with unit p-norm factors. These include the tensor trace norm (nuclear 2-norm) as studied by e.g., Yuan & Zhang (2016) and Cheng et al. (2016). However, it also includes two other norms of potential interest: for 3D tensors, the nuclear 3-norm has a smooth multi-convex variational form which is straightforward to minimize with stochastic gradient descent. Second, the nuclear -norm, whose unit ball is the convex hull of rank-1 tensors with values in {-1, 1}. To the best of our knowledge, for matrices, the nuclear -norm has not been considered as a regularizer. However, all recovery and generalization guarantees for the max-norm of matrices (Srebro & Shraibman, 2005; Cai et al., 2016) are based on the relationship between the max-norm and the nuclear -norm. Whereas it is unclear how to extend the max-norm for general tensors, the nuclear -norm is a tensor norm already, and
1

Under review as a conference paper at ICLR 2018

the statistical guarantees in terms of generalization error for matrices with max-norm regularization are straightforward to extend to tensors with nuclear -norm regularization.
This paper presents several contributions. First, we propose alternative regularizers for the canonical decomposition of tensors, and show how they can be optimized at scale with stochastic gradient descent (SGD). Second, we perform extensive experiments on several datasets for link prediction in knowledge bases, which is an archetypical task for large-scale tensor completion, with a challenging state of the art (Nickel et al., 2011; Bordes et al., 2013; Trouillon et al., 2016; Kadlec et al., 2017a). Our results show on the one hand that the level of performance attainable with the classical CP model has been underestimated on datasets such as FB15K and WN, because too low ranks were systematically used. On other (smaller and larger) datasets which actually do require regularization, our results show that the usual methods for the canonical decomposition are extremely difficult to tune, and require a careful selection of the rank and early stopping to achieve acceptable results. On the other hand, the regularizations we propose and in particular the nuclear 3-norm, have a much better behavior and achieve similar or better results. In particular, they show little dependence on the optimization parameters and rank of the decomposition, as long as the rank is large enough, leaving the regularization parameter as the only critical tuning parameter of the method. We also provide empirical evidence that the nuclear 3-norm can be plugged in other models to replace the usual regularizers, with similar or better performances.

2 TENSOR COMPLETION WITH NUCLEAR p-NORM REGULARIZATION

Three dimensional1 tensor completion is the problem of estimating a tensor X^  Rn1×n2×n3 given a training set of sampled entries S = {(i1, j1, k1, y1), ..., (i|S|, j|S|, k|S|, y|S|)}, where each sample contains the indices in the tensor and an associated observed target binary value. There are two main objectives associated with matrix or tensor completion. The first one is to predict the values of missing entries, and is the extension to higher-order arrays of the ratings prediction task in collaborative filtering. In that case, a test set is composed of a triple of indices (i, j, k) together with a desired value y that is directly compared to the predicted value X^i,j,k. The second objective is the extension to tensors of one-class collaborative filtering or link prediction (see e.g. Pan et al., 2008). In that case, we assume that there is an underlying binary tensor from which we only observe some positive entries and the goal is to rank unknown positive entries above (unknown) negative entries. The evaluation is then performed using a ranking performance metric. For instance, in link prediction in relational data, a (training) sample is a triple (i, j, k) encoding that entity i is linked to k by relationship j, such as (Washington, capital_of, USA). A common task is to predict the possible relationships between two entities. In this case, a test sample is defined by a pair of entities (i, k) and a relationship j that the system needs to predict. The performance is a function of the rank of j in the list of relationships j ordered by decreasing values of X^i,j ,k. In the remainder of this work, tensor completion will always refer to link prediction, where missing entries are considered as zeros.
Given the training set S, we propose to learn the tensor by solving a regularized empirical risk minimization problem of the form:

X^ = argmin

(X; i, j, k, y) + (X) ,

XRn1×n2×n3 (i,j,k,y)S

(1)

where (X; i, j, k, y) is the instantaneous loss associated to the training sample (i, j, k, y),  is a regularizer and  > 0 is a regularization coefficient. For ratings prediction, is usually the square error loss (X; i, j, k, y) = (Xi,j,k - y)2. For link prediction, S is only composed of positive examples, and the loss can be the multiclass log-loss: (X; i, j, k, 1) = -Xi,j,k + log j exp(Xi,j ,k) .
We are interested in the nuclear p-norms for tensors, as defined by Friedland & Lim (2014), whose unit balls are the convex hull of rank-1 tensors with unit p-norm factors. More precisely, denoting by  the tensor product and by . p the p-norm of a real-valued vector, the nuclear p-norm of

1To simplify notations, we focus on order-3 tensors in this paper. However, our approach extends naturally to order-D tensors.

2

Under review as a conference paper at ICLR 2018

X  Rn1×n2×n3 for p  [1, +], is

RR

X ,p := min

|r| X =

r

u(r1)



u(r2)



ur(3),

max
d=1..3

u(rd)

p  1, R  N

.

r=1

r=1

r=1..R

(2)

where r  R and ur(d)  Rnd for r  {1, ..., R} and d  {1, 2, 3}. Friedland & Lim (2014, Proposition 4.3) guarantees that the minimum in (2) is attained for a rank R  n1n2n3.

Note that nuclear p-norm is an atomic norm in the terminology of (Chandrasekaran et al., 2012) whose atoms are the rank-1 tensors u  v  w with unit p-norm factors. Atomic norms are norms that induce solutions that are combinations of a small number of atoms. Indeed, here the 1-norm regularization on the weights r, is sparsifying and so will encourage a reduction of the number of terms in the sum, which correspond to learning a tensor with low canonical rank. One might question the choice of the 1-norm on  and consider the possibility of using -norms. But for  < 1 the corresponding tensor function is nonconvex and Friedland & Lim (2014) show that for  > 1 the infimum is not attained and thus does not yield usable expansions.

The choice p = 2 in (2) yields the usual nuclear norm of tensors, or tensor trace norm, used for tensor completion by e.g., Yuan & Zhang (2016) and Cheng et al. (2016). It is the natural extension to tensors of the nuclear (or trace) norm for matrices, whose unit ball is the convex hull of rank-1 matrices with unit 2-norm factors. For reasons that we will describe later, in this paper we are mostly interested in the nuclear 3-norm and nuclear -norm.

Scalable algorithms to find a global minimum of (1) with nuclear p-norm regularizers are not available, even though the problem is convex in X as soon as is. As for the case of matrix norms regularization, an alternative is to work with the variational formulations of the norms. To simplify notations, we introduce the set of canonical decompositions of a tensor of rank at most R :

R
UR(X) = (ur(d))d=1..3,r=1..R | X = u(r1)  u(r2)  u(r3), r, d, ur(d)  Rnd .
r=1

(3)

The nuclear p-norm admits the two following variational forms:

1R X ,p = min 3

3

u(rd)

3 p

(ur(d))  UR(X), R  N

r=1 d=1

(4a)

R

= min

max
d=1..3

ur(d)

3 p

(ur(d))  UR(X), R  N

.

r=1

(4b)

The first one is proposed in Haeffele & Vidal (2015). We prove both for the general case in the

Appendix.

Given an estimated upper bound on the optimal R, the original problem can then be re-written as a non-convex problem using one of the variational forms (4a) or (4b). For example, using (4a):

min
(u(rd)Rnd ) d=1..3 (i,j,k,y)S
r=1..R

R
u(r1)  ur(2)  u(r3); i, j, k, y

 +
3

R

3

ur(d)

3 p

.

r=1

r=1 d=1

(5)

Note that each loss term depends on the data only via Xi,j,k =

R r=1

ur(1,i) u(r2,j) ur(3,k) ,

which

involves

only 3r parameters (and where u(rd,i) denotes the ith entry of ur(d)).

If R is large enough the global optimum of (5) is the same as that of (1). While the original formulation

is convex in its tensor argument X, it is NP-hard to optimize. The advantage of (5) is that whenever

is smooth with respect to X, the problem as a function of (u(rd))d,r is a composite optimisation

problem with a non-convex smooth part and a convex, possibly non-smooth, regularizer (it is easy to

check that both

R r=1

3 d=1

ur(d)

3 p

and

R r=1

maxd=1..3

u(rd)

3 p

are

convex

in

(ur(d) )d,r ).

Reddi

et al. (2016) showed that in this setting, minibatch proximal stochastic gradient descent (with

minibatches that increase in size) converges to a stationary point. Thus, SGD techniques can be used.

In practice, the maximum rank R can be limited by the computational or memory budget.

We next discuss related work before presenting more precise formulations and algorithms.

3

Under review as a conference paper at ICLR 2018
3 RELATED WORK
Matrix completion Norm-based regularization has been extensively studied in the context of matrix completion. The trace norm (or nuclear norm) has been proposed as a convex relaxation of the rank (Srebro et al., 2005) for matrix completion in the setting of ratings prediction, with strong theoretical guarantees (Candès & Recht, 2009). While efficient algorithms to solve the convex problems have been proposed (see e.g. Cai et al., 2010; Jaggi et al., 2010), the practice is still to use the matrix equivalent of the nonconvex formulation (5). For the trace norm (nuclear 2-norm), in the matrix case, the regularizer simply becomes the squared 2-norm of the factors, and lends itself to alternating methods or SGD optimization (Rennie & Srebro, 2005; Koren et al., 2009). When the samples are not taken uniformly at random from a matrix, some other norms are preferable to the usual nuclear norm. The weighted trace norm reweights elements of the factors based on the marginal rows and columns sampling probabilities, which can improve sample complexity bounds when sampling is non-uniform (Foygel et al., 2011; Negahban & Wainwright, 2012). Direct SGD implementations on the nonconvex formulation implicitly takes this reweighting rule into account and were used by the winners of the Netflix challenge (see the discussion in (Srebro & Salakhutdinov, 2010, Section 5)). The max-norm was subsequently proposed as an alternative to weighted trace norm that would also be robust to non-uniform sampling Srebro & Shraibman (2005); Cai et al. (2016). There is no known equivalent of the max-norm for tensors; however, the theoretical guarantees hold because the max-norm is related to the matrix equivalent of the nuclear -norm (Srebro & Shraibman, 2005, Corollary 2). It is this observation that motivates our use of the nuclear -norm for tensors.
Tensor completion and decompositions There is a large body of literature on low-rank tensor decompositions (see Kolda & Bader, for a comprehensive review). Closely related to our work is the canonical decomposition of tensor (also called CANDECOMP/PARAFAC or CP) (Hitchcock, 1927), which solves a problem similar to (5) without the regularization (i.e.,  = 0), and usually the square loss.
Quite a few norm-based regularizations for tensors have been proposed. Several are based on unfolding tensors along each of its mode to obtain matricizations, and either regularize by the sum of trace norms of all its matricizations (Tomioka et al., 2010) or write the original tensor as a sum of tensors Tk whose kth matricization alone is regularized in trace norm (Wimalawarne et al., 2014). However, in the large scale setting even rank-1 approximations of matricizations involve too many parameters to be tractable.
Recently, the tensor trace norm (nuclear 2-norm) has been proposed as a regularizer for tensor completion Yuan & Zhang (2016), and an algorithm based on the generalized conditional gradient has been proposed by Cheng et al. (2016). This algorithm requires, in an inner loop, to compute a (constrained) rank-1 tensor that has largest dot-product with the gradient of the data-fitting term (gradient w.r.t. the tensor argument). This is possible in the one-class completion setup only with the square error loss, for which the gradient is a low-rank + sparse tensor when the argument is low-rank. However, on large-scale knowledge bases the state of the art is to use a binary log-loss or a multiclass log-loss (Trouillon et al., 2016; Kadlec et al., 2017a); in that case, the gradient is not properly structured and needs be computed explicitly, thereby making the approach of Cheng et al. (2016) too costly.
Link prediction in relational data The task of link prediction on relational data has been studied extensively in the past decade (Nickel et al., 2016), in particular in the context of knowledge bases. We focus here on models of relational data based on tensor factorization. RESCAL (Nickel et al., 2011) is a constrained version of Tucker decomposition in which the two modes of the tensor that correspond to entities have the same factor matrices. That is, given the rank of the decomposition R, the matrix corresponding to the (predicted) adjacency matrix of relationship j is X^:,j,: = U R(j)U T where U  RE×R is the factor matrix for the entities common to each relationships, and R(j)  RR×R is a matrix that depends on the relationship. TATEC Garcia-Duran et al. (2015) adds to RESCAL an additional, simpler model containing only bi-linear terms to avoid overfitting on easier datasets. The latent factor model of Jenatton et al. (2012) is a probabilistic model that has a factorization similar to RESCAL, but adds some structure for the relationships matrices R(j) by enforcing them to be a sum of a small number of shared rank-1 matrices. DistMult (Yang et al., 2014) is a CP
4

Under review as a conference paper at ICLR 2018

decomposition in which two modes of the tensor share the same factors; it can also be seen as RESCAL where the matrices R(j) are diagonal. Even though the modeling assumption made by DistMult is incorrect on most data, because it predicts symmetric interactions for all relationships, it was recently shown to achieve surprisingly good performances on several large-scale knowledge base completion benchmarks (Kadlec et al., 2017a). The ComplEx model of (Trouillon et al., 2016) is a specific form of the CP decomposition with complex numbers and shared factors, which takes the form X^:,j,: = Re(U R(j)U T ), where U  CE×R is the shared factor matrix of the entities, R(j)  CR×R is a diagonal (complex) matrix associated to relationship j, and U is the complex conjugate of U . The ComplEx decomposition is a means to share the embeddings between the two modes for the entities with a CP-like decomposition, without the incorrect modeling assumption of DistMult. Hayashi & Shimbo (2017) showed that the ComplEx decomposition is equivalent to the approach of holographic embeddings (Nickel et al., 2015).
These works on relational data aimed at finding modeling assumptions such that the overall tensor can be approximated with low rank. In practice, CP, RESCAL and ComplEx use an additional regularization based on the Frobenius norm of the factors. Whereas regularizing with the Frobenius norm of the factors is justified in the case of matrix factorization (because it corresponds to a trace norm regularization), there is no justification in the higher-order case beyond analogy.
The nuclear p-norm regularizers that we propose are intended to (a) provide tuning parameters that continuously trade-off between data fitting and limited model capacity, and (b) propose algorithms with performances that are less sensitive to other parameters such as the rank (the algorithm should yield the same results as soon as the rank is large enough) or the details of the optimization procedure.

4 FORMULATIONS AND ALGORITHMS FOR THE NUCLEAR 3 AND -NORMS

4.1 NUCLEAR 3-NORM

The nuclear 3-norm (resp. D-norm) is particularly suited to order-3 (resp. order-D) tensors because

its additive variational formulation becomes separable in each coefficient, given that

u(rd)

3 p

=

nd i=1

ur(d,i)|3. This leads to very cheap SGD udpates.

Given the separability of the norm and that only a subset of triples are sampled, we generalize to the tensor case the modification of the variational form that turns the trace-norm into the weighted tracenorm (Srebro & Salakhutdinov, 2010): we keep only in the regularization the terms corresponding to the sampled triplets. This leads to a formulation of the form

min
(ur(d))d,r (i,j,k,y)S

R
u(r1) u(r2) ur(3); i, j, k, y

 +
3

r=1

ur(1,i)|3 + u(r2,j)|3 + ur(3,k)|3

.

For a specific example (i, j, k, y), only those parameters useful for estimating X^i,j,k will be regularized, leading to the same computational complexity as the currently used Frobenius norm regularizer.
Viewing ComplEx as a form of canonical decomposition over C, we could easily extend this regularizer to the complex setting by replacing the gradient of the squared 2-norms with the gradient of the cubed 3-norms over the moduli, which keeps the computational cost of the updates the same.
In the following, the nuclear 3-norm will always refer to the weighted version described here.

4.2 NUCLEAR -NORM
Our choice of the nuclear -norm is motived by previous work on matrix completion (Srebro & Shraibman (2005)), suggesting the max-norm as an alternative to the trace norm which is robust to non-uniform sampling. We show in Appendix 7.2 that the max-norm and -norm have similar generalization guarantees. As far as we know, the nuclear -norm is the only norm that generalizes the guarantees of the max-norm to tensors of order D.

5

Under review as a conference paper at ICLR 2018

Despite the fact that the nuclear tensor p-norms are in theory all intractable to compute, we show that for p = , a stationary point of variational form (4b) can be found efficiently using proximal SGD.

Indeed, the regularizer maxd=1..3

u(rd)

3 

being

convex,

we

can

consider

its

proximal

operator:

R

Prox(u(d), ) = argmin
u~(d) r,d

u~r(d) - u(rd)

2 2

+



max
d=1..3

u~(rd)

3 

.

r=1

(6)

But this problem is separable in r:

Proxr(ur(d), ) = argmin

u~(rd)

d

u~r(d) - ur(d)

2 2

+



max
d=1..3

u~(rd)

3 

.

Since maxd=1..3

u~(rd)

3 

=

u~r 3 , with u~r the concatenation over d of all the u~(rd), we can set

x~ = u~r and x = ur, to re-write the problem as:

Proxr(x, ) = argmin

x~ - x

2 2

+



x~

3 

=

argmin

x~ - x

2 2

+

C 3

.

x~ C,x~

x~ C

(7)

Solving this proximal operator is similar to what one would do for the usual  norm regularization. It is actually possible to find the value of C analytically. The exact derivation and algorithm are given in the appendix. In practice, since the solution requires to sort all the parameters, we apply the proximal operator after a fixed number of gradient updates. This is a slight departure from the theory which would require minibatch updates of increasing sizes.

5 EXPERIMENTAL RESULTS
We begin in Section 5.1 with a comprehensive comparative study of regularized CP and show that our nuclear p-norms regularizers are better behaved than the one used in practice. We then show scalability by applying these algorithms to larger datasets in Section ??. Finally, we plug-in our nuclear 3-norm into a state of the art model, ComplEx, and show that we gain both in ease of tuning and performances on the SVO dataset. Detailed grid search parameters can be found in appendix 7.4.
5.1 SMALL DATASETS
NATION, UMLS and KINSHIP (Kemp et al. (2006)) are three small datasets on which we can study tensor completion in the setting of ratings prediction as in Nickel et al. (2011). We know from experiments in Nickel et al. (2011) that a good regularization is key on these datasets or CP will overfit. We study the effects of regularization on two tensor decomposition methods : CP and ComplEx. The regularizer used in practice for these two methods is the squared L2 norm of the factors. We

Figure 1: Smoothness of the nuclear norm regularizers on UMLS compared to L2.
report on Figure 1 the cross-validated validation MSE for different values of  on UMLS with three different regularizers and after 50 epochs (without early-stopping). While the curves for the nuclear norms are nicely unimodal in  and have invisible error bars, for the values of  that are sufficiently large to prevent overfitting, the performance of L2 has large variance, which is most probably due to
6

Under review as a conference paper at ICLR 2018

(a) Stability relative to the rank.

(b) Stability relative to the number of epochs.

Figure 2: Stability of the nuclear p-norms relative to number of epochs and decomposition rank.

the non-convexity of this regularizer. This unstable behavior makes to difficult to find good values of the regularization coefficient. As a confirmation, a grid-search over {epochs, rank, } for the CP-L2 baseline chose a low  combined with a small number of epochs thus preferring early-stopping to prevent overfitting.
Furthermore, and in stark contrast with L2, we observe in Figures 2a and 2b that the rank and the number of epochs plays no significant roles when we use the nuclear norms.
Finally, for the three datasets, we split the data in 10-fold, use 8 folds for training, one for validation (choice of hyperparameters) and one for testing, and repeat 10 times on different folds. Similarly to Nickel et al. (2011) and Bailly et al. (2015), we use the ratings prediction setting, and unobserved values are set to zero before fitting the tensor with an L2 loss. Final mean and standard errors for each method are reported in Table 1. For the baselines CP and ComplEx, we grid search over the rank and , setting a low ADAGRAD learning rate and test after each epoch for early stopping based on validation set performance. For the nuclear norm regularized methods, we train for a fixed number of epochs that is sufficient to reach convergence on the training set and without any early stopping. The rank and learning rate are picked based on preliminary tests.
We compare the AUC for each baseline on the three datasets with that of the same model with our proposed regularizers. We include in the comparison the results for two state-of-the-art methods, SITAR (Bailly et al., 2015) and LFM (Jenatton et al., 2012). We observe a consistent increase of performance for both CP and ComplEx when regularized with nuclear norms, except for CP- on NATION.
Using ComplEx with our regularizer, we match the state of the art results of SITAR on two out of three datasets.

CP-L3 CP- CP- baseline
ComplEx- baseline ComplEx- L3
RESCAL LFM* SITAR

NATION 0.857 ± 0.01 0.825 ± 0.02 0.846 ± 0.02
0.869 ± 0.02 0.878 ± 0.02
0.84 0.909 ± 0.009 0.890 ± 0.019

UMLS 0.975 ± 0.004 0.978 ± 0.003 0.963 ± 0.005
0.972 ± 0.005 0.982 ± 0.004
0.98 0.990 ± 0.003 0.977 ± 0.003

KIN 0.949 ± 0.004 0.950 ± 0.003 0.944 ± 0.005
0.969 ± 0.004 0.971 ± 0.005
0.95 0.946 ± 0.005 0.969 ± 0.004

Table 1: 10-fold cross validated AUC on the three datasets in ratings prediction. *LFM does not consider missing elements as negative, which is an easier setting.

7

Under review as a conference paper at ICLR 2018

5.2 BIG DATASETS
5.2.1 FB15K AND WN18
We consider FB15K and WN18, two datasets used frequently in the link prediction literature (Bordes et al., 2013). We follow the same protocol as in Trouillon et al. (2016) and select our best model based on the Mean Reciprocal Rank (see e.g. Kadlec et al., 2017b, for a definition). Precisely, we learn separate representations of the relations and their inverse, using one or the other to rank the left hand side or right hand side.
All factorization-based methods perform as well as the state of the art (DistMult, Kadlec et al. (2017b)), as shown in Table 2 (for a more complete overview of the state-of-the-art on these datasets and a description of the metrics, see Kadlec et al. (2017a)).
Note that the results reported here for CP are without regularization.
We tried larger ranks (up to 10000), but never needed regularization, because our models never seemed to overfit. The fact that regularization seems somewhat useless may be because the data exhibits very low noise, or because overfitting may require ranks that are too large to try with our computational budget.
This suggest that FB15K and WN might not be the best datasets to test new structured formulations.

TransE DistMult* ComplEx
CP

H@1 50.0 79.7 74.8
77.7

FB15K
H@10 83.2 89.3 87.0

MRR 0.633 0.798 0.793

87.0 0.810

H@1 12.6 78.4 94.0
94.3

WN p@10 94.3 94.6 94.9
95.4

MRR 0.478 0.797 0.944
0.947

Table 2: All methods are roughly equivalent on FB15K and WN18, given a high enough rank. *Results for DistMult include a specifically selected ensemble model for H@1 and a single model selected on H@10 rather than MRR.  Results are for our re-implementation of these two models, giving better results than what is reported in the literature.

5.2.2 SVO
Subject-Verb-Object (SVO) is a large dataset introduced in Jenatton et al. (2012). Contrary to what we observed on FB15K and WN18, learning on this dataset requires careful regularization. Pictured in figure 3 is a fine grid-search over  for two versions of the ComplEx model trained with a ranking-loss. Everything else being equal, we observe a gap in validation performances that the Frobenius regularized version cannot bridge.
We use this dataset to showcase how the nuclear 3-norm can improve performances on large tensor completion without any additional effort on an off-the-shelf algorithm like ComplEx. This improvement puts ComplEx trained with a ranking loss at 79H@5% just shy of the state of the art, 80H@5% in Garcia-Duran et al. (2015), without any additional efforts. Results on this dataset are similar for CP, we report experiments for this setting in the appendix.
In terms of timings, our implementation of ComplEx with a nuclear 3-norm regularizer runs at about 40s/epoch for a ranking log-loss on 20 cores, and converges in around 1000 epochs, as seen on figure 3. We do not notice any difference in runtime for the nuclear norms compared to the usual Frobenius norm, provided that the proximal operator for the nuclear -norm is applied after n_entities epochs, which is frequent enough in practice.
6 CONCLUSION
We studied two particular norms in the nuclear p-norms family, and their practical use in a completion settings. The nuclear -norm extends the generalization guarantees of the matrix max-norm to higher
8

Under review as a conference paper at ICLR 2018

Figure 3: Grid-search over  for two differently regularized versions of ComplEx.

order tensor and has a fast proximal descent algorithm. For the practicioner, the nuclear 3-norm can be plugged without effort into models that are related to CP, and leads to consistent improvements in generalization performances. Having a regularizer that is insensitive to the rank allowed us to try higher ranks without fear of overfitting for CP, which we re-introduce as a competitive method, matching the state of the art on FB15K and WN18. An interesting direction for future work would be to derive the proximal operator for a weighted version of the tensor trace norm (or nuclear 2-norm). We used ADAGRAD in a non-convex setting, a rigorous proof of its convergence to stationary points in this setting would be helpful. Another interesting question is to explore the statistical guarantees for the nuclear 3-norm.

REFERENCES

Raphaël Bailly, Antoine Bordes, and Nicolas Usunier. Semantically Invariant Tensor Factorization. 2015.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-

sana Yakhnenko.

Translating Embeddings for Modeling Multi-relational Data.

In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-

berger (eds.), Advances in Neural Information Processing Systems 26, pp. 2787­

2795. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/

5071-translating-embeddings-for-modeling-multi-relational-data.

pdf.

Jian-Feng Cai, Emmanuel J Candès, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956­1982, 2010.

T Tony Cai, Wen-Xin Zhou, and others. Matrix completion via max-norm constrained optimization. Electronic Journal of Statistics, 10(1):1493­1525, 2016.

Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.

Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational mathematics, 12(6):805­849, 2012.

Hao Cheng, Yaoliang Yu, Xinhua Zhang, Eric Xing, and Dale Schuurmans. Scalable and sound low-rank tensor learning. In Artificial Intelligence and Statistics, pp. 1114­1123, 2016.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011. URL http://www.jmlr.org/papers/v12/duchi11a.html.

9

Under review as a conference paper at ICLR 2018
Rina Foygel, Ohad Shamir, Nati Srebro, and Ruslan R Salakhutdinov. Learning with the weighted trace-norm under arbitrary sampling distributions. In Advances in Neural Information Processing Systems, pp. 2133­2141, 2011.
Shmuel Friedland and Lek-Heng Lim. Nuclear norm of higher-order tensors. arXiv preprint arXiv:1410.6072, 2014. URL https://arxiv.org/abs/1410.6072.
Silvia Gandy, Benjamin Recht, and Isao Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27(2):025010, 2011.
Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, and Yves Grandvalet. Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases. arXiv:1506.00999 [cs], June 2015. URL http://arxiv.org/abs/1506.00999. arXiv: 1506.00999.
Benjamin D. Haeffele and Rene Vidal. Global Optimality in Tensor Factorization. Deep Learning, and Beyond. CoRR, abs/1506.0, 4, 2015.
Katsuhiko Hayashi and Masashi Shimbo. On the Equivalence of Holographic and Complex Embeddings for Link Prediction. arXiv preprint arXiv:1702.05563, 2017.
Frank L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Studies in Applied Mathematics, 6(1-4):164­189, 1927. URL http://onlinelibrary.wiley.com/doi/ 10.1002/sapm192761164/full.
Martin Jaggi, Marek Sulovsk, and others. A simple algorithm for nuclear norm regularized problems. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 471­478, 2010.
Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes, and Guillaume Obozinski. A latent factor model for highly multi-relational data. December 2012. URL https://hal.inria.fr/ hal-00776335/document.
Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge Base Completion: Baselines Strike Back. arXiv preprint arXiv:1705.10744, 2017a.
Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge Base Completion: Baselines Strike Back. arXiv preprint arXiv:1705.10744, 2017b.
Charles Kemp, Joshua B Tenenbaum, Thomas L Griffiths, Takeshi Yamada, and Naonori Ueda. Learning systems of concepts with an infinite relational model. In AAAI, volume 3, pp. 5, 2006.
Tamara G. Kolda and Brett W. Bader. Tensor Decompositions and Applications. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.454. 202&rep=rep1&type=pdf.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8), 2009.
Jason D Lee, Ben Recht, Nathan Srebro, Joel Tropp, and Ruslan R Salakhutdinov. Practical largescale optimization for max-norm regularization. In Advances in Neural Information Processing Systems, pp. 1297­1305, 2010.
Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 13(May): 1665­1697, 2012.
Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge base completion. arXiv preprint arXiv:1703.08098, 2017. URL https://arxiv.org/abs/ 1703.08098.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 809­816, 2011. URL http://machinelearning.wustl.edu/ mlpapers/paper_files/ICML2011Nickel_438.pdf.
10

Under review as a conference paper at ICLR 2018
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic Embeddings of Knowledge Graphs. arXiv preprint arXiv:1510.04935, 2015. URL http://arxiv.org/abs/1510. 04935.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11­33, 2016. URL http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7358050.
Rong Pan, Yunhong Zhou, Bin Cao, Nathan N Liu, Rajan Lukose, Martin Scholz, and Qiang Yang. One-class collaborative filtering. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, pp. 502­511. IEEE, 2008.
Sashank J Reddi, Suvrit Sra, Barnabás Póczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. In Advances in Neural Information Processing Systems, pp. 1145­1153, 2016.
Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pp. 713­719. ACM, 2005.
Bernardino Romera-Paredes, Massimiliano Pontil, and CSML Lunch Seminar. A New Convex Relaxation for Tensor Completion. URL http://events.csml.ucl.ac.uk/userdata/ lunch_talks/2013_11_29_brp.pdf.
Nathan Srebro. Learning with matrix factorizations. 2004. URL https://dspace.mit.edu/ handle/1721.1/30507.
Nathan Srebro and Ruslan R Salakhutdinov. Collaborative filtering in a non-uniform world: Learning with the weighted trace norm. In Advances in Neural Information Processing Systems, pp. 2056­ 2064, 2010.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference on Computational Learning Theory, pp. 545­560. Springer, 2005.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In Advances in neural information processing systems, pp. 1329­1336, 2005.
Ryota Tomioka, Kohei Hayashi, and Hisashi Kashima. Estimation of low-rank tensors via convex optimization. arXiv preprint arXiv:1010.0789, 2010.
Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016. URL https: //arxiv.org/abs/1606.06357.
Kishan Wimalawarne, Masashi Sugiyama, and Ryota Tomioka. Multitask learning meets tensor factorization: task imputation via convex optimization. In Advances in neural information processing systems, pp. 2825­2833, 2014.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014. URL http://arxiv.org/abs/1412.6575.
Ming Yuan and Cun-Hui Zhang. On tensor completion via nuclear norm minimization. Foundations of Computational Mathematics, 16(4):1031­1068, 2016.
11

Under review as a conference paper at ICLR 2018

7 APPENDIX

7.1 PROOF OF EQNS. 4A AND 4B

We will prove a version for tensors of any order D :
Proposition 1. For X a tensor or order D, the nuclear p-norm has the following equivalent variational forms :

1R X ,p = min D

D

ur(d)

D p

(ur(d))  UR(X), R  N

r=1 d=1

R

= min

max
d=1..D

ur(d)

D p

(u(rd))  UR(X), R  N

.

r=1

(8a) (8b)

Proof. We first show that

X ,p  min

1 D

R

D

ur(d)

D p

(ur(d))  UR(X), R  N

r=1 d=1

R

 min

max
d=1..D

ur(d)

D p

(u(rd))  UR(X), R  N

.

r=1

(9a) (9b)

For X, let (u(rd)), be the decomposition that realizes the nuclear p-norm given in equation 2, where we got rid of  by scaling one of the factors. For a fixed r, we can rescale the factors with a vector c such that c  C = c  R+ , c1c2 . . . cD = 1 , leaving the product of the norms unchanged.

d, u~(rd) p = cd u(rd) p = D ur(1) . . . u(rD) . Now that the factors have equal norm:

D

u~(rd)

1D =
D

u~r(d)

D p

=

max
d=1..D

u~r(d)

D p

.

d=1

d=1

(10)

Thus proving the inequalities. For the opposite inequality, we start with a decomposition (u(rd)) realizing the min in either 4a or 4b. Normalizing each factor ur(d) to have unit p-norm and writing the product of the scalings as d, we can use Equation 10 to go back to the original nuclear p-norm formulation.

7.2 GENERALIZATION GUARANTEES FOR THE NUCLEAR -NORM
Proposition 2. For all target tensors Y  {±1}n1×...×nD and an i.i.d sample S of |S| entries in Y sampled under P, with probability 1 -  over the sample selection, the following holds for an L-lipshitz loss and X such that X ,  M :

DP (X; Y ) < DS(X; Y ) + 6L M

D d=1

nd

+

log

4 

|S| 2|S|

(11)

Proof. We follow the proof in (Srebro, 2004, Section 6.2.4). Our class of tensor BM = {X |

X ,  M } is directly the scaled convex hull of a finite set of rank-one sign tensors of size

|BM |  2

D d=1

nd .

Using

(Srebro,

2004,

Theorem

47),

we

can

bound

its

Rademacher

Complexity

by

R^S(BM )  M

4 7

D d=1

nd

+

log |S|



6M

|S|

D d=1

nd

|S|

Where we bounded log |S| by its maximum value :

D d=1

nd,

and

used

 35

<

6.

(12)

12

Under review as a conference paper at ICLR 2018

7.3 NUCLEAR -NORM PROXIMAL DESCENT

We derive the updates for the slightly more complicated (diagonal) ADAGRAD algorithm Duchi et al.
(2011), in the case of composite mirror descent. Our proof follows what is done in Section 5 of
Duchi et al. (2011). To simplify notations in the remainder, as was done in the paper, we use the separability of the loss to work on a fixed r, and set x the concatenation of u(rd) for all d. At time t, let xt be the parameter vector, gt the gradient of the loss,  the step-size, and Ht = I + diag(Gt)1/2 the diagonal adagrad weights (for further details : Duchi et al. (2011)). With  our regularization
parameter, we can write the composite mirror descent update :

xt+1 = argmin{
x

gt, x

1 + (x) +
2

x, Htx

-

Htxt, x }

(13)

Setting u = gt - Htxt, we can rewrite this as :

xt+1 = argmin{ u, x
x

1 + (x) +
2

x, Htx }

(14)

With z = Ht1/2x and a = diag(Ht-1/2), this amounts to solving :

xt+1 = a

argmin{ z + a

u

2 2

+

(a

z

z)}

(15)

With v = -a

u, we need to describe how to solve, for a 0 and z  Rd :

argmin{

z-v

2 2

+

(a

z)}

z

(16)

The specific regularizer we are interested in is the nuclear -norm. We can rewrite our problem with

constraints,

adding

a

1 3

factor

to

the

regularizer

to

simplify

the

final

algorithm's

form

:

argmin
z,C

{

z-v

2 2

+



 3

C3}

a z C

(17)

Introducing dual variable µ 0, our Lagrangian can then be written :

L(z, µ) =

z-v

2 2

+



 3

C

3

+

d

µi(ai|zi| - C)

i

The KKT conditions give us:

(18)

zi L(z, µ) = 0  2(zi - vi) + µiai sign(zi) = 0 C
µi > 0 = |zi| = ai

Using

these

conditions

we find

that

zi

=

sign(vi

)

C ai

if

|vi|

>

C ai

,

and

zi

=

vi

otherwise.

Let



be

a

non-increasing ordering of the sequence (|vi|ai). Then for  defined as the largest index such that

C  I = [|v(-1)|a(-1), |v()|a()], we can write the objective as a convex function of C :

f (C) =
i<

C a(i)

- v(i)

2 + C3 3

(19)

Setting the gradient to zero gives the condition :

C f (C) = 0 

2 i< a(i)

C a(i) - v(i)

+ C2 = 0

(20)

We are looking for roots of this polynomial that lies in the interval I, leading to Algorithm 1.

13

Under review as a conference paper at ICLR 2018

Algorithm 1 Step for the nuclear -norm and adaptive composite mirror descent

procedure STEP(xt, Ht, gt, a, , ) v  a (Htxt - gt)   argsort(|vi|ai) for   len(x) do

(, , )  , 2

i< a-(2i), -2

|v(i) | i< a(i)

if |x()|2 +|x()| +   0 then

C

-+

 2 -4 2

break

end if

end for

for i <  do

y(i)  sign(v(i))C end for

for i   do

y(i)  a(i)v(i) end for

return y

end procedure

Descending order

7.4 GRID SEARCHES

7.4.1 SMALL DATASETS

· KINSHIP:

­ CP Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R  {50, 100, 150, 200, 250, 300},  = 1e-2.

­ CP nuclear 3-norm: R = 200, epoch = 50,  {5e-4, 6e-4, 7e-4, 8e-4, 9e-4, 1e-3, 2e-3, 3e-3},  = 1e-1.



­ CP nuclear -norm: R = 200, epoch = 50,   {1e-6, 5e-5, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3}, mb = 100,  = 1e-1.

­ ComplEx Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R  {50, 100, 150, 200},  = 1e-2.

­ ComplEx nuclear 3-norm: R = 200, epoch = 50,   {5e-6, 7.5e-6, 1e-5, 2.5e-5, 5e-5, 7.5e-5, 1e-4},  = 1e-1.

· UMLS:

­ CP Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R  {50, 100, 150, 200, 250, 300},  = 1e-2.

­ CP nuclear 3-norm: R = 200, epoch = 50,  {5e-5, 7.5e-5, 1e-4, 3e-4, 5e-4, 7e-4, 9e-4, 1e-3, 2e-3},  = 1e-1.



­ CP nuclear -norm: R = 200, epoch = 50,   {0.8, 0.9, 1, 2, 3, 5, 10, 20, 30}, mb = 10000,  = 1e-1.

­ ComplEx Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R  {50, 100, 150, 200},  = 1e-2.

­ ComplEx nuclear 3-norm: R = 200, epoch = 50,   {5e-5, 7.5e-5, 1e-4, 2e-4, 3e-4, 4e-4, 5e-4, 6e-4, 7e-4, 8e-4, 9e-4, 1e-3, 2e-3},  = 1e-1.

· NATION:

­ CP Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R  {50, 100, 150, 200, 250, 300},  = 1e-2.

­ CP nuclear 3-norm: R = 200, epoch = 50,  {6e-3, 7e-3, 8e-3, 9e-3, 1e-2, 1.25e-2, 2e-2, 3e-2},  = 1e-1.



­ CP nuclear -norm: R = 200, epoch = 50,   {1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6}, mb = 50,  = 1e-1.

14

Under review as a conference paper at ICLR 2018

­ ComplEx Baseline :   {0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4}, R {50, 100, 150, 200},  = 1e-2.
­ ComplEx nuclear 3-norm: R = 200, epoch = 50,  {5e-3, 6e-3, 7e-3, 8e-3, 9e-3, 1e-2, 2e-2, 3e-3, 4e-4, 5e-5},  = 1e-1.

 

7.4.2 SVO
· CP (baseline and nuclear 3-norm) : R = 1000,  {1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2},  = 1e-1.
· ComplEx (baseline and nuclear 3-norm) : R = 2000,  {5e-4, 6e-4, 7e-4, 8e-4, 9e-4, 1e-3, 2e-3, 3e-3, 4e-3, 5e-3},  = 1e-1.

 

15

