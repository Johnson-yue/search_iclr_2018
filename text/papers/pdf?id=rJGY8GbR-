Under review as a conference paper at ICLR 2018
DEEP MEAN FIELD THEORY: VARIANCE AND WIDTH VARIATION BY LAYER AS METHODS TO CONTROL GRADIENT EXPLOSION
Anonymous authors Paper under double-blind review
ABSTRACT
A recent line of work has studied the statistical properties of neural networks to great success from a mean field theory perspective, making and verifying very precise predictions of neural network behavior and test time performance. In this paper, we build upon these works to explore two methods for taming the behaviors of random residual networks (with only fully connected layers and no batchnorm). The first method is width variation (WV), i.e. varying the widths of layers as a function of depth. We show that width decay reduces gradient explosion without affecting the mean forward dynamics of the random network. The second method is variance variation (VV), i.e. changing the initialization variances of weights and biases over depth. We show VV, used appropriately, can reduce gradient explosion of tanh and ReLU resnets from exp(( L)) and exp((L)) respectively to constant (1). A complete phase-diagram is derived for how variance decay affects different dynamics, such as those of gradient and activation norms. In particular, we show the existence of many phase transitions where these dynamics switch between exponential, polynomial, logarithmic, and even constant behaviors. Using the obtained mean field theory, we are able to track surprisingly well how VV at initialization time affects training and test time performance on MNIST after a set number of epochs: the level sets of test/train set accuracies coincide with the level sets of the expectations of certain gradient norms or of certain proxies for expressiveness, as computed within our theory.
1 INTRODUCTION
Deep mean field theory studies how random neural networks behave with increasing depth, in the limit as widths go to infinity. Some of the landmark results include precise predictions of norms of activation vectors across depth in feedforward (Poole et al., 2016) and residual networks 1 (Yang and Schoenholz, 2017), correlations between corresponding activation vectors computed from two input vectors (Poole et al., 2016; Yang and Schoenholz, 2017), and gradient norms (Schoenholz et al., 2016; Yang and Schoenholz, 2017). The latter is used to remarkably predict test time performance given only initialization variances (Schoenholz et al., 2016; Yang and Schoenholz, 2017). Overall, a program emerged toward building a mean field theory for state-of-the-art neural architectures as used in the wild, so as to provide optimal initialization parameters quickly for any neural network user.
In this paper, we contribute to this program by studying how width variation (WV), as practiced commonly, can change the behavior of quantities mentioned above, with gradient norm being of central concern. We find that WV can dramatically reduce gradient explosion without affecting the mean dynamics of forward computation, such as the activation norms, although possibly increasing deviation from the mean in the process (Section 4).
We also study a second method, variance variation (VV), for mean field dynamics of a random neural network (Section 5). In this paper, we focus on its application to tanh and ReLU residual networks,
1without batchnorm and with only fully connected layers
1

Under review as a conference paper at ICLR 2018

where we show that VV can dramatically ameliorate gradient explosion, and in the case of ReLU resnet, activation explosion 2.
Previous works (Poole et al., 2016; Schoenholz et al., 2016; Yang and Schoenholz, 2017) have focused on how network architecture and activation functions affect the dynamics of mean field quantities, subject to the constraint that initialization variances and widths are constant across layers. In each combination of (architecture, activation), the mean field dynamics have the same kinds of asymptotics regardless of the variances. For example, tanh feedforward networks have exp((l)) forward and backward dynamics, while tanh residual networks have poly(l) forward and exp(( l)) backward dynamics. Such asymptotics were considered characteristics of the (architecture, activation) combination (Yang and Schoenholz, 2017). We show by counterexample that this perception is erroneous. In fact, as discussed above, WV can control the gradient dynamics arbitrarily and VV can control forward and backward dynamics jointly, all without changing the network architecture or activation. To the best of our knowledge, this is the first time methods for reducing gradient explosion or vanishing have been proposed that vary initialization variance and/or width across layers.
We find that gradient norms and a measure of expressiveness, or more precisely their expectations computed in our theory, make surprisingly good predictors, respectively in two separate phases, of how VV at initialization affects performance after a fixed amount of time training Section 6. However, the predictions are counterintuitive, in that larger gradient explosion seems to correlate with better performance. We explain away this phenomena in one phase by observing expressivity is worsening as gradient explosion is ameliorated, but it is still unclear whether in the other phase larger gradient explosion causes better performance.

2 PRELIMINARIES

Notations and Settings. We adopt the notations of Yang and Schoenholz (2017) and review them

briefly. Consider a vanilla feedforward neural network of L layers, with each layer l having N (l)

neurons;

here layer 0 is the input layer.

Let x(0)

=

(x1(0),

.

.

.

,

x(0)
N (0)

)

be

the

input

vector

to

the

network, and let x(l) for l > 0 be the activation of layer l. Then a neural network is given by the

equations x(il) = (h(il)), hi(l) =

N (l-1) j=1

wi(jl)xj(l-1)

+

b(il)

where

(i)

h(l)

is

the

pre-activation

at

layer l, (ii) w(l) is the weight matrix, (iii) b(l) is the bias vector, and (iv)  is a nonlinearity, for

example tanh or ReLU, which is applied coordinatewise to its input. To lighten up notation, we

suppress the explicit layer numbers l and write xi = (hi), hi =

N j=1

wij xj

+

bi

where

·

implicitly

denotes ·(l), and · denotes ·(l-1) (and analogously, · denotes ·(l+1)). When the widths are constant

N (l) = N (m), m, l, residual network (He et al., 2015; 2016) adds an identity connection or skip

shortcut that "jumps ahead" every couple layers. We adopt one of the simplified residual architectures

defined in Yang and Schoenholz (2017) for ease of analysis 3, in which every residual block is given

by

M
xi = vij (hj ) + ai + xi,
j=1

N
hi = wij xj + bi.
j=1

where M (l) is the width of the "hidden layer" of the residual block, (vi(jl))Ni,j(=l)1,M(l) is a new set of

weights

and

(ai(l)

)N (l)
i=1

is

a

new

set

of

biases

for

every

layer

l.

If

we

were

to

change

the

width

of

a

residual network, as is done in practice, we need to insert "projection" residual blocks (He et al.,

2015; 2016) every couple layers. We assume the following simplified projection residual block in

2This is the phenomenon where in deep ReLU resnet, the activation vectors blow up exponentially with depth, invalidating the computation with infs; see Yang and Schoenholz (2017)
3It is called full residual network by Yang and Schoenholz (2017), but in this paper, we will simply assume this architecture whenever we say residual network.
2

Under review as a conference paper at ICLR 2018

this paper, for the ease of presentation 4:

M
xi = vij (hj ) + yi + ai,
j=1

N
hi = wij xj + bi,
j=1

N
yi = ij xj .
j=1

where

y

=

(yi

)N (l)
i=1

is

a

"projection"

of

x

=

(xi

)N (l-1)
i=1

5,

and

( )N (l),N (l-1)
ij i,j=1

is

the

"projection"

matrix. Note that we only consider fully-connected affine layers instead of convolutional layers.

Deep mean field theory is interested in the "average behavior" of these network when the weights and

biases, wi(jl), bi(l), i(jl), vi(jl), and practice, they respectively have

as(ital)nadraerdsadmevpilaetdioi.nis.d. w(flr)o/mGNa(ul-ss1i)a,ndb(ils)t,rib(ul)t/ionNs.

(Fl-ol1l)o,wiv(nlg)/staMnda(lr)d,

and a(l); here we normalize the variance of weight parameters so that, for example, the variance

of each hi is w2 , assuming each xj is fixed. While previous works have all focused on fixing ·(l)

to be constant across depth, in this paper we are interested in studying varying ·(l). In particular,

other than (l) which we fix at 1 across depth (so that "projection" doesn't act like an "expansion"

or "contraction"), we let ·(l) = ·l-· for each · = v, w, a, b, where ·, · are constant w.r.t. l.

Hereafter the bar notation ·, ·, · do not apply to s, so that, by a, for example, we always mean the

constant a.

Statistical Assumptions. We make several key "mean field" assumptions, which were formulated in its entirety first in Yang and Schoenholz (2017) (though Axiom 2(a) has been stated first by Schoenholz et al. (2016)). While these assumptions may be mathematically unsatisfying, identifying them and discovering that they lead to highly precise prediction is in fact one of the most important contributions of deep mean field theory.
Axiom 1 (Symmetry of activations and gradients). (a) We assume (h(il))2 = (h(jl))2 and (xi(0))2 = (x(j0))2 for any i, j, l. (b) We also assume that the gradient E/xi(l) with respect to the loss function E satisfies (E/xi(l))2 = (E/x(jl))2 for any i, j, l.
Axiom 2 (Gradient independence). (a) We assume the we use a different set of weights for backpropagation than those used to compute the network outputs, but sampled i.i.d. from the same distributions. (b) For any loss function E, we assume that the gradient at layer l, E/x(il), is independent from all activations hj(l) and x(jl-1) from the previous layer.
One can see that Axiom 1(a) is satisfied if the input x(0)  {±1}N and Axiom 1(b) is satisfied if Axiom 2 below is true and the gradient at the last layer E/xL  {±1}N . Axiom 2(a) was first made in Schoenholz et al. (2016) for computing the mean field theory of gradients for feedforward tanh networks. This is similar to the practice of feedback alignment Lillicrap et al. (2014).

Mean Field Quantities. Now we define the central quantities studied in this paper.
Definition 2.1. Fix an input x(0). Define the length quantities q(l) := (h(1l))2 and p(l) := (x1(l))2 for l > 0 and p(0) = x(0) 2/N . Here the expectations · are taken over all random initialization of weights and biases for all layers l, as N   (large width limit).
Note that in our definition, the index 1 does not matter by Axiom 1.
Definition 2.2. Fix two inputs x(0) and x(0) . We write · to denote a quantity · with respect to the input x(0) . Then define the correlation quantities (l) := h(1l)h(1l) and (l) := x(1l)x(1l) for l > 0 and (0) = x(0) · x(0) /N , where the expectations · are taken over all random initialization of weights and biases for all layers l, as N   (large width limit). Again, here the index 1 does not matter by Axiom 1. Additionally, define the cosine distance quantities e(l) := (l)/ p(l)p(l) and c(l) := (l)/ q(l)q(l) .
4If the reader is disappointed by our simplifications, he or she is encouraged to consult He et al. (2016), which empirically explored many variations of residual blocks
5"projection" is in quotes because the dimension N (l) can be less than or greater than N (l-1); but we will follow the traditional wording

3

Under review as a conference paper at ICLR 2018

Table 1: Mean Field Dynamics with VV and WV.

Forward (Thm A.1)

Backward (Thm A.2)

q = w2 l-w p + b2l-b p = v2l-v V(0, q) + a2l-a
 = w2 l-w  + b2l-b  = v2l-v W(0, q, q, ) + a2l-a .

/

=

(N/N)(v2w2 l-v-w V(0, q) + 1) (v2w2 l-v-w V(0, q) + 1)

b = (N/M)v2l-v V(0, q)

w = (N/M)v2l-v V(0, q)p

v = V(0, q) a =  = p.

The two cases for / are resp. for a projection and a normal residual block, assuming  = 1.

Definition 2.3. Fix an input x(0) and a gradient vector (E/xi(L))i of some loss function E with respect to the last layer x(L). Then define the gradient quantities (l) := (E/x1(l))2 , ·(l) := (E/·(1l))2 for · = a, b, and ·(l) := (E/·(1l1))2 for · = w, v. Here the expectations are taken with Axiom 2 in mind, over both random initialization of forward and backward weights and biases, as N   (large width limit). Again, the index 1 or 11 does not matter by Axiom 1.
Asymptotic notations. The expressions f = O(g)  g = (f ) have their typical meanings, and f = (g) iff f = O(g), g = O(f ). We take f (x) = O~(g(x))  g(x) = ~ (f (x)) to mean f (x) = O(g logk x) for some k  Z (this is slightly different from the standard usage of O~), and f = ~ (g)  f = O~(g) & g = O~(f ). All asymptotic notations are sign-less, i.e. can indicate either positive or negative quantities, unless stated otherwise.

3 THEORY

We recall integral transforms from Yang and Schoenholz (2017):

Definition 3.1. Define the transforms V and W by V(µ, ) := E[(z)2 : z  N (µ, )] and

W(µ, ,  , ) := E[(z)(z ) : (z, z )  N (µ,

 

 

)].

Table 1 summarizes the equations governing the dynamics of various mean field quantities when we vary variances and/or widths; they are proved similarly to how corresponding recurrences were proved in Yang and Schoenholz (2017); Schoenholz et al. (2016).

4 WIDTH VARIATION
Width variation is first passingly mentioned in Schoenholz et al. (2016) as a potential way to guide gradient dynamics for feedforward networks. We develop a complete mean field theory of WV for residual networks.
Via Table 1 and Thm A.2, we see that width variation (WV) has two kinds of effects on the mean gradient norm: when compared to no width variation, it can multiply the squared gradient norm of biases bi or weights wij by N/M (which doesn't "stack", i.e. does not affect the squared gradient norm of lower layers), or it can multiply the squared gradient norm of xi by N/N (which "stacks", in the sense above, through the dynamics of ). We will focus on the latter "stacking" effect and assume N (l) = M (l)
Suppose from layer l to layer m, (m) rises to r times (l). If we vary the width so that N (m-1) is rN (m), then this gradient expansion is canceled, and (m-1) = (N (m)/N (m-1))(v2w2 l-v-w V(0, q(m)) + 1) (m) = (v2w2 l-v-w V(0, q(m)) + 1) (l) so that it is as if we restarted backpropagation at layer m.

4

Under review as a conference paper at ICLR 2018

no WV 250 WV
theory no WV 200 WV
150
100
50
0 0 20

p: WV vs no WV
40 60 layer l

80

1.05 1.00 0.95 0.90 0.85 0.80 0.75 0.70 0.65
0

e: WV vs no WV
20 40 60 layer l

103

102

101

100

no WV WV e(0) = 0.01 no WV WV e(0) = 0.79
80

10-1 10-2 10-3

 (l) b w v (l) WV
b WV w WV v WV

gradients: WV vs no WV

80 60 40 20 layer l, reversed

0

Figure 1: Using WV to control the gradient explosion of a tanh resnet. Shades indicate standard deviation (taken in normal scale, but possibly displayed in log scale), while solid lines indicate the corresponding mean. The left two plots show that mean forward dynamics are more or less preserved, albeit variance explodes toward the deeper layers, where WV is applied. The last plot show that the gradient dynamics is essentially suppressed to be a constant compared to the exp( L) dynamics of tanh resnet without width decay. Dashed lines indicate theoretical estimates in all three plot; solid, simulated data, which is generated from random residual networks with 100 layers and N (0) = 2048, and we half the widths at layers l = m2 for m = 4, 5, . . . , 9.

Remarkably, changing the width does not change the mean field forward dynamics (for example the recurrences for p, q, ,  remain the same) (Thm A.1). But, as always, if we reduce the width as part of WV (say, keeping N (0) the same but reducing the widths of later layers), the variance of the sampled dynamics will also increase; if we increase the width as part of WV (say, keeping N (L) the
same but increasing the widths of earlier layers), the variance of the sampled dynamics will decrease.

We can apply this theory of WV to tanh residual networks ( = tanh in Table 1) without VV.

By Yang and Schoenholz log( (m)/ (l)) = A( l

(2017), tanh residual networks with 
- m) + (log l - log m) where

all A

· =

=
4 3

0 have gradient

 .2 v2w
 v2 +a2

If

dynamics we place

projection blocks projecting N (l-1)  N (l)/ exp(A) at layer l = n2, for n = 1, 2, . . ., then the

gradient norms would be bounded (above and below) across layers, as reasoned above. Indeed, this

is what we see in Fig. 1. The rightmost subfigure compares, with log scale y-axis, the gradient

dynamics with no WV to that with WV as described above. We see that our theory tracks the mean

gradient dynamics remarkably precisely for both the WV and the no WV cases, and indeed, WV

effectively caps the gradient norm for l  16 (where WV is applied). The left two figures show

the forward dynamics of p and e, and we see that the WV does not affect the mean dynamics as

predicted by theory. However, we also see dramatic increase in deviation from the mean dynamics at

every projection layer in the forward case. The backward dynamics similarly see large deviations

(1 standard deviation below mean is negative for a and w), although the deviations for is more

tamed but still much larger than without WV.

Therefore, width variation is unique in a few ways among all the techniques discussed in the mean field networks literature so far, including variance decay as studied below, adding skip connections, or changing activation functions: It can ameliorate or suppress altogether gradient explosion (or vanishing) problems without affecting the mean forward dynamics of p, q, , , c, e. To do so, it has to choose a trade off from the following spectrum: At one end, we truncate neurons from the original network (say, keeping N (0) the same), so that we have fewer parameters, less compute, but larger deviations from the mean dynamics. At the other, we add neuron to the original network (say, keeping N (L) the same), so that we have more parameters, more compute, and smaller deviations from the mean dynamics.

5 VARIANCE VARIATION
As discuss in Section 4, WV is essentially a post-hoc technique to tweak an existing gradient dynamic without changing the forward dynamics. Thus in this section we assume all widths are constant, N (l) = N (m) = M (n) for any m, l, n, so that WV can be applied as a "touch-up" if necessary.
5

Under review as a conference paper at ICLR 2018

v = 0.1, w = 0.1, a = 0.1, b = 0.1;  = 0.2,  = 0.1 102
101

v = 0.5, w = 0.5, a = 0.5, b = 0.5;  = 1.0,  = 0.5 101

v = 0.8, w = 0.8, a = 0.5, b = 0.5;  = 1.6,  = 0.5

100 100

101 102 layer l

log (0)/ (l)

1

 -



l

1

-



103

100 100

101 102 layer l

 (0)/ (l) l
103

100 100

101 102 layer l

 (0)/ (l) 1
103

Figure 2: We verify the asymptotic characterizations of given by Thm A.8. Here solid lines indicate

computations done via the recurrences Table 1, while dashed lines indicate asymptotics proved in Thm A.8.

To facilitate visualization, all green (but not blue) dashed lines are shifted vertically to match the end point of

the corresponding solid lines. From left to right: (a) log-log plot of the log of (0)/ (l), against the predicted

leading

term

 1-

l1-

,

when



=

.2;

thus

(0) /

(l) is superpolynomial. As in Thm A.8,  =

1 2

v2

w2

.

(b)

log-log plot of (0)/ (l) when  = 1, showing it is polynomial. (c) (0)/ (l) is bounded above when  = 1.6.

5.1 RESIDUAL NETWORK WITH RELU
It was shown in Yang and Schoenholz (2017) that, with no variance decay, in an ReLU resnet, both the mean squared activation norm (p) and the mean squared gradient norm ( ) explode exponentially with depth, and this causes training to fail for even 100 layer networks. We show that this problem is in fact extremely easy to fix, requiring no architectural changes at all, only that · be increased from 0 so that the randomization variances decay across depth (Thms A.6 and A.8).
Gradient quantities. The main driver of this gradient mollification is  := v + w. When   [0, 1), the gradient norm varies like (0)/ (l) = exp((l1-)); when  = 0 this recapitulate the exponential behavior derived in Yang and Schoenholz (2017). When  = 1, it experiences a sharp phase transition, where now (0)/ (l) = poly(l). As  becomes larger than 1, (0)/ (l) = (1), all bounded! Fig. 2 verifies this result empirically, and in fact show that our computed asymptotic expansions in Thm A.8 are highly accurate predictors. It is both interesting and important to note that the gradient norms for actual trainable parameters, such as w and v, are affected differently by . In fact, w(1)/(wl) is bounded(!) with l when  < 1 (the  = 0 case is already observed in Yang and Schoenholz (2017)) but phase transitions to poly(l) for   1, while (v1)/v(l) is already poly(l) when  < 1, and remains so as  is increased to > 1. Curiously, v turns out to be more predictive of performance than in the  > 1 regime, in a surprising way; see Section 6.
Length quantities. Similarly,  is the primary conduit for mollifying the behavior of squared activation norms p and q (Thm A.6). Like the gradient dynamics, p = exp((l1-)) when  < 1; when  = 0 this recapitulates the results of Yang and Schoenholz (2017). As  rise to 1 and above, p experiences a phase transition into polynomial dynamics, but unlike the case of , it is not constant when  > 1. Instead, a different parameter,  := min(v + b, a) drives the asymptotics of p in the  > 1 regime. When   [0, 1) is small, p grows like (l1-). The instant  hits 1, p is just logarithmic, p = (log l). As  shoots past 1, p becomes constant. Thus the dynamics of p is governed by a zigzag through (·)·=v,w,a space. Fig. 3 goes through each of the five cases discussed above and verifies that our asymptotics are correct.
Cosine distance. e = /p measures how well the input space geometry (angles, in this case) is preserved as the input space is propagated through each layer. Its dynamics are much simpler than those of p and above. If e(0) = 1, then e(l) = 1 for all l trivially. If e(0) < 1, then we have one of the following two cases, · If   1 or   1, then e  1 irrespective of initial data p(0) and (0). · If  > 1 and  > 1, then e converges to a fixed point e < 1 which depends on the initial data p(0) and (0), at a rate of (l1-). Thus ReLU very much likes to collapse the input space into a single point (e = 1 means every two input vectors get mapped to the same output vector), and the only way to prevent this from happening is to make the ·s so high, that higher layers barely modifies the computation done by lower layers at all. Indeed, the second condition  > 1 and  > 1 ensures that
6

Under review as a conference paper at ICLR 2018

 = 0.2,  = 0.1 v = w = 0.1, a = 0.1, b = 0.1 102
101

 = 1.0,  = 0.5 v = w = 0.5, a = 0.5, b = 0.5
101

 = 1.6,  = 0.5 v = w = 0.8, a = 0.5, b = 0.5 102
101

 = 1.6,  = 1 v = w = 0.8, a = 1, b = 0.5
100

 = 1.6,  = 1.3 v = w = 0.8, a = 1.5, b = 0.5 101
100

100

10-1 10-2
100

logp

1

 -



l

1

-



logp

-

1

 -



l

1

-



l1 - 2

101 102 layer l

100

10-1

p/logl l
p/logl l - 1

103 100 101 102 layer l

100

10-1

p l1 - 
p l -

103 100 101 102 layer l

10-1

10-2

p/logl

10-3

1
p l -1

103 100 101 102

layer l

10-1

10-2

10-3

p 1

p l -

103

10-4 100

101

102

layer l

103

Figure 3: We verify the asymptotic characterizations of p given by Thm A.6. Here solid lines indicate

computations done via the recurrences Table 1, while dashed lines indicate asymptotics proved in Thm A.6. In

all but the leftmost plot, we show both p and p = p - p (possibly adjusted for log factors) and their asymptotics.

To facilitate visualization, all dashed lines except the red one in the leftmost plot are shifted vertically to match

the end point of the corresponding solid lines. In the leftmost plot, the red lines are respectively log p (solid)

and

the

leading

term

 1-

l1-

in

its

asymptotic

expansion

(dashed).

The

green

lines

in

the

same

plot

are

the

remainder (solid) and its polynomial asymptotic form, ignoring the leading coefficient (dashed). From left to

right gives one example for each of the 5 cases discussed in the main text: (a)  < 1 and p = exp(poly(l)). (b)

 = 1 and p phase transitions into polynomial dynamics. (c)  > 1,  < 1 and p = poly(l). (d)  > 1,  = 1 and p = (log l). (e)  > 1,  > 1 and p becomes bounded but p = (l-).

p = (1) as discussed above (Thm A.6), so that as l  , layer l's residual adds to x(l-1) only a vector of vanishing size compared to the size of x(l-1).

5.2 RESIDUAL NETWORK WITH TANH
While ReLU resnet depends heavily on  = v + w and  = min(v + b, a), µ^p := min(v, a) and ^p := 1 - w + µ^p - 2v are the key quantity determining the dynamics in the case of tanh resnet, with  playing a minor role. We will study tanh resnets in the setting where q  ; otherwise, p is bounded, meaning that higher layers become essentially unused by the neural network (similar to the discussion made in Section 5.1(Cosine distance) above). In this setting, it can be shown that µ^p  1 (Thm A.3).

Gradient quantities. By Thm A.4, as long as µ^p stays below 1, the asymptotics of is entirely

governed by ^p, which is 1 when all ·s are 0 and most of the time decreases as ·s are increased.

When ^p > 0,

(0)/

(l)

=

exp((l

1 2

^p

));

the

results

of

Yang

and

Schoenholz

(2017)

are

recovered

by setting all ·s to 0, thus ^p = 1 and

(0)/

(l)

=

exp((l

1 2

)).

When

^p

hits

0,

(0)/ (l) becomes

polynomial, and as ^p dips below 0, gradient expansion becomes bounded.

If µ^p = 1, (0)/ (l) is automatically suppressed to be subpolynomial. The only minor phase transition here is going from  = 1 to  > 1 (and  cannot beless than 1 by our assumption that q  ). In the former case, the gradient expansion is exp(( log l)), while in the latter case it is
bounded.

Length quantities have simpler asymptotics determined by µ^p: either µ^p < 1 and p = (l1-µ^p ),

or µ^p = 1 and p = (log l) (Thm A.3). Cosine distance, unlike the case of ReLU resnets, can

be controlled effectively by a and v (Thm A.5). When a > v, the magnitude of a(il) drops much more quickly with depth than that of vi(jl), so that higher layers experience the chaotic phase

(Schoenholz et al., 2016; Yang and Schoenholz, 2017), driving e(l) toward the limit point e = 0. On

the other end, when a < v, vi(jl) vanishes in comparison to a(il) with large l, so that the higher layers experience the stability phase (Schoenholz et al., 2016; Yang and Schoenholz, 2017), collapsing all

inputs to the same output vector, sending e(l)  1. Only when a = v could the fixed point e be

controlled

explicitly

by

v

and

a,

with

e

given

by

the

equation

e

=

v2

2 

arcsin(e )+a2 v2 +a2

as

in

the

case with no VV (Yang and Schoenholz, 2017).

7

Under review as a conference paper at ICLR 2018

Figure 4: Zigzagging through ·-space with ReLU resnet. In each pair, plots indicate performance as w = b is varied (the zig, left) and, with w = b fixed at 2, as a is varied (the zag, right). The left pair corresponds to test set performance; the right pair corresponds to training set performance. Brighter color means higher accuracy, for both the heatmaps and overlaid contour plots; color scales are the same for all heatmaps. See main text for details.

6 EXPERIMENTS

As explained in Section 5.1, a zigzag of parameters control various asymptotics of ReLU resnet:

"zigging"  from 0 to > 1, and then "zagging"  from 0 to > 1. To investigate how this zigzag affects

neural network training and performance, we trained a grid of ReLU residual networks, pinning all

·s at 1, but varying the ·s as follows: 1. The zig: fix v = a = 0; fix w = b and increase both

from 0 to 2 (making  = w + v go from 0 to 2 as well); 2. The zag: fix v = a = 0, w = b = 2;

increase a from 0 to 2 (increasing  from 0 to 2 as well). For each setting of the hyperparameters,

we train a network on MNIST with those hyperparameters for 30 epochs. We then report the accuracy

that the network achieved on the training and test sets 6. Fig. 4 shows the result, with the zig on

the left and the zag on the right. In the zig, we have overlaid a contour plot of p - , which is

almost identical to the contour plots of p and (0)/ (l), and for simplicity's sake, in the following we

assume that they are exactly identical. This contour decreases to the right. The dashed line is given

by

 1-

l1-

=

C

for

C



10.

In

the

zag,

we

have

overlaid

a

contour

plot

of

v(1)/(vl).

This

contour

increases to the upper right.

In addition, we provide heatmap and contour plots of various quantities of interest such as p, e, and v(1)/v(l) in both the zig and the zag regimes in Fig. A.1 and Fig. A.2 respectively.

Discussion. First notice that training fails in the upper left corner of the zag. This happens because

of numerical instability caused by exploding p and

(0)/

(l),

which

grow

like

exp(

 1-

l1-

+

o(l1-)). Indeed, one of the contour lines of p traces out almost exactly where training fails. The

dashed line is a level set of the dominant term of asymptotic expansion of log p, and we see it agrees

with the contour of p very well.

Second, observe that performance actually dips in the direction where (0)/ (l) decreases, quite counterintuitively. This can be explained by noting that gradient against weights, w and v, in fact respectively remain bounded and polynomial in l (and changes rather mildly with ; see Fig. A.1); gradient against biases do experience the same behavior as , but in general they are much less important than the weights, as parameters go. In addition, the performance is also dipping in the direction where p -  decreases (exponentially) in (, L)-space. This is the quantity that essentially underlies the exponential expressivity result (as told from an extrinsic curvature perspective) of Poole et al. (2016); as p -  decreases dramatically, it gets harder and harder for a linear functional in the final layer to tell apart two input vectors. This exponential loss in expressivity dominates the effect on performance more than a polynomial reduction in gradient explosion.

Third, it is remarkable that in the zag regime, the level curves of (v1)/v(l) accurately predict the contour of the test set performance, in such a counterintuitive way that greater gradient explosion (v1)/(vl) correlates with better performance. It is currently unclear if in certain situations like this, larger gradient expansion is actually beneficial, or if there is a quantity yet undiscovered which has

6Thus we are investigating a question of training speed rather than one of final generalization accuracy

8

Under review as a conference paper at ICLR 2018
the same level curves and can explain away this seeming paradox (like how p -  explains away (0)/ (l) above, in the zig regime). Of the quantities that appear in Fig. A.2, none foots this bill.
7 CONCLUSION
In this paper, we derived the mean field theory of width and variance variation and showed that they are powerful methods to control forward (VV) and backward (VV + WV) dynamics. We proved that even with a fixed architecture and activation function, the mean field dynamics of a residual neural network can still be manipulated at will by these two methods. Extraordinarily, the mean field theory we developed allowed us to accurately predict the performances of trained MNIST models, but one puzzling aspect remains where test set accuracy seems to increase as gradient explosion worsens. Open Problems. We solved a small part, width variation, of the program to construct mean field theories of state-of-the-art neural networks used in practice. Many open problems still remain, and the most important of them include but is not limited to 1. batchnorm, 2. convolution layers, and 3. recurrent layers. In addition, more work is needed to mathematically justify our "physical" assumptions Axiom 1 and Axiom 2 to a "math" problem. We hope readers will take note and contribute toward deep mean field theory.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pages 342­350, 2009. URL http://papers.nips.cc/paper/ 3628-kernel-methods-for-deep-learning.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512. 03385. arXiv: 1512.03385.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual Networks. arXiv:1603.05027 [cs], March 2016. URL http://arxiv.org/abs/1603.05027. arXiv: 1603.05027.
Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random feedback weights support learning in deep neural networks. arXiv:1411.0247 [cs, q-bio], November 2014. URL http://arxiv.org/abs/1411.0247. arXiv: 1411.0247.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. arXiv:1606.05340 [cond-mat, stat], June 2016. URL http://arxiv.org/abs/1606.05340. arXiv: 1606.05340.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/abs/ 1611.01232. arXiv: 1611.01232.
Greg Yang and Samuel S. Schoenholz. Meanfield Residual Network: On the Edge of Chaos. In Advances in neural information processing systems, 2017.
10

Under review as a conference paper at ICLR 2018

0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 1.0 1.0 1.2 1.2 1.4 1.4 1.6 1.6 1.8 1.8 2.0 2.0

log1 + logp
297 288 279 270 261 252 243 234 225 216 207 198 189 180 171 162 153 144 135 126 117 108
99 90 81 72 63 54 45 36 27 18
9 0

297

288

279

270

261

4

252 243

234

225

216

207

198

3 189 180

171

162

153

144

135

2 126 117

108

99

90

81

72

1 63

54

45

36

27

18

9

00

log1 + log(p - )

297

288

279

270

261

4.0 252

243

234

225

216

3.2

207 198

189

180

171

162

2.4 153 144

135

126

117

108 1.6 99

90

81

72

63

0.8

54 45

36

27

18

9

0.0 0

log(1 - /p)

zig: a = 0, v = 0, w = b : 02

-1.5 -3.0 -4.5 -6.0

297 288 279 270 261 252 243 234 225 216 207 198 189 180 171 162 153 144 135 126 117 108
99 90 81 72 63 54 45 36 27 18
9 0

log1 + log(0)/(l)

297 288 279 270 261 4 252 243 234 225 216 207 198 3 189 180 171 162 153 144 135 2 126 117 108
99 90 81 72 1 63 54 45 36 27 18
9 00

logv(0)/v(l)

297

288

279 6.0 270

261

252

243

234

225

4.5

216 207

198

189

180

171

162

3.0 153 144

135

126

117

108

99

1.5 90 81

72

63

54

45

36

0.0 27

18

9

0

logw(0)/w(l)

0 -1 -2 -3 -4 -5

250 250 250 250 250 250

200 200 200 200 200 200

150 150 150 150 150 150

100 100 100 100 100 100

50 50 50 50 50 50

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

Figure A.1: Heatmap and contour plots of various quantities in the "zig" regime of Section 6. Note that we squashed things like p with large range so that the colors are meaningful.

logp
297 288 279 270 261 252 243 234 225 216 207 198 189 180 171 162 153 144 135 126 117 108
99 90 81 72 63 54 45 36 27 18
9 0

245

238

231

224

5 217

210

203

196

189

182

4 175

168

161

154

147

140

3 133 126

119

112

105

98

2

91 84

77

70

63

56

49 1 42

35

28

21

14

7

00

log(p - )

0.5850 0.5825 0.5800 0.5775 0.5750 0.5725

297 288 279 270 261 252 243 234 225 216 207 198 189 180 171 162 153 144 135 126 117 108
99 90 81 72 63 54 45 36 27 18
9 0

log(1 - /p)

zag: a : 02, v = 0, w = b = 2
245 238 231 224 217 210 -1 203 196 189 182 175 168 161 -2 154 147 140 133 126 119 112 -3 105
98 91 84 77 70 63 -4 56 49 42 35 28 21 14 -5 7
0

log (0)/ (l)

0.3105 0.3090 0.3075 0.3060 0.3045

297 288 279 270 261 252 243 234 225 216 207 198 189 180 171 162 153 144 135 126 117 108
99 90 81 72 63 54 45 36 27 18
9 0

logv(0)/v(l)

297

10

288 279

270

261

252

243

234

8 225

216

207

198

189

180

6 171 162

153

144

135

126

4

117 108

99

90

81

72

63 2 54

45

36

27

18

9

00

logw(0)/w(l)

0 -1 -2 -3 -4 -5

250 250 250 250 200 200
200 200 200 200 150 150
150 150 150 150
100 100 100 100 100 100
50 50 50 50 50 50

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8

Figure A.2: Heatmap and contour plots of various quantities in the "zag" regime of Section 6. Note that we squashed things like p with large range so that the colors are meaningful.

Appendices

A MAIN THEOREMS

A.1 RECURRENCES

Yang and Schoenholz (2017) gave recurrences for mean field quantities p, q, , , under the assumption of constant initialization variances across depth. The proofs there carry over straightforwardly when variance varies from layer to layer. Schoenholz et al. (2016) also derived backward dynamics for when the width of the network is not constant. Generalizing to the residual network case requires some careful justifications of independences, so we provide proof for gradient dynamics; but we omit the proof for the forward dynamics as it is not affected by nonconstant width and is almost identical to the constant variance case.
Theorem A.1. For any nonlinearity  in an FRN, regardless of whether widths vary across layers,
q(l) = w2 l-w p(l-1) + b2l-b p(l) = v2l-v V(0, q(l)) + a2l-a + p(l-1) (l) = w2 l-w (l-1) + b2l-b
(l) = v2l-v W(0, q(l), q(l) , (l)) + a2l-a + (l-1).

Theorem A.2. Suppose a random residual network receives a fixed gradient vector

E/xi(L)

N (l) i=1

with respect to some cost function E, at its last layer. For any nonlinearity  in an FRN, under

11

Under review as a conference paper at ICLR 2018

Axiom 1 and Axiom 2, whenever ()2 has finite variance for any Gaussian variable ,

=

(N/N)(v2w2 l-v-w V(0, q) + 2 ) (v2w2 l-v-w V(0, q) + 1)

b = (N/M)v2l-v V(0, q)

w = (N/M)v2l-v V(0, q)p

v = V(0, q)

a =

 = p.

if layer l has projection connection if layer l has identity connection

Proof. We will show the proof for the projection connection case; the identity connection case is similar but easier.

Write

(l) i

=

.E
 x(il)

We

have

the

following

derivative

computations:

xi xj

= ij

+

k

vik

 (hk

)

hk xj

,

xi hj

=

vij  (hj ),

xi vik

=

(hk ),

hi xj

= wij ,

xi = 1, ai

hi wij

= xj,

xi ij

= xj

hi = 1, bi

Then

N
j=
i=1 N
=
i=1

i(ij

+

M k=1

vik

 (hk )

hk xj

)

M
i(ij + vik(hk)wkj )

k=1

Thus,

NM

2 j

=

[

i(ij + vik(hk)wkj )]2

i=1 k=1

NM

=

2 i

[(ij

+

vik(hk)wkj )]2

i=1 k=1

MM
+ 2 i i (ij + vik(hk)wkj )(i j + vi k(hk)wkj )

i<i k=1

k=1

=N

M

2 1

(

121

+

[

v1k(hk)wkj ]2 )

k=1

MM
+ 2 i i (ij + vik(hk)wkj )(i j + vi k(hk)wkj )

i<i k=1

k=1

where in the second equality, we expanded algebraically, and in the third equality, we use the symmetry

assumption Axiom 1 and the independence assumption Axiom 2. Now, [

M k=1

v1k

 (hk

)wkj

]2

=

M k=1

v12k  (hk )2 wk2j

=

M k=1

v2 l-v M

V (0,

q)

w2 l-w N

=

v2l-v

V (0,

q)

w2 l-w N

by the indepen-

dence of {vik}i,k  {hk}k  {wkj}k,j (by our independence assumptions Axiom 2). Similarly,

because {ij}j  {i j}j  {vik}k  {vi k}k for i = i is mutually independent by our assumptions,

one can easily see that (ij +

M k=1

vik(hk)wkj )(i

j

+

M k=1

vi

k(hk)wkj )

= 0.

12

Under review as a conference paper at ICLR 2018

Therefore,

=N

( 2 N

+

v2l-v

V (0,

q)

w2 l-w N

)

N =
N

(2 + v2w2 l-v-w V(0, q)).

For the other gradients, we have (where we apply Axiom 2 implicitly)

E =

N

E xi hj =

N

bj i=1 xi hj bj i=1

ivij (hj )

N
b = [

ivij (hj )]2

=N

v2l-v M

V(0, q)

=

N M

v2l-v

V(0, q);

i=1

E = N E xi hj = N wji i=1 xi hj wji i=1

ivij (hj )xj

N
w =

ivij (hj )xj

=N

v2l-v V(0, q)p M

=

N M

v2l-v

V(0, q)p

i=1

E

E =

xi

=

vik xi vik

i(hk) = v = [ i(hk)]2 =

V(0, q)

E = E xi = E ai xi ai xi

=

a =

E

E =

xi

=

ij xi ij

ixj =  =

p

A.2 RESIDUAL NETWORKS WITH TANH

In this section we derive the asymptotics of various mean field quantities for tanh resnet. The main proof technique is to bound the dynamics in question with known dynamics of difference equations (as in Yang and Schoenholz (2017)).
Let µ^p := min(v, a). Theorem A.3. Suppose  = tanh, and q(l)   as l  .

1. If µ^p < 1, then 1 > w + µ^p and p(l) = (l1-µ^p ) q(l) = (l1-w-µ^p ).

More specifically,

(v2l1-v , v2w2 l1-v-w ) (p(l), q(l))  (a2l1-a , a2w2 l1-a-w )
((a2 + v2)l1-v , (a2 + v2)w2 l1-v-w )

if v < a if a < v if a = v.

2. If µ^p = 1, then w = 0, and p(l) = (log l) = q(l). More specifically,

(v2, w2 v2 log l) (p(l), q(l))  (a2 log l, w2 a2 log l)
((a2 + v2) log l, w2 (a2 + v2) log l)

if v < a if v > a if v = a.

13

Under review as a conference paper at ICLR 2018

3. µ^p cannot be greater than 1.

Proof. Claim 1. We have V(0, q) = 1 -

2 

q-1/2

+

(q-3/2)

by

Lemma

B.5.

Thus

p - p = v2l-v (1 - 2/q-1/2 + (q-3/2)) + a2l-a

= v2l-v + a2l-a - o(l-v )

v2l1-v p  a2l1-a
(a2 + v2)l1-v

if v < a if a < v if a = v

where we used the assumption 1 - µ^p > 0. Thus p(l) = (l1-µ^p ) and goes to infinity with l. Then q = w2 l-w p + b2l-b . Because we assumed q  , the first term necessarily dominates the second, and 1 - µ^p - w > 0. The possible asymptotics of q are then

v2w2 l1-v-w q  a2w2 l1-a-w
(a2 + v2)w2 l1-v-w

if v < a if v > a if a = v

which gives q(l) = (l1-w-µ^p ).

Claim 2. If µ^p = 1, then

v2 log l p  a2 log l
(a2 + v2) log l

if v < a if v > a if v = a.

Then for q to go to infinity, w has to be 0, so that q = (log l) as well, and

w2 v2 log l q  w2 a2 log l
w2 (a2 + v2) log l

if v < a if v > a if v = a.

Claim 3. If µ^p > 1, then p = (1) = q = (1), a contradiction.

Theorem A.4. Suppose  = tanh, and q(l)   with l. Then log( (0)/ (l)) is

If µ^p < 1

If 1 - w + µ^p - 2v > 0

(l

1 2

(1-w

+µ^p

-2v

)

)

If 1 - w + µ^p - 2v = 0

(log l)

If 1 - w + µ^p - 2v < 0

(1)

If µ^p = 1
If v + w < 1 This case cannot happen
If v +w = 1 ( log l)
If v + w > 1 (1)

14

Under review as a conference paper at ICLR 2018

Proof. By Thm A.2 and Lemma B.4, we have
log( / ) = log(1 + v2w2 l-v-w V(0, q)) = log(1 + v2w2 l-v-w (Cq-1/2 + (q-3/2))) = v2w2 l-v-w Cq-1/2 + (l-2v-2w )

where

C

=

2 3

2 

.

Since

q(l)

has

different

growth

rates

depending

on

the

hyperparameters,

we

need

to consider different cases:

· If µ^p < 1, then Thm A.3 implies w + µ^p < 1 and q = (l1-w-µ^p ). So l-v-w q-1/2 =

l-v

-w

(l-

1 2

(1-w

-µ^p

)

)

=

(l

1 2

(-1-w

+µ^p

-2v

)

).

 If 1 - w + µ^p - 2v > 0, log (0)/ (l) =

(l

1 2

(1-w

+µ^p

-2v

)

).

l m=1

(m

1 2

(-1-w +µ^p -2v ) )

=

 If 1 - w + µ^p - 2v = 0, then l-v-w q-1/2 = (l-1). Thus log (0)/ (l) =

l m=1

(m-1)

=

(log

l).



If 1 - w + µ^p - 2v

< 0, then

1 2

(-1

-

w

+ µ^p - 2v) > 1, so that log

(0)/

(l) =

(1).

·

If (lµ^-pv=-w1/, thloegn

by l).

Thm

A.3,

w

=

0 and q

=

(log l).

So l-v-w q-1/2 =

 

If v + w If v + w

< 1, then v < 1 = = 1, then log (0)/ (l)

µ^p =

< 1, contradiction.

l m=1

(l-1

 / log

l)

=

 ( log

l).

 If v + w > 1, then log (0)/ (l) = (1).

Theorem A.5. Suppose  = tanh and q  . If e(0) < 1, then e(l) converges to a fixed point e,

given by the equations

0





v2

2 

arcsin(e )+a2

e = v2+a2



1

if a > v if a = v if a < v.

Note that in the case a = v, we recover the fixed point of tanh residual network without variance decay.

Proof. We have

v2l-v W(0, q, q, cq) + a2l-a =  -  = ep - ep

= ep - ep + ep - eep

= (e - e)p + e(p - p)

=

(e

+

(e

-

e)

p

p -

p

)(p

-

p)

v2l-v W(0, q, q, cq) + a2l-a v2l-v V(0, q) + a2l-a

-e

=

(e

-

e)

p

p -

p

Using Lemma B.1, ?? and Lemma B.5, we can see that the LHS is monotonic (increasing or decreasing) for large enough l. Therefore e(l) is a bounded monotonic sequence for large enough l, a fortiori it has a fixed point.

15

Under review as a conference paper at ICLR 2018

If we express e = e + , (l) = (e + (l))p(l), then

v2l-v W(0, q, q, cq) + a2l-a v2l-v V(0, q) + a2l-a

= e +

+(

-

p )p - p

RHS It's easy to verify via Thm A.3 that either p/(p - p) = (l log l) (when µ^p = 1) or p/(p - p) =

(l) (all other cases). If - = ((l log l)-1), then = (log log l) (by Euler-MacLaurin formula),

which would be a contradiction to

= o(1). Thus (

-

)

p p-p

=

o(1).

7

Hence

the

RHS

goes

to

0

with l.

LHS If v > a, then the LHS converges to 1. So e = 1.

If v

<

a, then the LHS 

W(0,q,q,cq) V(0,q)

.

As l

 , c



e

 e, and W(0, q, q, eq) 

2 

arcsin(e),

and

V(0, q)



1.

Therefore,

e

=

2 

arcsin(e),

for

which

there

are

2

solutions,

0,

the

stable fixed point, and 1, the unstable fixed point. In particular, for all e(0) < 1, e(l)  0.

If v = a, then taking limits l  , we get

v2

2 

arcsin(e) + a2 v2 + a2

=

e.

A.3 RESIDUAL NETWORK WITH RELU

The asymptotics of ReLU resnet depends on the following values:

 = v + b,

 = min(v + b, a),



=

1 2

v2w2

Theorem A.6.

Let 

=

ReLU.

Then with 

=

1 2

v2w2

and



= min(v + b, a) = v +

min(b, a - v), we have the following asymptotics of p and q:

· If v + w > 1

 If   [0, 1)

p = (l1-) and q = (lmax(1--w,-b)).

 If  = 1

p = (log l) and q =

(l-w log l) (l-b )

if w  b otherwise.

 If  > 1 p = (1), p - p = (l-) and q = (l- min(w,b)).

· If v + w = 1

 If  = 1 - 

p = (lmax(,1-)), and q = (lmax(max(,1-)-w,-b)).

 Otherwise

p = (l log l) and q =

(l-w log l) (l-b )

if b  w -  otherwise

· If v + w < 1

7 In fact, since must be o(log log · · · log l) for any chain of logs, - = o((l log l log log l · · · log(k) l)-1)

for any k, where log(k) is k-wise composition of log; so (

-

)

p p-p

=

o((log log l · · · log(k) l)-1)

for any

k.

16

Under review as a conference paper at ICLR 2018

 p, q

=

exp(

 1-

l1-

+

~ (lmax(0,1-2))).

In particular, p =

exp(

 1-v -w

l1-v -w

+

R(l;

,

v

+

w )

+

O(1)),

R

is

the

same

R

as

in

Lemma B.7, depending on only l, , and v + w, with

(l1-2) if  < 1/2  R = (log l) if  = 1/2

(1)

if  > 1/2.

and

q

=

exp(

 1-

l1-v -w

+

R(l;

,

v

+

w )

-

w

log

l

+

O(1)).

In particular, p, q = poly(l) if v + w  1.

Proof.

Since V(0, q) =

1 2

q,

we

have

p - p = v2l-v V(0, q) + a2l-a

=

1 2

v2l-v

(w2

l-w

p

+

b2l-b

)

+

a2l-a

=

1 2

v2w2 l-v-w

p

+

1 2

v2

b2

l-v

-b

+ a2l-a .

We apply Lemma B.8 with 

=

v + w, 

=

1 2

v2w2

,

and



=

min(v + b, a)

=

v +

min(b, a - v) to upper and lower bound our dynamics here, with appropriately chosen C. With a

little bit of case work, we obtain the desired results.

Theorem A.7. Let  = ReLU. Suppose e(0) < 1.

· If v + w  1 or   1, then liml e(l) = 1. · If v + w > 1 and  > 1, then e(l) converges to a fixed point e < 1, dependent on the
initial data p(0) and (0), at a rate of (l-+1) where  = min(v + b, a).

Proof. By Cho and Saul (2009); Yang and Schoenholz (2017), we have W(0, q, q, cq)

V(0, q)J1(c)

=

1 2

q

·

1 

(

1 - c2 + ( - arccos(c))c). As in the proof of Thm A.5, we have

v2l-v q/2J1(c) + a2l-a v2l-v q/2 + a2l-a

-

e

=

(e

-

e)

p

p -

p

=

But v2 l-v q/2J1(c)+a2 l-a
v2 l-v q/2+a2 l-a

 J1(c)  c  e, where the last inequality holds for all large l, by

Lemma B.1. So the LHS is nonnegative for large l, which implies e(l) is a bounded monotonically

increasing sequence for large enough ls and thus has a limit e.

Writing e = e + , we have

e +

+(

-

p )p - p

=

v2l-v q/2J1(c) + a2l-a v2l-v q/2 + a2l-a

()

Suppose v + w  1 or   1.

LHS. By Thm A.6, p = (1) (assuming v, w > 0), so that p/(p - p) = O~(l). As in the proof of Thm A.5, - cannot be ~ (l-1), or else  . Thus ( - )p/(p - p) = o(1), and the LHS in the limit l   becomes e.

RHS. In all cases, we will find e = 1.

If q = o(lv-a ), in the RHS a2l-a dominates, and the RHS converges to 1.

If

q

=

(lv-a ),

then

in

the

RHS

1 2

v2

l-v

q

dominates.

By

Lemma

B.1:

· If p = (lw-b ), then c  e  e, so that in the limit l  , e = J1(e). This equation has solution e = 1.

17

Under review as a conference paper at ICLR 2018

· If p = o(lw-b ), then c  1, so that e = J1(1) = 1.

·

If p  Clw-b , then c 

w2 Ce+b2 w2 C+b2

,

and

we

have

an

equation

e

=

J1

(

w2 Ce+b2 w2 C+b2

).

Note

that since e

 [0, 1],

w2 Ce+b2 w2 C+b2

=

w2 C w2 C+b2

·

e

+

b2 w2 C+b2

· 1  e, with equality iff e

= 1.

Since J1 e = 1.

is

monotonic,

e

=

J (1

w2 Ce+b2 w2 C+b2

)

iff

the

equality

condition

above

is

satisifed,

i.e.

If q  Dlw-b for some D, then the RHS is

1 2

v2DJ1(c)

+

a2

1 2

v2D

+

a2

+

o(1)

by the same logic as in the proof of Lemma B.1. As in the above case, Lemma B.1 yields the following results:

·

If p the

= (lw-b ), then c RHS of this equation

 is

e a

 e, so that in the limit l convex combination of J1

 (e

)an, de1=, an12dv2J12D1(v2Je1D(e+))+a2 ea2

. Since by the

monotonicity of J1, the equality can hold only if J1(e) = e. The only such e is 1.

·

If p = o(lw-b ), then c  1, so that e

=

1 2

v2

DJ1

(1)+a2

1 2

v2

D+a2

= 1.

· If p



Clw-b , then c



,w2 Ce+b2
w2 C+b2

and

we

have

an

equation

e

=

1 2

1 v2 D+a2

(

1 2

v2DJ1(

w2 Ce+b2 w2 C+b2

)

+

a2).

By

the

monotonicity

of

J1,

the

RHS

is

at

least

e12v2D11+,ta2h(e21eqv2uDali(tyw2cw2CaCen++hob2lb2d)o+nlya2i)f,ew h=ic1h. is a convex combination of e and 1. Since

Suppose v + w > 1 and  > 1.
By Thm A.6, p = (1) and therefore  = (1). Both converge to fixed points p and  (possibly dependent on initial data p(0) and (0)) because they are monotonically increasing sequences. Thus e = /p.
Unwinding the proof of Thm A.6, we see that p - p = l-, so that p/(p - p) = (l). Since the RHS of Eq. ( ) cannot blow up, it must be the case that ( - ) = O(l-) = = O(l-+1). If ( - )p/(p - p) = F + o(1) for some constant F, then the LHS becomes e + F in the limit. Yet, unless (0) = p(0), e < 1. Therefore, F > 0 (or else, like in the case of v + w  1 or   1, e = 1) whenever (0) < p(0), and = (l-+1).

Theorem A.8.

Suppose 

=

ReLU.

Then for 

=

min(v + b, a) and 

=

1 2

v2w2

as in Thm A.6:

· If v + w > 1, then (0)/ (l) = (1).

 If   [0, 1)

/(w1) (wl) = (l-1+v ) /(v1) (vl) = (l- max(1--w,-b))
 If  = 1 If w  b
/w(1) (wl) = (lv / log l) /v(1) v(l) = (lw / log l)

/b(1) b(l) = (lv ) /a(1) a(l) = (1).
/b(1) b(l) = (lv ) /(a1) a(l) = (1)

18

Under review as a conference paper at ICLR 2018

Otherwise  If  > 1

/(w1) w(l) = (lv / log l) /v(1) v(l) = (lb )

/b(1) b(l) = (lv ) /(a1) a(l) = (1)

/(w1) w(l) = (lv ) /v(1) v(l) = (lmin(w,b))

/(b1) b(l) = (lv ) /(a1) a(l) = (1)

· If v + w = 1, then (0)/ (l) = (l).

 If  = 1 - 

/w(1) w(l) = (l+v-max(,1-)) /(v1) (vl) = (l-max(max(,1-)-w,-b))
 Otherwise If b  w - 
/(w1) (wl) = (lv / log l) /(v1) v(l) = (lw / log l)
Otherwise

/b(1) b(l) = (l+v ) /a(1) (al) = (l )
/(b1) b(l) = (l+v ) /a(1) a(l) = (l )

/w(1) w(l) = (lv / log l) /(v1) v(l) = (l+b )

/(b1) (bl) = (l+v ) /a(1) (al) = (l )

· If v + w < 1, then

(0)/

(l)

=

exp(

 1-v -w

l1-v

-w

+

~ (lmax(1-2v-2w,0))).

/(w1) w(l) = (1)

/b(1) (bl)

=

exp( 1

-

 v

-

w

l1-v -w

+

~ (lmax(1-2v-2w,0)))

/v(1) (vl) = (lw )

/a(1) a(l)

=

exp( 1

-

 v

-

w

l1-v -w

+

~ (lmax(1-2v-2w,0))).

Proof.

Using Thm A.2 and the fact that V(0, q) =

1 2

q,

V (0,

q)

=

1 2

,

we

get

=

(

1 2

v2

w2

l-v

-w

+

1)

,

w

=

1 2

v2

l-v

p,

so

b

=

1 2

v2

l-v

1 v = 2 q,

,

(b1) (bl)

=

1 2

v2

(1)

1 2

v2

l-v

(l)

= lv

(0) (l)

(w1) (wl)

=

1 2

v2

(1)p(0)

1 2

v2

l-v

(l)p(l-1)

=

lv

p(0) p(l-1)

(v1) v(l)

=

1 2 1 2

(1)q(1) (l)q(l-1)

=

q(0) q(l-1)

(0) (l)

(a1) a(l)

=

(1)
(l) .

(0) (l)

a =

,

(0)
Suppose v + w > 1. Then (l) = (1) by Lemma B.7. By Thm A.6:

19

Under review as a conference paper at ICLR 2018

(0)
· If v + w > 1, then (l) = (1) by Lemma B.7.

 If   [0, 1) p = (l1-) and q = (lmax(1--w,-b)). So /(w1) w(l) = (l-1+v )
/v(1) (vl) = (l- max(1--w,-b))  If  = 1, then p = (log l) and
If w  b q = (l-w log l) so that
/w(1) w(l) = (lv / log l)
/v(1) v(l) = (lw / log l) Otherwise
q = (l-b ) so that
/w(1) w(l) = (lv / log l)
/v(1) v(l) = (lb )  If  > 1
p = (1) and q = (l- min(w,b)). So
/w(1) (wl) = (lv )
/(v1) v(l) = (lmin(w,b))

/b(1) b(l) = (lv ) /a(1) a(l) = (1).
/b(1) (bl) = (lv ) /(a1) (al) = (1)
/b(1) b(l) = (lv ) /(a1) a(l) = (1)
/b(1) b(l) = (lv ) /a(1) a(l) = (1)

· If v + w = 1, then by Lemma B.7

(0)/

(l)

=

(l

1 2

v2

w2

)

=

(l ).

 If  = 1 -  p = (lmax(,1-)), and q = (lmax(max(,1-)-w,-b)). So

/w(1) (wl) = (l+v-max(,1-)) /v(1) v(l) = (l-max(max(,1-)-w,-b))  Otherwise, p = (l log l), and

/(b1) b(l) = (l+v ) /a(1) a(l) = (l )

If b  w -  q = (l-w log l), so

/w(1) w(l) = (l+v-/ log l) = (lv/ log l)
/(v1) v(l) = (l+w-/ log l) = (lv/ log l)
Otherwise q = (l-b ), so

/b(1) b(l) = (l+v ) /(a1) a(l) = (l )

/w(1) w(l) = (l+v-/ log l) = (lv / log l) /v(1) (vl) = (l+b )

/(b1) b(l) = (l+v ) /a(1) a(l) = (l )

· If v + w < 1, then by Lemma B.7,

(0)/

(l)

=

exp(

 1-v -w

l1-v

-w

+

R(l; , v

+

w)), where R = ~ (lmax(1-2v-2w,0)) is as in Lemma B.7.



We

have

p

=

exp(

 1-v -w

l1-v

-w

+ R(l; , v + w) + O(1)),

where

R

is

the

same

as

above,

and

q

=

exp(

 1-

l1-v

-w

+ R(l; , v + w) - w log l + O(1)).

Thus

/w(1) (wl) = exp(O(1)) = (1)

/b(1) b(l)

=

exp( 1

-

 v

-

w

l1-v -w

+

R

+

v

log

l)

/(v1) v(l) = exp(w log l + O(1)) = (lw )

/a(1) a(l)

=

exp( 1

-

 v

-

w

l1-v -w

+

R).

20

Under review as a conference paper at ICLR 2018

B LEMMAS
In this section, we present many lemmas used in the proofs of the main theorems. In all cases, the lemmas here have already appeared in some form in Yang and Schoenholz (2017), and for completeness, we either include them and their proofs here or improve upon and extend them, with the blessing of the authors.
Lemma B.1. Asymptotically,

c

=

w2 l-w  + b2l-b w2 l-w p + b2l-b

1 

-

w2

b-2lb-w

p(1

-

/p)

+

(l2(b -w ) p2 (1

-

/p))

   

= 1 + O(lb-w p)



/p 

+

b2 w-2 lw -b

p-1(/p

-

1)

+

(l2(w -b ) p-2 (/p

-

1))

=

 
   

= /p + O(lw-b p-1)

w2 C/p+b2 w2 C+b2

+ E Rlb-w (/p -

1)

+

((Rlb-w

)2(/p

-

1))





     

=

w2 C/p+b2 w2 C+b2

+ o(1)

if p = o(lw-b )
if p = (lw-b )
if p = Clw-b + R for some constant C and remainder R = o(lw-b ).

where E

=

.w2 b2
(w2 C+b2)2

Proof. If p = o(lw-b ), then

(b2l-b + w2 l-w p)-1 = b-2lb (1 + w2 b-2lb-w p)-1

= b-2lb (1 - w2 b-2lb-w p + (w2 b-2lb-w p)2 - · · · )

w2 l-w  + b2l-b w2 l-w p + b2l-b

= (w2 l-w  + b2l-b )b-2lb (1 - w2 b-2lb-w p + (w2 b-2lb-w p)2 - · · · )

= (w2 b-2lb-w  + 1)(1 - w2 b-2lb-w p + (w2 b-2lb-w p)2 - · · · )

= 1 - w2 b-2lb-w (p - ) + ((lb-w )2p(p - ))

= 1 + O(lb-w p).

If p = (lw-b ), then

(w2 l-w p + b2l-b )-1 = (w2 l-w p)-1(1 + b2w-2lw-b p-1)-1

= (w2 l-w p)-1(1 - b2w-2lw-b p-1 + (b2w-2lw-b p-1)2 - · · · )

w2 l-w  + b2l-b w2 l-w p + b2l-b

= (/p + b2w-2lw-b p-1)(1 - b2w-2lw-b p-1 + (b2w-2lw-b p-1)2 - · · · )

= /p + b2w-2lw-b p-2( - p) + (l2(w-b)p-3( - p))

= /p + O(lw-b p-1).

21

Under review as a conference paper at ICLR 2018

If p  Clw-b for some C and R = p - Clw-b = o(lw-b ), then

w2 l-w p

+

b2l-b

=

l-w p(w2

+

b2l-b l-w p

)

=

l-w p(w2

+

C

+

b2 Rlb-w

)

=

C

l-w p + Rlb-w

(w2 (C

+ Rlb-w ) + b2)

w2 l-w 

+ b2l-b

=

C

l-w p + Rlb-w

(w2 /p(C

+ Rlb-w ) + b2)

w2 l-w  + b2l-b w2 l-w p + b2l-b

=

w2 /p(C + Rlb-w ) + b2 w2 (C + Rlb-w ) + b2

=

(

w2 /pC + b2 w2 C + b2

+

w2 /pRlb-w w2 C + b2

)(1

-

w2 Rlb-w w2 C + b2

+···)

=

w2 /pC + b2 w2 C + b2

+

Rlb-w

(

w2 /p w2 C + b2

-

w2 /pC + b2 w2 C + b2

w2

w2 C+

b2

)

+

·

·

·

=

w2 /pC + b2 w2 C + b2

+

Rlb-w

w2

w2 C+

b2

(/p

-

w2 /pC + b2 w2 C + b2

)

+

·

·

·

=

w2 /pC + b2 w2 C + b2

+

Rlb-w

w2

w2 C+

b2

b2(/p - 1) w2 C + b2

+ ((Rlb-w )2(/p - 1))

=

w2 /pC + b2 w2 C + b2

+

o(1)

For any function f that is (k + 1)-times differentiable in a neighborhood of 0, we have the asymptotic

expansion

f (z) =

k

dnf dzn

zn (0)
n!

+

O(zk+1), as

z



0.

n=0

Since

dn d(1/q)n

q1/2V(0,

q)

q

=

(-1)n 2n 2


2(z)z2n dz
-

whenever the RHS is integrable, we have

Lemma B.2. Suppose 2(z)z2n is integrable over z  R for all 0  n  N + 1. Then V(0, q) =

q-1/2(

N n=0

Cnq-n

+

O(q-N -1 ))

as

q



,

where

Cn

:=

(-1)n 2nn! 2


2(z)z2n dz.
-

Note that sechd(z) = (e-d|z|) for z   as long as d > 0, so that Cn from the above result converges when  = sechd. Therefore

Lemma B.3. Let d > 0. We have V sechd(0, q) q-1/2 n0 Cnq-n, where

Cn

:=

(-1)n 2nn! 2


sech2d(z)z2n dz.
-

As corollaries, we obtain the following asymptotics.

Lemma B.4.

Vtanh(0, q) =

2 3

2 

q

-1/2

+

(q-3/2)

as

q



.

22

Under review as a conference paper at ICLR 2018

Proof. Use Lemma B.3 along with the fact that tanh(z) = sech2(z) and

sech4

z dz

=

2 3

tanh z

+

1 2

sech2

z

tanh

z.

Lemma B.5. 1 - V tanh(0, q) =

2 

q-1/2

+

(q-3/2)

as

q



.

Proof. Use Lemma B.3 along with the fact that 1 - tanh2(z) = sech2(z) and sech2 z dz = tanh z.

Lemma B.6. Let d  R and 1 < M < N with N - M  Z0. Set (M, N, d) :=

N a=M

ad.

If

we fix M and let N  ,

(1)





log N + O(1)

(M, N, d)

=

 N d+1
d+1

+

O(1)

N - M + 1



 

1 d+1

N

d+1

+

1 2

N

d

+

O(N max(0,d-1))

if d < -1 if d = -1 if -1 < d < 0 if d = 0 if d > 0

Proof. Euler-MacLaurin formula.

Lemma B.7. Suppose (l) satisfies the recurrence

(l) =

(l-1)(1

+

 l

).

for some nonzero constant   R independent of l.

· If  > 1, then (l) = (1).

· If  = 1, then (l) = (l).

· If 0 <  < 1, then

(l)

=

exp(

 1-

l1-

+

~ (lmax(0,1-2))).

exp(

 1-

l1-

+

R),

where

remainder

R

is

(l1-2) if  < 1/2  R = (log l) if  = 1/2

(1)

if  > 1/2

In particular,

(l) =

Proof. We have

log (l) = log (l-1) + log(1 + /l)

= log (l-1) + /l + (2/l2)

for large l. If  > 1, then l l- converges, and log (l) = log (0) - (1)

(l) = (1).

If  = 1, then

log (l) = log (0) +  log l + (1)

(l) = (l).

If  < 1, then for a remainder R that is

log

(l) = log

(0)

+

1

 -

l1- 

+

R

(l1-2) if  < 1/2 
(log l) if  = 1/2

(1)

if  > 1/2

Exponentiating gives the desired result.

23

Under review as a conference paper at ICLR 2018

Lemma B.8. Suppose (l) = Cl- + (l-1)(1 + /l) for   R, C = 0, and  = 0. Then

· If  > 1, then
 (l) = (l1-) if   [0, 1);  (l) = (log l) if  = 1;  (l) = (1) if  > 1.

· If  = 1, then
 (l) = (lmax(,1-)) if 1 -  = .  (l) = (l log l) if 1 -  = .

· If  < 1, then



(l)

=

exp(

 1-

l1-

+

~ (lmax(0,1-2))).

In particular,

(l)

=

exp(

 1-

l1-

+

R(l; , ) + O(1)), where R is the same R as in Lemma B.7, depending on only

l, , and  but not on  or C, with

(l1-2) if  < 1/2  R = (log l) if  = 1/2

(1)

if  > 1/2.

Furthermore, for  = - = 1: (l)  l-1 if  > 2, (l)  l1- if  < 2, and (l)  l log l if  = 2.

Proof. We can unwind the recurrence to get

l

(l) = C

m-

l

 (1 + n ) +

(0)

l (1 + n )

m=1

n=m+1

n=1

Suppose  > 1. By Lemma B.7, we get
l
(l) = (1) m- + (0)(1)
m=1
(l1-) if   [0, 1)  = (log l) if  = 1 (1) if  > 1.

Now suppose  = 1. By Lemma B.7, we get

l

(l) =

m-(m-l) + (0)(l)

m=1

where the constants hidden inside the  are the same in every term of the sum. If  > 1 - , then

m-- = o(m-1), so that

l m=1

m--

=

(1),

and

(l) = (l) + (0)(l)

= (l).

On the other hand, if  < 1 - , then

l m=1

m--

=

(l1--).

So

(l) = (l1-) + (0)(l) = (l1-).

24

Under review as a conference paper at ICLR 2018

If  = 1 - , then

l m=1

m--

=

(log

l).

So

(l) = (l log l) + (0)(l)

= (l log l).

Finally, if   (0, 1), then by Lemma B.7,

l

(l)

=

Ce  1-

l1-

+R

m e + e ,-

- 1-

m1-

+~ (lmax(0,1-2)

)

 1-

l1-

+R

m=1

where R is the remainder as in Lemma B.7. The sum

m e can bel
m=1

-

- 1-

m1-

+~ (lmax(0,1-2)

)

upper and lower bounded by integrals of the form x-e-µx1- dx for appropriate µs, which are

finite (they are bounded by values of incomplete Gamma functions). Thus

(l)

=

(1)e

 1-

l1-

+R

where

(l1-2) if  < 1/2  R = R + O(1) = (log l) if  = 1/2

(1)

if  > 1/2.

A fortiori,

= e .(l)

 1-

l1-

+~ (lmax(0,1-2)

)

For our "furthermore" claim: the case of  = -1 telescopes, so that the upper and lower constants hidden in  can both be taken to be 1.

25

