Under review as a conference paper at ICLR 2018
VARIATIONAL MESSAGE PASSING WITH STRUCTURED INFERENCE NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Powerful models require powerful algorithms to be effective on real-world problems. We propose such an algorithm for a class of models that combine deep models with probabilistic graphical models. Our algorithm is a natural-gradient message-passing algorithms whose messages automatically reduce to stochasticgradients for the deep components of the model. Using a special-structure inference network, our algorithm exploits the structural properties of the model to gain computational efficiency while retaining the simplicity and generality of deeplearning algorithms. By combining the strength of two different types of inference procedures, our approach offers a framework that simultaneously enables structured, amortized, and natural-gradient inference for complex models.
1 INTRODUCTION
To analyze real-world data, machine learning relies on models that can extract useful patterns. For this purpose, deep models such as Deep Neural Networks (DNNs) are a popular choice and can learn flexible representations. Another popular choice are probabilistic graphical models (PGMs) which can find interpretable structures in the data. Recent work on combining these two types of models hopes to borrow their complimentary strengths and provide powerful models that are also easy to interpret (Johnson et al., 2016; Krishnan et al., 2015; Archer et al., 2015; Fraccaro et al., 2016). To apply such hybrid models to real-world problems, we need efficient algorithms that can extract useful structure from the data. However, the two fields of deep learning and PGMs traditionally use different types of algorithms. For deep learning, stochastic-gradient (SG) methods are the most popular choice, e.g., those based on back-propagation. These algorithms are not only widely applicable, but also employ amortized inference to enable fast inference at test time (Rezende et al., 2014; Kingma & Welling, 2013). On the other hand, most popular algorithms for PGMs exploit the model's graphical structure to gain computational efficiency, e.g., variational message passing (VMP) (Winn & Bishop, 2005), expectation propagation (Minka, 2001), Kalman filtering, and more recently stochastic variational inference (SVI) (Hoffman et al., 2013). The two fields of deep learning and PGMs employ fundamentally different inferential strategies and a natural question is, whether we can design algorithms that combine their respective strengths. In this paper, we propose such an algorithm. Our algorithm is a natural-gradient message-passing algorithm whose messages automatically reduce to stochastic-gradients for the deep components of the model. In addition to such a flexible message-passing scheme, our algorithm performs amortized inference by using a structured inference network that mimics the structure of the model. Overall, this enables Structured, Amortized, and Natural-gradient (SAN) updates and therefore we call our algorithm the SAN algorithm. Unlike many existing methods that continue using SG methods even when some of the model components are PGMs, our SAN algorithm exploits the probabilistic structure to gain computational efficiency. Our algorithm is in a similar spirit to the work of Johnson et al. (2016) but it enjoys much simpler implementation. Experimental comparisons show that our method gives comparable performance to the method of Johnson et al. (2016) while simplifying and generalizing it.
1

Under review as a conference paper at ICLR 2018

Latent mixture model

Latent state-space model

 zn

 xn

NN

yn

N

(a) Model

 zn ¯

PGM

 xn ¯

NN

yn NN N

(b) Inference Network

x1 x2 x3 NN
y1 y2 y3
(c) Model

PGM

PGM

x4 x1 x2 x3 x4

NN

NN

y4 y1 y2 y3 y4

(d) Inference Network

Figure 1: Two examples of model classes that combine probabilistic graphical models with deep
neural networks. The two figures on the left show the model and corresponding inference network
for the latent mixture model, while the two figures on the right show the same for the latent statespace model. For the mixture model,  := {µ, } contains the parameters of Gaussian mixture components, and ¯ contains the corresponding parameters for the inference network. Our inference
network mimics the structure of the model. The only difference is that the arrows from yn to xn are reversed. Another important aspect is that the inference network uses PGM by a different set of parameters PGM. These differences play an important role in the design of our algorithm.

2 THE MODEL AND CHALLENGES WITH ITS INFERENCE

We consider the modeling of data vectors yn by using local latent vectors xn. The correlations among N data vectors y := {y1, y2, . . . , yN } can then be captured by using a probabilistic graphical model (PGM) over all the latent vectors x := {x1, x2, . . . , xN }. This combination of probabilistic graphical model and neural network is referred to as structured auto-encoder (SVAE) by Johnson et al. (2016) and has also been used in other recent works (Krishnan et al., 2015; Archer et al., 2015).
Following previous approaches, we consider the following model in this paper:

N
p(y, x, ) = p(y|x, )p(x, ) = p(NN) p(yn|xn, NN)
n=1

p(x|PGM)p(PGM) ,

(1)

DNN

PGM

where NN are the parameters of DNN, PGM parameters of PGM, and  := {NN, PGM}. Below, we give two examples of graphical models.

Mixture Models: To discover patterns in the data, we can model xn by using a finite mixture model
with K mixture components. To do so, we define additional discrete latent vector zn whose k'th element is znk  {0, 1}. We can use then as a Gaussian mixture model as the PGM prior,

NK

p(y, x, z|) =

p(yn|xn, NN) kN (xn|µk, k)znk ,

n=1

k=1

(2)

where µk and k are mean and covariance of the k'th mixture and k are the mixture proportions summing to 1, and PGM := {k, k}Kk=1 and k := {µk, k}. The parameter prior can be chosen as described in Bishop (2006) and the corresponding graphical model is shown in Figure 1a. This is
an example of a model that combines PGM with deep networks.

State-Space Models: When yn is a time series, we can model the latent xn using a linear dynamical system, for example, we can use the following model:

N

p(y, x|) =

p(yn|xn, NN)N (xn|Axn-1, Q) N (x0|µ0, 0),

n=1

(3)

where A is the transition matrix, Q is the process noise, and µ0 and 0 are the mean and covariance
of the initial distribution. Here, PGM := {A, Q, µ0, 0}. Again, we can use standard conjugatepriors for PGM (Beal, 2003). The graphical model is shown in Figure 1c.

2

Under review as a conference paper at ICLR 2018

2.1 CHALLENGES IN INFERENCE
For these models, our goal is to approximate the posterior distribution p(x, |y) using an inference network that can be used to perform amortized inference (Ritchie et al., 2016). Such inference networks have previously been used in variational auto-encoders (VAE) (Kingma & Welling, 2013) where they are shown to perform extremely well, however, in our case, designing such highperformance networks is a challenging task.
The main challenge lies in designing networks that can flexibly capture the structure present in the PGM component, i.e., p(x|). In VAEs, a DNN with parameters  is used as the inference network q(x|y, ). This choice is reasonable since all of the structure in the posterior is attributed to the DNN likelihood p(y|x, ). The PGM part in this case is simply a standard normal distribution N (x|0, I) which does not have any special structure. Therefore using a pure DNN based inference-network is sufficient.
Our model is explicitly designed to exploit the PGM structure. Therefore, it is essential to capture such structure in the inference network. However, a purely DNN based inference network puts all its power in the likelihood term and ignores the structure. This might give inaccurate predictions. For example, consider the case where x is a time series (as shown in Fig. 1c). Here, the prediction of a future observation, say, y10 at time n = 10 given past data, say, {y1, y2, y3} would be inaccurate because the inference network does not capture the correlation between x10 and x3. Rather, the network relies on a DNN that is conditioned on the unavailable observation y10. For such forecasting and imputation tasks, the inference network must capture the structure of the posterior accurately.
A solution to this problem is to exploit the PGM structure in the inference network by replacing all of its edges by neural networks (Krishnan et al., 2015; Fraccaro et al., 2016). When the PGM itself is complex, this solution is reasonable, but when it is a simple model, such as linear dynamical system, this approach might be too aggressive. Using DNNs in such cases would dramatically increase the number of parameters. This, in turn, will lead to a loss in speed and a possible deterioration in performance.
Johnson et al. (2016) propose an alternative double-loop procedure to make sure that the probabilistic structure in PGM can be captured in the posterior distribution. Their algorithm however requires running a double loop algorithm where the inner loop needs to be run until convergence otherwise the algorihtm might diverge. We give more details in the next section, but their approach is computationally intensive and might converge slowly.
To summarise, existing methods either simply ignore the graphical model structures or do not exploit them efficiently.
To solve this problem, we propose an inference network that preserves the structure of the posterior to facilitate fast and accurate amortized inference. We incorporate this network within a messagepassing framework. For the deep components of the model, these messages automatically reduce to stochastic-gradients, while for the rest of the model, they are used to perform natural-gradient updates, just like in SVI (Hoffman et al., 2013). We give two examples of our approach and apply them to the experiments used in Johnson et al. (2016). Our results show that our method performs comparably, while simplifying and generalizing their approach.

3 CONJUGATE-STRUCTURE INFERENCE NETWORKS

As explained in the previous section, designing an inference network that preserves the PGM's probabilistic structure and enables flexible amortized inference is a challenging task. We solve this problem by proposing a structured inference network that takes the following form:

q(x|y, )

:=

1 Z ()

q(x|y, NN) p(x|PGM)

DNN

PGM

(4)

where p(x|PGM) is the same distribution as p(x|PGM) but is parametrized by a new parameter PGM, and q(x|y, NN) is an exponential-family distribution whose natural parameters are parametrized by a DNN in a similar fashion as in VAE. Z() is the normalizing constant to make sure that the network integrates to 1.

3

Under review as a conference paper at ICLR 2018

The role of the PGM term is to enforce the PGM structure, while the role of DNN term is to enable flexibility. Below, we give an example for the latent GMM model shown in Figure 1a. The first component here is an inference network as in VAE, while the second and third components correspond to a GMM:

q(x|y, ) = 1 N Z ()

K
N (xn|myn , Vyn ) ¯kN (xn|µ¯ k, ¯ k)znk ,

n=1

k=1

(5)

where myn := m(yn, NN) and Vyn := V(yn, NN) are mean and covariance parameterized by a DNN with parameter NN, PGM := {¯ k, ¯k}kK=1 and ¯ k = {µ¯ k, ¯ k} are the parameters on the PGM part in the inference network. The graphical model of this inference network is shown in
Figure 1b, where we clearly see that the structured inference network mimics the structure of the
model.

Here is another example for latent LDS model shown in Figure 1c.

q(x|y, )

=

1 Z ()

N
N (xn|myn , Vyn ) N (xn|A¯ xn-1, Q¯ ) N (x0|µ¯ 0, ¯ 0),

n=1

(6)

where myn and Vyn are defined similarly to the mixture-model case, and the PGM parameters are PGM := {A¯ , Q¯ , µ¯ 0, ¯ 0}. The graphical model of this inference network is shown in Figure 1d.

We now discuss some important features of our structure inference network.

Sampling from the inference network: For amortized inference, we need to be able to generate samples from q(x|y, ) which is easy when the normalizing constant Z() is available in closed form. To satisfy this requirement, we choose the network q(x|y, NN) to be conjugate to p(x|PGM), i.e., they both must take the same functional form with respect to x. For example, for the inference
network shown in (5), the inference network is very similar to a GMM whose normalizing constant
can be obtain using the following expression:

NK

Z() :=

¯k N (myn |µ¯ k, Vyn + ¯ k)

n=1 k=1

(7)

Similarly, the inference network of latent LDS model is an LDS where the marginal likelihood (which is equal to Z()) can be computed in O(N ) using the forward-backward algorithm (a.k.a.
Kalman filter) Bishop (2006).

In general, we choose the first term in the inference network to be a member of a conjugate exponential family. Sampling can then be performed using a conjugate message-passing algorithm. We call our inference network a conjugate-structure inference networks.

Independence from : An important feature of our inference network is that it is independent
of . Due to this, our method does not suffer from the "implicit gradient" problems encountered in the method of Johnson et al. (2016). Their method approximates q(x)  q(x|y, )p(x, ) by
optimizing a variational lower bound. As a result, the parameters of q(x) depend on . Due to this
dependence, if the inner loop of their algorithm is not run until convergence, the outer loop might not
have correct gradients which might result in the divergence of their method. By simply using new parameters PGM, we make sure that the normalizing constant Z as well as the generated samples are all independent of PGM. This simplifies the inference because we do not need to worry about the inference network when taking gradients with respect to the variational parameters corresponding
to the variational distribution of .

The additional parameters PGM do not lead to a massive increase in parameters since the number of global parameters PGM is usually quite small. For example, for the GMM case, the increase in number of parameter is only O(K) which is usually much smaller than N .

Non-conjugate and nonlinear models: Another consequence of our proposed inference network is that when the original PGM is non-conjugate (e.g., a T-distribution), we can still use a conjugate term in the second term (e.g., a Gaussian) in (4). We only have to make sure that the inference network captures useful structure in the posterior. This modification enables us to design easy-tosample inference networks for complex models. The conjugate structure in our inference network

4

Under review as a conference paper at ICLR 2018

simplifies the inference by replacing difficult terms in the model by conjugate terms in the inference network.
The final variational approximation: Using the above inference network, we can now specify the overall posterior approximation by assuming a mean-field approximation among x and :

p(x, |y)  q(x|y, )q(|),

(8)

where q(|) is a fixed-form minimal exponential-family distribution parametrized by .

4 VARIATIONAL MESSAGE PASSING

In the previous section, we proposed a conjugate-structure inference network. Now, we describe a variational message-passing algorithm to perform efficient inference with it. An attractive property of our algorithm is that it enables natural-gradient updates for the PGM part, while its messages reduce to stochastic gradients when applied to deep neural networks. Our algorithm is an extension of the method proposed by Khan & Lin (2017) and we extend it to allow the use of inference network.

Our goal is to maximize the following lower bound:

log p(y)  Eq

p(y, x, ) log q(x|y, )q(|)

:= L(, )

(9)

A straightforward solution is to use stochastic-gradient methods, e.g., RMSprop or Adam. These methods are suitable for estimating deterministic parameters , however they may be inefficient to estimate . This is because  is the parameter of a distribution and SG methods are not suited for this purpose (see a discussion in Hoffman et al. (2013)). A preferred way to optimize with respect to  is to use natural-gradient methods such as stochastic variational inference (Hoffman et al., 2013). Natural gradients exploit the geometry of the probabilistic model and show faster convergence. For mean-field variational inference, they are also easier to compute and natural-gradient updates can be performed locally at a node within the message-passing framework. SG methods cannot exploit nice properties of natural-gradients and might be slow to converge.

An alternative is to use natural-gradients for both  and , but this is computationally intensive because a direct computation of the Fisher matrix for deterministic parameters is difficult. We could also simply apply natural-gradient updates to  while use SG methods for , but this approach is not principled and the step-size selection might be difficult to tune due to the interaction between these different types of updates.

We propose a principled approach to solve this problem by using the method of Khan & Lin
(2017) which automatically recover SG updates for  as a special case, while enabling natural-
gradient updates for . The only issue is that their method does not directly apply to determin-
istic parameters such as . To solve this problem, we associate  with a Gaussian distribution q() := N (|µ, diag(2)) where µ and  are vectors of mean and standard deviation. Given this distribution, we maximize the following lower bound:

L¯(µ, , ) := Eq()[L(, )].

(10)

This approach is known as variational optimization (Staines & Barber, 2012) and it works due to the following inequality: max L(, )  Eq()[L(, )], i.e., the maximum of a function is lower bounded by its expectation. Therefore, maximizing the expectation also maximizes the function. The variance eventually shrinks to zero when the algorithm converges, therefore the resulting algorithm converges to a local minima of L even though we optimize L¯. The good news is that now all of the optimization parameters, namely , µ, and  are parameters of distributions. Therefore, we can use natural gradient updates.
We now give detailed updates for the parameter q(PGM|PGM), q(NN|NN), and q(|µ, 2 ).
Update for q(PGM|PGM): We assume that p(PGM) and p(x|PGM) are conjugate to each other, although the algorithm can easily be modified when the relationship is non-conjugate. Conjugacy implies that the two distribution can be rewritten as follows (e.g., see Section 2 in Khan & Lin

5

Under review as a conference paper at ICLR 2018

(2017)):

p(PGM) = h(PGM) exp [ T(PGM),  - A()] ,

(11)

p(x|PGM) = exp [ T(PGM), x(x) - fx(x)] ,

(12)

where  is the natural parameter, T is the sufficient statistics, ·, · is an inner product, h is the base
measure, A() is the log-partition function, and x and fx are functions that depend on x only. The factors that involve PGM are p(x|PGM) and p(PGM), therefore using Algorithm 2 of Khan & Lin (2017), we can write the natural-gradient updates as follows:

PGM  (1 - )PGM +  Eq(x|y,)q() [ + x(x)]

(13)

where  > 0 is a step-size. This update is simple and does not require any gradient computations.
To compute the expectation we simply generate samples x from the inference network. Given this sample, the computation of x is equivalent to a variational inference in the model p(x, PGM) which can be performed using standard methods such as SVI and VMP. Therefore we can reuse
existing software to perform this update.

Update for q(NN|NN): We assume that the prior of all elements of NN is a Gaussian with mean µ0 and variance 02. We will use a Gaussian approximation which makes sense since the prior is Gaussian as well. Since the number of parameters for DNNs is usually large, we will use a meanfield approximation. For simplicity, we show updates only for the i'th element of NN. We denote the mean and variance of this element by µi and i2 respectively.
The factors that involve NN in the joint distribution (1) are p(NN) and p(y|x, NN). To derive the updates using Algorithm 2 of Khan & Lin (2017), we first note that the natural parameter of a Gaussian is equal to µi/i2. Using this and a few straightforward manipulations to express the gradients in terms of µi and i2, we get the following updates:

i-2  (1 - )i-2 -  1/02 + i2 Eq()q()q(x|y,) (log p(y|x, NN))

(14)

µi  µi - i2 µ0/02 + µi Eq()q()q(x|y,) (log p(y|x, NN))

(15)

These gradients can be approximated by using samples from the inference network followed by
the reparameterization trick. These updates are very similar to the updates of VAE using adaptive-
gradient methods. The update for mean is scaled by the variance which acts as the scaling vector in the adaptive methods. There are three small differences. First, a regularization term µ0/02 and 1/02 is added. Second, gradient with respect to the variance is computed. Third gradient are computed
at a randomly sampled location from q. In our experiments, this does not make a big difference and
our method converges at a very similar rate as the adaptive gradient methods.

Update for q(|µ, 2): To derive these updates, we note that the neighbors of  are the factors that use x, i.e., p(y|x, NN), p(x|PGM) and q(x|y, ) itself. It is easier to derive the update by simply taking the derivative of the lower bound as a function of  shown below:

L¯(µ, 2 ) = Eq()q()q(x|y,)

log

p(y|x, NN)p(x|PGM) q(x|y, )

(16)

Since the distribution is Gaussian we can use updates very similar to (14)-(15). To compute the
gradient we can use reparameterization trick since a sample from q(x|y, ) is a function of µ and 2 . The updates again resemble adaptive-gradient methods and can be easily obtained.

Summary of the method: The method of Khan & Lin (2017) is guaranteed to converge under stochastic SVI like updates where a local variable update is followed by the update of global variable. Since we are using an inference network for the local variables, this is equivalent to generating samples from q(x|y, ). Using the generated samples we can either perform the above three updates in parallel of use a sequential mode. It is also allowed to randomly pick a global variable and update them.

5 EXPERIMENTS AND RESULTS
The main goal of our experiment is to show that our SAN algorithm gives similar results to the method of Johnson et al. (2016). The advantage of our algorithm over the method of Johnson et al.

6

Under review as a conference paper at ICLR 2018

102 102

MSE MSE

101
100 102

103
Iterations

101
100 104

GMM VAE SVAE (Johnson et. al.) SAN
103 104
Iterations

105

Figure 2: Reconstruction mean-squared error (MSE) computed on the Pinwheel dataset (left) and the Auto dataset (right). The performance has been measured every 500 iterations but the markers are shown only at iteration 500, 2000, 7000 and 75000. In the left figure, we compare to SVAE and GMM, where we see that SAN converges faster than SVAE, although both methods achieve similar error but perform better than GMM. In the right figure, we compare to VAE, where we see very similar trends. The performance of the GMM is represented as a constant because training converges after a few iterations already.

(2016) is that our algorithm is simpler and more general than their method, and does not involve a double-loop procedure. For this reason, we apply our algorithm to the two examples considered in Johnson et al. (2016), namely the latent Gaussian mixture model (GMM) and latent linear dynamical system (LDS). In this section, we show results on GMM (see details of the model in in section 3). An additional result for LDS is included in Appendix A. Our results show that the SAN algorithm can learn complex representations with interpretable structure, similar to the method of Johnson et al. (2016).
For the GMM case, we compare SAN to three algorithms: first, the standard variational EM algorithm applied to GMM (referred to as `GMM'), second, the VAE approach of Kingma & Welling (2013), and third, the SVAE approach of Johnson et al. (2016).
We use two datasets. The first dataset is the synthetic two-dimensional Pinwheel dataset (N = 5000 and D = 2), and the second dataset is the Auto dataset (N = 392 and D = 6, available in the UCI repository). The latter contains information about cars. The dataset also contains a five-class label which indicates the number of cylinders in a car. We use these labels to validate our results. For both datasets we use 70% data for training.
For all methods, we tune their stepsizes, the number of mixture components, and the latent dimensionality on a validation set. We train the GMM baseline on the full dataset, and, for VAE and SVAE, we use stochastic minibatches of size 64. Our neural-network likelihood consists of two layers with 50 hidden units and an output layer of dimensionality 6 for the Auto dataset and 2 for the Pinwheel dataset.
In Figure 2, we compare the performance during training. In the left figure, we compare to SVAE and GMM, where we see that SAN converges faster than SVAE. Both methods achieve similar performance upon convergence and perform better than GMM. In the right figure, we compare to VAE and observe similar trends. In this figure, the performance of GMM is represented as a constant because training converges after a few iterations already. We found that the implementation provided by Johnson et al. (2016) does not perform well on the Auto dataset which is why we have not included it in the comparison. We also compared test log-likelihood and imputation error which show very similar trends.
In Figure 3, we show samples generated from the learned distribution p(y) for all methods. On the top of this plot, we show the data with true labels. These labels were not used during training. The plots (a)-(c) show results on Pinwheel dataset, while plots (d)-(e) shows results on the Auto dataset. For the Auto dataset, each label corresponds to the number of cylinders present in a car. We observe
7

Under review as a conference paper at ICLR 2018

(a) GMM

(b) SAN

(c) VAE

(d) GMM

(e) SAN

(f) VAE

Figure 3: These plots show the learned representations p(y) in the background of the Pinwheel dataset (3a, 3b, 3c) and the Auto dataset (3d, 3e, 3f). In the foreground, we show 300 data samples which are colored according to the true labels (not used during the training). Different colors signify a mixture component and color intensities indicate the density. We use K = 10 mixture components to train all models. For the Auto dataset, we show only the first two principle components.

that SAN can learn meaningful partition of the data space, while VAE does not posses this capability. We also observe that SAN and VAE can learn flexible patterns while GMM does not do so. These results show that with SAN we are able to combine complementary characteristics of VAE and GMM. For both datasets, our method learns flexible representations containing meaningful structural information.
6 DISCUSSION AND CONCLUSION
We propose a message-passing algorithm for the models that use both deep networks and probabilistic graphical models. We discussed the challenges involved when designing an algorithm to exploit the structural properties. Our algorithm can automatically adjust its messages and adapt it to the type of the node. For the DNN part, the messages reduce to computation of stochastic gradients, while on the PGM part, the messages are equal to natural gradients. Experimental comparisons confirm that our method gives similar results to existing methods, while using simpler updates. Our experimental results in this paper are limited to small scale data. We found that it is nontrivial to implement a message-passing framework that goes well with the deep learning framework. Our current implementation is based on AutoGrad and TensorFlow, both of which are slow due to additional communication overhead due to messages passing. We are going to pursue this direction in the future and investigate good platforms to integrate the capabilities of these two different flavors of algorithms.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.
Matthew James Beal. Variational algorithms for approximate Bayesian inference. University of London London, 2003.
Christopher M Bishop. Pattern recognition. Machine Learning, 128:1­58, 2006. Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in Neural Information Processing Systems, pp. 2199­2207, 2016. Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303­1347, 2013. Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. In Advances in Neural Information Processing Systems, pp. 2946­2954, 2016. Mohammad Emtiyaz Khan and Wu Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. arXiv preprint arXiv:1703.04265, 2017. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. Rahul G Krishnan, Uri Shalit, and David Sontag. Deep Kalman filters. arXiv preprint arXiv:1511.05121, 2015. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Intelligent Signal Processing, pp. 306­351. IEEE Press, 2001. T. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2001. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. Daniel Ritchie, Anna Thomas, Pat Hanrahan, and Noah D. Goodman. Neurally-guided procedural models: Learning to guide procedural models with deep neural networks. CoRR, abs/1603.06143, 2016. URL http://arxiv.org/abs/1603.06143. Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012. John Winn and Christopher M Bishop. Variational message passing. Journal of Machine Learning Research, 6(Apr):661­694, 2005.
9

Under review as a conference paper at ICLR 2018

A RESULTS FOR LATENT LINEAR DYNAMICAL SYSTEM

In this experiment, we apply our SAN algorithm to the latent LDS discussed in Section 3. For comparison, we compare our method, Structured Variational Auto-Encoder (SVAE) (Johnson et al., 2016), and LDS on the Dot dataset used in Johnson et al. (2016). Our results show that our method achieves comparable performance to SVAE. For LDS, we perform batch learning for all model parameters using the EM algorithm. For SVAE and SAN, we perform mini-batch updates for all model parameters. We use the same neutral network architecture as in Johnson et al. (2016), which contains two hidden layers with tanh activation function. We repeat our experiments 10 times and measure model performance in terms of the following mean absolute error for  -steps ahead prediction. The error measures the absolute difference between the ground truth and the generative outputs by averaging across generated results.

N T -

1

N (T -  )d
n=1 t=1

||yt+,n - Ep(yt+,n|y1:t,n) yt+,n ||1

(17)

where N is the number of testing time series with T time steps, d is the dimensionality of observation y, and observation yt+,n denotes the ground-truth at time step t +  .

Mean prediction error

1.0 0.9
0.8 LDS 0.7 SVAE SAN0.6
0.5 0.4 0.3
2.5 5.0 7.5 10.0 12.5 15.0 17.5
 -steps ahead
Figure 4: Prediction error, where shadows denote the standard errors across 10 runs
From Figure 4, we can observe that our method performs as good as SVAE and outperforms LDS. Our method is slightly robust than SVAE. In Figure 5, there are generated images obtained from all methods. From Figure 5, we also see that our method performs as good as SAVE and is able to recover the ground-truth observation.

10

Under review as a conference paper at ICLR 2018

(a) SAN

(b) SVAE

(c) LDS

(d) Ground Truth

Figure 5: Generated images, where each column of pixels represents an observation, each row of pixels represents one time step, and each vertical white line denotes the first time step to generate images

11

