Under review as a conference paper at ICLR 2018
GLOBAL OPTIMALITY CONDITIONS FOR DEEP NEURAL
NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete. For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum. Our conditions provide an efficiently checkable test for global optimality, which is remarkable because such tests are typically intractable in nonconvex optimization. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.
1 INTRODUCTION
Since the advent of AlexNet (Krizhevsky et al., 2012), deep neural networks have surged in popularity, and have redefined the state-of-the-art across many application areas of machine learning and artificial intelligence, such as computer vision, speech recognition, and natural language processing.
Despite these successes, a concrete theoretical understanding of why deep neural networks work well in practice remains elusive. From the perspective of optimization, a significant barrier is imposed by the nonconvexity of training neural networks. Moreover, it was proved by Blum & Rivest (1988) that training even a 3-node neural network to global optimality is NP-Hard in the worst case, so there is little hope that neural networks have properties that make global optimization tractable.
But despite the difficulties of optimizing weights in neural networks, the empirical successes suggest that the local minima of their loss surfaces could be close to global minima; and several papers have recently appeared in the literature attempting to provide a theoretical justification for the success of these models. For example, by relating neural networks to spherical spin-glass models from statistical physics, Choromanska et al. (2015) provided some empirical evidence that depth of neural networks makes the performance of local minima close to that of global minima.
Another line of results (Yu & Chen, 1995; Soudry & Carmon, 2016; Xie et al., 2016; Nguyen & Hein, 2017) provides conditions under which a critical point of the empirical risk is a global minimum. Such results roughly involve proving that if full rank conditions of certain matrices (as well as some additional technical conditions) are satisfied, derivative of the risk being zero implies loss being zero. However, these results are obtained under restrictive assumptions; for example, Nguyen & Hein (2017) require the width of one of the hidden layers to be as large as the number of training examples. Soudry & Carmon (2016) and Xie et al. (2016) require the product of widths of two adjacent layers to be at least as large as the number of training examples, meaning that the number of parameters in the model must grow rapidly as we have more training data available. Another recent paper (Haeffele & Vidal, 2017) provides a sufficient condition for global optimality when the neural network is composed of subnetworks with identical architectures connected in parallel and a regularizer is designed to control the number of parallel architectures.
Towards obtaining a more precise characterization of the loss-surfaces, a valuable conceptual simplification of deep nonlinear networks is deep linear neural networks, in which all activation functions are linear and the output of the entire network is a chained product of weight matrices with the input vector. Although at first sight a deep linear model may appear overly simplistic, even its optimization is nonconvex, and only recently theoretical results on this problem have started emerging.
1

Under review as a conference paper at ICLR 2018

Interestingly, already in 1989, Baldi & Hornik (1989) showed that some shallow linear neural networks have no local minima. More recently, Kawaguchi (2016) extended this result to deep linear networks and proved that any local minimum is also global while any other critical point is a saddle point. Subsequently, Lu & Kawaguchi (2017) provided a simpler proof that any local minimum is also global, with fewer assumptions than (Kawaguchi, 2016). Motivated by the success of deep residual networks (He et al., 2016a;b), Hardt & Ma (2017) investigated loss surfaces of deep linear residual networks and showed every critical point is a global minimum in a near-identity region; subsequently, Bartlett et al. (2017) extended this result to a nonlinear function space setting.
1.1 OUR CONTRIBUTIONS
Inspired by this recent line of work, we study deep linear and nonlinear networks, in settings either similar to or more general than existing work. We summarize our main contributions below.
· We provide both necessary and sufficient conditions for a critical point of the empirical risk to be a global minimum (in comparison, Kawaguchi (2016) only proves that every critical point of the risk is either a global minimum or a saddle). Specifically, Theorem 2.1 shows that if the hidden layers are wide enough, then a critical point of the risk function is a global minimum if and only if the product of all parameter matrices is full-rank. This concise condition provides an efficient test on whether a given critical point is a global minimum or a saddle; it is worth noting such tests are intractable for general nonconvex optimization (Murty & Kabadi, 1987). In Theorem 2.2, we consider the case where some hidden layers have smaller width than both the input and output layers, and again provide necessary and sufficient conditions for global optimality.
· Under the same assumption as (Hardt & Ma, 2017) on the data distribution, namely, a linear model with Gaussian noise, we can modify Theorem 2.1 to handle the population risk. As a corollary, we not only recover Theorem 2.2 in (Hardt & Ma, 2017), but also extend it to a strictly larger set, while removing their assumption that the true underlying linear model has a positive determinant.
· Motivated by (Bartlett et al., 2017), we extend our results on deep linear networks to obtain sufficient conditions for global optimality in deep nonlinear networks, although only via a function space view; these are presented in Theorems 4.1 and 4.2.

2 GLOBAL OPTIMALITY CONDITIONS FOR DEEP LINEAR NEURAL NETWORKS

In this section, we describe the problem formulation and notations for deep linear neural networks, state main results (Theorems 2.1 and 2.2), and explain their implication.

2.1 PROBLEM FORMULATION AND NOTATION

Suppose we have m input-output pairs, where the inputs are of dimension dx and outputs of dimension dy. Let X  Rdx×m be the data matrix and Y  Rdy×m be the output matrix. Suppose we have H hidden layers in the network, each having width d1, . . . , dH . For notational simplicity we let d0 = dx and dH+1 = dy. The weights between adjacent layers can be represented as matrices Wk  Rdk×dk-1 , for k = 1, . . . , H + 1, and the output of the network can be written as the product of weight matrices WH+1, . . . , W1 and data matrix X: WH+1WH · · · W1X.
We consider minimizing the summation of squared error loss over all data points (i.e. empirical risk),

minimize

L(W )

:=

1 2

WH+1WH · · · W1X - Y

2 F

,

where W is a shorthand notation for the tuple (W1, . . . , WH+1).

(1)

Assumptions. We assume that dx  m and dy  m, and that XXT and Y XT have full ranks. These assumptions are common when we consider supervised learning problems with deep neural networks (e.g. Kawaguchi (2016)). We also assume that the singular values of Y XT (XXT )-1X are
all distinct, which is made for notational simplicity and can be relaxed without too much difficulty.

Notation. Given a matrix A, let max(A) and min(A) denote the largest and smallest singular values of A, respectively. Let row(A), col(A), null(A), rank(A), and A F be respectively the row

2

Under review as a conference paper at ICLR 2018

space, column space, null space, rank, and Frobenius norm of matrix A. Given a subspace V of Rn, we denote V  as its orthogonal complement. Given a set V, let Vc denote the complement of V.
Let us denote k := mini{0,...,H+1} di, and define p  argmini{0,...,H+1} di. That is, p is any layer with the smallest width, and k = dp is the width of that layer. Here, p might not be unique, but our results hold for any layer p with smallest width. Notice also that the product WH+1 · · · W1 can have rank at most k.
Let Y XT (XXT )-1X = U V T be the singular value decomposition of Y XT (XXT )-1X  Rdy×dx . Let U^  Rdy×k be a matrix consisting of the first k columns of U .

2.2 NECESSARY AND SUFFICIENT CONDITIONS FOR GLOBAL OPTIMALITY

We now present two main theorems for deep linear neural networks. The theorems describe two sets, one for the case k = min{dx, dy} and the other for k < min{dx, dy}, inside which every critical point of L(W ) is a global minimum. Moreover, the sets have another remarkable property that every critical point outside of these sets is a saddle point. Previous works as Kawaguchi (2016) and Lu & Kawaguchi (2017) showed that any local minimum is a global minimum, and any other critical points are saddle points. In this paper, we are partitioning the domain of L(W ) into two sets clearly delineating one set which only contains global minima and the other set with only saddle points. Theorem 2.1. If k = min{dx, dy}, define the following set
V1 := {(W1, . . . , WH+1) : rank(WH+1 · · · W1) = k} . Then, every critical point of L(W ) in V1 is a global minimum. Moreover, every critical point of L(W ) in V1c is a saddle point. Theorem 2.2. If k < min{dx, dy}, define the following set
V2 := (W1, . . . , WH+1) : rank(WH+1 · · · W1) = k, col(WH+1 · · · Wp+1) = col(U^ ) .
Then, every critical point of L(W ) in V2 is a global minimum. Moreover, every critical point of L(W ) in V2c is a saddle point.
Theorems 2.1 and 2.2 provide necessary and sufficient conditions for a critical point of L(W ) to be globally optimal. From an algorithmic perspective, they provide easily checkable conditions, which we can use to determine if the critical point the algorithm encountered is a global optimum or not.
In Hardt & Ma (2017), the authors consider minimizing population risk of linear residual networks:

minimize

1 2

Ex,y

(I + WH+1) · · · (I + W1)x - y

2 F

,

(2)

where dx = d1 = · · · = dH = dy = d. They assume that x is drawn from a zero-mean distribution with a fixed covariance matrix, and y = Rx +  where  is iid standard Gaussian noise and R is

the true underlying matrix with det(R) > 0. With these assumptions they prove that whenever

max(Wi) < 1 for all i, any critical point is a global minimum (Hardt & Ma, 2017, Theorem 2.2).

Under the same assumptions on data distribution, we can slightly modify Theorem 2.1 to derive
a population risk counterpart, and in fact notice that the result proved in Hardt & Ma (2017) is a
corollary of this modification because having max(Wi) < 1 for all i is a sufficient condition for (I + WH+1) · · · (I + W1) having full rank. Moreover, notice that we can remove the assumption det(R) > 0 which was required by Hardt & Ma (2017). We state this special case as a corollary:

Corollary 2.3 (Theorem 2.2 of Hardt & Ma (2017)). Under assumptions on data distribution as

described

above,

any

critical

point

of

1 2

Ex,y

(I + WH+1) · · · (I + W1)x - y

2 F

is a global min-

imum if max(Wi) < 1 for all i.

Remarks. The previous result (Kawaguchi, 2016) assumed dy  dx and showed that: 1) every local minimum is a global minimum, and 2) any other critical point is a saddle point. A subsequent paper by Lu & Kawaguchi (2017) proved 1) without the assumption dy  dx, but as far as we know there is no result showing 2) in the case of dy > dx. We provide the proof for this case in Lemma B.1. In fact, we propose an alternative proof technique for handling degenerate critical
points, which is much simpler than the technique presented by Kawaguchi (2016).

3

Under review as a conference paper at ICLR 2018

3 ANALYSIS OF DEEP LINEAR NETWORKS

In this section, we provide proofs for Theorems 2.1 and 2.2.

3.1 SOLUTIONS OF THE RELAXED PROBLEM

We first analyze the globally optimal solution of a "relaxation" of L(W ), which turns out to be very useful while proving Theorems 2.1 and 2.2. Consider the relaxed risk function

1 L0(R) = 2

RX - Y

2 F

,

where R  Rdy×dx and rank(R)  k. For any W , the product WH+1WH · · · W1 has rank at most k and setting R to be this product gives the same loss values: L0(WH+1WH · · · W1) = L(W ).
Therefore, L0 is a relaxation of L and

inf L0(R)  inf L(W ).

R:rank(R)k

W

This means that if there exists W such that L(W ) = infR:rank(R)k L0(R), then W is a global minimum of the function L. This observation is very important in proofs; we will show that inside certain sets, any critical point W of L(W ) must satisfy R = WH+1 · · · W1, where R is a global optimum of L0(R). This proves that L(W ) = L0(R) = infR:rank(R)k L0(R), thus showing that W is a global minimum of L.

By restating this observation as an optimization problem, the solution of problem in (1) is bounded below by the minimum value of the following:

minimize

1 2

RX - Y

2 F

subject to rank(R)  k.

(3)

In case where k = min{dx, dy}, (3) is actually an unconstrained optimization problem. Note that L0 is a convex function of R, so any critical point is a global minimum. By differentiating and setting the derivative to zero, we can easily get the unique globally optimal solution

R = Y XT (XXT )-1.

(4)

In case of k < min{dx, dy}, the problem becomes non-convex because of the rank constraint, but its exact solution can still be computed easily. We present the solution of this case as a proposition
and defer the proof to Appendix C due to its technicalities.

Proposition 3.1. Suppose k < min{dx, dy}. Then the optimal solution to (3) is R = U^ U^ T Y XT (XXT )-1,

(5)

which is the orthogonal projection of Y XT (XXT )-1 onto the column space of U^ .

3.2 PARTIAL DERIVATIVES OF L(W )

By simple matrix calculus, we can calculate the derivatives of L(W ) with respect to Wi, for i = 1, . . . , H + 1. We present the result as the following lemma, and defer the details to Appendix C.

Lemma 3.2. The partial derivative of L(W ) with respect to Wi is given as

L Wi

= WiT+1 · · · WHT +1(WH+1WH · · · W1X - Y )XT W1T

· · · WiT-1,

for i = 1, . . . , H + 1.

(6)

This result will be used throughout the proof of Theorems 2.1 and 2.2. For clarity in notation, note that when i = 1, W1T · · · W0T is just an identity matrix in Rdx×dx . Similarly, when i = H + 1, WHT +2 · · · WHT +1 is an identity matrix in Rdy×dy .

We also state an elementary lemma which proves useful in our proofs, whose proof we defer to

Appendix C.

Lemma 3.3. 1. For any A  Rm×n and B  Rn×l where m  n,

AB

2 F



m2 in(A)

B

2 F

.

2. For any A  Rm×n and B  Rn×l where n  l,

AB

2 F



m2 in(B)

A

2 F

.

4

Under review as a conference paper at ICLR 2018

3.3 PROOF OF THEOREM 2.1

We prove Theorem 2.1, which addresses the case k = min{dx, dy}. First, recall that the set defined in Theorem 2.1 is
V1 := {(W1, . . . , WH+1) : rank(WH+1 · · · W1) = k} . As seen in (4), the unique minimum point of L0 has rank k. So, no point W  V1c can be a global minimum of L. Therefore, by Kawaguchi (2016, Theorem 2.3.(iii)) and Lemma B.1, any critical point in V1c must be a saddle point.
For the rest of our proof, we need to consider two cases: dy  dx and dx  dy. If dx = dy, both cases work. The outline of the proof is as follows: we define a new set W , show that any critical point in the set W is a global minimum, and then show that every W  V1 is also in W for some
> 0. This proves that any critical point of L(W ) in V1 is also a critical point in W for some > 0, hence a global minimum.

The following proposition proves the first step:

Proposition 3.4. Assume that k = min{dx, dy}. For any > 0, define the following set:

W :=

{(W1, . . . , WH+1) : min(WH+1 · · · W2)  } , {(W1, . . . , WH+1) : min(WH · · · W1)  } ,

if dy  dx, if dx  dy.

Then any critical point of L(W ) in W is a global minimum point.

Proof. (If dy  dx) Consider (6) in the case of i = 1. We can observe that W2T · · · WHT +1  Rd1×dy and that d1  dy. Then by Lemma 3.3.1,

L W1

2
 m2 in(WH+1 · · · W2)
F

(WH+1WH · · · W1X - Y )XT

2 F



2

(WH+1WH · · · W1X - Y )XT

2 F

.

By the above inequality, any critical point in W satisfies

i,

L Wi

=0



(WH+1WH · · · W1X

- Y )XT

=

0,

which means that WH+1WH · · · W1 = Y XT (XXT )-1. The product is the unique globally optimal

solution (4) of the relaxed problem in (3), so W is a global minimum point of L.

(If dx  dy) Consider (6) for i = H + 1. We can observe that W1T · · · WHT  Rdx×dH and that dx  dH . Then by Lemma 3.3.2,

L WH+1

2

F

2

(WH+1WH · · · W1X - Y )XT

2 F

,

and the rest of the proof flows in a similar way as the previous case.

The next proposition proves the theorem: Proposition 3.5. For any point W  V1, there exists an > 0 such that W  W .

Proof. Define a new set W, a "limit" version (as  0) of W , as

W :=

{(W1, . . . , WH+1) : rank(WH+1 · · · W2) = dy} , {(W1, . . . , WH+1) : rank(WH · · · W1) = dx} ,

if dy  dx, if dx  dy.

We show that V1  W by showing that Wc  V1c. Consider

Wc =

{(W1, . . . , WH+1) : rank(WH+1 · · · W2) < dy} , {(W1, . . . , WH+1) : rank(WH · · · W1) < dx} ,

if dy  dx, if dx  dy.

Then any W  Wc must have rank(WH+1 · · · W1) < min{dx, dy} = k, so W  V1c. Thus, any W  V1 is also in W, so either rank(WH+1 · · · W2) = dy or rank(WH · · · W1) = dx, depending on the cases. Then, we can set

=

min(WH+1 · · · W2), min(WH · · · W1),

if dy  dx, if dx  dy.

We always have > 0 because the matrices are full rank, and we can see that W  W .

5

Under review as a conference paper at ICLR 2018

3.4 PROOF OF THEOREM 2.2

In this section we prove Theorem 2.2, which tackles the case k < min{dx, dy}. Note that this assumption also implies that 1  p  H.

As for the proof of Theorem 2.1, define

V1 := {(W1, . . . , WH+1) : rank(WH+1 · · · W1) = k} .

The globally optimal point of the relaxed problem (3) has rank k, as seen in (5). Thus, any point outside of V1 cannot be a global minimum. Then, by Kawaguchi (2016, Theorem 2.3.(iii)) and Lemma B.1, it follows that any critical point in V1c must be a saddle point. The remaining proof considers points in V1.

For this section, let us introduce some additional notations to ease presenation. Define

E := (WH+1 · · · W1X - Y )XT  Rdy×dx , Ai := WiT+1 · · · WHT +1  Rdi×dy , Bi := W1T · · · WiT-1  Rdx×di-1 ,

i = 1, . . . , H + 1,

so that

L Wi

=

AiEBi.

Notice that AH+1

and B1

are identity matrices.

Now consider any tuple W  V1. Since the full product WH+1 · · · W1 has rank k, any partial products Ai and Bi must have rank(Ai)  k and rank(Bi)  k, for all i. Then, consider Ap  Rk×dy and Bp+1  Rdx×k. Since rank(Ap)  k and rank(Bp+1)  k, we can see that rank(Ap) =
rank(Bp+1) = k. Also, notice that Ai = Wi+1Ai+1 and Bi+1 = BiWi, so that

rank(A1)  rank(A2)  · · ·  rank(Ap) and rank(BH+1)  rank(BH )  · · ·  rank(Bp+1).

However, we have k  rank(A1) and k  rank(BH+1), so the ranks are all identically k. Also,

row(A1)  row(A2)  · · ·  row(Ap) and col(BH+1)  col(BH )  · · ·  col(Bp+1),

but it was just shown that the these spaces have the same dimensions, which equals k, meaning

row(A1) = row(A2) = · · · = row(Ap) and col(BH+1) = col(BH ) = · · · = col(Bp+1).

Using this observation, we can now state a proposition showing necessary and sufficient conditions for a tuple W  V1 to be a critical point of L(W ).
Proposition 3.6. A tuple W  V1 is a critical point of L if and only if ApE = 0 and EBp+1 = 0.

Proof. (If part) ApE

=

0 implies that col(E)



row(Ap)

=

···

=

row(A1),

so

L Wi

=

AiEBi = 0 · Bi = 0, for i = 1, . . . , p. Similarly, EBp+1 = 0 implies row(E)  col(Bp+1) =

···

=

col(BH+1), so

L Wi

=

AiEBi

=

Ai

·0

=

0 for i

=

p + 1, . . . , H

+ 1.

(Only if part) We have

L Wi

=

AiEBi

=

0 for all i.

This means that

col(EBi)  row(Ai) = row(Ap) for i = 1, . . . , p

row(AiE)  col(Bi) = col(Bp+1) for i = p + 1, . . . , H + 1.

Now recall that B1 and AH+1 are identity matrices, so col(E)  row(Ap) and row(E)  col(Bp+1), which proves ApE = 0 and EBp+1 = 0.

Now we present a proposition that specifies the necessary and sufficient condition in which a critical point of L(W ) in V1 is a global minimum. Recall that when we take the SVD Y XT (XXT )-1X = U V T , U^  Rdy×k is defined to be a matrix consisting of the first k columns of U .
Proposition 3.7. A critical point W  V1 of L(W ) is a global minimum point if and only if col(WH+1 · · · Wp+1) = row(Ap) = col(U^ ).

Proof. Since W is a critical point, by Proposition 3.6 we have ApE = 0. Also note from the definitions of Ai's and Bi's that WH+1 · · · W1 = ApT BpT+1, so
ApE = Ap(ApT BpT+1X - Y )XT = ApApT BpT+1XXT - ApY XT = 0.

6

Under review as a conference paper at ICLR 2018

Because rank(Ap) = k, and ApApT  Rk×k is invertible, so Bp+1 is determined uniquely as
BpT+1 = (ApATp )-1ApY XT (XXT )-1,
thus WH+1 · · · W1 = ApT BpT+1 = ATp (ApATp )-1ApY XT (XXT )-1.
Comparing this with (5), W is a global minimum solution if and only if
U^ U^ T Y XT (XXT )-1 = WH+1 · · · W1 = ApT (ApATp )-1ApY XT (XXT )-1.
This equation holds if and only if ATp (ApATp )-1Ap = U^ U^ T , meaning that they are projecting Y XT (XXT )-1 onto the same subspace. The projection matrix ATp (ApATp )-1Ap is onto row(Ap), while U^ U^ T is onto col(U^ ). From this, we conclude that W is a global minimum point if and only if row(Ap) = col(U^ ).
From Proposition 3.7, we can define the set V2 that appeared in Theorem 2.2, and conclude that every critical point of L(W ) in V2 is a global minimum, and any other critical points are saddle points.

4 EXTENSION TO DEEP NONLINEAR NEURAL NETWORKS

In this section, we present some sufficient conditions for global optimality for deep nonlinear neural networks via a function space view. Given a smooth nonlinear function h that maps input to output, Bartlett et al. (2017) described a method to decompose it into a number of smooth nonlinear functions h = hH+1  · · ·  h1 where hi's are close to identity. Using Fre´chet derivatives of the population risk with respect to each function hi, they showed that when all hi's are close to identity, any critical point of the population risk is a global minimum. One can see that these results are direct generalization of Theorems 2.1 and 2.2 of Hardt & Ma (2017) to nonlinear networks and utilize the classical "small gain" arguments often used in nonlinear analysis and control (Khalil, 1996; Zames, 1966). Motivated by this result, we extended Theorem 2.1 to deep nonlinear neural networks and obtained sufficient conditions for global optimality in function space.

4.1 PROBLEM FORMULATION AND NOTATION

Suppose the data X  Rdx and its corresponding label Y  Rdy are drawn from some distribution.
Notice that in this section, X and Y are random vectors instead of matrices. We want to predict Y
given X with a deep nonlinear neural network that has H hidden layers. We express each layer of the network as functions hi : Rdi-1  Rdi , so the entire network can be expressed as a composition of functions: hH+1  hH  · · ·  h1. Our goal is to obtain functions h1, . . . , hH+1 that minimize the population risk functional:

1 L(h) = L(h1, . . . , hH+1) := 2 E

hH+1  · · ·  h1(X) - Y

2 2

,

where h is a shorthand notation for (h1, . . . , hH+1). It is well-known that the minimizer of squared error risk is the conditional expectation of Y given X, which we will denote h(x) = E[Y | X = x]. With this, we can separate the risk functional into two terms

1 L(h) = 2 E

hH+1  · · ·  h1(X) - h(X)

2 2

+ C,

where the constant C denotes the variance that is independent of h1, . . . , hH+1. Note that if hH+1  · · ·  h1 = h almost surely, the first term in L(h) vanishes and the optimal value L of L(h) is C.

Assumptions. Define the function spaces as the following:

F := Fi :=

h : Rdx  Rdy | h is differentiable, h(0) = 0, and sup
x

h(x) 2 <  x2

,

h : Rdi-1  Rdi | h is differentiable, h(0) = 0, and sup
x

h(x) 2 <  x2

,

7

Under review as a conference paper at ICLR 2018
where Fi are defined for all i = 1, . . . , H + 1. Assume that h  F , and that we are optimizing L(h) with h1  F1, . . . , hH+1  FH+1. In other words, the functions in F , F1, . . . , FH+1 are differentiable and show sublinear growth starting from 0. Notice that hH+1  · · ·  h1  F , because a composition of differentiable functions is also differentiable, and a composition of sublinear functions is also sublinear. We also assume that di  min{dx, dy} for all i = 1, . . . , H + 1, which is identical to the assumption k = min{dx, dy} in Theorem 2.1.
Notation. To simplify multiple composition of functions, we denote hi:j = hi  hi-1  · · ·  hj+1  hj. As in the matrix case, h0:1 and hH+1:H+2 mean identity maps in Rdx and Rdy , respectively. Given a function f , let J[f ](x) be the Jacobian matrix of function f evaluated at x. Let Dhi [L(h)] be the Fre´chet derivative of L(h) with respect to hi evaluated at h. The Fre´chet derivative Dhi [L(h)] is a linear functional that maps a function (direction)   Fi to a real number (directional derivative).
4.2 SUFFICIENT CONDITIONS FOR GLOBAL OPTIMALITY
Here, we present two theorems which give sufficient conditions for a critical point (Dhi [L(h)] = 0 for all i) in the function space to be a global optimum. The proofs are deferred to Appendix A. Theorem 4.1. Consider the case dx  dy. If there exists > 0 such that
1. J [hH+1:2](z)  Rdy×d1 has min(J [hH+1:2](z))  for all z  Rd1 ,
2. hH+1:2(z) is twice-differentiable,
then any critical point of L(h), in terms of Dh1 [L(h)], . . . , DhH+1 [L(h)], is a global minimum. Theorem 4.2. Consider the case dx  dy. Assume that there exists some j  {1, . . . , H + 1} such that dx = dj-1 and dy  dj. If there exist 1, 2 > 0 such that
1. hj-1:1 : Rdx  Rdj-1 = Rdx is invertible,
2. hj-1:1 satisfies hj-1:1(u) 2  1 u 2 for all u  Rdx ,
3. J [hH+1:j+1](z)  Rdy×dj has min(J [hH+1:j+1](z))  2 for all z  Rdj ,
4. hH+1:j+1(z) is twice-differentiable,
then any critical point of L(h), in terms of Dh1 [L(h)], . . . , DhH+1 [L(h)], is a global minimum.
Note that these theorems give sufficient conditions, whereas Theorems 2.1 and 2.2 provide necessary and sufficient conditions. So, if the sets we are describing in Theorems 4.1 and 4.2 do not contain any critical point, the claims would be vacuous. We ensure that there are critical points in the sets, by presenting the following proposition, whose proof is also deferred to Appendix A. Proposition 4.3. For each of Theorems 4.1 and 4.2, there exists at least one global minimum solution of L(h) satisfying the conditions of the theorem.
Discussion and Future work. Theorems 4.1 and 4.2 state that in certain sets of (h1, . . . , hH+1), any critical point in function space a global minimum. However, this does not imply that any critical point for a fixed sigmoid or arctan network is a global minimum. As noted in (Bartlett et al., 2017), there is a downhill direction in function space at any suboptimal point, but this direction might be orthogonal to the function space represented by a fixed network, and may hence result in local minima in the parameter space of the fixed architecture. Understanding the connection between the function space and parameter space of commonly used architectures is an open direction for future research; such results could potentially also provide guidance for new architecture design. Bartlett et al. (2017) made some assumptions on the function spaces including the following: the function h is invertible and there exists a point where the Jacobian matrix has positive determinant, which correspond to the assumption that det(R) > 0 in Hardt & Ma (2017). Please note that in our setup we do not require such assumptions on h.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
Peter Bartlett, Steve Evans, and Phil Long. Deep residual networks: Representation and optimization properties, 2017. Talk by Peter Bartlett at the Computational Challenges in Machine Learning Workshop at Simons Institute for the Theory of Computing, Berkeley, CA, USA.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Proceedings of the 1st International Conference on Neural Information Processing Systems, pp. 494­501. MIT Press, 1988.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015.
Benjamin D Haeffele and Rene´ Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331­7339, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on Learning Representations, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630­645. Springer, 2016b.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Hassan K Khalil. Noninear Systems. Prentice-Hall, New Jersey, 1996.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.
Katta G Murty and Santosh N Kabadi. Some np-complete problems in quadratic and nonlinear programming. Mathematical programming, 39(2):117­129, 1987.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 2603­2612, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv preprint arXiv:1611.03131, 2016.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300­1303, 1995.
George Zames. On the input-output stability of time-varying nonlinear feedback systems part one: Conditions derived using concepts of loop gain, conicity, and positivity. IEEE transactions on automatic control, 11(2):228­238, 1966.
9

Under review as a conference paper at ICLR 2018

A ANALYSIS OF DEEP NONLINEAR NETWORKS

A.1 NOTATIONS

In this section, we introduce additional notations that are used in the proofs. To emphasize that
the Fre´chet derivative Dhi [L(h)] is a linear functional that outputs a real number, we will write Dhi [L(h)]() in an inner-product form Dhi [L(h)],  . This notation also helps avoiding confusion coming from multiple parentheses and square brackets.

There are many different kinds of norms that appear in the proofs. Given a finite-dimensional real

vector v, v 2 denotes its 2 norm. For a matrix A, its operator norm is defined as A op =

supx

Ax x

2 2

.

Let h



F.

Then define a "generalized" induced norm for nonlinear functions with

sublinear growth:

h nl = supx

h(x) x2

2

,

where

the

subscript

nl

is

used

to

emphasize

that

this

norm

is for nonlinear functions. The norm · nl is defined in the same way for Fi's. Now, given a

linear functional G that maps a function f  Fi to a real number G, f , define the operator norm

G op = supfFi

G,f f nl

.

A.2 FRE´ CHET DERIVATIVES

By definition of Fre´chet derivatives, we have

Dhi [L(h)], 

= lim L(h1, . . . , hi +
0

, . . . , hH+1) - L(h) ,

where   Fi is the direction of perturbation and Dhi [L(h)],  is the directional derivative along that direction . From the definition of L(h),

L(h1, . . . , hi + , . . . , hH+1)

1 =2E

hH+1:i+1  (hi +

)  hi-1:1(X) - h(X)

2 2

+C

1 =2E

hH+1:i+1(hi:1(X) +

(hi-1:1(X))) - h(X)

2 2

+C

1 =2E

hH+1:1(X) +

J [hH+1:i+1](hi:1(X))(hi-1:1(X)) + O(

2) - h(X)

2 2

+C

=L(h) + E (hH+1:1(X) - h(X))T J [hH+1:i+1](hi:1(X))(hi-1:1(X)) + O( 2).

Therefore, Dhi [L(h)],  = E (hH+1:1(X) - h(X))T J [hH+1:i+1](hi:1(X))(hi-1:1(X)) .
This equation (7) will be used in the proof of Theorems 4.1 and 4.2.

(7)

A.3 PROOF OF THEOREM 4.1

From (7), consider Dh1 [L(h)]. For any   F1, Dh1 [L(h)],  = E (hH+1:1(X) - h(X))T J [hH+1:2](h1(X))(X) .

Let A(X) = J[hH+1:2](h1(X)). Since A(X) has full row rank by assumption, A(X)A(X)T is invertible. Then define a particular direction

~(X) = A(X)T (A(X)A(X)T )-1(hH+1:1(X) - h(X)),

so that

Dh1 [L(h)], ~ = E

hH+1:1(X) - h(X)

2 2

.

It remains to check if ~  F1. It is easily checked that ~(0) = 0 because hH+1:1(0) - h(0) = 0. Since J[hH+1:2] is differentiable by assumption and h1  F1, A(X) is differentiable and A(X)T , (A(X)A(X)T )-1 are differentiable functions. Also, hH+1:1 - h  F , so we can conclude that ~ is differentiable.

10

Under review as a conference paper at ICLR 2018

Moreover, if we decompose A(X) with SVD, A(X) = U V T ,  is of the form  = [1 0] and

A(X)T (A(X)A(X)T )-1 = V T U T (U V T V T U T )-1 = V T U T (U 12U T )-1

= V T U T U -1 2U T = V

1-1 0

UT,

from which we can see that

A(X)T (A(X)A(X)T )-1 op = max(A(X)T (A(X)A(X)T )-1)  1/ ,

by our assumption. Note that, for any X  Rdx ,

~(X) 2 = 


A(X)T (A(X)A(X)T )-1(hH+1:1(X) - h(X)) 2 A(X)T (A(X)A(X)T )-1 op hH+1:1(X) - h(X) 2
A(X)T (A(X)A(X)T )-1 op hH+1:1 - h nl X 2 .

Since this holds for any X, we have

~ nl  A(X)T (A(X)A(X)T )-1 op hH+1:1 - h nl  hH+1:1 - h nl ,

which ensures that ~  F1. Finally,

Dh1 [L(h)] op 

Dh1 [L(h)], ~ ~ nl



E

hH+1:1(X) - h(X)

2 2

(L(h) - L)

hH+1:1 - h nl

=

hH+1:1 - h

,
nl

which yields

Dh1 [L(h)] op hH+1:1 - h nl  (L(h) - L).

From this we can see that if we have a critical point of L(h), then Dh1 [L(h)] op = 0 implies L(h) = L, which means that the critical point is a global minimum of L(h).

A.4 PROOF OF THEOREM 4.2

Recall that by assumption we have j  {1, . . . , H + 1} such that dx = dj-1 and dy  dj. Consider Dhj [L(h)], then for any   Fj,

Dhj [L(h)],  = E (hH+1:1(X) - h(X))T J [hH+1:j+1](hj:1(X))(hj-1:1(X)) .

As done in the previous theorem, for any w  Rdj-1 , let A(w) = J [hH+1:j+1](hj(w)). Since A(w) has full row rank by assumption, A(w)A(w)T is invertible. Then define

~(w) = A(w)T (A(w)A(w)T )-1(hH+1:1 - h)  h-j-11:1(w),

so that

Dhj [L(h)], ~ = E

hH+1:1(X) - h(X)

2 2

.

We need to check if ~  Fj. It is easily checked that ~(0) = 0. Since J[hH+1:j+1] is differentiable by assumption and hj  Fj, A(w) is differentiable, and so are A(w)T and (A(w)A(w)T )-1. The inverse function of a differentiable and invertible function is also differentiable, so (hH+1:1 - h)  h-j-11:1 is differentiable. Hence, we can conclude that ~ is differentiable.
As seen in the previous section,

A(w)T (A(w)A(w)T )-1 op = max(A(w)T (A(w)A(w)T )-1)  1/ 2.

By the assumption that hj-1:1 is invertible and hj-1:1(u) 2  1 u 2,

v 2

1

h-j-11:1(v)

,
2

11

Under review as a conference paper at ICLR 2018

for all v  Rdj-1 . From this, we can see that hj--11:1 nl  1/ 1. For any w  Rdj-1 ,

~(w) 2 =   

A(w)T (A(w)A(w)T )-1(hH+1:1 - h)  h-j-11:1(w) 2 A(w)T (A(w)A(w)T )-1 op (hH+1:1 - h)  hj--11:1(w) 2 A(w)T (A(w)A(w)T )-1 op hH+1:1 - h nl hj--11:1(w) 2 A(w)T (A(w)A(w)T )-1 op hH+1:1 - h nl hj--11:1 nl w 2 .

From this, we have

~ nl 

A(w)T (A(w)A(w)T )-1 op hH+1:1 - h nl

h-j-11:1


nl

hH+1:1 - h nl .
12

Finally,

Dhj [L(h)] op 

Dhj [L(h)], ~ ~ nl



1 2E

hH+1:1(X) - h(X)

2 2

hH+1:1 - h nl

=

1 2(L(h) - L) hH+1:1 - h nl

,

which yields

Dhj [L(h)] op hH+1:1 - h nl  1 2(L(h) - L).

A.5 PROOF OF PROPOSITION 4.3

(Theorem 4.1) By assumption, we have d1  dy. Set h1(x) = (h(x), 0, . . . , 0) where for every x  Rdx , the first dy components of h1(x) are identical to h(x), and all other components are zero.
For the rest of hi's, define hi : Rdi-1  Rdi to be

hi(w) =

(w1, . . . , wdi ), (w1, . . . , wdi-1 , 0, . . . , 0),

if di  di-1, if di > di-1,

(8)

for all w  Rdi-1 . Since di  dy for all i, we can check that hH+1  · · ·  h1 = h, and hi 
Fi for all i. Moreover, for all z  Rd1 , J[hH+1:2](z) is all 0 except 1's in diagonal entries, so min(J [hH+1:2](z))  1 and hH+1:2(z) is twice-differentiable.

(Theorem 4.2) It is given that we have j  {1, . . . , H + 1} such that dx = dj-1 and dy  dj. Set hj(x) = (h(x), 0, . . . , 0), where the first dy components are h(x) and the rest are zero. All the rest of hi are set as in (8). Then, it can be easily checked that hi  Fi for all i and all the conditions
of the theorem are satisfied.

B DEFERRED LEMMA

Lemma B.1. Suppose we are given a data matrix X  Rdx×m and an output matrix Y  Rdy×m, where dx < dy. Assume XXT and Y XT have full ranks. Consider minimizing the empirical
squared error risk:

1 L(W1, . . . , WH+1) := 2

WH+1WH · · · W1X - Y

2 F

,

where Wk  Rdk×dk-1 , k = 1, . . . , H + 1 are weight matrices of the linear neural network, and d0 = dx and dH+1 = dy for simplicity in notation. Also let W denote the tuple (W1, . . . , WH+1). Then, any critical point of L(W ) that is not a local minimum is a saddle point.

Proof. For this lemma, we separate the proof into two cases: WH · · · W1 = 0 and WH · · · W1 = 0. The crux of the proof is to show that any critical point cannot be a local maximum. Then, any critical point is either a local minimum or a saddle point, so the conclusion of this lemma follows.
In case of WH · · · W1 = 0, we use some of the results in Kawaguchi (2016) and examine the Hessian of L(W ) with respect to vec(WHT+1), where vec(A) denotes vectorization of matrix A.

12

Under review as a conference paper at ICLR 2018

Let Dvec(WHT+1)L(W ) be the partial derivative of L(W ) with respect to vec(WHT +1) in numerator layout. It was shown by Kawaguchi (2016, Lemma 4.3) that the Hessian matrix

H(W ) = Dvec(WHT+1)

T
Dvec(WHT+1)L(W ) =
=

I  (WH · · · W1X)(WH · · · W1X)T I  WH · · · W1XXT W1T · · · WHT ,

where  denotes the Kronecker product of two matrices. Notice that H(W ) is positive semidefinite. Since XXT is full rank, whenever WH · · · W1 = 0 there exists a strictly positive eigenvalue in H(W ), which means that there exists an increasing direction. So W cannot be a local maximum.
The case where WH · · · W1 = 0 requires a bit more careful treatment. Note that this case corresponds to where we have degenerate critical points, which are in many cases much harder to handle.
For any arbitrary > 0, we describe a procedure that perturbs the matrices W1, . . . , WH+1 by perturbations sampled from Frobenius norm balls of radius centered at 0, which we will denote as Bi( ), i = 1, . . . , H +1. Let U (Bi( )) be the uniform distribution over the ball Bi( ). The algorithm goes as the following:

1. For i  {1, . . . , H + 1}
1.1. Sample i  U (Bi( )), and define Vi = Wi + i. 1.2. If WH+1 · · · Wi+1Vi · · · V1 = 0, stop and return i = i.

First, recall that the set of rank-deficient matrices have Lebesgue measure zero, so for any sample i  U (Bi( )), Vi = Wi + i has full rank with probability 1. If we proceed the for loop until i = H + 1, we have a full-rank VH+1 · · · V1 with probability 1, which means that the algorithm must return i  {1, . . . , H + 1} with probability 1. Notice that before and after the i-th iteration, we
have

WH+1 · · · Wi Vi-1 · · · V1 = 0, WH+1 · · · Wi+1Vi · · · V1 = WH+1 · · · Wi+1(Wi + i )Vi-1 · · · V1 = 0,

meaning that WH+1 · · · Wi+1i Vi-1 · · · V1 WH+1 · · · Wi+1i Vi-1 · · · V1, and then notice that

=

0.

Define ^

=

WH+1 · · · Wi+1(Wi - i )Vi-1 · · · V1 = -^ .

Now, define two points

U (1) = (V1, . . . , Vi-1, Wi + i , Wi+1, . . . , WH+1), U (2) = (V1, . . . , Vi-1, Wi - i , Wi+1, . . . , WH+1),

and notice that they are all in the neighborhood of W , that is, the Cartesian product of -radius balls centered at W1, . . . , WH+1. Moreover, we have

1 L(W ) =
2

0·X -Y

2 F

=

1 2

Y

2 F

,

L(U (1)) = 1 2

^ X - Y

21 =
F2

Y

2 F

+

1 2

^ X

2
-

F

^ X, Y

,

L(U (2)) = 1 2

-^ X - Y

21 =

F2

Y

2 F

+

1 2

^ X

2
+

F

^ X, Y

,

from which we can see that at least one of L(W ) < L(U (1)) or L(W ) < L(U (2)) must hold. This shows that for any > 0, there is a point U in -neighborhood of W with a strictly greater function value L(U ). This proves that W cannot be a local maximum.

13

Under review as a conference paper at ICLR 2018

C DEFERRED PROOFS

C.1 PROOF OF PROPOSITION 3.1

In case of k < min{dx, dy}, we can decompose the loss function in the following way:

RX - Y

2 F

=

RX - Y XT (XXT )-1X + Y XT (XXT )-1X - Y

2 F

=

RX - Y XT (XXT )-1X

2 F

+

Y XT (XXT )-1X - Y

2 F

+ 2 tr((Y XT (XXT )-1X - Y )(RX - Y XT (XXT )-1X)T ).

Let us take a close look into the last term in the RHS. Note that Y XT (XXT )-1X is the orthogonal projection of Y onto row(X), so each row of Y XT (XXT )-1X - Y must be in null(X). Also,

(RX - Y XT (XXT )-1X)T = XT (RT - (XXT )-1XY T ).

It is XT right-multiplied with some matrix, so its columns must lie in col(XT ) = row(X). By the fact that null(X) = row(X),

(Y XT (XXT )-1X - Y )(RX - Y XT (XXT )-1X)T = 0,

thus holds.

1 L0(R) = 2

RX - Y XT (XXT )-1X

2 F

+

1 2

Y XT (XXT )-1X - Y

2 F

Now, (3) becomes a problem of minimizing

RX - Y XT (XXT )-1X

2 F

subject

to

the

rank

con-

straint rank(R)  k. The optimal solution for this is obtained when RX is the k-rank approxi-

mation of Y XT (XXT )-1X. Then, k-rank approximation of Y XT (XXT )-1X can be expressed

as U^ U^ T Y XT (XXT )-1X, where U^ is unique due to our assumption that all singular values are

distinct. Therefore,

R = U^ U^ T Y XT (XXT )-1

is the unique global minimum solution of (3) when k < min{dx, dy}.

C.2 PROOF OF LEMMA 3.2

L(W1, . . . , Wi-1, Wi + i, Wi+1, . . . , WH+1)

1 =
2

WH+1 · · · Wi+1(Wi + i)Wi-1 · · · W1X - Y

2 F

1 =
2

WH+1 · · · W1X - Y

+ WH+1 · · · Wi+1iWi-1 · · · W1X

2 F

=L(W ) + tr((WH+1 · · · Wi+1iWi-1 · · · W1X)T (WH+1 · · · W1X - Y )) + O( i F2 )

=L(W ) + tr(WiT+1 · · · WHT +1(WH+1 · · · W1X - Y )XT W1T · · · WiT-1iT ) + O(

i

2 F

).

From this, we can conclude that

L Wi

= WiT+1 · · · WHT +1(WH+1 · · · W1X

- Y )XT W1T

· · · WiT-1.

C.3 PROOF OF LEMMA 3.3

1. Since AT A m2 in(A)I, BT AT AB m2 in(A)BT B. Then

AB

2 F

=

tr(BT

AT

AB)



m2 in(A)

tr(BT

B)

=

m2 in(A)

B

2 F

.

2. Since BBT m2 in(B)I, ABBT AT m2 in(B)AAT . Then

AB

2 F

=

tr(BT

AT

AB)

=

tr(ABBT

AT

)



m2 in(B)

tr(AAT

)

=

m2 in(B)

A

2 F

.

14

