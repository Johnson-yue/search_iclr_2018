Under review as a conference paper at ICLR 2018
GAUSSIAN PROCESS NEURONS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a method to learn stochastic activation functions for use in probabilistic neural networks. First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks. Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling. Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model. The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions. Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.
1 INTRODUCTION
The popularity of deep learning and the implied race for better accuracy and performance has lead to new research of the fundamentals of neural networks. Finding an optimal architecture often focusses on a hyperparameter search over the network architecture, regularization parameters, and one of a few standard activation functions: tanh, ReLU (Glorot et al. (2011)), maxout (Goodfellow et al. (2013)) . . . Focussing on the latter, looking into activation functions has only taken off since Nair & Hinton (2010) introduced the rectified linear unit (ReLU), which were shown to produce significantly better results on image recognition tasks Krizhevsky et al. (2012). Maas et al. (2013) then introduced the leaky ReLU, which has a very small, but non-zero, slope for negative values. He et al. (2015) proposed the parameterized ReLU, by making the slope of the negative part of the leaky ReLU adaptable. It was trained as an additional parameter for each neuron alongside the weights of the neural network using stochastic gradient descent. Thus, the activation function was not treated as a fixed hyper-parameter anymore but as adaptable to training data. While the parameterized ReLU only has one parameter, this was generalized in Agostinelli et al. (2014a) to piecewise linear activation functions that can have an arbitrary (but fixed) number of points where the function changes it slope. This can be interpreted as a different parameterization of a Maxout network (Goodfellow et al. (2013)), in which each neuron takes the maximum over a set of different linear combinations of its inputs.
Instead of having a fixed parameter for the negative slope of the ReLU, Xu et al. (2015) introduced stochasticity into the activation function by sampling the value for the slope with each training iteration from a fixed uniform distribution. Clevert et al. (2015) and Klambauer et al. (2017) replaced the negative part of ReLUs with a scaled exponential function and showed that, under certain conditions, this leads to automatic renormalization of the inputs to the following layer and thereby simplifies the training of the neural networks, leading to an improvement in accuracy in various tasks.
Nearly fully adaptable activation functions have been proposed by Eisenach et al. (2016). The authors use a Fourier basis expansion to represent the activation function; thus with enough coefficients any (periodic) activation function can be represented. The coefficients of this expansion are trained as network parameters using stochastic gradient descent or extensions thereof.
Promoting a more general approach, Agostinelli et al. (2014b) proposed to learn the activation functions alongside the layer weights. Their adaptive piecewise linear units consist of a sum of hinge-shaped functions with parameters to control the hinges and the slopes of the linear segments. However, by construction the derivative of these activation functions is not continuous at the joints between two linear segments, which often leads to non-optimal optimizer performance.
1

Under review as a conference paper at ICLR 2018

To our knowledge, previous research on learning activation functions took place in a fully deterministic setting, i.e. deterministic activation functions were parameterized and included in the optimization of a conventional neural network. Here instead, we explore the setting of probabilistic activation functions embedded in a graphical model of random variables resembling the structure of a neural network. We develop the theory of Gaussian-process neurons and subsequently derive a lower-bound approximation using variational inference, in order to develop a computationally efficient version of the Gaussian Process neuron.
NOTATION
To define the model we will need to slice matrices along rows and columns. Given a matrix X, we will write Xi to select all elements of the i-th row and X j to select all elements of the j-th column.

2 GAUSSIAN PROCESSES

Gaussian Processes (GPs) are nonparametric models that provide flexible probabilistic approaches for function estimation. A Gaussian Process (Rasmussen & Williams, 2006) defines a distribution over a function f (x)  GP(m(x), k(x, x )) where m is called the mean function and k is the covariance function. For S inputs x  RS×N of N dimensions the corresponding function values f with fi f (Xi ) follow a multivariate normal distribution1
f  N (m, K(X, X))

with mean vector mi m(Xi ) and K(X, X) is the covariance matrix defined by (K(X, X))ij k(Xi , Xj ). In this work we use the zero mean function m(x) = 0 and the squared exponential (SE) covariance function with scalar inputs,

(x - x )2

k(x, x ) = exp - 2 2

,

(1)

where  is called the length-scale and determines how similar function values of nearby inputs are according to the GP distribution. Since this covariance function is infinitely differentiable, all function samples from a GP using it are smooth functions.

3 GAUSSIAN PROCESS NEURONS

We will first describe the fundamental, non-parametric model, which will be approximated in the
following sections for efficient training and inference. Let the input to the l-th layer of Gaussian Process neurons (GPNs) be denoted by Xl-1  RS×Nl-1 where S is the number of data points (samples) and Nl-1 is the number of input dimensions. A layer l  {1, . . . , L} of Nl GPNs indexed by n  {1, . . . Nl} is defined by the joint probability

P(X l n ,

F

l n

|

X l-1 )

=

P(F

l n

|

X l-1 )

P(X l n

|

F

l n

)

(2)

with the GP prior F l conditioned on the layer inputs Xl-1 multiplied with the weights W l,

F

l n

|

X l-1



N (0,

K

l

(X

l-1

W

l n

,

X

l-1W

l n

))

,

(3)

and an additive Gaussian noise distribution,

Xln

|

F

l n



N

(F

l n

,

(nl )21)

.

This

corresponds

to

a

probabilistic

activation

function

Fsln

=

fnl (Xsl-1

W

l n

)

with

(4)

fnl (z)  GP(0, k(z, z )) .

(5)

This GP has scalar inputs and uses the standard squared exponential covariance function. Analogous to standard neural networks, GPN layers can be stacked to form a multi-layer feed-forward network. The joint probability of such a stack is

L

P(X{l}, F {l}) = P(X0)

P(X

l

n,

F

l n

|

X

l-1

)

.

l=1

(6)

1We omit writing P (·).

2

Under review as a conference paper at ICLR 2018

U ln

V

l n

Xsl-1 1 ...

Asl n

Fsln

Xsln n2

Wn

Xsl-N1

sample s

inputs activation response output

Figure 1: The auxiliary parametric representation of a GPN using virtual observation inducing points V and targets U .

All input samples Xs0 , s  {1, . . . , S}, are assumed to be normally distributed with known mean and covariance,

S

P(X0) = N (Xs0 | µisn , sin ) .

(7)

s=1

To obtain predictions P(XL | X0), all latent variables in eq. (6) would need to be marginalized out; unfortunately due to the occurrence of Xl in the covariance matrix in eq. (3) analytic integration is intractable.2

3.1 AUXILIARY PARAMETRIC REPRESENTATION

The path to obtain a tractable training objective is to temporarily parameterize the activation func-

tion fnl (z) of each GPN using virtual observations (originally proposed by Quin~onero-Candela & Rasmussen (2005) for sparse approximations of GPs) of inputs and outputs of the function. These

virtual observations are only introduced as an auxiliary device and will be marginalized out later.

Each virtual Under these

observation assumptions

r consists on fdl , the

GofPaprsicoarlaPr(Finldnu|cXinlg-p1)oiinstreVprllnacaenddwciothrresponding

target

Urln.

F

l d

|

X

l-1

,

U

l n



N

(µFnl

,

F

l
n

)

(8)

where the mean and variance are those obtained by using the virtual observations as "training" points for a GP regression evaluated at the "test" points XlW ld,

µFnl

=

K(Xl-1W ln,

V

l n

)

(Knl

)-1

U

l n

F

l
d

=

K(Xl-1W ln,

Xl-1W ln)

-

K

(X

l-1

W

l n

,

V

ln)

(Knl

)-1 K(V ln, Xl-1W ln) .

(9) (10)

where Knl rt = K(Vrln, Vtln).

Given enough inducing points that lie densely between the layer's activations Al n = Xl-1W ln, the shape of the activation function becomes predominantly determined by the corresponding targets

of these inducing points. Consequently, the inter-sample correlation in eq. (8) becomes negligible,

allowing us to further approximate this conditional by factorizing it over the samples; thus we have

P(F

l n

|

Xl,

U

ln)

=

s P(Fsln | Xsl-1, U ln) with

Fsln | Xsl-1, U ln  N (µsFnl , Fssln) .

(11)

We

now

marginalize

eq.

(4)

over

F

l n

and

get

P(X l n

|

X l-1 ,

U ln)

=

s

P(Xsln

|

Xsl-1,

U

l n

)

with

Xsln

|

Xsl-1

,

U

l n



N (µsFnl , sFsln + (nl )2) .

(12)

2 The inverse of the covariance matrix appears in the PDF of a normal distribution. Thus, the dependency on Xl is highly non-linear and analytic calculation of the integral is not feasible.

3

Under review as a conference paper at ICLR 2018

U1 U2 U3

(a) X0

A1 F 1 X1 U1

A2 F 2 X2 U2

A3 F 3 X3 U3

(b) X0

A1 F 1 X1

A2 F 2 X2

A3 F 3 X3

Figure 2: A GPN feed-forward network prior distribution (a) for three layers and the approximation of its posterior obtained using variational inference and the central limit theorem on the activations (b). Each node corresponds to all samples and GPN units within a layer. Dotted circles represent variational parameters.

Although we now have a distribution for Xl that is conditionally normal given the values of the

bperecvaiuosuestlhaeyeinr,ptuhtefrmomargtihnealpsrePv(iXoulsnl)a,yler X{1l-, .1.a.p,pLe}a,rswnilol,ni-nlingeeanrelyratlh, rnooutgbhetnhoerkmearnlleyl

distributed, function in

the mean eq. (9).

By putting a GP prior on the distribution of the virtual observation targets U l as shown in fig. 1.,

U

l n



N

(0,

K (V

ln,

V

ln))

,

(13)

it can easily be verified that the marginal distribution of the response,

P(F

l n

|

Xl)

=

P(F

l n

|

X

l,

U

l n

)

P(U

l n

)

dU

l

n

=

N

(F

l n

|

0,

K (X

l-1W

l n

,

X l-1 W

l n

))

,

(14)

recovers the original, non-parametric GPN response distribution given by (3). The first use of this prior-restoring technique was presented in Titsias (2009) for finding inducing points of sparse GP regression using variational methods.

3.2 APPROXIMATE BAYESIAN INFERENCE FOR GPN NETWORKS

The joint distribution of a GPN feed-forward network is given by

L
P({X}1L, {A}L1 , {U }L1 , {F }L1 | X0) = P(Al | Xl-1) P(U l) P(F l | Al, U l) P(Xl | F l) , (15)
l=1
where to notation {·}L1 should be read as {·1, ·2, . . . , ·L}. A graphical model corresponding to that distribution for three layers is shown in fig. 2a. Since exact marginalization over the latent variables is infeasible we apply the technique of variational inference Wainwright et al. (2008) to approximate the posterior of the model given some training data by a variational distribution Q.

The information about the activation functions learned from the training data is mediated via the virtual observation targets U l, thus their variational posterior must be adaptable in order to store that
information. Hence, we choose a normal distribution factorized over the GPN units within a layer with free mean and covariance for the approximative posterior of U l,

Nl
Q(U l) = Q(U ln) ,
n=1

Q(U

l n

)

=

N

(U

l n

|

µUnl ,

U

l
n

)

.

(16)

This allows the inducing targets of a GPN to be correlated, but the covariance matrix can be constrained to be diagonal, if it is desired to reduce the number of variational parameters. We keep the

4

Under review as a conference paper at ICLR 2018

rest of the model distribution unchanged from the prior; thus the overall approximating posterior is

L
Q({U }1L, {X}1L-1, {A}1L, {F }1L) = P(Al | Xl-1) Q(U l) P(F l | Al, U l) P(Xl | F l) .
l=1

(17)

Estimating the variational parameters µUl and Ul requires maximizing the evidence lower bound (ELBO) given by

L=-

···

Q({U }1L,

{X}L1 -1,

{A}1L,

{F

}1L)

log

Q({U }L1 , {X}1L-1, {A}1L, {F }1L) P({U }L1 , {X}L1 , {A}1L, {F }L1 | X0)

·

d{U }1L d{X}1L d{A}1L d{F }L1 .

(18)

Substituting the distributions into this equation results in L = -Lreg + Lpred with

L
Lreg =
l=1

Q(U l)

log

Q(U l) P(U l)

dU l

,

(19)

Lpred = Q(F L) log P(XL | F L) dF L .

(20)

The term Lreg can be identified as the sum of the KL-divergences between the GP prior on the virtual observation targets and their approximative posterior Q(U l). Since this term enters L with a negative sign, its purpose is to keep the approximative posterior close to the prior; thus it can be understood as a regularization term. Evaluating using the formula for the KL-divergence between two normal distributions gives

L L Nl

Lreg = KL Q(U l) || P(U l) =

KL

Q(U

l n

)

||

P(U

l n

)

(21)

l=1 l=1 n=1



1 L 2

Nl

tr

K

(V

ln,

V

ln)-1

U

l
n

l=1 n=1



+

(µUnl )T

K (V

ln,

V

l n

)-1

µUnl

+

log

K(V ln, V ln)  .

U

l
n

The term Lpred cannot be evaluated yet because the exact marginal Q(F L) is still intractable.

3.2.1 CENTRAL LIMIT DISTRIBUTION OF ACTIVATIONS

Due to the central limit theorem the activation Al (weighted sum of inputs) of each GPN will converge to a normal distribution, if the number of incoming connections is sufficiently large ( 50) and the weights W l have a sufficiently random distribution. For standard feed forward neural networks
Wang & Manning (2013) experimentally showed that even after training the weights are sufficiently
random for this assumption to hold. Hence we postulate that the same is true for GPNs and assume that the marginal distributions Q(Al), l  {1, . . . , L}, can be written as

S
Q(Al) = Q(Als ) ,

Q(Als )  N Asl µsAl , sAl .

(22)

s=1

A graphical model corresponding to this approximate posterior is shown in fig. 2b. This allows the moments of Q(Al) to be calculated exactly and propagated analytically from layer to layer. For this

purpose we need to evaluate the conditional distributions

Q(Xl | Xl-1) = Q(F l | Al = W lXl-1) P(Xl | F l) dF l ,

(23)

Nl
Q(F l | Al) =
n=1

Q(U

l

n)

P(F

l n

|

Al

n

,

U

l

n)

dU

l n

.

(24)

Since Q(F l | Al) is the conditional of a GP with normally distributed observations, the joint distri-

bution

Q(F

l n

,

U

l n

|

Al

n)

=

Q(U

l n

)

P(F

l n

|

Al

n,

U

ln)

must

itself

be

normal,

Q(F

ln,

U

l n

|

Al

n)

=

N

F

l n

U

l n

µFnl µUnl

,

F

l
n

U F

F U

U

l
n

,

(25)

5

Under review as a conference paper at ICLR 2018

and

we

can

find

the

values

for

the

unknown

parameters

µFnl ,

F

l
n

(and

F U

=

TUF ) by

equating

the

moments

of

its

conditional

distribution

Q(F

l n

|

U

l n

,

Al

n)

with

P(F

l n

| U ln, Al n).

Thus by

solving the resulting equations we obtain for eq. (24),

Nl

Q(F l | Al) =

N

(F

l n

|

µFnl ,

F

l
n)

.

n=1

(26)

where

µFnl

=

K(Al n,

V

l n

)

K (V

l n

,

V

l n

)-1

µUnl

(27)

F

l
n

=

K(Al n,

Al n)

-

K(Al n,

V

ln)

K

U

l
n

K (V

ln,

Al n)

(28)

with

K

U

l
n

K (V

ln,

V

l n

)-1

-

K (V

l n

,

V

ln)-1

U

l
n

K (V

l n

,

V

ln)-1

.

(29)

For deterministic

observations,

that

is

U

l
n

=

0,

we

obtain

K

U

l
n

=

K (V

l n

,

V

ln)-1

and thus

recover the standard GP regression distribution as expected. If U l follows its prior, that is µUnl = 0

and

U

l
n

=

K

(V

ln,

V

l n

),

we

obtain

K

U

l
n

=

0

and

thus

recover

the

GP

prior

on

F l.

In

that

case

the virtual observations behave as if they were not present.

Having Q(F l | Al) immediately allows us to evaluate eq. (23) since P(Xl | F l) just provides additive Gaussian noise; thus we obtain

Nl

Q(Xl | Xl-1) =

N

(F

l n

|

µFnl ,

F

l
n

+

(nl )21)

.

n=1

Returning to Lpred from (20) and writing it as

(30)

Lpred =

Q(AL) Q(U L) P(F L | AL, U L) log P(XL | F L) dAL dU L dF L . (31)

shows that we first need to obtain the distribution Q(AL). This is done by iteratively calculating the marginals Q(Al) for l  {1, . . . , L}.

3.2.2 PROPAGATION OF UNCERTAINTY

For l  1 the marginal distribution of the activations is

Q(Al+1) =

Q(Al) Q(F l | Al) P(Xl | F l) P(Al+1 | Xl) dAl dF l dXl

(32)

where Q(F l | Al) is given by (26). We first evaluate the mean and covariance of the marginal Q(F l) = Q(Al) Q(F l | Al) dAl. For the marginal mean of the response µFsnl EQ(F l) Fsln =
EQ(Asl ) µFsnl we obtain

µFsnl

= EQ(Als

)

K(Asl n, V ln)

T

K

(V

ln,

V

l n

)-1

µUnl

= (sl

n)T

K

(V

ln,

V

l n

)-1

µUnl

with

(33)

sl tn

EQ(Asl n)[K(Asl n, Vtln)] =

2 2 + Asnln exp

-

(µAsnl 2(2

- +

Vtln)2 Asnln)

,

which was calculated by expressing the squared exponential kernel as a normal PDF and applying
the product formula for Gaussian PDFs (Bromiley, 2003). For the marginal covariances of the response Fs l we obtain by applying the law of total expectation

Fsnln

Cov(Fsln, Fsln ) = EQ(Asl

)

EQ(Fsln

,F l
sn

| Al)[Fsln Fsln ]

- µFsnl µFsnl

(34)

For the elements representing the variance, i.e. the diagonal n = n , this becomes

sFnln = EQ(Als ) sFsln + (µFsnl )2 - (µFsnl )2

= 1 - Tr

[K

U

l
n

-

l

n(l n)T

]

ls

n

- Tr sl n(sl n)T l n(l n)T

(35)

6

Under review as a conference paper at ICLR 2018

with l n

K (V

ln,

V

l n

)-1

µUnl

and,

using

the

same

method

as

above,

sl trn EQ(Asl )[K(Alsn, Vtln) K(Asl n, Vrln)]

=



2

2 + 2sAnln

exp- 

µ -Al Vtln+Vrln
sn 2
2 + 2Asnln

2

-

(Vtln - Vrln)2  42 

.

(36)

For off-diagonal elements, n = n , we observe that Fsln and Fsln are conditionally independent given Al because the activation functions of GPNs n and n are represented by two different GPs.

Hence we have

sFnln = EQ(Asl ) µsFnl µsFnl - µFsnl µsFnl = (l n)T sl nn l n - µsFnl µFsnl .

(37)

where

lsrtnn

EQ(Als ) K(Asl n, Vtln) K(Alsn , Vrln ) =

2 exp(Aslrtnn /Bsl nn ) [2 + Asnln] [2 + Asnl n ] - (nAnl )2

with

Aslrtnn Bsl nn

(Vrln - µsAnl )2 [2 + sAnln] + (Vtln - µAsnl )2 [2 + sAnl n ] + 2 (Vtln - µsAnl ) (µAsnl - Vrln ) Asnln 2 [2 + sAnln] [2 + sAnl n ] - (Asnl n)2 .

This concludes the calculation of the moments of F l. We can now state how the activation distribution propagates from a layer to the next. The marginal distribution of Al+1 is given by

Q(Al+1)

=

N

(Als+1

|

µAl+1
s

,

Al+1
s

)

(38)

with

µAl+1
s

=

W l+1 µFs l

(39)

Al+1
s

=

W l+1

Fs l + diag(l)2

(W l+1)T .

(40)

Thus Q(AL) in (31) can be calculated by iterating the application of eqs. (33), (35), (37), (39)
and (40) over the layers l. To save computational power only the variances can be propagated by assuming that Fs l is diagonal and therefore ignoring (37).

Now Lpred can be identified as the expected log-probability of the observations under the marginal distribution Q(F L) and thus we can expand it as follows,

Lpred = EQ(F L) log P(XL | F L)

 EQ(F L)

-S

Nl

log nL -

1 2

S

NL

n=1

s=1 n=1

XsLn - FsLn 2 (nL)2

=

-S

NL n=1

log

nL

-

1 2

S s=1

NL n=1

(XsLn)2

-

2

XsLn

EQ(F L) FsLn (nL)2

+ EQ(F L) (FsLn)2

(41)

where S is the number of training samples and XL are the training targets. The distribution Q(F L)

is of arbitrary form, but only its first and second moments are required to evaluate Lpred. For the first moment we obtain EQ(F L) Fsln = µFsnL with µsFnL given by (33) and the second moment evaluates to
EQ(F L) (FsLn)2 = VarQ(F L) FsLn + EQ(F L) FsLn 2 = sFnLn + µFsnLn 2

= 1 - tr

K

U

L
n

-



L n

(Ln)T

Ls n ,

(42)

with



L n

K (V

Ln,

V

L n

)-1

µUnL

and

L

from

(36).

This concludes the calculation of all terms of the variational lower bound (18). The resulting ob-
jective is a fully deterministic function of the parameters. Training of the model is performed by maximizing L w.r.t. to the variational parameters µUl , Ul and the model parameters l, W l and V l. This can be performed using any gradient-descent based algorithm. The necessary derivatives
are not derived here and it is assumed that this can be performed automatically using symbolic or
automatic differentiation in an appropriate framework.

7

Under review as a conference paper at ICLR 2018
3.3 COMPUTATIONAL AND MODEL COMPLEXITY
The activation function are represented using 2R variational parameters per GPN, where R is the number of inducing points and targets. It can be shown that R = 10 linearly spaced inducing points are enough to represent the most commonly used activation functions (sigmoid, tanh, soft ReLU) with very high accuracy. The number of required parameters can be reduced by sharing the same activation function within groups of neurons or even across whole layers of neurons. If the inducing points V l are fixed (for example by equally distributing them in the interval [-1, 1]), the kernel matrices K(V l, V l) and their inverses can be precomputed since they are constant. The number of parameters and the computational complexity of propagating the means and covariances only depend on R and are therefore independent of the number of training samples. Thus, like a conventional neural network, a GPN network can inherently be trained on datasets of unlimited size.
4 CONCLUSION
We have presented a non-parametric model based on GPs for learning of activation functions in a multi-layer neural network. We then successively applied variational to make fully Bayesian inference feasible and efficient while keeping its probabilistic nature and providing not only best guess predictions but also confidence estimations in our predictions. Although we employ GPs, our parametric approximation allows our model to scale to datasets of unlimited size like conventional neural networks do.
We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication. In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters.
REFERENCES
Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014a.
Forest Agostinelli, Matthew D. Hoffman, Peter J. Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. CoRR, abs/1412.6830, 2014b.
Paul Bromiley. Products and convolutions of Gaussian probability density functions. Tina-Vision Memo, 3, 2003.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Carson Eisenach, Zhaoran Wang, and Han Liu. Nonparametrically learning activation functions in deep neural nets. 2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.
I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout Networks. ArXiv e-prints, February 2013.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Gu¨nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. arXiv preprint arXiv:1706.02515, 2017.
8

Under review as a conference paper at ICLR 2018
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. Curran Associates, Inc., 2012.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML, volume 30, 2013.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10, pp. 807­814, USA, 2010. Omnipress. ISBN 978-1-60558-907-7.
Joaquin Quin~onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939­1959, 2005.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.
Michalis K Titsias. Variational learning of inducing variables in sparse gaussian processes. In International Conference on Artificial Intelligence and Statistics, pp. 567­574, 2009.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1­2):1­305, 2008.
Sida I Wang and Christopher D Manning. Fast dropout training. In ICML (2), pp. 118­126, 2013. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853, 2015.
9

