Under review as a conference paper at ICLR 2018
EMPIRICAL RISK LANDSCAPE ANALYSIS FOR UNDERSTANDING DEEP NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
This work aims to provide comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an l-layer linear neural network consisting of di neurons in the i-th layer, we prove the gradient of its empiricalrisk uniformly converges to the one of its population risk, at the rate of O(r2l l maxi dis log(d/l)/n). Here d is the total weight dimension, s is the number of nonzero entries of all the weights and the magnitude of weights per layer is upper bounded by r. Moreover, we prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks and provide convergence guarantee for each pair. We also establish the uniform convergence of the empirical risk to its population counterpart and further derive the stability and generalization bounds for the empirical risk. In addition, we analyze these properties for deep nonlinear neural networks with sigmoid activation functions. We prove similar results for convergence behavior of their empirical risk gradients, non-degenerate stationary points as well as the empirical risk itself. To our best knowledge, this work is the first one theoretically characterizing the uniform convergence of the gradient and stationary points of the empirical risk of DNN models, which benefits the theoretical understanding on how the neural network depth l, the layer width di, the network size d, the sparsity in weight and the parameter magnitude r determine the neural network landscape.
1 INTRODUCTION
Deep learning has achieved remarkable success in many fields, such as computer vision (Hinton et al., 2006; Szegedy et al., 2015; He et al., 2016), natural language processing (Collobert & Weston, 2008; Bakshi & Stephanopoulos, 1993), and speech recognition (Hinton et al., 2012; Graves et al., 2013). However, theoretical understanding on the properties of deep learning models still lags behind their practical achievements (Shalev-Shwartz et al., 2017; Kawaguchi, 2016) due to their high non-convexity and internal complexity. In practice, parameters of deep learning models are learned by minimizing the empirical risk via (stochastic-)gradient descent. Therefore, some recent works (Bartlett & Maass, 2003; Neyshabur et al., 2015) analyzed the convergence of the empirical risk to the population risk, which are however still far from fully understanding the landscape of the empirical risk in deep learning models. Beyond the convergence properties of the empirical risk itself, the convergence and distribution properties of its gradient and stationary points are also essential in landscape analysis. A comprehensive landscape analysis can reveal important information on the optimization behavior and practical performance of deep neural networks, and will be helpful to designing better network architectures. Thus, in this work we aim to provide comprehensive landscape analysis by looking into the gradients and stationary points of the empirical risk. Formally, we consider a DNN model f (w; x, y) : Rd0 × Rdl  R parameterized by w  Rd consisting of l layers (l  2) that is trained by minimizing the commonly used squared loss function over sample pairs {(x, y)}  Rd0 × Rdl from an unknown distribution D, where y is the target output for the sample x. Ideally, the model can find its optimal parameter w by minimizing the
1

Under review as a conference paper at ICLR 2018

population risk through (stochastic-)gradient descent by backpropagation:

min J (w)
w

E(x,y)D f (w; x, y),

where

f (w; x, y)

=

1 2

v(l) - y

2 2

is

the

squared

loss

associated

to

the

sample

(x,

y)



D

in

which

v(l) is the output of the l-th layer. In practice, as the sample distribution D is usually unknown and

only finite training samples

(x(i), y(i))

n i=1

i.i.d.

drawn

from

D

are

provided,

the

network

model

is

usually trained by minimizing the empirical risk:

min
w

J^n(w)

1n n f (w; x(i), y(i)).
i=1

(1)

Understanding the convergence behavior of J^n(w) to J (w) is critical to statistical machine learning algorithms. In this work, we aim to go further and characterize the landscape of the empirical risk J^n(w) of deep learning models by analyzing the convergence behavior of its gradient and stationary points to their corresponding population counterparts. We provide analysis for both multi-layer linear
and nonlinear neural networks. In particular, we obtain following new results.

· We establish the uniform convergence of empirical gradient wJ^n(w) to its population counterpart wJ (w). Specifically, when the sample size n is not less than O max(l3r2/(2s log(d/l)), s log(d/l)/l) , with probability at least 1 -  the convergence rate is O(r2l l maxi dis log(d/l)/n), where there are s nonzero entries in the parameter w, the output dimension of the i-th layer is di and the magnitude of the weight parameter of each layer is upper bounded by r. This result implies that as long as the training sample size n is sufficiently large, any stationary point of J^n(w) is also a stationary point of J (w) and vise versa, although both J^n(w) and J (w) are very complex.
· We then prove the exact correspondence of non-degenerate stationary points between J^n(w) and J (w). Indeed, the corresponding non-degenerate stationary points also uniformly
converge to each other at the same convergence rate as the one revealed above with an extra
factor 2/. Here  > 0 accounts for the geometric topology of non-degenerate stationary
points (see Definition 1).

Based on the above two new results, we also derive the uniform convergence of the empirical risk J^n(w) to its population risk J (w), which helps understand the generalization error of deep learning models and stability of their empirical risk. These analyses reveal the role of the depth l of a neural
network model in determining its convergence behavior and performance. Also, the results tell that the width factor maxi di, the nonzero entry number s of weights, and the total network size d are also critical to the convergence and performance. In addition, controlling magnitudes of the parameters
(weights) in DNNs are demonstrated to be important for performance. To our best knowledge, this
work is the first one theoretically characterizing the uniform convergence of empirical gradient and
stationary points in both deep linear and nonlinear neural networks.

2 RELATED WORK
To date, only a few theories have been developed for understanding DNNs which can be roughly divided into following three categories. The first category aims to analyze training error of DNNs. Baum (1988) pointed out that zero training error can be obtained when the last layer of a neural network has more units than training samples. Later, Soudry & Carmon (2016) proved that for DNNs with leaky rectified linear units (ReLU) and a single output, the training error achieves zero at any of their local minima as long as the product of the number of units in the last two layers is larger than the training sample size.
The second category of analysis works (Dauphin et al., 2014; Choromanska et al., 2015a; Kawaguchi, 2016; Tian, 2017) focus on analyzing loss surfaces of DNNs, e.g., how the stationary points are distributed. Those results are helpful to understanding performance difference of large- and small-size networks (Choromanska et al., 2015b). Among them, Dauphin et al. (2014) experimentally verified that a large number of saddle points indeed exist for DNNs. With strong assumptions, Choromanska

2

Under review as a conference paper at ICLR 2018
et al. (2015a) connected the loss function of a deep ReLU network with the spherical spin-class model and described locations of the local minima. Later, Kawaguchi (2016) proved the existence of degenerate saddle points for deep linear neural networks with squared loss function. They also showed that any local minimum is also a global minimum. By utilizing techniques from dynamical system analysis, Tian (2017) gave guarantees that for two-layer bias-free networks with ReLUs, the gradient descent algorithm with certain symmetric weight initialization can converge to the ground-truth weights globally, if the inputs follow Gaussian distribution. Recently, Nguyen & Hein (2017) proved that for a fully connected network with squared loss and analytic activation functions, almost all the local minima are globally optimal if one hidden layer has more units than training samples and the network structure after this layer is pyramidal. Besides, some recent works, e.g., (Zhang et al., 2016; 2017), tried to alleviate analysis difficulties by relaxing the involved highly nonconvex functions into ones easier.
In addition, some existing works (Bartlett & Maass, 2003; Neyshabur et al., 2015) analyze the generalization performance of a DNN model. Based on the Vapnik­Chervonenkis (VC) theory, Bartlett & Maass (2003) proved that for a feedforward neural network with one-dimensional output, the best convergence rate of the empirical risk to its population risk on the sample distribution can be bounded by its fat-shattering dimension. Recently, Neyshabur et al. (2015) adopted Rademacher complexity to analyze learning capacity of a fully-connected neural network model with ReLU activation functions and bounded inputs.
However, although gradient descent with backpropagation is the most common optimization technique for DNNs, none of existing works analyzes convergence properties of gradient and stationary points of the DNN empirical risk. For single-layer optimization problems, some previous works analyze their empirical risk but essentially differ from our analysis method. For example, Negahban et al. (2009) proved that for a regularized convex program, the minimum of the empirical risk uniformly converges to the true minimum of the population risk under certain conditions. Gonen & ShalevShwartz (2017) proved that for nonconvex problems without degenerated saddle points, the difference between empirical risk and population risk can be bounded. Unfortunately, the loss of DNNs is highly nonconvex and has degenerated saddle points (Fyodorov & Williams, 2007; Dauphin et al., 2014; Kawaguchi, 2016), thus their analysis results are not applicable. Mei et al. (2017) analyzed the convergence behavior of the empirical risk for nonconvex problems, but they only considered the single-layer nonconvex problems and their analysis demands strong sub-Gaussian and subexponential assumptions on the gradient and Hessian of the empirical risk respectively. Their analysis also assumes a linearity property on gradient which is difficult to hold or verify. In contrast, our analysis requires much milder assumptions. Besides, we prove that for deep networks which are highly nonconvex, the non-degenerate stationary points of empirical risk can uniformly converge to their corresponding stationary points of population risk at the rate of O( s/n) which is faster than the rate O( d/n) for single-layer optimization problems in (Mei et al., 2017). Also, Mei et al. (2017) did not analyze the convergence rate of the empirical risk, stability or generalization error of DNNs as this work.
3 PRELIMINARIES
Throughout the paper, we denote matrices by boldface capital letters, e.g. A. Vectors are denoted by boldface lowercase letters, e.g. a, and scalars are denoted by lowercase letters, e.g. a. We define the r-radius ball as Bd(r) {z  Rd | z 2  r}. To explain the results, we also need the vectorization operation vec(·). It is defined as vec(A) = (A(:, 1); · · · ; A(:, t))  Rst that vectorizes A  Rs×t along its columns. We use d = lj=1djdj-1 to denote the total dimension of weight parameters, where dj denotes the output dimension of the j-th layer.
In this work, we consider both linear and nonlinear DNNs. Suppose both networks consist of l layers. We use u(j) and v(j) to respectively denote the input and output of the j-th layer, j = 1, . . . , l.
Deep linear neural networks: The function of the j-th layer is formulated as
u(j) W (j)v(j-1)  Rdj , v(j) u(j)  Rdj , j = 1, · · · , l,
where v(0) = x is the input and W (j)  Rdj×dj-1 is the weight matrix of the j-th layer.
3

Under review as a conference paper at ICLR 2018

Deep nonlinear neural networks: We adopt the sigmoid function as the non-linear activation function. The function within the j-th layer can be written as

u(j) W (j)v(j-1)  Rdj , v(j) hj (u(j)) = ((u(1j)); · · · ; (u(djj)))  Rdj , j = 1, · · · , l,

where u(ij) denotes the i-th entry of u(j) and (·) is the sigmoid function, i.e., (a) = 1/(1 + e-a).

Following the common practice, both DNN models adopt the squared loss function defined as

f (w; x, y)

=

1 2

v(l) - y

22, where w

=

(w(1); · · · ; w(l))



Rd contains all the weight pa-

rameters and w(j) = vec W (j)  Rdjdj-1 . Then the empirical risk J^n(w) is J^n(w) =

1 n

n i=1

f

(w;

x(i),

y(i)

)

=

1 2n

n i=1

v((il)) - y(i)

22, where v((il)) is the network's output of x(i).

4 RESULTS FOR DEEP LINEAR NEURAL NETWORKS

We first analyze linear neural network models and present following new results: (1) the uniform convergence of the empirical risk gradient to its population counterpart and (2) the convergence properties of non-degenerate stationary points of the empirical risk. As a corollary, we also derive the uniform convergence of the empirical risk to the population one, which further gives stability and generalization bounds. In the next section, we extend the analysis to non-linear neural network models.
We assume the input datum x is  2-sub-Gaussian and has bounded magnitude, as formally stated in Assumption 1.
Assumption 1. The input datum x  Rd0 has zero mean and is  2-sub-Gaussian, i.e.,

E[exp ( , x )]  exp

12 2



2 2

,   Rd0 .

Besides, the magnitude x is bounded as x 2  rx, where rx is a positive universal constant.

Note that any random vector z consisting of independent entries with bounded magnitude is subGaussian and satisfies Assumption 1 (Vershynin, 2012). Moreover, for such a random z, we have  = z   z 2  rx. Such an assumption on bounded magnitude generally holds for natural data, e.g., images and speech signals. Besides, we assume the weight parameters w(j) of each layer are bounded as w   = {w | w(j)  Bdjdj-1 (rj), j = 1, · · · , l} where rj is a constant. For notational simplicity, we let r = maxj rj. Such an assumption is common (Xu & Mannor, 2012). Here we assume the entry value of y falls in [0, 1]. For any bounded target output y, we can always scale it to satisfy such a requirement.
The results presented for linear neural networks here can be generalized to deep ReLU neural networks by applying the results from Choromanska et al. (2015a) and Kawaguchi (2016), which transform deep ReLU neural networks into deep linear neural networks under proper assumptions.

4.1 UNIFORM CONVERGENCE OF EMPIRICAL RISK GRADIENT

We first analyze the convergence of gradients for the DNN empirical and population risks. To our best knowledge, these results are the first ones giving guarantees on gradient convergence, which help better understand the landscape of DNNs and their optimization behavior. The results are stated blow.

Theorem 1. Suppose Assumption 1 on the input datum x holds and the activation functions in a

deep neural network are linear. Then the empirical gradient uniformly converges to the population

gradient in Euclidean norm. Specifically, there exist two universal constants cg and cg such that
if n  cg max(l3r2rx4/(cqs log(d/l)2 4 log(1/)), s log(d/l)/(l 2)) where cq = max0il di, then

sup J^n(w) - J (w)  g

w

2

cg g lcq

s log(dn/l) + log(12/) n

holds with probability at least 1 - , where s denotes the number of nonzero entries of all weight parameters and g = max  r2l-1, r2l-1, rl-1 .

4

Under review as a conference paper at ICLR 2018

From Theorem 1, one can observe that with an increasingly larger sample size n, the difference between empirical risk and population risk gradients decreases monotonically at the rate of O(1/ n) (up to a log factor). Theorem 1 also characterizes how the depth l contributes to obtaining small difference between the empirical and population risk gradients. Specifically, a deeper neural network needs more training samples to mitigate the difference. Also, due to the factor d, training a network of larger size using gradient descent also requires more training samples. We observe a factor of
maxi di (i.e. cq), which prefers a DNN architecture of balanced layer sizes (without extremely wide layers). This result also matches the trend and empirical performance in deep learning applications advocating deep but thin networks (He et al., 2016; Szegedy et al., 2015).
By observing Theorem 1, imposing certain regularizations on the weight parameters is useful. For example, reducing the number of nonzero entries s encourages sparsity regularization like w 1. The results also suggest not choosing large-magnitude weights w in order for a smaller factor r by adopting regularization like w 22.
Theorem 1 also reveals the point derived from optimizing that the empirical and population risks have similar properties when the sample size n is sufficiently large. For example, an /2-stationary point w~ of J^n(w) is also an -stationary point of J (w) with probability 1 -  if n  c ( g/ )2lcqs log(d/l) with c being a constant. Here -stationary point for a function F means the point w satisfying
wF 2  . Understanding such properties is useful, since in practice one usually computes an -stationary point of J^n(w). These results guarantee the computed point is at most a 2 -stationary point of J (w) and is thus close to the optimum.

4.2 UNIFORM CONVERGENCE OF STATIONARY POINTS

We then proceed to analyze the distribution and convergence properties of stationary points of the DNN empirical risk. Here we consider non-degenerate stationary points which are geometrically isolated and thus unique in local regions. Since degenerate stationary points are not unique in a local region, we cannot expect to establish one-to-one corresponding relationship (see below) between them in empirical risk and population risk.
Definition 1. (Non-degenerate stationary points) (Gromoll & Meyer, 1969) If a stationary point w is said to be a non-degenerate stationary point of J (w), then it satisfies
inf i 2J (w)  ,
i
where i 2J (w) denotes the i-th eigenvalue of the Hessian 2J (w) and  is a positive constant.

Non-degenerate stationary points include local minima/maxima and non-degenerate saddle points, while degenerate stationary points refer to degenerate saddle points. Then we introduce the index of non-degenerate stationary points which can characterize their geometric properties.
Definition 2. (Index of non-degenerate stationary points) (Dubrovin et al., 2012) The index of a symmetric non-degenerate matrix is the number of its negative eigenvalues, and the index of a non-degenerate stationary point w of a smooth function F is simply the index of its Hessian 2F (w).

Suppose that J (w) has m non-degenerate stationary points that are denoted as {w(1), w(2), · · · , w(m)}. We prove following convergence behavior of these stationary points.
Theorem 2. Suppose Assumption 1 on the input datum x holds and the activation functions in a deep neural network are linear. Then if n  ch max(l3r2rx4/(cqs log(d/l)2 4 log(1/)), s log(d/l)/2) where ch is a constant, for k  {1, · · · , m}, there exists a non-degenerate stationary point wn(k) of J^n(w) which corresponds to the non-degenerate stationary point w(k) of J (w) with probability at least 1 - . In addition, wn(k) and w(k) have the same non-degenerate index and they satisfy

wn(k) - w(k)

2

2cg g 

lcq

s log(dn/l) + log(12/) ,
n

(k = 1, · · · , m)

with probability at least 1 - , where the parameters cq, g, and cg are given in Theorem 1.

Theorem 2 guarantees the one-to-one correspondence between the non-degenerate stationary points of the empirical risk J^n(w) and the popular risk J (w). The distances of the corresponding pairs

5

Under review as a conference paper at ICLR 2018

become smaller as n increases. In addition, the corresponding pairs have the same non-degenerate index. This implies that the corresponding stationary points have the same geometric properties, such as whether they are saddle points. Accordingly, we can develop more efficient algorithms, e.g. escaping saddle points (Ge et al., 2015), since Dauphin et al. (2014) empirically proved that saddle points are usually surrounded by high error plateaus. Also when n is sufficiently large, the properties of stationary points of J^n(w) are similar to the points of the population risk J (w) in the sense that they have exactly matching local minima/maxima and non-degenerate saddle points. By comparing Theorems 1 and 2, we find that the requirement for sample number in Theorem 2 is more restrict, since establishing exact one-to-one correspondence between the non-degenerate stationary points of J^n(w) and J (w) and bounding their uniform convergence rate to each other are more challenging. From Theorems 1 and 2, we also notice that the uniform convergence rate of non-degenerate stationary points has an extra factor 1/. This is because bounding stationary points needs to access not only the gradient itself but also the Hessian matrix. See more details in proof.
Kawaguchi (2016) pointed out that degenerate stationary points indeed exist for DNNs. However, since degenerate stationary points are not isolated, such as forming flat regions, it is hard to establish the unique correspondence for them as for non-degenerate ones. Fortunately, by Theorem 1, the gradients at these points of J^n(w) and J (w) are close. This implies that a degenerate stationary point of J (w) will also give a near-zero gradient for J^n(w), i.e., it is also a stationary point for J^n(w).

4.3 UNIFORM CONVERGENCE, STABILITY AND GENERALIZATION OF EMPIRICAL RISK

Based on the above results, we can derive the uniform convergence of empirical risk to population risk easily. In this subsection, we first give the uniform convergence rate of empirical risk for deep linear neural networks in Theorem 3, and then use this result to derive the stability and generalization bounds for DNNs in Corollary 1.
Theorem 3. Suppose Assumption 1 on the input datum x holds and the activation functions in a deep neural network are linear. Then there exist two universal constants cf and cf such that if n  cf max(l3rx4/(dls log(d/l)2 4 log(1/)), s log(d/l)/( 2dl)), then

sup J^n(w) - J (w)  f cf  max
w

dl r2l, rl

s log(dn/l) + log(8/) n

(2)

holds with probability at least 1 - . Here l is the number of layers in the neural network, n is the sample size and dl is the dimension of the final layer.

From Theorem 3, when n  +, we have |J^n(w) - J (w)|  0. According to the definition of uniform convergence (Vapnik & Vapnik, 1998; Shalev-Shwartz et al., 2010), under the distribution D, the empiricalrisk of a deep linear neural network converges to its population risk uniformly at the rate of O(1/ n). Theorem 3 also explains the roles of the depth l, the network size d, and the
number of nonzero weight parameters s in a DNN model.

Based on VC-dimension techniques, Bartlett & Maass (2003) proved that for a feedforward neural network with polynomial activation functions and one-dimensional output, with probability at least

1 -  the convergence bound satisfies |J^n(w) - inff J (w)|  O( ( log2(n) + log(1/))/n).

Here  is the shattered parameter and can be as large as the VC-dimension of the network model, i.e.

at the order of O(ld log(d)+l2d) (Bartlett & Maass, 2003). Note that Bartlett & Maass (2003) did not

reveal the role of the magnitude of weight in their results. In contrast, our uniform convergence bound

is supw |J^n(w)-J (w)|  O( (s log(dn/l) + log(1/))/n). So our convergence rate is tighter.

Neyshabur et al. (2015) proved that the model with ReLU activation functions

Rademacher complexity of a and one-dimensional output

fisulOly-croln/nencted(sneeeuCraolrnoelltawroyr2k

in (Neyshabur et al., 2015)). Then by Shwartz & Ben-David, 2014a), we have

applying Rademacher complexity | supf (J^n(w) - J (w))|  O((rl

based argument(Shalev+ log(1/))/ n) with

probability at least 1 -  where the loss function is the training error g = 1(v(l)=y) in which v(l) is

the output is O(r2l

of (s

the l-th layer in the network model f (w; x, y). The convergence rate in log(d/l) + log(1/))/n) and has the same convergence speed O(1/ n)

our theorem w.r.t. sample

6

Under review as a conference paper at ICLR 2018

number n. Note that our convergence rate involves r2l since we use squared loss instead of the training error in (Neyshabur et al., 2015). The extra parameters s and d are involved since we consider the parameter space rather than the function hypothesis f in (Neyshabur et al., 2015), which helps people more transparently understand the roles of the network parameters. Besides, the Rademacher complexity cannot be applied to analyzing convergence properties of the empirical risk gradient and stationary points as our techniques.

Based on Theorem 3, we proceed to analyze the stability property of the empirical risk and the
convergence rate of the generalization error in expectation. Let S = {(x(1), y(1)), · · · , (x(n), y(n))} denote the sample set in which the samples are i.i.d. drawn from D. When the optimal solution wn to problem (1) is computed by deterministic algorithms, the generalization error is defined as g = J^n(wn) - J (wn). But one usually employs randomized algorithms, e.g. stochastic gradient descent (SGD), for computing wn. In this case, stability and generalization error in expectation
defined in Definition 3 are more applicable.

Definition 3. (Stability and generalization in expectation) (Vapnik & Vapnik, 1998; Shalev-Shwartz

et al., 2010; Gonen & Shalev-Shwartz, 2017) Assume a randomized algorithm A is employed,

((x(1), y(1)), · · · , (x(n), y(n)))  D and wn = argminw J^n(w) is the empirical risk minimizer

(ERM).

For

every

j



[n],

suppose

wj

=

argminw

1 n-1

i=j fi(w; x(i), y(i)). We say that the

ERM is on average stable with stability rate k under distribution D if ESD,A,(x(j),y(j))D

1 n

n j=1

fj (wj ; x(j), y(j)) - fj (wn; x(j), y(j))

 k. The ERM is said to have generalization

error with convergence rate k under distribution D if we have ESD,A J (wn) - J^n(wn)  k.

Stability measures the sensibility of the empirical risk to the input and generalization error measures the effectiveness of ERM on new data. Generalization error in expectation is especially important for applying DNNs considering their internal randomness, e.g. from SGD optimization. Now we present the results on stability and generalization performance of deep linear neural networks.

Corollary 1. Suppose Assumption 1 on the input datum x holds and the activation functions in a deep neural network are linear. Then with probability at least 1 - , both the stability rate and the
generalization error rate of ERM of a deep linear neural network are at least f :

1n ESD,A,(x(j),y(j))D n

fj - fj

j=1

 f and

ESD,A J (wn) - J^n(wn)

 f,

where fj and fj respectively denote fj(wj ; x(j), y(j)) and fj(wn; x(j), y(j)), and f is defined in Eqn. (2).

According to Corollary 1, both the stability rate and the convergence rate of generalization error are O( f ). This result indicates that deep learning empirical risk is stable and its output is robust to small perturbation over the training data. When n is sufficiently large, small generalization error of DNNs
is guaranteed.

5 RESULTS FOR DEEP NONLINEAR NEURAL NETWORKS
In the above section, we analyze the empirical risk optimization landscape for deep linear neural network models. In this section, we extend our analysis to deep nonlinear neural networks which adopt the sigmoid activation function. Our analysis techniques are also applicable to other third-order differentiable activation functions, e.g., tanh function with different convergence rate. Here we assume the input data are i.i.d. Gaussian variables. Assumption 2. The input datum x is a vector of i.i.d. Gaussian variables from N (0,  2).
Since for any input, the sigmoid function always maps it to the range [0, 1]. Thus, we do not require the input x to have bounded magnitude. Such an assumption is common. For instance, Tian (2017) and Soudry & Hoffer (2017) both assumed that the entries in the input vector are from Gaussian distribution. We also assume w   as in (Xu & Mannor, 2012). Here we also assume that the entry

7

Under review as a conference paper at ICLR 2018

value of the target output y falls in [0, 1]. Similar to the analysis of deep linear neural networks, here we also aim to characterize the empirical risk gradient, stationary points and empirical risk for deep nonlinear neural networks.

5.1 UNIFORM CONVERGENCE OF GRADIENT AND STATIONARY POINTS

Here we analyze convergence properties of gradients of the empirical risk for deep nonlinear neural networks.

Theorem 4. Assume the input sample x obeys Assumption 2 and the activation functions in a deep
neural network are sigmoid functions. Then the empirical gradient uniformly converges to the
population gradient in Euclidean norm. Specifically, there are two universal constants cy and cy such that if n  cy cdl3r2/(s log(d) 22 log(1/)) where cd = max0il di, then with probability at least 1 - 

sup J^n(w) - J (w)  l

w

2



512 729 cyl(l + 2) (lcr + 1) cdcr

s log(dn/l) + log(4/) ,
n

where cr = max(r2/16, r2/16 l-1), and s denotes the nonzero entry number of all weights.

Similar to deep linear neural networks, the layer number l, width di, number of nonzero parameter entries s, network size d and magnitude of weights are all critical to the convergence rate. Also, since there is a factor maxi di in the convergence rate, it is better to avoid choosing an extremely wide layer. Interestingly, when analyzing the representation ability of deep learning, Eldan & Shamir (2016) also suggested non-extreme-wide layers, though the conclusion was derived from a different perspective. By comparing Theorems 1 and 4, one can observe that there is a factor (1/16)l-1 in the convergence rate in Theorem 4. This is because the convergence rate accesses the Lipschitz constant and when we bound it, sigmoid activation function brings the factor 1/16 for each layer.

Now we analyze the non-degenerate stationary points of the empirical risk for deep nonlinear neural networks. Here we also assume that the population risk has m non-degenerate stationary points denoted by {w(1), w(2), · · · , w(m)}.
Theorem 5. Assume the input sample x obeys Assumption 2 and the activation functions in a deep neural network are sigmoid functions. Then if n  cs max cdl3r2/(s log(d) 22 log(1/)) , s log(d/l)/2 where cs is a constant, for k  {1, · · · , m}, there exists a non-degenerate stationary point wn(k) of J^n(w) which corresponds to the non-degenerate stationary point w(k) of J (w) with probability at least 1 - . Moreover, wn(k) and w(k) have the same non-degenerate index and they obey

wn(k) - w(k)

2  2

512 729 cyl(l + 2) (lcr + 1) cdcr

s

log(dn/l)

+

log(4/) ,

(k

=

1,

·

·

·

,

m)

n

with probability at least 1 - , where cy, cd and cr are the same parameters in Theorem 4.

According to Theorem 5, there is one-to-one correspondence between the non-degenerate stationary points of J^n(w) and J (w). Also the corresponding pair has the same non-degenerate index, implying they have exactly matching local minima/maxima and non-degenerate saddle points. When n is sufficiently large, the non-degenerate stationary point wn(k) in J^n(w) is very close to its corresponding non-degenerate stationary point w(k) in J (w). As for the degenerate stationary points, Theorem 4 guarantees the gradients at these points of J (w) and J^n(w) are very close to each other.

5.2 UNIFORM CONVERGENCE, STABILITY AND GENERALIZATION OF EMPIRICAL RISK

Here we first give the uniform convergence analysis of the empirical risk and then analyze its stability and generalization.

Theorem 6. Assume the input sample x obeys Assumption 2 and the activation functions in a deep neural network are the sigmoid functions. If n  18l2r2/(s log(d) 22 log(1/)), then

sup J^n(w) - J (w)  n
w



9 8

cy

cd

(1

+

cr

(l

-

1))

s log(nd/l) + log(4/) n

holds with probability at least 1-, where cy, cd and cr are given in Theorem 4.

(3)

8

Under review as a conference paper at ICLR 2018

From Theorem 6, we obtain that under the distribution D, the empirical risk of a deep nonlinear neural network converges at the rate of O(1/ n) (up to a log factor). Theorem 6 also gives similar results as Theorem 3, including the inclination of regularization penalty on weight and suggestion on non-extreme-wide layers.

We then establish the stability property and the generalization error of the empirical risk for nonlinear neural networks. By Theorem 6, we can obtain the following results.
Corollary 2. Assume the input sample x obeys Assumption 2 and the activation functions in a deep neural network are sigmoid functions. Then with probability at least 1 - , we have

1n ESD,A,(x(j),y(j))D n

fj - fj

j=1

 n and

ESD,A J (wn) - J^n(wn)

 n,

where n is defined in Eqn. (3). The notations fj and fj here are the same in Corollary 1.

By Corollary 2, we know that both the stability convergence rate and the convergence rate of generalization error are O(1/ n). This result accords with Theorems 8 and 9 in (Shalev-Shwartz et al., 2010) which implies O(1/ n) is the bottleneck of the stability and generalization convergence rate for generic learning algorithms. From this result, we have that if n is sufficiently large, the empirical risk can be expected to be very stable. This also dispels misgivings of the random selection of training samples in practice. Such a result indicates that the deep nonlinear neural network can offer good performance on testing data if it achieves small training error.

6 PROOF ROADMAP

Here we briefly introduce our proof roadmap. Due to space limitation, all the proofs of Theorems 1  6 and Corollaries 1 and 2 as well as technical lemmas are deferred to the supplementary material.

The proofs of Theorems 1 and 4 are similar but essentially differ in some techniques

for bounding probability due to their different assumptions. For explanation simplic-

ity, we define four events: E = {supw J^n(w) - J (w) 2 > t}, E1 =

{supw

1 n

n i=1

f (w, x(i))-f (wkw ,

x(i))

2

1 n

ni=1f (wkw , x(i)) - Ef (wkw , x) 2 > t/3},

> t/3}, E2 = {supwki wNi, i[l] and E3 = {supw Ef (wkw , x)

-Ef (w, x) 2 > t/3}, where wkw = [wk1w ; wk2w ; · · · ; wkl w ] is constructed by selecting wki w 

Rdidi-1 from didi-1 /d-net Ni such that w - wkw 2  . Note that in Theorems 1 and 4, t is

respectively set to g and l. Then we have P(E)  P(E1) + P(E2) + P(E3). So we only need to

separately bound P(E1), P(E2) and P(E3). For P(E1) and P(E3), we use the gradient Lipschitz

constant and the properties of -net to prove P(E1)  /2 and P(E3) = 0, while bounding P(E2)

needs more efforts. Here based on the assumptions, we prove that P(E2) has sub-exponential tail

associated to the sample number n and the networks parameters, and it satisfies P(E2)  /2 with

proper conditions. Finally, combining the bounds of the three terms, we obtain the desired results.

To prove Theorems 2 and 5, we first prove the uniform convergence of the empirical Hessian to its population Hessian. Then, we define such a set D = {w   : J (w) 2 < and infi i 2J (w)  }. In this way, D can be decomposed into countably components, with each component containing either exactly one or zero non-degenerate stationary point. For each
component, the uniform convergence of gradient and the results in differential topology guarantee that if J (w) has no stationary points, then J^n(w) also has no stationary points and vise versa. Similarly, for each component, the uniform convergence of Hessian and the results in differential topology guarantee that if J (w) has a unique non-degenerate stationary point, then J^n(w) also has a unique non-degenerate stationary point with the same index. After establishing exact correspondence
between the non-degenerate stationary points of empirical risk and population risk, we use the
uniform convergence of gradient and Hessian to bound the distance between the corresponding pairs.

We adopt a similar strategy to prove Theorems 3 and 6. Specifically, we divide the event supw|J^n(w)-J (w)| > t into E1, E2 and E3 which have the same forms as their counterparts in the proofs of Theorem 1 with the gradient replaced by the loss function. To prove P(E1)  /2 and P(E3) = 0, we can use the Lipschitz constant of the loss function and the -net properties. The

9

Under review as a conference paper at ICLR 2018
remaining is to prove P(E2). We also prove that it has sub-exponential tail associated to the sample number n and the networks parameters and it obeys P(E2)  /2 with proper conditions. Then we utilize the uniform convergence of J^n(w) to prove the stability and generalization bounds of J^n(w) (i.e. Corollaries 1 and 2).
7 CONCLUSION
In this work, we provided theoretical analysis on the landscape of empirical risk optimization for deep linear/nonlinear neural networks with (stochastic-)gradient descent, including the properties of the gradient and stationary points of empirical risk as well as the uniform convergence, stability, and generalization of the empirical risk itself. To our best knowledge, most of the results are new to deep learning community. These results also reveal that the depth l, the nonzero entry number s of all weights, the network size d and the width of a network are critical to the convergence rates. We also prove that the weight parameter magnitude is important to the convergence rate. Indeed, small magnitude of the weights is suggested. All the results are consistent with the widely used network architectures in practice.
REFERENCES
R. Alessandro. Lecture notes of advanced statistical theory I, CMU. http://www.stat.cmu.edu/ ~arinaldo/36755/F16/Scribed_Lectures/LEC0914.pdf, 2016.
B. Bakshi and G. Stephanopoulos. Wave-net: A multiresolution, hierarchical neural network with localized learning. AIChE Journal, 39(1):57­81, 1993.
P. Bartlett and W. Maass. Vapnik-chervonenkis dimension of neural nets. The handbook of brain theory and neural networks, pp. 1188­1192, 2003.
E. Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193­215, 1988.
A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015a.
A. Choromanska, Y. LeCun, and G. Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In COLT, pp. 1756­1760, 2015b.
R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pp. 160­167, 2008.
Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, pp. 2933­2941, 2014.
B. Dubrovin, A. Fomenko, and S. Novikov. Modern geometry--methods and applications: Part II: The geometry and topology of manifolds, volume 104. Springer Science & Business Media, 2012.
R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In COLT, pp. 907­940, 2016.
Y. Fyodorov and I. Williams. Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity. Journal of Statistical Physics, 129(5-6):1081­1116, 2007.
R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In COLT, pp. 797­842, 2015.
A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. COLT, 2017.
A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, pp. 6645­6649, 2013.
D. Gromoll and W. Meyer. On differentiable functions with isolated critical points. Topology, 8(4):361­369, 1969.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pp. 770­778, 2016.
10

Under review as a conference paper at ICLR 2018
G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7): 1527­1554, 2006.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82­97, 2012.
K. Kawaguchi. Deep learning without poor local minima. In NIPS, pp. 1097­1105, 2016. S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for non-convex losses. Annals of Statistics,
2017. S. Negahban, B. Yu, M. Wainwright, and P. Ravikumar. A unified framework for high-dimensional analysis of
M-estimators with decomposable regularizers. In NIPS, pp. 1348­1356, 2009. B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In COLT, pp.
1376­1401, 2015. Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In ICML, 2017. P. Rigollet. Statistic s997 lecture notes, MIT mathematics. MIT OpenCourseWare, pp. 23­24, 2015. M. Rudelson and R. Vershynin. Hanson-wright inequality and sub-gaussian concentration. Electronic Communi-
cations in Probability, 18(82):1­9, 2013. S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge
Univ. Press, Cambridge, pp. 375­382, 2014a. S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge
university press, 2014b. S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence.
JMLR, 11:2635­2670, 2010. S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of deep learning. ICML, 2017. D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer
neural networks. arXiv preprint arXiv:1605.08361, 2016. D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks.
arXiv preprint arXiv:1702.05777, 2017. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, pp. 1­9, 2015. Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in
convergence and critical point analysis. ICML, 2017. V. N. Vapnik and V. Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998. R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, compressed sensing. Cambridge
Univ. Press, Cambridge, pp. 210­268, 2012. H. Xu and S. Mannor. Robustness and generalization. Machine Learning, 86(3):391­423, 2012. Y. Zhang, J. Lee, and M. Jordan. 1-regularized neural networks are improperly learnable in polynomial time. In
ICML, pp. 993­1001, 2016. Y. Zhang, P. Liang, and M. Wainwright. Convexified convolutional neural networks. ICML, 2017.
11

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY MATERIAL OF EMPIRICAL RISK LANDSCAPE ANALYSIS FOR UNDERSTANDING DEEP NEURAL NETWORKS
A STRUCTURE OF THIS DOCUMENT
This document gives some other necessary notations and preliminaries for our analysis in Sec. B. Then we prove Theorems 1 3 and Corollary 1 for deep linear neural networks in Sec. C. Then we present the proofs of Theorems 4  6 and Corollary 2 for deep nonlinear neural networks in Sec. D.
In both Sec. C and D, we first present the technical lemmas for proving our final results and subsequently present the proofs of these lemmas. Then we utilize these technical lemmas to prove our desired results. Finally, we give the proofs of other auxiliary lemmas.

B NOTATIONS AND PRELIMINARY TOOLS

Beyond the notations introduced in the manuscript, we need some other notations used in this document. Then we introduce several lemmas that will be used later.

B.1 NOTATIONS

Throughout this document, we use ·, · to denote the inner product. A  C denotes the Kronecker product between A and C. Note that A and C in A  C can be matrices or vectors. For a matrix

A  Rn1×n2 , we use A F =

i,j A2ij to denote its Frobenius norm, where Aij is the (i, j)-th

entry of A. We use A op = maxi |i(A)| to denote the operation norm of a matrix A  Rn1×n1 ,

where i(A) denotes the i-th eigenvalue of the matrix A. For a 3-way tensor A  Rn1×n2×n3 , its

operation norm is computed as

A op = sup 3 , A = Aijkij k,

 21

i,j,k

where Aijk denotes the (i, j, k)-th entry of A. Also we denote the vectorization of W (j) (the weight matrix of the j-th layer) as
w(j) = vec W (j)  Rdj dj-1 .

We denote Ik as the identity matrix of size k × k.

For notational simplicity, we further define e v(l) - y as the output error vector. Then the squared

loss

is

defined

as

f (w; x, y)

=

1 2

e

22, where w = (w(1); · · · ; w(l))  Rd contains all the weight

parameters.

B.2 TECHNICAL LEMMAS

We first introduce Lemmas 1 and 2 which are respectively used for bounding the 2-norm of a vector and the operation norm of a matrix. Then we introduce Lemmas 3 and 4 which discuss the topology of functions. In Lemma 5, we give the relationship between the stability and generalization of empirical risk.

Lemma 1. (Vershynin, 2012) For any vector x  Rd, its 2-norm can be bounded as

x

2



1 1-

sup


, x .

where  = {1, . . . , kw } be an -covering net of Bd(1).

Lemma 2. (Vershynin, 2012) For any symmetric matrix X  Rd×d, its operator norm can be

bounded as

X

op



1 1-2

sup | , X | .


where  = {1, . . . , kw } be an -covering net of Bd(1).

12

Under review as a conference paper at ICLR 2018

Lemma 3. (Mei et al., 2017) Let D  Rd be a compact set with a C2 boundary D, and f, g : A  R be C2 functions defined on an open set A, with D  A. Assume that for all w  D and all t  [0, 1], tf (w) + (1 - t)g(w) = 0. Finally, assume that the Hessian 2f (w) is
non-degenerate and has index equal to r for all w  D. Then the following properties hold:

(1) If g has no critical point in D, then f has no critical point in D.

(2) If g has a unique critical point w in D that is non-degenerate with an index of r, then f also has a unique critical point w in D with the index equal to r.
Lemma 4. (Mei et al., 2017) Suppose that F (w) :   R is a C2 function where w  . Assume that {w(1), . . . , w(m)} is its non-degenerate critical points and let D = {w   : F (w) 2 <
and infi i 2F (w)  }. Then D can be decomposed into (at most) countably components, with each component containing either exactly one critical point, or no critical point. Concretely, there exist disjoint open sets {Dk}kN, with Dk possibly empty for k  m + 1, such that
D = k=1Dk .

Furthermore, w(k)  Dk for 1  k  m and each Di, k  m + 1 contains no stationary points.

Lemma 5. (Shalev-Shwartz & Ben-David, 2014b; Gonen & Shalev-Shwartz, 2017) Assume that

D is a sample distribution and randomized algorithm A is employed for optimization. Suppose

that ((x(1), y(1)), · · · , (x(n), y(n)))  D and wn = argminw J^n(w). For every j  {1, · · · , n},

suppose

wj

=

argminw

1 n-1

i=j fi(w; x(i), y(i)). For arbitrary distribution D, we have

1n ESD,A,(x(j),y(j))D n
j=1

fj - fj

= ESD, A J (wn) - J^n(wn)

.

where fj and fj respectively denote fj(wj ; x(j), y(j)) and fj(wn; x(j), y(j)).

C PROOFS FOR DEEP LINEAR NEURAL NETWORKS

In this section, we first present the technical lemmas in Sec. C.1 and then we give the proofs of these lemmas in Sec. C.2. Next, we utilize these lemmas to prove the results in Theorems 1 3 and
Corollary 1 in Sec. C.3. Finally, we give the proofs of other lemmas in Sec. C.4.

C.1 TECHNICAL LEMMAS

Here we present the technical lemmas for proving our desired results. For brevity, we also define Bj:s as follows:

Bs:t W (s)W (s-1) · · · W (t)  Rds×dt-1 , (s  t); Bs:t I, (s < t).

(4)

Lemma 6. Assume that the activation functions in the deep neural network f (w, x) are linear functions. Then the gradient of f (w, x) with respect to w(j) can be written as

w(j) f (w, x) = (Bj-1:1x)  BlT:j+1 e, (j = 1, · · · , l), where  denotes the Kronecke product. Then we can compute the Hessian matrix as follows:

w(1)

2f (w,

x)

=

w(2) 





w(1) f (w, x)
w(1) f (w, x) ...

w(l) w(1) f (w, x)

· · · w(1) w(l) f (w, x) 

··· ...

w(2)

w(l) f (w, x) ...

 , 


· · · w(l) w(l) f (w, x)

where Qst w(s) w(t) f (w, x) is defined as

 

BtT-1:s+1



Bs-1:1xeT BlT:t+1

+

Bs-1:1xxT BtT-1:1



BlT:s+1Bl:t+1

, if s < t,



Qst = Bs-1:1xxT Bs-1:1  Bl:s+1T Bl:s+1 ,

if s = t,

 

BlT:s+1exT BtT-1:1

Bs-1:t+1 + Bs-1:1xxT BtT-1:1



BlT:s+1Bl:t+1

,

if s > t.

13

Under review as a conference paper at ICLR 2018

Lemma 7. Suppose Assumption 1 on the input data x holds and the activation functions in deep neural network are linear functions. Then for any t > 0, the objective f (w, x) obeys





P

1n n

f (w, x(i))-E(f (w, x(i))) > t

i=1

 2 exp-cf n min f2 max

t2 dlf2 4,  2

t , f2 2 ,

where cf is a positive constant and f = rl.

Lemma 8. Suppose Assumption 1 on the input data x holds and the activation functions in deep

neural

network

are

linear

functions.

Then

for

any

t

>

0

and

arbitrary

unit

vector





d-1
S

,

the

gradient f (w, x) obeys

1n Pn

, wf (w, x(i)) -Ewf (w, x(i)) > t

i=1

 3 exp

-cg n min

t2 l max (g 2, g 4, g  2) ,

t lg max (,  2)

,

where cg is a constant; g = cqr2(2l-1) and g = cqr2(l-1) in which cq = max0il di.

Lemma 9. Suppose Assumption 1 on the input data x holds and the activation functions in deep

neural

network

are

linear

functions.

Then

for

any

t

>

0

and

arbitrary

unit

vector





d-1
S

,

the

Hessian 2f (w, x) obeys

1n Pn

, (2wf (w, x(i)) - Ew2 f (w, x(i))) > t

i=1

 5 exp

-ch n min

t2 t  2l2 max (g, g 2, h) , gl max (,  2)

,

where g = r4(l-1) and h = r2(l-2).

Lemma 10. Suppose the activation functions in deep neural network are linear functions. Then for

any w  Bd(r) and x  Bd0 (rx), we have

wf (w, x)

2



 g ,

where g = ctlrx4r4l-2.

in which ct is a constant. Further, for any w  Bd(r) and x  Bd0 (rx), we also have

2f (w, x) op 

2f (w, x)

F

  l l,

where

l

ct rx4r4l-2.

in which ct is a constant. With the same condition, we can bound the operation norm of 3f (w, x). That is, there exists a universal constant p such that 3f (w, x) op  p.

Lemma 11. Suppose Assumption 1 on the input data x holds and the activation functions in deep neu-

ral network are linear functions. Then there exist two universal constant cg and ch such that the sam-

ple Hessian converges uniformly to the population Hessian in operator norm. Specifically, there exit

two

universal

constants

ch1

and

ch2

such

that

if

n



ch2

max(



2

l2

2p r2 h2 2s log(d/l)

,

s

log(d/l)/(l

2

)),

then

sup
w

2J^n(w)-2J (w)

op  ch1  lh

d log(nl)+log(20/) n

holds with probability at least 1 - , where h = max  r2(l-1), r2(l-2), rl-2 .

C.2 PROOFS OF TECHNICAL LEMMAS

To prove the above lemmas, we first introduce some useful results.

Lemma 12. (Rudelson & Vershynin, 2013) Assume that x = (x1; x2; · · · ; xk)  Rk is a random vector with independent components xi which have zero mean and are independent i2-sub-Gaussian variables. Here maxi i2   2. Let A be an k × k matrix. Then we have





E exp  

Aij xixj - E(

Aijxixj)  exp

2 22

A

2 F

, ||  1/(2

A 2).

i,j:i=j

i,j:i=j

14

Under review as a conference paper at ICLR 2018

Lemma 13. Assume that x = (x1; x2; · · · ; xk)  Rk is a random vector with independent components xi which have zero mean and are independent i2-sub-Gaussian variables. Here maxi i2   2. Let a be an n-dimensional vector. Then we have

kk

E exp 

aixi2 - E

aix2i

i=1 i=1

k

 E exp 1282 4

a2i

i=1

,

||



2

1 maxi

. ai

Lemma 14. For Bj:t defined in Eqn. (4), we have the following properties:

Bs:t op  Bs:t F  r and where r = rs-t+1  max r, rl and f = rl.

Bl:1 op  Bl:1 F  f ,

Lemma 13 is useful for bounding probability. The two inequalities in Lemma 14 can be obtained by using w(j) 2  r (j = 1, · · · , l). We defer the proofs of Lemmas 13 and 14 to Sec. C.4.2.

C.2.1 PROOF OF LEMMA 6

Proof. When the activation functions are linear functions, we can easily compute the gradient of f (w, x) with respect to w(j):

w(j) f (w, x) = (Bj-1:1x)  BlT:j+1 e, (j = 1, · · · , l),

where  denotes the Kronecker product. Now we consider the computation of the Hessian matrix.

For brevity, let Qs =

(Bs-1:1x)  BlT:s+1

.

Then

we

can

compute

2
w(s)

f

(w,

x)

as

follows:

2
w(s)

f

(w,

x)

=

2f (w, x) w(Ts)w(s)

=

2f (w, x) w(Ts)w(s)

=

(Qse) w(Ts)

=

vec (Qse) w(Ts)

vec =

QsBl:s+1W (t)Bs-1:1x w(Ts)

 =

(Bs-1:1x)T  (QsBl:s+1) w(Ts)

vec

W (s)

=(Bs-1:1x)T  (Bs-1:1x)  BlT:s+1 Bl:s+1

=x (Bs-1:1x)T  (Bs-1:1x)  BlT:s+1Bl:s+1

=y (Bs-1:1x)T  (Bs-1:1x)  BlT:s+1Bl:s+1

=z (Bs-1:1x)(Bs-1:1x)T  BlT:s+1Bl:s+1 ,

where x holds since Bj-1:1x is a vector and for any vector x, we have (x  A)B = x  (AB). y holds because for any four matrices Z1  Z3 of proper sizes, we have (Z1  Z2)  Z3 = Z1  (Z2  Z3). z holds because for any two matrices z1, z2 of proper sizes, we have z1z2T = z1  z2T = z2T  z1.

Then, we consider the case s > t:

w(t)

w(s) f (w, x)

2f (w, x) = w(Tt)w(s)

=

2f (w, x) w(Tt)w(s)

=

(Qse) w(Tt)

=

vec (Qse) w(Tt)

vec =

QsBl:t+1W (t)Bt-1:1x w(Tt)

vec +

(Bs-1:1x)  BlT:s+1 w(Tt)

e

.

Notice,

here

we

just

think

that

Qs

in

the

( )vec QsBl:t+1W (t)Bt-1:1x
 w(Tt)

is

a

constant

matrix

and

is

not

related

to

W (t).

Similarly,

we

also

take

e

in

 vec(((Bs-1:1 x)BlT:s+1 )e)
 w(Tt)

as

a

constant

vector.

Since

we have

vec QsBl:t+1W (t)Bt-1:1x w(Tt)

=

Bs-1:1xxT BtT-1:1

 BlT:s+1Bl:t+1

,

15

Under review as a conference paper at ICLR 2018

we only need to consider

vec

(Bs-1:1x)  BlT:s+1 e w(Tt)

vec =

(Bs-1:1x)  w(Tt)

BlT:s+1e

vec (Bs-1:1x) BlT:s+1e T = w(Tt)

vec =

Bs-1:t+1W (t) Bt-1:1xeT Bl:s+1 wtT

 =

Bt-1:1xeT Bl:s+1 T  Bs-1:t+1vec wtT

W (t)

= Bt-1:1xeT Bl:s+1 T  Bs-1:t+1.

Therefore, for s > t, by combining the above two terms, we can obtain

w(t) w(s) f (w, x) = BlT:s+1exT BtT-1:1 Bs-1:t+1 + Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1 . Then, by similar method, we can compute the Hessian for the case s < t as follows: w(t) w(s)f (w, x) = BtT-1:s+1  Bs-1:1xeT BlT:t+1 + Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1 . The proof is completed.

C.2.2 PROOF OF LEMMA 7 Proof. We first prove that v(l), which is defined in Eqn. (5), is sub-Gaussian.
v(l) = W (l) · · · W (1)x = Bl:1x. Then by the convexity in  of exp(t) and Lemma 14, we can obtain

(5)

E exp , v(l) - E(v(l))

=E (exp ( , Bl:1x - EBl:1x ))

E exp BlT:1, x

 exp

BlT:1 22 2 2

x
 exp

f2  2



2 2

,

2

(6)

where x uses the conclusion that Bl:1 op  Bl:1 F  f in Lemma 14. This means that v(l) is centered and is f2 2-sub-Gaussian. Accordingly, we can obtain that the k-th entry of v(l) is also zk 2-sub-Gaussian, where zk is a universal positive constant. Note that maxk zk  f2. Let vi(l)
16

Under review as a conference paper at ICLR 2018

denotes the output of the i-th sample x(i). By Lemma 13, we have that for s > 0,

1n Pn
i=1

vi(l)

2 2

-

E

vi(l)

2 2

t >
2

n
=P s
i=1

vi(l)

2 2

-

E

vi(l)

2 2

nst >
2

x
 exp

- snt 2

E

n
s

i=1

vi(l)

2 2

-

E

vi(l)

2 2

y
 exp

- snt

2

n
Es

i=1

vi(l)

2 2

-

E

vi(l)

2 2

z
 exp

- snt

2

n
exp 128dls2f4 4

i=1

|s|



1 f2  2

{
 exp

-c n min

t2 t dlf4  4 , f2  2

.

Note that x holds because of Chebyshev's inequality. y holds since x(i) are independent. z is established by applying Lemma 13. We have { by optimizing s. Since v(l) is sub-Gaussian, we have

P

1n n

yT vi(l) - EyT vi(l)

t >
2

n
P s

yT vi(l) - EyT vi(l)

nst >
2

i=1 i=1

 exp

- nst 2

E exp

n
s

yT vi(l) - EyT vi(l)

i=1

 exp - nst 2

n
E exp s yT vi(l) - EyT vi(l)

i=1

x
 exp

- nst

n
exp

f2  2s2

y

2 2

22

i=1

y
 exp

-

8f2

nt2 2

y

2 2

,

where x holds because of Eqn. (6) and we have y since we optimize s.

Since the loss function f (w, x) is defined as f (w, x) = v(l) - y 22, we have

f (w, x) - E(f (w, x)) =

v(l) - y

2 2

-E(

v(l) -y

22) =

v(l)

2 2

-E

v(l)

2 2

+ yT v(l) -EyT v(l)

.

Therefore, we have

1n

Pn

f (w, x(i)) - E(f (w, x(i))) > t

i=1

1n P n
i=1

vi(l)

2 2

-

E

vi(l)

2 2

t >
2

1n +P n

yT vi(l) - EyT vi(l)

t >
2

i=1

t2 t2 t 2 exp -cf n min dlf4 4 , f2 2 , f2 2

.

where cf

is a constant. Note that

y

2 2

is

the

label

of

x,

then

it

can

also

be

bounded.

The

proof

is

completed.

C.2.3 PROOF OF LEMMA 8
Proof. For brevity, let Qj denote w(j) f (w, x). Then, by Lemma 6 we have w(j) f (w) = (Bj-1:1x)  BlT:j+1 e =x (Bj-1:1x)(BlT:j+1e) =y Bj-1:1  BlT:j+1 (x  e) ,
(7)

17

Under review as a conference paper at ICLR 2018

where x holds since Bj-1:1x is a vector, and y holds because for any four matrices Z1  Z4 of
proper sizes, we have (Z1Z3)(Z2Z4) = (Z1Z2)(Z3Z4). Note that e = v(l)-y = Bl:1x-y. Then we know that the i-th entry Qij has the form Qji = p,q zpijqxpxq + p ypijxp + rij (Step 1 blow will give the detailed analysis) where xp denotes the p-th entry in x. Note that zpijq, ypij and rij are constants and independent on x.

We divide   R

l j=1

dj

dj-1

into



=

(1; · · ·

; l)

where

j



Rdj dj-1 .

Let

ji

denote

the

i-th

entry in j. Accordingly, we have

l
E , wf (w, x) - Ewf (w, x) = j, Qj - EQj = E1 + E2 + E3,
j=1

where E1, E2, and E3 are defined as

E1=

 l dj dj-1



 ij zpijq (xpxq - Expxq) , E2 =

 l dj dj-1



 ij zpijp x2p - Exp2 ,

p,q:p=q j=1 i=1

p j=1 i=1

 l dj dj-1



E3 = 

ji ypij  (xp - Exp) .

p j=1 i=1

(8)

Thus, we can further separate the event as:

P (E > t)  P

1 n

n

E1k

>

t 3

+P

k=1

1 n

n

E2k

>

t 3

+P

k=1

1 n

n

E3k

>

t 3

k=1

.

Thus, to prove our conclusion, we can respectively establish the upper bounds of the three events.

To the end, for each input sample x(i), we divide its corresponding Qj - EQj into E1, E2 and
E3. Then we bound the three events separately. Before that, we first give several equalities. Since Bj:s = W (j)W (j-1) · · · W (s) (j  s), by Lemma 14 we have

Bj:s

2 F

 r2(j-s+1)

and

Bl:t+1

2 F

Bt-1:s+1

2 F

Bs-1:1

2 F

 r2(l-2),

(9)

These two inequalities can be obtained by using

W (i)

2 F

=

w(i)

2 2



r2.

Step 1. Divide Qj - EQj: Note that e = v(l) - y = Bl:1x - y. Let Hj = Bj-1:1  BlT:j+1. Then we can further write Eqn. (7) as

Qj = w(j) f (w) = Hj (x  (Bl:1x) - x  y) = Hj ((Id0  Bl:1) (x  x) - x  y) , (10)

where Id0  Rd0×d0 is the identity matrix. According to Eqn. (10), we can write the i-th entry of Qj as the form Qji = p,q zpijqxpxq + p ypijxp + rij where xp denotes the p-th entry in x. Let Zj = Hj (Id0  Bl:1)  R .djdj-1×d20 Then, we know that the i-th entry Qij = Z(i, :)x , where x = x  x = [x1x; x2x; · · · , xd0 x]  Rd02 . In this way, we have zpijq = Zj(i, (p - 1)d0 + q) which further implies

(zpijq)2  cq

Zj(i, :)

2 2

,

(11)

p,q

where cq = max0il di.

We divide the i-th row Hj(i, :) into Hj(i, :) = [Hj1i, Hj2i, · · · , Hjdi0 ] where Hjpi  R1×dl . Then we have ypij = yT Hjpi. This yields

(ypij )2  cq (yT Hjpi)2  cq

y

2 2

Hjpi

2 2

=

cq

y

2 2

Hj(i, :)

2 2

.

pp

p

(12)

Let ji denote the i-th entry of j. Then, by Eqn. (8), we can obtain

j, (Qj - E(Qj)) =

apq (xpxq - Expxq)+ app x2p - Ex2p + bp (xp - Exp)

j

p,q:p=q

pp

= E1 + E2 + E3,

18

Under review as a conference paper at ICLR 2018

where apq and bp are defined as

l dj dj-1

l dj dj-1

apq =

ij zpijq and bp =

ij ypij .

j=1 i=1

j=1 i=1

Note that for any four matrices of proper sizes, we have (Q1 Q2)(Q3 Q4) = (Q1Q3)(Q2Q4), indicating Zj = Bj-1:1  BlT:j+1 (Id0  Bl:1) = Bj-1:1  BlT:j+1Bl:1 . This gives

cq

Zj

2 F



cq

Bj-1:1

2 F

Bl:j+1

2 F

Bl:1

2 F

x


cq r2(l-1) r2l

= cqr2(2l-1)

.

(13)

Note that Eqn. (13) uses the conclusion in Eqn. (9). Therefore, we can have the following bound:

dj dj-1

dj dj-1

(zpijq)2 

(zpijq )2

x


cq

dj dj-1

Zj(i, :)

2 2

=

cq

Zj

2 F



,

i=1 i=1 p,q

i=1

(14)

where x uses Eqn. (11). Then we can utilize Eqn. (14) and

l j=1

apq as follows:

dj dj i=1

-1

(ij

)2

= 1 to bound

 l dj dj-1

2

l dj dj-1

 dj dj-1



ap2q



l

 



ij zpijq

l 



(ji )2 

(zpijq)2  l.

j=1 i=1

j=1 i=1

i=1

which further gives

l dj dj-1

 dj dj-1

a2pq  l 

(ji )2 



(zpijq )2 

x


l.

p,q j=1 i=1

i=1 p,q

where x uses Eqn. (14). Similarly, we can obtain

dj dj-1

dj dj-1

(ypij )2 

cq

y

2 2

Hj(i, :)

2 2

=

cq

y

2 2

Hj

2 F



cq

y

2 2

r2(l-1)

.

i=1 i=1

So we can bound bp as

l dj dj-1

2

l dj dj-1

 dj dj-1



b2p  l 

ji ypij   l 

(ij )2 

(ypij )2  l ,

j=1 i=1

j=1 i=1

i=1

where  = cq y 22r2(l-1). Accordingly, we can have

l dj dj-1

 dj dj-1

b2p  l 

(ij )2 



(ypij )2

x


l

,

p j=1 i=1

i=1 p

where x uses (15).

(15)

19

Under review as a conference paper at ICLR 2018

Step 2. Bound P(E1 > t/3), P(E2 > t/3) and P(E3 > t/3): Let Ehk1 denotes the Eh1 which corresponds to the k-th sample x(k). Therefore, we can bound





P

1 n

n

E1k

>

t 3

n
=P s 

apkq

xkpxkq - Exkpxkq

>

snt 3

k=1

k=1 p,q:p=q

x
 exp

- nst 3


n



E exp s 

apkq xpkxqk - Expkxqk 

k=1 p,q:p=q

y
 exp

- nst 3


n



E exp s 

apkq xkpxkq - Expkxqk 

k=1

p,q:p=q

z
 exp

- nst

3


n



exp 2 2s2

(apkq )2 

k=1

p,q:p=q

|s|  1 2 l

 exp - nst

n
exp 2 2s2l

3

j=1

{
 exp

-c n min

t2 l

2

,

t l

,

where x holds because of Chebyshev's inequality. y holds since x(i) are independent. z is

established by applying Lemma 12. We have { by optimizing s. Similarly, by Lemma 13 we can

bound P

1 n

n k=1

E2k

>

t 3

as follows:

P

1 n

n

E2k

>

t 3

 exp - nst 3

n
E exp s

k=1

k=1

apkp (xpk)2 - E(xkp)2
p

 exp - nst

n
exp 128 4s2l

3

k=1

|s|  1  2 l

 exp

-c n min

t2 l 4

,

t l 2

.

Finally, since x(i) are independent sub-Gaussian, we can use Hoeffding inequality and obtain

P

1 n

n

E3k

>

t 3

P

1n n

k=1

k=1

bpk xpk - Exkp
p

t >
3

exp

-

c 

nt2 l 2

.

Step 3. Bound P(E > t): By comparing the values of  and  , we can obtain

P (E > t) P

1 n

n

E1j

>

t 3

+P

1 n

n

E2j

>

t 3

+P

1 n

n

E3j

>

t 3

k=1

k=1

k=1

3 exp

-cg n min

t2 l max (g 2, g 4, g  2) ,

t lg max (,  2)

,

where g = cqr2(2l-1) and g = cqr2(l-1) in which cq = max0il di. The proof is completed.

C.2.4 PROOFS OF LEMMA 9

Proof. For brevity, let Qts denote w(t) w(s) f (w, x) . Then, by Lemma 6 we have

 

BlT:s+1exT BtT-1:1

 Bs-1:t+1 +

Bs-1:1xxT BtT-1:1



BlT:s+1Bl:t+1

,



Qts = Bs-1:1xxT Bs-1:1  Bl:s+1T Bl:s+1 ,

if s > t, if s = t,

 

BtT-1:s+1



Bs-1:1xeT BlT:t+1

+

Bs-1:1xxT BtT-1:1



BlT:s+1Bl:t+1

, if s < t.

20

Under review as a conference paper at ICLR 2018

Then we know that the (i, k)-th entry Qtiks has the form Qtiks = p,q zpikqxpxq + p ypikxp + rik (explained in the following Step 1. I) where xp denotes the p-th entry in x. Note that zpikq, ypik and rik are constant and independent on x. For convenience, we let Qts = Hts + Gts, where Gts = Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1 and Hts is defined as

 

BlT:s+1exT BtT-1:1

 Bs-1:t+1,

Hts = 0,

 BtT-1:s+1  Bs-1:1xeT BlT:t+1 ,

if s > t, if s = t, if s < t.

Let

1n E=
n

,

w2 f (w, x) - Ew2 f (w, x)



1 , Eh = n

n

t,(Hts -E(Hts)) s ,

j=1

j=1 t,s

1n Eg = n

t,(Gts -E(Gts)) s .

j=1 t,s

Then we divide the event as two events:

P (E > t) = P (Eh + Eg > t)  P (Eh > t/2) + P (Eg > t/2) .

Now we look each event separately. Similar to Qts, the (i, k)-th entry Htisk has the form Htisk = p,q zpikqxpxq + p ypikxp + rik. We divide the unit vector   Rd as  = (1; · · · ; l) where
j  Rdjdj-1 . For input vector x, let t,s t, (Hts - E(Hts)) s = Eh1 + Eh2 + Eh3, where





Eh1=



(itsk)zpikq(xpxq -Expxq) , Eh2 = 

(itsk)zpikq xp2 -Exp2 ,

p,q:p=q t,s i,k

p t,s i,k



Eh3 = 

(tisk)ypik (xp - Exp) ,

p t,s i,k

(16)

where xp denotes the p-th entry in x and ji denotes the i-th entry of j. Let Ehj1 , Ehj2 , and Ehj3 denote the Eh1 , Eh2 , and Ehj3 of the j-th sample. Thus, considering n samples, we can further separately divide the two events above as:









t P Eh > 2



P

1 n

n

Ehj 1

>

t 6

+P

1 n

n

Ehj 2

>

t 6+P



1 n

n

Ehj 3

>

t 6



.

j=1

j=1

j=1

Similarly, we can define Eg1, Eg2 and Eg3.









t P Eg > 2



P

1 n

n

Egj1

>

t 6

+P

1 n

n

Egj2

>

t 6

+

P



1 n

n

Egj3

>

t 6



.

j=1

j=1

j=1

Thus,

to

prove

our

conclusion,

we

can

respectively

establish

the

upper

bounds

of

P

Eh

>

t 2

and

P

Eg

>

t 2

.

Step

1:

Bound

P

Eh

>

t 2

To achieve our goal, for each input sample x(i), we divide its corresponding t,s(Hts - EHts)
as Eh1, Eh2 and Eh3. Then we bound the three events separately. Before that, we first give two equalities. Since Bj:s = W (j)W (j-1) · · · W (s) (j  s), by Lemma 14 we have

Bj:s

2 F

 r2(j-s+1)

and

Bl:t+1

2 F

Bt-1:s+1

2 F

Bs-1:1

2 F

 r2(l-2),

(17)

These two inequalities can be obtained by using

W (i)

2 F

=

w(i)

2 2



r2.

21

Under review as a conference paper at ICLR 2018

I. Divide Hts - EHts: For t = s, we can write the (i, k)-th entry Htisk as the form Htisk = p,q zpikqxpxq + p ypikxp + rik. Now we try to bound zpikq and ypik. We first consider the case s < t.
Note that e = v(l) - y = Bl:1x - y. Specifically, we have

Hts = BtT-1:s+1  Bs-1:1xxT BlT:1BlT:t+1 - Bs-1:1xyT BlT:t+1 .

So the (i , k )-th entry in the matrix Bs-1:1xxT BlT:1BlT:t+1 is [Bs-1:1xxT BlT:1BlT:t+1]i k = (Bs-1:1)(i , :)x(Bl:1Bl:t+1)(k , :)x = xT ((Bs-1:1)(i , :))T (Bl:1Bl:t+1)(k , :)x, where A(i , :)
denotes the i -th row of A. Let ii = mod(i, ds), kk = mod(k, dt-1), ii = i/ds and kk = k/dt-1 . In this case, the (i, k)-th entry Htisk = [Bt-1:s+1]kk ii xT ((Bs-1:1)(ii, : ))T (Bl:1Bl:t+1)(kk, :)x + [Bt-1:s+1]kk ii yT (Bl:t+1)(kk, :)T (Bs-1:1)(ii, :)x. Therefore, we have

(zpikq)2 =[Bt-1:s+1]2kk ii

((Bs-1:1)(ii, :))T (Bl:1Bl:t+1)(kk, :)

2 F

p,q

[Bt-1:s+1]k2k ii

(Bs-1:1)(ii, :)

2 2

(Bl:1Bl:t+1)(kk, :)

2 2

.

Therefore, we can further establish

(zpikq)2 

[Bt-1:s+1]2kk ii

(Bs-1:1)(ii, :)

2 2

(Bl:1Bl:t+1)(kk, :)

2 2

i,k p,q

i,k



[Bt-1:s+1]2kk ii

(Bs-1:1)(ii, :)

2 2

(Bl:1Bl:t+1)(kk, :)

2 2

i,k

=

(Bt-1:s+1)(kk , :)

2 2

Bs-1:1

2 F

(Bl:1Bl:t+1)(kk, :)

2 2

k

=

Bt-1:s+1

2 F

Bs-1:1

2 F

Bl:1Bl:t+1

2 F

x r4(l-1) .

(18)

where x uses Eqn. (17). Similarly, we can bound

(ypik)2 =[Bt-1:s+1]k2k ii
p
[Bt-1:s+1]2kk ii
So it further yields

yT (Bl:t+1)(kk, :)T (Bs-1:1)(ii, :)

2 F

y

2 2

(Bl:t+1)(kk, :)

2 2

(Bs-1:1)(ii, :)

2 2

.

(ypik)2 

[Bt-1:s+1]k2k ii

y

2 2

(Bl:t+1)(kk, :)

2 2

(Bs-1:1)(ii, :)

2 2

i,k p

i,k



y

2 2

Bt-1:s+1

2 F

Bl:t+1

2 F

Bs-1:1

2 F

x


y

2 2

r2(l-2)

,

(19)

where x uses Eqn. (17). Note that for the case s  t, Eqn. (18) and (19) also holds. Let ij denote the i-th entry of j. Then, by Eqn. (16), we can obtain

( t, (Hts -E(Hts))s ) =

apq (xpxq -Expxq)+ app xp2 -Exp2 + bp (xp -Exp)

t,s

p,q:p=q

pp

= Eh1 + Eh2 + Eh3,

where apq and bp are defined as

apq =

(itsk)zpikq and bp =

(it sk )ypik .

t,s i,k

t,s i,k

Then according to Eqn. (18) and t,s i,k(itks )2 = 1, we can bound apq as follows:



2 

  



ap2q  l2  (tisk)zpikq  l2  (itks )2 (zpikq)2  l2  (itsk)2  l2.

t,s i,k

t,s i,k

i,k

t,s i,k

22

Under review as a conference paper at ICLR 2018

which further yields

 





ap2q  l2  (tisk)2 

(zpikq)2  l2  (tiks )2  l2.

p,q t,s i,k

i,k p,q

t,s i,k

Similarly, by using Eqn. (19), we have



2 

 

bp2  l2

 (itks )ypik  l2



(itsk)2 

(ypik )2 

x




l2.

t,s i,k

t,s i,k

i,k

Accordingly, we can have

 



bp2  l2  (tiks )2 

(ypik)2   l2.

p t,s i,k

i,k p

II. Bound P(Eh1 > t/6), P(Eh2 > t/6) and P(Eh3 > t/6): Let Ehj1 denotes the Ehj1 which corresponds to the j-th sample x(i). Therefore, we can bound

  



1 Pn

n

Ehj 1

>

t 6

P s

n



apj q

xjpxjq - Exjpxjq

snt > 6 

j=1

j=1 p,q:p=q

x
 exp

- nst 6


n



E exp s 

apjq xpj xqj - Expj xqj 

j=1 p,q:p=q

y
 exp

- nst 6


n



E exp s 

apjq xpj xqj - Expj xqj 

j=1

p,q:p=q

z nst  exp -
6


n



exp 2 2s2

(apj q )2 

j=1

p,q:p=q

|s|  1 2 l 

 exp - nst

n
exp 2 2s2l2

6

j=1

{
 exp

-c n min

t2 l2

2

,

t l

,

where x holds because of Chebyshev's inequality. y holds since x(i) are independent. z is established because of Lemma 12. We have { by optimizing s. Similarly, we can bound

P

1 n

n j=1

Ehj 2

>

t 6

as follows:



1 Pn

n

Ehj 2

>

t 6

 exp

j=1

 exp

 exp

- nst 6

n
E exp s

j=1

ajpp (xjp)2 - E(xpj )2
p

- nst

n
exp 128 4s2l2

6

j=1

|s|



1 2l 

-c n min

t2 l2

4

,

t l

2

.

Finally, since x(i) are independent sub-Gaussian, we can use Hoeffding inequality and obtain

 

1 Pn

n

Ehj 3

>

t 
6

=

1 Pn

n

j=1

j=1

bjp xjp - Exjp
p



> t   exp 6

-c 

nt2 l2 2

.

23

Under review as a conference paper at ICLR 2018

Since for s = t, P

1 n

n j=1

Ehj 1

>

t 6

=P

1 n

n j=1

Ehj 2

>

t 6

=P

1 n

n j=1

Ehj 3

>

t 6

= 0,

the above upper bounds also hold.

III:

Bound

P

Eh

>

t 2

By comparing the values of  and  , we can obtain

    

t P Eh > 2

P

1 n

n

Ehj 1

>

t 6



+

P

1 n

n

Ehj 2

>

t 6

+

P



1 n

n

Ehj 3

>

t 6



j=1

j=1

j=1

3 exp

-c2n min

l2

max

(

t2 2, 

4,

q 

2)

,

 l

t max

(,



2)

,

where q = r2(l-2).

Step

2:

Bound

P

Eg

>

t 2

To achieve our goal, for each input sample x(i), we also divide its

corresponding t,s(Gts - EGts) as Eh1, Eh2 and Eh3. Then we bound the three events separately.

Before that, we first give several equalities.

I. Divide Gts - EGts: Dividing Gts - EGts is more easy than dividing Hts - EHts since the later has more complex form. Since Gts = Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1 . we also can write the (i, k)-th entry Gtiks as the form Gitks = p,q zpikqxpxq + p ypikxp + rik. But here ypik = 0.

Then similar to the step in dividing Hts - EHts, we can bound

a2pq  gl2 and

ap2q  gl2 where g = r4(l-1).

p,q

II. Bound P(Eg1 > t/6), P(Eg2 > t/6) and P(Eg3 > t/6): Since ypik = 0, P(Eh3 > t/6) = 0. Similar to the above methods, we can bound



1 Pn

n

Egj1

>

t 
6

 exp

j=1

-c1n

t2 t gl2 2 , gl

,

and



1 Pn

n

Egj2

>

t 6



exp

j=1

-c1 n

t2 t gl2 4 , gl 2

.

III:

Bound

P

Eh

>

t 2

We

can

obtain

P

Eg

>

t 2

as follows:









t P Eg > 2

P

1 n

n

Egj1

>

t 6



+P

1 n

n

Egj2

>

t 6

+

P



1 n

n

Egj3

>

t 6



j=1

j=1

j=1

2 exp

-c2n min

t2 t gl2 max ( 2,  4) , gl max (,  2)

.

Step 3: Bound P(E > t) Finally, we combine the above results and obtain

P (E > t) P

t Eh > 2

+P

t Eg > 2

5 exp

-ch n min

t2 t  2l2 max (g, g 2, h) , gl max (,  2)

where g = r4(l-1) and h = r2(l-2).

,

C.2.5 PROOF OF LEMMA 10

Proof. Before proving our conclusion, we first give an inequality:

e

2 2

=

Bl:1x - y

2 2



Bl:1x

2 2

+

2

yT Bl:1x

+

y

2 2

x


rx2 f2

+

2rxf

y

2+

y

2 2

,

24

Under review as a conference paper at ICLR 2018

where f = rl. Notice, x holds since by Lemma 14, we have

Bl:1

2 F

 r2l.

Then we consider wf (w, x). Firstly, by Lemma 6 we can bound

w(j) f (w, x)

2 2

as

follows:

w(j) f (w, x)

2 2

=

(Bj-1:1x)  BlT:j+1

e 2
2

Bj-1:1

2 2

x

2 2

Bl:j+1

2 2

e

2 2

x rx2f21

rx2f2 + 2rxf y 2 +

y

2 2

,

where f1 = r(l-1). x holds since we have

Bl:j+1

2 F

Bj-1:1

2 F

 r2(l-1) by using

W (i)

2 F

=

w(i)

2 2



r2.

Therefore,

we

can

further

obtain

l

wf (w, x)

2 2

=

w(i) f (w, x)

2 2



lrx2 f21

rx2f2 + 2rxf y 2 +

y

2 2

.

i=1

Notice, y is the label of sample and the weight magnitude r is usually lager than 1. Then we have y 2  rl. Also, the values in input data are usually smaller than rl. Thus, we have

wf (w, x)

2 2



ct lrx4 r4l-2

g ,

where ct is a constant. Then we use the inequality 2f (w, x) op  2f (w, x) F to bound 2f (w, x) op. Next we only need to give the upper bound of 2f (w, x) F . Let f2 = rl-2.
We first consider Qst w(s) w(t) f (w, x) . By Lemma 6, if s < t, we have

Qst

2 F

=

2

BtT-1:s+1  Bs-1:1xeT BlT:t+1 + BtT-1:s+1  Bs-1:1xeT BlT:t+1

Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1

2 F

2 F

+

Bs-1:1xxT BtT-1:1  BlT:s+1Bl:t+1

2

Bt-1:s+1

2 F

Bs-1:1

2 F

x

2 2

e

2 2

Bl:t+1

2 F

+2

Bs-1:1

2 F

x

2 2

x

2 2

Bt-1:1

2 F

Bl:s+1

2 F

Bl:t+1

2 F

x 2f22 rx2

rx2f2 + rxf y 2 +

y

2 2

+ 2f41 rx4,

2 F

where x holds since we use

Bl:t+1

2 F

Bt-1:s+1

2 F

Bs-1:1

2 F

 f22 and

Bs-1:1

2 F

Bl:s+1

2 F

 f21 . Note that when s  t, the above inequality also holds. Similarly, consider the values in input

data and the values in label, we have

Qst

2 F

 ct rx4r4l-2

l,

where ct is a constant. Therefore, we can bound

2f (w, x) op  2f (w, x) F 

l

l

Qst

2 F

  l l.

s=1 t=1

On the other hand, if the activation functions are linear functions, f (w, x) is fourth order differentiable when l  2. This means that x3wf (w, x) exists. Also since for any input x  Bd0 (rx) and w  , we can always find a universal constant p such that

3wf (w, x) op = sup 3 , 3wf (w, x) = [3wf (w, x)]ijkijk  p < +.

 21

i,j,k

We complete the proofs.

C.2.6 PROOF OF LEMMA 11

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

25

Under review as a conference paper at ICLR 2018

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in (Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /d

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th
layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Now we use the decomposition strategy to bound our goal:

2J^n(w) - 2J (w) op

=

1 n

n

2f (w, x(i)) - E(2f (w, x))

i=1 op

1n =
n

2f (w, x(i)) - f (wkw , x(i))

1 +
n

n

2f (wkw , x(i)) - E(2f (wkw , x))

i=1 i=1

+ E(2f (wkw , x)) - E(2f (w, x)) op

1n 
n

2f (w, x(i)) - 2f (wkw , x(i))

+

1 n

n
2f (wkw , x(i)) - E(2f (wkw , x))

i=1 op i=1

op

+ E(2f (wkw , x)) - E(2f (w, x)) . op

Here we also define four events E0, E1, E2 and E3 as

E0 =

sup
w

2J^n(w) - 2J (w)

t op

,




E1 = sup w

1n n
i=1

2f (w, x(i)) - 2f (wkw , x(i))





t ,

op 3 





E2 =

sup

jw[n j ],j=[l]



1 n

n
2f (wkw , x(i)) - E(2f (wkw , x))
i=1

 op

t ,
3

E3 =

sup
w

E(2f (wkw , x)) - E(2f (w, x))

op



t 3

.

Accordingly, we have

P (E0)  P (E1) + P (E2) + P (E3) .

So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0). 26

Under review as a conference paper at ICLR 2018

Step 1. Bound P (E1): We first bound P (E1) as follows:

P (E1) =P

sup
w

1n n

2f (w, x(i)) - 2f (wkw , x(i))

i=1

t 3
2

x


3 t

E

sup
w

1n n
i=1

2f (w, x(i)) - 2f (wkw , x(i))

2



3 t

E

sup
w

2f (w, x) - 2f (wkw , x) 2



3 t

E

1
sup n
w

n i=1

2f (w, x(i)) - 2f (wkw , x(i))

w - wkw 2

sup w - wkw 2
w

y


3p

,

t

where x holds since by Markov inequality and y holds because of Lemma 10.

Therefore, we can set Then we can bound P(E1):

t  6p . 



P(E1)



. 2

Step 2. Bound P (E2): By Lemma 2, we know that for any matrix X  Rd×d, its operator norm

can be computed as

X

op



1 1-2

sup | , X | .


where  = {1, . . . , kw } be an -covering net of Bd(1).

Let

1/4

be

the

1 4

-covering

net

of

Bd(1)

but

it

has

only

s

nonzero

entries.

So

the

size

of

its

-net is

d s

3

s
 exp (s log (12d)) .

1/4

Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd ). Then we can bound P (E2) as follows:

P (E2) =P

sup
jw[nj ] j[l]

1 n

n

2f (wkw , x(i)) - E(2f (wkw , x))

t 3

i=1 2

P sup 2
jw[nj ] j[l],1/4

,

1 n

n

2f (wkw , x(i)) - E

2f (wkw , x)

i=1



t 3


l
 exp (s log (12d)) exp  sj log
j=1

3rd


 sup P
jw[nj ] j[l],1/4

1n ,
n
i=1

2f (wkw , x(i))-E 2f (wkw , x)

t 6

x
 exp

s log

36rd2

10 exp

-ch n min

t2 t 36 2l2 max (g, g 2, h) , 6gl max (,  2)

where x holds since by Lemma 9, we have

P

1n n

, (w2 f (w, x) - E2wf (w, x))

>t

i=1

 10 exp

-ch n min

t2 t



2l2

max

(g ,

g 2,

, h)

g l

max

(,

 2)

,

,

27

Under review as a conference paper at ICLR 2018

where g = r4(l-1) and h = r2(l-2). Let d = s log(36d2r/ )+log(20/). Thus, if we set

 t  max 

36 2l2 max (g, g 2, h) d

, 6gl max ,  2

 d
,

ch n

ch n

then we have

P (E2)



 .
2

Step 3. Bound P (E3): We first bound P (E3) as follows:

P (E3) =P P P
x
P

sup
w

E(2f (wkw , x)) - E(2f (w, x))

t  23

E sup
w

(2f (wkw , x) - 2f (w, x)

t 23

1
sup n
w

n i=1

2f (w, x(i)) - 2f (wkw , x(i))

w - wkw 2

p

t 3

,

sup
w

w - wkw

t 2 3

where x holds because of Lemma 10. We set enough small such that p < t/3 always holds. Then it yields P (E3) = 0. Step 4. Final result: For brevity, let 2 = 36 2l2 max g, g 2, h and 3 = 6gl max ,  2 . To ensure P(E0)  , we just set = 36rl/n and



t  max  6p 

,

3p

,



2(s log(36d2r/

)+log(20/)) ,

3(s log(36d2r/

)+log(20/)) 

ch n

ch n

 = max  216pr ,
n



2(s log(d2nl)+log(20/)) , 3(s log(36d2n/l)+log(20/))  .

ch n

ch n

Thus,

there

exit

two

universal

constants

ch1

and

ch2

such

that

if

n



ch2

max(



2

l2

2p r2 h2 2s log(d/l)

,

s log(d/l)/(l 2)), then

sup
w

2J^n(w)-2J (w)

op  ch1  lh

d log(nl)+log(20/) n

holds with probability at least 1 - , where h = max  r2(l-1), r2(l-2), rl-2 . The proof is completed.

C.3 PROOFS OF MAIN THEOREMS

C.3.1 PROOF OF THEOREM 1

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in

(Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /d

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

28

Under review as a conference paper at ICLR 2018

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Accordingly, we can
decompose J^n(w) - J (w) as
2

J^n(w) - J (w)
2

=

1 n

n

f (w, x(i)) - E(f (w, x))

i=1 2

1n

1n

= n

f (w, x(i)) - f (wkw , x(i))

+ n

f (wkw , x(i)) - E(f (wkw , x))

i=1 i=1

+ E(f (wkw , x)) - E(f (w, x))

2

 1n n

f (w, x(i)) - f (wkw , x(i))

+

1 n

n
f (wkw , x(i)) - E(f (wkw , x))

i=1 2 i=1

2

+ E(f (wkw , x)) - E(f (w, x)) .
2
Here we also define four events E0, E1, E2 and E3 as

E0 = E1 = E2 =

sup J^n(w) - J (w)  t ,

w

2

1n

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

t 3
2

,

1n

t

sup
jw[n j ],j=[l]

n

f (wkw , x(i)) - E(f (wkw , x))

 3

i=1 2

,

E3 = sup E(f (wkw , x)) - E(f (w, x))
w

t . 3

2

Accordingly, we have

P (E0)  P (E1) + P (E2) + P (E3) .

So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).

Step 1. Bound P (E1): We first bound P (E1) as follows:

P (E1) =P

sup
w

1n n

f (w, x(i)) - f (wkw , x(i))

i=1

t 3
2

x


3 t

E

sup
w

1n n
i=1

f (w, x(i)) - f (wkw , x(i))

2



3 t

E

sup
w

1 n

n i=1

f (w, x(i)) - f (wkw , x(i))

w - wkw 2

2 sup w - wkw 2
w

3 t

E

sup 2J^n(w, x)

w

2

,

where x holds since by Markov inequality, we have that for an arbitrary nonnegative random variable

x,

then

P(x



t)



E(x) t

.

29

Under review as a conference paper at ICLR 2018

Now we only need to bound E

supw

2J^n(w, x)
2

. Now we utilize Lemma 10 to achieve

this goal:

E

sup 2J^n(w, x)

w

2

= E

sup
w

2f (w, x) - 2f (w, x)

2

where l = ct rx4r4l-2. Therefore, we have



P (E1) 

3l

l t

.

We further let Then we can bound P(E1):

 t  6l l .


P(E1)



 .
2

  l l.

Step 2. Bound P (E2): By Lemma 1, we know that for any vector x  Rd, its 2-norm can be

computed as

x

2



1 1-

sup


, x .

where  = {1, . . . , kw } be an -covering net of Bd(1).

Let

1/2

be

the

1 2

-covering

net

of

Bd(1)

but

it

has

only

s

nonzero

entries.

So

the

size

of

its

-net is

d s

3

s
 exp (s log (6d)) .

1/2

Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd ). Then we can bound P (E2) as follows:

1n

t

P (E2) =P

sup
jw[n j ],j=[l]

n

f (wkw , x(i)) - E(f (wkw , x))

 3

i=1 2

1n

=P

sup 2
jw[n j ],j=[l],1/2

, n

f (wkw , x(i)) - E (f (wkw , x))

i=1


l
 exp (s log (6d)) exp  sj log
j=1


3rd  sup P
jw[n j ],j=[l],1/2

1n n
i=1

t 3
,

f (wkw , x(i)) - E (f (wkw , x))

t 6

x 18rd  exp s log

t2 t 6 exp -cg n min 36l max (g 2, g 4, g  2) , 6 lg max (,  2)

where x holds since by Lemma 8, we have

1n Pn
i=1

, wf (w, x(i)) -Ewf (w, x(i)) > t

 3 exp

-cg n min

t2 l max (g 2, g 4, g  2) ,

t lg max (,  2)

,

where cg is a constant; g = cqr2(2l-1) and g = cqr2(l-1) in which cq = max0il di.

Let 2 = 36l max g 2, g 4, g  2 and 3 = 6 lg max ,  2 . Thus, if we set

t  max

2(s log(18dr/

)+log(12/)) ,

3(s log(18dr/

)+log(12/))

,

cg n cg n

,

30

Under review as a conference paper at ICLR 2018

then we have



P (E2)



. 2

Step 3. Bound P (E3): We first bound P (E3) as follows:

We set

P (E3) =P

sup
w

E(f (wkw , x)) - E(f (w, x))

2



t 3

=P

sup
w

E (f (wkw , x) - f (w, x) w - wkw 2

2) sup
w

w - wkw

2



t 3

P

E sup
w

2J^n(w, x)

t  23

P

 l l

t 3

.

 enough small such that l l < t/3 always holds. Then it yields P (E3) = 0.

Step 4. Final result: Finally, to ensure P(E0)  , we just set = 18lr/n and

t  max = max

 6l l


,

 3l l

,

2(s log(18dr/

)+log(12/)) ,

3(s log(18dr/

)+log(12/))

cg n cg n

108l2lr , n

2(s

log(dn/l)+log(12/)) ,

3(s

log(dn/l)+log(12/))

.

cg n cg n

Notice, we have l = ct rx4r4l-2 where ct is a constant. Therefore, there exists two universal

constants cg and cg

such that n  cg

max(

cq s

l3 r2 rx4 log(d/l)2 4

log(1/)

,

s

log(d/l)/(l

2)),

then

sup J^n(w)-J (w)  cg g

w

2

lcq

s log(dn/l)+log(12/) n

holds with probability at least 1 - , where g = max  r2l-1, r2l-1, rl-1 .

C.3.2 PROOF OF THEOREM 2
Proof. Suppose that {w(1), w(2), · · · , w(m)} are the non-degenerate critical points of J (w). So for any w(k), it obeys

inf
i

ik

2J (w(k))

 ,

where ki 2J (w(k)) denotes the i-th eigenvalue of the Hessian 2J (w(k)) and  is a constant. We further define a set D = {w  Rd | J (w) 2  and infi |i 2J (w(k)) |  }. According
to Lemma 4, D = k=1Dk where each Dk is a disjoint component with w(k)  Dk for k  m and Dk does not contain any critical point of J (w) for k  m + 1. On the other hand, by the continuity of J (w), it yields J (w) 2 = for w  Dk. Notice, we set the value of blow which is actually a function related to n.

Then by utilizing Theorem 1, we let sample number n sufficient large such that

sup J^n(w) - J (w)  zg

w

2

2

holds with probability at least 1 - , where if n  cg

max(

cq

s

l3 r2 rx4 log(d/l)2 4

log(1/)

,

s

log(d/l) l 2

),

zg

=

cg g

lcq

s

log(dn/l)+log(12/) n

.

31

Under review as a conference paper at ICLR 2018

This further gives that for arbitrary w  Dk, we have

inf tJ^n(w) + (1 - t)J (w) = inf t J^n(w) - J (w) + J (w)

wDk

2 wDk

2

 inf
wDk

J (w) 2 - sup t
wDk

J^n(w) - J (w)

2

. 2
Similarly, by utilizing Lemma 11, let n be sufficient large such that

(20)

sup
w

2J^n(w) - 2J (w)

 op  zs  2

holds

with

probability

at

least

1

-

,

where

if

n



ch2

max(



2

l2

2p r2 h2 2s log(d/l)

,

s

log(d/l)/(l

2

)),

zs = ch1  lh

s

log(nl)+log(20/) n

.

Assume that b  Rd is a vector and satisfies bT b = 1. In this case, we can bound ik 2J^n(w) for arbitrary w  Dk as follows:

inf
wDk

ik

2J^n(w)

= inf min bT 2J^n(w)b
wDk bT b=1

= inf min bT 2J^n(w) - 2J (w) b + bT 2J (w)b
wDk bT b=1

 inf min bT 2J (w)b - min bT 2J^n(w) - 2J (w) b

wDk bT b=1

bT b=1

 inf min bT 2J (w)b - max bT 2J^n(w) - 2J (w) b

wDk bT b=1

bT b=1

=

inf
wDk

inf
i

|ki

2f (w(k), x)

|-

2J^n(w) - 2J (w) op

 . 2
This means that in each set Dk, 2J^n(w) has no zero eigenvalues. Then, combine this and Eqn. (20), by Lemma 3 we know that if the population risk J (w) has no critical point in Dk, then the empirical risk J^n(w) has also no critical point in Dk; otherwise it also holds. By Lemma 3, we can also obtain that in Dk, if J (w) has a unique critical point w(k) with non-degenerate index sk, then J^n(w) also has a unique critical point w(nk) in Dk with the same non-degenerate index sk. The first conclusion is
proved.

Now we bound the distance between the corresponding critical points of J (w) and J^n(w). Assume that in Dk, J (w) has a unique critical point w(k) and J^n(w) also has a unique critical point wn(k). Then, there exists t  [0, 1] such that for any z  Bd(1), we have

 J (wn(k)) 2

= max
zT z=1

J (wn(k)), z

= max J (w(k)), z
zT z=1

+

2J (w(k) + t(wn(k) - w(k)))(wn(k) - w(k)), z

x


2 1/2
2J (w(k)) (wn(k) - w(k)), (wn(k) - w(k))

y


wn(k) - w(k)

2,

where x holds since J (w(k)) = 0 and y holds since w(k) + t(wn(k) - w(k)) is in Dk and for any w  Dk we have infi |i 2J (w) |  . Consider the conditions in Lemma 11 and Theorem 1,

we

can

obtain

that

if

n



ch

max(

cq s

l3 r2 rx4 log(d/l)2 4

log(1/) ,

s

log(d/l)/ 2 )

where

ch

is

a

constant,

then

wn(k) - w(k)

2

2cg g 

lcq

s log(dn/l)+log(12/) n

32

Under review as a conference paper at ICLR 2018

holds with probability at least 1 - .

C.3.3 PROOF OF THEOREM 3

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in

(Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /d

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th
layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Now we use the decomposition strategy to bound our goal:

J^n(w) - J (w) =

1 n

n

f (w, x(i)) - E(f (w, x))

i=1

1n 1n

= n

f (w, x(i))-f (wkw , x(i))

+ n

f (wkw , x(i))-Ef (wkw , x)+Ef (wkw , x)-Ef (w, x)

i=1 i=1

1 n 1n

 n

f (w, x(i))-f (wkw , x(i))

+ n

f (wkw , x(i))-Ef (wkw , x) + Ef (wkw , x)-Ef (w, x) .

i=1 i=1

Then, we define four events E0, E1, E2 and E3 as

E0 = E1 = E2 =

sup J^n(w) - J (w)  t ,
w

1n

t

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

 3

,

1n t

sup
jw[n j ],j=[l]

n f (wkw , x(i))-E(f (wkw , x))  3
i=1

,

t

E3 =

sup
w

E(f (wkw , x))-E(f (w, x))

 3

.

Accordingly, we have P (E0)  P (E1) + P (E2) + P (E3) .
So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).

Step 1. Bound P (E1): We first bound P (E1) as follows:

1n

t

P (E1) =P

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

 3

x3 1 n

tE

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))



3 t

E

1
sup n
w

n i=1

f (w, x(i)) - f (wkw , x(i))

w - wkw 2

sup w - wkw 2
w

3 t

E

sup J^n(w, x)

w

2

,

33

Under review as a conference paper at ICLR 2018

where x holds since by Markov inequality, we have that for an arbitrary nonnegative random variable

x, then

P(x



t)



E(x) . t

Now we only need to bound E supw J^n(w, x) . Therefore, by Lemma 10, we have
2

E sup J^n(w, x)

w

2

=E

sup
w

1 n

n
f (w, x(i))

i=1

2

= E sup f (w, x) 2
w



 g

.

where

g

=

ct lrx4 r4l-2 .

Therefore,

we

have P (E1)



3

g . t

We further let

t  6 g . 

Then we can bound P(E1):

P(E1)



 .
2

Step 2. Bound P (E2): Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd ). We can bound P (E2) as follows:

P (E2) =P

sup
jw[nj ] j[l]

1 n

n

f (wkw , x(i)) - E(f (wkw , x))

i=1

t 3


l
 exp  sj log
j=1


3rd  sup P
jw[nj ] j[l]

1n

t

n

f (wkw , x(i)) - E(f (wkw , x))

 3

i=1





x 3dr s

t2 t

4

exp -cf n min  9f2 max

dlf2 4,  2

, 3f2 2  ,

where x holds because in Lemma 7, we have





P

1n n

f (w, x(i))-E(f (w, x(i))) > t

i=1

 2 exp-cf n min f2 max

t2 dlf2 4,  2

t , f2 2 ,

where cf is a positive constant and f = rl. Thus, if we set



t  max 

9f2(s log(3rd/ ) + log(8/)) max

dlf2 4,  2

,

3f2 2(s log(3rd/

) + log(8/)) ,

 cf n

cf n



then we have

P (E2)



 .
2

Step 3. Bound P (E3): We first bound P (E3) as follows:

t

P (E3) =P

sup
w

E(f (wkw , x)) - E(f (w, x))

2 3

=P

sup
w

E (f (wkw , x) - f (w, x) w - wkw 2

2) sup
w

w - wkw

2



t 3

P

E sup
w

Jw(w, x)

2



t 3

x
P

 g

t 3

,

34

Under review as a conference paper at ICLR 2018

where x holds since we utilize Lemma 10. We set enough small such that g < t/3 always

holds. Then it yields P (E3) = 0.

Step 4. Final result: To ensure P(E0)  , we just set

=

3rl/n.

Note

that

6g 

> 3g . Thus

we can obtain



t  max6g ,

9f2(s log(3rd/

)+log(8/)) max

dlf2 4,  2

,

3f2 2(s log(3rd/

)+log(8/)) 



cf n

cf n



= max18lgr ,  n



9f2(s log(dn/l)+log(8/)) max

dlf2 4,  2

,

3f2 

2(s

log(dn/l)+log(8/)) 

.

cf n

cf n



Note that we have g = ctlrx4r4l-2 where ct is a constant. Then Then there exist four universal

constants cf and cf such that if n  cf max

dl

s

l3 rx4 log(d)2 4

log(1/)

,

s

log(d)/(

2

dl

)

, then

sup J^n(w) - J (w)  cf f  max

w

2

holds with probability at least 1 - .

dlf , 1

s log(dn/l) + log(8/) n

C.3.4 PROOF OF COROLLARY 1

Proof. By Lemma 5, we know s = g. Thus, the remaining work is to bound s. Actually, we can have

1 ESD,A,(x(j),y(j))D n

n

fj (wj ;x(j),y(j))-fj (wn;x(j),y(j))

j=1

ESD

sup J^n(w) - J (w)
w

 sup J^n(w) - J (w)
w
 f.

Thus, we have g = s  f . The proof is completed.

C.4 PROOF OF OTHER LEMMAS

C.4.1 PROOF OF LEMMA 13

Lemma 15. (Rigollet, 2015) Suppose a random variable x is  2-sub-Gaussian, then the random variable x2 - Ex2 is sub-exponential and obeys:

E exp  x2 - Ex2

 exp 2562 4 2

,

||



1 16 2 .

(21)

Proof. Here we utilize Lemma 15 to prove our conclusion. We have

kk

E exp 

aixi2 - E

aix2i

i=1 i=1

k
=x E exp ai xi2 - Ex2i

i=1

y


k

E exp

1282ai2i4

,

i=1

||



1 maxi ai 2

k

E exp 1282 4

ai2

,

i=1

where x holds since xi are independent and y holds because of Lemma 15.

35

Under review as a conference paper at ICLR 2018

C.4.2 PROOF OF LEMMA 14

Proof. Since the 2-norm of each w(j) is bounded, i.e. w(j) 2  r (1  j  l), we can obtain

Bs:t

2 F



2
W (s)
F

2
W (s-1) · · ·
F

2
W (t)  r2(t-s+1)
F

r2

x


max

r2, r2l

,

where x holds since the function r2x obtains its maximum at two endpoints x = 1 and x = l for case

r < 1 and r  1, respectively. On the other hand, we have Bs:t op  Bs:t F  r. Specifically,

we have

Bl:1

2 F

 r2l

f2 .

D PROOFS FOR DEEP NONLINEAR NEURAL NETWORKS

In this section, we first present the technical lemmas in Sec. D.1. Then in Sec. D.2 we give the proofs of these lemmas. Next, we utilize these technical lemmas to prove the results in Theorems 4  6 and
Corollary 2 in Sec. D.3. Finally, we give the proofs of other lemmas in Sec. D.4.

D.1 TECHNICAL LEMMAS

Here we present the key lemmas and theorems for proving our desired results. For brevity, we define an operation G which maps an arbitrary vector z  Rk into a diagonal matrix G(z)  Rk×k with its i-th diagonal entry equal to (zi)(1 - (zi)) in which zi denotes the i-th entry of z. We further define Ai  Rdi-1×di as follows:

Ai = (W (i))T G(u(i))  Rdi-1×di (i = 1, · · · , l),

(22)

where W (i) is the weight matrix in the i-th layer and u(i) is the linear output of the i-th layer. In this section, we define

Bs:t = AsAs+1 · · · At  Rds-1×dt , (s  t) and Bs:t = I, (s > t).

(23)

Lemma 16. Suppose that the activation function in deep neural network are sigmoid functions. Then the gradient of f (w, x) with respect to w(j) can be formulated as

w(j) f (w, x) = vec G(u(j))Bj+1:l(v(l) - y) (v(j-1))T , (j = 1, · · · , l - 1),

and w(l) f (w, x) = vec
Besides, the loss f (w, x) is -Lipschitz,

G(u(l))(v(l) - y) (v(l-1))T .

wf (w, x) 2  ,

where  =

1 16

cy cd

(1

+

cr (l

-

1))

in

which

cy ,

cd

and

cr

are

defined

as

v(l) - y

2 2



cy

< +,

cd = max(d0, d1, · · · , dl)

and cr = max

r2 ,
16

r2 l-1 16

.

Lemma 17. Suppose that the activation functions in deep neural network are sigmoid functions. Then there exists two universal constants cs1 and cs2 such that
2wf (w, x) op  w2 f (w, x) F  ,

where  =

cs1 crcd2l4 in which cd = maxi di and cr = max

r2 16

,

r2 16

l-1

. Moreover, the

gradient wf (w, x) is -Lipschitz, i.e.

wf (w1, x) - wf (w2, x) 2  w1 - w2 2.

Similarly, there also exist a universal constant  such that

3wf (w, x) op  w3 f (w, x) F  .

36

Under review as a conference paper at ICLR 2018

Lemma 18. Suppose that the activation function in deep neural network are sigmoid functions. Then we have
wxf (w, x) op  wxf (w, x) F ,

where  =

26 38

l(l

+

2)cy cr cd

(lcr

+

1)

in

which

cy ,

cd

and

cr

are

defined

in

Lemma

16.

Lemma 19. Suppose that the input sample x obeys Assumption 2 and the activation functions in deep neural network are sigmoid functions. The gradient of the loss is 82 2-sub-Gaussian. Specifically, for any   Rd, we have

E ( , wf (w, x) - Ewf (w, x) )  exp

82 2



2 2

2

,

where  =

26 38

l(l

+

2)cy cr cd

(lcr

+

1)

in

which

cy ,

cd

and

cr

are

defined

in

Lemma

16.

Lemma 20. Suppose that the input sample x obeys Assumption 2 and the activation functions in

deep neural network are sigmoid functions. The Hessian of the loss, evaluated on a unit vector, is

sub-Gaussian.

Specifically,

for

any

unit





d-1
S

(i.e.

 2 = 1), there exist universal constant 

such that

E t , 2wf (w, x) - E2wf (w, x) 

 exp

8t22 2 2

.

Notice,  obeys   xw2 f (w, x) op.

Lemma 21. Assume that the input sample x obeys Assumption 2 and the activation functions in

deep neural network are sigmoid functions. Then the sample Hessian uniformly converges to the

population Hessian in operator norm. That is, there exists such two universal constants cm and cm

such

that

if

n



,cm 2l2r2
2 22s log(d) log(1/)

then

sup
w

2J^n(w)-2J (w)

op  cm

s log(dn/l)+log(4/) n

holds with probability at least 1 - . Here  is the same parameter in Lemma 20.

D.2 PROOFS OF TECHNICAL LEMMAS

For brevity, we also define

Ds:t =

W (s)

2 F

··

·

W (t)

2 F

(s



t)

and

Ds:t = 1, (s > t).

We define a matrix Pk  Rdk2 ×dk whose ((s - 1)dk + s, s) (s = 1, · · · , dk) entry equal to

(u(sk))(1 - (u(sk)))(1 - 2(us(k))) and rest entries are all 0. On the other hand, since the values in

v(l) belong to the range [0, 1] and y is the label,

v(l) - y

2 2

can

be

bounded:

v(l) - y

2 2



cy

<

+,

where cy is a universal constant. We further define cd = max(d0, d1, · · · , dl).

Then we give a lemma to summarize the properties of G(u(i)) defined in Eqn. (22), Bs:t defined in Eqn. (23), Ds:t and Pk.
Lemma 22. For G(u(i)) defined in Eqn. (22), Bs:t defined in Eqn. (23), Ds:t and Pk, we have the following properties:

(1) For arbitrary matrices M and N of proper sizes, we have

G(u(i))M

2 F



1 16

M

2 F

and

N G(u(i))

2 F

1 16

N

2 F

.

(2) For arbitrary matrices M and N of proper sizes, we have

Pk M

2 F



26 38

M

2 F

and

N Pk

2 F



26 38

N

2 F

.

37

Under review as a conference paper at ICLR 2018

(3) For arbitrary matrices M and N of proper sizes, we have

Bs:t

2 F

1  16t-s+1 Ds:t

and

1 16t-s+1 Ds:t  cst  cr,

where cst =

r 4

2(t-s+1) and cr = max

r2 16

,

r2 16

l-1

.

(4) For arbitrary matrices M , N and I of proper sizes, let m = vec (M ). Then we have

(N

 I)m

2 F



M

2 F

N

2 F

and

(I  N )m

2 F



M

2 F

N

2 F

.

It should be pointed out that we defer the proof of Lemma 22 to Sec. D.4.

D.2.1 PROOF OF LEMMA 16
Proof. We use chain rule to compute the gradient of f (w, x) with respect to w(j). We first compute several basis gradient. According to the relationship between u(j), v(j), W (j) and f (w, x), we have

v(l) f (w, x) = v(l) - y,

v(i) f (w, x)

=

 u(i+1) v(i)

f (w, x)  u(i+1)

=

(W (i+1))T

f (w, x) u(i+1) ,

u(i) f (w, x)

=

v(i) u(i)

f (w, x) v(i)

=

G(u(i)) f (w, x) , v(i)

u(i) W (i) f (w, x) = w(i)

f (w, x) u(i)

T
= v(i-1)

f (w, x) u(i)

T
,

(i = 1, · · · , l - 1),

(i = 1, · · · , l),

(24)

(i = 1, · · · , l).

Then by chain rule, we can easily compute the gradient of f (w, x) with respect to w(j) which is formulated as

T

w(j) f (w, x) = vec v(j-1) G(u(j))Aj+1Aj+2 · · · Al(v(l) - y)

, (j = 1, · · · , l - 1),

and

T

w(l) f (w, x) = vec v(l-1) G(u(l))(v(l) - y)

.

Besides, since the values in v(l) belong to the range [0, 1]. Combine with Lemma 22, we can bound wf (w, x) 2 as follows:

l

wf (w, x)

2 2

=

w(j) f (w, x)

2 2

j=1

=

v(l-1)

T
G(u(l))(v(l) - y)

2 l-1
+

v(j-1)

T
G(u(j))Bj+1:l(v(l) - y)

2

F j=1

F

1  16 dl-1

v(l) - y

2
+

1

2 16

2 l-1

v(l) - y

dj-1

2

Bj+1:l

2 F

j=1

x1 1  16 cycd + 16 cycdcr(l - 1),

where cy, cd, cr are defined as

v(l) - y

2 2



cy ,

cd = max(d0, d1, · · · , dl)

and

cr = max

r2 ,
16

r2 l-1 16

.

Notice, x holds since in Lemma 22, we have

Bs:t

2 F



r

2(t-s+1)
 max

4

r2 ,
16

r2 l-1 16

.

38

Under review as a conference paper at ICLR 2018

Thus, we can obtain wf (w, x) 2 
The proof is completed.

1 16

cy

cd

(1

+

cr

(l

-

1))

.

D.2.2 PROOF OF LEMMA 17

For convenience, we first give the computation of some gradients.
Lemma 23. Assume the activation functions in deep neural network are sigmoid functions. Then the following properties hold:

(1)

We

can

compute

the

gradients

f (w,x)  u(i)

and

f (w,x)  v (i)

as

f (w, x) u(i)

=

G(u(i))Bi+1:l(v(l)

- y)

and

f (w, x) v(i)

=

Bi+1:l(v(l)

-

y).

(2)

We

can

compute

the

gradient

 u(i)  w(j )

as

u(i) = (v(j-1))T  w(j)

G(u(j))Bj+1:i-1(W (i))T

u(i) w(i)

= (v(i-1))T

 Idi



R ,di×didi-1

(i = j).

T
 R ,di×djdj-1 (i > j).

(3)

We

can

compute

the

gradient

 v (i)  w(j )

as

v(i) = (v(j-1))T  w(j)

T
G(u(j))Bj+1:i  R ,di×djdj-1 (i  j).

It should be pointed out that the proof of Lemma 23 can be founded Sec. D.4.

Proof. To prove our conclusion, we have two steps: computing the Hessian and bounding its operation norm.

Step

1.

Compute

the

Hessian:

We

first

consider

the

computation

of

:2f (w,x)
 w(Ti)  w(j )

2f (w, x)  vec w(Ti)w(j) =

G(u(j))Aj+1Aj+2 · · · Al(v(l) - y) (v(j-1))T w(Ti)

.

Recall that we define

Bs:t = AsAs+1 · · · At  Rds-1×dt , (s  t) and Bs:t = I, (s > t).

Then we have

2f (w, x) w(Ti)w(j) =

v(j-1)(v(l)

-

y)T

B

T j+1:l

 Idj

vec G(u(j))

w(Ti)

(

Q1ij )

l
+ v(j-1)(v(l) - y)T BkT+1:l  G(u(j))Bj+1:k-1WkT
k=j+1

vec G(u(k))

w(Ti)

(

Qi2j )

+ v(j-1)(v(l) - y)T BiT+1:lG(u(i))  G(u(j))Bj+1:i-1

vec WiT w(Ti)

(

Qi3j )

+ v(j-1)  G(u(j))Bj+1:l

(v(l) - y) w(Ti) (

Q4ij )

+ Idj-1 

G(u(j))Bj+1:l(v(l) - y)

 v (j -1) w(Ti) (

Q5ij )

39

Under review as a conference paper at ICLR 2018

Case

I:

i

>

j.

We

first

consider

the

case

that

i>j.

In

this

is

case,

Qi1j = 0

since

vec(G(u(j)))
 w(Ti)

=

0.

Computing

Q2ij

needs

more

efforts.

By

utilizing

the

computation

of

 u(k)  w(i)

in

Lemma

23,

we

have

vec G(u(k)) w(i)

vec G(u(k)) = u(k)

u(k) w(i) = Pk

v(i-1))T 

G(u(i))Bi+1:k-1(W (k))T

T

, (k > i)

where Pk is a matrix of size dk2 × dk whose ((s - 1)dk + s, s) (s = 1, · · · , dk) entry equal to (us(k))(1 - (us(k)))(1 - 2(u(sk))) and rest entries are all 0. When k = i,

vec G(u(k)) w(k)

vec G(u(k)) u(k) = u(k) w(k) = Pk

(v(k-1))T  Idk

 R .d2k×dkdk-1

Note

that

for

k

<

i,

we

have

 G(u(k) )
 w(i)

=

0.

For

brevity,

let

Dk

v(j-1)(v(l) - y)T BkT+1:l  G(u(j))Bj+1:k-1WkT (k = i, · · · , l).

(25)

Therefore, we have

Q2ij = DiPi (v(i-1))T  Idi

l
+ DkPk
k=i+1

(v(i-1))T  G(u(i))Bi+1:k-1(W (k))T T

.

Then we consider Qi3j.

Qi3j = v(j-1)(v(l) - y)T BTi+1:lG(u(i))  G(u(j))Bj+1:i-1 .

Also

we

can

use

the

computation

of

 v (l)  w(i)

in

Lemma

23

and

compute

Qi4j

as

follows:

Q4ij =v(j-1) 

G(u(j))Bj+1:l

(v(l) - y) w(Ti)

= v(j-1)  G(u(j))Bj+1:l

(v(i-1))T  G(u(i))Bi+1:l T

.

Finally, since i > j, we can compute Q5ij = 0.

Case

II:

i

=

j.

We

first

consider

: G(u(k) )
 w(k)

vec G(u(k)) w(Tk)

vec G(u(k)) u(k) = u(k) w(Tk) = Pk

(v(k-1))T  Idk

 R ,d2k×dkdk-1

where Pk is a matrix of size d2k ×dk whose (s, (s-1)dk +s) entry equal to (us(k))(1-(us(k)))(1- 2(u(sk))) and rest entries are all 0. Q1jj can be computed as

Q1jj =

v(j-1)(v(l) - y)T BjT+1:l

 Idj

vec G(u(j)) w(Tj)

= v(j-1)(v(l) - y)T BTj+1:l  Idj

Pj (v(j-1))T  Idj .

As for Qj2j, by Eqn. (25) we have

l

Qj2j =

Dk Pk

k=j+1

v(j-1))T  G(u(j))Bj+1:k-1(W (k))T T

.

Since i = j, Qj3j does not exist. For convenience, we just set Qj3j = 0.

40

Under review as a conference paper at ICLR 2018

Now we consider Q4jj which can be computed as follows:

Q4jj =v(j-1) 

G(u(j))Bj+1:l

(v(l) -y) w(Tj)

= v(j-1)  G(u(j))Bj+1:l

T
(v(j-1))T  G(u(j))Bj+1:l

.

Finally, since i = j, we can compute Qj5j = 0.

Case

III:

i

<

j.

Since

2f (w,x) wwT

is

symmetrical,

we

have

Qkij

=

Qkji

(k

=

1, · · ·

, 5).

Step 2. Bound the operation norm of Hessian: We mainly use Lemma 22 to achieve this goal. From Lemma 22, we have

(1) For arbitrary matrices M and N of proper size, we have

G(u(i))M

2 F



1 16

M

2 F

and

N G(u(i))

2 F

1 16

N

2 F

.

(2) For arbitrary matrices M and N of proper size, we have

Pk M

2 F



26 38

M

2 F

and

N Pk

2 F



26 38

N

2 F

.

(3) For Bs:t and Ds:t, we have

Bs:t

2 F



1 16t-s+1 Ds:t

and

1 16t-s+1

Ds:t



cr ,

where cr = max

r2 16

,

r2 16

l

.

(4) For arbitrary matrices M , N and I of proper sizes, let m = vec (M ). Then we have

(N

 I)m

2 F



M

2 F

N

2 F

and

(I

 N )m

2 F



M

2 F

N

2 F

.

The values of entries in v(h) are bounded by 0  (u(hi))  1 which leads to

v(h)

2 F

 dh



cd,

where cd = maxi di. On the other hand, since the values in v(l) belong to the range [0, 1] and y is

the label,

v(l) - y

2 2

can

be

bounded:

v(l) - y

2 2



cy

<

+,

where cy is a universal constant.

We first define

Ckij = DkPk

v(i-1))T 

T
G(u(i))Bi+1:k-1(W (k))T

and Cij =DiPi (v(i-1))T  Idi =

v(i-1)  Idi (DiPi)T T =x v(i-1)  (DiPi)T T

=(v(i-1))T  (DiPi) , where Dk is defined in Eqn. (25). x holds since for an arbitrary vector u  Rk and an arbitrary matrix M  Rk×k, we have (u  Ik) M = u  M .

Case I: i > j. According to the definition of Cij and Ckij, we have Q2ij = Cij +

l k=i+1

Ckij .

So

we have

2

2f (w, x) w(Ti)w(j)

=
F

Qi1j + Q2ij + Q3ij + Q4ij + Q5ij

2 F

l2

= Cij +

Ckij + Q3ij + Q4ij

k=i+1

F

=(l - i + 3)

C ij

2 F

+

l

k=i+1

Ckij

2
+
F

Q3ij

2
+
F

Q4ij

2 F

.

41

Under review as a conference paper at ICLR 2018

Here we bound each term separately:

C ij

2 F



v(j-1)

2 F

2
v(l) - y
F

Bi+1:l

2 F

1 16

Bj+1:i-1WiT



26 38

cy

dj-1di-1

1 16l-i

Di+1:l

1 16i-j

Dj

+1:i



26 38

cy

dj-1di-1

1 16l-j

Dj

+1:l

26  38 cydj-1di-1cr.

2 26 F 38

v(i-1) 2
F

Similarly, we can bound

Ckij

2 F

as

follows:

Ckij

2 F



v(j-1)

2 F

v(l) -y

2 F

Bk+1:l

21 F 16

Bj+1:k-1WkT

2 26 F 38

v(i-1) 2 1 F 16



26 38

cy

dj

-1di-1

1 16l-k

Dk+1:l

1 16k-j-1

Dj+1:k

1 16k-i-1

Di+1:k

26 1 1 = 38 cydj-1di-1 16l-j-1 Dj+1:l 16k-i-1 Di+1:k



214 38

cy

dj-1

di-1

c2r .

Bi+1:k-1(W (k))T

2 F

We also bound

Qi3j

2
as
F

Qi3j

2

F

v(j-1) 2
F

2
v(l) - y

1

F 16

Bi+1:l

2 F

1 16

Bj+1:i-1

2 F



1 28 cydj-1cr.

Finally, we bound

Qi4j

2
as follows:
F

Qi4j

2

F

v(j-1)

21 F 16

Bj+1:l

2 F

v(i-1)

21 F 16

Bi+1:l

2 F



1 28

dj-1di-1

c2r

.

2

Note that di  cd. Thus, we can bound

as2f (w,x)
w(j)w(Ti) F

2f (w, x) 2

w(Ti)w(j) F

(l - i + 3)

26 38 cydj-1di-1cr +

l

214 38

cy

dj

-1di-1c2r

+

1 28 cydj-1cr

+

1 28

dj-1

di-1c2r

k=i+1

(l + 1)

64 6561

cy

c2d

cr

+

4096 6561 cy(l

-

2)c2dc2r

+

1 256 cycdcr

+

1 256

cd

c2r

.

Case II: i = j. According to the definition of Cij and Ckij, we have Qj2j =

l k=j+1

Ckjj

.

Similarly, we have

2f (w, x) w(Ti)w(j)

2
=
F

Q1jj + Q2jj + Qj3j + Qj4j + Qj5j

2
=
F

l

Q1jj +

Ckjj + Q4ij

k=j+1



(l - j + 2) 

Qj1j

2
+
F

l

Ckjj

2
+
F

Q4jj

2
.
F

k=j+1

2 F

42

Under review as a conference paper at ICLR 2018

Thus, we can bound

Q1jj

2
first:
F

Qj1j

2

F

v(j-1) 2
F

2
v(l) - y
F

Bj+1:l

2 F

26 38

v(j-1)

2 F



26 38

cy

dj2-1

cr

.

As for Qj2j, we have

Ckij

2 F



v(j-1)

2 F

v(l) - y

2 F

Bk+1:l

21 F 16

Bj+1:k-1WkT

2 26 F 38

=

26 38

cy

d2j-1

1 16l-k

Dk+1:l

1 16k-j

-1

Dj+1:k

1 16k-j-1

Dj+1:k



214 38

cy

dj2-1

c2r

.

v(j-1)

21 F 16

Bj+1:k-1(W (k))T

2 F

Then we bound

Q4jj

2 F

:

Q4jj

2

F

v(j-1)

21 F 16

Bj+1:l

2 F

v(j-1)

21 F 16

Bj+1:l

2 F



1 28

dj2-1cr2.

Note that for any input, we have cv

=

maxj

v(j-1)(v(l) - y)T

2 F



maxj

v(j-1)

2 F

(v(l) - y

2 F

 cycd, where

v(l) - y

2 F

can be bounded by a constant cy. Thus, we can bound

2

as2f (w,x)
w(Ti)w(j) F

2f (w, x) w(Ti)w(j)

2
(l - i + 3)
F

26 38

cy dj2-1 cr

+

l

214 38

cy

d2j-1

cr2

+

1 28

d2j-1cr2

k=i+1

(l + 2)

64 6561

cy

cd2 cr

+

4096 6561 cy(l

-

1)cd2 c2r

+

1 256

c2d

c2r

.

Case

III:

i

<

j.

Since

2f (w,x) wwT

is

symmetrical,

we

have

Qikj

=

Qjki

(k

=

1, · · ·

, 5).

Thus,

it

yields

2

2f (w, x) w(Ti)w(j)

(l + 1)
F

64 6561

cy

cd2 cr

+

4096 6561 cy(l

-

2)cd2 cr2

+

1 256 cycdcr

+

1 256

cdc2r

.

Final result: Thus we can bound

2wf (w, x) op  2wf (w, x) F



(l - 1)l max
i,j:i=j

2
2f (w, x)

l

w(j)w(Ti)

+
F j=1

2f (w, x) w(j)w(Ti)

2 F

 (l - 1)l(l+1)

64 6561

cy

cd2 cr

+

4096 6561

cy (l

-

2)cd2c2r +

1 256 cycdcr

+

1 256

cdcr2

+(l + 2)

64 6561

cy

c2dcr

+

4096 6561 cy(l

-

1)lcd2 cr2

+

1 256

lcd2 cr2

1 2

 cs1 crcd2l4,
where cs1 and cs2 are two constants. Since w2 f (w, x) op  w2 f (w, x) F , we know that the gradient wf (w, x) is -Lipschitz, where  = cs1 crc2dl4.

43

Under review as a conference paper at ICLR 2018

On the other hand, since for any input x, (x) belongs to [0, 1], the values of the entries of w3 f (w, x) can be bounded. Thus, we can bound
3wf (w, x) op = sup 3 , w3 f (w, x) = [w3 f (w, x)]ijkijk   < +.
 21
We complete the proof.

D.2.3 PROOF OF LEMMA 18

For convenience, we first give the computation of some gradients.

Lemma 24. Assume the activation functions in deep neural network are sigmoid functions. Then we

can compute the gradients

 u(j )  u(1)

and

 v (j )  u(1)

as

u(j) u(1) = v(j) u(1) =

T
G(u(1))A2 · · · Aj-1(W j )T  Rdj×d1 , (j > 1). G(u(1))A2 · · · Aj T  Rdj×d1 , (j > 1).

It should be pointed out that the proof of Lemma 24 can be founded Sec. D.4.

Proof. To prove our conclusion, we have two steps: computing xwf (w, x) and bounding its operation norm.

Step 1. Compute xwf (w, x):

We

first

consider

the

computation

of

2f (w,x) xT w(j)

:

2f (w, x)  vec xT w(j) =

G(u(j))Aj+1Aj+2 · · · Al(v(l) - y) (v(j-1))T xT

.

Recall that we define
Ai = (W (i))T G(u(i))  Rdi-1×di . Bs:t = AsAs+1 · · · At  Rds-1×dt , (s  t) and Bs:t = I, (s > t).

Then we have

2f (w, x) xT w(j) =

v(j-1)(v(l) - y)T BjT+1:l

 Idj

vec G(u(j)) xT (

Qj1)

l
+

v(j-1)(v(l) - y)T BTk+1:l  G(u(j))Bj+1:k-1WkT

k=j+1

+ v(j-1)  G(u(j))Bj+1:l

(v(l) - y) xT (

Qj3)

+ Idj-1 

G(u(j))Bj+1:l(v(l) - y)

 v (j -1) xT (

Qj4)

By using Lemma 24, we can compute Q1ij as

vec G(u(k)) xT (

Q2j )

vec G(u(k)) xT

vec G(u(k)) u(k) = u(k) xT = Pk

G(u(1))B2:k-1(W k)T

T
.

Thus, we have

Q1j = =

v(j-1)(v(l)

-

y)T

B

T j+1:l

vec G(u(j))

 Idj

xT

v(j-1)(v(l) - y)T BjT+1:l  Idj Pk G(u(1))B2:k-1(W k)T

T
.

44

Under review as a conference paper at ICLR 2018

As for Qj2, we also can utilize Lemma 24 to compute it:

l
Qj2 =

v(j-1)(v(l)

-

y)T

B

T k+1:l



G(u(j))Bj+1:k-1WkT

k=j+1

vec G(u(k)) xT

l
=
k=i+1

v(j-1)(v(l)

-

y)T

B

T k+1:l



G(u(j))Bj+1:k-1WkT

T
Pk G(u(1))B2:k-1(W k)T .

Then we consider Qi3j.

Qj3 = v(j-1) 

G(u(j))Bj+1:l

(v(l) - y) xT =

v(j-1) 

G(u(j))Bj+1:l

T
G(u(1))B2:l .

Q4j can be computed as follows:

Qj4 = Idj-1  G(u(j))Bj+1:l(v(l) -y)

 v (j -1) xT =

Idj-1  G(u(j))Bj+1:l(v(l) -y)

T
G(u(1))B2:j .

Step 2. Bound the operation norm of Hessian: We mainly use Lemma 22 to achieve this goal. From Lemma 22, we have

(1) For arbitrary matrices M and N of proper size, we have

G(u(i))M

2 F



1 16

M

2 F

and

N G(u(i))

2 F

1 16

N

2 F

.

(2) For arbitrary matrices M and N of proper size, we have

Pk M

2 F



26 38

M

2 F

and

N Pk

2 F



26 38

N

2 F

.

(3) For Bs:t and Ds:t, we have

Bs:t

2 F



1 16t-s+1 Ds:t

and

1 16t-s+1

Ds:t



cr ,

where cr = max

r2 4

,

r2 16

l-1

.

(4) For arbitrary matrices M , N and I of proper sizes, let m = vec (M ). Then we have

(N

 I)m

2 F



M

2 F

N

2 F

and

(I

 N )m

2 F



M

2 F

N

2 F

.

The values of entries in v(h) are bounded by 0  (uh(i))  1 which leads to

v(h)

2 F

 dh



cd,

where cd = maxi di. On the other hand, since the values in v(l) belong to the range [0, 1] and y is

the label,

v(l) - y

2 2

can

be

bounded:

v(l) - y

2 2



cy

<

+,

where cy is a universal constant.

We first define Ckj = v(j-1)(v(l) - y)T BkT+1:l  G(u(j))Bj+1:k-1WkT

T
Pk G(u(1))B2:k-1(W k)T .

Then we have Q2j =

l k=j+1

Ckj .

So

we

have

2f (w, x) xT w(j)

2
=
F

Q1j + Q2j + Qj3 + Qj4

2
=
F

2 l

Q1j +

Ckj + Q3j + Q4j

k=j+1

F



=(l - j + 3) 

Qj1

2
+
F

l

Ckj

2
+
F

Qj3

2
+
F

Qj4

2
.
F

k=j+1

45

Under review as a conference paper at ICLR 2018

Then we bound each term separately:

Qj1

2

F

v(j-1) 2
F

2
v(l) - y
F

Bj+1:l

2 F

26 1 38 16

B2:k-1(W k)T

2 F



26 38

cy

dj -1 c2r .

Similarly, we bound

Ckj

2
:
F

Ckj

2
=
F

v(j-1)

2 F

2
v(l) - y
F

Bk+1:l

2 F

1 16

Bj+1:k-1WkT

26 1

1

1

= 38 cydj-1 16l-k Dk+1:l 16k-j-1 Dj+1:k 16k-1 D2:k



26 38

cy

dj-1cr2

.

2 26 1 F 38 16

B2:k-1(W (k))T

2 F

We also bound

Q3ij

2
as
F

Qi3j

2

F

v(j-1) 2 1 2 16

Bj+1:l

2 F

1 16

B2:l

2 F



1 28

dj-1

cr2

.

Finally, we bound

Q4j

2
as follows:
F

Q4j

21 =
F 16

Bj+1:l

2 F

v(l) - y

2 F

1 16

B2:j

2 F



1 28 cycr.

Since cd = maxi di, we can bound

2f (w,x)  w(j )  xT

2
as
F



2f (w, x) xT w(j)

2 F

(l

-

j

+

26 3)38

cy

dj-1c2r

l
+
k=j+1

26 38

cy

dj-1

c2r

+

1 28 cydj-1cr

+

1 28 cycr



(l

+

2)

26 38

cy dj -1 c2r

+

l

26 38

cy

dj

-1c2r

+

1 28 cydj-1cr

+

1 28 cycr .

k=j+1

Final result: Thus we can bound

wxf (w, x) op  wxf (w, x) F 

l 2f (w, x) 2 j=1 w(j)xT F





l

(l

+

2)

26 38

cy dj -1 cr2

+

l

26 38

cy

dj-1

c2r

+

1 28 cydj-1cr

+

1 28 cycr

j=1

k=j+1



26 38 l(l + 2)cycrcd (lcr + 1),

where cd = maxj dj. The proof is completed.

D.2.4 PROOF OF LEMMAS 19 AND 20

Lemma 25. (Alessandro, 2016; Rigollet, 2015) Let (x1, · · · , xk) be a vector of i.i.d. Gaussian variables from N (0,  2) and let f : Rd0  R be L-Lipschitz. Then the variable f (x) - Ef (x) is sub-Gaussian. That is, we have

P (f (x) - Ef (x) > t)  exp

- t2 2L2 2

,

(t  0),

or E ((f (x) - Ef (x)))  exp 42L2 2 , (  0).

Remarkably, this is a dimension free inequality.

46

Under review as a conference paper at ICLR 2018

Proof of Lemma 19. We first define a function g(x) = zT wf (w, x) where z  Rd is a constant vector. Then we have xg(x) = x zT wf (w, x) = xwf (w, x)z. Then by Lemma 18, we

can obtain xg(x) 2 = xwf (w, x)z 2   z 2, where  =

26 38

l(l

+

2)cy cr cd

(lcr

+

1)

in which cy, cd and cr are defined in Lemma 18. This means g(x) is  z 2-Lipschitz. Thus, by

Lemma 25, we have

E (t

z, wf (w, x) - Ewf (w, x) ) = E (t (g(x) - Eg(x)))  exp

4t22

z

2 2



2

.

Let  = tz. This further gives

E ( , wf (w, x) - Ewf (w, x) )  exp

42 2



2 2

,

which means , wf (w, x) - Ewf (w, x) is 82 2-sub-Gaussian.

Proof of Lemma 20. We first define a function h(x) = zT w2 f (w, x)z where z  Sd-1, i.e. z 2 = 1. Then h(w) is a -Lipschitz function, where  = xw2 f (w, x) op. Note that since the sigmoid function is infinitely differentiable function, xw2 f (w, x) exists. Also since for any input x, (x) belongs to [0, 1]. Thus, the values of the entries in xw2 f (w, x) can be bounded. So according to the definition of the operation norm of a 3-way tensor, the operation norm of xw2 f (w, x) can be bounded by a constant. Without loss of generality, let x2wf (w, x) op   < +. Thus, by Lemma 25, we have

E t z, 2wf (w, x) - Ew2 f (w, x) z

= E (t (h(x) - Eh(x)))  exp

8t22 2 2

.

This means that the hessian of the loss evaluated on a unit vector is 82 2-sub-Gaussian.

D.2.5 PROOF OF LEMMA 21

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in

(Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /(ld)

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Accordingly, we can
47

Under review as a conference paper at ICLR 2018

decompose

2J^n(w) - 2J (w)

as follows: op

2J^n(w) - 2J (w) op

=

1 n

n

2f (w, x(i)) - E(2f (w, x))

i=1 op

1n =
n

2f (w, x(i)) - f (wkw , x(i))

1 +
n

n

2f (wkw , x(i)) - E(2f (wkw , x))

i=1 i=1

+ E(2f (wkw , x)) - E(2f (w, x)) op

1n 
n

2f (w, x(i)) - 2f (wkw , x(i))

+

1 n

n
2f (wkw , x(i)) - E(2f (wkw , x))

i=1 op i=1

op

+ E(2f (wkw , x)) - E(2f (w, x)) . op
Here we also define four events E0, E1, E2 and E3 as

E0 =

sup
w

2J^n(w) - 2J (w)

t op

,




E1 = sup w

1n n
i=1

2f (w, x(i)) - 2f (wkw , x(i))





t ,

op 3 





E2 =

sup

jw[n j ],j=[l]



1 n

n
2f (wkw , x(i)) - E(2f (wkw , x))
i=1

 op

t ,
3

E3 =

sup
w

E(2f (wkw , x)) - E(2f (w, x))

op



t 3

.

Accordingly, we have P (E0)  P (E1) + P (E2) + P (E3) .
So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).

Step 1. Bound P (E1): We first bound P (E1) as follows:

P (E1) =P

sup
w

1n n

2f (w, x(i)) - 2f (wkw , x(i))

i=1

t 3
2

x


3 t

E

sup
w

1n n
i=1

2f (w, x(i)) - 2f (wkw , x(i))

2



3 t

E

sup
w

1 n

n i=1

2f (w, x(i)) - 2f (wkw , x(i))

w - wkw 2

2 sup w - wkw 2
w



3 t

E  sup
w

1 n

n

3f (w, x(i))



i=1 op

y 3 ,
t

where x holds since by Markov inequality and y holds because of Lemma 17.

Therefore, we can set

t  6 . 

48

Under review as a conference paper at ICLR 2018

Then we can bound P(E1):

P(E1)



 .
2

Step 2. Bound P (E2): By Lemma 2, we know that for any matrix X  Rd×d, its operator norm can be computed as

1

X

op  1 - 2

sup | , X | .


where  = {1, . . . , kw } be an -covering net of Bd(1).

Let

1/4

be

the

1 4

-covering

net

of

Bd(1)

but

it

has

only

s

nonzero

entries.

So

the

size

of

its

-net is

d s

3

s
 exp (s log (12d)) .

1/4

Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd ). Then we can bound P (E2) as follows:

P(E2) =P

sup
jw[n j ],j=[l]

1 n

n

2f (wkw , x(i)) - E(2f (wkw , x))

t 3

i=1 2

=P sup 2
jw[n j ],j=[l],1/4

,

1 n

n

2f (wkw , x(i)) - E

2f (wkw , x)

i=1




l
 exp (s log (12d)) exp  sj log
j=1

3rd


 sup P
jw[n j ],j=[l],1/4

1n ,
n
i=1

t 3

2f (wkw , x(i))-E 2f (wkw , x)



t . 6

Since by Lemma 20, , w2 f (w, x) - E2wf (w, x)  where   Bd(1) is 82 2-subGaussian, i.e.

E t , w2 f (w, x) - E2wf (w, x) 

 exp

8t22 2 2

.

Thus,

1 n

n i=1

,

w2 f (w, x) - E2wf (w, x)



is 82 2/n-sub-Gaussian random variable. So

we can obtain

P

1n n

y, w2 f (w, x) - Ew2 f (w, x) y

t 6

nt2

 2 exp

- 722 2

.

i=1

Note d = j djdj-1. Then the probability of E2 is upper bounded as

P (E2)  2 exp

- nt2 722 2

+

s log

36d2r

.

Thus, if we set then we have

72 (s log(36d2r/ ) + log(4/))

t  

,

n

P (E2)



 .
2

49

Under review as a conference paper at ICLR 2018

Step 3. Bound P (E3): We first bound P (E3) as follows:

P (E3) =P P =P P P

sup
w

E(2f (wkw , x)) - E(2f (w, x))

t 23

E sup
w

(2f (wkw , x) - 2f (w, x)

2



t 3

E sup
w

2f (w, x) - 2f (wkw , x) w - wkw 2

2 sup
w

w - wkw

2



t 3

E sup
w

3f (w, x)

op



t 3

 t . 3

We set enough small such that  < t/3 always holds. Then it yields P (E3) = 0. Step 4. Final result: To ensure P(E0)  , we just set = 36rl2/n and

t  max

6 , 



72 (s log(36rd2/ )+log(4/)) n

108r = max n , c4

d log(nl)+log(4/) n

.

Therefore, there exists such two universal constants cm

and cm such that if n 

,cm 2l2r2
2 22s log(d) log(1/)

then

sup
w

2J^n(w)-2J (w)

op  cm

s log(dn/l)+log(4/) n

holds with probability at least 1 - .

D.3 PROOFS OF MAIN THEORIES

D.3.1 PROOF OF THEOREM 4

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in

(Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /(ld)

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Accordingly, we can
50

Under review as a conference paper at ICLR 2018

decompose J^n(w) - J (w) as follows:
2

J^n(w) - J (w)
2

=

1 n

n

f (w, x(i)) - E(f (w, x))

i=1 2

1n

1n

= n

f (w, x(i)) - f (wkw , x(i))

+ n

f (wkw , x(i)) - E(f (wkw , x))

i=1 i=1

+ E(f (wkw , x)) - E(f (w, x))

2

1n

1n

 n

f (w, x(i)) - f (wkw , x(i))

+ n

f (wkw , x(i)) - E(f (wkw , x))

i=1 2 i=1

2

+ E(f (wkw , x)) - E(f (w, x)) .
2
Here we also define four events E0, E1, E2 and E3 as

E0 = E1 = E2 =

sup J^n(w) - J (w)  t ,

w

2

1n

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

t 3
2

,

1n

t

sup
jw[n j ],j=[l]

n

f (wkw , x(i)) - E(f (wkw , x))

 3

i=1 2

t

E3 = sup E(f (wkw , x)) - E(f (w, x))
w

. 3

2

Accordingly, we have

P (E0)  P (E1) + P (E2) + P (E3) .

So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).

,

Step 1. Bound P (E1): We first bound P (E1) as follows:

1n

t

P (E1) =P

sup
w

n f (w, x(i)) - f (wkw , x(i))
i=1

 3
2

x3 1 n

tE

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

2



3 t

E

sup
w

1 n

n i=1

f (w, x(i)) - f (wkw , x(i))

w - wkw 2

2 sup w - wkw 2
w

3 t

E

sup 2J^n(w, x)

w

2

,

where x holds because of Markov inequality. Then, we bound E

supw

2J^n(w, x)
2

follows:

E sup 2J^n(w, x)  E

w

2

sup
w

1 n 2f (w,x) n
i=1 2

=E

sup
w

2f (w,x)

2

x
,

where x holds since by Lemma 17, we have

w2 f (w, x) op  2wf (w, x) F ,

as

51

Under review as a conference paper at ICLR 2018

where  =

cs1 crcd2l4 in which cd = maxi di and cr = max

r2 16

,

r2 16

l-1

. Therefore, we have

We further let Then we can bound P(E1):

P (E1)



3 t

.

t  6 . 

P(E1)



 .
2

Step 2. Bound P (E2): By Lemma 1, we know that for any vector x  Rd, its 2-norm can be

computed as

x

2



1 1-

sup


, x .

where  = {1, . . . , kw } be an -covering net of Bd(1).

Let

1/2

be

the

1 2

-covering

net

of

Bd(1)

but

it

has

only

s

nonzero

entries.

So

the

size

of

its

-net is

d s

3

s
 exp (s log (6d)) .

1/2

Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd . Then we can bound P (E2) as follows:

1n

t

P (E2) =P

sup
jw[n j ],j=[l]

n

f (wkw , x(i)) - E(f (wkw , x))

 3

i=1 2

=P

sup 2
jw[n j ],j=[l],1/2

1 ,
n

n

f (wkw , x(i)) - E (f (wkw , x))

i=1


l
 exp (s log(6d)) exp  sj log
j=1

3rd


 sup P
jw[n j ],j=[l],1/2

1n n
i=1

t 3
,

t

f (wkw , x(i)) - E (f (wkw , x))

. 6

Since by Lemma 19, y, f (w, x) is 82 2-sub-Gaussian, i.e.

E ( , wf (w, x) - Ewf (w, x) )  exp

82 2



2 2

2

,

where  =

26 38

l(l

+

2)cy cr cd

(lcr

+

1)

in

which

cy ,

cd

and

cr

are

defined

in

Lemma

16.

Thus,

1 n

n i=1

y, f (w, x)

is 82 2/n-sub-Gaussian random variable. Thus, we can obtain

1n

t nt2

Pn

y, f (wkw , x(i)) - E (f (wkw , x))

 6

 exp - 722 2

.

i=1

Notice,

j djdj-1 = d. In this case, the probability of E2 is upper bounded as

P (E2)  exp

-

nt2 722

2

+ d log

18r

.

Thus, if we set

t  

72 (s log(18d2r/ ) + log(4/)) ,

n

52

Under review as a conference paper at ICLR 2018

then we have

P (E2)



 .
2

Step 3. Bound P (E3): We first bound P (E3) as follows:

t

P (E3) =P

sup
w

E(f (wkw , x)) - E(f (w, x))

2 3

=P

sup
w

E (f (wkw , x) - f (w, x) w - wkw 2

2) sup
w

w - wkw

t 2 3

P

E sup
w

2J^n(w, x)

t 23

x
P

 t 3

.

where x holds since by Lemma 17. We set enough small such that  < t/3 always holds. Then it yields P (E3) = 0.

Step 4. Final result: To ensure P(E0)  , we just set = 18r/n and

6 72 (s log(18d2r/ ) + log(4/)) t  max , 
n

108 r

72 (s log(nl) + log(4/))

= max

, 

.

n n

 Note that  = O( lcd). Therefore, there exists a universal constant cy such that if n  cy cdl3r2/(s log(d) 22 log(1/)), then

sup J^n(w)-J (w)  

w

2

512 729 cyl(l + 2) (lcr + 1) crcd

s log(dn/l)+log(4/) n

holds with probability at least 1 - , where cy, cd and cr are defined in Lemma 16.

D.3.2 PROOF OF THEOREM 5

Proof. Suppose that {w(1), w(2), · · · , w(m)} are the non-degenerate critical points of J (w). So for any w(k), it obeys

inf
i

ki

2J (w(k))

 ,

where ki 2J (w(k)) denotes the i-th eigenvalue of the Hessian 2J (w(k)) and  is a constant. We further define a set D = {w  Rd | J (w) 2  and infi |i 2J (w(k)) |  }. According
to Lemma 4, D = k=1Dk where each Dk is a disjoint component with w(k)  Dk for k  m and Dk does not contain any critical point of J (w) for k  m + 1. On the other hand, by the continuity of J (w), it yields J (w) 2 = for w  Dk. Notice, we set the value of blow which is actually a function related n.

Then by utilizing Theorem 4, we let sample number n sufficient large such that

sup J^n(w) - J (w)  

w

2

2

holds with probability at least 1 - , where  = 

512 729

cy l(l

+

2)

(lcr

+

1)

cr cd

s

log(dn/l)+log(4/) n

.

This further gives that for arbitrary w  Dk, we have

inf tJ^n(w) + (1 - t)J (w) = inf t J^n(w) - J (w) + J (w)

wDk

2 wDk

2

 inf
wDk

J (w) 2 - sup t
wDk

J^n(w) - J (w)

2

 . (26) 2

53

Under review as a conference paper at ICLR 2018

Similarly, by utilizing Lemma 21, let n be sufficient large such that

sup
w

2J^n(w) - 2J (w)

op  cm

s log(dn/l)+log(4/)   n2

holds with probability at least 1 - . Assume that b  Rd is a vector and satisfies bT b = 1. In this case, we can bound ki 2J^n(w) for arbitrary w  Dk as follows:

inf
wDk

ki

2J^n(w)

= inf min bT 2J^n(w)b
wDk bT b=1

= inf min bT 2J^n(w) - 2J (w) b + bT 2J (w)b
wDk bT b=1

 inf min bT 2J (w)b - min bT 2J^n(w) - 2J (w) b

wDk bT b=1

bT b=1

 inf min bT 2J (w)b - max bT 2J^n(w) - 2J (w) b

wDk bT b=1

bT b=1

=

inf
wDk

inf
i

|ki

2f (w(k), x)

-

2J^n(w) - 2J (w) op

 . 2

This means that in each set Dk, 2J^n(w) has no zero eigenvalues. Then, combining this and Eqn. (26), by Lemma 3 we know that if the population risk J (w) has no critical point in Dk, then the empirical risk J^n(w) has also no critical point in Dk; otherwise it also holds. By Lemma 3, we can also obtain that in Dk, if J (w) has a unique critical point w(k) with non-degenerate index sk, then J^n(w) also has a unique critical point wn(k) in Dk with the same non-degenerate index sk. The
first conclusion is proved.

Now we bound the distance between the corresponding critical points of J (w) and J^n(w). Assume that in Dk, J (w) has a unique critical point w(k) and J^n(w) also has a unique critical point wn(k). Then, there exists t  [0, 1] such that for any z  Bd(1), we have

 J (wn(k)) 2

= max
zT z=1

J (wn(k)), z

= max J (w(k)), z
zT z=1

+

2J (w(k) + t(wn(k) - w(k)))(wn(k) - w(k)), z

x


2 1/2
2J (w(k)) (wn(k) - w(k)), (wn(k) - w(k))

y


wn(k) - w(k)

2,

where x holds since J (w(k)) = 0 and y holds since w(k) + t(wn(k) - w(k)) is in Dk and for any w  Dk we have infi |i 2J (w) |  . Then if n 
cs max cdl3r2/(s log(d) 22 log(1/)), s log(d/l)/2 where cs is a constant, then

wn(k) - w(k)

2



2 

512 729 cyl(l + 2) (lcr + 1) crcd

s log(dn/l)+log(4/) n

holds with probability at least 1 - . The proof is completed.

D.3.3 PROOF OF THEOREM 6

Proof. Recall that the weight of each layer has magnitude bound separately, i.e. w(j) 2  r.

Assume that w(j) has sj non-zero entries. Then we have

l j=1

sj

=

s.

So

here

we

separately

assume

wj = {w1j, · · · , wnj j } is the djdj-1 /d-covering net of the ball Bdjdj-1 (r) which corresponds

54

Under review as a conference paper at ICLR 2018

to the weight w(j) of the j-th layer. Let n j be the /l-covering number. By -covering theory in (Vershynin, 2012), we can have

nj

dj dj-1 sj

3r sj  exp
dj dj-1 /(ld)

sj log

3rdj dj-1 dj dj-1 /d

3rd = exp sj log

.

Let w   be an arbitrary vector. Since w = [w(1), · · · , w(l)] where w(j) is the weight of the j-th layer, we can always find a vector wkjj in wj such that w(j) - wkjj 2  djdj-1 /d. For brevity, let jw  [n j] denote the index of wkjj in -net wj. Then let wkw = [wkj1 ; · · · ; wkjj ; · · · ; wkjl ]. This means that we can always find a vector wkw such that w - wkw 2  . Accordingly, we can
decompose J^n(w) - J (w) as

J^n(w) - J (w) =

1 n

n

f (w, x(i)) - E(f (w, x))

i=1

1 =
n

n

f (w, x(i))-f (wkw , x(i))

1 +
n

n
f (wkw , x(i))-Ef (wkw , x)+Ef (wkw , x)-Ef (w, x)

i=1 i=1

1 n 1n

 n

f (w, x(i))-f (wkw , x(i))

+ n

f (wkw , x(i))-Ef (wkw , x) + Ef (wkw , x)-Ef (w, x) .

i=1 i=1

Then, we define four events E0, E1, E2 and E3 as

E0 = E1 = E2 = E3 =

sup J^n(w) - J (w)  t ,
w

1n

t

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

 3

,

1n t

sup
jw[n j ],j=[l]

n f (wkw , x(i))-E(f (wkw , x))  3
i=1

t

sup
w

E(f (wkw , x))-E(f (w, x))

 3

.

,

Accordingly, we have

P (E0)  P (E1) + P (E2) + P (E3) .

So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).

Step 1. Bound P (E1): We first bound P (E1) as follows:

1n

t

P (E1) =P

sup
w

n
i=1

f (w, x(i)) - f (wkw , x(i))

 3

x


3 t

E

sup
w

1n n
i=1

f (w, x(i)) - f (wkw , x(i))



3 t

E

1
sup n
w

n i=1

f (w, x(i)) - f (wkw , x(i))

w - wkw 2

sup w - wkw 2
w

3 t

E

sup J^n(w, x)

w

2

,

where x holds since by Markov inequality, for an arbitrary nonnegative random variable x, then we

have

P(x



t)



E(x) . t

55

Under review as a conference paper at ICLR 2018

Now we only need to bound E

supw

J^n(w, x)
2

. Then by Lemma 16, we can bound it as

follows:

E sup J^n(w, x)

w

2

E

sup
w

1 n

n

f (w, x(i))

i=1 2

 ,

where  =

1 16

cy

cd

(1

+

cr (l

-

1))

in

which

cy ,

cd

and

cr

are

defined

in

Lemma

16.

Therefore, we have We further let Then we can bound P(E1):

3

P (E1) 

. t

t  6 . 

P(E1)



 .
2

Step 2. Bound P (E2): Recall that we use jw to denote the index of wkjj in -net wj and we have jw  [n j], (n j  exp sj log 3rd . We can bound P (E2) as follows:

P (E2) =P

sup
jw[n j ],j=[l]

1 n

n

f (wkw , x(i)) - E(f (wkw , x))

i=1

t 3


l
 exp  sj log
j=1


3rd  sup P
jw[n j ],j=[l]

1 n

n

f (wj, x(i)) - E(f (wj, x))

t 3

.

i=1

Since when the activation functions are sigmoid functions, the loss f (w, x) is -Lipschitz. Besides,

we assume x to be a vector of i.i.d. Gaussian variables from N (0,  2). Then by Lemma 25, we know

that the variable f (x) - Ef (x) is 82 2-sub-Gaussian. Thus, we have

P (|f (x) - Ef (x)| > t)  2 exp

-

t2 22

2

,

(t  0),

where  =

1 16

cy

cd

(1

+

cr

(l

-

1))

in

which

cy ,

cd

and

cr

are

defined

in

Lemma

16.

Thus,

1 n

n i=1

f (wj,

x(i))

-

E(f (wj,

x))

is

82 2/n-sub-Gaussian

random

variable.

Thus,

we

can

obtain

1n

t nt2

Pn

f (wj, x(i)) - E(f (wj, x))

 3

 2 exp

- 182 2

.

i=1

Notice

l j=1

sj

=

s.

In

this

case,

the

probability

of

E2

is

upper

bounded

as

P (E2)  2 exp

-

nt2 182

2

+

s log

3dr

.

Thus, if we set

then we have

t  

18 (s log(3dr/ ) + log(4/)) ,
n

P (E2)



 .
2

Step 3. Bound P (E3): We first bound P (E3) as follows:

t

P (E3) =P

sup |E(f (wkw , x))
w

-

E(f (w, x))|



3

=P

sup |E (f (wkw , x) - f (w, x))| sup

w

w - wkw 2

w

w - wkw

2



t 3

P

E sup
w

Jw(w, x)

2



t 3

x
P

 t 3

,

56

Under review as a conference paper at ICLR 2018

where x holds since by Lemma 16, for arbitrary x and w  , we have wf (w, x) 2  . We set enough small such that  < t/3 always holds. Then it yields P (E3) = 0.

Step

4.

Final

result:

Notice,

we

have

6 

 3 . To ensure P(E0)  , we just set

= 3r/n and

t  max

6 , 

18 (s log(3dr/ ) + log(4/))

18r

= max

, 

18 (s log(nd)+log(4/))

.

n

n n

Therefore, if n  18l2r2/(s log(d) 22 log(1/)), then

sup J^n(w) - J (w)  
w

9 8

cy

cd

(1

+

cr

(l

-

1))

s log(nd/l) + log(4/) n

holds with probability at least 1 - , where cy, cd, and cr are defined as

v(l) - y

2 2



cy

< +,

cd = max(d0, d1, · · · , dl)

and

cr = max

r2 ,
16

r2 l-1 16

.

The proof is completed.

D.3.4 PROOF OF COROLLARY 2

Proof. By Lemma 5, we know s = g. Thus, the remaining work is to bound s. Actually, we can have

1 ESD,A,(x(j),y(j))D n

n

fj (wj ;x(j),y(j))-fj (wn;x(j),y(j))

j=1

ESD

sup J^n(w) - J (w)
w

 sup J^n(w) - J (w)
w
 n.

Thus, we have g = s  n. The proof is completed.

D.4 PROOF OF OTHER LEMMAS

D.4.1 PROOF OF LEMMA 22

Proof. Since G(u(i)) is a diagonal matrix and its diagonal values are upper bounded by (u(hi))(1 - (uh(i)))  1/4 where uh(i) denotes the h-th entry of u(i), we can conclude

G(u(i))M

2 F



1 16

M

2 F

and

N G(u(i))

2 F



1 16

N

2 F

.

Note that Pk is a matrix of size d2k × dk whose ((s - 1)dk + s, s) (s = 1, · · · , dk) entry equal to (us(k))(1 - (u(sk)))(1 - 2(u(sk))) and rest entries are all 0. This gives

(u(sk))(1

-

(us(k)))(1

-

2(u(sk)))

=

1 3

(3(us(k)))(1

-

(u(sk)))(1

-

2(u(sk)))

 1 3(u(sk)) + 1 - (us(k)) + 1 - 2(u(sk)) 33

3

23  34 .

This

means

the

maximal

value

in

Pk

is

at

most

23 34

.

Consider

the

structure

in

Pk ,

we

can

obtain

Pk M

2 F



26 38

M

2 F

and

N Pk

2 F



26 38

N

2 F

.

57

Under review as a conference paper at ICLR 2018

As for Bs:t, we have

Bs:t

2 F



As

2 F

As+1

2 F

·

·

·

At

2 F

2 22
= (W s)T G(u(s)) (W (s+1))T G(u(s+1)) · · · (W (t))T G(u(t))
F FF



1 16t-s+1

2
W (s)
F

2
W (s+1) · · ·
F

2
W (t)
F

1 = 16t-s+1 Ds:t.

Since the 2-norm of each w(j) is bounded, i.e. w(j) 2  r, we can obtain

1 16t-s+1

Ds:t 

1 16t-s+1

r2(t-s+1)

=

r 2(t-s+1) 4

cst.

Now we prove the final result. According to the property of Kronecker product that for any matrices A, B and X of proper sizes, vec (AXB) = (BT  A)vec (X), we have

vec M N T = (N  I)vec (M ) = (N  I)m.

This further yields

(N

 I)m

2 F

=

vec

MNT

2 F

=

MNT

2 F



By similar way, we can obtain

(I  N )m

2 F



M

2 F

N

2 F

.

The proof is completed.

M

2 F

N

2 F

.

D.4.2 PROOF OF LEMMA 23

Proof.

By utilizing the chain rule in Eqn. (24) in Sec. D.2.1, we can easily compute

f (w,x)  u(i)

and

f (w,x)  v (i)

as

follows:

f (w, x) u(i)

=

G(u(i))Ai+1

· · · Al(v(l)

- y)

=

G(u(i))Bi+1:l(v(l)

- y)

and

f (w, x) v(i)

=

Ai+1

· · · Al(v(l)

-

y)

=

Bi+1:l(v(l)

-

y).

Therefore, we can further obtain

f (w, x)

w(j)

=vec G(u(j))Aj+1Aj+2 · · · Al(v(l) - y) (v(j-1))T

=vec G(u(j))Aj+1Aj+2 · · · Ai-1(W (i))T G(u(i))Ai+1 · · · Al(v(l) - y) (v(j-1))T

= v(j-1)  G(u(j))Aj+1Aj+2 · · · Ai-1(W (i))T vec G(u(i))Ai+1 · · · Al(v(l) - y)

= v(j-1)  G(u(j))Aj+1Aj+2 · · · Ai-1(W (i))T

f (w, x) .
u(i)

Note

that

we

have

f (w,x)  w(j )

=

 u(i)  w(j )

f (w,x)  u(i)

. This gives

u(i) = (v(j-1))T  w(j)

G(u(j))Bj+1:i-1(W (i))T

When i = j, we have

T
 Rdi×dj dj-1 (i > j).

Similarly, we can obtain

u(i) w(i)

= (v(i-1))T

 Idi

 R .di×didi-1

v(i) = (v(j-1))T w(j)

G(u(j))Aj+1Aj+2 · · · Ai

T =(v(j-1))T

G(u(j))Bj+1:i

T
Rdi×dj dj-1 (i  j).

The proof is completed.

58

Under review as a conference paper at ICLR 2018

D.4.3 PROOF OF LEMMA 24

Proof. By Lemma 23, we have

f (w, x) u(i)

=

G(u(i))Bi+1:l(v(l)

- y)

and

f (w, x) v(i)

=

Bi+1:l(v(l)

-

y).

Therefore, we can further obtain

f (w, x) u(1)

=G(u(1))A2

· · · Al(v(l)

-

y)

=G(u(1))A2 · · · Aj-1(W j )T G(u(j))Aj+1 · · · Al(v(l) - y)

= G(u(1))A2 · · · Aj-1(W j )T

f (w, x)

u(j)

.

Note

that

we

have

f (w,x)  u(1)

=

u(j) T  u(1)

f (w,x)  u(j )

. This gives

u(j) u(1) =

G(u(1))A2 · · · Aj-1(W j )T

T
=

G(u(1))B2:j-1(W j )T

T
 Rdj×d1 (j > 1).

Similarly, we can obtain

v(j) u(1) =

T
G(u(1))A2 · · · Aj =

T
G(u(1))B2:j  Rdj×d1 (j > 1).

The proof is completed.

59

