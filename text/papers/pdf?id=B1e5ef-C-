Under review as a conference paper at ICLR 2018
A COMPRESSED SENSING VIEW OF UNSUPERVISED TEXT EMBEDDINGS, BAG-OF-n-GRAMS, AND LSTMS
Anonymous authors Paper under double-blind review
ABSTRACT
Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the "meaning" of text and a form of unsupervised learning useful in downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of BonG representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to establish. Our experimental results support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of pretrained word embeddings (e.g. GloVe, word2vec): they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.
1 INTRODUCTION
Much attention has been paid to using LSTMs (Hochreiter & Schmidhuber, 1997) and similar models to compute text embeddings (Bengio et al., 2003; Collobert & Weston, 2008). Once trained, the LSTM can sweep once or twice through a given piece of text, process it using only limited memory, and output a vector with moderate dimensionality (a few hundred to a few thousand), which can be used to measure text similarity via cosine similarity or as featurization for downstream tasks.
The powers and limitations of this method have not been formally established. For example, can such neural embeddings compete with and replace traditional linear classifiers trained on trivial Bag-of-n-Grams (BonG) representations? Tweaked versions of BonG classifiers are known to be a surprisingly powerful baseline (Wang & Manning, 2012) and have fast implementations (Joulin et al., 2017). They continue to give better performance on many downstream supervised tasks such as IMDB sentiment classification (Maas et al., 2011) than purely unsupervised LSTM representations (Kiros et al., 2015; Hill et al., 2016; Pagliardini et al., 2017). Even a very successful character-level (and thus computation-intensive, taking a month of training) approach does not reach BonG performance on datasets large than IMDB (Radford et al., 2017). Meanwhile there is evidence suggesting that simpler linear schemes give compact representations that provide most of the benefits of word-level LSTM embeddings (Wieting et al., 2016; Arora et al., 2017). These linear schemes consist of simply adding up standard pretrained word embeddings (with a few modifications) (Mikolov et al., 2013; Pennington et al., 2014).
The current paper ties these disparate threads together by giving an information-theoretic account of linear text embeddings. We describe linear schemes that preserve n-gram information as lowdimensional embeddings with provable guarantees for any text classification task. The previous linear schemes, which used unigram information, are subcases of our approach, but our best schemes can also capture n-gram information with low additional overhead. Furthermore, we show that the original unigram information can be (approximately) extracted from the low-dimensional embedding using sparse recovery/compressed sensing (Candes & Tao, 2005). Our approach also fits in the tradition
1

Under review as a conference paper at ICLR 2018
of the older work on distributed representations of structured objects, especially the works of Plate (1995) and Kanerva (2009). The following are the main results achieved by this new world-view:
1. Using random vectors as word embeddings in our linear scheme (instead of pretrained vectors) already allows us to rigorously show that low-memory LSTMs are provably at least as good as every linear classifier operating on the full BonG vector. This is a novel theoretical result in deep learning, obtained relatively easily. By contrast, extensive empirical study of this issue has been inconclusive (apart from character-level models, and even then only on smaller datasets (Radford et al., 2017)). Note also that empirical work by its nature can only establish performance on some available datasets, not on all possible classification tasks. We prove this theorem in Section 4 by providing a nontrivial generalization of a result combining compressed sensing and learning (Calderbank et al., 2009). In fact, before our work we do not know of any provable quantification of the power of any text embedding.
2. We study theoretically and experimentally how our linear embedding scheme improves when it uses pretrained embeddings (GloVe etc.) instead of random vectors. Empirically we find that this improves the ability to preserve Bag-of-Words (BoW) information, which has the following restatement in the language of sparse recovery: word embeddings are better than random matrices for "sensing" BoW signals (see Section 5). We give some theoretical justification for this surprising finding using a geometric result in compressed sensing.
3. Section 6 provides empirical results supporting the above theoretical work, reporting accuracy of our linear schemes on multiple standard classification tasks. Our embeddings are consistently competitive with recent results and perform much better than all previous linear methods. Among unsupervised word-level representations they achieve state of the art performance on both the binary and fine-grained SST sentiment classification tasks (Socher et al., 2013). Since our document representations are fast, compositional, and simple to implement given standard word embeddings, they provide strong baselines for future work.

Text

Transducer Embedding

2 RELATED WORK
Neural text embeddings are instances of distributed representations long studied in connectionist approaches, because they decay gracefully with noise and allow distributed processing. Hinton (1990) provided an early problem formulation, and Plate (1995) provided an elementary solution, the holographic distributed representation, which represents structured objects using convolutional product of vectors and has an easy and more compact implementation using the fast Fourier transform (FFT). Plate suggested applying such ideas to text, where "structure" can be quantified using parse trees and other graph structures. Our method also closely related to the sparse distributed memory system of Kanerva (2009), to which our method is closely related. In the unigram case our embedding reduces to the familiar sum of word embeddings, which is known to be surprisingly powerful (Wieting et al., 2016), and with a few modifications even more so (Arora et al., 2017).
Compressed representations of BonG vectors were studied through the lens of classical information theory by Paskov et al. (2013), who computed representations using large linear programs (LPs). Their embeddings are still high-dimensional (d > 100K) and quite complicated to implement. Our embedding is simpler, more compact, and leverages readily available word embeddings. Pagliardini et al. (2017) also introduce a linear scheme, representing documents as an average of learned word and bigram embeddings. However, the motivation and benefits of encoding BonGs in low-dimensions are not made explicit. The novelty in the current paper is the connection to compressed sensing, which is concerned with recovering high-dimensional sparse signals x  RN from low-dimensional linear measurements Ax, specifically by studying conditions on matrix A  Rd×N when this is possible (see Appendix A for an overview of compressed sensing and the previous work of Calderbank et al. (2009) that we build upon).
2

Under review as a conference paper at ICLR 2018

3 DOCUMENT EMBEDDINGS

In this section we define the two types of representations that our analysis will relate:
1. high-dimensional BonG vectors counting the occurrences of each k-gram for k  n
2. low-dimensional embeddings, from simple vector sums to novel n-gram-based embeddings
Although some of these representations have been previously studied and used, we define them so as to make clear their connection via compressed sensing, i.e. that representations of the second type are simply linear measurements of the first.
We now define some notation. Let V be the number of words in the vocabulary and Vn be the number of n-grams (independent of word order), so that V = V1. Furthermore set Vnsum = kn Vk and Vnmax = maxkn Vk. We will use words/n-grams and indices interchangeably, e.g. if (a, b) is the ith of V2 bigrams then the one-hot vector e(a,b) will be 1 at index i. Where necessary we will use {, } to denote a multi-set and (, ) to denote a tuple. For any m vectors vi  Rd for i = 1, . . . , m we define [v1, . . . , vm] to be their concatenation, which is thus an element of Rmd. Finally, for any subset X  RN we denote by X the set {x - x : x, x  X }.

3.1 THE BAG-OF-n-GRAMS VECTOR

Assigning to each word a unique index i  [V ] we define the Bag-of-Words (BoW) representation xBoW of a document to be the V -dimensional vector whose ith entry is the number of times word i
occurs in the document. The n-gram extension of BoW is the Bag-of-n-Grams (BonG) representation, which counts the number of times any k-gram for k  n appears in a document. Linear classification
over such vectors has been found to be a strong baseline (Wang & Manning, 2012).

For ease of analysis we simplify the BonG approach by merging all n-grams in the vocabulary that contain the same words but in a different order. We call these features n-cooccurrences and find that the modification does not affect performance significantly (see Table 3 in Appendix E.1). Formally for a document w1, . . . , wT we define the Bag-of-n-Cooccurrences (BonC) vector as the concatenation

T T -n+1

xBonC =

ewt , . . . ,

e{wt ,...,wt+n-1 }

t=1 t=1

which

is

thus

a

vector

in

ZVnsum
+

.

Note

that

in

the

unigram

case

this

is

equivalent

to

xBoW

=

(1)

T t=1

ewt

.

3.2 LOW-DIMENSIONAL n-GRAM EMBEDDINGS

Now suppose each word w has a vector vw  Rd for some d V . Then given a document

w1, . . . , wT we define its unigram embedding as zu =

T t=1

vwt .

While

this

is

a

simple

and

widely

used featurization, we focus on the following straightforward relation with BoW: if A  Rd×V is

a matrix whose columns are word vectors vw then AxBoW =

T t=1

Aewt

=

T t=1

vwt

=

zu.

Thus

in terms of compressed sensing the unigram embedding of a document is a d-dimensional linear

measurement of its Bag-of-Words vector.

We could extend this unigram embedding to n-grams by first defining a representation for each n-

gram as the tensor product of the we would have vb = vw1 vwT2 and

vectors of its constituent words.

more generally vg =

n t=1

vwt

Thus for each bigram b = for each n-gram g = (w1,

(w1, w2) . . . , wn).

The document embedding would then be the sum of the tensor representations of all n-grams.

The major drawback of this approach is of course the blowup in dimension, which in practice

prevents its use beyond n = 2. To combat this a low-dimensional sketch or projection of the

tensor product can be used, such as the circular convolution operator of Plate (1995). Since we are

interested in representations that can also be constructed by an LSTM, we instead sketch this tensor

product using the element-wise multiplication operation, which we find also usually works better

than circular convolution in practice (see Table 4 in Appendix E.1). Thus for the n-cooccurrence

g

=

{w1, . . . , wn},

we

define

the

distributed

cooccurrence

(DisC)

embedding

v~g

=

d n-1 2

n t=1

vwt

.

The coefficient is required when the vectors vw have unit norm to ensure that the product also has

3

Under review as a conference paper at ICLR 2018

norm close to 1 (see Lemma B.1). In addition to their convenient form, DisC embeddings have nice theoretical and practical properties: they preserve the original embedding dimension, they reduce to unigram (word) embeddings for n = 1, and under mild assumptions they satisfy useful compressed sensing properties with overwhelming probability (Lemma 4.1).

We then define the DisC document embedding to be the nd-dimensional weighted concatenation, over k  n, of the sum of the DisC vectors of all k-grams in a document:

T T -n+1

z~(n) = C1 v~wt , . . . , Cn

v~{wt ,...,wt+n-1 }

t=1 t=1

(2)

Here scaling factors Ck are set so that all spans of d coordinates have roughly equal norm (for random

embeddings Ck = 1; for word embeddings Ck = 1/k works well). Note that since v~wt = vwt we

have z~(1) = zu in the unigram case. Furthermore, as with unigram embeddings by comparing (1) and

(2) one can easily construct a

n k=1

dn

×

Vnsum

matrix

A~n

such

that

z~(n)

=

A~nxBonC.

3.3 LSTM REPRESENTATIONS

As discussed previously, LSTMs have become a common way to apply the expressive power of RNNs, with success on a variety of classification, representation, and sequence-to-sequence tasks. For document representation, starting with h0 = 0m an m-memory LSTM initialized with word vectors vw  Rd takes in words w1, . . . , wT one-by-one and computes the document representation

ht = f (Tf (vwt , ht-1))  ht-1 + i(Ti(vwt , ht-1))  g(Tg(vwt , ht-1))

(3)

where ht  Rm is the hidden representation at time t, the forget gate f , input gate i, and input function g are a.e. differentiable nondecreasing elementwise "activation" functions Rm  Rm, and affine transformations T(x, y) = Wx + Uy + b have weight matrices W  Rm×d, U  Rm×m and bias vectors b  Rm. The LSTM representation of a document is then the state at the last time step, i.e. zLSTM = hT . Note that we will follow the convention of using LSTM memory to refer to the dimensionality of the hidden states. Since the LSTM is initialized with an embedding for each word it requires O(m2 + md + V d) computer memory, but the last term is just a lookup table so the
vocabulary size does not factor into iteration or representation complexity.

From our description of LSTMs it is intuitive to see that one can initialize the gates and input functions so as to construct the DisC embeddings defined in the previous section. We state this formally and give the proof in the unigram case (the full proof appears in Appendix B.3):
Proposition 3.1. Given word vectors vw  Rd, one can initialize an O(nd)-memory LSTM (3) that takes in words w1, . . . , wT (padded by an end-of-document token assigned vector 0d) and constructs the DisC embedding (2) (up to zero padding), i.e. such that for all documents zLSTM = z~(n).

Proof (Unigram Case). Set f (x) = i(x) = g(x) = x, Tf (vwt , ht-1) = Ti(vwt , ht-1) = 1d, and

Tg(vwt , ht-1) = C1vwt . Then ht = ht-1 + C1vwt , so since h0 = 0d we have the final LSTM

representation zLSTM = hT = C1

t t=1

vwt

=

z~(1).

By Proposition 3.1 we can construct a fixed LSTM that can compute compressed BonC representations on the fly and be further trained by stochastic gradient descent using the same memory.

4 LSTMS AS COMPRESSED LEARNERS

Our main contribution is to prove the first rigorous analysis of the performance of the text embeddings that we are aware of, showing that the embeddings of Section 3.2 can provide performance on downstream classification tasks at least as well any linear classifier over BonCs. Before stating the theorem we make two mild simplifying assumptions on the BonC vectors:

1.

The vectors are scaled by

T

1 n

,

where

T

is the maximum document length.

2. No n-cooccurrence contains a word more than once.

4

Under review as a conference paper at ICLR 2018

Theorem 4.1. Let S = {(xi, yi)}mi=1 be drawn i.i.d. from a distribution D over BonC vectors of

documents of length at most T satisfying assumptions 1 and 2 above and let w0 be the linear classifier

minimizing the logistic loss

D. Then for dimension d = 

T22log

nVnmax 

one can initialize an

O(nd)-memory LSTM over i.i.d. word embeddings vw  Ud{±1/ d} such that w.p. (1 - )(1 - 2)

the classifier w^ learned using the l2-regularized logistic loss over its representations satisfies

11

D (w^)  D (w0) + O

w0 2

 + log m

(4)

The above theoretical bound shows that LSTMs match BonC performance as   0, which we can realize by increasing the embedding dimension d (c.f. Figure 5).

4.1 COMPRESSED SENSING AND LEARNING

Compressed sensing is concerned with recovering a high-dimensional k-sparse signal x  RN from a few linear measurements; given a design matrix A  Rd×N this is formulated as

minimize w 0 subject to Aw = z

(5)

where z = Ax is the measurement vector. As l0-minimization is NP-hard, research has focused on sufficient conditions for tractable recovery. One such condition is the Restricted Isometry Property (RIP), for which Candes & Tao (2005) proved that (5) can be solved by convex relaxation:
Definition 4.1. A  Rd×N is (X , )-RIP for some subset X  RN if  x  X

(1 - ) x 2  Ax 2  (1 + ) x 2

(6)

We will abuse notation and say (k, )-RIP when X is the set of k-sparse vectors. This is the more common definition, but ours allows a more general Theorem 4.2 and a tighter bound in Theorem 4.1.

Following these breakthroughs, Calderbank et al. (2009) studied whether it is possible to use the

low-dimensional output of compressed sensing as a surrogate representation for classification. They

proved a learning-theoretic bound on the loss of an SVM classifier in the compressed domain

compared to the best classifier in the original domain. In this work we are interested in comparing

the performance of LSTMs with BonC representations, so we need to generalize Calderbank et al. (2009) result to handle Lipschitz losses and an arbitrary set X  RN of high-dimensional signals:

Theorem 4.2. For any subset X  RN containing the origin let A  Rd×N be (X , )-RIP and

let m samples S = {(xi, yi)}mi=1  X × {-1, 1} be drawn i.i.d. from some distribution D over X

with x 2  R. If is a -Lipschitz convex loss function and w0  RN is its minimizer over D

then w.p. 1 - 2 the linear classifer w^A  Rd minimizing the l2-regularized empirical loss function

SA (w)

+

1 2C

w

2 2

over

the

compressed

sample

SA

=

{(Axi, yi)}mi=1



Rd

×

{-1,

1}

satisfies

D(w^A)  D(w0) + O

R w0 2

11  + log
m

(7)

for appropriate choice of C. Recall that X = {x - x : x, x  X } for any X  RN .

While a detailed proof of this theorem is spelled out in Appendix C, the main idea is to compare the distributional loss incurred by a classifier w^ in the original space to the loss incurred by Aw^ in the compressed space. We show that the minimizer of the regularized empirical loss in the original space (w^) is a bounded-coefficient linear combination of samples in S, so the loss incurred by w^ depends only on inner products between points in X . Thus using RIP and a generalization error result by Sridharan et al. (2008) we can bound the loss of w^A, the regularized classifier in the compressed domain. Note that to get back from Theorem 4.2 the O( ) bound for k-sparse inputs of Calderbank et al. (2009) we can set X to the be the set of k-sparse vectors and assume A is (2k, )-RIP.

4.2 PROOF OF MAIN RESULT
To apply Theorem 4.2 we need the design matrix A~n transforming BonCs into the DisC embeddings of Section 3.2 to satisfy the following RIP condition (Lemma 4.1), which we prove in Appendix D:

5

Under review as a conference paper at ICLR 2018

Lemma 4.1. Assume the setting of Theorem 4.1 and let A~n be the nd×Vnsum-dimensional matrix such

that the DisC embedding and BonC representation of any document are related by z~(n) = A~nxBonC.

If d = 

T2 2

log

nVnmax 

then A~n is

XT(n),  -RIP w.p. 1 - , where XT(n) is the set of BonC

vectors of documents of length at most T .

Proof of Theorem 4.1. Let S^ = {(A~nxi, yi) : (xi, yi)  S}, where A~n is as in Lemma 4.1. Then by the same lemma A~n is XT(n),  -RIP w.p. 1 - , where XT(n) is the set of BonC vectors of
documents of length at most T . By BonC assumption (1) all BonCs lie within the unit ball, so we can apply Theorem 4.2 with the logistic loss,  = 1, and R = 1 to get that a classifier w^ trained using l2-regularized logistic loss over S^ will satisfy the required bound (4). Since by Proposition 3.1 onecan initialize an O(nd)-memory LSTM that takes in i.i.d. Rademacher word vectors vw  Ud{±1/ d} such that zLSTM = z~(n) = A~nx  x  XT(n), this completes the proof.

5 SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS
Theorem 4.1 is proved using random vectors as word embeddings in the scheme of Section 3. However, in practice LSTMs are often initialized with standard word embeddings such as GloVe. These embeddings cannot fit into the usual compressed sensing worldview since the matrix defined by the embeddings cannot satisfy RIP. This follows essentially from the definition: word embeddings seek to capture word similarity, so similar words (e.g. synonyms) have embeddings with high inner product, which violates RIP. Thus the efficacy of real-life LSTMs must have some other explanation. But in this section we present the surprising empirical finding that pretrained word embeddings are more efficient than random vectors at encoding and recovering BoW information via compressed sensing. We further sketch a potential explanation for this surprising result, though a rigorous explanation is left for subsequent work.

5.1 PRETRAINED EMBEDDINGS PRESERVE SPARSE INFORMATION
In recent years word embeddings have been discovered to have many remarkable properties, most famously the ability to solve analogies (Mikolov et al., 2013). Our connection to compressed sensing indicates that they should have another: preservation of sparse signals as low-dimensional linear measurements. To examine this we subsample documents from the SST (Socher et al., 2013) and IMDB (Maas et al., 2011) sentiment classification datasets, embed them as d-dimensional unigram embeddings (2) for d = 50, 100, 200, . . . , 1600, and try to recover the BoW vectors by l1-minimization (i.e. basis pursuit - see Appendix A), measuring success as the F1 score of retrieved words. For embeddings we train Squared Norm (SN) vectors (Arora et al., 2016) on a corpus of Amazon reviews (McAuley et al., 2015) and use normalized i.i.d. Rademacher vectors as a baseline. SN is used due to its similarity to GloVe and its formulation via an easy-to-analyze generative model that may provide a mathematical framework to understand the results (see Appendix E.2), while the Amazon corpus is used for its semantic closeness to the sentiment datasets
Figures 1 and 2 show that pretrained embeddings require a lower dimension d than random vectors to recover natural language BoW. This is surprising since they certainly do not satisfy the RIP property often used in compressed sensing; indeed as shown in Figure 2 recovery is bad for randomly-generated word collections. The latter outcome indicates that the fact that a document is a set of mutually meaningful words is important for sparse recovery using embeddings trained on co-occurrences. We achieve similar results with other objectives (e.g. GloVe/word2vec) and other corpora (see Appendix E.1), although there is some sensitivity to the sparse recovery method, as l0-surrogate minimization methods work well but greedy methods, such as Orthogonal Matching Pursuit, work poorly, likely due to their dependence on mutual incoherence (Tropp, 2004).
For the n-gram case (i.e. BonC recovery for n > 1), although we know by Lemma 4.1 that DisC embeddings composed from random vectors satisfy RIP, for pretrained vectors it is unclear how to reason about suitable n-gram embeddings without a rigorous understanding of the unigram case, and experiments do not show the same recovery benefits. One could perhaps do well by training on cooccurrences of word tuples, but such embeddings could not be used by a word-level LSTM.

6

Under review as a conference paper at ICLR 2018

Figure 1: Average F1-score of 200 recovered BoW vectors from SST (left) and IMDB (right) compared to dimension. Pretrained word embeddings (SN trained on Amazon reviews) need half the dimensionality of normalized Rademacher vectors to achieve near-perfect recovery. Note that IMDB documents are on average more than ten times longer than SST documents.

Figure 2: F1-score of 500 recovered BoWs compared to number of unique words. Real documents (left) are drawn from the SST and IMDB corpora; random signals (right) are created by picking words at random. For d = 200, pretrained embeddings are better than Rademacher vectors as sensing vectors for natural language BoW but are worse for random sparse signals.

5.2 UNDERSTANDING SPARSE RECOVERY OF NATURAL LANGUAGE DOCUMENTS
As shown in Figure 2, the good performance of pretrained embeddings for linear sensing is a local phenomenon; recovery is only efficient on naturally occurring collections of words. Applying statistical RIP/incoherence ideas (Barg et al., 2015) is ruled out since they require word collections to be RIP with high probability, whereas standard word embeddings are trained give high inner product to words appearing in the same document. We thus turn to an older result that relies on a different geometric condition to establish local recovery. Theorem 5.1. (Donoho & Tanner, 2005) Suppose the columns of A  Rd×N are in general position and x  R+N is a nonnegative k-sparse vector for k < d. Then x is recoverable using l1-minimization iff the columns of A corresponding to the support supp(x) of x form a k-dimensional face of the convex hull conv(A) of the columns of A together with the origin.
Recalling that as a count vector xBoW is nonnegative, this result equates recovery of a BoW with the vectors of its words being vertices of the same face of polytope conv(A). Whereas RIP accomplishes this because vectors are so far apart that no one vector will be inside the simplex formed by any k others, pretrained embeddings satisfy this by having commonly co-occurring words close together and other words far away, reducing the effective vocabulary to words near the context c and making it easier to form a face. This intuition is formalized in the following corollary of Theorem 5.1:
Corollary 5.1. If x  RN+ is recoverable then supp(x) and the set of all other vectors are linearly separable. The converse holds if the vectors are unit-norm and their convex hull contains the origin.
Proof. = : If x is recoverable then by Theorem 5.1 the vectors in supp(x) form a face of conv(A). By convexity of conv(A) any hyperplane containing this face will separate the vectors in supp(x) from the rest. = : Since the points in supp(x) lie on the surface of a sphere and are in general position they will form a non-degenerate simplex that is linearly separated from the rest of conv(x) and thus is one of its faces. Then Theorem 5.1 implies recoverability of x.
Thus perfect recovery of the BoW is roughly equivalent to the existence of a hyperplane separating embeddings of words in the document from those of the rest of the vocabulary. Intuitively, words in the same document are trained to have similar embeddings and so will be more easy to separate than random vectors, providing some justification for why pretrained embeddings are better for sensing. As these similarity properties are also often used to explain the efficacy of pretrained embeddings on downstream tasks, we conjecture that there is a deeper relationship between word embeddings, sparse recovery, and classification performance that may perhaps be more rigorously understood using a generative model of text (see Appendix E.2). Note that the bounds in Section 4 depend on RIP and not perfect recoverability so the empirical results of Section 5.1 by themselves do not apply to that setting. They do show that the framework of compressed sensing remains relevant even in the case of non-random, pretrained word embeddings.
7

Under review as a conference paper at ICLR 2018

Representation n d MR CR SUBJ MPQA TREC SST (±1) SST IMDB

BonC (1)

1 V1 77.1 77.0 91.0 85.1 86.8 2 V2sum 77.8 78.0 91.8 85.8 90.0 3 V3sum 77.8 78.3 91.3 85.6 89.8

80.7 40.1 88.2 80.9 41.0 89.8 80.1 41.2 90.0

DisC (2)

1 1600 79.6 81.0 92.4 87.8 87.4 2 3200 80.1 81.5 92.6 87.9 89.0 3 4800 80.0 81.3 92.6 87.9 90.2

84.6 45.3 89.1 85.5 45.7 89.6 85.2 46.2 89.7

SIF1 1 1600 79.5 81.2 92.5 88.0 87.4

Sent2Vec2 1 700 76.2 79.2 91.2 87.1 85.8

Sent2Vec2 2 700 76.3 78.8 91.1 86.7 84.2

CFL3

5 100K+

84.2 45.1 88.9 80.2 42.4 85.5 80.0 41.8 85.3
90.4

Paragraph Vec.4

74.8 78.1 90.5 74.2 91.8

skip-thoughts4

4800 80.3 83.8 94.2 88.9

93.0

SDAE5

2400 74.6 78.0 90.8 86.9 78.4

CNN-LSTM6

4800 77.8 82.0 93.6 89.4 92.6

85.1 45.4

byte mLSTM7

4096 86.8 90.6 94.7 88.8

90.6

91.6 54.2 92.2

 Vocabulary sizes (i.e. BonC dimensions) vary by task; usually 10K-100K.  Vocabulary limited to (unordered) n-grams that occur at least 10 times in the training corpus. 1 Arora et al. (2017) Reported performance of best hyperparameter using Amazon GloVe embeddings. 2,4,7 Pagliardini et al. (2017); Kiros et al. (2015); Radford et al. (2017) Latest pretrained models. 3,5,6 Paskov et al. (2013); Hill et al. (2016); Gan et al. (2017) From publication (+emb version of latter two).

Table 1: Evaluation of DisC and recent unsupervised word-level approaches on standard classification tasks, with the character LSTM of Radford et al. (2017) shown for comparison. The top three results for each dataset are bolded, the best is italicized, and the best word-level performance is underlined.

6 EMPIRICAL FINDINGS
Our theoretical results show that simple tensor product sketch-based n-gram embeddings can approach BonG performance and be computed by a low-memory LSTM. In this section we compare these text representations and others on several standard tasks, verifying that DisC performance approaches that of BonCs as dimensionality increases and establishing several baselines for text classification.
Tasks: We test classification on MR movie reviews (Pang & Lee, 2005), CR customer reviews (Hu & Liu, 2004), SUBJ subjectivity dataset (Pang & Lee, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), TREC question classification (Li & Roth, 2002), SST sentiment classification (binary and fine-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011). The first four are evaluated using 10-fold cross-validation, while the others have train-test splits. In all cases we use l2-regularized logistic regression with regularization determined by cross-validation.

Rep. n SICK-R (r/) SICK-E MRPC (Acc./F1)

DisC (2)

1 2 3

73.6 / 71.0 75.6 / 72.1 76.2 / 72.2

81.9 83.0 82.6

73.4 / 81.4 74.1 / 81.8 74.0 / 81.7

estimated

SIF Sent2Vec Sent2Vec

1 1 2

75.2 / 69.4 69.3 / 64.6 70.1 / 65.3

80.4 78.8 78.4

71.3 / 80.5 71.0 / 80.4 70.2 / 79.4

skip-thoughts SDAE
CNN-LSTM

83.1 / 76.4

83.0

73.1 / 80.9 73.7 / 80.7 76.4 / 83.8

byte mLSTM 78.4 / 71.9

79.5

74.6 / 82.1

Models information is the same as those in Table 1.

Figure 3: Time needed to initialize

Table 2: Performance of DisC and other recent approaches model, construct document representa-

on pairwise similarity and classification tasks. The top three tions, and train a linear classifier on a

results for each task are bolded and the best is underlined. 16-core compute node.

8

Under review as a conference paper at ICLR 2018

Figure 4: IMDB sentiment classification performance of uni- Figure 5: IMDB sentiment classi-

gram embeddings (left) and bigram DisC embeddings (right) fication performance compared to

compared to the dimensionality of the original vectors.

the training sample size.

We further test DisC on the SICK relatedness and entailment tasks (Marelli et al., 2014) and the MRPC paraphrase detection task (Dolan & Brockett, 2005). The inputs here are pairs of sentences a and b, which we featurize in the standard way using the concatenation [|xa - xb|, xa xb], where xa, xb are the documents embeddings of a and b (Tai et al., 2015). We use l2-regularized logistic regression for SICK entailment and MRPC and use ridge regression to predict similarity scores for SICK relatedness, with regularization determined by cross-validation. Since BonGs are not used for these tasks our theory says nothing about performance here, even in the random embedding case; we include these evaluations to show that our representations are useful for tasks besides classification.
Embeddings: In the main evaluation (Table 1) we use normalized 1600-dimensional GloVe embeddings (Pennington et al., 2014) trained on the Amazon Product Corpus (McAuley et al., 2015). We also compare the SN vectors of Section 5 trained on the same corpus with random vectors when varying the dimension (Figure 5). Both embeddings and code will be released for reproducibility.
Results: We find that DisC embeddings, despite their simplicity, perform consistently well relative to recent unsupervised methods; comparing to word-level approaches our representation is the top performer on the two SST tasks and competes with skip-thoughts and CNN-LSTM on many others (note that the latter two results are concatenations of two LSTM representations). While success in some cases may be explained by training on a large and in-domain corpus, being able to use so much text without extravagant computational resources is one of the advantages of a simple approach. Overall our method is useful as a strong baseline, often beating BonCs and many more complicated approaches while taking much less time to represent and train on documents than neural representations (Figure 3).
Finally, we analyze empirically how well our model approximates BonC performance. As predicted by Theorem 4.1, the performance of random embeddings on IMDB approaches that of BonC as dimension increases and the isometry distortion  decreases (Figure 5). Using pretrained (SN) vectors DisC embeddings approach BonC performance much earlier, even beating it in the unigram case.
7 CONCLUSION
In this paper we explored the close connection between compressed sensing, learning, and natural language representation. We first related LSTMs and BonG methods via word embeddings, coming up with new, simple document embeddings based on tensor product sketches. Then we connected these representations to classification performance, providing a generalization of the compressed learning results of Calderbank et al. (2009) to convex Lipschitz loss functions and using this to prove a bound on the loss of a low-dimensional LSTM classifier in terms of its (modified) BonG counterpart. We know of no such prior result, and empirical work had been unable to resolve the issue. Finally, we showed how pretrained word embeddings fit into this sparse recovery framework, demonstrating and explaining their ability to efficiently preserve natural language information.
Our work attempts to further our understanding of how best to relate compositional and distributional semantics, with the latter preserving the information of the former through the theory of compressed sensing. We expect the techniques and insights we introduce to find application in a wide variety of tasks in natural language processing.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385­399, 2016.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In Proceedings of the International Conference on Learning Representations, 2017.
Alexander Barg, Arya Mazumdar, and Rongrong Wang. Restricted isometry property of random subdictionaries. IEEE Transactions on Information Theory, 61, 2015.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137­1155, 2003.
Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain. Technical report, 2009.
Emmanuel Candes and Terence Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51:4203­4215, 2005.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, 2008.
Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing, 2005.
David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations by linear programming. Proceedings of the National Academy of Sciences of the United States of America., 102:9446­9451, 2005.
Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin Carin. Learning generic sentence representations using convolutional neural networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of NAACL-HLT 2016, 2016.
Geoffrey Hinton. Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence, 46:47­75, 1990.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9: 1735­1780, 1997.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, 2017.
Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation, 1:139­159, 2009.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems, 2015.
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics, 2002.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 2011.
10

Under review as a conference paper at ICLR 2018
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic model on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation, 2014.
Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and complementary products. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems. 2013.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings using compositional n-gram features. arXiv, 2017.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, 2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, 2005.
Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning. In Advances in Neural Information Processing Systems, 2013.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014.
Tony Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 1995.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment, 2017. arXiv.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.
Karthik Sridharan, Nathan Srebro, and Shai Shalev-Schwartz. Fast rates for regularized objectives. In Advances in Neural Information Processing Systems. 2008.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, 2015.
Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50, 2004.
Sida Wang and Christopher D. Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, 2012.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39:165­210, 2005.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic sentence embeddings. In Proceedings of the International Conference on Learning Representations, 2016.
11

Under review as a conference paper at ICLR 2018

A OVERVIEW OF COMPRESSED SENSING

The field of compressed sensing is concerned with recovering a high-dimensional k-sparse signal x  RN from few linear measurements. In the noiseless case this is formulated as

minimize w 0 subject to Aw = z

(8)

where A  Rd×N is the design matrix and z = Ax is the measurement vector. Since l0-minimization

is NP-hard, a foundational approach is to replace it by its convex surrogate, the l1-norm, and

characterize conditions under which the solution to (8) is equivalent to the following LP, known as

basis pursuit (BP):

minimize w 1 subject to Aw = z

(9)

One such condition is the Restricted Isometry Property (RIP): Definition A.1. A  Rd×N is (k, )-RIP if for all k-sparse x  RN
(1 - ) x 2  Ax 2  (1 + ) x 2

A line of work started by Candes & Tao (2005) used the RIP property to characterize matrices A

such that (8) and (9) have the same minimizer; this occurs with overwhelming probability when

d=O

k

log

N k

and

dAij  N (0, 1)  i, j or

dAij  U {-1, 1}  i, j.

Since the ability to recover a signal x from a compressed representation Ax implies information
preservation, a natural next step is to consider learning after compression. Calderbank et al. (2009) show that if we have i.i.d. k-sparse samples {(xi, yi)}mi=1 and A is (2k, )-RIP then the loss of a classifier trained on {(Axi, yi)}mi=1 using the l2-regularized hinge loss is bounded by that of the best linear classifier over the original samples. In Theorem 4.2 we provided a generalization of this result
to arbitrary convex Lipschitz loss functions.

RIP is a strong requirement, both because it is not necessary for perfect, stable recovery of k-sparse vectors using O~(k) measurements and because in certain settings we are interested in using the above ideas to recover specific signals -- those statistically likely to occur--rather than all k-sparse signals. The usual necessary and sufficient condition for this is the local nullspace property (NSP):
Definition A.2. Given a matrix A  Rd×N , a vector x  RN satisfies the local nullspace property if wSx 1 < wS¯x 1  w  ker(A)\{0N }, where Sx = supp(x) is the index support of x.

NSP is generally difficult to show, but the following geometric results provides useful intuition for our case of nonnegative x:
Theorem A.1 (Donoho & Tanner (2005)). Suppose the columns of A  Rd×N are in general position and x  RN+ is a nonnegative k-sparse vector for k < d. Then x satisfies local NSP (and thus local equivalence of (8) and (9)) iff the columns supp(x) of A corresponding to the support of x form a k-dimensional face of the convex hull conv(A) of the columns of A together with the origin.

An alternative way to solve (8), without using the convex surrogate, is to use a greedy algorithm such as matching pursuit (MP) and orthogonal matching pursuit (OMP), which pick basis vectors one at a time by multiplying the measurement vector by AT and choosing the column with the largest inner product. However, guarantees for these algorithms are not based on NSP but rather on mutual incoherence (Tropp, 2004):
Definition A.3. A  Rd×N is µ-incoherent if maxa,a |aT a |  µ, where the maximum is taken over any two distinct columns a, a of A.

We use this property in our proof of Lemma 4.1.

12

Under review as a conference paper at ICLR 2018

B DOCUMENT EMBEDDINGS

B.1 PERFORMANCE COMPARISON OF ALTERNATIVE REPRESENTATIONS

In this section we compare the performance of several alternative representations with the ones presented in the main evaluation (Table 1). Table 3 provides a numerical justification for our use of unordered n-grams (cooccurrences) instead of n-grams, as the performance of the two featurizations are closely comparable. In Table 4 we examine the use of circular convolution instead of elementwise multiplication as linear measurements of BonC vectors Plate (1995). To construct the former from a document w1, . . . , wT we compute

T T -n+1

vwt , . . . ,

F -1

t=1 t=1

t+n-1
F (vw )
 =t

(10)

where F is the discrete Fourier transform and F -1 its inverse. Note that for n = 1 this is equivalent to the simple unigram embedding (and thus also to the DisC embedding in (2)).

Model Representation MR CR SUBJ MPQA TREC SST (±1) SST IMDB

Bigram

BonG BonC

78.3 78.2 91.9 85.7 90.0 77.8 78.0 91.8 85.8 90.0

80.8 40.4 89.8 80.9 41.0 89.8

Trigram

BonG BonC

77.9 78.3 91.6 85.6 89.2 77.8 78.3 91.3 85.6 89.8

80.2 40.5 90.0 80.1 41.2 90.0

Table 3: The performance of an l2-regularized logit classifier over Bag-of-n-Grams (BonG) vectors is generally similar to that of Bag-of-n-Cooccurrences (BonC) vectors for n = 2, 3 (largest differences bolded). Evaluation settings are the same as in Section 6. Note that for unigrams the two representations are equivalent.

Model Representation MR CR SUBJ MPQA TREC SST (±1) SST IMDB

Bigram

DisC (2) Conv. (10)

80.1 81.5 92.6 79.1 81.0 92.8

87.9 87.4

89.0 90.4

85.5 45.7 89.6 83.5 45.7 89.5

Trigram

DisC (2) Conv. (10)

80.0 81.3 92.6 78.8 80.6 92.6

87.9 87.2

90.2 90.2

85.2 46.2 89.7 83.1 44.7 89.5

Table 4: Performance comparison of element-wise product (DisC) and circular convolution for encoding local cooccurrences (best result for each task is bolded). Evaluation settings are the same as in Section 6. Note that for unigrams the two representations are equivalent.

B.2 SCALING FACTOR FOR DISC EMBEDDINGS



Lemma B.1. If word vectors vw  Rd are drawn i.i.d. from U d{±1/ d} then for any n-gram

g = (w1, . . . , wn) we have E

v~g

2 2

=

1.

The

same

result

holds

true

with

the

additional

assumption

that all words in g are distinct if the word vectors are i.i.d. d-dimensional spherical Gaussians.

Proof.

d

E

v~g

2 2

=

E

i=1

n

n-1
d2

vwk i

k=1

2d
= dn-1 E
i=1

n
vw2 k i
k=1

d
= dn-1

n1 =1

d

i=1 i=1

13

Under review as a conference paper at ICLR 2018

B.3 PROOF OF PROPOSITION 3.1

Let f (x) = i(x) = g(x) = x with

Tf (vwt , ht-1) =

1nd 0(n-1)d

 0d×nd

 

...



Ti(vwt ,

ht-1)

=

 

...

  

...

0(n-2)d×nd

···
I(n-2)d ... ...
I(n-2)d

 C1Id 

 ... 



Tg(vwt ,

ht-1)

=

Cn

d

n-1 2

 

Id

Id 


vwt

 

...

 

Id

0d×d 

0(n-2)d×d

 1d 

Id

   

ht-1

+

0(n-1)d  1d 

 0d×d 

0(n-2)d

0(n-2)d×d

Substituting these parameters into the LSTM update (3) and using h0 = 0 we have  t > 0 that

t



t



C1 vw

  =1

  

...

   

C1 v~w
 =1
...

   

ht

=



Cn

d

n-1 2

t-n+1

  =1

 

vwt

n k=1

  v w+k-1  

=

  n-1 Cnd 2  

t-n+1

 

 =1

v~{w

,...,w

+n-1

}

 



  v~wt 

  

...

  

...

  

vn-1
k=1 wt+k-n+1

v~{wt-n+2 ,...,wt }

Thus

T



C1 v~wt

 t=1



  

...



 

z~(n)



hT

=

  n-1 Cnd 2  

T -n+1

 

v~{wt

,...,wt+n-1

}

 

t=1 

=

  

v~wT ...

  

   

v~wT ...

 

v~{wT -n+2,...,wT }





v~{wT -n+2,...,wT }

Note that ht  R(2n-1)d so as desired the LSTM has O(nd)-memory. Although hT contains (n-1)d more dimensions than z~(n), by padding the end of the document with an end-of-document token

whose word vector is 0d the entries in those dimensions will be set to zero by the update at the last step. Thus up to zero padding we will have zLSTM = hT = z~(n).

14

Under review as a conference paper at ICLR 2018

C PROOF OF THEOREM 4.2

Throughout this section we assume the setting described in Theorem 4.2. Furthermore for some positive constant C define the l2-regularization of the loss function as

L(w) =

1 (w) +
2C

w

2 2

m

Lemma C.1.

Let w^ be the classifier obtained minimizing LS(w) =

1 m

(wT

xi, yi)

+

1 2C

w

2 2

,

i=1

where (·, ·) is a convex -Lipschitz function in the first cordinate. Then

m
w^ = iyixi
i=1

(11)

where

|i|



C m



i.

This

result

holds

in

the

compressed

domain

as

well.

Proof. If is an -Lipschitz function, its sub-gradient at every point is bounded by . So by convexity, the unique optimizer is given by taking first-order conditions:

w1 0 = wLS(w) = C + m

m

wT xi

(wT xi, yi)xi

i=1

=

C w^ =
m

m

-yiw^T xi

(w^T xi, yi)yixi

i=1

(12)

Since is Lipschitz, |wT xi (wT xi, yi)|  . Therefore the first-order optimal solution (12) of w^

can

be

expressed

as

(11)

for

some

1, . . . , m

satisfying

|i|



C m



i,

which

is

the

desired

result.

Lemma C.2. x, x  X = (1 + )xT x - 2R2  (Ax)T (Ax )  (1 - )xT x + 2R2

Proof. Since A is (X , )-RIP we have (1 - ) x - x 2  A(x - x ) 2  (1 + ) x - x 2. Also since 0N  X , A is also (X , )-RIP and the result then follows by the same argument as in
(Calderbank et al., 2009, Lemma 4.2-3).

Corollary C.1.

w^

2 2



2 C 2 R2

and

w^A

2 2



2 C 2 (1

+

)2R2.

Proof. The first bound follows by expanding

w^

2 2

and using

x 2  R; the second follows by

expanding

w^A

2 2

,

applying

Lemma

C.2

to

bound

inner

product

distortion,

and

using

x 2  R.

Lemma C.3. Let w^ be the linear classifier minimizing LS. Then LD(Aw^)  LD(w^) + O(2CR2)

Proof. By Lemma C.1 we can re-express w^ using Equation 11 and then apply the inequality from Lemma C.2 to get

m
(Aw^)T (Ax) = iyi(Axi)T (Ax)

i=1

 iyi (1 - )xiT x + 2R2 + iyi (1 + )xiT x - 2R2

i:i yi 0

i:i yi <0

mm

= w^T x -  |iyi|xTi x + 2R2 |iyi|  w^T x + 3CR2

i=1 i=1

15

Under review as a conference paper at ICLR 2018

m
(Aw^)T (Ax) = iyi(Axi)T (Ax)

i=1

 iyi (1 + )xTi x - 2R2 + iyi (1 - )xiT x + 2R2

i:i yi 0

i:i yi <0

mm

= w^T x +  |iyi|xiT x - 2R2 |iyi|  w^T x - 3CR2

i=1 i=1

for any x  RN . Since is -Lipschitz taking expectations over D implies

D(Aw^)  D(w^) + 32CR2

(13)

Substituting Equation 11 applying Lemma C.2 also yields

mm

Aw^

2 2

=

ij yiyj (Axi)T (Axj )

i=1 j=1

 ij yiyj (1 - )xTi xj + 2R2
i,j:ij yiyj 0

+ ij yiyj (1 + )xTi xj - 2R2
i,j:ij yiyj <0

 ij yiyj xiT xj + -|ij yiyj |xTi xj + 2R2|ij yiyj |
i,j i,j



w^

2 2

+

32 C 2 R2 

which implies

1 2C

Aw^

2 2



1 2C

w^

2 2

+

3 2CR2 2

(14)

Together the inequalities bounding the loss term (13) and the regularization term (14) imply the result.

Lemma C.4. Let w^ be the linear classifier minimizing LS and let w be the linear classifier minimizing LD. Then with probability 1 - 

LD(w^)  LD(w) + O

2CR2 1 log
m

This result holds in the compressed domain as well.

Proof. By Corollary C.1 we have that w^ is contained in a closed convex subset independent of S.

Therefore since

is

-Lipschitz,

L

is

1 C

-strongly

convex,

and

x 2  O(R), we have by (Sridharan

et al., 2008, Theorem 1) that with probability 1 - 

LD(w^) - LD(w)  2 [LS(w^) - LS(w)]+ + O

2CR2 1 log
m

Then since by definition w^ minimizes LS(w) we have that LS(w^)  LS(w), which substituted into the previous equation completes the proof.

Proof of Theorem 4.2. Applying Lemma C.4 in the compressed domain yields

D(w^A) 

1 D(w^A) + 2C

w^A

2 2

=

LD (w^A )



LD(wA )

+

O

2CR2 1 log
m

where wA minimizes LD. By definition of wA , LD(wA )  LD(Aw^), so together with Lemma C.3 and the previous inequality we have

D(w^A)  LD(Aw^) + O

2CR2 1 log
m

 LD(w^) + O 2CR2

11  + log
m

16

Under review as a conference paper at ICLR 2018

We now apply Lemma C.4 in the sparse domain to get

D(w^A)  LD(w) + O

2 C R2

11  + log
m

where w minimizes LD. By definition of w, LD(w)  LD(w0) =

D (w0 )

+

1 2C

w0

22, so by

the previous inequality we have

D(w^A) 

1 D(w0) + 2C

w0

2 2

+

O

2 C R2

11  + log
m

Substituting the C that minimizes the r.h.s. of this inequality completes the proof.

D PROOF OF LEMMA 4.1

We assume the setting described in Lemma 4.1, where we are concerned with the RIP condition of the matrix A~n when multiplying vectors x  XT(n), the set of BonC vectors for documents of length at most T . This matrix can be written as

 A1

A~n

=

0d×V1

 

...

0d×V1

0d×V2 ...
... ···

··· ... ... 0d×Vn-1

0d×Vn  ...  

0d×Vn  An

where Ap is the d × Vp matrix whose columns are the DisC embeddings of all p-grams in the vocabulary (and thus A~1 = A1 = A, the matrix of the original word embeddings). Note that from (1) any x  XT(n) can be writen as x = [x1, . . . , xn], where xp is a T -sparse vector whose entries correspond to p-grams. Thus we also have A~nx = [A1x1, . . . , Anxn].

Lemma D.1. If Ap is (2k, )-RIP w.p. 1 -   p  [n] then A~n is Xk(n),  -RIP w.p. at least 1 - n.

Proof. By union bound we have that Ap is (2k, )-RIP  p  [n] with probability at least 1 - n. Thus by Definition 4.1 we have w.p. 1 - n that  x  Xk(n)

nn

A~nx

2 2

=

Apxp

2 2



(1 + )2

xp

2 2

=

(1

+

)2

x

2 2

p=1

p=1

Similarly,

A~nx

2 2

 (1 - )2

x 22. From Definition 4.1, taking the square root of both sides of both

inequalities completes the proof.

 Lemma D.2. If the word embeddings are drawn i.i.d. from Ud{±1/ d} then for any p  [n] the

matrix Ap  Rd×Vp of DisC embeddings is µ-incoherent w.p. at least 1 - exp -Cµ2d + 2 log Vp

for some universal positive constant C.

Proof. Consider any two distinct p-grams g = {g1, . . . , gp} and g = {g1, . . . , gp} and denote their

p-1
DisC embeddings by a = d 2

p t=1

vgtand

a

p-1
=d 2

p t=1

vgt

,

respectively.

Now

since

the

word embeddings are drawn from Ud{±1/ d} any entry of the elementwise product of two of them

will be ±1/d. Therefore we have

aiai =

p

p-1
d2

vgt i

t=1

p

p-1
d2

vgt i

t=1

p

= dp-1

vgt ivgt i  U

t=1

±1 d

where we have used the fact that g, g are distinct and no word occurs more than once in a p-gram to
see that there will be at least one word occurring exactly once in the product and so by symmetry the distribution over ±1/d will be uniform.

17

Under review as a conference paper at ICLR 2018

We then have the expectation E(aT a ) = E

d i=1

aiai

=

d i=1

E(aiai)

=

0

so

by

independence

of

row entries we can apply Hoeffding's inequality over variables aiai  [-1/d, 1/d] to get

P |aT a |  µ = P

d
aiai  µ
i=1

  2 exp -2µ2

d4 d2
i=1

-1  = exp -Cµ2d

for some constant C. Applying the above to all pairs of columns a = a of Ap and taking a union

bound yields P

max |aT a |  µ
a=a

 Vp2 exp -Cµ2d . The result follows by Definition A.3.

Lemma D.3. If A  Rd×V is µ-incoherent and has unit-norm columns then it is (k, kµ)-RIP  k  N.

Proof. Consider any k-sparse vector x  RV . Then since

a

2 2

= 1 for any column a of A we have

Ax

2 2

-

x

2 2

=

xT (AAT - IV )x



|aiT aj ||xixj |  µ

xi2 + xj2 2

< kµ

x

2 2

i=j i=j

The result follows from Definition 4.1.

Proof of Lemma 4.1.

Substituting µ =

 T

,

Lemmas

D.2

and

D.3

imply

that

for

some

C

> 0 matrix

Ap is (2T, )-RIP w.p. at least 1 - exp

-C

2 d T2

+

log

Vm2ax

 p  [n]. Then by Lemma D.1 A~n

is

XT(n),  -RIP w.p. at least 1 - n exp

-C

2 d T2

+

log Vm2ax

= 1 - exp

-C

2 d T2

+

log

nVm2ax

.

Equivalently, if d = 

T2 2

log

nVmax 

then A~n is

XT(n),  -RIP w.p. at least 1 - .

E SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS
E.1 PERFORMANCE COMPARISON OF ALTERNATIVE EMBEDDINGS
We show in Figure 6 that the surprising effectiveness of word embeddings as linear measurement vectors for BoW signals holds for other embeddings trained on other corpora as well. Specifically, we see that many widely used embeddings, when normalized, match the efficiency of random vectors for retrieving SST BoW and are in fact more efficient when retrieving IMDB BoW. Interestingly, SN vectors are the most efficient and are also the only embeddings for which normalization does not negatively affect performance.

Recovery of SST Documents

Legend
Spherical Gaussian Vectors Normalized Rademacher Vectors GloVe (normalized/not normalized) SN (normalized/not normalized) word2vec (normalized/not normalized)

Recovery of IMDB Documents

F1-Score F1-Score

Random Vectors

Amazon Common Google

Reviews

Crawl News

Training Corpus

English Wikipedia

Random Vectors

Amazon Common Google

Reviews

Crawl News

Training Corpus

English Wikipedia

Figure 6: Efficiency of pretrained embeddings as sensing vectors at d = 300 dimensions, measured via the F1-score of the original BoW. 200 documents from each dataset were compressed and recovered in this experiment. word2vec embeddings trained on Google News and GloVe embeddings trained on Common Crawl were obtained from public repositories (Mikolov et al., 2013; Pennington et al., 2014) while Amazon and Wikipedia embeddings were trained for 100 iterations using default GloVe/SN settings. 300-dimensional normalized random vectors are used as a baseline.

18

Under review as a conference paper at ICLR 2018

E.2 A MODEL-BASED THEORETICAL APPROACH

In Section 5.2 we gave some intuition for why pretrained word embeddings are efficient sensing vectors for natural language BoW by examining a geometric characterization of local equivalence due to Donoho & Tanner (2005) in light of the usual similarity properties of word embeddings. However, this analysis does not provide a rigorous theory for our empirical results. In this section we briefly discuss a model-based justification that may lead to a stronger understanding.

We need a model that relates how a BoW vector is generated to the word embeddings trained over

words co-occurring in the same BoW. As a starting point consider the model of Arora et al. (2016), in

which a corpus is generated by a random walk ct over the surface of a ball in Rd; at each time t a word w is emitted according to

P(w|ct)  exp ct, vw

(15)

Minimizing the (GloVe-like) SN objective approximately maximizes the log-likelihood of a corpus based on this model.

Thus in an approximate sense a document of length T is generated by setting a context vector c and emitting T words via (15) with ct = c. This model is a convenient one for analysis due its simplicity and invariance to word order as well as the fact that the approximate maximum likelihood document vector is the sum of the embeddings of words in the document. Building upon the intuition established following Corollary 5.1 one can argue that, if we have the true latent SN vectors, then embeddings of words in the same document (i.e. emitted by the same context vector) will be close to each other and thus easy to separate from the embeddings of other words in the vocabulary.

However, we find empirically that not all of the T words closest to the sum of the word embeddings (i.e. the context vector) are the ones emitted; indeed individual word vectors in a document may have small, even negative inner product with the context vector and still be recovered via BP. Thus any further theoretical argument must also be able to handle the recovery of lower probability words whose vectors are further away from the context vector than those of words that do not appear in the document. We thus leave to future work the challenge of explaining why embeddings resulting from this (or another) model provide such efficient sensing matrices for natural language BoW.

19

