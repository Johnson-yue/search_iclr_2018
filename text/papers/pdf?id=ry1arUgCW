Under review as a conference paper at ICLR 2018

DORA THE EXPLORER: DIRECTED OUTREACHING REINFORCEMENT ACTION-SELECTION
Anonymous authors Paper under double-blind review

ABSTRACT
Exploration is a fundamental aspect of Reinforcement Learning. Two key challenges are how to focus exploration on more valuable states, and how to direct exploration toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality, considering only the immediate one step exploration value. While there are a few model-based solutions to this difficulty, a model-free approach is still missing. We propose E-values, a generalization of counters that can be used to evaluate the propagating exploratory value over stateaction trajectories. We compare our approach to commonly used RL techniques, and show that using E-value improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to learn continuous MDPs.

1 INTRODUCTION
"If there's a place you gotta go - I'm the one you need to know." (Map, Dora The Explorer)

We consider Reinforcement Learning in a Markov Decision Process (MDP). An MDP is a five-tuple M = (S, A, P, R, ) where S is a set of states and A is a set of actions. The dynamics of the process is given by P (s |s, a) which denotes the transition probability from state s to state s via the action a. Each such transition also has a distribution R (r|s, a) from which the reward for such transitions is sampled. Given a policy  : S  A, a function ­ possibly stochastic ­ deciding which actions to take in each of the states, the state-action value function Q : S × A  R is defined as:
Q (s, a) = E [r + Q (s ,  (s ))]
r,s R×P (·|s,a)

Where  is the discount factor. The agent's goal is finding an optimal policy  that maximizes Q (s,  (s)). For brevity, Q Q. There are two main approaches for learning . The first is a model-based approach, where the agent learns an internal model of the MDP (namely P and R). Given a model, the optimal policy could be found using dynamic programming methods such as Value Iteration (Sutton & Barto, 1998). The other alternative is a model-free approach, where the agent learns only the value function of states or state-action pairs, without learning a model (Mongillo et al., 2014).

The ideas put forward in this paper are relevant to any model-free learning of MDPs. For concreteness, we focus on a particular example, Q-Learning (Watkins & Dayan, 1992; Sutton & Barto, 1998). Q-Learning is a common method for learning Q, where the agent iteratively updates its values of Q (s, a) by performing actions and observing their outcomes. At each step after the agent takes action at the agent is transferred from st to st+1 observing reward r, it applies the update rule regulated by a learning rate :

Q (st, at)  (1 - ) Q (st, at) + 

r

+



max
a

Q

(st+1,

a)

1.1 EXPLORATION AND EXPLOITATION

One of the challenges in every reinforcement learning framework is balancing between Exploration and Exploitation. Seemingly, the agent may want to choose the alternative associated with the

1

Under review as a conference paper at ICLR 2018
highest expected reward, a behavior known as exploitation. However, in that case it may fail to learn that there are better options. Therefore exploration, namely the taking of new actions and the visit of new states may also be beneficial. It is important to note that exploitation is also inherently relevant for learning, as we want the agent to have better estimations of the valuable state and actions and we care less about the exact values of actions the agent already knows to be clearly inferior.
Formally, to guarantee convergence to Q, the Q-Learning algorithm must visit each state-action pair infinitely many times. Naively, a random walk behavior of the agent could account for exploration, and would suffice for converging asymptotically. However, random exploration has two major limitations when the learning process is finite. First, the agent will not explore the more relevant trajectories by exploiting its current knowledge about the world. For example, an action with a known disastrous outcome will be explored over and over again. Second, the agent will not be biased in favor of exploring unvisited trajectories more than the visited ones ­ hence "wasting" exploration resources on actions and trajectories which are already well known to it.
A widely used method to deal with the first problem is the -greedy schema (Sutton & Barto, 1998), in which with probability 1 - the agent greedily chooses the best action, and with probability it chooses a random action to encourage exploration. Another popular alternative, emphasizing the preference to learn about actions estimated to be with higher rewards, is to draw actions from a Boltzmann Distribution (Softmax) over the learned Q values, regulated by a Temperature parameter. While such approaches leads to more informed exploration that is based on learning experience, they still fails to address the second issue, namely they are not directed (Thrun, 1992) towards gaining more knowledge by biasing actions in the direction of unexplored trajectories.
Another important approach in the study of efficient exploration is the notion of Sample Complexity of Exploration as defined in the PAC-MDP literature (Kakade et al., 2003). Relevant to our work is Delayed Q Learning (Strehl et al., 2006), a model-free algorithm that has theoretical PAC-MDP guarantees. However, to ensure these theoretical guarantees this algorithm uses a conservative exploration which might be impractical (see also (Kolter & Ng, 2009) and Appendix B).
1.2 CURRENT DIRECTED EXPLORATION AND ITS LIMITATIONS
In order to achieve directed exploration, the estimation of an exploration or information value of different actions (often termed exploration bonus) is needed. The most commonly used exploration bonus is based on counting (Thrun, 1992) ­ for each pair (s, a), store a counter C (s, a) that indicates how many times the agent performed action a at state s so far. Counter-based methods are widely used both in practice and in theory (Kolter & Ng, 2009; Strehl & Littman, 2008; Guez et al., 2012; Busoniu et al., 2008). Other options for evaluating exploration include recency and value difference (or error) measures (Thrun, 1992; Tokic & Palm, 2011). While all of these exploration measures can be used for directed exploration, their major limitation is that the exploratory value of a state-action pair is evaluated with respect only to its immediate outcome, one step ahead. It seems desirable to determine the exploratory value of an action not only by how much new immediate knowledge the agent gains from it, but also by how much more new knowledge could be gained from a trajectory starting with it.
2 LEARNING EXPLORATION VALUES
2.1 PROPAGATING EXPLORATION VALUES
The challenge discussed in 1.2 is in fact similar to that of learning the value functions. The value of a state-action represents not only the immediate reward, but also the temporally discounted sum of expected rewards over a trajectory starting from this state and action. Similarly, the "explorationvalue" of a state-action should represent not only the immediate knowledge gained but also the expected future gained knowledge. This suggests that a similar approach used for value-learning might be appropriate for learning the exploration values as well, using exploration-values as the immediate reward. However, because it is reasonable to require exploration values to decrease over repetitions of the same trajectories, a naive implementation would violate the Markovian property.
This challenge has been addressed in a model-based settings: The idea is to use at every step the current estimate of the parameters of the MDP in order to compute, using dynamic programming, the
2

Under review as a conference paper at ICLR 2018
future exploration bonus(Little & Sommer, 2014). However, this solution cannot be implemented in a model-free settings. Therefore, a satisfying approach for propagating directed exploration in model-free reinforcement learning is still missing. In this section, we propose such an approach.
2.2 E-VALUES
We propose a novel approach for directed exploration, based on two parallel MDPs. One MDP is the original MDP, which is used to estimate the value function. The second MDP is identical except for one important difference. We posit that there are no rewards associated with any of the state-actions. Thus, the true value of all state-action pairs is 0. We will use an RL algorithm to "learn" the "actionvalues" in this new MDP which we denote as E-values. We show that these E-values represent the missing knowledge and thus can be used for propagating directed exploration. This will be done by initializing E-values to 1. These positive initial conditions will subsequently result in an optimistic bias that will lead to directed exploration, by giving high estimations only to state-action pairs from which an optimistic outcome was not yet excluded by the agent's experience.
Formally, given an MDP M = (S, A, P, R, ) we construct a new MDP M = (S, A, P, 0, E) with 0 denoting the identically zero function, and 0  E < 1 is a discount. The agent now learns both Q and E values concurrently, while initially E (s, a) = 1 for all s, a. Note that while E = 0, intuitively the value of E (s, a) at a given timestep during training stands for the knowledge, or uncertainty, that the agent has regarding this state-action pair. Eventually, after enough exploration, there is no additional knowledge left to discover which corresponds to E (s, a)  0.
For learning E, we use the SARSA algorithm (Rummery & Niranjan, 1994; Sutton & Barto, 1998) which differs from Watkin's Q-Learning by being on-policy, following the update rule:
E (st, at)  (1 - E) E (st, at) + E (r + EE (st+1, at+1)) Where E is the learning rate. For simplicity, we will assume throughout the paper that E = .
Note that this learning rule updates the E-values based on E (st+1, at+1) rather than maxa E (st+1, a), thus not considering potentially highly informative actions which are never selected. This is important for guaranteeing that exploration values will decrease when repeating the same trajectory (as we will show below). Maintaining these additional updates doesn't affect the asymptotic space/time complexity of the learning algorithm, since it is simply performing the same updates of a standard Q-Learning process twice.
2.3 E-VALUES AS GENERALIZED COUNTERS
The logarithm of E-Values can be thought of as a generalization of visit counters, with propagation of the values along state-action pairs. To see this, let us examine the case of E = 0 in which there is no propagation from future states. In this case, the update rule is given by:
E (s, a)  (1 - ) E (s, a) +  (0 + EE (s , a )) = (1 - ) E (s, a) So after visited n times, the value of the state-action pair is (1 - )n, where  is the learning rate. By taking a logarithm transformation, we can see that log1- (E) = n. Moreover, in the case where s is a terminal state having only one action, it behaves this way for any value of E.
In cases where E > 0 and for non-terminal states E will decrease slower, and therefore log1- E will increase slower than a counter. The exact rate will depend on the MDP, the policy and the specific value of E. Crucially, for state-actions which lead to many potential states, each visit contributes less to the generalized counter, because more visits are required to exhaust the potential outcomes of the action. To gain more insight, consider the MDP depicted in figure 1a, a tree with the root as initial state and the leaves as terminal states. If actions are chosen sequentially, one leaf after the other, we expect that each complete round of choices (which will result with k actual visits of the (s, start) pair) will be roughly equivalent to one generalized counter. Simulation of this and other simple MDPs show that E-values behave according to such intuitions (see figure 1b).
An important property of E-values is that it decreases over repetition. Formally, by completing a trajectory of the form s0, a0, . . . , sn, an, s0, a0 in the MDP, the maximal value of E (si, ai) will decrease. To see this, assume that E (si, ai) was maximal, and consider its value after the update:
E (si, ai)  (1 - ) E (si, ai) + EE (si+1, ai+1)
3

Under review as a conference paper at ICLR 2018

(a) (b)
Figure 1: (1a) Tree MDP, with k leaves. (1b) log1- E (s, start) as function of visit cycles, for different trees of k leaves (color coded). For each k, a cycle consists of visiting all leaves, hence k visits of the start action. log1- E behaves as a generalized counter, where each cycle contributes approximately one generalized visit.

Because E < 1 and E (si+1, ai+1)  E (si, ai), we get that after the update, the value of E (si, ai) decreased. For any non-maximal (sj, aj), its value after the update is a convex combination of its previous value and EE (sk, ak) which is not larger than its composing terms, which in turn are smaller than the maximal E-value.

3 APPLYING E-VALUES
The logarithm of E-values can be considered as a generalization of counters. As such, algorithms that utilize counters, can be generalized to incorporate E-values. Here we consider two such generalizations.

3.1 E-VALUES AS REWARD EXPLORATION BONUS

In model-based RL, counters have been used to create an augmented reward function. Motivated by this result, augmenting the reward with a counter-based exploration bonus has also been used in model-free RL (Storck et al., 1995; Bellemare et al., 2016). E-Values can naturally generalize this approach, by replacing the standard counter with its corresponding generalized counter (log1- E).

To show the advantage of using E-values over standard counters,

we tested an

-greedy

agent

with

an

exploration

bonus

of

1 log1- E

added to the observed reward on the bridge MDP (Figure 2). The

focus of this work is learning, hence we were interested a conver-

gence measure E (Q (s, a) - Q (s, a))2 , Where P (s|) is

the probability of arriving at s by following the optimal policy .

We varied the value of E from 0 ­ resulting effectively in standard counters ­ to E = 0.9. Our results (Figure 3) show that adding the exploration bonus to the reward leads to more efficient explo-

Figure 2: Bridge MDP

ration and hence faster learning. This effect further increases with

the increase of E, with generalized counters significantly outperforming standard counters.

3.2 E-VALUES AND ACTION-SELECTION RULES
Another way in which counters can be used to assist exploration is by adding them to the estimated Q-value. In this framework, action-selection is a function not only of the Q-values but also of the counters. Several such action-selection rules have been proposed (Thrun, 1992; Meuleau & Bourgine, 1999; Kolter & Ng, 2009). These usually take the form of a deterministic policy that maximizes some combination of the estimated value of rewards with a counter-based exploration bonus. It is easy to generalize such rules using E-values ­ simply replace the counters C by the generalized counters log1- (E). Next, we consider a special family of action-selection rules that are derived as deterministic equivalents of standard stochastic rules.
4

Under review as a conference paper at ICLR 2018

3.3 DETERMINIZATION OF STOCHASTIC DECISION RULES

Stochastic action-selection rules are commonly used in RL. In their simple form they include rules such as the -greedy or Softmax exploration described above. In this framework, exploratory behavior is usually achieved due to the stochasticity of action selection without direct dependence on past choices. At first glance, it might be unclear how E-values can contribute or improve such rules. We now turn to show that by using counters, for every stochastic rule there exist equivalent deterministic rules. Once turned to deterministic counter-based rules, it is again possible improve them using E-values.

The stochastic action-selection rules determine the frequency of choosing the different actions in the limit of a large number of repetitions, while abstracting away the specific order of choices. This fact is a key to understanding the relation between a deterministic rule and a stochastic one. An equivalence of two such rules can only be an in-the-limit equivalence, and can be seen as choosing a specific realization of sample from the distribution. Therefore, in order to derive a deterministic equivalent of a given stochastic rule, we only have to make sure that both rules agree on their stationary distribution ­ the probability, in the limit, by which each action is selected. As the probability for each action is likely to depend on the current Q-values, we have to consider fixed Q-values to define this equivalence.

We prove that given a stochastic action-selection rule f (a|s), every deterministic policy that does

not choose an action that was visited too many times until now (with respect to the expected number

according to the probability distribution) is a determinization of f . Formally, lets assume that given

a certain Q function and state s we wish a certain ratio between different choices of actions a  A to

hold. We denote the frequency of this ratio fQ (a|s). For brevity we assume s and Q are constants and denote fQ (a|s) = f (a). We also assume a counter C (s, a) is kept denoting the number of
choices of a in s. For brevity we denote C (s, a) = C (a) and a C (s, a) = C. When we look at the counters after T steps we use subscript CT (a). Following this notation, note that CT = T .

Theorem 3.1. For any sub-linear function b (t) and for any deterministic policy which chooses at

step

T

an

action

a

such

that

CT (a) T

-

f

(a)



b (t)

it

holds

that

a



A

lim CT (a) = f (a) T  T

Proof. For a full proof of the theorem see Appendix A in the supplementary materials

The result above is not a vacuous truth ­ we now provide two possible determinization rules that

achieves

it.

One

rule

is

straightforward

from

the

theorem,

using

b

=

0,

choosing

arg mina

C (a) C

-

f (a). Another rule follows the probability ratio between the stochastic policy and the empirical dis-

tribution:

arg

maxa

f (a) C (a)

.

We

denote

this

determinization

LLL,

because

when

generalized

counters

are used instead of counters it becomes arg maxa logf (s, a) - loglog1-E (s, a).

Now we can combine the two main ideas presented in this paper, and replace the visit counters C (s, a) with the generalized counters log1- (E (s, a)) to create Directed Outreaching Reinforcement Action-Selection ­ DORA the explorer. By this combination, we can transform any stochastic
or counter-based action-selection rule into a deterministic rule in which exploration propagates over
the states and the expected trajectories to follow.

Input: Stochastic action-selection rule f , learning rate , Exploration discount factor E initialize Q (s, a) = 0, E (s, a) = 1; foreach episode do
init s; while not terminated do
Choose a = arg maxx log fQ (x|s) - log log1- E (s, x); Observe transitions (s, a, r, s , a ); Q (s, a)  (1 - ) Q (s, a) +  (r +  maxx Q (s , x)); E (s, a)  (1 - ) E (s, a) + EE (s , a );
end
end
Algorithm 1: DORA algorithm using LLL determinization for stochastic policy f

5

Under review as a conference paper at ICLR 2018

3.4 RESULTS ­ FINITE MDPS

To test this algorithm, the first set of experiments were done on Bridge environments of various lengths k (Figure 2). We considered the following agents: -greedy, Softmax and their respective LLL determinizations (as described in 3.3) using both counters and E-values. In addition, we compared a more standard counter-based agent in the form of a UCB-like algorithm (Auer et al.,

2002) following an action-selection rule with exploration bonus of

log C

t

.

We

tested

two

variants

of this algorithm, using ordinary visit counters and E-values. Each agent's hyperparameters ( and

Temperature) were fitted separately to optimize learning. For stochastic agents, we averaged the

results over 50 trials for each execution. Unless stated otherwise, E = 0.9.

We also used a normalized version of the bridge environment, where all rewards are between 0 and 1, to compare DORA with the Delayed Q-Learning algorithm (Strehl et al., 2006).

Our results (Figure 4) demonstrate that E-value based agents outperform both their counter-based and their stochastic equivalents on the bridge problem. As shown in figure 4, Stochastic and counterbased -greedy agents, as well as the standard UCB fail to converge. E-value agents are the first to reach low error values, indicating that they learn faster and more effectively. Similar results were achieved on other gridworld environments, such as the Cliff problem (Sutton & Barto, 1998) (not shown). We also achieved competitive results with respect to Delayed Q Learning (see supplementary B and figure 7 there).

To test to what extent generalized counters are a measure of the amount of missing knowledge about

the values of state-action pairs in the world, we conducted another experiment. We studied the

behavior of an E-value LLL Softmax on the short bridge environment. We recorded C, log1- (E)

and

Q-Q Q

for each s, a at the end of each episode. Given state-action pair, the amount of missing

knowledge for it is measured through the distance between its estimated value (Q) and its value

according to the optimal policy (Q ). Generally, this is expected to be a monotonously-decreasing

function of the number of visits (C). This is indeed true, as depicted in figure 5 (left). However,

over all state-action pairs, visit counters do not capture well the amount of missing information, as

the convergence level depends not only on the counter but also on the identity of the state-action

it counts. By contrast, considering the convergence level as a function of the generalized counter

(figure 5, right) reveals a strikingly different pattern. Independently of the state-action identity, the

convergence level is a unique function of the generalized counter. These results demonstrate that

generalized counters are a useful measure of the amount of missing knowledge.

4 E -VALUES WITH FUNCTION APPROXIMATION
So far we discussed E-values in the tabular case, relying on finite (and small) state and action spaces. However, a main motivation for using model-free approach is that it can be successfully applied in large MDPs where tabular methods are intractable. In this case (in particular for continuous MDPs), achieving directed exploration is a non-trivial task. Because revisiting a state or a state-action pair is very unlikely, and because it is intractable to store individual values for all state-action pairs, counter-based methods cannot be directly applied. In fact, most implementations in these cases adopt simple exploration strategies such as -greedy or softmax (Bellemare et al., 2016).
There are standard model-free techniques to estimate value function in function-approximation scenario. Because learning E-values is simply learning another value-function, the same techniques can be applied for learning E-values in these scenarios. In this case, the concept of visit-count ­ or a generalized visit-count ­ will depend on the representation of states used by the approximating function. Importantly, E-values provide a measure for uncertainty that does not depend on any model estimation, such as transition models or density models(Bellemare et al., 2016; Ng et al., 1999).
4.1 RESULTS ­ FUNCTION APPROXIMATION
To test our approach in the function-approximation scenario, we used the MountainCar environment (Moore, 1990). We trained two neural networks in parallel, for predicting Q and E values, and compared the performance of two agents. The first consisted only of the Q-network, with a softmax action-selection rule, and the second its DORA variant consisted of Q and E networks following the

6

Under review as a conference paper at ICLR 2018

Figure 3: Convergence of -greedy on the bridge environment (k = 5) with and without exploration bonuses added to the reward. Note the logarithmic x axis.

Figure 4: Convergence measure of all agents, bridge environment (k = 15). E-values agents are the first to converge, suggesting their superior learning abilities

Figure 5: Convergence of Q to Q for individual state-action pairs (color coded), with respect to counters (left) and generalized counters (right). Results obtained from E-Value LLL Softmax on the bridge MDP (k = 5). Triangle markers indicate pairs with "east" actions, which constitute the optimal policy of crossing the bridge. Circle markers indicate state-action pairs that are not part of the optimal policy. Generalized counters are a useful measure of the amount of missing knowledge.

Figure 6: Probability of reaching goal on MountainCar (computed by averaging over 10 consecutive episodes), as a function of training episodes. While Softmax exploration fail to solve the problem within 3000 episodes, LLL E-values agents reach high success rates.

LLL determinization for softmax. In addition, we also tested our approach using a more traditional linear approximation with tile-coding features (Sutton & Barto, 1998), learning the weights vectors for Q and E in parallel.
The results depicted in figure 6 demonstrate that using E-values, learned by a standard DQN technique (Mnih et al., 2015), lead to better performance in the MountainCar problem. We achieved similar results using linear approximation (not shown), with E-values agents outperforming -greedy and softmax. We tested several Softmax agents with different Temperature values, but none was able to successfully solve the Mountain-Car problem within 3000 episodes (see red line in Figure 6). By contrast, the Softmax LLL E-values agents, in which E-values were learned by standard techniques (DQNs (Mnih et al., 2015)), achieved a substantially better performance in the same problem, reaching high success rates with various Temperatures. This is true both for a high value of E (green line) and for E = 0 (blue line), which can be thought of as an analogue of a counter-based agent, without propagating exploration.
The implementation of an E-value based agent require two networks, which might have a computational cost of up to double the cost of a naive Q-Learning implementation. A future direction for reducing this cost would be using one network with two separated streams in the final layers, predicting Q and E values based on mutual learned state representations in earlier layers.
7

Under review as a conference paper at ICLR 2018
5 RELATED WORK
The idea of using reinforcement-learning techniques to estimate exploration can be traced back to Storck et al. (1995) and Meuleau & Bourgine (1999) whom also analyzed propagation of uncertainties and exploration values. These works followed a model-based approach, and did not fully deal with the problem of non-Markovity arising from using exploration bonus as the immediate reward. A related approach was used by Little & Sommer (2014), where exploration was investigated by information-theoretic measures. Such interpretation of exploration can also be found in other works (Schmidhuber (1991); Sun et al. (2011); Houthooft et al. (2016)).
Efficient exploration in model-free RL was also analyzed in PAC-MDP framework, most notably the Delayed Q Learning algorithm by Strehl et al. (2006). For further discussion and comparison of our approach with Delayed Q Learning, see 1.1 and Appendix B.
In terms of generalizing Counter-based methods, we only know of one such attempt by Bellemare et al. (2016). This generalization provides a way to implement visit counters in large, continuous state and action spaces by using density models. Our generalization is different, as it aims first on generalizing the notion of visit counts themselves, from actual counters to "propagating counters". In addition, our approach does not depend on any estimated model ­ which might be an advantage in domains for which good density models are not available. Nevertheless, we believe that an interesting future work will be comparing between the approach suggested by Bellemare et al. (2016) and our approach, in particular for the case of E = 0.
REFERENCES
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235­256, 2002.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 1471­1479. Curran Associates, Inc., 2016.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications and Reviews, 38 (2), 2008, 2008.
Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using sample-based search. In Advances in Neural Information Processing Systems, pp. 1025­1033, 2012.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pp. 1109­1117, 2016.
Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD thesis, 2003.
J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 513­520. ACM, 2009.
Daniel Y Little and Friedrich T Sommer. Learning and exploration in action-perception loops. Closing the Loop Around Neural Systems, pp. 295, 2014.
Nicolas Meuleau and Paul Bourgine. Exploration of multi-state environments: Local measures and back-propagation of uncertainty. Machine Learning, 35(2):117­154, 1999.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Gianluigi Mongillo, Hanan Shteingart, and Yonatan Loewenstein. The misbehavior of reinforcement learning. Proceedings of the IEEE, 102(4):528­541, 2014.
8

Under review as a conference paper at ICLR 2018
Andrew William Moore. Efficient memory-based learning for robot control. 1990. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, volume 99, pp. 278­287, 1999. Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems. Uni-
versity of Cambridge, Department of Engineering, 1994. Ju¨rgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991. 1991
IEEE International Joint Conference on, pp. 1458­1463. IEEE, 1991. Jan Storck, Sepp Hochreiter, and Ju¨rgen Schmidhuber. Reinforcement driven information acquisi-
tion in non-deterministic environments. In Proceedings of the international conference on artificial neural networks, Paris, volume 2, pp. 159­164. Citeseer, 1995. Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309­1331, 2008. Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac modelfree reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pp. 881­888. ACM, 2006. Yi Sun, Faustino Gomez, and Ju¨rgen Schmidhuber. Planning to be surprised: Optimal bayesian exploration in dynamic environments. In International Conference on Artificial General Intelligence, pp. 41­51. Springer, 2011. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998. Sebastian B. Thrun. Efficient exploration in reinforcement learning, 1992. Michel Tokic and Gu¨nther Palm. Value-difference based exploration: adaptive control between epsilon-greedy and softmax. In KI 2011: Advances in Artificial Intelligence, pp. 335­346. Springer, 2011. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.
9

Under review as a conference paper at ICLR 2018

A PROOF OF THE DETERMINIZATION THEOREM

The proof for the determinization mentioned in the paper is achieved based on the following lemmata.

Lemma A.1. The absolute sum of positive and negative differences between the empiric distribution (deterministic frequency) and goal distribution (non-deterministic frequency) is equal.

f (a) - C (a) = -

f (a) - C (a)

CC

a:f

(a)

C(a) C

a:f

(a)<

C(a) C

Proof. Straightforward from the observation that

C (a)

f (a) =

=1

C

aa

Lemma A.2. For any t

max Ct (a) - f (a)  1 + b (t)

at

t

Proof. The proof of A.2 is done by induction. For t = 1

a  A : Ct (a) - f (a) = max Ct (a) - f (a) t at

Hence we look at a  A.
Ct (a) - f (a)  Ct (a) tt
 1 + b (1) 1
assume the claim is true for t = T then for t = T + 1 There exists a such that CT (a) /T - f (a)  b (t) which the algorithm chooses for this a. For it

CT +1 (a) - f (a) = CT (a) + 1 - f (a)

T +1

T +1

= CT (a) - f (a) + 1

T +1

T +1

= CT (a) - (T + 1) f (a) + 1

T +1

T +1

 1 + b (t) T +1

It also holds that a  A s.t. a = a

CT +1 (a) - f (a) = CT (a) - f (a)

T +1

T +1

= CT (a) - (T + 1) f (a) T +1

< CT (a) - T f (a) T +1

 1 + b (t) T +1

Proof of 3.1. It holds from A.2 together with A.1 that in the step t in the worst case all but one of

the

actions

have

Ct (a) t

- f (a)

=

1 t

and

the

last

action

has

f (a) -

Ct (a) t

=

-

|A|-1 t

.

So

by

the

bound

on sum of positives and negatives we get:

lim CT (a) = f (a) T  T

10

Under review as a conference paper at ICLR 2018
Figure 7: Convergence of E-value LLL and Delayed Q-Learning on, normalized bridge environment (k = 15). MSE was noramlized for each agent to enable comparison
B COMPARISON WITH DELAYED Q-LEARNING
Because Delayed Q learning initializes its values optimistically, which result in a high MSE, we normalized the MSE of the two agents (separately) to enable comparison. Notably, to achieve this performance by the Delayed Q Learning, we had to manually choose a low value for m (in Figure 7, m = 10), the hyperparameter regulating the number of visits required before any update. This is an order of magnitude smaller than the theoretical value required for even moderate PAC-requirements in the usual notion of , , such m also implies learning in orders of magnitudes slower. In fact, for this limit of m  1 the algorithm is effectively quite similar to a "Vanilla" Q-Learning with an optimistic initialization, which is possible due to the assumption made by the algorithm that all rewards are between 0 and 1. In contrast, because our approach separate reward values and exploratory values, we are able to use optimism for the later without assuming any prior knowledge about the first ­ while still achieving competitive results to an optimistic initialization based on prior knowledge.
C IMPLEMENTATION OF Q AND E NETWORKS
We used a fully-connected feed-forward Q network with two hidden layers, of 64 and 128 units, and a tanh activation function. The network has an input layer with two units where it receives a state ­ vector of position and velocity ­ and an output layer with 3 units (and no activation function) which corresponds to the Q values of the three possible actions. The E network is identical, except for the fact that weights of the last layer were initialized to 0 (instead of random initialization) and this layer has a sigmoid activation function. This initialization and activation function assures that E values of all state-action pairs start initially as a positive constant (0.5) and are always non-negative. Both networks were trained using experience-replay technique, with the weights of the target network being updated each 10000 and 1000 steps for the Q and E networks respectively. We used ADAM optimizer with learning rate 0.0001. For the DQN with experience-replay technique, we used replay-memory buffer of size 50000. The networks were updated every 4 timesteps, with batches of size 32. The target networks used to calculate TD-error were updated every 10000 and 1000 steps for Q and E network respectively. Discount factor (for Q) was always 0.99.
11

