Under review as a conference paper at ICLR 2018
POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we propose the generation of accented speech using generative adversarial networks. Through this work we make two main contributions a) The ability to condition latent representations while generating realistic speech samples b) The ability to efficiently generate long speech samples by using a novel latent variable transformation module that is trained using policy gradients. Previous methods are limited in being able to generate only relatively short samples or are not very efficient at generating long samples. The generated speech samples are validated through a number of various evaluation measures viz, a WGAN critic loss and through subjective scores on user evaluations against competitive speech synthesis baselines and detailed ablation analysis of the proposed model. The evaluations demonstrate that the model generates realistic long speech samples conditioned on accent efficiently.
1 INTRODUCTION
In this paper we solve the problem of synthesising speech data as a raw audio waveform using generative adversarial networks (GANs). While, generative adversarial frameworks have been used in a variety of settings mainly for images, generating speech using GANs has been more challenging. Speech requires generating a continuous sequential waveform requiring explicit control on content generated. Adversarial training of a recurrent sequence is more challenging especially in the continuous setting. One way to solve this is by generating an output sample of a predefined fixed size. However, generating longer samples would not be possible in this setting.
In this paper, we solve the problem by treating it as generation of a sequence of speech synthesis segments. Naive implementation of this would not allow for generation of a continuous speech output as they would be discontinuous segments. We address this problem by using recurrent units that constrain and condition the latent variables used for generation. These recurrent units transform the latent variables between the part generations. End-to-end gradient based training of these are not directly possible. In order to solve this, we make use of a policy gradient framework. This allows for generation of a long sequence of speech from a sequence of generators that have latent variables trained using policy gradients. A discriminator evaluates the full sequence to adversarially provide feedback regarding the realism of the generated audio samples.
An important aspect of speech synthesis is the underlying accent that is used to generate the speech. We condition the latent variables for the accent and content. We condition the accent using class based information using accent supervision available in standard accent datasets. This discrete latent variable differs from the continuous content based latent variables and are conditioned differently.
To summarize, through this paper we make the following contributions:
· We provide an efficient GAN based framework that generates speech using a segmented speech synthesis approach
· An efficient policy gradient based approach for training that allows for training rich set of latent variables
· Explicit conditioning on accent to enable generation of a variety of natural audio with the ability to control the accent.
1

Under review as a conference paper at ICLR 2018
· Generation of long speech signals using a sequence of generators that are jointly discriminated using a single discriminator
· Thorough evaluation of the framework using a Wasserstein Critic, accent discriminator, speech based content recognition and user evaluation.
2 RELATED WORK
Deep feed-forward neural networks (Zen et al., 2013) and recurrent neural networks (Fan et al., 2014) have both been used to build statistical parametric speech synthesis systems. More recent approaches have explored the use of neural networks for the entire speech synthesis pipeline (Oord et al., 2016; Mehri et al., 2017; Wang et al., 2017; Arik et al., 2017). Our work is most similar to two neural speech generation systems, Wavenet (Oord et al., 2016) and SampleRNN (Mehri et al., 2017), that both present powerful generative models of audio and operate directly on raw audio waveforms. Both Wavenet and SampleRNN, however, are slower than our approach on account of their autoregressive properties. Our speed up in generating samples comes from not having to reencode samples during generation and instead sampling from rich latent variables with the help of recurrent units trained using policy gradients.
Our proposed approach is an end-to-end paradigm that is based on GANs. GANs have been used for speech synthesis in combination with traditional statistical speech synthesis models (Kaneko et al., 2017; Yang et al., 2017) and for voice conversion where a WGAN objective was introduced to improve the quality of the generated samples (Hsu et al., 2017). SEGAN (Pascual et al., 2017) is the only prior work that uses GANs in an end-to-end manner for speech generation. They provide a speech enhancement system that takes raw audio waveforms as input and produce enhanced speech as output. This system is one of the baselines we compare against.
Prior work on accented speech synthesis has only made use of HMM-based synthesis models (Tomokiyo et al., 2005; Karhila & Wester, 2011; Garc´ia Lecumberri et al., 2014; Toman & Pucher, 2015). We provide the first end-to-end neural framework to generate accented speech.
Our work draws inspiration from the SeqGAN framework(Yu et al., 2017), which first introduced the idea of modeling the generator as an agent with a stochastic policy which is trained via policy gradients. In this work, discrete sequence generation is modeled in a GAN framework. The authors avoid obtaining generator gradients through a policy update step. As we obtain segments of speech synthesis, we also benefit by using a policy update. Further, work by Dai et al. (Dai et al., 2017) have applied policy update through an actor-critic framework where the policy update for in between steps are obtained by completing the action through a deterministic LSTM update. This model could be used with the updates being obtained through deterministic policy gradients Hunt et al. (2016). However, we observed in our setting that obtaining updates using the method followed in SeqGAN framework Yu et al. (2017) worked better.
3 ARCHITECTURE
Our proposed neural network does recurrent generation of speech samples. This generation is done by first encoding a speech sample to a rich latent variable. The latent variables are of two kinds. The first set of variables combine to form an embedding that captures the style and content of the speech sample, while the other set corresponds to discrete class variables that mark the accent of the speech. We condition the latent variables to come from rich distributions which therefore enable the model to capture diverse modes of the generated data. The model does not need original samples during inference, and is capable of generating arbitrarily long samples starting from a random value for any given class of speech (i.e. the accent).
Figure 1 illustrates the architecture of our proposed approach. It has an encoder layer which outputs latent variables z and a decoder which transforms z into output y. We would like y to be distributed like the input x and z to be distributed according to a rich distribution. The former is enforced by a discriminator network, Cµ and the latter by a critic network C. Given a partial input, x1:j, we shall compute z1:T where zj+1:T are sampled using a recurrent LSTM unit trained via policy gradients. Below, we describe each of these components in more detail.
2

Under review as a conference paper at ICLR 2018

3.1 ENCODER-DECODER

Our model auto-encodes the speech sample to a constrained latent variable representation. The encoder passes the speech sample through large kernel convolutions interspersed with Relu units acting as non-linear entities between the layers. The decoder is a deconvolution network with optional skip connections between the deconvolution layers and convolution layers of the encoder, that allows for transfer of information. We typically do not add skip connections in the auto-encoder setting, but add them while training the generation from noise as in LSGANs (Mao et al.) or WGANs (Arjovsky et al., 2017). These layers are also interspersed with LeakyRelu which help perform better since they allow gradients to backpropagate to the encoder layers more effectively.

The loss function for the encoder (parameterized by ) and the decoder (parameterized by ) is defined as:

J,(o, x; µ) := ||o - x||2 + (1 - Cµ(o))2

(1)

where Cµ is the discriminator network and  is a tunable hyperparameter.

3.2 DISCRIMINATOR

The discriminator network Cµ is a standard convolutional network, similar to the one used for encoding, but significantly deeper. The extra layers are added to make the discriminator more robust in
differentiating between real and fake speech samples from the generator. We adopt the least squares
loss function for the discriminator since it has a smoother curve and hence allows for more meaningful gradients. The loss function for the discriminator (parameterized by µ) when the input segment is x and the output from the decoder is y is given as Jdisc(Cµ(x), Cµ(y)) where Jdisc is defined as:

Jdisc(a, b) := ||1 - a||22 + ||b||22

(2)

3.3 LATENT VARIABLES FOR CONDITIONING
The latent variables used in our model are conditioned on relevant distributions. The class variables are passed through a softmax layer, in order to convert them to modes similar to class variables. The embedding variable is conditioned to be sampled from a Gaussian distribution. This conditioning is done by training a discriminant critic C that is responsible for differentiating between latent variables that are being sampled from the associated distribution and the ones which are being generated from the encoder. This enables the encoder to learn richer distributions, that further enables the decoder to learn more efficient layers since it is more confident about the distribution underlying the latent variable.

J J! Jµ

C Critic

Cµ Discriminator

Decoder
D
E
Encoder

LSTM!

D
·

LSTM!

D
·

Figure 1: Schematic diagram illustrating the proposed GAN-based framework.

3

Under review as a conference paper at ICLR 2018

3.4 REINFORCEMENT LEARNING FORMULATION

During recurrent generation of a long speech sequence, we need to ensure correlation between the
generated segments. Towards this, we incorporate a mechanism to force the sequence of latent variables z1, . . . , zT generated by encoding a speech sample x1, . . . , xT to be somewhat predictable from a prefix of the sequence. This mechanism involves a memory element (GRU or LSTM) that is trained to transform zj to zj+1.

We would like to train the memory elements to produce a full valid sequence z1, . . . , zT from any prefix. This motivates using a reinforcement learning approach with delayed rewards to train the
memory elements to produce zj that can be completed to form a valid sequence. Here, the state corresponds to the latent variables zj; actions correspond to the transformation from zj to zj+1 (alternately, the action can be viewed as producing the next segment of speech, as it is a deterministic
function of zj+1). The policy is parametrized by the parameters  of a memory element .

The gradients used to train the policy parameters  are derived in Sutton et al. (1999), as:

J () = E(s,a)[Q(s, a) log (a|s)]

(3)

where s is the state, a is the action, and Q(s, a) is the expected reward, and (a|s) is the probability assigned to action a by the policy, given state s. As mentioned above, in our case, the state s is
the latent variable zj produced by the encoder after seeing a prefix x1:j of a segment x1:T ; the action is the next state z^j(j+)1 predicted using the LSTM network (with some added noise). The expected reward Q(s, a) is replaced by the output of the discriminator Cµ(o(j)), where o(j) is a predicted output given z^j(j+)1: Note that the reward is higher if the discriminator is fooled more into considering o(j) to be from the real distribution. Here, instead of the expected reward, a single empirical sample of the prediction o(j) is used. The expectation over (s, a) is also approximated by empirically
summing over samples.1 To predict o(j) we predict z^k(j) for all k > j, using  , where  is the value of  at the beginning of processing a batch. (This corresponds to an off-policy update of the
parameter .) Thus, we instantiate (3) as

J(zj , z^j(j+)1, o(j); µ, ) =

Cµ(o(j)) log (z^j(j+)1|zj )

(4)

where the summation is over all examples in a batch and all j  {1, . . . , T - 1}, with the prediction

o(j) computed as in Line 12 of Algorithm 1. As mentioned above we define z^j(j+)1 = (zj) + , where is an error vector (we consider all the vectors to be row vectors below). More precisely, we

take



N (0, 2I).

Then, we have (z^j(j+)1|zj)



exp(

-1 22

||zj+1

- (zj)||22).

Then,

 log (z^j(j+)1|zj )

=



-||z^j(j+)1

- (zj )||22 22

=

z^j(j+)1

-  2

(zj

)





(zjT

).

Combined, our final expression for the gradient is

J(zj , z^j(j+)1, o(j); µ, ) =

Cµ(o(j))

(z^j(j+)1

- (zj)) 2

  (zjT

).

(5)

Our complete training algorithm is outlined in Algorithm 1.

4 EXPERIMENTAL SETUP
We evaluate the efficacy of our proposed approach with the help of two standard datasets 1) The CSLU Foreign Accented English dataset (Lander, 2007) that contains accented speech in English from native speakers of 22 different languages and, 2) The VCTK dataset (Veaux et al., 2010) that consists of speech from 109 speakers with varying accents. We used less than 5 hours of training data from both datasets. We trained our models using speech from 5-15 accents in the CSLU corpus and 20-30 speakers for the VCTK task.
1This could be viewed as a Monte Carlo approach of estimating Q(s, a). An alternate approach using an actor critic model based on deterministic policy gradients Hunt et al. (2016) turned out to not perform as well in our setting.

4

Under review as a conference paper at ICLR 2018

Algorithm 1 ACCENTGAN training pseudocode.

1: Inputs: data D, batch size m, segment length T , number of iterations Niter, weight  2: Initialize , µ, , , 

3: for i = 1 to Niter do 4:   

Make a working copy of the policy parameter

5: Sample a batch of m segments x1:T from data D 6: for j = 1 to T do

7: zj  E(xj ) 8: yj = D(zj ) 9: for k = j + 1 to T do

10: z^k(j) =  (zk-1) + where  N (0, 2I) 11: y^k(j) = D(zk) 12: o(j) = [y1:j |y^j(j+)1:T ] 13:   update(, J(zj , z^j(j+)1, o(j); µ, )) 14:   N (0, I)

Policy gradients in Eqn 5

15:   update(,  Jdisc(C (), C (zj))

Jdisc defined in Eqn 2

16: (, )  update((, ), J,(oT , x; µ)) 17: µ  update(µ, µJdisc(Cµ(x1:T ), Cµ(y1:T ))

J, defined in Eqn 1

4.1 TRAINING SETUP
Our model was implemented using Tensorflow (Abadi et al., 2016). The encoder is a 12-layer convolution layer network with a kernel size of 31, while the decoder is a mirror image of the encoder network with the addition of skip connections. We use Relu (Arora et al.) units between the layers of the encoder network, while we use LeakyRelu (Arora et al.) units between the layers of the decoder network. We are able to fit up to 12 recurrent layers of the model in a single GPU while training.
The discriminator network has 2 more layers than the generator network giving it more discriminative power. The layers in this case again have kernel width of 31 and downsample by 2 every layer. We clip the weights if we are training Wasserstein critics (described below), but typically we do not need to clip the weights. We generally also update the discriminator more times than we update the generator for both the speech discriminator and the latent variable discriminator. We also add batch normalization layers in order to ensure faster and more effective training (Ioffe & Szegedy).
A discriminator critic is used to optimize the latent variables and fit them to good distributions; we use an MLP with a large layer of 750 units, outputting a single logit, which is trained by the least squares GAN loss function. The MLP layers are interspersed with LeakyRelu units.
While doing inference and sampling from the trained model, we can either generate from noise for random content and a random accent. Or, we could give a single seed of about 1 second, using which we can upsample up to 12 seconds. We can also give smaller seeds for samples which are recorded at higher frequencies. This would allow us to do generation on higher frequencies using end to end deep learning models, since we are able to generate small fractions of a second and concatenate them together to generate longer speech samples.
4.2 EVALUATION SETUP
We evaluate our model using both objective and subjective evaluation measures.
4.2.1 OBJECTIVE EVALUATION MEASURES
Wasserstein distance Our first objective evaluation metric is the Wasserstein distance from an independent Wasserstein GAN (Arjovsky et al., 2017) critic which is trained on held out data and used to evaluate generated samples from our models. This measure has been adopted in prior work and is indicative of whether the generator is overfitting or whether the model is subject to mode collapse (Rosca et al., 2017; Danihelka et al., 2017). This WGAN critic is trained to maximize the
5

Under review as a conference paper at ICLR 2018

following loss function (in the stochastic setting, with batch size N ):

LCRITIC

=

1 N

NN

Cµ(xhieldout) -

Cµ(xgi )

i=1 i=1

where xheldout corresponds to a batch of samples from the heldout set and xg is a batch of generated samples. We train the model by clipping the weights but do not add the gradient penalty since large values of Wasserstein distances help us gain a better understanding of the model.
We have a single critic for the VCTK dataset trained on 50% of the held out data, while we have two critics in the CSLU dataset. The first critic is trained over 5 accents, since we train all the ablations and the baseline models for 5 accents, keeping in mind that a wide diversity often creates problems for simpler models. We train an alternate critic over 15 accents, which we use to evaluate the final model .

Accent recognition accuracy Our second objective metric is accent recognition accuracy (ARA) using an accent identification system. We train a classifier on speech samples generated for N different accents and predict the accents of unseen generated samples. This is compared with a classifier trained on roughly the same number of ground-truth speech samples from the same N accents. We expect the ARA rates to be comparable across both systems, suggesting that our model
generates accented speech samples that are very close in quality to the ground-truth samples.

4.2.2 SUBJECTIVE EVALUATION
The main objective of our model is to generate high-quality accented speech samples. In order to assess whether the speech accents are produced clearly in the generated samples and the speech is of high quality, we set up a human evaluation task to address both these questions. Six samples each from five different accents were generated using both our model (henceforth referred to as "AccentGAN") and the SEGAN model. We also used six ground-truth samples from each accent in the user study. Twelve pairs of samples, six samples each from two different accents, were presented to each user. In every pair, the constituent samples came from either AccentGAN or SEGAN or a ground-truth sample. No two utterances were identical in content across the twelve different pairs of samples. Users were asked first to listen to three reference samples in each accent to familiarize themselves with the underlying accent. Then, the users were asked to mark naturalness for both the samples in the pair on a 1-100 scale and they were also asked to mark on a numeric scale which of the two samples they thought was closer to the accent in the reference samples.

5 RESULTS AND ANALYSIS
5.1 WGAN CRITIC BASED BENCHMARKING OF METHOD
We compare AccentGAN with other models trained on the 5 most common accents in the CSLU dataset and across the VCTK dataset. We seek to show that our generative model mimics the distribution better and generates more real sounding speech samples. We also show that the model performs considerably better than the present state of the art models and that each component of the model is key to its performance. PolicyGAN and ConditionedGAN are two ablations of the model; the first one does not involve conditioning of the latent variables. ConditionedGAN only conditions the latent variable and did not use recurrent generation. Table 2 shows absolute values of the WGAN distance in all cases. (Figure 1 shows how the Wasserstein distance varies as we increase the number of training accents.)
5.2 HUMAN EVALUATION
A total of 24 users participated in our study. The users were all undergraduate and graduate students and all of them were fluent speakers of English. Each combination of samples (AccentGAN vs. SEGAN, AccentGAN vs. ground-truth, SEGAN vs. ground-truth) got 2-3 scores from different users. Figures 2 and 3 show histograms of naturalness scores computed for AccentGAN vs. groundtruth samples and AccentGAN vs. SEGAN samples, respectively. (A score of 1.0 suggests that

6

Under review as a conference paper at ICLR 2018

Number of accents
5 7 10 13 15

Wasserstein distance
708.45 ± 5.1 716.42 ± 4.3 704.41 ± 4.1 682.23 ± 3.4 672.43 ± 5.2

Table 1: Wasserstein distances using an alternate critic that is pre-trained on held-out data for 15 accents. This measures the average Wasserstein distance of samples generated by AccentGAN from ground truth samples, as we increase the number of accents we train on.

Model
SpeechGAN SEGAN
SampleRNN Conditioned GAN
Policy GAN AccentGAN

WGAN CRITIC
346.21 ± 3.21 312 ± 4.32
441.76 ± 6.05 291.16 ± 3.2 302.41 ± 3.4 201.98 ± 7.8

Model
SpeechGAN SEGAN
SampleRNN Conditioned GAN
Policy GAN AccentGAN

WGAN CRITIC
276.42 ± 15.02 266.88 ± 5.06 412.88 ± 13.45 305.88 ± 15.05 337.34 ± 20.04 164.55 ± 30.05

Table 2: Wasserstein distances on (a) VCTK dataset (b) CSLU dataset using critics trained on 20 people and 5 accents, respectively.

both systems were equally matched in naturalness scores, and scores greater and less than 1 suggest preferences for the respective constituent systems.) We observe that for most pairs of speech samples, AccentGAN and ground-truth were equally matched. There were roughly equal number of speech samples that were rated higher for AccentGAN compared to ground-truth and vice-versa. For AccentGAN vs. SEGAN samples, however, we see a clear preference for AccentGAN samples in Figure 3.
Figure 4(a) shows a box plot of weighted accent preferences. The users assigned a continuous value between -50 and 50 for accent preference, which was bucketed into 5 classes and associated with a weight from 2, 1, 0, 1, 2, thus giving more importance to samples which had a higher similarity score compared to samples with lower similarity scores. This re-weighting reduced all the values to the range -100 to 100. We see that preferences were fairly well-matched when AccentGAN samples were paired against ground-truth samples.
We note here that we did not use either WaveNet or SampleRNN in our user evaluation study since both systems did not generate high quality speech clips on account of the small amount of data that we used during training over the CSLU accented speech data, which is quite sparse.
5.3 ACCENT RECOGNITION ACCURACY BASED EVALUATION
To compute accent recognition accuracies, we extract i-vector (Dehak et al., 2011) features from the speech samples; i-vectors are a low-dimensional representation of the speech signal that capture all relevant information about a speaker. i-vectors have been successfully used for accent identification in prior work (Bahari et al., 2013). We extract 100-dimensional i-vectors using a background model that was trained on more than 100 hours of American accented speech (Godfrey & Edward, 1993) and we train a linear SVM classifier to predict the accent class of a test sample.
Figure 4(b) shows 3-fold accent recognition accuracies (and deviation statistics) on a test set of 130 samples (and training/validation sets of size 250 and 910, respectively). Each test sample was one of 15 accents. We observe very similar performance from using AccentGAN samples and ground-truth samples. We note here that the accent accuracies are low since we did not optimize the speech features and the classifier. Keeping these variables constant, our objective was to find out how AccentGAN samples fared against ground-truth samples.
7

Under review as a conference paper at ICLR 2018

Number of values Number of values

AccentGAN vs Ground Truth Naturalness Histogram
15
10 # values
5
0 Mean of Naturalness
Figure 2: AccentGAN vs. Ground truth

AccentGAN vs SEGAN Naturalness Histogram
10 8 # values 6 4 2 0 Mean of Naturalness
Figure 3: AccentGAN vs. SEGAN

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5
33222222222211111111110000000000................................77892806119029383700142535415664

Samples Accuracy Deviation

Ground Truth AccentGAN

9.2% 9.3%

0.14 0.08

Model
Different Accents AccentGAN
ConditionedGAN

L1 loss difference
0.14 0.023 0.036

Figure 4: (a) Accent preference values from user evaluation study (b) Accent recognition accuracies for ground truth vs. AccentGAN samples (c) Difference between L1 losses for ground truth sample and accent transfer sample
5.4 TRANSFER LEARNING
We experiment with transferring the content of speech across accents. This was done to ascertain whether the learned representations are (as claimed) disentangled in terms of accents and content. We also did this to generate samples with a specific accent and new content which appeared in other accents during training.
We pass a batch of samples into the auto-encoder and then change the accent being conditioned for. We computed the true L1 loss between the sample generated by accent transfer via AccentGAN and another ablation, shown in Figure 4(c). We show that our model performs better. As reference, we also show the true L1 loss between samples with same content and different accents. We performed this experiment on the VCTK dataset since it has parallel corpora which allows us to conduct this study.
5.5 COMPUTATIONAL EFFICIENCY
We compare our model against SampleRNN on three computational objectives, training time, data requirements and inference time. We took roughly 6 hours to train AccentGAN on the CSLU dataset while SampleRNN took about 72 hours of training time. During inference, we could sample up to 10 samples of length 12 seconds each in one second of computation time on Tesla P-100 GPUs, which is an order of magnitude faster than SampleRNN.
8

Under review as a conference paper at ICLR 2018
We also notice that models such as SEGAN need to add more computationally intensive convolutional layers to generate longer samples, while we can generate arbitrary long samples by recurrently applying the autoencoder networks, given a rich enough seed. We successfully generated a speech sample which was 16 seconds long from a second of real speech.
6 CONCLUSION
We present a GAN-based approach for speech generation that efficiently generates long speech samples via policy gradients. We enable the generation of accented speech by explicitly conditioning on a latent variable that uses accent supervision. Future work includes extending this framework and conditioning on text to build text-to-speech synthesis systems.
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech. arXiv preprint arXiv:1705.08947, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding Deep Neural Networks with Rectified Linear Units. pp. 1­21.
Mohamad Hasan Bahari, Rahim Saeidi, David Van Leeuwen, et al. Accent recognition using ivector, gaussian mean supervector and gaussian posterior probability supervector for spontaneous telephone speech. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 7344­7348, 2013.
Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. Towards diverse and natural image descriptions via a conditional gan. In Proceedings of ICCV, 2017.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Comparison of maximum likelihood and gan-based training of real nvps. arXiv preprint arXiv:1705.05263, 2017.
Najim Dehak, Patrick J Kenny, Re´da Dehak, Pierre Dumouchel, and Pierre Ouellet. Front-end factor analysis for speaker verification. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):788­798, 2011.
Yuchen Fan, Yao Qian, Feng-Long Xie, and Frank K Soong. TTS synthesis with bidirectional LSTM based recurrent neural networks. In Proceedings of Interspeech, 2014.
Mar´ia Luisa Garc´ia Lecumberri, Roberto Barra Chicote, Rube´n Pe´rez Ramo´n, Junichi Yamagishi, and Martin Cooke. Generating segmental foreign accent. 2014.
John Godfrey and Holliman Edward. Switchboard-1 release 2 ldc97s62. 1993.
Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang. Voice conversion from unaligned corpora using variational autoencoding Wasserstein generative adversarial networks. Proceedings of Interspeech, 2017.
Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous learning control with deep reinforcement. 2016.
Sergey Ioffe and Christian Szegedy. Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift.
9

Under review as a conference paper at ICLR 2018
Takuhiro Kaneko, Hirokazu Kameoka, Nobukatsu Hojo, Yusuke Ijima, Kaoru Hiramatsu, and Kunio Kashino. Generative adversarial network-based postfilter for statistical parametric speech synthesis. Proceedings of ICASSP, 2017.
Reima Karhila and Mirjam Wester. Rapid adaptation of foreign-accented hmm-based speech synthesis. In Proceedings of Interspeech, 2011.
T. Lander. Cslu: Foreign accented english release 1.2 ldc2007s08. Linguistic Data Consortium, 2007.
Xudong Mao, Qing Li, Haoran Xie, Raymond Y K Lau, and Zhen Wang. Least Squares Generative Adversarial Networks. pp. 1­17.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio generation model. Proceedings of ICLR, 2017.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.
Santiago Pascual, Antonio Bonafonte, and Joan Serra`. Segan: Speech enhancement generative adversarial network. Proceedings of Interspeech, 2017.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 1999.
Markus Toman and Michael Pucher. Evaluation of state mapping based foreign accent conversion. In Proceedings of Interspeech, 2015.
Laura Mayfield Tomokiyo, Alan W Black, and Kevin A Lenzo. Foreign accents in synthetic speech: development and evaluation. In Proceedings of Interspeech, 2005.
Christophe Veaux, Junichi Yamagishi, and Kirsten MacDonald. Cstr vctk corpus. http:// homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html, 2010.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: A fully end-to-end text-to-speech synthesis model. arXiv preprint arXiv:1703.10135, 2017.
Shan Yang, Lei Xie, Xiao Chen, Xiaoyan Lou, Xuan Zhu, Dongyan Huang, and Haizhou Li. Statistical parametric speech synthesis using generative adversarial networks under a multi-task learning framework. Proceedings of ASRU, 2017.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of AAAI, 2017.
Heiga Zen, Andrew Senior, and Mike Schuster. Statistical parametric speech synthesis using deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 7962­7966. IEEE, 2013.
10

