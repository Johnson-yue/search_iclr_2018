Under review as a conference paper at ICLR 2018

ON THE DISCRIMINATION-GENERALIZATION TRADEOFF IN GANS
Anonymous authors Paper under double-blind review

ABSTRACT
Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically neural networks. The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable). In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. This is a very mild condition satisfied even by neural networks with a single neuron. Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics. When evaluated with neural or Wasserstein distances, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training. Our analysis sheds lights on understanding the practical performance of GANs.

1 INTRODUCTION

Generative adversarial networks (GANs) (Goodfellow et al., 2014) and its variants can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions. Mathematically, GANs minimize the integral probability metric (IPM) (Mu¨ller, 1997), that is,

min
G

dF (µ^m, ) := sup
f F

Exµ^m [f (x)] - Ex [f (x)]

,

(1)

where µ^m is the empirical measure of the observed data, and F and G are the sets of discriminators and generators, respectively.

1. Wasserstain GAN (W-GAN) (Arjovsky et al., 2017). F = Lip1(X) := {f : ||f ||Lip  1}. Corresponding to the Wasserstain-1 distance.
2. MMD-GAN (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017a). F is taken as the unit ball in certain Reproducing Kernel Hilbert Space (RKHS), corresponding to the Maximum Mean Discrepency (MMD).
3. Energy-based GANs (Zhao et al., 2016). F is taken as the set of continuous functions bounded between 0 and m for some constant m > 0, corresponding to the total variation distance (Arjovsky et al., 2017).
4. f-GAN (Nowozin et al., 2016). Minimizing the moment matching loss defined over all possible functions plus a regularization term (Liu et al., 2017). See also Section 4.

Due to computational tractability, however, the practical GANs take F as a parametric function
class, typically, Fnn = {f(x) :   } where f(x) is a neural network indexed by parameter  that take values in   Rp. Consequently, the related dFnn (µ, ) is called neural network distance, or neural distance (Arora et al., 2017). Although dFnn (µ, ) is meant to be a surrogate, its properties can be fundamentally different or even irrelevant with the original objective functions. For example,
in W-GAN, because Fnn is a much smaller discriminator set than Lip1(X), it is unclear from the current GAN literature whether dFnn (µ, ) is a discriminative metric in that dFnn (µ, ) = 0 implies

1

Under review as a conference paper at ICLR 2018

µ = . This discrimination is critical to ensure the consistency of the learning result. This motivated us to study the properties of dFnn (µ, ) with parametric function sets Fnn, instead of the original Wasserstein distance or f -divergence.
A more broad question is in developing learning bounds and studying how it depends on the discriminator set F and the generator set G, under different evaluation metrics of interest. Specifically, assume m is an (approximate) solution of (1), we are interested in obtaining bounds between m and the underlying true distribution µ, under a given evaluation metric deval(µ, m). Existing analysis have been mostly focusing on the case when the evaluation metric coincides with the optimization metric, that is, deval(µ, ) = dF (µ, ), which, however, favors smaller discriminator sets that defines "easier" evaluation metrics. It is of interest to develop bounds for evaluation metrics independent of F, such as bounded Lipschitz distance that metrizes weak convergence, and KL divergence that connects to testing likelihood.

Contribution. We show that the role of discriminators F is best illustrated by the conditions under which dF (µ, ) metrizes weak convergence (or convergence in distribution), that is,

dF (µ, m)  0 if and only if m µ,

(2)

for any probability measures µ and m. The choice of F should strike a balance to achieve (2):

i) F should be large enough to make dF (µ, ) discrimiantive in that dF (µ, m)  0 can imply that m converges to µ. Further, with a given metric deval(µ, ), the discriminator set F should be large enough so that a small dF (µ, ) implies a small deval(µ, ) in certain sense. These are basic requirements in justifying dF (µ, ) as a valid learning objective function.
ii) F should also be relatively small so that m µ implies that dF (µ, m) approaches to zero. This is essential to guarantee that the training and testing loss are similar to each other and hence the algorithm is generalizable. Further, in order to obtain a low sample complexity, F should be sufficiently small so that dF (µ, m) decays with a sufficiently fast rate, preferably O(1/ m).

The theme of this work is to characterize the conditions under which i) and ii) hold and develop bounds of deval(µ, m) that characterize the role of discriminators F and generators G. Our contribution are summarized as follows.

1. We show that a discriminator set F is discriminative once the linear span of F is dense in the set of bounded continuous (or Lipschitz) functions. This is a surprisingly mild condition that can satisfied, for example, even for neural networks consists of a single neuron. See Section 2.
2. We develop techniques using neural distances dF (µ, ) to provide upper bounds of different evaluation metrics deval(µ, ) of interest, including bounded Lipschitz (BL) distance and KL divergence, which provides a key step for developing learning bounds of GANs under these metrics. See Section 2.1.
3. We characterize the generalizability of GANs using the Rademacher complexity of discriminator set F and put together bounds of between the true distributions µ and GAN estimators m under different evaluation metrics deval(µ, ) in Section 3. Under the neural and BL distances, our bounds (Corollary 3.2-3.3) show that a discriminator set with vanishing Rademacher complexity effectively acts as a type "universal" regularization term that helps prevent overfitting regardless of the size of the generator or hypothesis set G. This is in sharp contrast with the typical statistical learning framework in which large hypothesis sets necessarily increase the risk of overfitting.
4. On the other hand, when the KL divergence is used as the evaluation metric, our bound (Corollary 3.5) suggests that the generator and discriminator sets have to be compatible in that the log density ratios of the generators and the true distributions should exist and be included inside the linear span of the discriminator set. The strong condition that log-density ratio should exist partially explains the counter-intuitive behavior of testing likelihood in flow GANs (e.g., Danihelka et al., 2017; Grover et al., 2017).
5. We extend our analysis to study a class of neural f -divergence that is the learning objective of f -GAN. We establish the connection between neural f -divergence and neural distance, and establish similar results on the discrimination and generalization properties.

2

Under review as a conference paper at ICLR 2018

1.1 NOTATIONS

R RWe use X to denote a subset of d. For each function f : X  , we define the maximum norm
as f  = supxX |f (x)|, and the Lipschitz norm f Lip = sup{|f (x) - f (y)|/ x - y : x, y  X, x = y}, and the bounded Lipschitz (BL) norm f BL = max{ f Lip, f }. The Banach space of continuous functions on X is denoted by C(X), and the set of bounded continuous function is Cb(X) = {f  C(X) : f  < }.

The set of Borel probability measures on X is denoted by PB(X). In this paper, we assume that all

measures involved belongs to PB(X), which is sufficient in all practical applications. We denote by

Eµ[f ] the integral of f with respect to probability measure µ. The weak convergence, or convergence

in distribution is denoted by n . Given a base measures  (e.g., Lebegure measure), the density

of

µ



PB (X ),

if

it exists,

is

denoted

by µ

=

dµ d

.

We do

not

assume

density

exists

in

our

main

theoretical results.

2 DISCRIMINATIVE PROPERTIES OF NEURAL DISTANCES FOR GAN

As listed in the introduction, many variants of GAN can be viewed as minimizing the integral probability metric (1). Without loss of generality, we assume that the discriminator set F is even, i.e., f  F implies -f  F. Intuitively speaking, minimizing (1) towards zero corresponds to matching the moments Eµ[f ] = E[f ] for all discriminators f  F . In their original formulation, all those discriminator sets are non-parametric, infinite dimensional, and large enough to guarantee that
dF (µ, ) = 0 implies µ = .

In practice, however, the discriminator set is typically restricted to parametric function classes of form Fnn = {f :   }. When f is a neural network, we call dFnn (µ, ) a neural distance following Arora et al. (2017). Neural distances are the actual object function that W-GAN optimizes
in practice because they can be practically optimized and can leverage the representation power of
neural networks. Therefore, it is of great importance to directly study neural distances, instead of
Wasserstein metric, in order to understand practical performance of GANs.

Because the parameter function set Fnn is much smaller than the non-parametric sets like Lip1(X), a key question is whether Fnn is large enough so that moment matching on Fnn (i.e., dFnn (µ, ) = 0) implies µ = . It turns out the answer is affirmative once Fnn is large enough so that its linear span (instead of Fnn itself) forms an universal approximator. This is a rather weak condition, which is satisfied even by very small sets such as neural networks with a single neuron. We make this
concrete in the following.
Definition 2.1. Let (X, · ) be a metric space and F is a set of functions on X. We say that dF (µ, ) (and F ) is discriminative if

dF (µ, ) = 0  µ = 

for any two Borel probability measures µ,   PB(X). In other words, F is discriminative if the moment matching on F , that is, Eµ[f ] = E[f ] for any f  F implies µ = .

The key observation is that Eµ[f ] = E[f ] for any f  F implies the same holds true for all f in the linear span of F. Therefore, it is sufficient to require the linear span of F, instead of F itself, to be large enough to well approximate all the indicator test functions.
Theorem 2.2. For a given function set F  Cb(X), define

R Nn
spanF := {0 + ifi : i  , fi  F , n  }.
i=1

(3)

Then dF (µ, ) is discriminative if spanF is dense in the space of bounded continuous functions Cb(X) under the uniform norm || · ||, that is, for any f  Cb(X) and > 0, there exists an f  spanF such that f - f   . An equivalent way to put is that Cb(X) is included in the closure of spanF, that is,

cl(spanF )  Cb(X).

(4)

Further, (4) is a necessary condition for dF (µ, ) to be discriminative if X is a compact space.

3

Under review as a conference paper at ICLR 2018

Remark 2.1. The basic idea of characterizing probability measures using functions in Cb(X) is closely related to the concept of weak convergence. Recall that a sequence µn weakly converges µ, i.e., µn µ, iff Eµn [f ]  E[f ] for all f  Cb(X). Theorem 2.2 can be viewed as the result of the uniqueness of weakly convergence, that is, if µn µ and µn , then µ = . Remark 2.2. Because the set of bounded Lipschitz functions BL(X) = {f  Cb(X) : ||f ||Lip < } is dense in Cb(X), the condition in (4) can be replaced by a weaker condition
cl(spanF)  BL(X).
One can define a norm || · ||BL for functions in BL(X) by ||f ||BL = max{||f ||Lip, ||f ||}. This defines the bounded Lipschitz (BL) distance,
dBL(µ, ) = max {Eµ f - E f : ||f ||BL  1}.
f BL(X)
RThe BL distance is known to metrize weak convergence in sense that dBL(µ, n)  0 is equivalent
to n µ for all Borel probability measures on d; see section 8.3 in Bogachev (2007).

Neural Distances are Discriminative. The key message of Theorem 2.2 is that it is sufficient to
require cl(spanF)  Cb(X) (Condition (4)), which is a much weaker condition than the perhaps more straightforward condition cl(F)  Cb(X). In fact, (4) is met by function sets that are much
smaller than what we actually use in practice. For example, it is satisfied by the neural networks

with only a single neuron, i.e.,

R RFnn = {(wT x + b) : w  d, b  }.

(5)

This is because its span spanFnn includes neural networks with infinite numbers of neurons, which are well known to be universal approximators in Cb(X) according to classical theories (e.g., Cybenko, 1989; Hornik et al., 1989; Hornik, 1991; Leshno et al., 1993; Barron, 1993). We recall the following classical result.
R RTheorem 2.3 (Theorem 1 in Leshno et al. (1993)). Let  :  be a continuous activation Rfunction and X  d be any compact set. Let Fnn be the set of neural networks with a single
neuron as defined in (5), then spanFnn is dense in C(X) if and only if  is not a polynomial.
RThe above result that requires that the parameters [w, b] to take values in d+1. In practice, however,
we can only efficiently search in bounded parameter sets of [w, b] using local search methods like
Rgradient descent. We observe that it is sufficient to replace d+1 with a bounded parameter set  Nfor non-decreasing homogeneous activation function such as (u) = max{u, 0} with   ; note
that  = 1 is the widely used rectified linear unit (ReLU).
R NCorollary 2.4. Let X  d be any compact set, and (u) = max{u, 0} (  ), and Fnn =
{(w x + b) : [w, b]  }. Then spanFnn is dense in Cb(X) if
R{ :   0,   } = d+1.

RFor the case when  = {  d+1 :  2  1}, Bach (2017) not only proves that spanFnn is
dense in BL(X) (and thus dense in C(X)), but also gives the convergence rate.

RTherefore, for ReLU activation functions, Fnn with bounded parameter sets, like { :   1}
or { :  = 1} for any norm on d+1, is sufficient to discriminate any two Borel probability measures. Note that this is not true for some other activation functions such as tanh or sigmoid,
Rbecause there is an approximation gap between span{(w x + b) : [w, b]    d+1} and RCb(X) when   d+1 is bounded; see e.g., Barron (1993) (Theorem 3). From this perspective,
homogeneous activation functions such as ReLU are preferred for as discriminators.

One advantage of using bounded parameter sets  is that it makes Fnn have a bounded Lipschitz norm, that is, Fnn  LipK (X) for some K, and hence upper bounded by Wasserstein distance. In fact, W-GAN uses weight clipping to explicitly enforce boundness ||||  . But we should point out that the Lipschitz constraint does not help in making F discriminative since the constraint decreases, instead of enlarges, the function set F. Instead, the role of the Lipschitz constraint should
be mostly in stabilizing the training (Arjovsky et al., 2017) and assuring a generalization bound as
we discuss in Section 3. Another related way to justify the Lipschitz constraint is its relation to
metrizing weak convergence, as we discuss in the sequel.

4

Under review as a conference paper at ICLR 2018

Neural Distance and Weak Convergence. If F is discriminative, then dF (µ, ) = 0 implies µ = . In practice, however, we often can not achieve dF (µ, ) = 0 strictly. Instead, we often have dF (µ, n)  0 for a sequence of n and want to establish the weak convergence n µ.
Theorem 2.5. (X, dX ) be any metric space. If spanF is dense in Cb(X), we have limn dF (µ, n) = 0 implies n weakly converges to µ.
Additionally, if F is contained in a bounded Lipchitz function space, i.e., there exists 0 < C <  such that ||f ||BL  C for all f  F , then n weakly converges to µ implies limn dF (µ, n) = 0.
Theorem 10 of Liu et al. (2017) states a similar result for generic adversarial divergences, but does not obtain the specific weak convergence result for neural distances due to lacking of Theorem 2.2. Another difference is that Theorem 10 of Liu et al. (2017) heavily relies on the compactness assumption of X, while our result does not need this assumption. We provide the proof for Theorem 2.5 in Appendix D.
When X is compact, Wasserstein distance and the BL distance are equivalent and both metrize weak convergence. As we discussed earlier, the condition cl(spanF ) = Cb(X) and F  LipK(X) are satisfied by neural networks Fnn with a single ReLU activations and bounded parameter set . Therefore, the related neural distance dFnn is topologically equivalent to the Wasserstein and BL distance, because all of them metrize the weak convergence. This does not imply, however, that they are equivalent in the metric sense (or strongly equivalent) since the ratio dBL(µ, )/dFnn (µ, ) can be unbounded. In general, the neural distances are weaker than the BL distance because of smaller F. In Section 2.1 (and particularly Corollary 2.8), we draw more discussion on the bounds between BL distance and neural distances.

2.1 DISCRIMINATIVE POWER OF NEURAL DISTANCES

Theorem 2.2 characterizes the condition under which a neural distance is discriminative, and shows that even neural networks with a single neuron are sufficient to be discriminative. This does not explain, however, why it is beneficial to use larger and deeper networks as we do in practice. What is missing here is to frame and understand how discriminative or strong a neural distance is. This is because even if dF (µ, ) is discriminative,it can be relatively weak in that dF (µ, ) may be small when µ and  are very different under standard metrics (e.g., BL distance). Obviously, a larger F yields a stronger neural distance, that is, if F  F , then dF (µ, )  dF (µ, ). For example, because it is reasonable to assume that neural networks are bounded Lipschitz when X and  are bounded, we can control a neural distance with the BL distance:
dF (µ, )  CdBL(µ, ),
C := supfF {||f ||BL} < . A more difficult question is if we can establish inequalities from the other direction, that is, controlling dBL(µ, ), or in general a stronger dF (µ, ), with a weaker dF (µ, ) in some way. In this section, we characterizes conditions under which this is possible and develop bounds that allows us to use neural distances to control stronger distances such as BL distance, and even KL divergence. These bounds are used in Section 3 to translate generalization bounds in dF (µ, ) to that in BL distance and KL divergence.
The core of the discussion involves understanding how dF (µ, ) can be used to control the difference of the moment | Eµ g - E g| for g outside of F . We address this problem by two steps, first controlling functions in spanF, and then functions in cl(spanF) that is large enough to include Cb(X) for neural networks.

Controlling Functions in spanF . We start with understanding how dF (µ, ) can bound | Eµ g -

E g| for g  spanF. This can be characterized by introducing a notion of norm on spanF.

Proposition 2.6. define in (3), the

For each g  spanF that F -variation norm ||g||F,1

can be of g is

decomposed the infimum

into of

g=
n
i=1

n i=1

wi

fi

|wi| among

+ w0 as we all possible

decompositions of g, that is,

nn

||g||F,1 = inf

N R|wi| : g = wifi + w0, n  , w0, wi  , fi  F .

i=1 i=1

Then we have

| Eµ g - E g|  ||g||F,1 dF (µ, ), g  spanF .

5

Under review as a conference paper at ICLR 2018

Intuitively speaking, ||g||F,1 denotes the "minimum number" of functions in F needed in order to represent g. As F becomes larger, ||g||F,1 decreases and dF (µ, ) can better control | Eµ g - E g|. Precisely, if F  F then ||g||F ,1  ||g||F,1. Therefore, although adding more neurons in F may not necessarily increase spanF , it decreases ||g||F,1 and yields a stronger neural distance.

Controlling Functions in cl(spanF). A more critical question is how the neural distance dF (µ, ) can also control the discrepancy Eµ g - E g for functions outside of spanF but inside cl(spanF). The bound in this case is characterized by a notion of error decay function defined as
follows.

Proposition 2.7. Given a function g, we say that g is approximated by F with error decay function
(r) if for any r  0, there exists an fr  spanF with ||fr||F,1  r such that ||f - fr||  (r). Therefore, g  cl(spanF ) if and only if infr0 (r) = 0. We have

| Eµ g - E g|  inf {2 (r) + r dF (µ, )}.
r0

In particular, if

(r)

=

O(r-)

for

some



>

0,

then

|

Eµ

g

-

E

g|

=

O(dF

(µ,

)

 +1

).

It requires further efforts to derive the error decay function for specific F and g. In particular,

Proposition 6 of Bach (2017) allows us to derive the decay rate of approximating bounded Lipschitz

functions with rectified neurons, yielding a bound between BL distance and neural distance.

RCorollary 2.8. Let X be the unit ball of d under norm ||·||q for some q  [2, ), that is, X = {x 

Rd : ||x||q  1}. Consider F consisting of a single rectified neuron F = {max(v [x; 1], 0) : v 

R Nd+1, ||v||p = 1} where  

,

1 p

+

1 q

=

1.

Then

we

have

dBL(µ,

)

=

O~(dF

(µ,

)

1 +(d+1)/2

),

(6)

where O~ denotes the big-O notation ignoring the logarithm factor.

The result in (6) shows that dF (µ, ) gives a increasingly weaker bound when the dimension d increases. This is expected because we approximate an non-parametric set with a parametric one.

Likelihood and KL divergence. Maximum likelihood has been the predominant approach in statistical learning, and testing likelihood forms a standard criterion for testing unsupervised models. The recent advances in deep unsupervised learning, however, makes it questionable whether likelihood is the right objective for training and evaluation (e.g., Theis et al., 2015). For example, some recent empirical studies (e.g., Danihelka et al., 2017; Grover et al., 2017) showed a counter-intuitive phenomenon that both the testing and training likelihood (assuming generators with valid densities are used) tend to decrease, instead of increase, as the GAN loss is minimized. A hypothesis for explaining this is that the neural distances used in GANs are too weak to control the KL divergence properly. Therefore, from the theoretical perspective, it is desirable to understand under what conditions (even if it is a very strong one), the neural distance can be strong enough to control KL divergence. This can be done by the following simple result.

Proposition 2.9. Assume µ and  have positive density functions µ(x) and (x), respectively. Then
KL(µ||) + KL(||µ) = Eµ log(µ/ ) - E log(µ/ ).
If log(µ/)  spanF , then

KL(µ||) + KL(||µ)  || log(µ/ )||F,1 dF (µ, ).

(7)

If log(µ/)  cl(spanF ) with an error decay function (r) = O(r-), then

KL(µ||)

+

KL(||µ)

=

O(dF

(µ,

)

 +1

).

(8)

This result shows that we require that the density ratio log(µ/) exists and behaves nicely in spanF or cl(spanF) in order to bound KL divergence with dF (µ, ). If either µ or  is an empirical measure, the bound is vacuum since KL(µ, ) + KL(µ, ) equals infinite, while dF (µ, ) remains finite once F is bounded, i.e., ||f ||   <  for all f  F. Obviously, this is rather strong condition that is hard to satisfy in practice, because practical data distributions and genera-
tors in GANs often have no densities or at least highly peaky densities. We draw more discussion in
Corollary 3.5.

6

Under review as a conference paper at ICLR 2018

3 GENERALIZATION PROPERTY OF GANS

Section 2 suggests that it is better to use larger discriminator sets F in order to obtain stronger neural distances. However, why do regularization techniques, such as the weight clipping in WGAN, which effectively shrink the discriminator sets, help GAN training in practice?

The answer to this question has to do with the fact that we observe the true model µ only through an

i.i.d. sample of size m (whose empirical measure is denoted by µ^m), and hence can only optimize the empirical loss dF (µ^m, ), instead of the exact loss dF (µ, ). Therefore, generalization bounds

are required to control the exact loss dF (µ, ) when we can only minimize its empirical version dF (µ^m, ). Specifically, let G be a class of generators that may or may not include the unknown true distribution µ. Assume m minimizes the GAN loss dF (µ^m, ) up to an (  0) accuracy,

that is,

dF (µ^m, m)  inf dF (µ^m, ) + .
G

(9)

We are interested in bounding the difference between m and the unknown µ under certain evaluation

metric. Depending on what we care about, we may interested in the generalization error in terms of

the neural distance dF (µ, m) - infG dF (µ, ), or other standard quantities of interest such as BL

distance dBL(µ, ) and KL divergence KL(µ, ) or the testing likelihood.

In this section, we develop a framework for understanding the generalization proprieties of GANs. We show that the discriminator set F should be small enough to be generalizable, striking a tradeoff with the other requirement that it should be large enough to be discriminative. A surprising result is that the generalization error can be bounded purely by the Rademacher complexity of the discriminator set F and is independent of the generator set G. Therefore, G can be chosen to be as large as possible to reduce the model error. Using the bound in (2.1), we also discuss the conditions under which we can bound the negative testing likelihood for GANs.

3.1 GENERALIZATION IN NEURAL DISTANCE

Using the standard derivation and the optimality condition (9), we have (see Appendix E)

dF

(µ,

m)

-

inf
G

dF

(µ,

)



2

sup
f F

|Eµ[f

]

-

Eµ^m

[f

]|

+

= 2dF (µ, µ^m) + .

(10)

This reduces the problem to bounding the discrepancy dF (µ, µ^m) := supfF |Eµ[f ] - Eµ^m [f ]| between the true model µ and its empirical version µ^m. This can be achieved by the uniform
concentration bounds developed in statistical learning theory (e.g., Vapnik & Vapnik, 1998) and
empirical process (e.g., Van de Geer, 2000). In particular, the concentration property related to
supfF |Eµ[f ] - Eµ^m [f ]| can be characterized by the Rademacher complexity of F (w.r.t. measure µ), defined as

Rm(µ)(F ) := E

sup
f F

2 m

i

if (Xi)

,

(11)

where the expectation is taken w.r.t. Xi  µ, and Rademacher random variable i: prob(i =
1) = prob(i = -1) = 1/2. Intuitively, Rm(µ)(F ) characterizes the ability of overfitting with pure random labels using functions in F and hence relates to the generalization bounds. Standard results

in learning theory shows that

sup |Eµ[f ] - Eµ^m [f ]|  Rm(µ)(F ) + O(
f F

log(1/) ),
m

where  = supfF ||f ||. Combining this with (10), we obtain the following result.
Theorem 3.1. Assume that all discriminators are bounded by , i.e., f    for any f  F. Let µ^m be an empirical measure of an i.i.d. sample of size m drawn from µ. Assume m  G satisfies dF (µ^m, m)  infG dF (µ^m, ) + . Then with probability at least 1 - , we have

dF

(µ,

m)

-

inf
G

dF

(µ,

)



Rm(µ)(F

)

+

2

2 log(1/) +,
m

where Rm(µ)(F ) is the Rademacher complexity of F defined in (11).

(12)

7

Under review as a conference paper at ICLR 2018

Theorem 3.1 relates the generalization error of GANs to the Rademacher complexity of the discriminator set F. The smaller the discriminator set F is, the more generalizable the result is. Therefore, the choice of F should strike a subtle balance between the generalizability and the discriminative power: F should be large enough to make dF (µ, ) discriminative as we discuss in Section 2.1, and simultaneously should be small enough to have a small generalization error in (12). It turns
out parametric neural discriminators strike a good balance for this purpose, given that it is both
discriminative as we show in Section 2.1, and give small generalization bound as we show in the
following.
RCorollary 3.2. Let X be the unit ball of d under norm || · ||2, that is, X = {x  Rd : ||x||2  1}. Assume that F is neural networks with a single rectified linear unit (ReLU) RF = {max(v [x; 1], 0) : v  d+1, ||v||2 = 1}. Then with probability at least 1 - ,

dF (µ, m)  inf dF (µ, )
G

+

C m

+

(13)

and

dBL(µ, m) = O~

inf dF (µ, )
G

+

C m

+

1 (d+3)/2

,

(14)

 where C = 2 2 + 4

log(1/) and O~ denotes the big-O notation ignoring the logarithm factor.

Note that the three terms in Eqn. (13) takes into account the model error (infvG dF (µ, )), sample complexity and generalization error (C/ m), and optimization error ( ), respectively. Assume zero

model error and optimization error, then (1) dF (µ, m) = O(m-1/2), which achieves the typical

parametric

convergence

rate;

(2)

we

have

dBL(µ, m)

=

O~(m-

1 d+3

),

which

becomes

slower

as

the dimension d increases. This decrease is because of the non-parametric nature of BL distance,

instead of learning algorithm. As we show in Appendix B, we obtain a similar rate of dBL(µ, m) =

O(m-

1 d

),

even

if

we

directly

use

BL

distance

as

the

learning

objective.

Similar results can be obtained for general parametrized discriminator class as follows.

Corollary 3.3. Under the condition of Theorem 3.1, we further assume that (1) F = {f :     [-1, 1]p} is a parametric function class with p parameters in a bounded set  and that (2) every f is L-Lipschitz continuous with respect to the parameters . Then with probability at least 1 - , we

have

dF (µ, m)  inf dF (µ, )
G

+

C m

+

2

,

(15)

where C = 2( 2p log(Lp/ ) + 2 log(1/)).

This result can be easily applied to neural discriminators, since neural networks f(x) are generally Lipschitz w.r.t. the parameter , once the input domain X is bounded.

With the basic result in Theorem 3.1, we can also discuss the learning bounds of GANs with choices of non-parametric discriminators. Making use of Rademacher complexity of bounded sets in a RKHS (e.g., Lemma 22 in Bartlett & Mendelson (2003)), we give the learning bound of MMD-based GANs (Li et al., 2015; Dziugaite et al., 2015) as follows. We present the results for Wasserstain distance and total variance distance in Appendix B, and highlight the advantages of using parametric neural discriminators.
Corollary 3.4. Under the condition of Theorem 3.1, we further assume that F = {f  H : f H  1} where H is a RKHS whose positive definite kernel k(x, x ) satisfies k(x, x)  Ck < + for all x  X. Then with probability at least 1 - ,

where C = 2 1 +

dF (µ, m)  inf dF (µ, )
G

+

C m

+

 2 log(1/) Ck.

,

(16)

Remark 3.1 (Comparisons with results in Arora et al. (2017)). Arora et al. (2017) also discussed the generalization properites of GANs under a similar framework. In particular, they developed bounds of form |dF (µ, ) - dF (µ^m, ^m)| where µ^m and ^m are empirical versions of the target

8

Under review as a conference paper at ICLR 2018

distribution µ and  with sample size m. Our framework is similar, but considers bounding the quantity dF (µ, m) - infG dF (µ, ), which is of more direct interest. In fact, our Eqn. (10) shows that our generalization error can be bounded by the generalization error studied in Arora et al.
(2017), and thus our results are similar. Our framework allows us to understand the generalization
properties of GANs more clearly, and observes the surprising fact that the generalization of GANs is independent with the hypothesis set G.

The Role of the Generator Set G. Compared with the discriminator set F that involves a subtle trade-off, the role of the generator set, or the hypothesis set G only contributes to the model error infvG dF (µ, ), and does not influence the generalization error. This violates the conventional intuition in the traditional statistical learning frameworks, where larger hypothesis set always leads
to higher risk of overfitting. For example, the generalizability of the maximum likelihood estimator maxG Eµ^m [log (x)] is determined by the size of the hypothesis set G, forming the classical trade-off between the model error and overfitting risk. However, under the GAN framework, one
does not have to scarify generalizability for model error as long as the discriminator set is small
enough. This seems a critical difference, perhaps advantage of GANs over the traditional learning
frameworks.

Bounding the KL Divergence and Testing Likelihood. However, the above result depends on the evaluation metric we use, which are dF (µ, ) or dBL(µ, ). If we are interested evaluating the model using even stronger metrics, such as KL divergence or equivalently testing likelihood, then the generator set G enters the scene in a more subtle way, in that a larger generator set G should be companioned with a larger discriminator set F in order to provide meaningful bounds on KL divergence. This is illustrated in the following result obtained by combining Theorem 3.1 and Proposition 2.9.
Corollary 3.5. Assume both the true µ and all the generators   G have positive densities µ and , respectively. Assume F consists of bounded functions with  := supfF ||f || < .
Further, assume the discriminator set F is compatible with the generator set G in the sense that log(/µ)  spanF ,   G, with a compatible coefficient defined as

F,G := sup || log( /µ)||F,1 < .
G

Then

KL(µ, m)  F,G

2Rm(µ)(F ) + 2

2 log(1/)/m +  inf
G

KL(µ, ) +

.

(17)

Different from the earlier bounds, the bound in (17) depends on the compatibility coefficient F,G that casts a more interesting trade-off on the choice of the generator set G: the generator set G
should be small and have well behaved density functions to ensure a small F,G, while should be
large enough to have a small model error infG KL(µ, ). Related, the discriminator set should be large enough to include all density ratios log(µ/) in a ball of radius F,G of spanF , and should also be small to have a low Rademacher complexity Rm(µ)(F ). Obviously, one can also extend Corollary 3.5 using (8) in Proposition 2.7, to allow log(µ/)  cl(spanF ) in which case the compatibility of G and F should be mainly characterized by the error decay function (r). KL(µ, m) = Eµ[log pµ] - Eµ[log pm ] is the difference between the testing likelihood Eµ[log pm ] of estimated model m and the best possible testing likelihood Eµ[log pµ]. Therefore, Corollary 3.5 also provides a bound for testing likelihood. Unfortunately, the condition in Corollary 3.5 is rather
strong, in that it requires that both the true distribution µ and the generators  to have positive
densities and the log-density ratio log(µ/) is well-behaved. In practical applications of computer vision, however, both µ and  tend to concentrate on local regions or sub-manifolds of X, with
very peaky densities, or even no valid densities; this causes the compatibility coefficient F,G very large, or infinite, making the bound in (17) loose or vacuum. This provides a potential explain for
some of the recent empirical findings (e.g., Danihelka et al., 2017; Grover et al., 2017) that the
negative testing likelihood is uncorrelated with the GAN loss functions, or even increases during
the GAN training progress. The underlying reason here is that the neural distance is not strong
enough to provide meaningful bound for KL divergence. See Appendix G for an illustration using
toy examples.

9

Under review as a conference paper at ICLR 2018

4 NEURAL -DIVERGENCE

We have been focusing on GANs that are based on integral probability metrics (IPM), which cover W-GANs and MMD-GANs. f -GAN is another broad family of GANs that are based on minimizing f -divergence (also called -divergence) (Nowozin et al., 2016), which includes the original GAN by Goodfellow et al. (2014). 1 However, -divergence has substantially different properties from IPM
(see e.g., Sriperumbudur et al. (2009)), and is not defined as the intuitive moment matching form as IPM. In this section, we extend our analysis to -divergence by interpreting it as a form of penalized moment matching. Similar to the case of IPM, we analyze the neural -divergence that restricts the discriminators to parametric sets F for practical computability, and establish its discriminative and generalizable properties under mild conditions that practical f -GANs satisfy.

Assume that µ and  are two distributions on X. Given a convex, lower-semicontinuous univariate

function  that satisfies (1) = 0, the related -divergence is d(µ || ) = E



dµ d

. If  is

strictly convex, then a standard derivation based on Jensen's inequality shows that -divergence is

nonnegative and discriminative: d(µ || )  (1) = 0 and the equality holds iff µ = . Different choices of  recover popular divergences as special cases. For example, (t) = (t - 1)2 recovers

Pearson 2 divergence, and (t) = (u + 1) log((u + 1)/2) + u log u gives the Jensen-Shannon

divergence used in the vanilla GAN Goodfellow et al. (2014).

In this work, we find it helps to develop intuition by introducing another convex function (t) := (t + 1), defined by shifting the input variable of  by +1; the -divergence becomes

d(µ || ) = E



dµ - 1 d

=

 (x)
X

µ(x) - 1  (x)

 (dx),

(18)

where we should require that (0) = 0; in right hand side of (18), we assume µ and  are the density functions of µ and , respectively, under a base measure  . The key advantage of introducing
 is that it gives a suggestive variational representation that can be viewed as a regularized moment matching. Specially, assume  is the convex conjugate of , that is, (t) = supy{yt - (y)}. By standard derivation, we can show that

d(µ || )  sup (Eµ[f ] - E [f ] - , [f ]) , with , [f ] := Ex [(f (x))], (19)
f A

Rwhere A is the class of all functions f : X  dom() where dom() = {t : (t)  }, and

the

equality

holds

if



(

µ 

(x) (x)

- 1)



A

where



is

the

inverse

function

of



.

In

(19),

the

term

, [f ], as we show in Lemma 4.1 in sequel, can be viewed as a type of complexity penalty on f

that ensures the supreme is finite. This is in contrast with the IPM dF (µ, ) in which the complexity

constraint is directly imposed using the function class F, instead of a regularization term.

R RLemma 4.1. Assume  :   {} is a convex, lower-semicontinuous function with conjugate

 and (0) = 0. The penalty , [f ] in (19) has the following properties

i) , [f ] is a convex functional of f , and , [f ]  0 for any f .
Rii) There exists a constant b0   {} such that (b0) = 0. Further, if  is strictly convex, then
, [f ] = 0 implies f (x) = b0 all most surely under measure .

In practice, it is impossible to numerically optimize over the class of all functions in (19). Instead, practical f -GANs restrict the optimization to a parametric set F of neural networks, yielding the following neural -divergence:

d,F (µ || ) = sup (Eµ[f ] - E [f ] - , [f ]) .
f F

(20)

Note that this can be viewed as a generalization of the F-related IPM dF (µ, ) by considering  = 0. However, the properties of the neural -divergence can be significantly different from
that of dF (µ, ). For example, d,F (µ || ) is not even guaranteed to be non-negative for arbitrary discriminator sets F because of the negative regularization term. Fortunately, we can still establish
the non-negativity and discriminative property of d,F (µ || ) under certain weak conditions on F . Moreover, the property that dF (µ, ) = 0 implies moment matching on F, which is the key step to

1In this section, we call it -divergence because f has been used for discriminators.

10

Under review as a conference paper at ICLR 2018

establish the discriminative power, is not necessarily true for neural divergence. Fortunately, it turns
out that d,F (µ || ) = 0 implies moment matching on features defined by the last linear layer of
discriminators.
RTheorem 4.1. Assume F includes the constant function b0  , which satisfies (b0) = 0 as
defined in Lemma 4.1. We have

i) 0  d,F (µ || )  dF (µ, ) for µ, . As a result, dF (µ, ) = 0 implies d,F (µ || ) = 0.
In other words, moment matching on F is a sufficient condition of zero neural -divergence.

ii) Further, we assume F has the following form:

F  {(f0 + c0) : ||  f0 , and f0  F0},

(21)

R Rwhere F0 is any function set, and f0 > 0 is positive number associated with each f0  F0, and
c0 is a constant and  :  is any function that satisfies (c0) = b0 and  (c0) > 0. Here 
can be viewed as the activation of the last layer of a deep neural network whose earlier layers are specified by F0. Assume (y) is differentiable at y = b0. Then

d,F (µ || ) = 0 implies dF0 (µ , ) = 0.

In other words, moment matching on F0 is a necessary condition of zero neural -divergence.

iii) cl(spanF0)  Cb(X) is a sufficient condition for d,F to be discriminative, i.e., d,F (µ || ) = 0 implies µ = .

Condition (21) defines a commonly used structure of F that naturally satisfied by the f -GANs used in practice; in particular, the output activation function  plays the role of ensuring the output of F respects the input domain of the convex function . For example, the vanilla GAN has  = - log(1 - exp(t)) - t with an input domain of (-, 0), and activation function is taken to be (t) = - log(1 + exp(-t)). See Table 2 of Nowozin et al. (2016) for the list of output activation functions related to commonly used .
Similar to Theorem 2.5, under the conditions of Theorem 4.1, we have limn d,F (µ || n) = 0 implies n µ. See the exact statement in Theorem C.1 in Appendix C.
Remark 4.1. Our results on neural -divergence can in general extended to the more unified framework of Liu et al. (2017) in which divergences of form maxf E(x,y)µ [f (x, y)] are studied. We choose to focus on -divergence because of its practical importance. Our Theorem 4.1 i) can be viewed as a special case of Theorem 4 of Liu et al. (2017) and our Theorem 4.1 ii) is related to Theorem 5 of Liu et al. (2017). However, note that Theorem 5 of Liu et al. (2017) requires a rather counter-intuitive condition, while our condition in Theorem 4.1 ii) is clear and satisfied by all divergence listed in Nowozin et al. (2016).

Similar to the case of neural distance, we can establish generalization bounds for neural divergence.
Theorem 4.2. Assume that f   for any f  F. µ^m is an empirical distribution with m samples from µ, and m  G satisfies d,F (µ^m || m)  infG d,F (µ^m || ) + . Then with probability at least 1 - , we have

d,F

(µ

||

m)



inf
G

d,F

(µ

||

)

+

Rm(µ)(F

)

+

2

where Rm(µ)(F ) is the Rademacher complexity of F .

2 log(1/) +,
m

(22)

With Theorem 4.2, we obtain generalization bounds for difference choices of F, as we had in section 3. We list those results in Appendix C.

5 RELATED WORK
There is a surge of research interest in GANs; however, most of the work has been empirical in nature. There has been some theoretical literature on understanding GANs, including the discrimination and generalization properties of GANs.

11

Under review as a conference paper at ICLR 2018

The discriminative power of GANs is typically justified by assuming that the discriminator set F

has enough capacity. For example, Goodfellow et al. (2014) assumes that the optimal discriminator

pdata(x) pdata(x) +pg(x)



F.

Similar capacity assumptions have been made in nearly all other GANs to prove

its discriminative power; see, e.g., Zhao et al. (2016); Nowozin et al. (2016); Arora et al. (2017).

However, discriminators are in practice taken as certain parametric function class, like neural net-

works, which violates these capacity assumptions. The universal approximation property of neural

networks is used to justify the discriminative power empirically. In this work, we show that the

GAN loss is discriminative if spanF can approximate any continuous functions. This condition is

very weak and can be satisfied even when none of the discriminators is close to the optimal dis-

criminator. The MMD-based GANs (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017a) avoid

the parametrization of discriminators by taking advantage of the close-form solution of the optimal

discriminator in the non-parametric RKHS space. Therefore, the capacity assumption is satisfied in

MMD-based GANs, and their discriminative power is easily justified.

Liu et al. (2017) defines a notion of adversarial divergences that includes a number of GAN objective functions. They show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Our treatment of the neural divergence was directly inspired by them. We refer to Remark 4.1 for a detailed comparison. Liu et al. (2017) also show that for objective functions that are strict adversarial divergence, convergence in the objective function implies weak convergence. However, they do not provide a condition under which a adversarial divergence is strict. A major contribution of our work is to fill this gap, and to provide such a condition that is sufficient and necessary.

Dziugaite et al. (2015) studies generalization error, defined as dF (µ, m) - infG dF (µ, ) in our notation, for MMD-GAN in terms of fat-shattering dimension. Moreover, Dziugaite et al. (2015) obtains a generalization bound that incorporates the complexity of the hypothesis set G. Although their overall error bound is still O(m-1/2), their work shows the possibility to sharpen our Gindependent bound. Arora et al. (2017) studies the generalization properties of GANs through the quantity dF (µ, ) - dF (µ^m, ^m) (in our notations). The main difference between our work and Arora et al. (2017) is the definition of generalization error; see more discussions in Remark 3.1. Moreover, Arora et al. (2017) allows only polynomial number of samples from the generated distribution because the training algorithm should run in polynomial time. We do not consider this issue because in this work we only study the statistical properties of the objective functions and do not touch the optimization method. Finally, Arora et al. (2017) showed that the GAN loss can approach its optimal value even if the generated distribution has very low support, and (Arora & Zhang, 2017) showed empirical evidence for this problem. Our result is consistent with their results because our generalization error is measured by the neural distance/divergence.

Finally, there has been some other lines of research on understanding GANs. Li et al. (2017b) study the dynamics of GAN's training and find that: a GAN with an optimal discriminator provably converges, while a first order approximation of the discriminator leads to unstable dynamics and mode collapse. Lei et al. (2017) study WGAN and optimal transportation by convex geometry, and provide a close-form formula for the optimal transportation map. Hu et al. (2017) provide a new formulation of GANs and variational autoencoders (VAEs), and thus unify the most two popular methods to train deep generative models. We'd like to mention other recent interesting research on GANs, e.g., (Guo et al., 2017; Sinn & Rawat, 2017; Nock et al., 2017; Mescheder et al., 2017; Tolstikhin et al., 2017).

6 CONCLUSIONS
We studied the discrimination and generalization properties of GANs with parameterized discriminator class such as neural networks. A neural distance is guaranteed to be discriminative whenever the linear span of its discriminator set is dense in the set of bounded continuous functions. On the other hand, a neural divergence is discriminative whenever the linear span of features defined by the last linear layer of its discriminators is dense in the set of bounded continuous functions. We also provide a generalization bound for GANs in different evaluation metrics. In terms of neural or Wasserstein distances, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. This raises an interesting discrimination-generalization balance in GANs. Fortunately, several GAN methods in

12

Under review as a conference paper at ICLR 2018
practice already choose their discriminator set at the sweet point, where both the discrimination and generalization hold. Finally, our generalization bound in KL divergence provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.
There are several directions that we would like to explore in the future. First of all, in this paper, we do not talk about methods to compute the neural distance/divergence. This is typically a non-concave maximization problem and is extremely difficult to solve. Many methods have been proposed to solve this kind of minimax problems, but both stable training methods and theoretical analysis of these algorithms are still missing. Secondly, our generalization bound depends purely on the discriminator set. It's possible to obtain sharper bounds by incorporating structural information from the generator set. Finally, we would like to extend our analysis to conditional GANs (see, e.g., Mirza & Osindero (2014); Springenberg (2015); Chen et al. (2016); Odena et al. (2016)), which have demonstrated impressive performance (Reed et al., 2016a;b; Zhang et al., 2017).
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1­53, 2017.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463­482, March 2003. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=944919.944944.
Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Comparison of maximum likelihood and gan-based training of real nvps. arXiv preprint arXiv:1705.05263, 2017.
Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI'15, pp. 258­267, Arlington, Virginia, United States, 2015. AUAI Press. ISBN 978-0-9966431-0-8. URL http://dl.acm.org/ citation.cfm?id=3020847.3020875.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Bridging implicit and prescribed learning in generative models. arXiv preprint arXiv:1705.08868, 2017.
Jianbo Guo, Guangxiang Zhu, and Jian Li. Generative adversarial mapping networks. arXiv preprint arXiv:1709.09820, 2017.
13

Under review as a conference paper at ICLR 2018
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251­257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P Xing. On unifying deep generative models. arXiv preprint arXiv:1706.00550, 2017.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pp. 793­800, 2009.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.
Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, and David Xianfeng Gu. A geometric view of optimal transportation and generative model. arXiv preprint arXiv:1710.05488, 2017.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861­867, 1993.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnaba´s Po´czos. Mmd gan: Towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584, 2017a.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dynamics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017b.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718­1727, 2015.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. arXiv preprint arXiv:1705.10461, 2017.
Marvin L. Minsky and Seymour A. Papert. Perceptrons: Expanded Edition. MIT Press, Cambridge, MA, USA, 1988. ISBN 0-262-63111-3.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
Alfred Mu¨ller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429­443, 1997. ISSN 00018678. URL http://www.jstor. org/stable/1428011.
Richard Nock, Zac Cranko, Aditya Krishna Menon, Lizhen Qu, and Robert C Williamson. f-gans in an information geometric nutshell. arXiv preprint arXiv:1707.04385, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learning what and where to draw. In NIPS, 2016a.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text-to-image synthesis. In ICML, 2016b.
14

Under review as a conference paper at ICLR 2018
Mathieu Sinn and Ambrish Rawat. Towards consistency of adversarial training for generative models. arXiv preprint arXiv:1705.09199, 2017.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scho¨lkopf, and Gert RG Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.
Lucas Theis, Aa¨ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scho¨lkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
Sara A Van de Geer. Applications of empirical process theory, volume 91. Cambridge University Press Cambridge, 2000.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. ICCV, 2017.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.
15

Under review as a conference paper at ICLR 2018

A DISCRIMINATIVE IN P,p(X)

In Theorem 2.2, we show that the integral probability metric (IPM) dF is discriminative in the Borel measure set PB(X) if spanF is dense in the space of bounded continuous functions Cb(X). There is considerable interest in discontinuous function class approximation, such as neural net-
works with the threshold activation function (u) = 1u>0. Discontinuous functions are not used as often as continuous ones due to the lack of good training algorithms, but they are of theoretical
interest because of their close relationship to classical perceptrons Minsky & Papert (1988). In this
discontinuous scenario, we have a similar result as below.

Given a Borel probability measure  on X, we can also define the Banach space Lp (X) = {f :

E [|f |p]



},

the set of probability measures P,p(X)

=

{µ



PB (X )

:

dµ d



Lp (X)} and

the subspace of Borel measures M,p(X)

=

{µ



MB (X )

:

dµ d



Lp (X )}.

Here,

dµ d

is the

Radon-Nikodym derivative.

Let

p



(1, ]

and

q

=

p p-1

.

Similar

to

Definition

2.1,

we

define

that

F



Lq (X)

is

discriminative

in P,p(X) if for any two probability measures µ,   P,p(X),

Eµ[f ] = E [f ] f  F
implies µ = . Similar to Theorem 2.2, we have that F  Lq (X) is discriminative in P,p(X) if and only if span(F  {1}) is dense in Lq (X).
Different function classes (e.g., Cb(X) or Lq (X)) can discriminate different probability measures (e.g., PB(X) or P,p(X)). For GAN's applications where we want to generate high dimensional objects (e.g., images) from low dimensional hidden variables (e.g., a Gaussian random noise), the induced probability measures on the high dimensional space is not resolutely continuous with respect to its Lebesgue measure. Moreover, the empirical distribution of data points is not resolutely continuous with respect to its Lebesgue measure, either. Therefore, a discriminator function class whose span is only dense in Lq (X) is not enough. However, if the generator function is continuous, the induced probability measures are Borel. The empirical distribution of data points is a Borel measure, too. Therefore, a discriminator function class whose span is dense in Cb(X) is sufficient to discriminate the data distribution and the generated distributions.

B GENERALIZATION ERROR OF OTHER DISCRIMINATOR SETS F
With the basic result in Theorem 3.1, we discuss the learning bounds of GANs with other choices of non-parametric discriminator sets F. This allows us to highlight the advantages of using parametric neural discriminators. For simplicity, we assume zero model error and optimization so that the bound is solely based on the generalization error dF (µ, µ^m) between µ and its empirical version µ^m.
1. Bounded Lipschitz distance, F = {f  C(X) : ||f ||BL  1}, which is equivalent to
RWasserstein distance when X is compact. When X is a convex bounded set in d, we have
Rm(µ)(F )  m-1/d for d > 2 (see Corollary 12 in Sriperumbudur et al. (2009)), and hence dBL(µ, ) = O(m-1/d). This is comparable with Corollary 3.2. This bound is tight. Assume that µ is the uniform distribution on X. A simple derivation (similar to Lemma 1 in Arora et al. (2017)) shows that dF (µ, µ^m)  c(1-m exp(-(d))) for some constant only depending on X. Therefore, one must need at least m = exp((d)) samples to reduce dF (µ, µ^m), and hence the generalization bound, to O( ).
2. Total variation (TV) distance, F = {f  C(X) : f  min{1, }}. It is easy to verify that Rm(µ)(F ) = 2. Therefore, Eqn. (12) cannot guarantee generalization even when we have infinite number of samples, i.e., m  . The estimate given in Eqn. (12) is tight. Assume that µ is the uniform distribution on X. It is easy to see that dTV(µ, µ^m) = 2 almost surely. Therefore, m is close to µ^m implies that it is order 1 away from µ, which means that generalization does not hold in this case.
With the statement that training with the TV distance does not generalize, we mean that training with TV distance does not generalize in TV distance. More precisely, even if the training loss on

16

Under review as a conference paper at ICLR 2018

empirical samples is very small, i.e., TV(µ^m, m) = O( ), the TV distance to the unknown target distribution can be large, i.e., dT V (µ, m) = O(1). However, this does not imply that training with TV distance is useless, because it is possible that training with a stronger metric leads to asymptotic vanishing in a weaker metric. For example, dT V (µ^m, m) = O( ) implies dFnn (µ^m, m) = O( ), and thus a small dFnn (µ, m).
Take the Wasserstein metric as another example, even though we only establish dW (µ, m) = O(m-1/d) (assuming zero model error (µ  G) and optimization = 0), it does not eliminate the possibility that the weaker neural distance has a faster convergence rate dFnn (µ, m) = O(m-1/2). From the practical perspective, however, TV and Wasserstein distances are less clearly favorable than neural distance because the difficulty of calculating and optimizing them.

C MORE RESULTS ON NEURAL DIVERGENCE

Theorem C.1. Let (X, dX ) be a compact metric space and F0  C(X) satisfy cl(spanF0)  C(X). Further, we assume F has the following form:

F  {(f0 + c0) : ||  f0 , and f0  F0},

R Rwhere F0 is any function set, and f0 > 0 is positive number associated with each f0  F0, and
c0 is a constant and  :  is any function that satisfies (c0) = b0 and  (c0) > 0. Then if limn d,F (µ || n) = 0, n converges to µ in the distribution sense.

Further, if there exists C > 0 such that F  {f  C(X) : f Lip  C}, we have

lim
n

d,F

(µ

||

n)

=

0



n

µ.

Notice that we assume that (X, dX ) be a compact metric space here for simplicity. A non-compact result is available but its proof is messy and non-intuitive.

Proof. The first half is a direct application of Theorem 4.1 and Theorem 10 in Liu et al. (2017).
For the second half, we have
d,F (µ || n)  dF (µ, n)  CdW (µ, n),
where we use Theorem 4.1 i) in the first inequality and the Lipschitz condition of F in the second ineqaulity. Since dW metrizes the weak convergence for compact X, we obtain dW (µ, n)  0 and thus d,F (µ || n)  0.

For neural divergence with bounded parameter space and Lipschitz continuous discriminators, we have the following result.

Corollary C.2. Under the condition of Theorem 4.2, we further assume that (1) F = Fnn = {f :     [-1, 1]p} is a parametric function class with p parameters in a bounded set  and that (2)

every f is L-Lipschitz continuous with respect to the parameters . Then with probability at least

1 - , we have



d,Fnn (µ

||

m)



inf
G

d,Fnn (µ

||

)

+

Cm + 2 ,

(23)

where C = 2( 2p log(Lp/ ) + 2 log(1/)).

Proof. The proof is the same with that of Corollary 3.3.

D PROOF OF RESULTS IN SECTION 2
Proof of Theorem 2.2. In one direction, suppose that Eµ[f ] = E[f ] for all f  F (Eµ[1] = E[1] = 1 holds by definition) and that cl(span(F  {1})) = Cb(X). Since Eµ[f ] is continuous in Cb(X) for any probability measure µ, we have Eµ[f ] = E[f ] for all f  Cb(X). Then we have µ = . One has several methods to prove the last step. One is to take f (x) = eiwT x for any

17

Under review as a conference paper at ICLR 2018
Rw  d and to use the fact that the characteristic function uniquely determines a Borel probability
measure. Here, we provide a longer but more elementary proof.
NLet h(t) = max{1 - t, 0} for t  0 and F  X be a closed set. For any k  , define fk(x) =
h(kd(x, F )) and Fk = {x  X : d(x, F )  1/k}. It's easy to verify that (1) fk is continuous, (2) Fk is a closed set and that (3) 1F  fk  1Fk . Then we have
µ(F )  Eµ[fk] = E [fk]  (Fk),
Nand similarly, (F )  µ(Fk) for all k  . Since F1  · · ·  Fk  · · ·  F and k=1Fk =
F , we have limk µ(Fk) = µ(F ) and limk (Fk) = (F ). Therefore, we have µ(F ) = (F ). Remind that any Borel probability measure µ is regular, i.e., µ(A) = sup{µ(F ) : F  A is closed} = inf{µ(G) : G  A is open} for any Borel set A 2. Therefore, we have µ = .
In the other direction, suppose that F  Cb(X) is discriminative in PB(X). Assume that cl(span(F  {1})) is a strictly closed subspace of Cb(X). Take g  Cb(X)\cl(span(F )) and
Rg  = 1. By the Hahn-Banach theorem, there exists a bounded linear functional L : C(X) 
such that L(f ) = 0 for any f  cl(span(F  {1})) and L = 0. Thanks to the Riesz representation theorem for compact metric spaces, there exists a signed, regular Borel measure m  MB(X) such that
L(f ) = f f  Cb(X).
m
Suppose m = µ -  are the Hahn decomposition of m, where µ and  are two nonnegative Borel measures. Then we have L(f ) = µ f -  f for any f  Cb(X). Thanks to L(1) = 0, we have 0 < µ(X) = (X) < . We can assume that µ and  are Borel probability measures. (Otherwise, we can use the normalized nonzero linear functional L/µ(X) whose Hahn decomposition consists of two Borel probability measures.) Since L(f ) = 0 for any f  cl(span(F )), we have µ f =  f for any f  F. Since F  Cb(X) is discriminative, we have µ =  and thus L = 0, which leads to a contradiction.
R RProof of Corollary 2.4. Thanks to { :   0,   } = d+1, for any [w, b]  n+1, there
exists [w0, b0]   and  > 0 such that
(w x + b) = ((w0 x + b0)) = (w0 x + b0),
where we used (u) = max{u, 0} in the last step. Therefore, we have
RspanFnn  span{(w x + b) : [w, b]  d+1}.
Thanks to Theorem 2.3, we know that spanFnn is dense in Cb(X).
Proof of Theorem 2.5. Given a function g  Cb(X), we say that g is approximated by F with error decay function (r) if for any r  0, there exists fr  spanF with ||fr||F,1  r such that ||f - fr||  (r). Obviously, (r) is an non-increasing function w.r.t. r. Thanks to cl(spanF ) = Cb(X), we have limr (r) = 0. Now denote rn := dF (µ, n)-1/2 and correspondingly fn := frn . We have
| Eµ g - En g|  | Eµ g - Eµ fn| + | E g - E fn| + | Eµ fn - En fn|  2 (rn) + rn dF (µ, n). = 2 (rn) + 1/rn.
If limn dF (µ, n) = 0, we have limn rn = . Thanks to limr (r) = 0, we prove that limn | Eµ g - En g| = 0. Since this holds true for any g  Cb(X), we conclude that n weakly converges to µ.
If F  BLC (X) for some C > 0, we have dF (µ, )  CdBL(µ, ) for any µ, . Because the bounded Lipschitz distance (also called FortetMourier distance) metrizes the weak convergence, we obtain that n µ implies dBL(µ, n)  0, and thus dF (µ, n)  0.
2This can be proved by the fact that these sets form a -algebra.
18

Under review as a conference paper at ICLR 2018

Proof of Proposition 2.6. Let g =

n i=1

wifi

+

w0.

Then

we

have

nn

| Eµ g - E g| = |wi|| Eµ fi - E fi| 

|wi| dF (µ, ).

i=1 i=1

The result is obtain by taking infimum over all possible wi.

Proof of Proposition 2.7. For any r  0, we have | Eµ g - E g|  | Eµ g - Eµ fr| + | E g - E fr| + | Eµ fr - E fr|  2 (r) + r dF (µ, ).
Taking the infimum on r > 0 on the right side gives the result.

Proof of Corollary 2.8. Proposition 5 of Bach (2017) shows that for any bounded Lipschitz function g that satisfies ||g||BL : = max{||g||, ||g||Lip}  , we have (r) = O((r/)-1/(+(d-1)/2) log(r/)). Using Proposition 2.7, we get

|

Eµ

g

-

E

g|



O~(||g||BL

dF (µ,

1
 ) +(d+1)/2

),

The result follows BL(µ, ) = supg{| Eµ g - E g| : ||g||BL  1}.

E PROOF OF RESULTS IN SECTION 3

Proof of Equation 10 Using the standard derivation and the optimality condition (9), we have

dF

(µ,

m)

-

inf
G

dF

(µ,



)

= dF (µ, m) - dF (µ^m, m) + dF (µ^m, m) - inf dF (µ, )
G



dF

(µ,

m)

-

dF

(µ^m,

m)

+

inf
G

dF

(µ^m,

)

-

inf
G

dF

(µ,

)

+

.

Therefore, we obtain

dF (µ, m) - inf dF (µ, )  2 sup |dF (µ, ) - dF (µ^m, )| + .

G

G

Combining with the definition (1), we obtain

dF (µ,

m)

-

inf
G

dF

(µ,

)



2

sup
f F

|Eµ[f ]

-

Eµ^m [f ]|

+

.

Proof of Theorem 3.1. Consider the function h(X1, X2, . . . , Xm) = sup |Eµ[f ] - Eµ^m [f ]| .
f F
Since f takes values in [-, ], changing Xi to another independent copy Xi can change h by no more than 4/m. McDiarmid's inequality implies that with probability at least 1 - ,

sup |Eµ[f ] - Eµ^m [f ]|  E sup |Eµ[f ] - Eµ^m [f ]| + 2

f F

f F

2 log(1/) .
m

Standard argument on Rademacher complexity gives

1

E

sup |Eµ[f ] - Eµ^m [f ]|
f F

 2 E ,X

sup f F m

i

if (Xi)

:= Rm(F ).

Combining the two estimates above and Eqn. (10), we conclude the proof.

19

Under review as a conference paper at ICLR 2018

Proof of Corollary 3.2. Part of the proof is from Proposition 7 in Bach (2017). More accurately, the discriminator set we use here is
nn
F = wi max(vi [x; 1], 0) : |wi|  1, vi 2 = 11  i  n
i=1 i=1
N
for a fix n  . Since x 2  1 and v 2  1, it is easy to see that f   2 for all f  F .

We want to estimate Rm(µ)(F ) and then use Theorem 3.1 to prove the result. First, it's easy to verify

that

22

sup
f F

m

i

if (Xi) =

sup v 2=1 m

i

i max(v

[Xi; 1], 0) .

Then we have

Rm(µ)(F ) = E

2

sup v 2=1 m

i

i max(v

[Xi; 1], 0)

E

2

sup v 2=1 m

i

iv [Xi; 1]

2 = mE

i[Xi; 1] ,
i2

where we use the 1-Lipschitz property of max(x, 0) and Talagrand's contraction lemma (Ledoux

& Talagrand, 2013) in the inequality step. From Kakade et al. (2009), we get the Rademacher

complexity of linear functions

 E i[Xi; 1]  2m.

i2

Therefore, we obtain





Rm(µ)(F )



2

2 .

m

Combined with f   2 and Theorem 3.1, we finish the proof.

Proof of Corollary 3.3. Given a sample (X1, . . . , Xm), we first examine the sample Rademacher

complexity R^m(F ) := E

supf F

2 m

i if (Xi) . Since F is even (i.e., f  F implies -f 

F ), we have R^m(F ) = E

supf F

2 m

i if (Xi) .

Let ^ be a finite set such that every point in  is within distance /2L of a point in ^ . Since   [-1, 1]p, standard constructions give an ^ satisfying log |^ |  p log(Lp/ ). Now for the finite set {(f(X1), . . . , f(Xm)) :   ^ }, we can bound its Rademacher complexity by the Massart Lemma:

2

2 log |^ |

2p log(Lp/ )

E

sup f:^ m

i f (Xi )
i

 2

 2 m

. m

(24)

Now for any   , we can find ^  ^ such that  - ^  /2L. Therefore,

2 m

i f (Xi )



2 m

2 if^(Xi) + m

i f (Xi )

-

2 m

if^(Xi)

ii

ii

22

 m

if^(Xi) + m

L 2L

ii

2

= m

if^(Xi) + .

i

Therefore, we have

22

E

sup f: m

i

i f (Xi )

 E

sup f:^ m

i

i f (Xi )

+.

(25)

20

Under review as a conference paper at ICLR 2018

Combining Eqn. (24) and (25), we obtain

R^m(F )  E

2 sup f: m

i

i f (Xi )

 2

2p log(Lp/ ) +.
m

Since the estimate above holds for an arbitrary sample, we conclude that Rm(F)

2

2p log(Lp/ m

)

+

. Combining this result with Theorem 3.1, we finish the proof.



Proof of Corollary 3.4. Lemma 22 in Bartlett & Mendelson (2003) shows that if supxX k(x, x)  Ck  +, we have Rm(µ)(F )  2 Ck/m for any µ  PB(X). Also, note that f (x)  ||f ||H k(x, x)  ||f ||H Ck. Combined with Theorem 3.1, we conclude the proof.

Proof of Corollary 3.5. Use Proposition 3.1 and note that KL(µ, m)  F,G dF (µ, ) and dF (µ, )  TV(µ, )   2KL(µ, ) by Pinsker's inequality.

F PROOF OF RESULTS IN SECTION 4

Proof of Lemma 4.1. i) It is obvious that , [f ] is convex given that f  is convex. By the convex conjugate, we have (t) = supy ty - (y) . Take t = 0 and note that (0) = 0, then we have (y)  0, y. This proves , [f ]  0.
ii) If  is strictly convex, then  is also strictly convex. This implies there exists at most a single value b0 such that (c) = 0. Given that (y)  0 for y, we arrive that Ex[(f (x))] = 0 implies (f (x)) = 0 almost surely under x  , which then implies f (x) = b0 almost surely.

Proof of Theorem 4.1. i) because b0  F and (b0) = 0, we have d,F (µ || )  Eµ[b0]-E [b0]- , [b0] = 0. Because , [f ]  0, we obtain d,F (µ || )  dF (µ, ) by comparing (20) with dF (µ, ) = supfF {Eµ f - E f }.
ii), note that d,F (µ || ) = 0 implies Eµ[f ] - E [f ]  , [f ], f  F . Therefore,

Eµ[(f0 + c0)] - E [(f0 + c0)]  , [(f0 + c0)], f0  F0, ||  f0 . This implies that

1  (Exµ[(f0(x) +

c0)] -

Ex [(f0(x) +

c0)])



1 

Ex [((f0(x)

+ c0))].

By the differentiability assumptions,

(26)

lim
0

(f0(x)

+ c0) 

-

(c0)

=



(c0)f (x),

lim
0

((f0(x)

+ c0)) 

-

((c0))

=



(b0)

(c0)f0(x)

=

0,

where we used the fact that ((c0)) = (b0) = 0 and  (b0) = 0 because b0 is a differentiable minimum point of . Taking the limit of   0 on both sides of (26), we get

 (c0)[Exµ[f0(x)] - Ex [f0(x)]]  0, f0  F0.

Because  (c0) > 0 by assumption, this implies Exµ[f0(x)] - Ex [f0(x)]. The same argument applies to -f0, and we thus we finally obtain Exµ[f0(x)] = Ex [f0(x)].
iii) Combining Theorem 2.2 and the last point, we directly get the result.

G INCONSISTENCY BETWEEN GAN'S LOSS AND TESTING LIKELIHOOD
In this section, we will test our analysis of the consistency of GAN objective and likelihood objective on two toy datasets, e.g., a 2D Gaussian dataset and a 2D 8-Gaussian mixture dataset.

21

Under review as a conference paper at ICLR 2018

G.1 A 2D GAUSSIAN EXAMPLE

The underlying ground-truth distribution is a 2D Gaussian with mean (0.5, -0.5) and covariance

matrix

1 128

17 15

15 17

. We take 105 samples as training dataset, and 1000 samples as testing dataset.

For a 2D Gaussian distribution, we use the following generator

x1 x2

=

1 l

1

es1 es2

z1 z2

+

b1 b2

,

(27)

R R Rwhere z =

z1 z2

is a standard 2D normal random vector, l 

,s =

s1 s2



2 and b =

b1 b2



2

are trainable parameters.

We train the generative model by WGAN with weight clipping. In the first experiment, the discriminator set is neural network with one hidden layer and 500 hidden neurons, i.e.,

500
Fnn = { i max(wi [x; 1], 0) : -0.05    0.05, -0.05  wi  0.05 i}.
i=1
Motivated by Corollary 3.5, in the second experiment, we take the quadratic polynomials as discriminator set, i.e.,
Fquad = {x Ax + b x : -0.05  A  0.05, -0.05  b  0.05}.

We plot their results in Figure 1. We can see that both training loss (neural distance) converge to
0 and that the testing log likelihood is increasing during the training, which is consistent with the
increase of the negative training loss. However, we can see that the quadratic polynomial discriminators Fquad leads to higher log likelihood and better generative model after finishing the training. This is expected because Corollary 3.5 guarantees that the log likelihood is bounded by the GAN loss (up to a constant), while it is not true for Fnn.

Figure 1: Negative GAN losses and testing likelihood. qgan: WGAN with quadratic polynomials as discriminator. wgan: WGAN with neural discriminator.
We can also maximize the likelihood on the training dataset to train the model, and we show its result in Figure 3. We can see that WGAN with quadratic polynomials as discriminators and maximal likelihood training leads to similar results. However, directly maximizing the likelihood converges much faster than the WGAN in this example.
In this simple Gaussian examples, the WGAN loss and the testing log likelihood is consistent. We indeed observe that by carefully choosing the discriminator set (as suggested in Corollary 3.5), the testing likelihood can be simultaneously optimized as we optimize the GAN objective.
22

Under review as a conference paper at ICLR 2018
Figure 2: Samples from trained generators. Left: WGAN with quadratic polynomials as discriminator. Right: WGAN with neural discriminator.
Figure 3: Upper: maximal likelihood estimates, trained with SGD. Middle: negative training loss. Right: log likelihood on testing dataset. G.2 AN EXAMPLE OF 2D 8-GAUSSIAN MIXTURE The underlying ground truth distribution is a 2D Gaussian mixture with 8 Gaussians and with equal weights. Their centers distributed equally on the circle centered at the origin and with radius 2, and their standard deviations are all 0.01414. We take 105 samples as training dataset, and 1000 samples as testing dataset. We show one batch (256) of training dataset and the testing dataset in Figure 4. We remind that the density of the ground-truth distribution is very singular.
Figure 4: Samples from training and testing datasets. We still use Eqn. (27) as the generator for a single Gaussian component. Our generator assume that there are 8 Gaussian components and they have equal weights, and thus our generator does not have any modeling error. The training parameters are eight sets of scaling and biasing parameters in Eqn. (27), each for one Gaussian component.
23

Under review as a conference paper at ICLR 2018
We first training the model by WGAN with clipping. We use an MLP with 4 hidden layers and relu activations as the discriminator set. We show the result in Figure 5. We can see that the generator's samples are nearly indistinguishable from the real samples. However, the GAN loss and the log likelihood are not consistent. In the initial stage of training, both the negative GAN loss and likelihood are increasing. As the training goes on, the generator's density gets more and more singular, the likelihood behaves erratically in the latter stage of training. Although the negative GAN loss is still increasing, the log likelihood oscillates a lot, and in fact over half of time the log likelihood is -. We show the generated samples at intermediate steps in Figure 6, and we indeed see that the likelihood start to oscillate inviolately when the generator's distribution gets singular. This inconsistency between GAN loss and likelihood is observed by other works. The reason for this consistency is that the neural discriminators are not a good approximation of the singular density ratios.
Figure 5: Left: samples from training dataset and samples from generator. Right: negative training loss and log likelihood (on testing dataset).
Figure 6: Left to right: generated samples at step 100, 200 and 300, respectively. We also trained the model by maximizing likelihood on the training dataset. We show the result in Figure 7. We can see that the maximal likelihood training got stuck in a local minimal, and fail to exactly recover all 8 components. The log likelihood on training and testing dataset are consistent as expected. Although the log likelihood ( 2.7) obtained by maximizing likelihood is higher than that ( 2.0) obtained by WGAN training, its generator is obviously worse than what we obtained in WGAN training. The reason for this is that the negative log-likelihood loss has many local minima and each of them are deeper than those in WGAN. Maximizing likelihood is much easier to get trapper in a local minimum. The FlowGAN proposed to combine the WGAN loss and the log likelihood to solve the inconsistency problem. We showed the FlowGAN result on this dataset in Figure 8. We can see that training by FlowGAN indeed makes the training loss and likelihood consistent. However, FlowGAN got stuck in a local minimum as maximizing likelihood did, which is not desirable.
24

Under review as a conference paper at ICLR 2018
Figure 7: Left: samples from training dataset and samples from generator. Right: log likelihood on training and testing dataset.
Figure 8: Left: samples from training dataset and samples from generator. Right: negative FlowGAN loss and log likelihood on testing dataset.
25

