Under review as a conference paper at ICLR 2018
EXTENDING THE FRAMEWORK OF EQUILIBRIUM PROPAGATION TO GENERAL DYNAMICS
Anonymous authors Paper under double-blind review
ABSTRACT
The biological plausibility of the backpropagation algorithm has long been doubted by neuroscientists. Two major reasons are that neurons would need to send two different types of signal in the forward and backward phases, and that pairs of neurons would need to communicate through symmetric bidirectional connections. We present a simple two-phase learning procedure for fixed point recurrent networks that addresses both these issues. In our model, neurons perform leaky integration and synaptic weights are updated through a local mechanism. Our learning method extends the framework of Equilibrium Propagation to general dynamics, relaxing the requirement of an energy function. As a consequence of this generalization, the algorithm does not compute the true gradient of the objective function, but rather approximates it at a precision which is proven to be directly related to the degree of symmetry of the feedforward and feedback weights. We show experimentally that the intrinsic properties of the system lead to alignment of the feedforward and feedback weights, and that our algorithm optimizes the objective function.
1 INTRODUCTION
Deep learning (LeCun et al., 2015) is the de-facto standard in areas such as computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012) and machine translation (Bahdanau et al., 2015). These applications deal with different types of data and share little in common at first glance. Remarkably, all these models typically rely on the same basic principle: optimization of objective functions using the backpropagation algorithm. Hence the question: does the cortex in the brain implement a mechanism similar to backpropagation, which optimizes objective functions?
The backpropagation algorithm used to train neural networks requires a side network for the propagation of error derivatives, which is vastly seen as biologically implausible (Crick, 1989). One hypothesis, first formulated by Hinton & McClelland (1988), is that error signals in biological networks could be encoded in the temporal derivatives of the neural activity and propagated through the network via the neuronal dynamics itself, without the need for a side network. Neural computation would correspond to both inference and error back-propagation. This work also explores this idea.
The framework of Equilibrium Propagation (Scellier & Bengio, 2017) requires the network dynamics to be derived from an energy function, enabling computation of an exact gradient of an objective function. However, in terms of biological realism, the requirement of symmetric weights between neurons arising from the energy function is not desirable. The work presented here extends this framework to general dynamics, without the need for energy functions, gradient dynamics, or symmetric connections.
Our approach is the following. We start from classical models in neuroscience for the dynamics of the neuron's membrane voltage and for the synaptic plasticity (section 3). Unlike in the Hopfield model (Hopfield, 1984), we do not assume pairs of neurons to have symmetric connections. We then describe an algorithm for supervised learning based on these models (section 4) with minimal extra assumptions. Our model is based on two phases: at prediction time, no synaptic changes occur, whereas a local update rule becomes effective when the targets are observed. The proposed update mechanism is compatible with spike-timing-dependent plasticity (Bengio et al., 2017), which supposedly governs synaptic changes in biological neural systems. Finally, we show that the proposed algorithm has the desirable machine learning property of optimizing an objective function (section 5). We show this experimentally (Figure 3) and we provide the beginning for a theoretical explanation.
1

Under review as a conference paper at ICLR 2018
2 MOVING BEYOND ENERGY-BASED MODELS AND GRADIENT DYNAMICS
Historically, models based on energy functions and/or gradient dynamics have represented a key subject of neural network research. Their mathematical properties often allow for a simplified analysis, in the sense that there often exists an elegant formula or algorithm for computing the gradient of the objective function (Ackley et al., 1985; Movellan, 1990; Scellier & Bengio, 2017). However, we argue in this section that
1. due to the energy function, such models are very restrictive in terms of dynamics they can model - for instance the Hopfield model requires symmetric weights,
2. machine learning algorithms do not require computation of the gradient of the objective function, as shown in this work and the work of Lillicrap et al. (2016).
In this work, we propose a simple learning algorithm based on few assumptions. To this end, we relax the requirement of the energy function and, at the same time, we give up on computing the gradient of the objective function.
We believe that, in order to make progress in biologically plausible machine learning, dynamics more general than gradient dynamics should be studied.
As discussed in section 6, another motivation for studying more general dynamics is the possible implementation of machine learning algorithms, such as our model, on analog hardware: analog circuits implement differential equations, which do not generally correspond to gradient dynamics.
2.1 GRADIENT DYNAMICS ARE NOT GENERIC DYNAMICS
Most dynamical systems observed in nature cannot be described by gradient dynamics. A gradient field is a very special kind of vector field, precisely because it derives from a primitive scalar function. The existence of a primitive function considerably limits the "number of degrees of freedom" of the vector field and implies important restrictions on the dynamics.
In general, a vector field does not derive from a primitive function. In particular, the dynamics of the leaky integrator neuron model studied in this work (Eq. 1) is not a gradient dynamics, unless extra (biologically implausible) assumptions are made, such as exact symmetry of synaptic weights (Wij = Wji) in the case of the Hopfield model.
2.2 MACHINE LEARNING DOES NOT REQUIRE GRADIENT COMPUTATION
Machine learning relies on the basic principle of optimizing objective functions. Most of the work done in deep learning has focused on optimizing objective functions by gradient descent in the weight space (thanks to backpropagation). Although it is very well known that following the gradient is not necessarily the best option ­ many optimization methods based on adaptive learning rates for individual parameters have been proposed such as RMSprop Tieleman & Hinton (2012) and Adagrad Duchi et al. (2011) ­ almost all proposed optimization methods rely on computing the gradient, even if they do not follow the gradient. In the field of deep learning, "computing the gradient" has almost become synonymous with "optimizing".
In fact, in order to optimize a given objective function, not only following the gradient unnecessary, but one does not even need to compute the gradient of that objective function. A weaker sufficient condition is to compute a direction in the parameter space whose scalar product with the gradient is negative, without computing the gradient itself.
A major step forward was achieved by Lillicrap et al. (2016). One of the contributions of their work was to dispel the long-held assumption that a learning algorithm should compute the gradient of an objective function in order to be sound. Their algorithm computes a direction in the parameter space that has at first sight little to do with the gradient of the objective function. Yet, their algorithm "learns" in the sense that it optimizes the objective function. By giving up on the idea of computing the gradient of the objective function, a key aspect rendering backpropagation biologically implausible could be fixed, namely the weight transport problem.
The work presented here is along the same lines. We give up on the idea of computing the gradient of the objective function, and by doing so, we get rid of the biologically implausible symmetric connections required in the Hopfield model. In this sense, the "weight transport" problem in the
2

Under review as a conference paper at ICLR 2018

backpropagation algorithm appears to be similar, at a high level, to the requirement of symmetric connections in the Hopfield model.
We suggest that in order to make progress in biologically plausible machine learning, it might be necessary to move away from computing the true gradients in the weight space. An important theoretical effort to be made is to understand and characterize the dynamics in the weight space that optimize objective functions. The set of such dynamics is of course much larger than the tiny subset of gradient dynamics.

3 CLASSICAL DYNAMICS IN NEUROSCIENCE

We denote by si the averaged membrane voltage of neuron i across time, which is continuous-valued and plays the role of a state variable for neuron i. We also denote by (si) the firing rate of neuron i. We suppose that  is a deterministic function (nonlinear activation) that maps the averaged voltage si to the firing rate (si). The synaptic strength from neuron j to neuron i is denoted by Wij.
3.1 LEAKY INTEGRATOR NEURON MODEL In biological neurons a classical model for the time evolution of the membrane voltage si is the rate-based leaky integrator neuron model, in which neurons are seen as performing leaky temporal integration of their past inputs Dayan & Abbott (2001):

dsi = dt

Wij (sj ) - si.

j

(1)

Unlike energy-based models such as the Hopfield model (Hopfield, 1984) that assume symmetric connections between neurons, in the model studied here the connections between neurons are not tied. Thus, our model is described by a directed graph, whereas the Hopfield model is best regarded as an undirected graph (Figure 1).

(a) The network model studied here is best represented by a directed graph.

(b) The Hopfield model is best represented by an undirected graph.

Figure 1: From the point of view of biological plausibility, the symmetry of connections in the Hopfield model is a major drawback (1b). The model that we study here is, like a biological neural network, a directed graph (1a).

3.2 SPIKE-TIMING DEPENDENT PLASTICITY
Spike-Timing Dependent Plasticity (STDP) is considered a key mechanism of synaptic change in biological neurons (Markram & Sakmann, 1995; Gerstner et al., 1996; Markram et al., 2012). STDP is often conceived of as a spike-based process which relates the change in the synaptic weight Wij to the timing difference between postsynaptic spikes (in neuron i) and presynaptic spikes (in neuron j) (Bi & Poo, 2001). In fact, both experimental and computational work suggest that postsynaptic voltage, not postsynaptic spiking, is more important for driving LTP (Long Term Potentiation) and LTD (Long Term Depression) (Clopath & Gerstner, 2010; Lisman & Spruston, 2010).

Similarly, Bengio et al. (2017) have shown in simulations that a simplified Hebbian update rule based on pre- and post-synaptic activity can functionally reproduce STDP:

dWij  (sj)dsi.

(2)

3

Under review as a conference paper at ICLR 2018

Throughout this paper we will refer to this update rule (Eq. 2) as "STDP-compatible weight change" and propose a machine learning justification for such an update rule.

3.3 VECTOR FIELD µ IN THE STATE SPACE
Let s = (s1, s2, . . .) be the global state variable and parameter W the matrix of connection weights Wij. We write µ(W, s) the vector whose components are defined as

µi(W, s) := Wij(sj) - si
j

(3)

defining a vector field over the neurons state space, indicating in which direction each neuron's

activity changes:

ds = µ(W, s).
dt

(4)

Since

(sj )

=

µi  Wij

(W,

s),

the

weight

change

Eq.

2

can

also

be

expressed

in

terms

of

µ

in

the

form

dWij



µi  Wij

(W,

s)dsi

.

Note that for all i

=

i

we

have

µi  Wij

= 0 since to each synapse Wij

corresponds a unique post-synaptic neuron si.

Hence dWij



µ  Wij

(W,

s)

·

ds.

We rewrite the

STDP-compatible weight change in the more concise form

dW  µ (W, s) · ds. W

(5)

4 A BIOLOGICALLY PLAUSIBLE LEARNING ALGORITHM FOR FIXED POINT RECURRENT NETWORKS WITHOUT TIED WEIGHTS

The framework and the algorithm in their general forms are described in Appendix A.
To illustrate our algorithm, we consider here the supervised setting in which we want to predict an output y given an input x. We describe a simple two-phase learning procedure based on the dynamics Eq. 4 and Eq. 5 for the state and the parameter variables. This algorithm is similar to the one proposed by Scellier & Bengio (2017), but here we do not assume symmetric weights between neurons. Note that similar algorithms have also been proposed by O'Reilly (1996); Hertz et al. (1997) or more recently by Mesnard et al. (2016). Our contribution in this work are theoretical insights into why the proposed algorithm works.

4.1 TRAINING OBJECTIVE

In the supervised setting studied here, the units of the network are split in two sets: the inputs
x whose values are always clamped, and the dynamically evolving units h (the neurons activity,
indicating the state of the network), which themselves include the hidden layers (h1 and h2 here) and an output layer (h0 here), as in Figure 2. In this context the vector field µ is defined by its components µ0, µ1 and µ2 on h0, h1 and h2 respectively, as follows:

µ0(W, x, h) = W01 · (h1) - h0, µ1(W, x, h) = W12 · (h2) + W10 · (h0) - h1, µ2(W, x, h) = W23 · (x) + W21 · (h1) - h2.

(6) (7) (8)

Here the scalar function  is applied elementwise to the components of the vectors. The neurons h

follow the dynamics

dh = µ(W, x, h).
dt

(9)

In this section and the next we use the notation h rather than s for the state variable.

The layer h0 plays the role of the output layer where the prediction is read. The target outputs, denoted by y, have the same dimension as the output layer h0. The discrepancy between the output units h0 and the targets y is measured by the quadratic cost function

1 C(h, y) :=
2

y - h0 2 .

(10)

4

Under review as a conference paper at ICLR 2018

Unlike in the continuous Hopfield model, here the feed-forward and feedback weights are not tied,
and in general the state dynamics Eq. 9 is not guaranteed to converge to a fixed point. However we
observe experimentally that the dynamics almost always converges. We will see in section 5 that, for
a whole set of values of the weight matrix W . the dynamics of the neurons h converges. Assuming this condition to hold, the dynamics of the neurons converge to a fixed point which we denote by h0 (beware not to confuse with the notation for the output units h0). The prediction h00 is then read out on the output layer and compared to the actual target y. The objective function (for a single training case (x, y)) that we aim to minimize is the cost at the fixed point h0, which we write

J := C h0, y .

(11)

Note that this objective function is the same as the one proposed by Almeida (1987); Pineda (1987). Their method to optimize J is to compute the gradient of J thanks to an algorithm which they call "Recurrent Backpropagation". Other methods related to Recurrent Backpropagation exist to compute the gradient of J - in particular the "adjoint method", "implicit differentiation" and "Backprop Through Time". These methods are biologically implausible, as argued in Appendix B.

Here our approach to optimize J is to give up on computing the true gradient of J and, instead, we propose a simple algorithm based only on the leaky integrator dynamics (Eq. 4) and the STDPcompatible weight change (Eq. 5). We will show in section 5 that our algorithm computes a proxy for the gradient of J. Also, note that in its general formulation, our algorithm applies to any vector field µ and cost function C (Appendix A)

4.2 EXTENDED DYNAMICS
The idea of Equilibrium Propagation (Scellier & Bengio, 2017) is to see the cost function C (Eq. 10) as an external potential energy for the output units h0, which can drive them towards their target y. Following the same idea we define the "extended vector field" µ as

µ := µ -  C , h

(12)

and we redefine the dynamics of the state variable h as

dh = µ(W, x, h, y). dt

(13)

The real-valued scalar   0 controls whether the output h0 is pushed towards the target y or not, and by how much. We call  the "influence parameter" or "clamping factor".

The differential equation of motion Eq. 13 can be seen as a sum of two "forces" that act on the

temporal derivative of the state variable h. Apart from the vector field µ that models the interactions

between

neurons

within

the

network,

an

"external

force"

-

C h

is

induced

by

the external

potential

C and acts on the output neurons:

- C h0

=

(y - h0),

- C = 0, hi

i  1.

(14) (15)

The form of Eq. 14 suggests that when  = 0, the output units h0 are not sensitive to the targets y from the outside world. In this case we say that the network is in the free phase (or first phase).
When  > 0, the "external force" drives the output units h0 towards the target y. When  0 (small positive value), we say that the network is in the weakly clamped phase (or second phase). Also, note that the case   , not studied here, would correspond to fully clamped outputs.

4.3 TWO-PHASE ALGORITHM AND BACKPROPAGATION OF ERROR SIGNALS
We propose a simple two-phase learning procedure, similar to the one proposed by Scellier & Bengio (2017). In the first phase of training, the inputs are set (clamped) to the input values. The state variable (all the other neurons) follows the dynamics Eq. 9 (or equivalently Eq. 13 with  = 0) and the output units are free. We call this phase the free phase, as the system relaxes freely towards the free fixed point h0 without any external constraints on his output neurons. During this phase, the synaptic weights are unchanged.

5

Under review as a conference paper at ICLR 2018

(a) The supervised network studied here has directed connections.

(b) In the framework of Equilibrium Propagation with the Hopfield energy, the network is assumed to have symmetric connections.

Figure 2: Input x is clamped. Neurons h include "hidden layers" h2 and h1, and "output layer"

h0 that corresponds to the layer where the prediction is read. Target y has the same dimension as

h0.

The clamping

factor



scales the

"external

force"

-

C h

that

attracts

the output h0

towards the

target y.

In the second phase, the influence parameter  takes on a small positive value  0. The state

variable follows the dynamics Eq. 13 for that new value of , and the synaptic weights follow the

STDP-compatible weight change Eq. 5. This phase is referred to as the weakly clamped phase.

The

novel

"external

force"

-

C h

in

the

dynamics

Eq.

13

acts

on

the

output

units

and

drives

them

towards their targets (Eq. 14). This force models the observation of y: it nudges the output units

h0 from their free fixed point value in the direction of their targets. Since this force only acts on the output layer h0, the other hidden layers (hi with i > 0) are initially at equilibrium at the

beginning of the weakly clamped phase. The perturbation caused at the output layer will then

propagate backwards along the layers of the network, giving rise to "back-propagating" error signals.

The network eventually settles to a new nearby fixed point, corresponding to the new value  0,

termed weakly clamped fixed point and denoted h.

4.4 VECTOR FIELD  IN THE WEIGHT SPACE

Our model assumes that the STDP-compatible weight change (Eq. 5) occurs during the second phase of training (weakly clamped phase) when the network's state moves from the free fixed point h0 to the weakly clamped fixed point h. Normalizing by a factor  and letting   0, we get the update
rule W  (W ) for the weights, where (W ) is the vector defined as

µ (W ) :=

W, x, h0 · h

.

W  =0

(16)

The vector (W ) has the same dimension as W . Formally  is a vector field in the weight space.

It

is

shown

in

section

5

that

(W )

is

a

proxy

to

the

gradient

J W

.

The

effectiveness

of

the

proposed

method is demonstrated through experimental studies (Figure 3).

5 THE VECTOR FIELD  AS A PROXY FOR THE GRADIENT
In this section, we attempt to understand why the proposed algorithm is experimentally found to optimize the objective function J (Figure 3). We say that W is a "good parameter" if:

1.

for

any initial state

for the

neurons,

the

state dynamics

dh dt

=

µ (W, x, h) converges

to a

fixed point - a condition required for the algorithm to be correctly defined,

2.

the

scalar

product

J W

· (W )

at

the

point

W

is negative

-a

desirable condition

for

the

algorithm to optimize the objective function J.

6

Under review as a conference paper at ICLR 2018

Experiments show that the dynamics of h (almost) always converges to a fixed point and that J consistently decreases (Figure 3). This means that, during training, as the parameter W follows the update rule W  (W ), all values of W that the network takes are "good parameters". In this section we attempt to explain why.

5.1

EXPLICIT FORMULAS FOR

J W

AND 

Theorem 1. The gradient of J can be expressed in terms of µ and C as

J C µ -1 µ

=- ·

·.

W h h

W

(17)

Similarly, the vector field  (Eq. 16) is equal to

(W ) = C · h

µ

T

-1

·

µ .

h W

(18)

In these expressions, all terms are evaluated at the fixed point h0.

Theorem

1

is

proved

in

Appendix

A.

Note

that

the

formulas

show

that

(W )

is

related

to

J W

and that the angle between these two vectors is directly linked to the "degree of symmetry" of the

Jacobian of µ.

An important particular case is the setting of Equilibrium Propagation (Scellier & Bengio, 2017),

in

which

the

vector

field

µ

is

a

gradient

field

µ

=

-

E h

,

meaning

that

it

derives

from

an

energy

function

E.

In

this case

the

Jacobian of µ is

symmetric since it

is the Hessian

of

E.

Indeed

µ h

=

-

2E h2

=

T

µ h

. Therefore, Theorem 1 shows that  is also a gradient field, namely the gradient

of

the

objective

function

J,

that

is



=

-

J W

.

Note

that

in

this

setting

the

set

of

"good

parameters"

is the entire weight space - for all W , the dynamics

dh dt

=

-

E h

(W,

h)

converges

to

an

energy

minimum,

and

W

converges

to

a

minimum

of

J

since

W



-

J W

.

We argue that the set of "good parameters" covers a large proportion of the weight space and that they contain the matrices W that present a form of symmetry or "alignment". In the next subsection, we discuss how this form of symmetry may arise from the learning procedure itself.

Error (%) MSE
Angle (deg)

Classification error

7.5

Training Validation

5.0

2.5

0.0 0

20 40 60 Epoch

0.015 0.010 0.005 0.000
0

Loss
Training Validation
20 40 60 Epoch

Weight alignment
90.0 89.5 89.0
0 20 40 60 Epoch

Figure 3: Example system trained on the MNIST dataset, as described in Appendix C. The objective
function is optimized: the training error decreases to 0.00% in around 70 epochs. The generaliza-
tion error is about 2%. Right: A form of symmetry or alignment arises between feedforward and feedback weights Wk,k+1 and Wk+1,k in the sense that tr(Wk,k+1 · Wk+1,k) > 0. This architecture uses 3 hidden layers each of dimension 512.

5.2 A FORM OF SYMMETRY ARISES
Experiments show that a form of symmetry between feedforward and feedback weights arises from the learning procedure itself (Figure 3). Although the causes for this phenomenon aren't understood very well yet, it is worth pointing out that similar observations have been made in previous work and different settings.

7

Under review as a conference paper at ICLR 2018
A striking example is the following one. A major argument against the plausibility of backpropagation in feedforward nets is the weight transport problem: the signals sent forward in the network and those sent backward use the same connections. Lillicrap et al. (2016) have observed that, in the backward pass, (back)propagating the error signals through fixed random feedback weights (rather than the transpose of the feedforward weights) does not harm learning. Moreover, the learned feedforward weights Wk,k+1 tend to 'align' with the fixed random feedback weights Wk+1,k in the sense that the trace of Wk,k+1 · Wk+1,k is positive.
Denoising autoencoders without tied weights constitute another example of learning algorithms where a form of symmetry in the weights has been observed as learning goes on (Vincent et al., 2010).
The theoretical result from Arora et al. (2015) also shows that, in a deep generative model, the transpose of the generative weights perform approximate inference. They show that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying linear units.
6 POSSIBLE IMPLEMENTATION ON ANALOG HARDWARE
Our approach provides a basis for implementing machine learning models in continuous-time systems, while requirements regarding the actual dynamics are reduced to a minimum. This means that the model applies to a large class of physical realizations of vectorfield dynamics, including analog electronic circuits. Implementations of recurrent networks based on analog electronics have been proposed in the past, e.g. Hertz et al. (1997), however, these models typically required circuits and associated dynamics to adhere to an exact theoretical model. With our framework, we provide a way of implementing a learning system on a physical substrate without even knowing the exact dynamics or microscopic mechanisms that give rise to it. Thus, this approach can be used to analog electronic system end-to-end, without having to worry about exact device parameters and inaccuracies, which inevitably exist in any physical system. Instead of approximately implementing idealized computations, the actual analog circuit, with all its individual device variations, is trained to perform the task of interest. Thereby, the more direct implementation of the dynamics might result in advantages in terms of speed, power, and scalability, as compared to digital approaches.
7 CONCLUSION
Our model demonstrates that biologically plausible learning in neural networks can be achieved with relatively few assumptions. As a key contribution, in contrast to energy-based approaches such as the Hopfield model, we do not impose any symmetry constraints on the neural connections. Our algorithm assumes two phases, the difference between them being whether synaptic changes occur or not. Although this assumption begs for an explanation, neurophysiological findings suggest that phase-dependent mechanisms are involved in learning and memory consolidation in biological systems. Theta waves, for instance, generate neural oscillatory patterns that can modulate the learning rule or the computation carried out by the network Orr et al. (2001). Furthermore, synaptic plasticity, and neural dynamics in general, are known to be modulated by inhibitory neurons and dopamine release, depending on the presence or absence of a target signal. Fre´maux & Gerstner (2016); Pawlak et al. (2010).
In its general formulation (Appendix A), the work presented in this paper is an extension of the framework of Scellier & Bengio (2017) to general dynamics. This is achieved by relaxing the requirement of an energy function. This generalization comes at the cost of not being able to compute the (true) gradient of the objective function but, rather a direction in the weight space which is related to it. Thereby, precision of the approximation of the gradient is directly related to the "alignment" between feedforward and feedback weights. Even though the exact underlying mechanism is not fully understood yet, we observe experimentally that during training the weights symmetrize to some extent, as has been observed previously in a variety of other settings (Lillicrap et al., 2016; Vincent et al., 2010; Arora et al., 2015). Our work shows that optimization of an objective function can be achieved without ever computing the (true) gradient. More thorough theoretical analysis needs to be carried out to understand and characterize the dynamics in the weight space that optimize objective functions. Naturally, the set of all such dynamics is much larger than the tiny subset of gradient-based dynamics.
Our framework provides a means of implementing learning in a variety of physical substrates, whose precise dynamics might not even be known exactly, but which simply have to be in the set of sup-
8

Under review as a conference paper at ICLR 2018
ported dynamics. In particular, this applies to analog electronic circuits, potentially leading to faster, more efficient, and more compact implementations.
REFERENCES
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. 9: 147­169, 1985.
L. B. Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. volume 2, pp. 609­618, San Diego 1987, 1987. IEEE, New York.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. Why are deep nets reversible: a simple theory, with implications for training. Technical report, arXiv:1511.05653, 2015.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR'2015, arXiv:1409.0473, 2015.
Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang, and Yuhuai Wu. STDPcompatible approximation of back-propagation in an energy-based model. Neural Computation, 29(3):555­577, 2017.
G. Bi and M. Poo. Synaptic modification by correlated activity: Hebb's postulate revisited. Annu. Rev. Neurosci., 24:139­166, 2001.
Claudia Clopath and Wulfram Gerstner. Voltage and spike timing interact in stdp­a unified model. Frontiers in synaptic neuroscience, 2, 2010.
Francis Crick. The recent excitement about neural networks. Nature, 337(6203):129­132, 1989.
Peter Dayan and L. F. Abbott. Theoretical Neuroscience. The MIT Press, 2001.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011.
Nicolas Fre´maux and Wulfram Gerstner. Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules. Frontiers in neural circuits, 9:85, 2016.
W. Gerstner, R. Kempter, J.L. van Hemmen, and H. Wagner. A neuronal learning rule for submillisecond temporal coding. Nature, 386:76­78, 1996.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS'2010, 2010.
J. A. Hertz, A. Krogh, B. Lautrup, and T. Lehmann. Nonlinear backpropagation: doing backpropagation without derivatives of the activation function. IEEE Transactions on neural networks, 8 (6):1321­1327, 1997.
Geoffrey Hinton, Li Deng, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6): 82­97, Nov. 2012.
Geoffrey E. Hinton and James L. McClelland. Learning representations by recirculation. In D. Z. Anderson (ed.), Neural Information Processing Systems, pp. 358­ 366. American Institute of Physics, 1988. URL http://papers.nips.cc/paper/ 78-learning-representations-by-recirculation.pdf.
J. J. Hopfield. Neurons with graded responses have collective computational properties like those of two-state neurons. 81, 1984.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS'2012). 2012.
9

Under review as a conference paper at ICLR 2018
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7, 2016.
John Lisman and Nelson Spruston. Questions about stdp as a general model of synaptic plasticity. Frontiers in synaptic neuroscience, 2, 2010.
H. Markram and B. Sakmann. Action potentials propagating back into dendrites triggers changes in efficacy. Soc. Neurosci. Abs, 21, 1995.
H. Markram, W. Gerstner, and P.J. Sjstrm. Spike-timing-dependent plasticity: A comprehensive overview. Frontiers in synaptic plasticity, 4(2), 2012.
Thomas Mesnard, Wulfram Gerstner, and Johanni Brea. Towards deep learning with spiking neurons in energy based models with contrastive hebbian plasticity. arXiv preprint arXiv:1612.03214, 2016.
Javier R. Movellan. Contrastive Hebbian learning in the continuous Hopfield model. In Proc. 1990 Connectionist Models Summer School, 1990.
Randall C. O'Reilly. Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural Computation, 8(5):895­938, 1996.
G Orr, G Rao, FP Houston, BL McNaughton, and Carol A Barnes. Hippocampal synaptic plasticity is modulated by theta rhythm in the fascia dentata of adult and aged freely behaving rats. Hippocampus, 11(6):647­654, 2001.
Verena Pawlak, Jeffery R Wickens, Alfredo Kirkwood, and Jason ND Kerr. Timing is not everything: neuromodulation opens the stdp gate. Frontiers in synaptic neuroscience, 2, 2010.
F. J. Pineda. Generalization of back-propagation to recurrent neural networks. 59:2229­2232, 1987. Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
based models and backpropagation. Frontiers in computational neuroscience, 11, 2017. T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res., 11, 2010.
10

Under review as a conference paper at ICLR 2018

Appendix

A GENERAL FORMULATION

In this Appendix, we present the framework and the algorithm in their general formulations and we prove the theoretical results.

A.1 PRELIMINARY DEFINITIONS

We consider a physical system specified by a state variable s and a parameter variable . The system is also influenced by an external input v, e.g. in the supervised setting v = (x, y) where y is the target that the system wants to predict given x.

Let s  µ(, v, s) be a vector field in the state space and C(, v, s) a cost function. We assume that the state dynamics induced by µ converges to a stable fixed point s0,v, corresponding to the
"prediction" from the model and characterized by

µ , v, s0,v = 0.

(19)

The objective function that we want to optimize is the cost at the fixed point

J (, v) := C , v, s0,v .

(20)

Note the distinction between J and C: the cost function is defined for any state s whereas the objective function is the cost at the fixed point. The training objective (for a single data sample v) is

find arg min J(, v).


(21)

Equivalently, the training objective can be reformulated as a constrained optimization problem:

find arg min C(, v, s)
,s
subject to µ (, v, s) = 0,

(22) (23)

where the constraint µ (, v, s) = 0 is the fixed point condition.

All traditional methods to compute the gradient of J (adjoint method, implicit differentiation, Recurrent Backpropagation and Backpropagation Through Time or BPTT) are thought to be biologically implausible. Our approach is to give up on computing the gradient of J and let the parameter variable  follow a vector field  in the parameter space which is "close" to the gradient of J.

Before defining  we first introduce the "extended vector field"

µ(, v, s)

:=

µ(, v, s) - 

C (, v, s),

s

(24)

where  is a real-valued scalar called "influence parameter". Then we extend the notion of fixed point for any value of . For any  we define the -fixed point s,v such that

µ , v, s,v = 0.

(25)

Under mild regularity conditions on µ and C, the implicit function theorem ensures that, for a fixed data sample v, the funtion (, )  s,v is differentiable.
Now for every  and v we define the vector (, v) in the parameter space as

(, v) := - C 

, v, s0,v

µ +


, v, s0,v

· s,v 

.

=0

(26)

As shown in section 4, the second term on the right hand side can be estimated in a biologically realistic way thanks to a two-phase training procedure.

11

Under review as a conference paper at ICLR 2018

As

compared

to

section

4,

the

definition

of

the

vector



(,

v)

contains

another

term

-

C 

, v, s0,v

in the general case where the cost function C also depends on the parameter . This extra term can

be measured in a biologically realistic way at the fixed point s0,v at the end of the free phase. For

example

if

C

includes

a

regularization

term

such

as

1 2





2, then (, v) will include a backmoving

force - modelling a form of synaptic depression.

A.2 MAIN RESULT AND EXPLICIT FORMULAS

Lemma 2. Let s  µ(, s) be a differentiable vector field, and s a fixed point characterized by

µ , s = 0.

(27)

Then the partial derivatives of the fixed point are given by

s = - 

µ s

, s

-1 · µ 

, s

(28)

and

s = - 

µ s

, s

-1 · µ 

, s

.

(29)

Proof of Lemma 2. First we differentiate the fixed point equation Eq. 27 with respect to :

d (27)  d

µ 

, s

µ +
s

, s

· s = 0. 

(30)

Rearranging the terms we get Eq. 28. Similarly we differentiate the fixed point equation Eq. 27 with

respect to :

d (27)  d

µ 

, s

µ +
s

, s

· s = 0. 

(31)

Rearranging the terms we get Eq. 29.

Theorem 3. The gradient of the objective function is equal to

J = C - C · µ -1 · µ

  s s



and the vector field  is equal to

 = -C + C ·  s

µ

T

-1

·

µ .

s 

(32) (33)

All the factors on the right-hand sides of Eq. 32 and Eq. 33 are evaluated at the fixed point s0.

Proof of Theorem 3. Let us compute the gradient of the objective function with respect to . Using the chain rule of differentiation we get

J = C + C · s0 .   s 

(34)

Hence Eq. 32 follows from Eq. 28 evaluated at  = 0. Similarly, the expression for the vector field

 (Eq. 33) follows from its definition (Eq. 26), the identity Eq. 29 evaluated at  = 0 and the fact

that

µ 

=

-

C s

.

We finish by stating and proving a last result. Consider the setting introduced in section 4 with the

quadratic cost function C

=

1 2

y - h0 2. In the weakly clamped phase, the "external influence"

- (y - h0) added to the vector field µ (with  0) slightly attracts the output state h0 to the

target y. It is intuitively clear that the weakly clamped fixed point is better than the free fixed point

in terms of prediction error. Proposition 5 below generalizes this property to any vector field µ and

any cost function C.

12

Under review as a conference paper at ICLR 2018

Proposition 4. Let s0 be a stable fixed point of the vector field s  µ(s), in the sense that s - s0 ·

µ (s) < 0 for s in the neighborhood of s0 (i.e. the vector field at s points towards s0). Then the

Jacobian

of

µ

at

the

fixed

point

µ s

s0

is negative, in the sense that

v, v · µ s0 · v  0. s

(35)

Proof. Let v be a vector in the state space,  > 0 a positive scalar and let s := s0 + v. For  small enough, the vector s is in the region of stability of s0. Using a first order Taylor expansion and the fixed point condition µ s0 = 0 we get

0 > s - s0 · µ (s) = v · µ s0 + v

(36) (37)

as   0. Hence the result.

= v · µ s0 · v + o 2 s

(38)

The following proposition shows that, unless the free fixed point s0,v is already optimal in terms of cost value, for  > 0 small enough, the nudged fixed point s,v achieves lower cost value than the free fixed point. Thus, a small perturbation due to a small increment of  nudges the network
towards a configuration that reduces the cost value.

Proposition 5.

Let s

be a stable fixed point of the extended vector field µ

=

µ

-



C s

.

Then the

derivative of the function

  C , s

(39)

at  = 0 is non-positive.

Proof of Proposition 5. Multiplying both sides of Eq. 31 on the left by -

 s 

T
and rearranging

the terms, we get

-

s

T
· µ =

s

T · µ · s  0.

   s 

(40)

The

inequality

holds

because

µ s

, s

is negative as s is a stable fixed point of µ (Eq. 35).

Since

µ 

=

-

C s

,

the

left-hand

side,

for



= 0, represents the derivative of

  C , s .

(41)

B ADJOINT METHOD AND RELATED ALGORITHMS
Earlier work have proposed various methods to compute the gradient of the objective function J (Eq. 20). One common method is the "adjoint method". In the context of fixed point recurrent neural networks studied here, the adjoint method leads to Backpropagation Through Time (BPTT) and "Recurrent Backpropagation" (Almeida, 1987; Pineda, 1987). BPTT is the method of choice today for deep learning but its biological implausibility is obvious - it requires the network to store all its past states for the propagation of error derivatives in the second phase. Recurrent Backpropagation corresponds to a special case of BPTT where the network is initialized at the fixed point. This algorithm does not need to store the past states of the network (the state at the fixed point suffices) but it still requires neurons to send a different kind of signals through a different computational path in the second phase, which seems therefore less biologically plausible than our algorithm.
Our approach is to give up on the idea of computing the gradient of the objective function. Instead we show that the STDP-compatible weight change computes a proxy to the gradient in a more biologically plausible way.
13

Under review as a conference paper at ICLR 2018

B.1 CONTINUOUS-TIME BACKPROPAGATION

For completeness, we state and prove a continuous-time version of Backpropagation Through Time and Recurrent Backpropagation. The formulas for the propagation of error derivatives (Theorem 6 and Corollary 7) will make it obvious that our algorithm is more biologically plausible than both of these algorithms.

To keep notations simple, we omit to write dependences in the data sample v. We consider the

dynamics

ds dt

=

µ(, s)

and

denote

by

st

the

state

of

the

system

at

time

t



0

when

it

starts

from

an

initial state s0 at time t = 0. Note that st converges to the fixed point s0 as t  . We then define

a family of objective functions

L(, s0, T ) := C (, sT )

(42)

for every couple (s0, T ) of initial state s0 and duration T  0. This is the cost of the state at time

t = T when the network starts from the state s0 at time t = 0. Note that L(, s0, T ) tends to J() as T   (Eq. 20).

We want to compute the gradient

L 

(,

s0,

T)

as

T



.

For that purpose, we fix T

to a large

value and we consider the following quantity

L L sT -t := s (, sT -t, t) ,

(43)

which represents the "partial derivative of the cost with respect to the state at time T - t". In other words this is the "partial derivative of the cost with respect to the (T - t)-th hidden layer"

if one regards the network as unfolded in time (though time is continuous here). The formulas in

Theorem 6 below correspond to a continuous-time version of BPTT for the propagation of the partial

derivatives

L sT -t

backward

in

time.

Theorem 6 (Continuous-Time Backpropagation Through Time). The process of "partial deriva-

tives"

L sT -t

is

such

that

d L =
dt sT -t

µ s (, sT -t)

T
·

L ,

sT -t

(44)

and

the

gradient

L 

(,

sT -t,

t)

is

such

that

d L dt  (, sT -t, t) =

µ  (, sT -t)

T
·

L .

sT -t

(45)

Computing

L sT -t

and

L 

(,

sT

-t,

t)

thanks

to

Eq.

44

and

Eq.

45

is

biologically

infeasible

since

it

requires storing the past states sT -t.

In the particular case where the network is initialized at the fixed point, then we have sT -t = s0 for all t and we get a continuous-time version of "Recurrent Backpropagation" (Almeida, 1987; Pineda,

1987).

Corollary 7 (Continuous-Time Recurrent Backpropagation).

The process

L s

, s0, t

for t  0

satisfies the differential equation

d L dt s

, s0, t

=

µ s

, s0

T · L s

, s0, t

.

(46)

and

the

process

L 

, s0, t

for t  0 satisfies

d L dt 

, s0, t

=

µ 

, s0

T · L s

, s0, t

.

(47)

Here

the

notation

L 

represents

the

partial

derivative

with

respect

to

the

first

argument,

which

does

not include the path through s0.

Recurrent Backpropagation does not require the state s go backward in time in the second phase.

The state of the network stays at the fixed point s0. However we still need a special computational

path

for

the

computation

of

L s

, s0, t . From the point of view of biological plausibility, it is not

T

clear how this can be done and how the transpose of the Jacobian

µ s

, s0

can be measured.

14

Under review as a conference paper at ICLR 2018

Proof of Theorem 6. To keep notations simple, we omit to write the dependence in . First we show

that for all s and t we have

L (s, t)

=

L (s,

t)

·

µ(s).

t s

(48)

To this end note that

L(su, t - u) = L(s0, t)

(49)

is independent of u. Therefore

d du

L(su,

t

-

u)

=

0

L L = - t (su, t - u) + s (su, t - u) · µ(su).

(50) (51)

Here we have used the chain rule of differentiation and the differential equation of motion. Evaluat-
ing this expression for u = 0 we get Eq. 48 since the initial point s0 is arbitrary. Then, differentiating Eq. 48 with respect to s, we get

2L (s, t)
ts

=

2L s2 (s, t)

·

µ(s)

+

µ (s)

T

·

L (s, t)

=

0.

s s

(52)

Now

let us

differentiate

L s

(sT

-t

,

t)

with

respect

to

t.

Using

the

chain

rule

of

differentiation,

the

differential equation of motion and Eq. 52 (at the point s = sT -t) we get

d L dt s (sT -t, t)

2L = ts (sT -t, t)

-

2L s2 (sT -t, t)

·

µ(sT -t)

=

µ s (sT -t)

T

·

L s (sT -t, t).

(53) (54) (55)

Hence

Eq.

44.

We

derive

Eq.

45

similarly

by

differentiating

L 

(,

sT -t,

t)

with

respect

to

t.

C IMPLEMENTATION DETAILS OF THE MODEL

Our model is a recurrently connected neural network without any constraint on the feedback weight values. We train multi-layered networks with 2 or 3 hidden layers, with no skip-layer connections and no lateral connections within layers.

Rather than doing the weight updates at all time steps, we use a single update at the end of the

weakly clamped phase:

W  µ h0 · h - h0 . h 

(56)

The prediction is made on the last layer at the free fixed point h00 at the end of the first phase

relaxation. The predicted value hpred is the index of the output unit whose activation is maximal

among the 10 output units:

hpred := arg max h00,i.

(57)

i

Implementation of the differential equation of motion. We start by clamping x to the data values. Then, to implement Eq. 13, we use the Euler method. We discretize time into short time lapses of duration and update the state variable h thanks to the following equation:

h  h - µ(W, x, h, y).

(58)

For our experiments, we choose the hard sigmoid activation function (hi) = 0  hi  1, where 

denotes the max and  the min. For this choice of , since  (hi) = 0 for hi < 0, it follows from

Eq. 1 and Eq. 14 that if hi

<

0 then

F hi

(,

v,



,

s)

=

-hi

>

0.

This force prevents the hidden

unit hi from going in the range of negative values. The same is true for the output units. Similarly,

15

Under review as a conference paper at ICLR 2018

hi cannot reach values above 1. As a consequence hi must remain in the domain 0  hi  1. Therefore, rather than the standard gradient descent (Eq. 58), we will use a slightly different update rule for the state variable h:

h  0  h - µ(W, x, h, y)  1.

(59)

This little implementation detail turns out to be very important: if the i-th hidden unit was in some state hi < 0, then Eq. 58 would give the update rule hi  (1 - )hi, which would imply again hi < 0 at the next time step (assuming < 1). As a consequence hi would remain in the negative range forever.

We use different learning rates for the different layers in our experiments. We do not have a clear explanation for why this improves performance, but we believe that this is due to the finite precision with which we approach the fixed points.

The hyperparameters chosen for each model are shown in Table 1 and the results are shown in Figure 3. We initialize the weights according to the Glorot-Bengio initialization (Glorot & Bengio, 2010). For efficiency of the experiments, we use minibatches of 20 training examples.

Architecture

Iterations

Iterations

 1 2 3 4

(first phase) (second phase)

784 - 512 - 512 - 10

200

100 0.001 1.0 0.4 0.1 0.01 --

784 - 512 - 512 - 512 - 10

200

100 0.001 1.0 1.0 0.1 0.04 0.002

Table 1: Hyperparameters. for both the 2 and 3 layered MNIST. Example system trained on the MNIST dataset, as described in Appendix C. The objective function is optimized: the training error decreases to 0.00%. The generalization error lies between 2% and 3% depending on the architecture. The learning rate is used for iterative inference (Eq. 59).  is the value of the clamping factor in the second phase. k is the learning rate for updating the parameters in layer k.

We were also able to train on MNIST using a Convolutional Neural Network (CNN). We got around 2% generalization error. The hyperparameters chosen to train this Convolutional Neural Network are shown in Table 2.

Operation Kernel Strides Feature Maps Non Linearity

Convolution 5 x 5

1

32

Relu

Convolution 5 x 5

1

64

Relu

Table 2: Hyperparameters for MNIST CNN experiments.

16

