Under review as a conference paper at ICLR 2018
TRANSFER LEARNING ON MANIFOLDS VIA LEARNED TRANSPORT OPERATORS
Anonymous authors Paper under double-blind review
ABSTRACT
Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.). We desire a method for learning a representation of these manifolds that can be used to increase robustness and reduce the training burden in machine learning tasks. In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error. Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another. The main contribution of this paper is to show that the manifold representation learned in the transport operators is valuable both as a probabilistic model to improve general machine learning tasks as well as for performing transfer learning in classification tasks. Specifically, we demonstrate that manifold transport operators can learn a robust representation of manifold structure. We then use this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space. These results are shown on stylized constructions using the classic swiss roll data structure for ease of visualization, as well as demonstrations of transfer learning in a data augmentation task for few-shot image classification.
1 INTRODUCTION
While significant progress has been made in training classifiers that can effectively discriminate between thousands of classes, the increasing classifier complexity requires large training datasets to capture variations within each class. In many settings, the within-class variation in a highdimensional dataset can be modeled as being low-dimensional due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.). When these variations are within a linear subspace, classic techniques such as principal component analysis (PCA) can be used to efficiently capture the transformations within the data. However, the manifold hypothesis states that in many cases of interest (e.g., images, sounds, text), the within-class variations lie on or near a low-dimensional nonlinear manifold (Bengio et al., 2013). In the neuroscience literature there is a hypothesis that the manifold nature of these variations is explicitly exploited by hierarchical processing stages to untangle the representations of different objects undergoing the same physical transformations (e.g., pose).
We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness and reduce the training burden in machine learning tasks. In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error. In short, we desire a representation that includes a generative model that can be used for a type of transfer learning where the variation manifold for one class is transferred to another class undergoing the same physical transformation, thereby reducing the training burden. Such an approach would also have the advantage of implying a probabilistic model on the manifold that can
1

Under review as a conference paper at ICLR 2018
be used in the context of standard machine learning tasks when the data lives on a low-dimensional manifold.
There are a large number of "manifold learning" algorithms that have been introduced in the literature to discover manifold structure from data. The most common approach to this task is to perform an embedding of the original data points after performing a transformation to preserve either local or global properties of the manifold (e.g., local neighborhood relationships, global geodesic distances, etc.) (Tenenbaum et al., 2000; Roweis & Saul, 2000; Weinberger & Saul, 2006; Belkin & Niyogi, 2003). Unfortunately, such approaches capture manifold structure through a transformation of the data points themselves into a lower dimensional space and therefore are not suitable for the desired tasks. Specifically, there is no generative model of the data in the original high-dimensional space, meaning that the inferred manifold structure is not transferrable to other data classes, is not amenable to strong interpolation/extrapolation, and does not provide a probabilistic model that can be used in machine learning tasks. Recently, there there are a number of methods that have been introduced that capture manifold structure through a variety of approaches that involve estimating local tangent planes (Dolla´r et al., 2007b;a; Bengio & Monperrus, 2005; Park et al., 2015). While these methods can admit representations that have some of the above advantages of generative models, the linear approximations can cause challenges when trying to perform transfer learning by extrapolating to out-of-sample points in manifold locations not well-represented in the training data.
As an alternative to the above, previous work has proposed unsupervised learning algorithms to efficiently learn Lie group operators that capture the structure of low-dimensional manifolds (Culpepper & Olshausen, 2009; Van Gool et al., 1995; Miao & Rao, 2007; Rao & Ruderman, 1999). In this approach, the manifold representation is described by analytic operators (called transport operators) found through unsupervised learning that define the process of transporting one data point to another rather than through an embedding of a point cloud, thereby providing a probabilistic generative model for the manifold structure. While previous work has demonstrated that the unsupervised learning process can learn operators that appear to follow the general manifold shapes, the utility of these manifold representations in machine learning tasks has not been shown. The main contribution of this paper is to show that the manifold representation learned in the transport operators is valuable both as a probabilistic model to improve general machine learning tasks as well as for performing transfer learning in classification tasks. Specifically, we demonstrate that manifold transport operators can learn a robust representation of manifold structure. We then use this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space. These results are shown on stylized constructions using the classic swiss roll data structure for ease of visualization, as well as demonstrations of transfer learning in a data augmentation task for few-shot image classification.
2 BACKGROUND AND RELATED WORK
Transfer learning problems can take a variety of forms (Pan & Yang, 2010), including using training data to approximate target distributions (after modification) (Shimodaira, 2000; Wang & Schneider, 2014; Huang et al., 2006; Gong et al., 2012; Ben-David et al., 2010), bias a classifier (Eaton et al., 2008), or define a prior on parameters (Raina et al., 2006; Srivastava & Salakhutdinov, 2013). Research by Freifeld et al. (2014) introduced a framework that can transfer a data distribution from one area of a manifold to another using parallel transport of the model parameters along the manifold. Some data augmentation techniques are closely related to transfer learning because they attempt to expand limited training sets beyond the original domain using information from classes with many examples (Wei et al., 2012; Hauberg et al., 2015; Freifeld et al., 2015).
Many of the manifold learning techniques for non-linear manifolds (e.g., Isomap (Tenenbaum et al., 2000), Locally-Linear Embedding (LLE) (Roweis & Saul, 2000), Maximum Variance Unfolding (MVU) (Weinberger & Saul, 2006), and Laplacian Eigenmaps (Belkin & Niyogi, 2003)) represent the manifold through a low-dimensional embedding of the data points. Although extensions to out-of-sample points have been proposed for several manifold learning techniques, these are highly dependent on the initial domain of the training data (Bengio et al., 2004). In contrast, there have been several recent techniques introduced (e.g., Locally Smooth Manifold Learning (LSML) (Dolla´r et al., 2007b;a) and Non-Local Manifold Tangent Learning (Bengio & Monperrus, 2005)) that use the data points to learn a function that maps high dimensional points to tangent planes to represent
2

Under review as a conference paper at ICLR 2018

Figure 1: Trajectories of the two dictionary elements, m, trained on point pairs on a swiss roll. Each plot shows a single transport operator acting on several example points.

the manifold. While these tangent planes can be estimated anywhere in the data space, error can accumulate quickly in the linear approximations when extrapolated away from the training domain. The recent Locally Linear Latent Variable Model (LL-LVM) estimates both the embedded points and the mapping between the high-dimensional and low-dimensional data using a probabilistic model which tries to maximize the likelihood of the observations (Park et al., 2015). While similar in spirit to LSML and Non-Local Manifold Tangent Learning, this technique has the added benefit of a metric which determines the quality of an embedding and a method to reconstruct high-dimensional representations of out-of-sample points in the embedding.

Manifold transport operators are Lie group operators that capture the paths on a manifold between
data points through an analytic operator (Culpepper & Olshausen, 2009). In this approach, we assume that two nearby points x0 RN x0, x1  RN living on a low-dimensional manifold can be related through a dynamical system that has the solution path

x1 = expm(A)x0 + n,

(1)

where A  RN×N : x = Ax is the operator capturing the dynamics, n is the error, and expm

is a matrix exponential. To allow for different geometrical characteristics at various points on the

manifold, each pair of points may require a different dynamics matrix A. We assume that this matrix

can be decomposed as a weighted sum of M dictionary elements (m  RN×N ) called transport

operators:

M

A = mcm.

(2)

m=1
The transport operators represent a set of primitives that describe local manifold characteristics. For
each pair of points (i.e., at each manifold location), the geometry will be governed by a small subset of these operators through the weighting coefficients c  RM .

Using the relationship between points in (1) and the decomposition in (2), we can write a probabilis-

tic generative model that allows efficient inference. We assume a Gaussian noise model, a Gaussian

prior on the transport operators (model selection), and a sparsity inducing prior on the coefficients

(model regularization). The resulting negative log posterior for the model is given by

1 2

x1 - exp

M
mcm
m=1

2



x0

+ 2

2m

m

2 F

+



c

1

(3)

where · F is the Frobenius norm. Following the unsupervised algorithm in (Culpepper & Ol-

shausen, 2009), we use pairs of nearby training points to perform unsupervised learning on the

transport operators using descent techniques (alternating between the coefficients and the transport

operators) on the objective in (3).

3 MANIFOLD TRANSPORT OPERATORS
3.1 LEARNING ACCURATE AND ROBUST MANIFOLD REPRESENTATIONS
Using the swiss roll manifold as a benchmark, we learn transport operators by randomly sampling points in a defined area on the manifold. During training, we randomly select an initial point (x0)

3

Under review as a conference paper at ICLR 2018
Figure 2: Example 2D embedding formed using the transport operator objective function as a similarity metric.
and select a second point (x1) from a defined neighborhood surrounding x0. For this pair we infer the coefficients c as described above using a fixed dictionary {m}. Using a batch of these point pairs and associated coefficients we then take a gradient step on the objective with respect to the dictionary while holding the coefficients fixed. In this example we fix the number of dictionary elements to two (setting  = 0) and set the coefficient  = 2. Fig 1 shows the trajectories of the transport operator dictionary elements resulting from this training. Each plot uses a few example starting points on the swiss roll, xi(0), and visualizes the effect of a single transport operator by applying a single dictionary element m to each point in time steps xi(t) = exp(mt)xi(0), t = 0, ...., T . In this case, the two operators have learned to move in an expanding (shrinking) rotational motion and in a transverse motion. As mentioned earlier, the generative model of the transport operator approach implies a probabilistic model that should have value in machine learning applications. Specifically, the objective function in (3) is the negative log posterior of the model, and serves as a quantifiable measure for how well a given pair of points is represented in the model. This measure combines the approximate length of the connecting path between the points (measured by the coefficient magnitudes) and the MSE of the approximation using the manifold transport operators. With the objective function as a similarity measure between pairs of points and running classical Multidimensional scaling (MDS), we can create an embedding of the points in 2D that shows their intrinsic low-dimensional structure (Fig 2). The quality of this embedding demonstrates that the learned transport operators are accurately capturing the low-dimensional structure of the manifold data. While many of the classic manifold learning techniques mentioned earlier are designed specifically to find such embeddings, they can be fragile to algorithmic assumptions. For example, most other manifold learning techniques require a local neighborhood definition and even small mistakes in the neighborhood definition can lead to major errors if shortcuts are created (i.e., points nearby in Euclidean distance are considered neighbors even though they are far away on the manifold). Additionally, with many algorithms there is no way to know if a neighborhood definition has caused a shortcut error to occur, and therefore there is no way to assess the quality of the output embedding. In contrast, the representation learned through the transport operator approach shows significant robustness properties due to two factors. While the transport operator model does require a neighborhood definition to select point pairs during training, this algorithm is more robust to mistakes in this definition because the information from each pair is averaged with many other point pairs in the learning process. Furthermore, after learning the transport operators on the dataset, the objective function value in (3) can be used to evaluate the likelihood of points being in the same neighborhood on the low-dimensional manifold. Fig 3 provides an example how the objective function metric can be used to correct the neighborhoods for an isomap embedding (Tenenbaum et al., 2000). When the isomap embedding is computed with a neighborhood defined by the k-nearest neighbors with k = 11, there are shortcut connections that lead to a distorted embedding. We compute the objective function between each pair of points using the learned transport operator representation and "cut" the connections shown in Fig 3(b) where the objective function is greater than threshold (based on outliers in a histogram of the objective function values). Running isomap on the corrected neighborhood definition produces a much more accurate embedding as demonstrated in Fig 3(a). As a note, while the recently proposed LL-
4

Under review as a conference paper at ICLR 2018

Original Isomap: k = 5

Cut Isomap: k = 5

True 2D Embedding

Original Isomap: k = 11 Cut Isomap: k = 11

(a) (b)
Figure 3: (a) Left column: True embedding of 400 swiss roll points. Middle column: Isomap embedding of swiss roll points for neighborhoods defined by k-nearest neighbors with k = 5 and k = 11. When k = 11, the embedding is distorted due to the large neighborhoods which include shortcuts. Right column: Embeddings after the transport operator objective function is used to identify and eliminate shortcuts. (b) View of the swiss roll points with lines between the points which had connections broken due to large objective function values.
LVM (Park et al., 2015) defines a probabilistic model that can be similarly used to identify when an entire embedding is bad due to a neighborhood error, the transport operator approach defines an objective function on each point pair that can be used to identify precisely which pairs are causing erroneous neighborhood definitions.
4 TRANSFER LEARNING
As mentioned earlier, the probabilistic generative model underlying the transport operators approach allows for the possibility of generating new points on (or near) the manifold. For example, after inferring the coefficients between a pair of points using (3), applying those coefficients gradually from the initial point will trace out a path between the two data points that remains on near manifold to allow natural interpolation between data points. Classic techniques based on data embeddings are hindered in their ability to interpolate between points because they are limited to simple interpolation techniques between existing data points in the embedding space. Techniques that learn global manifold mappings (i.e., LSML and Non-Local Manifold Tangent Learning) can similarly create paths between points by using the mapped tangent planes and a snake active contour method (see details in (Dolla´r et al., 2007b)).
We propose that this generative capability of transport operators can be used effectively for transfer learning on manifold models. Specifically, we can apply transport operators controlled by randomly drawn coefficients to individual anchor points to generate data points that are consistent with the model. With a single example data point from the target distribution, we can use small magnitude coefficients to explore the data space with a model consistent with the training dataset on another portion of the manifold. With no examples from the target area of the manifold, we can use larger magnitude coefficients to map out unseen portions of the space with paths that are consistent with the observed manifold geometry. In each case, to be successfully used for transfer learning, the manifold transformations must remain a faithful representations of the manifold shape outside the original training domain. We demonstrate these capabilities on the swiss roll manifold (where we have ground truth) and a classification test on the USPS handwritten digits with impoverished training data.
4.1 SWISS ROLL
We first demonstrate transfer learning on a swiss roll manifold by training transport operators on one limited portion of the swiss roll that does not represent the behavior of the manifold as a whole.
5

Under review as a conference paper at ICLR 2018

(a) Transfer learning scenario

(b) Manifold extrapolation

Figure 4: (a) The dark gray shaded area shows the training domain and the blue shaded area shows the testing domain. (b) The transport operators trained using only the black points in the gray shaded region and are applied with random sets of coefficients to the black 'x' points at the edge of the training domain to extrapolate to points outward.

Specifically, the training set does not represent the range of tangent plane orientations of the full model, and it does not extend the full width of the transverse directions present. Figure 4a shows the training domain in dark gray. The testing region is shown in blue, and it has no overlap with the training region. As described earlier, we can use the generative properties of the transport operator model to transfer our learned manifold knowledge by extrapolating from the training region to the testing region. Specifically, we extrapolate to new data points outside the training region by using datapoints on the edge of the training region as the starting point and applying the learned transport operators with random coefficients sampled from a uniform distribution. Repeating that process many times, Fig. 4b shows how we can transfer the learned knowledge to map out an approximation of the new portion of the swiss roll manifold. In this demonstration, each colored dot is the result of a random extrapolation from a datapoint on the edge of the training region using a single randomized set of coefficients.
The other techniques that learn global manifold mappings (i.e., LSML, Non-Local Manifold Tangent Learning) can extrapolate by computing local tangent directions at each point and projecting from known points along those directions. Because of the linearity assumption inherent in tangent plane approximations, there are limits to the size of the extrapolations that are likely before departing from the manifold. We demonstrate this phenomenon by testing the ability to estimate paths between points outside of the training domain. Using the same training/testing regions as before, we generate two points from the testing region (where no training data has been seen) and use the various manifold representations to infer a path on the manifold between the two points. Fig 5 shows example paths between points selected from the testing domain. In this case, the transport operator path is the only one that does not depart significantly from the manifold. Because the LSML and Non-Local Manifold Tangent Learning paths are estimated by recomputing tangents along the route to ensure the path remains on the manifold, the path offset suggests an error in the tangent mappings in this area due to the differences between the training and testing regions.
To quantify the error in manifold representation when transferring from the training region to the testing region, as above we generate point pairs and use each method to find paths between the points that are near the manifold. We compute a path offset metric that indicates how far an estimated path departs from the true manifold by first calculating the maximum distance between any point on the path and its closest corresponding point on a densely sampled manifold. The manifold points are defined using a uniformly spaced grid over a swiss roll with a distance of 0.4 between neighboring points. The distance metric is the mean value of the maximum offsets for all paths in the test set. We evaluate each algorithm using point pairs drawn from the training region (Fig 6a) and in the transfer learning scenario where the point pairs are drawn from the testing region (Fig 6b). When no transfer is required, all algorithms are able to generate paths with small maximum distances to the manifold. While overall performance decreases for all algorithms in the transfer learning case, the transport operator approach is consistently able to produce paths with smaller deviations from the manifold. In the transfer setting, we explore the performance on this task in more detail by dividing the test cases up into distance categories based on the ground truth path length. Fig 6c shows the performance as a function of distance category when the methods are trained with 300
6

Under review as a conference paper at ICLR 2018

Figure 5: Example estimates of paths on the manifold between points from the testing set in a transfer learning setting. The transport operator path follows the manifold much more closely than path estimates from the other algorithms. Ground truth is the geodesic path between the points.

Mean Maximum Distance From Manifold Mean Maximum Distance From Manifold Mean Maximum Distance from Manifold

4 3.5
3 2.5
2 1.5
1 0.5
0 0

No Transfer

TransOpt LSML NLMT Isomap

50 100 150 200 250 300 350 400 Number of Training Points

(a)

4 3.5
3 2.5
2 1.5
1 0.5
0 0

Transfer

TransOpt LSML NLMT Isomap

50 100 150 200 250 300 350 400 Number of Training Points

(b)

Distance from Manifold 6
TransOpt LSML 5 NLMT Isomap 4
3
2
1
0 0 5 10 15 20 25 30 35 40 Category Mean Distance
(c)

Figure 6: (a) Path deviation from the manifold as a function of number of training points when the point pairs are chosen from the training domain. (b) Path deviation from the manifold as a function of number of training points in a transfer setting when point pairs are selected from the testing domain which is distinct from the training domain. (c) Path deviation in the transfer learning task as a function of true path distance.

points, illustrating that the transport operator approach is performing notably better for longer path lengths.
4.2 USPS DIGIT DATA
While the swiss roll dataset is useful for visualizing the properties of learned manifold representations, we also utilize the USPS handwritten digit image dataset (Hull, 1994) to demonstrate our ability to use transport operator representations to perform transfer learning in a few-shot classification task that is simple enough to explore in detail. For training, we create a dataset that consists of 1000 examples of the digit `8' paired with that same image rotated 2.5 and transport operators are learned between those point pairs.1 We define the neighborhood graph with one nearest neighbor and all techniques are parametrized to learn a one-dimensional manifold (i.e. M = 0 and  = 0). In other words, without telling the algorithms about the concept of rotation, we seek to have them learn the general transformation manifold from examples of only slightly rotated `8' digits. This task extends the transfer learning example described in (Bengio & Monperrus, 2005) where they trained the Non-Local Manifold Tangent Learning mapping on all numerical digits and tested on a letter. In this case, we are training only on a single digit, providing less information for learning and increasing the chance of overfitting to the training class.
To demonstrate the performance of within-manifold transfer learning on USPS digit data, we select an initial example of an `8' digit from the test set and transform it using LSML and transport operators. With LSML, we compute the mapping from the input digit to the tangent vector in a onedimensional manifold space and transform the digit by taking steps in the tangent direction. With
1The images are padded to 18 × 18 to allow rotation and cropping without losing digit information.
7

Under review as a conference paper at ICLR 2018

Original

Rotated -10 degrees Transport Operator

LSML

Rotated - 20 degrees Transport Operator

LSML

Original

Transport Operator

LSML

(a) (b)
Figure 7: (a) The manifold techniques that are trained on rotated `8' digits are used to artificially rotate an `8'. The image on the left is the original `8'. The top row displays a small rotation using the true rotation, a transport operator, and an LSML tangent direction. The bottom row displays a rotation almost an order of magnitude larger than the training rotation. (b) An example of manifold transformations trained on `8' digits being applied to the letter `M'. The transport operator effectively rotates the `M' up to 45 while the LSML tangent transforms the `M' to look more like an `8' from the training data.
the transport operators, we apply the learned transport operator to the input digit to move in a path along the learned manifold. As shown in Fig. 7a, the transport operator approach can rotate the digit by larger rotational angles than LSML while retaining much of the original shape and introducing less distortion. Note that a 20 rotation is almost an order of magnitude larger than the 2.5 rotations that were used in the training dataset.
To highlight the performance when information is transferred between manifolds, we apply the transformation learned on rotated `8' digits to the letter `M'. Fig. 7b shows the original `M' as well as the result after applying the learned transformation from the transport operator approach and LSML. Despite being trained only on slightly rotated `8' digits, the transport operator can rotate the `M' by nearly 45 while maintaining the shape of the letter and without inducing much distortion. When the LSML tangent is applied to an `M', it transforms the M to look more like an `8'. This result indicates that the tangent mapping has learned a transformation that is more specific to a rotated `8' digit and it cannot be easily applied in this transfer setting to other inputs.
One application of transfer learning is data augmentation for few-shot learning classification tasks. In this approach, an impoverished training set from one class is bolstered by creating surrogate training data by transfer learning from another class with more abundant training data. In other words, the desire is to perform few-shot learning on new classes by transferring the variability learned from other classes into an augmented dataset for the new class. We test this data augmentation technique on rotated USPS digits classified by a convolutional neural network classifier (LeCun et al., 1998).2 In this demonstration we used four versions of the classifier that were tested using different training sets: 1) only the original USPS digits with no rotation introduced (naive classification), 2) the original USPS digits each rotated by 3, 6, 10, and 30 degrees (oracle classification), 3) the USPS digits transformed many times by using random coefficients with the transport operator that was learned only on rotated `8' digits, and 4) the USPS digits transformed by the LSML tangent vector learned only on rotated `8' digits.
After training, we test the classification accuracy on USPS digits rotated by varying angles and the resulting accuracies are shown in Fig 8. As expected, the network trained with the rotated digits (oracle classification) has seen training data that matches the testing data and performance is not significantly affected by a change in the rotation angle of the test digit. Also as expected, the network trained only on the original USPS data with no rotation (naive classification) experiences a significant performance decrease with larger rotations because that test data is increasingly different from the training data. The network trained using an augmented dataset from transfer learning with transport operators improves the performance significantly over naive classification. However, due to the distortions present with large transformations using tangent space approximations, the network trained using an augmented dataset from transfer learning with LSML achieves no performance
2We employed a LeNet convolutional network with 5 × 5 filters in the first and second convolutional layers.
8

Percent Classification Accuracy

Under review as a conference paper at ICLR 2018
Classification Accuracy vs Rotation Angle 1
0.8
0.6
0.4 Original
0.2 True Rotation Transport Operator LSML
0 0 5 10 15 20 25 30 Test Rotation Angle
Figure 8: Convolutional neural network classifiers were tested on rotated USPS digits. The classifiers were trained on USPS digits in four variations: the original data, data trained with true rotations of 3, 6, 10, 30, and data augmented through transfer learning using transport operators and LSML manifold approximations based on only seeing rotated `8' digits. The data augmentation approach based on transfer learning from transport operators achieves significant performance gains compared to the original impoverished dataset. This demonstrates the ability of transport operators to be used as a manifold representation for few-shot learning.
increase (and may even see a performance deficit) compared to only using the original USPS dataset with no rotation.
5 CONCLUSION
We have shown that manifold transport operators find an accurate and robust characterization of manifold data in the context of a probabilistic generative model. First, we have demonstrated that the implied probabilistic model can be utilized to provide benefits for typical manifold learning tasks (finding an embedding, path estimation) through increased robustness and quantitative quality evaluation for individual point pairs on the manifold. Second, we have demonstrated that the learned manifold representation captured in the transport operators can be transferred accurately to other portions of the manifold (including out-of-sample extensions) through applying the generative model with randomized coefficients. The transfer learning potential was shown in the context of an augmented data application to few-shot learning on digit classification. These results constitute some of the first demonstrations that transport operators can form the basis of a learned representation for manifold data that has utility in applications and transfer learning tasks. The presented simulations are proof-of-concept simulations that allow us to fully explore and visualize the results in a way that is impossible with larger or more complex datasets.
While successful in these demonstrations, as with any algorithm the results depend on the data characteristics being sufficiently captured by the model family. Though there is evidence in the literature that Lie group operators can capture complex transformations in images (Culpepper & Olshausen, 2009), it is unknown if the transport operator approach will be sufficient to represent the rich variations in more complex datasets. Future work will be required to determine the complexity of manifold models that can be captured successfully by the proposed manifold transport operator approach, including demonstrations of similar tasks on more complex image classification tasks. Additionally, the current model only captures local transformations between points, but it will likely be beneficial to develop the model further to capture more regularity in the coefficient behavior across multiple pairs of points.
ACKNOWLEDGMENTS
This work was partially supported by NSF Graduate Research Fellowship grant number DGE1148903, NSF grant number 2106CYZ, NSF CAREER grant CCF-1350954, and James S. McDonnell Foundation grant number 220020399.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373­1396, 2003.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151­175, 2010.
Yoshua Bengio and Martin Monperrus. Non-local manifold tangent learning. Advances in Neural Information Processing Systems, 17:129­136, 2005.
Yoshua Bengio, Jean-Franc¸ois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering. Advances in neural information processing systems, 16:177­184, 2004.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
Benjamin J Culpepper and Bruno A Olshausen. Learning transport operators for image manifolds. In NIPS, pp. 423­431, 2009.
Piotr Dolla´r, Vincent Rabaud, and Serge Belongie. Learning to traverse image manifolds. Advances in neural information processing systems, 19:361, 2007a.
Piotr Dolla´r, Vincent Rabaud, and Serge Belongie. Non-isometric manifold learning: Analysis and an algorithm. In Proceedings of the 24th international conference on Machine learning, pp. 241­ 248. ACM, 2007b.
Eric Eaton, Terran Lane, et al. Modeling transfer relationships between learning tasks for improved inductive transfer. In Machine Learning and Knowledge Discovery in Databases, pp. 317­332. Springer, 2008.
Oren Freifeld, Soren Hauberg, and Michael Black. Model transport: Towards scalable transfer learning on manifolds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1378­1385, 2014.
Oren Freifeld, Søren Hauberg, Kayhan Batmanghelich, and John W Fisher III. Highly-expressive spaces of well-behaved transformations: Keeping it simple. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2911­2919, 2015.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 2066­2073. IEEE, 2012.
Søren Hauberg, Oren Freifeld, Anders Boesen Lindbo Larsen, John W Fisher III, and Lars Kai Hansen. Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation. arXiv preprint arXiv:1510.02795, 2015.
Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Scho¨lkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pp. 601­608, 2006.
Jonathan J Hull. A database for handwritten text recognition research. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 16(5):550­554, 1994.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Xu Miao and Rajesh PN Rao. Learning the lie groups of visual invariance. Neural computation, 19 (10):2665­2693, 2007.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345­1359, 2010.
10

Under review as a conference paper at ICLR 2018
Mijung Park, Wittawat Jitkrittum, Ahmad Qamar, Zolta´n Szabo´, Lars Buesing, and Maneesh Sahani. Bayesian manifold learning: The locally linear latent variable model (ll-lvm). In Advances in Neural Information Processing Systems, pp. 154­162, 2015.
Rajat Raina, Andrew Y Ng, and Daphne Koller. Constructing informative priors using transfer learning. In Proceedings of the 23rd international conference on Machine learning, pp. 713­720. ACM, 2006.
Rajesh PN Rao and Daniel L Ruderman. Learning lie groups for invariant visual perception. Advances in neural information processing systems, pp. 810­816, 1999.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323­2326, 2000.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative transfer learning with tree-based priors. In Advances in Neural Information Processing Systems, pp. 2094­2102, 2013.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319­2323, 2000.
Luc Van Gool, Theo Moons, Eric Pauwels, and Andre´ Oosterlinck. Vision and Lie's approach to invariance. Image and vision computing, 13(4):259­277, 1995.
Xuezhi Wang and Jeff Schneider. Flexible transfer learning under support and model shift. In Advances in Neural Information Processing Systems, pp. 1898­1906, 2014.
Donglai Wei, Dahua Lin, and John Fisher III. Learning deformations with parallel transport. In Computer Vision­ECCV 2012, pp. 287­300. Springer, 2012.
Kilian Q Weinberger and Lawrence K Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In AAAI, volume 6, pp. 1683­1686, 2006.
11

