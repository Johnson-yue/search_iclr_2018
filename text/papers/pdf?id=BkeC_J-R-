Under review as a conference paper at ICLR 2018
COMBINATION OF SUPERVISED AND REINFORCEMENT LEARNING FOR VISION-BASED AUTONOMOUS CON-
TROL
Anonymous authors Paper under double-blind review
ABSTRACT
Reinforcement learning methods have recently achieved impressive results on a wide range of control problems. However, especially with complex inputs, they still require an extensive amount of training data in order to converge to a meaningful solution. This limitation largely prohibits their usage for complex input spaces such as video signals, and it is still impossible to use it for a number of complex problems in a real world environments, including many of those for video based control. Supervised learning, on the contrary, is capable of learning on a relatively small number of samples, however it does not take into account rewardbased control policies andis not capable to provide independent control policies. In this article we propose a model-free control method, which uses a combination of reinforcement and supervised learning for autonomous control and paves the way towards policy based control in real world environments. We use SpeedDreams/TORCS video game to demonstrate that our approach requires much less samples (hundreds of thousands against millions or tens of millions) comparing to the state-of-the-art reinforcement learning techniques on similar data, and at the same time overcomes both supervised and reinforcement learning approaches in terms of quality.
1 INTRODUCTION
Recently proposed reinforcement learning methods (Mnih et al. (2013), Kober & Peters (2012), Lillicrap et al. (2015)) allowed to achieve significant improvements in solving complex control problems such as vision-based control. They can define control policies from scratch, without mimicking any other control signals. Instead, reward functions are used to formulate criteria for control behaviour optimisation. However, the problem with these methods is that in order to explore the control space and find the appropriate actions, even the most recent state-of-the-art methods require several tens of millions of steps before their convergence (Mnih et al. (2013), Mnih et al. (2016)).
Recently proposed supervised methods for continious control problems, like Welschehold et al. (2016), are known to rapidly converge, however they do not provide independent policies. Contrary to many classification or regression problems, where there exists a definitive labelling, as it is presented in Szegedy et al. (2017), Krizhevsky et al. (2012), for many control problems an infinitely large number of control policies may be appropriate. In autonomous driving problems, for example, there is no single correct steering angle for every moment of time as it is appropriate to follow different trajectories; however such control sequences can be assessed by their average speeds, or time of the lap, or other criteria appropriate for some reason. The situation is the same for other control problems connected with robotics, including walking (Li & Wen (2017)) and balancing (Esmaeili et al. (2017)) robots, as well as in many others (Heess et al. (2017)). In these problems, also usually exist some criteria for assessment (for example, time spent to pass the challenge), which would help to assess how desirable these control actions are.
The problem becomes even more challenging if the results are dependent on the sequence of previous observations (Heess et al. (2015)), e.g. because of dynamic nature of the problem involving speed or acceleration, or the difference between the current and the previous control signal.
1

Under review as a conference paper at ICLR 2018

Annotated Dataset

Supervised Pre-training

Initial Model Environment

Rewards Control Signals

Reinforcement&Supervised Learning

Finetuned Model
Figure 1: The overall scheme of the proposed method
In many real-world problems, it is possible to combine the reinforcement and the supervised learning. For the problem of autonomous driving, it is often possible to provide parallel signals of the autopilot in order to use this information to restrict the reinforcement learning solutions towards the sensible subsets of control actions. Similar things can also be done for robotic control. Such real world models can be analytical, or trained by machine learning techniques, and may use some other sensors, which are capable to provide alternative information (e.g., the model trained on LiDAR data can be used to train the vision based model). However, although there were some works using partially labelled datasets within the reinforcement learning framework (Hester et al. (2017)), as far as we believe, the proposed problem statement, combining both the approaches, supervised and reinforcement learning, has never been published before.
The novelty of the approach, presented in this paper, is given as follows:
1. the novel problem statement, combining reinforcement and supervised learning, is proposed;
2. the training algorithm for the control method is proposed, based on this problem statement, and assessed on the control problems;
3. the novel greedy actor-critic reinforcement learning algorithm is proposed as a part of the training algorithm, containing interlaced data collection, critic and actor update stages.
The proposed method reduces the number of samples from millions or tens of millions, required to train the reinforcement learning model on visual data, to just hundreds of thousands, and also improves the quality against the supervised and reinforcement learning.
2 PROPOSED METHOD
The overall idea of the method is shown in figure 1 and can be described as follows: to perform an initial approximation by supervised learning and then, using both explicit labels and rewards, to fine-tune it.
For supervised pre-training, the annotated dataset should contain recorded examples of control by some existing model. The aim of this stage is to mimic the existing control methods in order to avoid nonsense control behaviour of the trained model.
For supervised and reinforcement learning based fine-tuning, a pretrained model is used as an initial approximation. Contrary to the standard reinforcement learning approaches, the pre-trained model helps it to avoid nonsense control values right from the beginning. Also, for the control it is assumed that there is access to labels, which helps to divert the reinforcement learning model from spending its time on exploring those combinations of inputs and control signals, which are most likely not to provide meaningful solutions.
2.1 SUPERVISED PRETRAINING
Hereinafter we consider the following problem statement for supervised pretraining. Let Z be the space of all possible input signals. To simplify the formalisation, we restrict ourselves to the image
2

Under review as a conference paper at ICLR 2018

signals z  Z = Rm×n, where m and n are the height and the width of the image, respectively. Such signals can form sequences of finite length l > 0 : z1, z2, . . . , zl  Zl. We define an operator from the continuous input subsequences to d-dimensional control signals a(zi, zi-1, . . . , zi-p+1|a) :
Zp  Rd, where i  [p, l], p  N is the number of frames used to produce the control signal,
and a are the parameters of the operator a(·). We denote ci = a(zi, . . . , zi-p+1|a), xi =
(zi, zi-1, . . . , zi-p+1).

The problem is stated as follows: given the set of N sequences

z^j  Zlj

N j=1

and

the

set

of

corre-

sponding signal sequences

c^ij  Rd

lj i=1

N
, produced by some external control method called
j=1

a 'reference actor', find the parameters a of the actor a(·|a), which minimise a loss function

(defined in section 3.1).

2.2 LABEL-ASSISTED REINFORCEMENT LEARNING

The reinforcement learning method is inspired by the DDPG algorithm (Lillicrap et al. (2015)), however, it is substantially reworked in order to meet the needs of the proposed combination of supervised and reinforcement learning. First, the problem statement and basic definitions, necessary for formalisation of the method, need to be given.

In line with the standard terminology for reinforcement learning, we refer to a model, generating control signals, as an agent, and to control signals as actions. Also, as it is usually done for reinforcement learning problem statements, we assume the states to be equivalent to the observations. Initially, the model receives an initial state of the environment x1. Then it applies the action c1, which results in transition into the state x2. The procedure repeats recurrently, resulting in sequences of states Xn = x1, x2, . . . , xn and actions Cn = c1, c2, . . . , cn . Every time the agents performs an action it receives a reward r(xi, ci)  R (Lillicrap et al. (2015)).
The emitted actions are defined by the policy , mapping states into actions. In general (Silver et al. (2014)), it can be stochastic, defining a probability distribution over the action space, however here the deterministic policy is assumed. For such a deterministic policy, one can express the discounted future reward using the following form of the Bellman equation for the expectation of discounted future reward Q(·, ·) (Lillicrap et al. (2015)):

Q(xi, ci) = Exi+1 [r(xi, ci) + Q(xi+1, a(xi+1))] ,

(1)

where a(·) is the operator from states to actions,   [0, 1] is a discount factor.

Q(·, ·) is approximated by the critic neural network in line with the actor-critic approach. Similarly to Lillicrap et al. (2015), the proposed method uses a replay buffer to provide a training set.

In many of the state-of-the-art works on reinforcement learning, the actor's parameters are trained by maximisation of the critic's approximation of Q(·, ·) , using gradient descent with gradients,
obtained using the chain rule (Lillicrap et al. (2015), Silver et al. (2014)):

a a(x|a)  Ea[a(x)Q(x, a(x)|Q)a a(x|a)],

(2)

where Q and a are the trainable parameters of the actor and the critic, respectively. In Lillicrap et al. (2015), Silver et al. (2014), the greedy optimisation algorithm is not used because of the performance issues; therefore, it might be impossible to collect diverse training sample for the critic as the actor would not be able to advance through the task. Instead, small steps in the gradient direction are applied to slightly modify the policy.

In the proposed method, contrary to the state-of-the-art works, the optimisation is carried out in a greedy way, so that the steps of testing the current policy, updating the critic and the actor are interlaced. In order to avoid deterioration of performance in the original model (which would essentially lead to the number of steps comparable with the state-of-the-art reinforcement learning models), the regularisation is used to bring the parameters closer towards some pre-defined (reference) policy.

In the following derivations we use a restriction that Q(·, ·)  0, which, as one can easily see from Equation (1), will be true if the rewards are non-negative. We also assume (due to the practical reasons, it doesn't affect further derivations) that the control signal is bounded between values (t1, t2),

3

Under review as a conference paper at ICLR 2018

and t1 and t2 are not included into the appropriate operational values. Based on these practical assumptions, the training sample for the critic is augmented with the values Q(x, t1) = Q(x, t2) = 0 for every value x.

The optimisation problem is based on the regularised Q-function f (x, a(x|a)) :

F (a) = f (x, a(x|a)|Q)  max,

xX

a

(3)

f (x, a(x|a)|Q) = Q(x, a(x|a)|Q) - Q(x, a^(x|a^)|Q) × (a(x|a), a^(x|a^)), (4)
where   0 is some coefficient, a^(·) is the reference actor, a^ are its parameters (which are neither accessible nor adjustable by the proposed method), and (·, ·) is a differentiable distance-like function. One can easily see that in the case  = 0 it completely coincides with the reinforcement problem, and with    the problem becomes a standard supervised training one. The weight Q(x, a^(x|a^) is given to encourage the actor to follow the reference policy if the expected reward is high, and not to encourage much otherwise.

After differentiating this expression with respect to a one can see that

F (a) = a a(x|a)×
xX

×

aQ(x,

a(x|a)|Q)

-

Q(x,

a^(x|a^

)|Q)



(a(x|a), a

a^(x|a^ ))

.

(5)

As in Lillicrap et al. (2015), the update of the critic is carried out by solving the following optimisa-

tion problem:

2

Q^(xi, ci|Q) - r(xi, ci) - Q^(xi+1, a(xi+1|a)|Q)  min,

(xi,ci)(X^ ,C^)

Q

(6)

where X^ and C^ are taken from the replay buffer, Q^ is the discounted reward function approxi-

mation. One can see that this equation is recurrent, and therefore the current target for training is

dependent on the values of the previous training stage.

The training procedure is carried out in the way described in Algorithm 1. In this algorithm, NUM EPOCHS denotes the maximum number of epochs during the training procedure.

Algorithm 1 The description of the training algorithm
Initialise and train the parameters a for the actor a(x|a) on a supervised dataset with observations and labels {XS, CS} Initialise the parameters of the critic Q, k = 0 Initialise the empty replay buffer B
while k < NUM EPOCHS do
Perform N TESTING EPISODES testing episodes of the length L TESTING EPISODES with the current parameters a, where the states xi, actions ci, reference actor actions cGi T , rewards ri = r(xi, ci) and subsequent states xi+1 are collected and put into the replay buffer B Perform N GD UPDATE CRITIC iterations of gradient descent, according to the optimisa-
tion problem (6), in order to update the parameters of the critic Q; values xi, xi+1, ci, ri are taken from the replay buffer B Perform N GD UPDATE ACTOR iterations of gradient descent for the optimisation problem (3) in order to update the parameters of the actor a; values xi, ciGT = a^(xi|a^) are taken from the replay buffer B k = k+1
end while

As it is seen from the algorithm, in a contrast to Lillicrap et al. (2015), the proposed model detaches the procedures of filling the replay buffer, training critic and actor into three subsequent steps, making the algorithm greedy rather than making steps every time control happens. This was done to assess the performance of the same model parameterisation during the epoch, as well as to exclude the problem when the control signal frequency (and hence performance) is affected by the optimisation time.

4

Under review as a conference paper at ICLR 2018

Image(i-1) Feature Extractor
Features(i-1)
FC1
FC2
FC3

Image(i) Feature Extractor

Features(i-1)

Features(i)

Features(i) Control Signal(i-1) FC1

FC2

FC3

Control Signal(i)

Control Signal(i)

Q_Function

Figure 2: The scheme of the actor (left) and critic (right) networks

3 RESULTS
To demonstrate the ability of the method to learn control signals, TORCS (Espi et al.)/ SpeedDreams (SpeedDreams) driving simulator was used. As a reference actor and for the baseline, the Simplix bot, which is a part of the standard package, was used. The outputs of the bot steering angles were restricted between -0.25 and 0.25. The car name was Lynx 220. In the case if the car has stuck (average speed is less than one unit as measured by the simulator), the recovery procedure is provided by the bot. The time and rewards for the recovery are excluded from consideration. The reward is defined as the current car speed measured by the simulator. The assessment has been carried out on a single computer, using NVIDIA GeForce GTX 980 Ti graphical processor on Ubuntu Linux operating system. The model parameters are described in the Appendix.

3.1 NETWORK ARCHITECTURE

The network architecture, used for the proposed actor-critic approach, is shown in Figure 2. For supervised pretraining, the dataset has been collected from the SpeedDreams simulator, combining the sequences of images and the corresponding control values produced by the bot. Using this dataset, the InceptionV1 network (Szegedy et al. (2015)) is first fine-tuned from the Tensorflow Slim implementation (Silberman & Guadarrama (2016)) on the collected dataset, mapping each single image directly into the control signals. The last (classification) layer of the network is replaced by a fully connected layer of the same size (1001) as the preceding logits layer; this layer is referred to as a feature extraction layer as it serves as an input space for the actor-critic model. Such an approach is used in order to avoid expensive storage of images in the replay buffer, as well as circumvent the challenge of training deep architectures within the reinforcement learning setting. Each of the actor and critic layers except the last contains ReLU nonlinearity (Nair & Hinton (2010)). The actor's last layer with dimensionality d of the control signal is followed by the tanh nonlinearity, which restricts the output between the boundary values (t1, t2) = (-1, 1); the last layer of the critic has no nonlinearities.
For supervised finetuning and pretraining stages, we use the following loss function:

|a(x) - c|

l(X, C) =

,

|c| +

xX,cC

(7)

where is a reasonably small constant (with respect to c, we use = 10-2), a(x) is an operator, transforming input vectors x to the control signals, c is the reference actor control signal. This was done because some of the control signals, supplied by the bot, were large enough to devaluate the impact of the smaller values within the loss.

5

Under review as a conference paper at ICLR 2018

Similarly, the distance-like function  in formula (4) is defined as:

(a - a^)T (a - a^)

(a, a^) =

|a^|2 +

,

(8)

where is a reasonably small constant (we use = 10-4).

To implement the formalisation, described in section 2.1, we use a syamese network architecture (see Bertinetto et al. (2016, October)), given in Figure 2. The features, calculated by the fine-tuned network, are submitted for the current and the previous frame (p = 2 in terminology of section 2.1). It is done in order to take into account the dynamic state of the environment, including speed. Also we need to mention that the previous control signal is submitted as an input for the critic (for the initial video frame in each sequence, we assume that the frames and control signals are the same for the current and the previous frames).

3.2 EXPERIMENTS
Table 2 and Figure 3 contain the comparison of the results given different values of the parameter . When  = 0, we rely solely on reinforcement learning; the higher is the parameter , the more restriction is put to stick to the supervised labels rather than improving the policy by reinforcement learning.
In Figure 3, the scatter points depict total rewards during each of the testing episodes, and the curves are depicting the mean total rewards for each epoch over all testing episode rewards. The total reward is calculated as an arithmetic sum of the rewards during one training episode. The left figure shows the results for all tested parameters, while the right one shows the comparison between the best performing one and the one with the largest value of  (i.e. the closest to supervised active learning). The shaded area in the right figure shows the standard deviation of the testing episodes performance during the epoch. One can see from the figure that the smaller values of the coefficient  tend to yield worse performance; however, it may be attributed to longer convergence time as it implies more reliance on reinforcement learning exploration. At the same time, the unlimited increase of the coefficient  does not help gaining further performance improvements. Even more, one can see from the right figure that after certain point the curve for  = 100 slowly declines; we suggest that it could be caused by overfitting to the bot values. We also see that for  = 0, when the supervised data is used only during the pre-training stage, the performance is much lower than for the rest of the graphs.
Table 2 shows the total rewards for different values of parameter . The value maxR shows the maximum total reward over one testing episode during all the training time (corresponds to the highest scatter point in Figure 3 for a given parameter ), the value maxR shows the maximum mean total reward, where the mean is calculated for each epoch over all testing episode total rewards (corresponds to the highest point of the curve in Figure 3 for a given parameter ).
In order to compare the rewards shown in these graphs and tables, we have also measured them the Simplix bot in the same conditions (frame per second rate) as the proposed algorithm. The total rewards for the Simplix bot, available in the TORCS simulator, are given in Table 1. The mean value of these rewards is Rbot = 81214.07. The maximum value achieved by the bot is maxRbot = 81931.15. The percentage of the proposed algorithm's rewards with respect to the bot one, in average and for the best performance, is also given in Table 2.
One of the notable things is the dramatic decrease of the number of samples: from millions (Lillicrap et al. (2015)) or tens of millions (Mnih et al. (2013)) measurements for standard reinforcement learning techniques (Lillicrap et al. (2015)) to just hundreds of thousands (15000 per training epoch, several tens of training epochs). For some of the reinforcement learning algorithms, trained on a problem of driving in a simulator, only some realisations were able to finish the task (Lillicrap et al. (2015)), and for those methods, which report solving similar problems by reinforcement learning only, the reported performance constitutes 75 - 90% (Mnih et al. (2016)), while we achieve up to 92.63% of the bot's performance as reported in Table 2.
The graphs in Figures 4 and 5 illustrate the difference between the unregularised Q-function Q(x, a(x|a)|Q) and its regularised counterpart f (x, a(x|a)|Q) (see Equation 4). The images on the top of the figures, recorded during the testing episode of the first epoch (directly after

6

Under review as a conference paper at ICLR 2018

Table 1: Simplix bot rewards

Test episode 1 Test episode 2 Test episode 3 Test episode 4 Test episode 5 Average

81315.25

81779.02

79481.26

81563.64

81931.15

81214.07

Table 2: Maximum sum-of-rewards values for different parameters  (for  = 0 these values are obtained during supervised pretraining)

 maxR

100%

·

maxR maxRbot

100 75511.69 92.16

0.25 75896.42 92.63

0.1 75866.78 92.60

0.05 75379.34 91.97

0.01 67766.42 82.71

0 62724.68 76.56

Epoch
9 17 32 27 20 1

maxR 72180.06 73339.50 73380.70 71670.40 65806.41 58469.25

100% · maxR
Rbot
88.88 90.30 90.35 88.25 81.03 71.99

Epoch
10 14 15 23 20 1

Figure 3: Sum-of-rewards for different parameters 

Figure 4: Regularised and non-regularised Q-function values, high reward,  = 0.25; the epochs' indices are shown on the right axis, and the values of a(·) are shown on the left axis
7

Under review as a conference paper at ICLR 2018
Figure 5: Regularised and non-regularised Q-function values, low reward,  = 0.25; the epochs' indices are shown on the right axis, and the values of a(·) are shown on the left axis
the supervised pretraining), have been used for computation of the Q-values. The graphs below these images show the evolution of the Q-function (and its regularised version f ) across the epochs. One can see that, as expected, the regularised Q-function is steeper than the unregularised one as it penalises the values which are too far away from the reference actor control signal. The difference between Figures 4 and 5 shows the importance of the factor Q(x, a^(x|a^)) in Equation 4: the penalty for not following the control signal is less, if the expected reward is smaller. It makes the algorithm explore values further from the bot's values, if it does not provide high expected reward.
4 CONCLUSION
The proposed method shows dramatic improvement in the number of samples (down to just several hundred thousand) comparing to the reinforcement learning methods, as well as improves the performance comparing to both supervised and reinforcement learning. We believe that such approach, combining reinforcement and supervised learning, could help to succeed in the areas of complex spaces where the state-of-the-art reinforcement learning methods are not working yet, as well as towards practical usage for real world models such as autonomous cars or robots. However, there are still a few limitations of the proposed method. First, it still requires label data during the training stage. We believe that in the future work it should be possible to get rid of label data and rely on the reinforcement learning exploration and pretrained model only. Such decrease of the training data could benefit to the range of practical tasks solvable by the proposed approach.
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese networks for object tracking. European Conference on Computer Vision, Springer International Publishing, 18:850­865, 2016, October.
N. Esmaeili, A. Alfi, and H. Khosravi. Balancing and trajectory tracking of two-wheeled mobile robot using backstepping sliding mode control: Design and experiments. Journal of Intelligent & Robotic Systems, pp. 1­13, 2017.
8

Under review as a conference paper at ICLR 2018
Eric Espi, Christophe Guionneau, and Bernhard Wymann et al. Torcs, the open racing car simulator. http://torcs.sourceforge.net. Accessed in October 2017.
N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver. Memory-based control with recurrent neural networks. arXiv preprint arXiv:1512.04455, 2015.
N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, and D. Silver. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286., 2017.
T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, A. Sendonaris, G. Dulac-Arnold, I. Osband, J. Agapiou, and J.Z. Leibo. Learning from demonstrations for real world reinforcement learning. arXiv preprint arXiv:1704.03732, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
J. Kober and J. Peters. Reinforcement learning in robotics: A survey. Reinforcement Learning, Springer Berlin Heidelberg, pp. 579­610, 2012.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
Kunquan Li and Rui Wen. Robust control of a walking robot system and controller design. Procedia Engineering, 174:947­955, 2017.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
V. Mnih, A.P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. International Conference on Machine Learning, pp. 1928­1937, 2016.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807­814, 2010.
N. Qian. On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1):145151, 1999.
Nathan Silberman and Sergio Guadarrama. Tf-slim: A high level library to define complex models in tensorflow. https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html, 2016. Accessed in October 2017.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 387­395, 2014.
SpeedDreams. an open motorsport sim. http://www.speed-dreams.org. Accessed in October 2017.
C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. AAAI, pp. 4278­4284, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Erhan Dumitru, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1­9, 2015.
Tim Welschehold, Christian Dornhege, and Wolfram Burgard. Learning manipulation actions from human demonstrations. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 3772­3777. IEEE, 2016.
9

Under review as a conference paper at ICLR 2018

Table 3: Parameters of the network

Parameter Critic Training Steps/ Epoch Actor Training Steps/ Epoch Testing episode length Testing episodes per epoch Actor learning rate Critic learning rate Supervised learning rate Supervised momentum Soft update coefficient Discount factor 

Name in Algorithm 1 N GD UPDATE CRITIC N GD UPDATE ACTOR L TESTING EPISODES N TESTING EPISODES
See Equation 1

Value 12500
12500
3000
5 1 · 10-4 1 · 10-5 1 · 10-4
0.9
0.1
0.9

A APPENDIX: PARAMETERISATION OF THE MODEL
The parameterisation for the experiments is given in Table 3; the parameters' verbal description is augmented with the names referencing to Algorithm 1.
For supervised-only pretraining of the actor network, the Momentum algorithm is used (Qian (1999)); for the rest of the stages, the Adam algorithm is used (Kingma & Ba (2014)). The proposed algorithm has been implemented in Python using TensorFlow framework (Abadi et al. (2016)). For the stage of supervised pretraining, in order to improve convergence of the model at the initial stage, the additional soft update coefficient was introduced for exponential smoothing of the parameters of the network during gradient descent optimisation.

10

