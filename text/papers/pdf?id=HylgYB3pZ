Under review as a conference paper at ICLR 2018
LINEARLY CONSTRAINED WEIGHTS: RESOLVING THE VANISHING GRADIENT PROBLEM BY REDUCING ANGLE BIAS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we first identify angle bias, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose linearly constrained weights (LCW) to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.
1 INTRODUCTION
Neural networks with a single hidden layer have been shown to be universal approximators (Hornik et al., 1989; Irie & Miyake, 1988). However, an exponential number of neurons may be necessary to approximate complex functions. A solution to this problem is to use more hidden layers. The representation power of a network increases exponentially with the addition of layers (Telgarsky, 2016; Eldan & Shamir, 2016). A major obstacle in training deep nets, that is, neural networks with many hidden layers, is the vanishing gradient problem. Various techniques have been proposed for training deep nets, such as layer-wise pretraining (Hinton & Salakhutdinov, 2006), rectified linear units (Nair & Hinton, 2010; Jarrett et al., 2009), variance-preserving initialization (Glorot & Bengio, 2010), and normalization layers (Ioffe & Szegedy, 2015; Gu¨lc¸ehre & Bengio, 2016).
In this paper, we first identify the angle bias that arises in the dot product of a nonzero vector and a random vector. The mean of the dot product depends on the angle between the nonzero vector and the mean vector of the random vector. We show that this simple phenomenon is a key cause of the vanishing gradient in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose the use of so-called linearly constrained weights (LCW) to reduce the angle bias in a neural network. LCW is a weight vector subject to the constraint that the sum of its elements is zero. A reparameterization technique is presented to embed the constraints on weight vectors into the structure of a neural network. This enables us to train a neural network with LCW by using optimization solvers for unconstrained problems, such as stochastic gradient descent. Preliminary experiments show that we can train a 100-layered MLP with sigmoid activation functions by reducing the angle bias in the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias in a neural network, although it was originally developed to overcome another problem, that is, the internal covariate shift problem. Preliminary experiments suggest that LCW helps train deep MLPs more efficiently than does batch normalization.
In Section 2, we define angle bias and discuss its relation to the vanishing gradient problem. In Section 3, we propose LCW as an approach to reduce angle bias in a neural network. We also present a reparameterization technique to efficiently train a model with LCW and an initialization method for LCW. In Section 4, we review related work; mainly, we examine existing normalization techniques from the viewpoint of reducing the angle bias. In Section 5, we present empirical results that show that it is possible to efficiently train a 100-layered MLP by reducing the angle bias using LCW. Finally, we conclude with a discussion of future works.
1

Under review as a conference paper at ICLR 2018

row index row index row index

0 20 40 60 80
0 20colu4m0 n i6n0dex80

1 0
1

(a) Random matrix W .

0 20 40 60 80
0 20colu4m0 n i6n0dex80

1 0

(b) Random matrix A.

0 20 40 60 80
0 20colu4m0 n i6n0dex80

10 5 0
5 10

(c) Multiplication W A.

Figure 1: Angle bias causes a horizontal stripe pattern in W A. (Best viewed in color)

2 ANGLE BIAS

We introduce angle bias by using the simple example shown in Figure 1. Figure 1(a) is a heat map representation of matrix W  R100×100, each of whose elements is independently drawn from a uniform random distribution in the range (-1, 1). Matrix A  R100×100 is also generated randomly, and its elements range from 0 to 1, as shown in Figure 1(b). We multiply W and A to obtain the matrix shown in Figure 1(c). Unexpectedly, a horizontal stripe pattern appears in the heat map of W A although both W and A are random matrices. This pattern is attributed to the angle bias that is defined as follows:
Definition 1. P is an m dimensional probability distribution whose expected value is 1m, where   R and 1m is an m dimensional vector whose elements are all one.
Theorem 1. Let a be an m dimensional randomvariable that follows P. Given w  Rm such that w > 0, the expected value of w · a is || m w cos w, where w is the angle between w and 1m.
 Proof. It follows from E(w·a) = w·E(a) = w·(1m) = w 1m cos w = || m w cos w, where E(x) denotes the expected value of random variable x.

Definition 2. From Theorem 1, the expected value of w · a depends on w as long as  = 0. The distribution of w · a is then biased depending on w; this is called angle bias.

In Figure 1, if we denote the i-th row vector of W and the j-th column vector of A by wi and aj, respectively, aj follows P with  = 0.5. The i-th row of W A is biased according to the angle between wi and 1m, because the (i, j)-th element of W A is the dot product of wi and aj. Note that if the random matrix A has both positive and negative elements, W A also shows a stripe pattern as
long as each column vector of A follows P with  = 0.

We can generalize Theorem 1 for any m dimensional distribution P^, instead of P, as follows:

Theorem 2. Let a^ be a random variable that follows an m dimensional probability distribution P^

whose expected value is µ^  Rm. Given w  Rm such that w > 0, it follows that

{

E(w · a^) =

w µ^ cos ^w if µ^ > 0, 0 otherwise,

where ^w is the angle between w and µ^.

Proof. The proof is the same as that of Theorem 1.

Theorem 2 states that the distribution of w · a^ is biased according to ^w unless µ^ = 0.

2.1 ANGLE BIAS IN A MULTILAYER PERCEPTRON

We consider a all layers. The

satcatnivdaatridonMvLePc.toFroirnsilmaypelriclitiys,dtheneontuedmbbyeraolf=ne(uarol1,n.s.m. ,

aislma)ssu.mTehde

to be the same weight vector

in of

2

Under review as a conference paper at ICLR 2018

Figure 2: Activations in each MLP layer. Inputs are randomly sampled from CIFAR-10.

the i-th neuron in layer l is denoted by wil  Rm. The activation of the i-th neuron in layer l + 1 is given by

ali+1 = f (zil+1) ,

(1)

zil+1 = wil+1 · al,

(2)

where f is a nonlinear activation function, and zil+1 denotes the preactivation value. We omit the bias term in Equation (2) for simplicity. We assume that a0 corresponds to the input vector to the
MLP. In Equations (1) and (2), variables al, zil+1, and ail+1 are regarded as random variables whose distributions are determined by the distribution of the input vector and the weight vectors. From
Theorem 2 and Equation (2), if al follows P^ with µ^ > 0, the preactivation zil+1 is biased according to the angle between wil+1 and µ^. The activation ali+1 is then also biased from Equation (1).

2.2 VISUALIZING EFFECT OF ANGLE BIAS USING CIFAR-10 DATASET

We illustrate the effect of angle bias in an MLP by using the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) that includes a set of 32 × 32 color (RGB) images. Each sample in CIFAR-10 is considered an input vector with 32 × 32 × 3 = 3072 real values, in which each variable is scaled into the range [-1, 1]. We consider an MLP with sigmoid activation functions that has 4 hidden layers with m = 128 neurons in each layer. The weights of the MLP are initialized according to
Glorot & Bengio (2010). We randomly took 100 samples from the dataset and input them into the
MLP. Figure 2 shows the activation pattern in each layer on the selected samples. We see stripe
patters in Layers 2, 3, and 4 in Figure 2 that are caused by angle bias. In Layer 4, the activation
value of each neuron is almost constant regardless of the input. In contrast, no stripe pattern appears in Layer 1, because each element of the input vector is scaled into the range [-1, 1] and its mean value is almost zero; this corresponds to the case in which µ^ = 0 in Theorem 2.

We calculated the angle between wil+1 and al for each sample1. Figure 3 shows boxplot summaries of the calculated angles on the first ten neurons in each layer, in which the 1%, 25%, 50%, 75%, and

99% quantiles are displayed as whiskers or boxes. We see that the angle is biased according to the

neurons in Layers with respect to the

2a,n3g,leanbdet4w.eTenhiws iml+e1aannsdthEat(tahle).dot

product

calculation

in

Equation

(2)

is

biased

2.3 RELATION TO VANISHING GRADIENT PROBLEM
Under the effect of angle bias, it is difficult to learn the weight vectors of an MLP for the following two reasons. The first reason is that, as mentioned in the previous section, the activation value for each neuron in Layer 4 in Figure 2 is almost constant regardless of the sample. This means that the output of the MLP does not depend on its input. At the same time, the output does not change if we adjust the weight vectors in Layer 1, that is, the gradients with respect to these weight vectors remain nearly zero; this corresponds to the vanishing gradient problem. The second reason is that the weight vectors in Layer 4 are also difficult to learn even though their gradients do not vanish. Because the input vector to Layer 4, that is, the activation vector in Layer 3, is almost the same for every sample, we have to predict the label of a sample based on a feature vector that is nearly
1The angle is given by arccos ((wil+1 · al) / ( wil+1 al ))

3

Under review as a conference paper at ICLR 2018

Figure 3: Boxplot summaries of the angle between the weight vector and the activation vector. Results for only the first ten neurons in each layer are displayed. Activation vectors are evaluated by using the samples shown in Figure 2.

identical in every sample. In such a situation, the gradients of weight vectors in Layer 4 are less meaningful.
If we use rectified linear activation instead of sigmoid activation, the gradients are less likely to vanish. However, the rate of each neuron being active2 is biased, because the distribution of preactivation zil+1 is biased. If a neuron is always active, it behaves as an identity mapping. If a neuron is always inactive, it is worthless because its output is always zero. As discussed in Balduzzi et al. (2017), the efficiency of the network decreases in this case. In this sense, angle bias may reduce the efficiency of a network with rectified linear activation.

3 LINEARLY CONSTRAINED WEIGHTS

There are two approaches to reduce angle bias in a neural network. The first one is to somehow make

the expected value of the activation of each neuron near zero, because angle bias does not occur

if µ^ (= )0 from Theorem 2. The second one is to somehow regularize the angle between wil+1 and E al . In this section, we propose a method to reduce angle bias in a neural network by using

the latter approach. We introduce WLC as follows:

Definition 3. WLC is a subspace in Rm defined by {}
WLC := w = (w1, . . . , wm)  Rm w1 + . . . + wm = 0 .

(3)

The following theorem holds for w  WLC:
Theorem 3. Let a be an m dimensional random variable that follows P. Given w  WLC such that w > 0, the expected value of w · a is zero.

Proof. E(w · a) = w · E(a) =  (w · 1m) =  (w1 + . . . + wm) = 0.
From Theorem 3, if wil+1  WLC and al follows P, we can resolve angle bias in zil+1 in Equation (2). Then, the distribution of each of zil+1 (i = 1, . . . , m) will likely be more similar to each other. The activation vector in layer l + 1, each of whose elements is given by Equation (1), is then expected to follow P. Therefore, if the input vector a0 follows P, we can inductively reduce the angle bias in each layer of an MLP by using weight vectors that are included in WLC. We call weight vector wil+1 in WLC linearly constrained weights (LCWs).
3.1 VISUALIZING THE EFFECT OF LCW
We built an MLP of the same size as that used in Section 2.2, but whose weight vectors are replaced with LCWs. We applied the minibatch-based initialization described in Section 3.3. Figure 4 shows the activation pattern in each MLP layer with LCW on the randomly selected samples that are used in Figure 2. When compared with Figure 2, we see no stripe pattern in Figure 4. The neurons in Layer 4 respond differently to each input sample; this means that a change in the input leads to a
2A neuron with rectified linear activation is said to be active if its output value is positive.

4

Under review as a conference paper at ICLR 2018

Figure 4: Activations in each MLP layer with LCW. The input samples are the same as those used in Figure 2.

Figure 5: Boxplot summaries of the angle between the weight vector and the activation vector on the first ten neurons in each MLP layer with LCW.

different output. Therefore, the network output changes if we adjust the weight vectors in Layer 1, that is, the gradients in Layer 1 do not vanish in Figure 4.

Figure 5 shows each MLP layer

boxplot summaries with LCW. We see

otfhatthethaenagnleglbeedtwisetreinbuwteils+1araonudndal90on

the first on each

ten neurons in neuron in each

layer. This indicates that al approximately follows P in each layer, and therefore, the angle bias is

resolved in the calculation of zil+1 by using LCW.

3.2 LEARNING LCW VIA REPARAMETERIZATION
A straightforward way to train a neural network with LCW is to solve a constrained optimization problem, in which a loss function is minimized under the condition that each weight vector is included in WLC. Although several methods are available to solve such constrained problems, for example, the gradient projection method (Luenberger & Ye, 2015), it might be less efficient to solve a constrained optimization problem than to solve an unconstrained one. We propose a reparameterization technique that enables us to train a neural network with LCW by using a solver for unconstrained optimization. We can embed the constraints on the weight vectors into the structure of the neural network by reparameterization.

Reparameterization Let wil  Rm be a weight vector in a neural network. We reparametrize wil

by using vector vil  Rm-1 as wil = Bmvil, where Bm  Rm×(m-1) is a basis of WLC, written as a

matrix of column vectors. For example, Bm is given by

()

Bm =

Im-1 -1m-1

 Rm×(m-1),

(4)

where Im-1 is the identity matrix of order (m - 1) × (m - 1).
It is obvious that wil = Bmvil  WLC. We then solve the optimization problem in which vil is considered as a new variable in place of wil. This optimization problem is unconstrained because vil  Rm-1. We can search for wil  WLC by exploring vil  Rm-1. In the experiments in Section 5, we used Bm in Equation (4). We also tried an orthonormal basis of WLC as Bm; however, there was little difference in accuracy. It is worth noting that the proposed reparameterization can be implemented easily and efficiently by using modern frameworks for deep learning using GPUs.

5

Under review as a conference paper at ICLR 2018

3.3 INITIALIZATION USING MINIBATCH STATISTICS

By introducing LCW, we can reduce the angle bias in zil+1 in Equation (2), which mainly affects the expected value of zil+1. It is also important to regularize the variance of zil+1, especially when the sigmoid activation is used, because the output of the activation will likely saturate when the variance

of zil+1 is too large. We apply an initialization method by which the variance of zil+1 is regularized based on a minibatch of samples. This type of initialization has also been used in previous studies

Mishkin & Matas (2016) and Salimans & Kingma (2016).

First,

we

randomly

generate

a

seed

vector

w^il+1



WLC1,

where

WLC1

:=

{ w



WLC

} w 1 .

Section A in the appendix describes the approach for sampling uniformly from WLC1. The scholar

var(iable of l+1

w^lil++11)is·thael neqcualaclusltahteedtairngaetlavyaelur-ewisze,

manner with the property that the standard deviation in which al is evaluated using a minibatch. Finally,

we use l+1w^il+1 as the initial value of wil+1. If we apply the reparameterization method described

in the previous section, the first m - 1 elements of l+1w^il+1 are used as the initial value of vil+1.

4 RELATED WORK
Ioffe & Szegedy (2015) proposed the batch normalization (BN) approach for accelerating the training of deep nets. BN was developed to address the problem of internal covariate shift, that is, training deep nets is difficult because the distribution of the input of a layer changes as the weights of the preceding layers change during training. The computation of the mean and standard deviation of zil+1 based on a minibatch is incorporated into the structure of the network, and zil+1 is normalized by using these statistics. Gu¨lc¸ehre & Bengio (2016) proposed the standardization layer (SL) approach, which is similar to BN. The main difference is that SL normalizes ail+1, whereas BN normalizes zil+1. Interestingly, both BN and SL can be considered mechanisms for reducing the angle bias. SL reduces the angle bias by forcing µ^ = 0 in Theorem 2. On the other hand, BN reduces the angle bias by normalizing zil+1 for each neuron. A drawback of both BN and SL is that the model has to be switched during inference to ensure that its output depends only on the input but not the minibatch. By contrast, LCW proposed in this paper does not require any change in the model during inference. Moreover, the computational overhead for training the LCW is lower than that of BN and SL, because LCW only requires the addition of layers that multiply Bm to vil, as described in Section 3.2, whereas BN and SL need to compute both the mean and the standard deviation of zil+1 or ail+1.
Salimans & Kingma (2016) proposed weight normalization (WN) to overcome the drawbacks of BN, in which a weight vector wil  Rm is reparametrized as wil = (gil/ vil )vil, where gil  R and vil  Rm are new parameters. By definition, WN does not have the property of reducing the angle bias, because the degrees of freedom of wil are unchanged by the reparameterization. Salimans & Kingma (2016) also proposed a minibatch-based initialization by which weight vectors are initialized so that zil+1 has zero mean and unit variance, indicating that the angle bias is reduced immediately after the initialization. The preliminary results presented in Section 5 suggest that starting with weight vectors that do not incur angle bias is not sufficient to train very deep nets. It is important to incorporate a mechanism that reduces the angle bias during training.
Ba et al. (2016) proposed layer normalization (LN) as a variant of BN. LN normalizes zil+1 over the neurons in a layer on a sample in the minibatch, whereas BN normalizes zil+1 over the minibatch on a neuron. From the viewpoint of reducing the angle bias, LN is not as direct as BN. Although LN does not resolve the angle bias, it is expected to normalize the degree of bias in each layer.

5 EXPERIMENTS
We conducted preliminary experiments using the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), as noted in Section 2.2. These experiments are aimed not at achieving state-of-the-art results but at investigating whether we can train a deep model by reducing the angle bias and empirically evaluating the performance of LCW in comparison to that of BN and WN.

6

Under review as a conference paper at ICLR 2018
Network structure We used MLPs with the cross-entropy loss function. Each network has 32 × 32 × 3 = 3072 input neurons and 10 output neurons, and it is followed by a softmax layer. We refer to an MLP that has L hidden layers and M neurons in each hidden layer as MLP(L, M ). Either a sigmoid activation function or a rectified linear activation function was used. MLPLCW denotes an MLP in which each weight vector is replaced by LCW. MLPBN denotes an MLP in which the preactivation of each neuron is normalized by BN. MLPWN denotes an MLP whose weight vectors are reparametrized by WN.
Initialization Plain MLP and MLPBN were initialized using the method proposed in Glorot & Bengio (2010). MLPLCW was initialized using the minibatch-based method described in Section 3.3 with z = 0.5. MLPWN was initialized according to Salimans & Kingma (2016).
Optimization MLPs were trained using a stochastic gradient descent with a minibatch size of 128 for 100 epochs. The learning rate starts from 0.1 and is multiplied by 0.95 after every two epochs.
Environment and implementation The experiments were performed on a system running Ubuntu 16.04 LTS with NVIDIA R Tesla R K80 GPUs. We implemented LCW using PyTorch version 0.1.12. We implemented BN using the torch.nn.BatchNorm1d module in PyTorch. We implemented WN by ourselves using PyTorch3.
5.1 EXPERIMENTAL RESULTS: MLPS WITH SIGMOID ACTIVATION FUNCTIONS
We first consider MLPs with sigmoid activation functions. Figure 6 shows the convergence and computation time for training MLPs with 100 layers and 128 neurons per layer. Figure 6(a) shows that the training accuracy of the plain MLP is 10% throughout the training, because the MLP output is insensible to the input because of the angle bias, as mentioned in Section 2.24. By contrast, MLPLCW or MLPBN is successfully trained, as shown in Figure 6(a), indicating that the angle bias is a crucial obstacle to training deep MLPs with sigmoid activation functions, because both LCW and BN can reduce the angle bias, as described in Section 4. MLPLCW achieves a higher rate of increase in the training accuracy compared to MLPBN. MLPWN and the plain MLP are not trainable because WN itself cannot reduce the angle bias.
Compared to plain MLP, the computational overhead of MLPLCW is approximately 55%, as shown in Figure 6(b); this is much lower than that of MLPBN. The overhead of MLPWN is large compared to that of MLPBN, although it contradicts the claim of Salimans & Kingma (2016). We think this is due to the implementation of these methods. The BN module we used in the experiments consists of a specialized function developed by GPU vendors, whereas the WN module was developed by ourselves. In this sense, the overhead of LCW may be improved by a more sophisticated implementation.
It seems that MLPLCW is less generalized than MLPBN in terms of the test accuracy in Figure 6(c). We have no clear explanation for this deviation, and further studies are needed to investigate the generalizability of neural networks.
Experimental results for MLPs of different sizes are reported in Section B in the appendix.
5.2 EXPERIMENTAL RESULTS: MLPS WITH RECTIFIED LINEAR ACTIVATION FUNCTIONS
We have also experimented with MLPs with rectified linear activation functions. In our experiments, we observed that the plain MLP with 20 layers and 256 neurons per layer was successfully trained. However, the training of MLPLCW of the same size did not proceed at all; in fact, the output values of the network exploded after a few minibatch updates. We have investigated the weight gradients of the plain MLP and MLPLCW. Figure 7 shows boxplot summaries of the weight gradients in each layer of both models, in which the gradients are evaluated by using a minibatch immediately after the initialization. By comparing Figure 7(a) and Figure 7(b), we find an exponential increase in the
3Although a module for WN is available in PyTorch version 0.2.0, we did not use it because the minibatchbased initialization (Salimans & Kingma, 2016) is not implemented for this module.
4CIFAR-10 includes samples from 10 classes equally. The prediction accuracy is therefore 10% if we predict all samples as the same class.
7

Under review as a conference paper at ICLR 2018

(a) Training accuracy.

(b) Elapsed time (s).

(c) Test accuracy.

Figure 6: Comparison of training of MLP(100, 128), MLPLCW(100, 128), MLPBN(100, 128), and MLPWN(100, 128) with sigmoid activation functions.

(a) MLP(20, 256).

(b) MLPLCW(20, 256).

Figure 7: Distributions of weight gradients in each layer of MLP(20, 256) or MLPLCW(20, 256) with rectified linear activation functions.

distributions of the weight gradients of MLPLCW in contrast to the plain MLP. Because the learning rate was the same for every layer in our experiments, this exponential increase of the gradients might hamper the learning of MLPLCW. The gradients in a rectifier network are sums of path-weights over active paths (Balduzzi et al., 2017). The exponential increase of the gradients therefore implies an exponential increase of active paths. As discussed in Section 2.3, we can prevent neurons from being always inactive by reducing the angle bias, which we think caused the exponential increase in active paths. We need further studies to make MLPLCW with rectified linear activation functions trainable. Possible directions are to apply layer-wise learning rates or to somehow regularize the distribution of the weight gradients in each layer of MLPLCW, which we leave as future work.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we have first identified the angle bias that arises in the dot product of a nonzero vector and a random vector. The mean of the dot product depends on the angle between the nonzero vector and the mean vector of the random vector. In a neural network, the preactivation value of a neuron is biased depending on the angle between the weight vector of the neuron and the mean of the activation vector in the previous layer. We have shown that such biases cause a vanishing gradient in a neural network with sigmoid activation functions. To overcome this problem, we have proposed linearly constrained weights to reduce the angle bias in a neural network; these can be learned efficiently by the reparameterization technique. Preliminary experiments suggest that reducing the angle bias is essential to train deep MLPs with sigmoid activation functions.
We have observed that reducing the angle bias causes an unfavorable side effect in the training of MLPs with rectified linear activation functions. We need further studies to make LCW applicable to rectifier networks. Future work also includes investigating the applicability of LCW for other neural network structures, such as convolutional or recurrent structures. The use of LCW in recurrent networks is of particular interest, for which batch normalization is not straightforwardly applicable.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In NIPS 2016 Deep Learning Symposium, 2016.
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In Proceedings of the 34th International Conference on Machine Learning, pp. 342­350, 2017.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 907­940, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
C¸ aglar Gu¨lc¸ehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. Journal of Machine Learning Research, 17(8):1­32, 2016.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504­507, 2006.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In David Blei and Francis Bach (eds.), Proceedings of the 32nd International Conference on Machine Learning, pp. 448­456. JMLR Workshop and Conference Proceedings, 2015.
Bunpei Irie and Sei Miyake. Capabilities of three-layered perceptrons. In IEEE International Conference on Neural Networks, volume 1, pp. 641­648, 1988.
Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th International Conference on Computer Vision, pp. 2146­2153, 2009.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer Publishing Company, Incorporated, 2015.
Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807­814. Omnipress, 2010.
PyTorch. http://pytorch.org/ (accessed: 2017/10/2).
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 901­909, 2016.
Matus Telgarsky. Benefits of depth in neural networks. In 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 1517­1539, 2016.
9

Under review as a conference paper at ICLR 2018

(a) Training accuracy.

(b) Elapsed time (s).

(c) Test accuracy.

Figure 8: Comparison of training of MLPLCW(5, 512), MLPBN(5, 512), MLPWN(5, 512), and MLP(5, 512) with sigmoid activation functions.

(a) Training accuracy.

(b) Elapsed time (s).

(c) Test accuracy.

Figure 9: Comparison of training of MLPLCW(50, 256), MLPBN(50, 256), MLPWN(50, 256), and MLP(50, 256) with sigmoid activation functions.

A SAMPLING UNIFORMLY FROM WLC1

We can generate a sample w that is uniformly distributed in WLC1 as follows:

1. w  N (0, Im): Generate a sample w from m-dimensional normal distribution N (0, Im).

2. w  w - E(w): Subtract the mean of elements of w, which corresponds to the projection of w onto WLC.
3. w  w/ w : Normalize w to a unit vector.

4.   U (0, 1): Generate a sample  from a uniform distribution in (0, 1).

5.

w



1
 m-2 w:

Shrink w

in proportion to the hypersurface area of the m - 1 dimensional

hypersphere of radius .

B ADDITIONAL GRAPHS
The convergence and computation time for training MLP(5, 512) and MLP(50, 256) are shown in Figure 8 and Figure 9, respectively. We see that LCW enables faster convergence with respect to the training accuracy for both models.

10

