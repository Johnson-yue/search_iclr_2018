Under review as a conference paper at ICLR 2018
SENSITIVITY AND GENERALIZATION IN NEURAL NET-
WORKS
Anonymous authors Paper under double-blind review
ABSTRACT
In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with different architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.
We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the input-output Jacobian of the network, and that this correlates well with generalization. We further establish that factors associated with poor generalization ­ such as full-batch training or using random labels ­ correspond to higher sensitivity, while factors associated with good generalization ­ such as data augmentation and ReLU non-linearities ­ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.
1 INTRODUCTION
The empirical success of deep learning eludes interpretation through lenses of computational complexity (Blum & Rivest, 1988), numerical optimization (Choromanska et al., 2015; Goodfellow & Vinyals, 2014; Dauphin et al., 2014) and statistical learning theory (Zhang et al., 2016): neural networks are highly non-convex models with extreme capacity that train fast and generalize well. In fact, not only do large networks demonstrate good test performance, but larger networks often generalize better. This phenomenon has been observed in targeted experiments (Neyshabur et al., 2015), historical trends of Deep Learning competitions (Canziani et al., 2016), and in the course of this work (Figure 1).
This observation is at odds with Occam's razor, the principle of parsimony, as applied to the intuitive notion of function complexity (see §A.1 for extended discussion). To resolve the apparent contradiction we argue that complexity of functions should not be examined separately from the input domain. f (x) = x3 sin(x) may seem decisively more complex than g(x) = x. But restrained to a narrow input domain of [-0.01, 0.01] they appear differently: g remains a linear function of the input, while f (x) = O x4 resembles a constant 0. In this work we find that such intuition applies to neural networks, that behave very differently close to the data manifold than away from it (§4.1).
We therefore analyze the complexity of models through their capacity to distinguish different inputs in the neighborhood of datapoints, or, in other words, their sensitivity. We study two simple metrics presented in §3 and find that one of them, norm of the input-output Jacobian correlates with generalization in a very wide variety of scenarios (§4.2, §4.3), up to being predictive of generalization at individual test points (§4.4).
Naturally robustness to perturbations is not a sufficient condition of generalization. A task of generating random numbers from a seed should be maximally sensitive to changes in the input; modelling chaotic systems may benefit from sensitivity etc. Instead, the purpose of this work is merely to
1

Under review as a conference paper at ICLR 2018 CIFAR10

Generalization gap

0.8 0.8

0.6 0.6

0.4 100k

1M 10M 100M
Number of weights

1B

0.4 1p

1n 1
Train loss

0.001

Figure 1: 2160 networks trained to 100% training accuracy on CIFAR10 (see §A.4.5 for experimental details). Left: while increasing capacity of the model allows for overfitting (top), very few models do, and the model with the most parameters yields the best achieved generalization (bottom right). Right: train loss does not correlate with generalization, and the best model (minimum along the y-axis) has training loss many orders of magnitude higher than models that perform worse (left). This observation rules out underfitting as the reason for poor generalization in low-capacity models. See (Neyshabur et al., 2015) for similar findings in the case of achievable 0 training loss.

present evidence that in certain common scenarios (such as image classification) sensitivity, as measured by the Jacobian norm, can serve as a complexity metric for the purpose of model comparison.
2 RELATED WORK AND SUMMARY OF CONTRIBUTIONS
We analyze complexity of neural networks through (1) estimating the number of linear regions a networks splits the input space into, and (2) measuring the norm of the input-output Jacobian within such regions.
A few prior works have examined measures related to the ones we consider. In particular, Pascanu et al. (2013); Montu´far et al. (2014); Raghu et al. (2016) have investigated the expressive power of fully-connected neural networks built out of piecewise-linear activation functions. Such functions are themselves piecewise-linear over their input domain, so that the number of linear regions into which input space is divided is a measure of how nonlinear the function is. A function with many linear regions has the capacity to build complex, flexible decision boundaries. It was argued in Pascanu et al. (2013); Montu´far et al. (2014) that an upper bound to the number of linear regions scales exponentially with depth but polynomially in width, for specific constructions. Raghu et al. (2016) derived a tight analytic bound, and considered the number of linear regions for generic networks with random weights, as would be appropriate, for instance, at initialization. However, the evolution of this measure after training has not been investigated before. We examine a related measure, the number of hidden unit transitions along one-dimensional trajectories in input space, for trained networks. Further motivation for this measure will be discussed in §3.
Another perspective on function complexity can be gained by studying their robustness to perturbations in input. Indeed, Rasmussen & Ghahramani (2000) demonstrate on a toy problem how complexity as measured by the number of parameters may be of little help for model selection, while measuring the output variation allows for Occam's razor application. In this work we apply related ideas to a large-scale practical context of neural networks with up to a billion free parameters (§4.2, §4.3), and discuss potential reasons for why sensitivity permits to invoke Occam's razor for neural networks (§A.1).
2

Under review as a conference paper at ICLR 2018
In the context of regularization, increasing robustness to perturbations is a widely-used strategy: data augmentation, noise injection (Jiang et al., 2009), weight decay (Krogh & Hertz, 1992), maxpooling, all indirectly reduce sensitivity of the model to perturbations, while Rifai et al. (2011); Sokolic et al. (2017) explicitly penalize the Frobenius norm of the Jacobian in the training objective.
Moreover, sensitivity to inputs has attracted equal amount of interest in the context of adversarial examples. Several attacks locate points of poor generalization in the directions of high sensitivity of the network (Goodfellow et al., 2014; Papernot et al., 2016), while certain defences regularize the model by penalizing sensitivity (Gu & Rigazio, 2014) or employing decaying (hence less sensitive) non-linearities (Kurakin et al., 2016).
In this work we relate several of such regularizing techniques to sensitivity, demonstrating through extensive experiments that improved generalization is always coupled with better robustness as measured by a single metric, the input-output Jacobian norm ( §4.2). While some of these findings confirm common-sense expectations (data augmentation reduces sensitivity, Figure 7), others challenge our intuition of what makes a neural network robust (ReLU-networks, with unbounded activations, are more robust than saturating HardSigmoid-networks, Figure 8).
One such finding demonstrates an inductive bias towards robustness in stochastic mini-batch optimization compared to full-batch training (Figure 9). Interpretation of this regularizing effect in terms of sensitivity is not new (Hochreiter & Schmidhuber, 1997; Keskar et al., 2016), yet we provide a new perspective by relating it to reduced sensitivity to inputs instead of parameters.
The inductive bias of SGD ("implicit regularization") has been previously studied in (Neyshabur et al., 2015), where it was shown through rigorous experiments how increasing the width of a singlehidden-layer network improves generalization, and an analogy with matrix factorization was drawn to motivate constraining the norm of the weights instead of their count. Neyshabur et al. (2017) further explored several weight-norm based measures of complexity that do not scale with the size of the model. One of our measures, the Frobenius norm of the Jacobian is of similar nature (since the Jacobian matrix size is determined by the task and not by a particular network architecture). However, this particular metric was not considered, and, to the best of our knowledge, we are the first to evaluate it in a large-scale setting (e.g. our networks are up to 65 layers deep and up to 216 units wide).
Finally, to the best of our knowledge, we are the first to compare behavior of a trained network on and off the data manifold (§4.1). The contrast between the two (Figure 2) further strengthens the empirical connection we draw between sensitivity and generalization; it also confirms that, as mentioned in §1, if a certain quality of a function is to be used for model comparison, input domain should always be accounted for.
3 SENSITIVITY METRICS
We propose two simple measures of sensitivity for a neural network f : Rd  Rn with respect to its input x  Rd (the output being unnormalized logits of the n classes). Assume f employs a piecewise-linear activation function, like ReLU. Then f itself, as a composition of linear and piecewise-linear functions, is a piecewise-linear map, splitting the input space Rd into disjoint regions, implementing a single linear mapping on each. Then we can measure two aspects of sensitivity by answering
1. How does the output of the network change as the input is perturbed within the linear region?
2. How likely is the linear region to change in response to change in the input?
We quantify these qualities as follows:
1. For a local sensitivity measure we adopt the Frobenius norm of the class probabilities Jacobian J(x) = f (x) /xT (with Jij(x) =  [f (x)]i /xj), where f =   f with  being the softmax function. Given points of interest xtest, we estimate the sensitivity of the function in those regions with the expected Jacobian norm:
Extest [ J (xtest) F ] ,
3

Under review as a conference paper at ICLR 2018

that we will further refer to as simply "Jacobian norm". Note that this does not require the

label of xtest.

Interpretation. Frobenius norm J(x) F =

ij Jij(x)2 estimates the average-case

sensitivity of f around x. Indeed, consider an infinitesimal Gaussian perturbation x  N (0, I): the expected magnitude of the output change is then

Ex

f (x) - f (x + x)

2 2

 Ex

J(x)x

2 2

= Ex

i

2
Jij xj
j

= Jij Jij Ex [xj xj ] = Ji2j Ex x2j

ijj ij

=

J (x)

2 F

2. To detect a change in linear region (further called a "transition"), we need to be able to
identify it. We do this analogously to (Raghu et al., 2016). For a network with piecewise-
linear activations, we can, given an input x, assign a code to each neuron in the network f ,
that identifies the linear region of the pre-activation of that neuron. E.g. each ReLU unit will have 0 or 1 assigned to it if the pre-activation value is less or greater than 0 respectively. Similarly, a ReLU6 unit (see definition in §A.3) will have a code of 0, 1, or 2 assigned, since it has 3 linear regions1. Then, a concatenation of codes of all neurons (denoted by c(x)) in the network uniquely identifies the linear region of the input x2.
Given this encoding scheme, we can detect a transition by detecting a change in the code. We then sample k points on a closed one-dimensional trajectory T (x) in the input space passing through the point x and count transitions t(x) along it to quantify the number of linear regions:

k-1

t(x) :=

c (x i) = c x (i+1)%k

i=0



x T (x)

c(x )

 (dx )

dx ,
1

(1)

where [·] is the Iverson bracket and the directional derivative c(x )/ (dx ) amounts to
the Dirac delta function with support on the boundaries. Since the number of all possible codes c (x) is finite, past a certain k the evaluation of the integral becomes exact3

By sampling multiple such trajectories around different points, we estimate the sensitivity

metric:

Extest [t (xtest)] ,

that we will further refer to as simply "transitions", "number of transitions" etc.

To assure the sampling trajectory is close to the data manifold (since this is the region of
interest), we construct it through horizontal translations of images in pixel space (Figure 2,
right). We similarly augment our training data with horizontal and vertical translations in
the corresponding experiments. As earlier, this metric does not require knowing the label of xtest.
Interpretation. We can draw a qualitative parallel between the number of transitions and curvature of the function. t(x) in Equation 1 amounts to the total 1-norm of the directional derivative of c (x) along T (x), which is 0 everywhere except for the transition boundaries. This property is shared by the derivative of f , a constant input-logits Jacobian Jl within each linear region. Then, computing the total curvature of f along T (x) amounts to

x T (x)

Jl (x )  (dx )

dx
F



1 2

k-1

i=0

Jl x (i+1)%k

- Jl (x i) F ,

1For a non-piecewise-linear activation like Tanh, we consider 0 as the boundary of two regions and find this metric qualitatively similar to counting transitions of a piecewise-linear non-linearity.
2Such encoding guarantees that different regions obtain different codes, but different codes might be assigned to the same region if all the neurons in any layer of the network are saturated (or if weights leading from the transitioning unit to active units are exactly zero, or exactly cancel). However, the probability of such an arrangement drops exponentially with width and hence is ignored in this work.
3Apart from a dense-enough sampling we rely on no two neurons transitioning simultaneously, which is extremely unlikely in the context of random initialization and stochastic optimization.

4

Under review as a conference paper at ICLR 2018
where, as in Equation 1, the equality becomes exact starting from a certain high enough precision k. This sum is similar to t(x) as defined in Equation 1, but quantifies the amount of change in between two linear regions in a non-binary way. However, estimating this integral on a densely sampled trajectory is a computationally-intensive task, which is one reason we instead count transitions.
As such, on a qualitative level, the two metrics (Jacobian norm and number of transitions) track the first and second order terms of the Taylor expansion of the function.
4 EXPERIMENTAL RESULTS
We relate sensitivity and generalization through a sequence of experiments of increasing level of nuance. In §4.1 we begin by comparing the sensitivity of trained neural networks on and off the training data manifold, i.e. in the regions of best and typical (over the whole input space) generalization. In §4.2 we compare sensitivity of identical trained networks that differ in a single parameter which is important for generalization. Further, §4.3 associates sensitivity and generalization in an unrestricted manner, i.e. comparing networks of a wide variety of hyper-parameters such as width, depth, non-linearity, weight initialization, optimizer, learning rate and batch size on CIFAR10 and CIFAR100, with a smaller, yet still substantial experiment confirming a similar trend on MNIST and Fashion-MNIST (Xiao et al., 2017). Finally, §4.4 explores how predictive sensitivity (as measured by the Jacobian norm) is for individual test points.
4.1 SENSITIVITY ON AND OFF THE TRAINING DATA MANIFOLD
We analyze the behavior of a trained neural network near and away from training data. We do this by comparing sensitivity of the function along 3 types of trajectories:
1. A random ellipse. This trajectory is extremely unlikely to pass anywhere near the real data, and indicates how the function behaves in random locations of the input space that it never encountered during training.
2. An ellipse passing through 3 training points of different class (see Figure 2, left). This trajectory does pass through the three data points, but in between it traverses images that are qualitatively similar to linear interpolations between different-class images, and hence lie outside of the natural images space. Sensitivity of the function along this trajectory allows comparison of its behavior on and off the data manifold, as it approaches and moves away from the 3 anchor points.
3. An ellipse through 3 training points of the same class. This trajectory is similar to the previous one, but, given the dataset used in the experiment (MNIST), is expected to traverse overall closer to the data manifold, since linear interpolations of the same digit are more likely to resemble a realistic image. Comparing transition density along this trajectory to the one through points of different classes allows further assesment of how sensitivity changes to approaching the data manifold.
We find that, according to both the Jacobian norm and transitions metrics, functions exhibit much more robust behavior around the training data (see Figure 5). We further visualize this effect in 2D in Figure 4, where we plot the transition boundaries of the last (pre-logit) layer of a neural network before and after training. After training we observe that training points lie in regions of low transition density.
5This depiction is only accurate in the case of a single-layer ReLU network with biases. Otherwise the partition into linear regions is more complex.
5

Under review as a conference paper at ICLR 2018 Interpolation Trajectory

Translation Trajectory

Figure 2: Two types of 1D trajectories in input space considered for transition counting. Boundaries between different linear regions are depicted with dotted lines5. Left: a circular interpolation between three training points results in samples between them lying outside of the data manifold, and as such allows comparison of the metrics of interest close to the data and away from it (see §4.1 for results and §A.4.2 for experimental details). Right: an interpolation between 28 horizontal translations of a single digits constrains all trajectory points to lie close to the translation-augmented data, and allows for a more realistic estimate of how often a model does transitions around the data. This metric is used to compare models in §4.2 and §4.3 for results.
4.2 SENSITIVITY AND GENERALIZATION FACTORS
In §4.1 we established that neural networks implement more robust functions in the vicinity of the training data manifold than away from it.
We now consider a more practical context of model selection. Given two perfectly trained neural networks, does the model with better generalization implement a less sensitive function?
We study common in the machine learning community ways to increase or damage generalization:
1. random labels (Figure 6);6
2. data augmentation (Figure 7);
3. ReLUs (Figure 8);
4. full-batch training (Figure 9);
We find that for each of them, the change in generalization is coupled with the respective change in sensitivity (i.e. lower sensitivity corresponds to smaller generalization gap) as measured by the Jacobian norm (and almost always for the transitions metrics).
4.3 SENSITIVITY AND GENERALIZATION GAP
We now perform a large-scale experiment to establish direct relationships between sensitivity and generalization in a realistic setting. In contrast to §4.1, where we selected locations in the input space, and §4.2, where we varied a single binary parameter impacting generalization, we now sweep over many different architectural and optimization choices (§A.4.5).
Our main result is presented in Figure 3, indicating a strong relationship between the Jacobian norm and generalization. In contrast, Figure 11 demonstrates that the number of transitions is not alone sufficient to compare networks of different sizes, as the number of neurons in the networks has a strong influence on transition count.
6How to read plots: for many possible hyper-parameter configurations, we train two models that share all parameters and optimization procedure, but differ in the single binary parameter listed above (i.e. trained on true or random labels; with or without data augmentation; etc). Out of all such network pairs, we select only those where each network reached a 100% training accuracy on the whole training set (apart from the data augmentation study). The two generalization or sensitivity values are then used as the x and y coordinates of a point corresponding to this pair of networks. Position of the point with respect to the diagonal y = x visually demonstrates which configuration has smaller generalization gap / lower sensitivity.
6

Under review as a conference paper at ICLR 2018

CIFAR10
1

CIFAR100
1

Generalization gap

0.8 0.8

0.6 0.6

0.4 1

100 10k

0.4 1

100 10k

Jacobian norm
Figure 3: Jacobian norm correlates with generalization gap. Left: 2160 networks with 100% train accuracy on CIFAR10. Right: 2097 networks with at least 99.9% training accuracy on CIFAR100. See §A.4.5 for experimental details.

4.4 SENSITIVITY AND PER-POINT GENERALIZATION
In §4.3 we established a correlation between sensitivity (as measured by the Jacobian norm) and generalization averaged over a large test set (104 points). We now investigate whether the Jacobian norm can be predictive of generalization at individual points.
As demonstrated in Figure 12, Jacobian norm at a point is predictive of the cross-entropy loss, but the relationship is not a linear one, and not even bijective (see §A.2 for analytic expressions explaining it). In particular, certain misclassified points (right sides of the plots) have a Jacobian norm many orders of magnitude smaller than that of the correctly classified points (left sides). However, we do remark a consistent tendency for points having the highest values of the Jacobian norm to be mostly misclassified. This observation makes the jacobian norm a promising quantity to consider in the contexts of active learning and confidence estimation in future research.
5 CONCLUSION
We have investigated sensitivity of trained neural networks through the input-output Jacobian norm and linear regions counting in the context of image classification tasks. We have presented extensive experimental evidence indicating that the local geometry of the trained function as captured by the norm of the input-output Jacobian can be predictive of generalization in many different contexts, and that it varies drastically depending on how close to the training data manifold the function is evaluated. We further established, experimentally and analytically, a connection between the crossentropy loss and the Jacobian norm, indicating that it can remain informative of generalization even at the level of individual test points.

8When applying data augmentation, the network is unlikely to encounter the canonical training data, hence few configurations achieved 100% training accuracy. However, we verified that all networks trained with data augmentation reached a higher test accuracy than their analogues without, ensuring that the generalization gap shrinks not simply because of lower training accuracy
7

Under review as a conference paper at ICLR 2018

Jacobian norm (y-axis) of a trained network along...
a random ellipse an ellipse through 3 training points of a different class an ellipse through 3 training points of the same class

10k

100

1

0.01

100 0

/3 (point 1)

2/3  (point 2) 4/3  (position along the trajectory)

5/3 (point 3)

2

Transition density (y-axis) of a trained network along...
a random ellipse an ellipse through 3 training points of a different class an ellipse through 3 training points of the same class

45

40

35

30

25

20

15

10 0

/3 (point 1)

2/3

 (point 2)

4/3 5/3 (point 3)

2

 (position along the trajectory)

Figure 4: A 100%-accurate (on training data) MNIST network implements a function that is much more stable near training data than away from it. Top: Jacobian norm as the input traverses an elliptical trajectory. Sensitivity drops significantly in the vicinity of training data while remaining uniform along random ellipses. Center: depiction of a hypothetical trajectory between three digits of different classes, highlighting the training points locations. Bottom: transition density behaves analogously. According to both metrics, as the input moves between points of different classes, the function becomes less stable than when it moves between points of the same class. This is consistent with the intuition that linear interpolations between different digits lie further from the data manifold than those between digits of the same class (which need not hold for more complex datasets). See §A.4.2 for experimental details.

8

Under review as a conference paper at ICLR 2018 Before Training

After Training

Figure 5: Transition boundaries of the last (pre-logits) layer. Training points are indicated by inset
squares. Left: boundaries before training. Right: after training, transition boundaries become highly non-isotropic, with training points lying in regions of lower transition density. See §A.4.3 for experimental details.

Jacobian norm

Transitions

10k 1M

w/ random labels w/ random labels

1000 100 10

100k 10k

1 1

10 100 1000
w/ true labels

10k

1000 1000

10k 100k
w/ true labels

1M

Figure 6: Networks trained on random labels consistently manifest higher sensitivity according to both transitions and Jacobian norm metrics (and, obviously, do not generalize). Each point on the plot corresponds to two neural networks that share all hyper-parameters and optimization procedure, but differ in training data labels as indicated by axes titles. The coordinates along each axis reflect the value of the quantity in the title of the plot in the respective setting (i. e. w/ true or random labels). All networks have reached 100% training accuracy on CIFAR10 in both settings. Left: Jacobian norm of networks that have fit random labels is significantly higher than that of networks fitting true labels. Right: networks trained on random labels make more transitions than those trained on true labels. See §A.4.5 for experimental details.

9

Under review as a conference paper at ICLR 2018

w/ data augmentation w/ data augmentation w/ data augmentation

Generalization gap
0.6 0.4 0.2

Jacobian norm
1000
100
10
1

Transitions
1M
100k
10k
1000

0.2 0.4 0.6
w/o data augmentation

0.1 0.1

1

10 100 1000

w/o data augmentation

1000 10k 100k
w/o data augmentation

1M

Figure 7: Data augmentation reduces the generalization gap and sensitivity. These plots are analogous to figures 6, 8 and 9, except that here networks are not filtered to have 100% train accuracy8.

Left: data augmentation reduces the generalization gap. Center: data augmentation results in a smaller Jacobian norm. Right: data augmentation leads to fewer transitions. See §A.4.4 for experi-

mental details.

Generalization gap

Jacobian norm

Transitions

HardSigmoid HardSigmoid HardSigmoid

0.8 0.7 0.6 0.5 0.4
0.4 0.5 0.6 0.7 0.8
ReLU

2 10
5

2

1

5

512

5 10 2

ReLU

1M

100k

10k

1000 1000

10k 100k
ReLU

1M

Figure 8: ReLU-networks generalize better than HardSigmoid-networks (see §A.3 for definition), and are less sensitive to inputs according to both Jacobian norm and transitions metrics. As in figures
6 and 7, each point corresponds to two 100%-accurate (on training set) neural networks different
only in their activation function. Left: ReLU-networks generalize better than HardSigmoid. Center: despite that HardSigmoid can only have non-zero derivative on a small interval ([-0.5; 0.5]), the Jacobian norm of ReLU networks is consistently lower than that of HardSigmoid. Right: ReLU networks make fewer transitions than HardSigmoid. See §A.4.5 for experimental details.

Generalization gap

Jacobian norm

Transitions

L-BFGS L-BFGS L-BFGS

0.8
0.6
0.4 0.4 0.6 0.8
SGD + Momentum

100 5

2 10
5

2 1 5
5

12

5 10 2

5 100

SGD + Momentum

1M

100k

10k

1000
1000 10k 100k
SGD + Momentum

1M

Figure 9: Full-batch training using L-BFGS generalizes worse than SGD with Momentum with batch of size 128, and, simultaneously, results in higher Jacobian norm. As in figures 6 and 8, each point corresponds to two 100%-accurate (on training set) neural networks different only in their optimizer and batch size. Left: L-BFGS generalizes worse than SGD + Momentum. Center: simultaneously, L-BFGS networks have higher Jacobian norm. Right: L-BFGS can lead to very high number of transitions (top), but for lower-capacity models it does not result in perform differently from models trained with SGD in terms of transitions. See §A.4.5 for experimental details.

10

Under review as a conference paper at ICLR 2018

MNIST

FASHION_MNIST

Generalization gap

0.1 0.1

0.05 0.05

5 0.1 2

512

5

12

5 10 2

Jacobian norm
Figure 10: Jacobian norm remains indicative of generalization gap even at much smaller scales of the gap for simple datasets. Left: networks with 100% train accuracy on MNIST. Right: networks with 100% train accuracy on Fashion-MNIST. See §A.4.4 for experimental details.

CIFAR10
1

CIFAR100
1

Generalization gap

0.8 0.8

0.6 0.6

0.4 1000 10k 100k 1M

0.4 1000 10k 100k 1M

Transitions
Figure 11: Number of transitions, in contrast to Figures 3 and 10, does not generally correlate with generalization gap. Left: 2160 networks with 100% train accuracy on CIFAR10. Right: 2097 networks with at least 99.9% training accuracy on CIFAR100. See §A.4.5 for experimental details

11

Under review as a conference paper at ICLR 2018

Jacobian norm

CIFAR100

100

1

0.01 100
1

10n

100p

1p 100n

1

10

100

0.001

0.01

0.1

1

10 100

Jacobian norm

CIFAR10

100

1

0.01 100
1

10n

100p

1p 100n

1

10

100

0.001

0.01

0.1

1

10 100

Jacobian norm

FASHION_MNIST

100

1

0.01

100

1

10n

100p

1p 100n

1

10

100

0.001

0.01

0.1

1

10 100

Jacobian norm

MNIST

100

1

0.01

100

1

10n

100p

1p 100n

1

10

100

0.001

0.01

0.1

1

10 100

Cross-entropy loss

Figure 12: Jacobian norm plotted against individual test point loss. Each plot shows 5 random networks that fit the respective training set to a 100% with each network having a unique color.
These plots confirm the relationship established in Figure 14 experimentally on different datasets and models. See §A.4.6 for experimental details.

12

Under review as a conference paper at ICLR 2018
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Anonymous. Deep neural networks as gaussian processes. International Conference on Machine Learning, 2018.
Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. In Machine Learning: From Theory to Applications, 1988.
Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. CoRR, abs/1605.07678, 2016.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015.
Yann Dauphin, Razvan Pascanu, aglar Gu¨lehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1487­1495. ACM, 2017.
Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. CoRR, abs/1412.6544, 2014.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068, 2014.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pp. 3059­3068, 2016.
Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789):947­951, 2000.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learninglecture 6a-overview of mini-batch gradient descent.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
William H Jefferys and James O Berger. Ockham's razor and bayesian analysis. American Scientist, 80(1):64­72, 1992.
Yulei Jiang, Richard M Zur, Lorenzo L Pesce, and Karen Drukker. A study of the effect of noise injection on the training of artificial neural networks. In Neural Networks, 2009. IJCNN 2009. International Joint Conference on, pp. 1428­1432. IEEE, 2009.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky. Convolutional deep belief networks on cifar-10.
13

Under review as a conference paper at ICLR 2018
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950­957, 1992.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
David J. C. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992.
David JC MacKay. Bayesian interpolation. 1991.
G. Montu´far, R. Pascanu, K. Cho, and Y. Bengio. On the Number of Linear Regions of Deep Neural Networks. Neural Information Processing Systems, February 2014.
Iain Murray and Zoubin Ghahramani. A note on the evidence and bayesian occam's razor, 2005.
Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto, 1994.
Radford M. Neal, Richard Mann, Carl Rasmussen, and Chris Williams. Bayesian learning for neural networks bayesian learning for neural networks. In Lecture Notes in Statistics, 1995.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. Proceeding of the international Conference on Learning Representations workshop track, abs/1412.6614, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. CoRR, abs/1706.08947, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on, pp. 372­387. IEEE, 2016.
R. Pascanu, G. Montufar, and Y. Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. International Conference on Learning Representations, December 2013.
M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein. On the Expressive Power of Deep Neural Networks. International Conference on Machine Learning, June 2016.
Carl E. Rasmussen and Zoubin Ghahramani. Occam's razor. In NIPS, 2000.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive autoencoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 833­840, 2011.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 2017.
Ercan Solak, Roderick Murray-Smith, William E Leithead, Douglas J Leith, and Carl E Rasmussen. Derivative observations in gaussian process models of dynamic systems. In Advances in neural information processing systems, pp. 1057­1064, 2003.
Matus Telgarsky. Representation benefits of deep feedforward networks. CoRR, abs/1509.08101, 2015.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations, November 2016.
14

Under review as a conference paper at ICLR 2018

A APPENDIX
A.1 DO NEURAL NETWORKS DEFY OCCAM'S RAZOR?
Here we briefly discuss the motivation of this work in the context of Occam's razor.
Occam's razor is an heuristic for comparing hypotheses based on their complexity, giving preference to simple models. It is commonly justified through the Bayesian "automatic Occam's razor" reasoning (Jefferys & Berger, 1992; MacKay, 1991) stating that given a dataset D and uniform prior on two competing hypotheses classes Hs (simple models, few parameters) and Hc, (complex models, many parameters), the posterior P [H|D]  P [D|H] is higher for a simple model Hs due to its likelihood P [D|Hs] having a smaller support in the dataset space (see Figure 13, left).
Occam's razor for neural networks. As seen in Figure 1, above reasoning does not apply to neural networks: the best achieved generalization is obtained by a model that has around 104 more parameters than the simplest model capable of fitting the dataset.
On one hand, (Murray & Ghahramani, 2005; Telgarsky, 2015) demonstrate on concrete examples that a high number of free parameters in the model doesn't necessarily entail high complexity. On the other hand, a large body of work on the expressivity of neural networks (see §2) shows that their ability to compute complex functions increases rapidly with size, while (Zhang et al., 2016) validates that they also easily fit complex (even random) functions with stochastic optimization. This indicates that weights of a neural network may actually correspond to its usable capacity, and hence lead to "smearing" the likelihood P [D|H] along a very large space of datasets D , making the dataset of interest D less likely.
Potential issues. We conjecture two reasons for why Occam's razor may not apply:
1. The approximate computation involved in estimating the "Occam's factor" (MacKay, 1991; 1992) used to assess the shape of the likelihood curves in Figure 13 (left) involves assumptions that may not hold for neural network in the context of stochastic optimization, and, in particular, do not account for the combinatorial growth of the acceptable volume of parameter space as width increases. We conjecture that a realistic plot might look like the one shown in (Figure 13, right), which renders model comparison by their classes H impossible without highly precise knowledge of the likelihood curves.
2. For the proposed hypotheses classes (i.e. hypotheses binned by number of parameters) P [H] is far from uniform. In fact, given the complexity of tasks we tackle with neural networks in practical applications, the prior might favor larger models to small ones (Neal et al., 1995).
We interpret this work as tackling the second issue: we suggest to define hypotheses classes based on sensitivity of the hypothesis (which yielded promising results in (Rasmussen & Ghahramani, 2000) on a toy task). Indeed, at least in the context of natural images classification, putting a prior on the number of parameters or Kolmogorov complexity of the hypothesis is extremely difficult. However, a statement that the true classification function is robust to small perturbations in the input is much easier to justify. As such, a prior P [H] in favor of robustness over sensitivity might fare better than a prior on specific network hyper-parameters.

A.2 BOUNDING THE JACOBIAN NORM

Here we analyze the relationship between the Jacobian norm and the cross-entropy loss at individual test points.
Target class Jacobian.We begin by relating the derivative of the target class probability Jy(x) to per-point cross-entropy loss l(x) = - log [f(x)]y(x) (where y(x) is the correct integer class).
We will denote f(x) by  and drop the x argument to de-clutter notation (e.g. write f instead of f (x)). Then the Jacobian can be expressed as

J = 1T

I - 1T

f xT ,

15

Under review as a conference paper at ICLR 2018

P [D0|H]

Expectation
Simple Hs

P [D0|H]

Reality?

Simple Hs

Complex Hc

D

D0

Zeros

?

D

Complex Hc D0
Noise

Figure 13: Occam's Razor: expectation vs hypothesized reality. Left: a classic depiction of Bayesian Occam's razor. Likelihood P [D |H] of a simple model class Hs with few parameters has smaller support in the dataset space, hence is more peaked. If the class fits the dataset D well, it
falls close to the peak and outperforms a complex class Hc with more parameters, having wider support. Right: suggested potential reality of neural networks. All datasets D with inputs and targets dimensions matching those of a particular dataset D are sorted according to likelihood P [D |H] of a complex class Hc from left to right along the horizontal axis. A simple class Hs peaks higher, but we don't know onto which side of the likelihood curves intersection D falls.

where is the Hadamard element-wise product. Then indexing both sides of the equation at the

correct class y yields

Jy = y (ey - )T

f xT

,

where ey is a vector of zeros everywhere except for ey = 1. Taking the norm of both sides results in


d

Jy

2 2

=

y2

(1 - y)2

k=1

fy

2n
+

xk

j=y



j

fj xk

2
=



d
= y2 (1 - y)2
k=1

fy xk

2n

d

+ j2

j=y k=1

fj xk

2
=



= y2 (1 - y)2

fy xT

2n
+ j2
2 j=y

 fj 2

xT


2

(2)

We now assume that magnitudes of the individual logit derivatives vary little in between logits and

over the input space9:

fi xT

2 2



1 n

Extest

f 2

 xtTest

,
F

which simplifies Equation 2 to


n

Jy

2 2



M y2

(1

-

y )2

+

j2 ,

j=y

9In the limit of infinite width, and fully Bayesian training, deep network logits are distributed exactly according to a Gaussian process (Neal, 1994; Anonymous, 2018). Similarly, each entry in the logit Jacobian also corresponds to an independent draw from a Gaussian process (Solak et al., 2003). It is therefore plausible that the Jacobian norm, consisting of a sum over the square of independent Gaussian samples in the correct limits, will tend towards a constant.

16

Under review as a conference paper at ICLR 2018

where M = Extest

f /xtTest

2 F

/n.

And

since



lies

on

the

(n

-

1)-simplex

n-1,

we

can

bound:

(1 - y)2 n-1

n
j2
j=y

(1 - y)2,

and finally

n

n -

1

M

y2

(1

-

y

)2

Jy

2 2

or, in terms of the cross-entropy loss l = - log y:

2M y2 (1 - y)2 ,

nM n-1

e-l

1 - e-l

Jy 2

 2M

e-l

1 - e-l

.

(3)

We validate these approximate bounds in Figure 14 (top).

Full Jacobian. Equation 3 establishes a close relationship between Jy and loss l = - log y, but of course, at test time we do not know the target class y. This allows us to only bound the full Jacobian

norm from below:

nM n-1

e-l

1 - e-l

Jy 2 J F .

(4)

For the upper bound, we assume the maximum-entropy case of y: i  (1 - y)/(n - 1), for i = y. The Jacobian norm is

nn

J

2 F

=

Ji

2 2

=

Jy

2 2

+

Ji

2 2

,

i=1 i=y

where we the first summand becomes:

Jy

2 2



M y2

(1 - y)2 + (n - 1)

1 - y n-1

2

=

Mn n-1

y2

(1

-

y )2

,

and each of the others

Ji

2 2



M

1 - y 2 n-1

1

-

1 - y n-1

2
+

y2 + (n - 2)

1 - y n-1

2

=

=

(n

M - 1)3

(1

-

y )2

ny2 + n - 2

2.

(5)

Adding n - 1 of such summands to

Jy

2 2

results

in



J

F



(n

M - 1)

(1

-

y

)

n2y2

+

n-2

=

M (n - 1)

1 - e-l

n2e-2l + n - 2,

(6)

compared against the lower bound (Equation 4) and experimental data in figure 14.

A.3 NON-LINEARITIES DEFINITIONS
Following activation functions are used in this work:
1. ReLU (Hahnloser et al., 2000): max(x, 0); 2. ReLU6 (Krizhevsky): min (max(x, 0), 6); 3. Tanh: hyperbolic tangent, (ex - e-x)/(ex + e-x); 4. HardTanh (Gulcehre et al., 2016): min (max(x, -1), 1); 5. HardSigmoid (Gulcehre et al., 2016): min (max(x + 0.5, 0), 1);

17

Under review as a conference paper at ICLR 2018

Jacobian norm (correct class)

CIFAR10

1

0.001 1

1n

1p

1f

1e-18

1e-21

100n

1

10

100

0.001

0.01

0.1

1

10 100

Jacobian norm

1

0.001 1

1n

1p

1f

1e-18

1e-21

100n

1

10

100

0.001

0.01

0.1

1

10 100

Figure 14: Top: Jacobian norm

Jy (x) 2 =

f (x)y /xT

of the true class y output probabil-
2

ity is tightly related to the cross-entropy loss (x-axis). Each point corresponds to one of the 1000 test

inputs to a 100% trained network on CIFAR10, while lines depict analytic bounds from Equation 3.

Bottom: Same experiment plotting the full Jacobian norm J F against cross-entropy. Solid lines correspond to the lower bound from Equation 4 and the norm approximation from Equation 6. See

§A.4.6 for experimental details.

18

Under review as a conference paper at ICLR 2018
A.4 EXPERIMENTAL SETUP
All experiments were implemented in Tensorflow (Abadi et al., 2016) and executed with the help of Vizier (Golovin et al., 2017). All networks had a were trained with a cross-entropy loss. All networks were trained without biases. All computations were done with 32-bit precision. Learning rate decayed by a factor of 0.1 every 500 epochs. Unless specified otherwise, initial weights were drawn from a normal distribution with zero mean and variance 2/n for ReLU, ReLU6 and HardSigmoid; 1/n for Tanh and HardTanh, where n is the number of inputs to the current layer. All inputs were normalized tohave zero mean and unit variance, or, in other terms, lie on the ddimensional sphere of radius d, where d is the dimensionality of the input. All reported values, when applicable, were evaluated on the whole training and test sets of sizes 50000 and 10000 respectively. When applicable, all trajectories/surfaces in input space were sampled with 220 points.
A.4.1 PLOTS AND ERROR BARS
All figures except for 12 and 14 are plotted with (pale) error bars (when applicable). The reported quantity was usually evaluated 8 times with random seeds from 1 to 810, unless specified otherwise. E.g. if a network is said to be 100%-accurate on the training set, it means that each of the 8 randomlyinitialized networks are 100% accurate after training. The error bar is centered at the mean value of the quantity and spans the standard error of the mean in each direction. If the bar appears to not be visible, it may be smaller than the mean value marker. Weight initialization, training set shuffling, data augmentation, picking anchor points of data-fitted trajectories, selecting axes of a zero-centered elliptic trajectory depend on the random seed.
A.4.2 SENSITIVITY ALONG A TRAJECTORY
Relevant figure 4. A 20-layer ReLU-network of width 200 was trained on MNIST 128 times, with plots displaying the averaged values. A random zero-centered ellipse was obtained by generating two axis vectors with normallydistributed entries of zero mean and variance 1/d where d is the dimensionality of the input (as such making the trajectory have an expected norm equal to that of training data) and sampling points on the ellipse with given axes. A random data-fitted ellipse was generated by projecting three arbitrary input points onto a plane where they fall into vertices of an equilateral triangle, and then projecting their circumcircle back into the input space.
A.4.3 LINEAR REGION BOUNDARIES
Relevant figure 5. A 15-layer ReLU6-network of width 300 was trained on MNIST; images were randomly translated with wrapping by up to 4 pixels in each direction, horizontally and vertically, as well as randomly flipped along each axis, and randomly rotated by 90 degrees clockwise and counter-clockwise. The sampling grid in input space was obtain by projecting three arbitrary input points into a plane as described in §A.4.2 such that the resulting triangle was centered at 0 and it's vertices were at a distance 0.8 form the origin. Then, a sampling grid of points in the [-1; 1]×2 square was projected back into the input space.
10If a particular random seed did not finish, it was not taken into account; we believe this nuance did not influence the conclusions of this paper.
19

Under review as a conference paper at ICLR 2018
A.4.4 SMALL EXPERIMENT
Relevant figures: 7 and 10. All networks were trained for 218 steps of batch size of 256 using SGD with momentum (Rumelhart et al.). Learning rate was set to 0.005 and momentum term coefficient to 0.9.
Data augmentation consisted of random translation of the input by up to 4 pixels in each direction with wrapping, horizontally and vertically. The input was also flipped horizontally with probability 0.5. For each dataset, networks of width {100, 200, 500, 1000, 2000, 3000}, depth {2, 3, 5, 10, 15, 20} and activation function {ReLU, ReLU6, HardTanh, HardSigmoid} were evaluated 8 times each.
A.4.5 LARGE EXPERIMENT
Relevant figures: 3, 9, 1, 6, 8. 335671 networks were trained for 219 steps with random hyper-parameters; if training did not complete, a checkpoint at step 218 was used instead, if available. When using L-BFGS, the maximum number of iterations was set to 2684. The space of available hyper-parameters included11:
1. CIFAR10 and CIFAR100 datasets cropped to a 24 × 24 center region; 2. all 5 non-linearities from §A.3; 3. SGD, Momentum, ADAM (Kingma & Ba, 2014), RMSProp (Hinton et al.) and L-BFGS
optimizers; 4. learning rates from {0.01, 0.005, 0.0005}, when applicable. Secondary coefficients were
fixed at default values of Tensorflow implementations of respective optimizers; 5. batch sizes of {128, 512} (unless using L-BFGS with the full batch of 50000); 6. standard deviations of initial weights from {0.5, 1, 4, 8} multiplied by the default value
described in §A.4; 7. widths from 1, 2, 4, · · · , 216 ; 8. depths from 2, 3, 5, · · · , 26 + 1 ; 9. true and random training labels; 10. random seeds from 1 to 8.
A.4.6 PER-POINT GENERALIZATION
Relevant figures: 12, 14. Here we trained and evaluated networks on random 500012 and 1000 subsets of training and test data of CIFAR100, CIFAR10, Fashion-MNIST and MNIST. The hyper-parameters consisted of nonlinearity (all functions from §A.3), width (50, 100 or 200) and depth (2, 5, 10, 20). Only one random seed (1) was considered. For each dataset, a random subset of 5 configurations among all the 100%accurate (on training) networks was plotted.
Figure 14 was generated from a similar experiment, with the exception that the whole training set was used and 218 training steps were done. it does not contain any CIFAR100 points since no network reached a 100% on the full dataset in this time. A single random 100%-accurate (on training data) network was drawn to compare experimental measurements to analytic bounds on the Jacobian norm.
11Due to time and compute limitations, this experiment was set up such that configurations of small size were more likely to get evaluated (e.g. only a few networks of width 216 were trained, and all of them had depth 2). However, based on our experience with smaller experiments (where each configuration got evaluated), we believe this did not bias the findings of this paper, while allowing them to be validated across a very wide range of scenarios.
12In a different experiment, precisely the one used in Figure 14, we trained on the whole 50000 dataset and obtained qualitatively similar plots.
20

