{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222676206,"tcdate":1511960749239,"number":3,"cdate":1511960749239,"id":"rkHhxN2lG","invitation":"ICLR.cc/2018/Conference/-/Paper499/Official_Review","forum":"r1wEFyWCW","replyto":"r1wEFyWCW","signatures":["ICLR.cc/2018/Conference/Paper499/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Official Reviewer 1","rating":"4: Ok but not good enough - rejection","review":"This paper focuses on the density estimation when the amount of data available for training is low. The main idea is that a meta-learning model must be learnt, which learns to generate novel density distributions by learn to adapt a basic model on few new samples. The paper presents two independent method.\n\nThe first method is effectively a PixelCNN combined with an attention module. Specifically, the support set is convolved to generate two sets of feature maps, the so called \"key\" and the \"value\" feature maps. The key feature map is used from the model to compute the attention in particular regions in the support images to generate the pixels for the new \"target\" image. The value feature maps are used to copmpute the local encoding, which is used to generate the respective pixels for the new target image, taking into account also the attention values. The second method is simpler, and very similar to fine-tuning the basis network on the few new samples provided during training. Despite some interesting elements, the paper has problems.\n\nFirst, the novelty is rather limited. The first method seems to be slightly more novel, although it is unclear whether the contribution by combining different models is significant. The second method is too similar to fine-tuning: although the authors claim that \\mathcal{L}_inner can be any function that minimizes the total loss \\mathcal{L}, in the end it is clear that the log-likelihood is used. How is this approach (much) different from standard fine-tuning, since the quantity P(x; \\theta') is anyways unknown and cannot be \"trained\" to be maximized.\n\nBesides the limited novelty, the submission leaves several parts unclear. First, why are the convolutional features of the support set in the first methods divided into \"key\" and \"value\" feature maps as in p_key=p[:, 0:P], p_value=p[:, P:2*P]? Is this division arbitrary, or is there a more basic reason? Also, is there any different between key and value? Why not use the same feature map for computing the attention and computing eq (7)?\n\nAlso, in the first model it is suggested that an additional feature can be having a 1-of-K channel for the supporting image label: the reason is that you might have multiple views of objects, and knowing which view contributes to the attention can help learning the density. However, this assumes that the views are ordered, namely that the recording stage has a very particular format. Isn't this a bit unrealistic, given the proposed setup anyways?\n\nRegarding the second method, it is not clear why leaving this room for flexibility (by allowing L_inner to be any function) to the model is a good idea. Isn't this effectively opening the doors to massive overfitting? Besides, isn't the statement that the function \\mathcal{L}_inner void? At the end of the day one can also claim the same for gradient descent: you don't need to have the true gradients of the true loss, as long as the objective function obtains gradually lower and lower values?\n\nLast, it is unclear what is the connection between the first and the second model. Are these two independent models that solve the same problem? Or are they connected?\n\nRegarding the evaluation of the models, the nature of the task makes the evaluation hard: for real data like images one cannot know the true distribution of particular support examples. Surrogate tasks are explored, first image flipping, then likelihood estimation of Omniglot characters, then image generation. Image flipping does not sound a very relevant task  to density estimation, given that the task is deterministic. Perhaps, what would make more sense would be to generate a new image given that the support set has images of a particular orientation, meaning that the model must learn how to learn densities from arbitrary rotations. Regarding Omniglot character generation, the surrogate task of computing likelihood of known samples gives a bit better, however, this is to be expected when combining a model without attention, with an attention module.\n\nAll in all, the paper has some interesting ideas. I encourage the authors to work more on their submission and think of a better evaluation and resubmit.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions","abstract":"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","pdf":"/pdf/0e20efde01724a86ebdd48ae3c022982640db115.pdf","TL;DR":"Few-shot learning PixelCNN","paperhash":"anonymous|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1wEFyWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper499/Authors"],"keywords":["few-shot learning","density models","meta learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222676254,"tcdate":1511842659926,"number":2,"cdate":1511842659926,"id":"rJ3vXv5xf","invitation":"ICLR.cc/2018/Conference/-/Paper499/Official_Review","forum":"r1wEFyWCW","replyto":"r1wEFyWCW","signatures":["ICLR.cc/2018/Conference/Paper499/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Few shot learning with autoregressive density estimation","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on few shot learning with autoregressive density estimation. Specifically, the paper improves PixelCNN with  1) neural attention, 2) meta learning techniques, and shows that the model achieve STOA few showt density estimation on the Omniglot dataset and demonstrate the few showt image generation on the Stanford Online Products dataset. \n\nThe model is interesting, however, several details are not clear, which  makes it harder to repeat the model and the experimental results. For example, what is the reason to use the (key, value) pair to encode these support images, what does the \"key\" means and what is the difference between \"keys\" and \"values\"? In the experiments, the author did not explain the meaning of \"nats/dim\" and how to compute it. Another question is about the repetition of the experimental results. We know that PixelCNN is already a quite complicated model, it would be even harder to implement the proposed model. I wonder whether the author will release the official code to public to help the community?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions","abstract":"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","pdf":"/pdf/0e20efde01724a86ebdd48ae3c022982640db115.pdf","TL;DR":"Few-shot learning PixelCNN","paperhash":"anonymous|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1wEFyWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper499/Authors"],"keywords":["few-shot learning","density models","meta learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222676294,"tcdate":1511798016705,"number":1,"cdate":1511798016705,"id":"HyKWS3KxM","invitation":"ICLR.cc/2018/Conference/-/Paper499/Official_Review","forum":"r1wEFyWCW","replyto":"r1wEFyWCW","signatures":["ICLR.cc/2018/Conference/Paper499/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A solid paper","rating":"7: Good paper, accept","review":"This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning. The application is an obvious target for research and some relevant citations are missing, e.g. \"Towards a Neural Statistician\" (Edwards et al., ICLR 2017). Nonetheless, I think the current paper seems interesting enough to merit publication.\n\nThe paper is well-produced, i.e. the overall writing, visuals, and narrative flow are good. It was easy to read the paper straight through while understanding both the technical details and more intuitive motivations.\n\nI have some concerns about the architectures and experiments presented in the paper. For architectures: the attention-based model seems powerful but difficult to scale to problems with more inputs for conditioning, and the meta PixelCNN model is a standard PixelCNN trained with the MAML approach by Finn et al. For experiments: the ImageNet flipping task is clearly tailored to the strengths of the attention-based model, and the presentation of the general Omniglot results could be improved. The image flipping experiment is neat, but the attention-based model's strong performance is unsurprising. I think the results in Tables 1/2 should be merged into a single table. It would make it clear that the MAML-based and attention-based models achieve similar performance on this task.\n\nOverall, I think the paper makes a nice contribution. The paper could be improved significantly, e.g., by showing how to scale the attention-based architecture to problems with more data or by designing an architecture specifically for use with MAML-based inference.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions","abstract":"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","pdf":"/pdf/0e20efde01724a86ebdd48ae3c022982640db115.pdf","TL;DR":"Few-shot learning PixelCNN","paperhash":"anonymous|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1wEFyWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper499/Authors"],"keywords":["few-shot learning","density models","meta learning"]}},{"tddate":null,"ddate":null,"tmdate":1510257290313,"tcdate":1510257290313,"number":1,"cdate":1510257290313,"id":"ByMcMEGJG","invitation":"ICLR.cc/2018/Conference/-/Paper499/Public_Comment","forum":"r1wEFyWCW","replyto":"r1wEFyWCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Previous work","comment":"Hi I think it would be worth adding https://arxiv.org/abs/1612.02192 to your related work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions","abstract":"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","pdf":"/pdf/0e20efde01724a86ebdd48ae3c022982640db115.pdf","TL;DR":"Few-shot learning PixelCNN","paperhash":"anonymous|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1wEFyWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper499/Authors"],"keywords":["few-shot learning","density models","meta learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739269553,"tcdate":1509124398783,"number":499,"cdate":1509739266897,"id":"r1wEFyWCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1wEFyWCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions","abstract":"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","pdf":"/pdf/0e20efde01724a86ebdd48ae3c022982640db115.pdf","TL;DR":"Few-shot learning PixelCNN","paperhash":"anonymous|fewshot_autoregressive_density_estimation_towards_learning_to_learn_distributions","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1wEFyWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper499/Authors"],"keywords":["few-shot learning","density models","meta learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}