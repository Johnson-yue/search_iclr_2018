{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222680477,"tcdate":1511845061958,"number":3,"cdate":1511845061958,"id":"HyCT3vclM","invitation":"ICLR.cc/2018/Conference/-/Paper53/Official_Review","forum":"r1BRfhiab","replyto":"r1BRfhiab","signatures":["ICLR.cc/2018/Conference/Paper53/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neat basic idea, but not enough","rating":"4: Ok but not good enough - rejection","review":"This paper explores a neat, simple idea intended to learn models suitable for fast membership queries about single classes (\"is this data point a member of this class [or set of classes]?\"). In the common case when the class prediction is made with a softmax function minimizing 1-of-K multiclass cross-entropy loss, this cannot in general be determined without essentially evaluating all K logits (inputs to the softmax). This paper describes how other losses (such as the natural multilabel cross-entropy) do not suffer this problem because all true labels' logits rank above all false labels' (so that any membership query can be answered by choosing a threshold), and models trained to minimize these losses perform better on class membership metrics. One of the new losses suggested, the batch cross-entropy, is particularly interesting in keeping with the recent work on using batch statistics; I would like to see this explored further (see below). The paper is well-written.\n\nI am not sure of the relevance of this work as written. The authors discuss how related work (e.g. Grave et al.) scales computationally with K, which is undesirable; however, training the entire network with a non-CE objective function is an end-to-end model change, and practical uptake may suffer without further justification. The problem (and the proposed solution by changing training objective) is of interest because standard approaches ostensibly suffer unfavorable runtime-to-performance tradeoffs, so this should be demonstrated. I would be more comfortable if the authors actually evaluated runtime, preferably against one or two of the other heuristic baselines they cite. \nThe notation is a little uneven. The main idea is stated given the premise of Fig. 1, that there exist logits which are computed and passed through a softmax neuron, but this is never formally stated. (There are a few other very minor quibbles, e.g. top of pg. 6: sum should be over 1...k). ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Principle of Logit Separation","abstract":"We consider neural network training, in applications in which there are many possible classes, but at test-time, the task is to identify only whether the given example belongs to a specific class, which can be different in different applications of the classifier. For instance, this is the case in an image search engine. We consider the Single Logit Classification (SLC) task: training the network so that at test-time, it would be possible to accurately identify if the example belongs to a given class, based only on the output logit for this class. \nWe propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC. \nWe show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. In total, we study seven loss functions. \nOur experiments show that indeed in almost all cases, losses that are aligned with Principle of Logit Separation obtain a 20%-35% relative performance improvement in the SLC task, compared to losses that are not aligned with it. We therefore conclude that the Principle of Logit Separation sheds light on an important property of the most common loss functions used by neural network classifiers. ","pdf":"/pdf/054339a4093f4cd5943b735b0ac840d1c94303cc.pdf","paperhash":"anonymous|the_principle_of_logit_separation","_bibtex":"@article{\n  anonymous2018the,\n  title={The Principle of Logit Separation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1BRfhiab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper53/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222680516,"tcdate":1511814198071,"number":2,"cdate":1511814198071,"id":"ryA44e5xf","invitation":"ICLR.cc/2018/Conference/-/Paper53/Official_Review","forum":"r1BRfhiab","replyto":"r1BRfhiab","signatures":["ICLR.cc/2018/Conference/Paper53/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Unconvincing formalization of a challenge.","rating":"3: Clear rejection","review":"The paper addresses the problem of a mismatch between training classification loss and a loss at test time. This is motivated by use cases in which multiclass classification problems are learned during training, but where binary or reduced multi-class classifications is performed at test time. The question for me is the following: if at test time, we have to solve \"some\" binary classification task, possibly drawn at random from a set of binary problems (this is not made precise in the paper), then why not optimize the same classification error or a surrogate loss at training time? Instead, the authors start with a multiclass problem, which may introduce a computational burden. when the number of classes is large as one needs to compute a properly normalized softmax. The authors now seem to ask, what if one were to use a multi-classification loss at training time, but then decides at test time that a binary classification of one-vs-all is asked for. \n\nIf one buys into the relevance of the setting, then of course, one is faced with the problem that the multiclass logits (aka raw scores) may not be calibrated to be used for binary classification by applying a fixed threshold. The authors call this sententiously \"Principle of logit separation\". Not too surprisingly, the standard multiclass losses do not have the desired property, however approaches that reduce multi-class to binary classification at training time do, namely unnormalized models with penalized log Z (self-normalization), the NCE approach, as well as (the natural in the proposed setting) binary classification loss. I find this almost a bit circular in the line of argumentation, but ok. It remains odd that while usually one has tried to reduce multiclass to binary, the authors go the opposite direction.\n\nThe main technical contribution of the paper is the batch-nornalization that makes sure that multiclass logits across mini-batches of data are better calibrated. One can almost think of that as an additional regularization. This seems interesting and does not create much overhead, if one applies mini-batched SGD optimization anyway. However, I feel this technique would need to be investigated with regard to general improvements in a multiclass setting and as such also benchmarked relative to other methods that could be applied. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Principle of Logit Separation","abstract":"We consider neural network training, in applications in which there are many possible classes, but at test-time, the task is to identify only whether the given example belongs to a specific class, which can be different in different applications of the classifier. For instance, this is the case in an image search engine. We consider the Single Logit Classification (SLC) task: training the network so that at test-time, it would be possible to accurately identify if the example belongs to a given class, based only on the output logit for this class. \nWe propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC. \nWe show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. In total, we study seven loss functions. \nOur experiments show that indeed in almost all cases, losses that are aligned with Principle of Logit Separation obtain a 20%-35% relative performance improvement in the SLC task, compared to losses that are not aligned with it. We therefore conclude that the Principle of Logit Separation sheds light on an important property of the most common loss functions used by neural network classifiers. ","pdf":"/pdf/054339a4093f4cd5943b735b0ac840d1c94303cc.pdf","paperhash":"anonymous|the_principle_of_logit_separation","_bibtex":"@article{\n  anonymous2018the,\n  title={The Principle of Logit Separation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1BRfhiab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper53/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222680560,"tcdate":1511725131544,"number":1,"cdate":1511725131544,"id":"B1mIOqdlz","invitation":"ICLR.cc/2018/Conference/-/Paper53/Official_Review","forum":"r1BRfhiab","replyto":"r1BRfhiab","signatures":["ICLR.cc/2018/Conference/Paper53/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A good solution to the problem of speeding up test-time classification is given. More motivation for the importance of the problem is needed","rating":"6: Marginally above acceptance threshold","review":"The paper is well-written which makes it easy to understand its main\nthrust - choosing loss functions so that at test time one can\naccurately (and speedily) determine whether an example is in a given\nclass, ie loss functions which are aligned with the \"Principle of Logit\nSeparation (PoLS)\". \n\nWhen the \"Principle of logit separation\" was first given (second page)\nI found it confusing and difficult to parse (too many \"any\"s, I could\nnot work out how the quantification worked). However, the formal\ndefinition (Definition 2.1) was fine. Why not just use this - and drop\nthe vague, wordy definition?\n\nThe paper is fairly 'gentle'. For example, we are taken through\nexamples of loss functions which satisfy \"PoLS\" and those which don't.\nNo 'deep' mathematical reasoning is required - but I don't see this as\na deficiency.\n\nThe experiments are reasonably chosen and, as expected, show the\nbenefits of using PoLS-aligned loss functions.\n\nMy criticism of the paper is that I don't think there is enough\nmotivation. We have that normal classification is linear in the number\nof classes. This modest computational burden (ie just linear),\napparently, is too slow for certain applications.  I would like more\nevidence for this, including some examples of this problem including\nin the paper. This is lacking from the current version.\n\n\ntypos, etc\n\nmax-maring -> max-margin\nthe seconds term -> the second term\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Principle of Logit Separation","abstract":"We consider neural network training, in applications in which there are many possible classes, but at test-time, the task is to identify only whether the given example belongs to a specific class, which can be different in different applications of the classifier. For instance, this is the case in an image search engine. We consider the Single Logit Classification (SLC) task: training the network so that at test-time, it would be possible to accurately identify if the example belongs to a given class, based only on the output logit for this class. \nWe propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC. \nWe show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. In total, we study seven loss functions. \nOur experiments show that indeed in almost all cases, losses that are aligned with Principle of Logit Separation obtain a 20%-35% relative performance improvement in the SLC task, compared to losses that are not aligned with it. We therefore conclude that the Principle of Logit Separation sheds light on an important property of the most common loss functions used by neural network classifiers. ","pdf":"/pdf/054339a4093f4cd5943b735b0ac840d1c94303cc.pdf","paperhash":"anonymous|the_principle_of_logit_separation","_bibtex":"@article{\n  anonymous2018the,\n  title={The Principle of Logit Separation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1BRfhiab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper53/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739511525,"tcdate":1508782796861,"number":53,"cdate":1509739508861,"id":"r1BRfhiab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1BRfhiab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Principle of Logit Separation","abstract":"We consider neural network training, in applications in which there are many possible classes, but at test-time, the task is to identify only whether the given example belongs to a specific class, which can be different in different applications of the classifier. For instance, this is the case in an image search engine. We consider the Single Logit Classification (SLC) task: training the network so that at test-time, it would be possible to accurately identify if the example belongs to a given class, based only on the output logit for this class. \nWe propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC. \nWe show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. In total, we study seven loss functions. \nOur experiments show that indeed in almost all cases, losses that are aligned with Principle of Logit Separation obtain a 20%-35% relative performance improvement in the SLC task, compared to losses that are not aligned with it. We therefore conclude that the Principle of Logit Separation sheds light on an important property of the most common loss functions used by neural network classifiers. ","pdf":"/pdf/054339a4093f4cd5943b735b0ac840d1c94303cc.pdf","paperhash":"anonymous|the_principle_of_logit_separation","_bibtex":"@article{\n  anonymous2018the,\n  title={The Principle of Logit Separation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1BRfhiab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper53/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}