{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222681267,"tcdate":1511858620270,"number":3,"cdate":1511858620270,"id":"rkEpbo5xz","invitation":"ICLR.cc/2018/Conference/-/Paper535/Official_Review","forum":"Sy5OAyZC-","replyto":"Sy5OAyZC-","signatures":["ICLR.cc/2018/Conference/Paper535/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper extensively compares simple word embedding based models (SWEMs) to RNN/CNN based-models on a suite of NLP tasks. \nExperiments on document classification, sentence classification, and natural language sequence matching show that SWEMs perform competitively or even better in the majority of cases.\nThe authors also propose to use max pooling to complement average pooling for combining information from word embeddings in a SWEM model to improve interpretability.\n\nWhile there is not much contribution in terms of technical novelty, I think this is an interesting paper that sheds new lights on limitations of existing methods for learning sentence and document representations. \nThe paper is well written and the experiments are quite convincing.\n- An interesting finding is that word embeddings are better for longer documents, whereas RNN/CNN models are better for shorter text. Do the authors have any sense on whether this is because of the difficulty in training an RNN/CNN model for long documents or whether compositions are not necessary since there are multiple predictive independent cues in a long text?\n- It would be useful to include a linear classification model that takes the word embeddings as an input in the comparison (SWEM-learned).\n- How crucial is it to retrain the word embeddings on the task of interest (from GloVe initialization) to obtain good performance?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Use of Word Embeddings Alone to Represent Natural Language Sequences","abstract":"To construct representations for natural language sequences, information from two main sources needs to be captured: (i) semantic meaning of individual words, and (ii) their compositionality. These two types of information are usually represented in the form of word embeddings and compositional functions, respectively. For the latter, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been considered. There has not been a rigorous evaluation regarding the relative importance of each component to different text-representation-based tasks; i.e., how important is the modeling capacity of word embeddings alone, relative to the added value of a compositional function? In this paper, we conduct an extensive comparative study between Simple Word Embeddings-based Models (SWEMs), with no compositional parameters, relative to employing word embeddings within RNN/CNN-based models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Moreover, in a new SWEM setup, we propose to employ a max-pooling operation over the learned word-embedding matrix of a given sentence. This approach is demonstrated to extract complementary features relative to the averaging operation standard to SWEMs, while endowing our model with better interpretability. To further validate our observations, we examine the information utilized by different models to make predictions, revealing interesting properties of word embeddings.\n","pdf":"/pdf/37da28cc60c8140c01c67b33e01f770987a52ab6.pdf","paperhash":"anonymous|on_the_use_of_word_embeddings_alone_to_represent_natural_language_sequences","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Use of Word Embeddings Alone to Represent Natural Language Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy5OAyZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper535/Authors"],"keywords":["Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222681309,"tcdate":1511820593630,"number":2,"cdate":1511820593630,"id":"rJ54aZ9gG","invitation":"ICLR.cc/2018/Conference/-/Paper535/Official_Review","forum":"Sy5OAyZC-","replyto":"Sy5OAyZC-","signatures":["ICLR.cc/2018/Conference/Paper535/AnonReviewer4"],"readers":["everyone"],"content":{"title":"A strong strong-baseline proposal","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word-embedding based models. Average and/or max pooling over word embeddings (which are initialized from pretrained embeddings) is used to obtain a fixed-length representation for natural language sequences, which is then fed through a single layer MLP classifier. In many of the 9 evaluation tasks, this approach is found to match or outperform single-layer CNNs or RNNs.\n\nThe varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis.\n\nMy only criticism would be the fact that the study is limited to English, even though the conclusions are explicitly scoped in light of this. Moreover, I wonder how well the findings would hold in a setting with a more severe OOV problem than is perhaps present in the studied datasets.\n\nBesides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets!\n\nMinor things:\n- It wasn't entirely clear how the text matching tasks are encoded. Are the two sequences combined into a single sequence before applying the model, or something else? I might have missed this detail.\n\n- Given the two ways of using the Glove embeddings for initialization (direct update vs mapping them with an MLP into the task space), it would be helpful to know which one ended up being used (i.e. optimal) in each setting.\n\n- Something went wrong with the font size for the remainder of the text near Figure 1.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Use of Word Embeddings Alone to Represent Natural Language Sequences","abstract":"To construct representations for natural language sequences, information from two main sources needs to be captured: (i) semantic meaning of individual words, and (ii) their compositionality. These two types of information are usually represented in the form of word embeddings and compositional functions, respectively. For the latter, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been considered. There has not been a rigorous evaluation regarding the relative importance of each component to different text-representation-based tasks; i.e., how important is the modeling capacity of word embeddings alone, relative to the added value of a compositional function? In this paper, we conduct an extensive comparative study between Simple Word Embeddings-based Models (SWEMs), with no compositional parameters, relative to employing word embeddings within RNN/CNN-based models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Moreover, in a new SWEM setup, we propose to employ a max-pooling operation over the learned word-embedding matrix of a given sentence. This approach is demonstrated to extract complementary features relative to the averaging operation standard to SWEMs, while endowing our model with better interpretability. To further validate our observations, we examine the information utilized by different models to make predictions, revealing interesting properties of word embeddings.\n","pdf":"/pdf/37da28cc60c8140c01c67b33e01f770987a52ab6.pdf","paperhash":"anonymous|on_the_use_of_word_embeddings_alone_to_represent_natural_language_sequences","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Use of Word Embeddings Alone to Represent Natural Language Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy5OAyZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper535/Authors"],"keywords":["Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222681352,"tcdate":1511634220646,"number":1,"cdate":1511634220646,"id":"r1HVB4DxG","invitation":"ICLR.cc/2018/Conference/-/Paper535/Official_Review","forum":"Sy5OAyZC-","replyto":"Sy5OAyZC-","signatures":["ICLR.cc/2018/Conference/Paper535/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of \"On the Use of Word Embeddings...\"","rating":"5: Marginally below acceptance threshold","review":"This paper empirically investigates the differences realized by using compositional functions over word embeddings as compared to directly operating the word embeddings. That is, the authors seek to explore the advantages afforded by RNN/CNN based models that induce intermediate semantic representations of texts, as opposed to simpler (parameter-free) approaches to composing these, like addition. \n\nIn sum, I think this is exploration is interesting, and suggests that we should perhaps experiment more regularly with simple aggregation methods like SWEM. On the other hand, the differences across the models is relatively modest, and the data resists clear conclusions, so I'm not sure that the work will be very impactful. In my view, then, this work does constitute a contribution, albeit a modest one. I do think the general notion of attempting to simplify models until performance begins to degrade is a fruitful path to explore, as models continue to increase in complexity despite compelling evidence that this is always needed.\n\nStrengths\n---\n+ This paper does highlight a gap in existing work, as far as I am aware: namely, I am not sure that there are generally known trade-offs associated with different compositional models over token embeddings for NLP. However, it is not clear that we should expect there to be a consistent result to this question across all NLP tasks.\n\n+ The results are marginally surprising, insofar as I would have expected the CNN/RNN (particularly the former) to dominate the simpler aggregation approaches, and this does not seem borne out by the data. Although this trend is seemingly reversed on the short text data, muddying the story. \n\nWeaknesses\n---\n- There are a number of important limitations here, many of which the authors themselves note, which mitigate the implications of the reported results. First, this is a small set of tasks, and results may not hold more generally. It would have been nice to see some work on Seq2Seq tasks, or sequence tagging tasks at least. \n\n- I was surprised to see no mention of the \"Fixed-Size Ordinally-Forgetting Encoding Method\" (FOFE) proposed by Zhang et al. in 2015, which would seem to be a natural point of comparison here, given that it sits in a sweet spot of being simple and efficient while still expressive enough to preserve word-order information. This actually seems like a pretty glaring omission given that it meets many of the desiderata the authors put forward. \n\n- The interpretability angle discussed seems underdeveloped. I'm not sure that being able to identify individual words (as the authors have listed) meaningfully constitutes \"interpretability\" -- standard CNNs, e.g., lend themselves to this as well by tracing back through the filter activations. \n\n- Some of the questions addressed seem tangential to the main question of the paper -- e.g., word vector dimensionality seems an orthogonal issue to the composition function, and would influence performance for the more complex architectures as well.\n\nSmaller comments\n---\n- On page 1, the authors write \"By representing each word as a fixed-length vector, these embeddings can group semantically similar words, while explicitly encoding rich linguistic regularities and patterns\", but actually I would say that these *implicitly* encode such regularities, rather than explicitly. \n\n- \"architecture in Kim 2014; Collobert et al. 2011; Gan et al. 2017\" -- citation formatting a bit weird here.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Use of Word Embeddings Alone to Represent Natural Language Sequences","abstract":"To construct representations for natural language sequences, information from two main sources needs to be captured: (i) semantic meaning of individual words, and (ii) their compositionality. These two types of information are usually represented in the form of word embeddings and compositional functions, respectively. For the latter, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been considered. There has not been a rigorous evaluation regarding the relative importance of each component to different text-representation-based tasks; i.e., how important is the modeling capacity of word embeddings alone, relative to the added value of a compositional function? In this paper, we conduct an extensive comparative study between Simple Word Embeddings-based Models (SWEMs), with no compositional parameters, relative to employing word embeddings within RNN/CNN-based models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Moreover, in a new SWEM setup, we propose to employ a max-pooling operation over the learned word-embedding matrix of a given sentence. This approach is demonstrated to extract complementary features relative to the averaging operation standard to SWEMs, while endowing our model with better interpretability. To further validate our observations, we examine the information utilized by different models to make predictions, revealing interesting properties of word embeddings.\n","pdf":"/pdf/37da28cc60c8140c01c67b33e01f770987a52ab6.pdf","paperhash":"anonymous|on_the_use_of_word_embeddings_alone_to_represent_natural_language_sequences","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Use of Word Embeddings Alone to Represent Natural Language Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy5OAyZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper535/Authors"],"keywords":["Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739249574,"tcdate":1509125746219,"number":535,"cdate":1509739246915,"id":"Sy5OAyZC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy5OAyZC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the Use of Word Embeddings Alone to Represent Natural Language Sequences","abstract":"To construct representations for natural language sequences, information from two main sources needs to be captured: (i) semantic meaning of individual words, and (ii) their compositionality. These two types of information are usually represented in the form of word embeddings and compositional functions, respectively. For the latter, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been considered. There has not been a rigorous evaluation regarding the relative importance of each component to different text-representation-based tasks; i.e., how important is the modeling capacity of word embeddings alone, relative to the added value of a compositional function? In this paper, we conduct an extensive comparative study between Simple Word Embeddings-based Models (SWEMs), with no compositional parameters, relative to employing word embeddings within RNN/CNN-based models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Moreover, in a new SWEM setup, we propose to employ a max-pooling operation over the learned word-embedding matrix of a given sentence. This approach is demonstrated to extract complementary features relative to the averaging operation standard to SWEMs, while endowing our model with better interpretability. To further validate our observations, we examine the information utilized by different models to make predictions, revealing interesting properties of word embeddings.\n","pdf":"/pdf/37da28cc60c8140c01c67b33e01f770987a52ab6.pdf","paperhash":"anonymous|on_the_use_of_word_embeddings_alone_to_represent_natural_language_sequences","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Use of Word Embeddings Alone to Represent Natural Language Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy5OAyZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper535/Authors"],"keywords":["Natural Language Processing","Deep Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}