{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222560001,"tcdate":1511847009189,"number":2,"cdate":1511847009189,"id":"B1YP4d5lf","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Review","forum":"HJIhGXWCZ","replyto":"HJIhGXWCZ","signatures":["ICLR.cc/2018/Conference/Paper1148/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting model, baselines are weak, not enough signal on it's generalization ability","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a model for prediction under uncertainty where the separate out deterministic component prediction and uncertain component prediction.\nThey propose to have a predictor for deterministic information generation using a standard transformer trained via MSE.\nFor the non-deterministic information, they have a residual predictor that uses a low-dimensional latent space. This low-dim latent space is first predicted from the residual of the (deterministic prediction - groundtruth), and then the low-dim encoding goes into a network that predicts a corrected image.\nThe subtleness of this work over most other video prediction work is that it isn't conditioned on a labeled latent space (like text to video prediction, for example). Hence inferring a structured latent space is a challenge.\nThe training procedure follows an alternative minimization in EM style.\n\nThe biggest weakness of the paper (and the reason for my final decision) is that the paper completely goes easy on baseline models. It's only baseline is a GAN model that isn't even very convincing (GANs are finicky to train, so is this a badly tuned GAN model? or did you spend a lot of time tuning it?).\n\nBecause of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline. Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try. In fact, a VAE would be nicely suited when proposing to work with low-dimensional latent spaces.\n\nThe main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.\nIf the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.\n\n[1] http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222560040,"tcdate":1511708726693,"number":1,"cdate":1511708726693,"id":"SykHOLdxz","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Review","forum":"HJIhGXWCZ","replyto":"HJIhGXWCZ","signatures":["ICLR.cc/2018/Conference/Paper1148/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nicely written, lack of comparison to other methods, poor results","rating":"3: Clear rejection","review":"This paper introduce a times-series prediction model that works in two phases. First learns a deterministic mapping from x to y. And then train another net to predict future frames given the input and residual error from the first network. And does sampling for novel inputs by sampling the residual error collected from the training set. \n\nPros:\nThe paper is well written and easy to follow.\nGood cover of relevant work in sec 3.\n\nCons\nThe paper emphasis on the fact the their modeling multi-modal time series distributions, which is almost the case for most of the video sequence data. But unfortunately doesnâ€™t show any results even qualitative like generated samples for other  work on next frame video prediction. The shown samples from model looks extremely, low quality and really hard to see the authors interpretations of it.\n\nThere are many baselines missing. One simple one would be what if they only used the f and draw z samples for N(0,1)? VAE is very power latent variable model which also not being compared against. It is not clear what implantation of GAN they are using?.Vanilla GAN is know to be hard to train and there has been many variants recently that overcome some of those difficulties and its mode collapse problem. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510361416553,"tcdate":1510361416553,"number":4,"cdate":1510361416553,"id":"SkbLY6QyM","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Comment","forum":"HJIhGXWCZ","replyto":"B1gd9XXkG","signatures":["ICLR.cc/2018/Conference/Paper1148/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1148/Authors"],"content":{"title":"Re^3: Relation to prior work in predictive coding","comment":"After the review period, we will have the opportunity to update the paper based on reviewer and public comments, so we will post a revised version then :)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510360571819,"tcdate":1510360571819,"number":3,"cdate":1510360571819,"id":"BkNb8pXyM","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Comment","forum":"HJIhGXWCZ","replyto":"SJb-YQmyG","signatures":["ICLR.cc/2018/Conference/Paper1148/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1148/Authors"],"content":{"title":"Re: Evaluation Setting","comment":"Thank you for the comment. Although there are indeed many works on video prediction, these are generally deterministic and we are not aware of other work on video data that performs multi-modal prediction other than (Goroshin et. al, 2015) and (Vondrick et. al 2015) mentioned in related work, who perform alternating minimization over latent variables. The focus of these works is different from ours in that Vondrick et. al perform prediction of high-level representations using a pre-trained network (rather than pixels) with the goal of predicting future actions or objects appearing in the video, and Goroshin et. al focus primarily on learning linearized representations and only apply the latent variable version of their model to very simple settings. We tried alternating minimization in early experiments on simple tasks, and found that it performed similarly or worse than our method (in terms of loss) while being considerably slower due to the inner optimization loop and also introduced new hyperparameters to tune such as the learning rate and number of iterations in the inner loop. Comparing to GANs seemed appropriate, since they are a widely used method which can in principle perform multi-modal generations (although as noted in the paper, they can suffer from mode collapse especially in the conditional setting). \n\nTo our knowledge, PSNR (along with SSIM) is one of the more common metrics for evaluating video generations, and is used in several works, for example:\n\nhttps://arxiv.org/pdf/1511.05440.pdf\nhttps://arxiv.org/pdf/1605.08104.pdf\nhttps://arxiv.org/pdf/1605.07157.pdf\nhttps://arxiv.org/pdf/1706.08033.pdf\n\nWe also computed SSIM, and found that the EEN performance also increases with the number of samples, although the difference is less pronounced than with PSNR or MSE. Note that the SSIM contains terms which compare statistics taken over windows of the image, meaning that small changes in object location between two images (for example, the paddle moving in Breakout) may not be reflected as much with this metric. However, we can include this metric along with MSE or others if the reviewers think it is appropriate.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510320743599,"tcdate":1510320743599,"number":4,"cdate":1510320743599,"id":"B1gd9XXkG","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Public_Comment","forum":"HJIhGXWCZ","replyto":"SyXTXQY0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re^2: Relation to prior work in predictive coding","comment":"> We will update the Related Work section in the revised paper to include reference to this work. \n\nI'm interested in the next version.\nHow do you publish the revised paper? Do you have any plans for that?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510320376720,"tcdate":1510320376720,"number":3,"cdate":1510320376720,"id":"SJb-YQmyG","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Public_Comment","forum":"HJIhGXWCZ","replyto":"HJIhGXWCZ","signatures":["~Xin_Yang1"],"readers":["everyone"],"writers":["~Xin_Yang1"],"content":{"title":"Evaluation Setting","comment":"While the authors provide some demonstration on Youtube, I'm unsure that the approach really improve the performance, \nas there is speculation about evaluation setting.\nAlthough there are lot of works for this area, the authors compares the method with GAN only.\nPNSR, the criterion author showed, is not common.\n\n(This is minor comment)\nYou need not to show all the URLs for each paper in the reference section."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092430441,"tcdate":1509664371494,"number":2,"cdate":1509664371494,"id":"Sko_8QFAb","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Comment","forum":"HJIhGXWCZ","replyto":"H1MzsbDRW","signatures":["ICLR.cc/2018/Conference/Paper1148/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1148/Authors"],"content":{"title":"Re: Question about approach","comment":"Thank you for the question. We actually do want *some* information from the target to seep through the latent variable z, i.e. we would like z to encode information about y which is not predictable from x. For example, if x and y are consecutive images and a new object which was not present in x appears in y from outside the frame (and cannot be predicted from x), we would like this information to be encoded in the latent variable z. \n\nHowever, it is true that we do not want z to encode information about y which could be predicted from x. The fact that z is of much lower dimension than y forces the network to compress the inherently unpredictable part of y in such a way that z must be combined with x to reconstruct y. This low dimensionality of z prevents the network from learning \\phi^{-1}(z) = g(x) - y. In most of our experiments, y is a set of 4 images of dimensions ranging in size from 84x84 to 240x240 (i.e. high-dimensional) whereas z has between 2 and 32 dimensions. In our video generations we condition on a set of frames from the test set, but use z vectors that are extracted from the disjoint training set. If a z vector encoded a lot of information about the specific target used to compute it (rather than general features such as \"the paddle moves left\" or \"a new pipe appears at this height\"), then there would be a mismatch between the conditioning frames and the generated frames (for example, different backgrounds), which does not appear to be the case. \n\nWe will clarify this in the updated paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092430491,"tcdate":1509663675107,"number":1,"cdate":1509663675107,"id":"SyXTXQY0-","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Official_Comment","forum":"HJIhGXWCZ","replyto":"SkdGXtICZ","signatures":["ICLR.cc/2018/Conference/Paper1148/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1148/Authors"],"content":{"title":"Re: Relation to prior work in predictive coding","comment":"Thank you for the references. Our model is indeed related to predictive coding models in the sense that both of them pass prediction errors between different parts of the network. They differ in that predictive coding models pass the error upwards to higher layers in the network at the same timestep, whereas our model passes the (compressed) error from the deterministic network backwards in time to serve as input to the latent variable network at the previous timestep.\n\nThese different designs serve different purposes. Schmidhuber's 1992 chunker passes incorrectly predicted inputs to the higher layers so they can be predicted from states further in the past. This is a way to address the vanishing gradient problem by allowing information to flow more easily across compressible parts of the sequence through the higher levels of the network (note that they test their model on deterministic sequences with long time lags). In contrast, our model is aimed at capturing multimodality in the time series by passing a small amount of information backward in time (which encodes the mode information of the future state), allowing the f network to make a more accurate prediction conditioned on the correct mode rather than predicting the average of different possible modes. \n\nWe will update the Related Work section in the revised paper to include reference to this work. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509581158576,"tcdate":1509526282259,"number":2,"cdate":1509526282259,"id":"H1MzsbDRW","invitation":"ICLR.cc/2018/Conference/-/Paper1148/Public_Comment","forum":"HJIhGXWCZ","replyto":"HJIhGXWCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question about approach","comment":"How can you be sure that no information of the target seeps through the latent variable $z$? After all, f only needs to learn to undo $\\phi$ and that $\\phi^{-1}(z) = x - y \\Leftrightarrow y = x - \\phi^{-1}(z)$."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092379646,"tcdate":1509139117651,"number":1148,"cdate":1510092359414,"id":"HJIhGXWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJIhGXWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Prediction Under Uncertainty with Error Encoding Networks","abstract":"In this work we introduce a new framework for performing temporal predictions\nin the presence of uncertainty. It is based on a simple idea of disentangling com-\nponents of the future state which are predictable from those which are inherently\nunpredictable, and encoding the unpredictable components into a low-dimensional\nlatent variable which is fed into the forward model. Our method uses a simple su-\npervised training objective which is fast and easy to train. We evaluate it in the\ncontext of video prediction on multiple datasets and show that it is able to consi-\ntently generate diverse predictions without the need for alternating minimization\nover a latent space or adversarial training.","pdf":"/pdf/bd3b0e1996f51903fe07077607eeae4c2b1bbafd.pdf","TL;DR":"A simple and easy to train method for multimodal prediction in time series. ","paperhash":"anonymous|prediction_under_uncertainty_with_error_encoding_networks","_bibtex":"@article{\n  anonymous2018prediction,\n  title={Prediction Under Uncertainty with Error Encoding Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJIhGXWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1148/Authors"],"keywords":[]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}