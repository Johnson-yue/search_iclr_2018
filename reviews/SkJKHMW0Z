{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222788232,"tcdate":1511843680817,"number":3,"cdate":1511843680817,"id":"rJFvPvqgz","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The proposed method should be better explained","rating":"5: Marginally below acceptance threshold","review":"This paper describes a method called relational network to add relational reasoning capacity to deep neural networks. The previous approach can only perform a single step of relational reasoning, and was evaluated on problems that require at most three steps. The current method address the scalability issue and can solve tasks with orders of magnitude more steps of reasoning. The proposed methods are evaluated on two problems, Sudoku and Babi, and achieved state-of-the-art results. \n\nThe proposed method should be better explained. What’s the precise definition of interface? It’s claimed that other constraint propagation-based methods can solve Sudoku problems easily, but don’t respect the interface. It is hard to appreciate without a precise definition of interface. The proposed recurrent relational networks are only defined informally. A definition of the model as well as related algorithms should be defined more formally. \n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"A core component of human intelligence is the ability to reason about objects and their interactions which is something even state-of-the-art deep learning models struggle with. Santoro et al. (2017) introduced the relational network to add such relational reasoning capacity to deep neural networks but the proposed network is severely limited in the complexity of the reasoning it can perform. We introduce the recurrent relational network which can solve tasks requiring an order of magnitude more steps of reasoning. We apply it to solving Sudoku puzzles and achieve state-of-the-art results solving 96.6% of the hardest Sudoku puzzles. For comparison the relational network fails to solve any puzzles. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state- of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can be added to any neural network model to add a powerful relational reasoning capacity.","pdf":"/pdf/d1342b83717b10f55f999ae90340abda341f384c.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222788278,"tcdate":1511811730296,"number":2,"cdate":1511811730296,"id":"Syc551clG","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, clearly presented new structured prediction algorithm. Paper marginally below acceptance threshold.","rating":"5: Marginally below acceptance threshold","review":"This paper introduces recurrent relational networks: a deep neural network for structured prediction (or relational reasoning). The authors use it to achieve state-of-the-art performance on Soduku puzzles and the BaBi task (a text based QA dataset designed as a set of to toy prerequisite tasks for reasoning).\n\nOverall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn't show it has a big impact (could do so by adding more tasks - as they suggest in the discussion). This is why I think the paper is marginally below the acceptance threshold but could be convinced otherwise.\n\nC an the authors give experimental evidences for their claim: \"As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy...\" - \n\nPros\n- The idea of the paper is clearly presented, the algorithm is easy to follow.\n- The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.\n\nCons\n- The recurrent relational networks is basically a complex learned message passing algorithm. As the authors themselves state there are several works from recent years which also tackle this (one missing reference is Deeply Learning the Messages in Message Passing Inference of Lin et al from NIPS 2016). It would been interesting to compare results to these algorithms.\n- For the Sudoku the proposed architecture of the network seems a bit to complex, for example why do a 16 embedding is needed for representing a digit between 0-9? Some other choices (batch size of 252) seem very specific.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"A core component of human intelligence is the ability to reason about objects and their interactions which is something even state-of-the-art deep learning models struggle with. Santoro et al. (2017) introduced the relational network to add such relational reasoning capacity to deep neural networks but the proposed network is severely limited in the complexity of the reasoning it can perform. We introduce the recurrent relational network which can solve tasks requiring an order of magnitude more steps of reasoning. We apply it to solving Sudoku puzzles and achieve state-of-the-art results solving 96.6% of the hardest Sudoku puzzles. For comparison the relational network fails to solve any puzzles. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state- of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can be added to any neural network model to add a powerful relational reasoning capacity.","pdf":"/pdf/d1342b83717b10f55f999ae90340abda341f384c.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222788331,"tcdate":1511627867042,"number":1,"cdate":1511627867042,"id":"r17v3MDxG","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Very nice direction but no complex relational reasoning demonstrated and missing related work","rating":"3: Clear rejection","review":"The paper introduced recurrent relational network (RRNs), an enhanced version of the\nexisting relational network, that can be added to any neural networks to add\nrelational reasoning capacity. RRNs are illustrated on sudoku puzzles and textual QA.\n\nOverall the paper is well written and structured. It also addresses an important research question: combining relational reasoning and neural networks is currently receiving a lot of attention, in particular when generally considering the question of bridging sub-symbolic and symbolic methods. Unfortunately, it is current form, the paper has two major downsides. First of all,  the sudoku example does not illustrate “complex relational reasoning” as claimed in the title. The problem is encoded at a positional level where \nmessages encoded as MLPs and LSTMs implement the constraints for sudoko. Indeed, \nthis allows to realise end-to-end learning but does not illustrate complex reasoning. \nThis is also reflected in the considered QA task, which is essentially coded as a positional problem. Consequently, the claim of the conclusions, namely that “we have\nproposed a general relational reasoning model” is not validated, unfortunately. Such\na module that can be connected to any existing neural network would be great. However, \nfor that one should show capabilities of relational logic. Some standard (noisy) \nreasoning capabilities such as modus ponens. This also leads me to the second downside. \nUnfortunately, the paper falls short on discussion related work. First of all, \nthere is the large field of statistical relational learning, see \n\nLuc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole:\nStatistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers 2016\n\nfor a recent overview. As it has the very same goals, while not using a neural architecture for implementation, it is very much related and has to be discussed. That\none can also use a neural implementation can be seen in \n\nIvan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:\nLogic Tensor Networks for Semantic Image Interpretation. IJCAI 2017: 1596-1602\n\nMatko Bosnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel:\nProgramming with a Differentiable Forth Interpreter. ICML 2017: 547-556\n\nLuciano Serafini, Artur S. d'Avila Garcez:\nLearning and Reasoning with Logic Tensor Networks. AI*IA 2016: 334-348\n\nGustav Sourek, Vojtech Aschenbrenner, Filip Zelezný, Ondrej Kuzelka:\nLifted Relational Neural Networks. CoCo@NIPS 2015\n\nTim Rocktäschel, Sebastian Riedel:\nEnd-to-end Differentiable Proving. CoRR abs/1705.11040 (2017)\n\nWilliam W. Cohen, Fan Yang, Kathryn Mazaitis:\nTensorLog: Deep Learning Meets Probabilistic DBs. CoRR abs/1707.05390 (2017)\n\nto list just some approaches. There are also (deep) probabilistic programming \napproaches such as Edward that should be mentioned as CPS like problems (Sudoku) can\ndefinitely be implement there. Moreover, there is a number of papers that discuss \nembeddings of relational data and rules such as \n\nWilliam Yang Wang, William W. Cohen:\nLearning First-Order Logic Embeddings via Matrix Factorization. IJCAI 2016: 2132-2138\n\nThomas Demeester, Tim Rocktäschel, Sebastian Riedel:\nLifted Rule Injection for Relation Embeddings. EMNLP 2016: 1389-1399\n\nand even neural-symbolic approaches with a long publication history. Unfortunately, \nnon of these approaches has been cited, giving the wrong impression that this is \nthe first paper that tackles the long lasting question of merging sub-symbolic and symbolic reasoning. BTW, there have been also other deep networks for optimisation, see e.g. \n\nBrandon Amos, J. Zico Kolter:\nOptNet: Differentiable Optimization as a Layer in Neural Networks. \nICML 2017: 136-145\n\nthat have also considered Sudoku. To summarise, I like very much the direction of the paper but it seems to be too early to be published. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"A core component of human intelligence is the ability to reason about objects and their interactions which is something even state-of-the-art deep learning models struggle with. Santoro et al. (2017) introduced the relational network to add such relational reasoning capacity to deep neural networks but the proposed network is severely limited in the complexity of the reasoning it can perform. We introduce the recurrent relational network which can solve tasks requiring an order of magnitude more steps of reasoning. We apply it to solving Sudoku puzzles and achieve state-of-the-art results solving 96.6% of the hardest Sudoku puzzles. For comparison the relational network fails to solve any puzzles. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state- of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can be added to any neural network model to add a powerful relational reasoning capacity.","pdf":"/pdf/d1342b83717b10f55f999ae90340abda341f384c.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739073266,"tcdate":1509135735227,"number":838,"cdate":1509739070602,"id":"SkJKHMW0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"A core component of human intelligence is the ability to reason about objects and their interactions which is something even state-of-the-art deep learning models struggle with. Santoro et al. (2017) introduced the relational network to add such relational reasoning capacity to deep neural networks but the proposed network is severely limited in the complexity of the reasoning it can perform. We introduce the recurrent relational network which can solve tasks requiring an order of magnitude more steps of reasoning. We apply it to solving Sudoku puzzles and achieve state-of-the-art results solving 96.6% of the hardest Sudoku puzzles. For comparison the relational network fails to solve any puzzles. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state- of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can be added to any neural network model to add a powerful relational reasoning capacity.","pdf":"/pdf/d1342b83717b10f55f999ae90340abda341f384c.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}