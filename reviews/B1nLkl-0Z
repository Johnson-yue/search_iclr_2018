{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222682378,"tcdate":1511796182934,"number":2,"cdate":1511796182934,"id":"ByJ1CsYgG","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces smoothed Q-values, a tweak on standard Q-functions. It demonstrates some nice theoretical properties and reasonably successful experiments. The paper is interesting and correct, but unlikely to make a big impact.","rating":"6: Marginally above acceptance threshold","review":"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.  It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.  Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG.\n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.  The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.\n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.  In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact.\n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222682420,"tcdate":1511753249966,"number":1,"cdate":1511753249966,"id":"S1qQ8ZFlf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer1"],"readers":["everyone"],"content":{"title":"seems to be a good paper, however, I do not even see an exact algorithm formulation","rating":"6: Marginally above acceptance threshold","review":"I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution. The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \"expected policy gradient\" by Ciosek & Whiteson. All these seems very sound and interesting.\n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical). Such representation style is highly discouraging and brings about un-necessary readability difficulties. \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is). So I suggest moving them to the Appendix and make the major focus more narrowed down.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739245678,"tcdate":1509125971867,"number":542,"cdate":1509739243016,"id":"B1nLkl-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}