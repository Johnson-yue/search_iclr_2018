{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222678108,"tcdate":1511801157982,"number":3,"cdate":1511801157982,"id":"S1CSbaKez","invitation":"ICLR.cc/2018/Conference/-/Paper513/Official_Review","forum":"HyjC5yWCW","replyto":"HyjC5yWCW","signatures":["ICLR.cc/2018/Conference/Paper513/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review (educated guess)","rating":"7: Good paper, accept","review":"The paper provides proof that gradient-based meta-learners (e.g. MAML) are \"universal leaning algorithm approximators\".\n\nPro:\n- Generally well-written with a clear (theoretical) goal\n- If the K-shot proof is correct*, the paper constitutes a significant contribution to the theoretical understanding of meta-learning.\n- Timely and relevant to a large portion of the ICLR community (assuming the proofs are correct)\n\nCon:\n- The theoretical and empirical parts seem quite disconnected. The theoretical results are not applied nor demonstrated in the empirical section and only functions as an underlying premise. I wonder if a purely theoretical contribution would be preferable (or with even fewer empirical results).\n\n* It has not yet been possible for me to check all the technical details and proofs.\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm","abstract":"Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.","pdf":"/pdf/bc48c0f38a6efb728ef20a8e43937b74f0e22784.pdf","TL;DR":"Deep representations combined with gradient descent can approximate any learning algorithm.","paperhash":"anonymous|metalearning_and_universality_deep_representations_and_gradient_descent_can_approximate_any_learning_algorithm","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyjC5yWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper513/Authors"],"keywords":["meta-learning","learning to learn","universal function approximation"]}},{"tddate":null,"ddate":null,"tmdate":1512222678148,"tcdate":1511774596751,"number":2,"cdate":1511774596751,"id":"SyTFKLYgf","invitation":"ICLR.cc/2018/Conference/-/Paper513/Official_Review","forum":"HyjC5yWCW","replyto":"HyjC5yWCW","signatures":["ICLR.cc/2018/Conference/Paper513/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Result looks interesting. Presentation could be further improved.","rating":"6: Marginally above acceptance threshold","review":"The paper tries to address an interesting question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm. The authors provide answers, both theoretically and empirically.\n\nThe presentation could be further improved. For example, \n\n-the notation $\\mathcal{L}$ is inconsistent. It has different inputs at each location.\n-the bottom of page 5, \"we then define\"?\n-I couldn't understand the sentence \"can approximate any continuous function of (x,y,x^*) on compact subsets of R^{dim(y)}\" in Lemma 4.1\". \n-before Equation (1), \"where we will disregard the last term..\" should be further clarified.\n-the paragraph before Section 4. \"The first goal of this paper is to show that f_{MAML} is a universal function approximation of (D_{\\mathcal{T}},x^*)\"? A function can only approximate the same type function.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm","abstract":"Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.","pdf":"/pdf/bc48c0f38a6efb728ef20a8e43937b74f0e22784.pdf","TL;DR":"Deep representations combined with gradient descent can approximate any learning algorithm.","paperhash":"anonymous|metalearning_and_universality_deep_representations_and_gradient_descent_can_approximate_any_learning_algorithm","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyjC5yWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper513/Authors"],"keywords":["meta-learning","learning to learn","universal function approximation"]}},{"tddate":null,"ddate":null,"tmdate":1512222678191,"tcdate":1511769175254,"number":1,"cdate":1511769175254,"id":"ByJP4Htez","invitation":"ICLR.cc/2018/Conference/-/Paper513/Official_Review","forum":"HyjC5yWCW","replyto":"HyjC5yWCW","signatures":["ICLR.cc/2018/Conference/Paper513/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Technically interesting work but practical significance seems highly questionable  ","rating":"4: Ok but not good enough - rejection","review":"This paper studies the capacity of the model-agnostic meta-learning (MAML) framework as a universal learning algorithm approximator. Since a (supervised) learning algorithm can be interpreted as a map from a dataset and an input to an output, the authors define a universal learning algorithm approximator to be a universal function approximator over the set of functions that map a set of data points and an  input to an output. The authors show constructively that there exists a neural network architecture for which the model learned through MAML can approximate any learning algorithm. \n\nThe paper is for the most part clear, and the main result seems original and technically interesting. At the same time, it is not clear to me that this result is also practically significant. This is because the universal approximation result relies on a particular architecture that is not necessarily the design one would always use in MAML. This implies that MAML as typically used (including in the original paper by Finn et al, 2017a) is not necessarily a universal learning algorithm approximator, and this paper does not actually justify its empirical efficacy theoretically. For instance, the authors do not even use the architecture proposed in their proof in their experiments. This is in contrast to the classical universal function approximator results for feedforward neural networks, as a single hidden layer feedforward network is often among the family of architectures considered in the course of hyperparameter tuning. This distinction should be explicitly discussed in the paper.  Moreover, the questions posed in the experimental results do not seem related to the theoretical result, which seems odd.\n\nSpecific comments and questions: \nPage 4: \"\\hat{f}(\\cdot; \\theta') approximates f_{\\text{target}}(x, y, x^*) up to arbitrary position\". There seems to be an abuse of notation here as the first expression is a function and the second expression is a value.\nPage 4: \"to show universality, we will construct a setting of the weight matrices that enables independent control of the information flow...\". How does this differ from the classical UFA proofs? The relative technical merit of this paper would be more clear if this is properly discussed.\nPage 4: \"\\prod_{i=1}^N (W_i - \\alpha \\nabla_{W_i})\". There seems to be a typo here: \\nabla_{W_i} should be \\nabla_{W_i} L.\nPage 7: \"These error functions effectively lose information because simply looking at their gradient is insufficient to determine the label.\" It would be interesting the compare the efficacy of MAML on these error functions as compared to cross entropy and mean-squared error.\nPage 7: \"(1) can a learner trained with MAML further improve from additional gradient steps when learning new tasks at test time...? (2) does the inductive bias of gradient descent enable better few-shot learning performance on tasks outside of the training distribution...?\". These questions seem unrelated to the universal learning algorithm approximator result that constitutes the main part of the paper. If you're going to study these question empirically, why didn't you also try to investigate them theoretically (e.g. sample complexity and convergence of MAML)? A systematic and comprehensive analysis of these questions from both a theoretical and empirical perspective would have constituted a compelling paper on its own.\nPages 7-8: Experiments. What are the architectures and hyperparameters used in the experiments, and how sensitive are the meta-learning algorithms to their choice?\nPage 8: \"our experiments show that learning strategies acquired with MAML are more successful when faced with out-of-domain tasks compared to recurrent learners....we show that the representations acquired with MAML are highly resilient to overfitting\". I'm not sure that such general claims are justified based on the experimental results in this paper. Generalizing to out-of-domain tasks is heavily dependent on the specific level and type of drift between the old and new distributions. These properties aren't studied at all in this work.   \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm","abstract":"Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.","pdf":"/pdf/bc48c0f38a6efb728ef20a8e43937b74f0e22784.pdf","TL;DR":"Deep representations combined with gradient descent can approximate any learning algorithm.","paperhash":"anonymous|metalearning_and_universality_deep_representations_and_gradient_descent_can_approximate_any_learning_algorithm","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyjC5yWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper513/Authors"],"keywords":["meta-learning","learning to learn","universal function approximation"]}},{"tddate":null,"ddate":null,"tmdate":1509739261737,"tcdate":1509124818978,"number":513,"cdate":1509739259074,"id":"HyjC5yWCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyjC5yWCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm","abstract":"Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.","pdf":"/pdf/bc48c0f38a6efb728ef20a8e43937b74f0e22784.pdf","TL;DR":"Deep representations combined with gradient descent can approximate any learning algorithm.","paperhash":"anonymous|metalearning_and_universality_deep_representations_and_gradient_descent_can_approximate_any_learning_algorithm","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyjC5yWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper513/Authors"],"keywords":["meta-learning","learning to learn","universal function approximation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}