{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222823206,"tcdate":1511776595726,"number":3,"cdate":1511776595726,"id":"H12U-wYgG","invitation":"ICLR.cc/2018/Conference/-/Paper937/Official_Review","forum":"SkaPsfZ0W","replyto":"SkaPsfZ0W","signatures":["ICLR.cc/2018/Conference/Paper937/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea to boost GCN.","rating":"6: Marginally above acceptance threshold","review":"The paper presents a Network of Graph Convolutional Networks (NGCNs) that uses\nrandom walk statistics to extract information from near and distant neighbors\nin the graph.\n\nThe authors show that a 2-layer Graph Convolutional Network, with linear\nactivation and W0 as identity matrix, reduces to a one-step random walk.\nThey build on this notion to  introduce the idea to make the GCN directly operate\non random walk statistics to better model information across distant nodes.\n\nGiven that it is not clear how many steps of random walk to use a-priori it is\nproposed to make a mixture of models whose outputs are combined by a\nsoftmax classifier, or by an attention based mixing (learning the mixing coefficients).\n\nI find that the comparison can be considered slightly unfair as NGCN has k-times\nthe number of GCN models in it. Did the authors compare with a deeper GCN, or\nsimply with a mixture of plain GCN using one-step random walk?\nThe datasets used for comparison are extremely simple, and I am glad that the\nauthors point out that this is a significant issue for benchmark driven research.\nHowever, doing calibration on a subset of the validation nodes via gradient\ndescent is not very clean as by doing it one implicitly uses those nodes for training.\nThe improvement of the calibrated model on 5 nodes per class (Table 3) seems\nto hint that this peeking into the validation is indeed happening.\n\nThe authors mention that feeding explicitly the information on distant nodes\nmakes learning easier and that otherwise such information it would be hard to\nextract from stacking several GCN layers. While this is true for the small datasets\nusually considered it is not clear at all whether this still holds when we will\nhave large scale graph benchmarks.\n\nExperiments are well conducted but lack a comparison with GraphSAGE and MoNet,\nwhich are the reference models for the selected benchmarks. A comparison would have made the contribution stronger in my opinion. Improvements in performance are minor\nexcept for decimated inputs setting reported in Table 3. In this last case though\nno statistics over multiple runs are shown.\n\nOverall I like the interpretation, even if a bit forced, of GCN as using one-step\nrandom walk statistics. The paper is clearly written.\nThe main issue I have with the approach is that it does not bring a very novel\nway to perform deep learning on graphs, but rather improves marginally upon\na well established one.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222823250,"tcdate":1511684791518,"number":2,"cdate":1511684791518,"id":"H1kTclOlf","invitation":"ICLR.cc/2018/Conference/-/Paper937/Official_Review","forum":"SkaPsfZ0W","replyto":"SkaPsfZ0W","signatures":["ICLR.cc/2018/Conference/Paper937/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting approach, but lacks justification","rating":"5: Marginally below acceptance threshold","review":"In this work a new network of GCNs is proposed. Different GCNs utilize different powers of the transition matrix to capture varying neighborhoods in a graph. As an aggregation mechanism of the GCN modules two approaches are considered: a fully connected layer on top of stacked features and attention mechanism that uses a scalar weight per GCN. The later allows for better interpretability of the effects of varying degree of neighborhoods in a graph.\n\nProposed approach, as authors noted themselves, is quite similar to DCNN (Atwood and Towsley, 2016) and becomes equivalent if the combined GCNs have one layer each. While comparison to vanilla GCN is quite extensive, there is no comparison to DCNN at all. I would be curious to see at least portion of the experiments of the DCNN paper with the proposed approach, where the importance of number of GCN layers is addressed. DCNN did well on Cora and Pubmed when more training samples were used. It also was tested on graph classification datasets, but the results were not as good for some of the datasets. I think that comparison to DCNN is important to justify the importance of using multilayer GCN modules.\n\nSome questions and concerns:\n- I could not quite figure out how many layers did each GCN have in the experiments and how impactful is this parameter \n- Why is it necessary to replicate GCNs for each of the transition matrix powers? In section 4.3 it is mentioned that replication factors r = 1 and r = 4 were used, but it is not clear from Table 2 what are the results for respective r.\n- Early stopping implementation seems a bit too intense. \"We invoke many runs over all datasets\" - how many? Mean and standard deviation are reported for top 3 performers, which is not enough to get a sense of standard deviation and mean. Kipf and Welling (2017) report results over 100 runs without selecting top performers if I understood correctly their setup. Could you please report mean and standard deviation of all the runs? Given relatively small performance improvement (comparatively to GCN), more than 3 (selected) runs are needed for comparison.\n- I liked the attention idea and its interpretation in Fig. 2. Could you please add the error bars for the attention weights. It is interesting to see them shifting towards higher powers of the transition matrix, but also it is important to know if this phenomena is statistically significant.\n- Following up on the previous item - did you try not including self connections when computing transition matrix powers? This way the effect of different degrees of neighborhoods in a graph could be understood better. When self-connections are present, each subsequent transition matrix power contains neighborhoods of lower degrees and interpretation becomes not as apparent.\n\nMinor comments:\n- Understanding of this paper quite heavily relies on the reader knowing Kipf and Welling (2017) paper. Particularly, the comment about approximations derived by Kipf and Welling (2017) in Section 3.3 and how directed graph was converted to undirected (Section 4.1) require a bit more details.\n- I am not quite sure why Section 2.3 is needed. Connection to graph embeddings is not given much attention in the paper later on (except t-SNE picture).\n- Typo in Fig. 1 caption - right and left are mixed up.\n- Typo in footnote on page 3.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222823295,"tcdate":1511586418193,"number":1,"cdate":1511586418193,"id":"BJququIlf","invitation":"ICLR.cc/2018/Conference/-/Paper937/Official_Review","forum":"SkaPsfZ0W","replyto":"SkaPsfZ0W","signatures":["ICLR.cc/2018/Conference/Paper937/AnonReviewer2"],"readers":["everyone"],"content":{"title":"possibly interesting ideas","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a novel graph convolutional network in which a variety of random walk steps are involved with multiple GCNs.\n\nThe basic idea, introducing long rage dependecy, would be interesting. Robustness for the feature remove is also interesting.\n\nThe validation set would be important for the proposed method, but for creating larger validation set, labeled training set would become small. How the good balance of training-and-validation can be determined?\n\nDiscussing choice of the degree would be informative. In introducing many degrees (GCNs) for small labeled nodes semi-supervised setting seems to cause over-fitting.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]}},{"tddate":null,"ddate":null,"tmdate":1511377436872,"tcdate":1511377436872,"number":1,"cdate":1511377436872,"id":"H1rQ5HmgG","invitation":"ICLR.cc/2018/Conference/-/Paper937/Official_Comment","forum":"SkaPsfZ0W","replyto":"BJfkzLgxf","signatures":["ICLR.cc/2018/Conference/Paper937/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper937/Authors"],"content":{"title":"RE: Validation set and comparison to GraphSAGE","comment":"Thomas,\n\nThanks for your kind words about our work!\n\nWe share your feelings about the challenges of assessing work that uses the benchmark splits, and we agree that testing on more datasets (e.g. graphs introduced in GraphSAGE) would further test if our model can generalize to other settings which hopefully do not suffer from the train VS validation size variance.\n\nWe were not aware of GraphSAGE at the time of our work (it is recent, to appear in NIPS). Nonetheless, it should be a one-line addition to our baseline (Kipf's GCN) and our model (NGCN), as it is just a layer-norm transformation (https://arxiv.org/abs/1607.06450).\n\nWe hope to add some additional experimental results during the rebuttal phase, as we are also quite interested in understanding the impact of newer models (e.g. GraphSAGE and/or mixture of CNNs) in the context of our proposed method. This should strengthen our work -- Thank you for the suggestion!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]}},{"tddate":null,"ddate":null,"tmdate":1511182809978,"tcdate":1511182809978,"number":1,"cdate":1511182809978,"id":"BJfkzLgxf","invitation":"ICLR.cc/2018/Conference/-/Paper937/Public_Comment","forum":"SkaPsfZ0W","replyto":"SkaPsfZ0W","signatures":["~Thomas_N._Kipf1"],"readers":["everyone"],"writers":["~Thomas_N._Kipf1"],"content":{"title":"Validation set and comparison to GraphSAGE","comment":"Very interesting work!\n\nI very much appreciate that you pointed out a significant issue with the benchmark dataset splits for Cora/Citeseer/Pubmed that are now often being used to compare models for semi-supervised learning on graph-structured data. Following https://arxiv.org/abs/1609.02907, the setting is typically as follows (as you mentioned): a small number of labeled examples are used for training (typically 20 labeled nodes per class), whereas a large fixed-size split of 500 labeled nodes is used for validation / hyperparameter optimization. \n\nWhile the hyperparameter optimization procedure in https://arxiv.org/abs/1609.02907 was kept to a bare minimum (same hyperparameter choice across all three datasets, chosen from a very small grid search on Cora), it is indeed possible to easily \"cheat\" the benchmark by making use of the rich information provided by the validation set, as your results denoted by 'calibrated' (where you perform gradient descent on some of the model parameters based on validation loss) nicely demonstrate. I am a bit worried that this issue affects a number of recently proposed models that make use of this benchmark (some of the other concurrent submissions to ICLR2018 included), as it is hard to evaluate how much effort has been put into hyperparameter optimization.\n\nIt looks to me like your uncalibrated model (i.e. without gradient descent optimization based on validation loss) is unaffected by this and indicates that your proposed Network of GCNs indeed helps improve model performance. \n\nRecently, a number of improvements of the GCN model have been proposed, and I think it would make your results stronger if you compared to the most prominent ones that have been published lately: GraphSAGE (https://arxiv.org/abs/1706.02216) and mixture model CNNs (https://arxiv.org/abs/1611.08402). Arguably their contributions are orthogonal to yours, so ideally these model improvements could easily be combined. Nonetheless it would provide a clearer picture to see how different contributions can make this class of model more powerful / stable.\n\nIt would be interesting to see an evaluation of your model on at least one different type of dataset (such as one of the benchmark datasets introduced in GraphSAGE), where calibration on the validation set hopefully wouldn't have such a large impact.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739024583,"tcdate":1509137254915,"number":937,"cdate":1509739021918,"id":"SkaPsfZ0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkaPsfZ0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Network of Graph Convolutional Networks \\\\ Trained on Random Walks","abstract":"Semi-supervised learning on graph-structured data has recently progressed with the introduction of Graph Convolutional Networks (GCNs).  At the same time, unsupervised learning of graph embeddings has benefited from random walks. In this paper, we marry the two, by feeding random walk statistics to multiple instances of GCNs, combining their output into a classification sub-network. Our overall architecture, Network of GCNs, is able to utilize information from near and distant neighbors. We evaluate on challenging node classification tasks, when the training data is very scarce, and we achieve state-of-the-art performance on Cora, Citeseer, and Pubmed. Further, we inspect how the classification sub-network circumvents adversarial input perturbations by shifting attention to distant nodes.","pdf":"/pdf/183183d6f43081020398c458aef6f341a2ee9619.pdf","TL;DR":"We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.","paperhash":"anonymous|network_of_graph_convolutional_networks_\\\\_trained_on_random_walks","_bibtex":"@article{\n  anonymous2018network,\n  title={Network of Graph Convolutional Networks \\\\ Trained on Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkaPsfZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper937/Authors"],"keywords":["Graph Convolution","Deep Learning","Network of Networks"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}