{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222743926,"tcdate":1511836118196,"number":3,"cdate":1511836118196,"id":"r1AAFH5xG","invitation":"ICLR.cc/2018/Conference/-/Paper754/Official_Review","forum":"HymuJz-A-","replyto":"HymuJz-A-","signatures":["ICLR.cc/2018/Conference/Paper754/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper explores how current CNN’s and Relational Networks fail to recognize visual relations in images. It firstly performs a performance analysis of CNN’s on SVRT, proposes a new visual relation challenge and shows how the proposed sort-of-CLEVR challenge can be slightly modified to break current state of the art approaches.","rating":"6: Marginally above acceptance threshold","review":"Strengths:\n\n-\tThere is an interesting analysis on how CNN’s perform better Spatial-Relation problems in contrast to Same-Different problems, and how Spatial-Relation problems are less sensitive to hyper parameters.\n\n-\tThe authors bring a good point on the limitations of the SVRT dataset – mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations, which make it difficult to quantify the effect of image variability on the task. And propose a challenge that addresses these issues and allows controlling different aspects of image variability.\n\n-\tThe paper shows how state of the art relational networks, performing well on multiple relational tasks, fail to generalize to same-ness relationships.\n\nWeaknesses:\n\n-\tWhile the proposed PSVRT dataset addresses the 2 noted problems in SVRT, using only 2 relations in the study is very limited.\n\n-\tThe paper describes two sets of relationships, but it soon suggests that current approaches actually struggle in Same-Different relationships. However, they only explore this relationship under identical objects. It would have been interesting to study more kinds of such relationships, such as equality up to translation or rotation, to understand the limitation of such networks. Would that allow improving generalization to varying item or image sizes?\n\nComments:\n\n-\tIn page 2, authors suggest that from that Gülçehre, Bengio (2013) that for visual relations “failure of feed-forward networks […] reflects a poor choice of hyper parameters. This seems to contradict the later discussion, where they suggest that probably current architectures cannot handle such visual relationships. \n\n-\tThe point brought about CNN’s failing to generalize on same-ness relationships on sort-of-CLEVR is interesting, but it would be good to know why PSVRT provides better generalization. What would happen if shapes different than random squared patterns were used at test time?\n\n-\tAuthors reason about biological inspired approaches, using Attention and Memory, based on existing literature. While they provide some good references to support this statement it would have been interesting to show whether they actually improve TTA under image parameter variations\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks","abstract":"The robust and efficient recognition of visual relations in images is a hallmark of biological vision. Here, we argue that, despite recent progresses in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity. We further show that another class of feedforward networks called relational networks (RNs) which were shown to successfully solve seemingly challenging visual question answering (VQA) challenges on the CLEVR datasets, suffer the same limitations. Motivated by the comparable success of biological vision, we argue that the incorporation of feedback mechanisms including working memory and attention will constitute a necessary step towards building machines that are capable of abstract visual reasoning.","pdf":"/pdf/3e5d19ffbed3d6c9a20186d1904f03d4ac42089a.pdf","TL;DR":"Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies.","paperhash":"anonymous|notsoclevr_visual_relations_strain_feedforward_neural_networks","_bibtex":"@article{\n  anonymous2018not-so-clevr:,\n  title={Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HymuJz-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper754/Authors"],"keywords":["Visual Relations","Visual Reasoning","SVRT","Attention","Working Memory","Convolutional Neural Network","Deep Learning","Relational Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222743971,"tcdate":1511731536922,"number":2,"cdate":1511731536922,"id":"rkFUZ2uxf","invitation":"ICLR.cc/2018/Conference/-/Paper754/Official_Review","forum":"HymuJz-A-","replyto":"HymuJz-A-","signatures":["ICLR.cc/2018/Conference/Paper754/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review of \"NOT-SO-CLEVR: VISUAL RELATIONS STRAIN FEEDFORWARD NEURAL NETWORKS\"","rating":"6: Marginally above acceptance threshold","review":"The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations.  They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.   They show that while recent approaches (e.g. relational networks) can generalize reasonably well on some tasks, these results do not generalize as well to held-out-object scenarios as might have been assumed. \n\nClarity:  The paper is fairly clearly written.   I think I mostly followed it.   \n\nQuality:  I'm intrigued by but a little uncomfortable with the generalization metrics that the authors use.   The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions.   The authors state that \". . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset.\"  Taken literally, this would rule out a lot of modern machine learning, even obviously very good work. On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships.  I'm a little worried that it's \"in the eye of the beholder\" whether a given generalization should be expected to work or not.     \n\nThere are essentially three scenarios of generalization discussed in the paper:\n        (a) various generalizations of image parameters in the PSVRT dataset\n        (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset\n        (c) from sort-of-CLEVR \"objects\" to PSVRT bit patterns\n\nThe result that existing architectures didn't do very well at these generalizations (especially b and c) *may* be important -- or it may not.    Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered.    After all, when we humans generalize to understanding relationships, exactly what variability is present in our \"training sets\" as compared to our \"testing\" situations?   How do the authors know that humans are effectively generalizing rather than just \"interpolating\" within their (very rich) training set?  It's not totally clear to me that if totally naive humans (who had never seen spatial relationships before) were evaluated on exactly the training/testing scenarios described above, that they would generalize particularly well either.   I don't think it can just be assumed a priori that humans would be super good this form of generalization.  \n\nSo how should authors handle this criticism?  What would be useful would either be some form of positive control.  Either human training data showing very effective generalization (if one could somehow make \"novel\" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN. If such were present, I'd rate this paper significantly higher.    \n\nAlso, I can't tell if I really fully believe the results of this paper.  I don't doubt that the authors saw the results they report.  However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different.   I can't point to exactly what would have to be different to make things \"work\", because it's really hard to do that ahead of actually trying to do the work.   However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR.  This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly.  I myself am very curious about what would happen and would love to see this exchange catalyzed. \n\nOriginality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.  However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area.   I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution.     ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks","abstract":"The robust and efficient recognition of visual relations in images is a hallmark of biological vision. Here, we argue that, despite recent progresses in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity. We further show that another class of feedforward networks called relational networks (RNs) which were shown to successfully solve seemingly challenging visual question answering (VQA) challenges on the CLEVR datasets, suffer the same limitations. Motivated by the comparable success of biological vision, we argue that the incorporation of feedback mechanisms including working memory and attention will constitute a necessary step towards building machines that are capable of abstract visual reasoning.","pdf":"/pdf/3e5d19ffbed3d6c9a20186d1904f03d4ac42089a.pdf","TL;DR":"Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies.","paperhash":"anonymous|notsoclevr_visual_relations_strain_feedforward_neural_networks","_bibtex":"@article{\n  anonymous2018not-so-clevr:,\n  title={Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HymuJz-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper754/Authors"],"keywords":["Visual Relations","Visual Reasoning","SVRT","Attention","Working Memory","Convolutional Neural Network","Deep Learning","Relational Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222744012,"tcdate":1511524501191,"number":1,"cdate":1511524501191,"id":"B1pcOYBlG","invitation":"ICLR.cc/2018/Conference/-/Paper754/Official_Review","forum":"HymuJz-A-","replyto":"HymuJz-A-","signatures":["ICLR.cc/2018/Conference/Paper754/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Important limitations of relational networks","rating":"5: Marginally below acceptance threshold","review":"Quality\n\nThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization.\n\nClarity\n\nThe rationale in the paper is straightforward. I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before.\n\nOriginality\n\nWhile others have pointed out limitations before, this paper considers relational networks for the first time.\n\nSignificance \n\nThis work demonstrates failures of relational networks on relational tasks, which is an important message. At the same time, no new architectures are presented to address these limitations.\n\nPros\n\nImportant message about network limitations.\n\nCons\n\nStraightforward testing of network performance on specific visual relation tasks. No new theory development. Conclusions drawn by testing on out of sample data may not be completely valid.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks","abstract":"The robust and efficient recognition of visual relations in images is a hallmark of biological vision. Here, we argue that, despite recent progresses in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity. We further show that another class of feedforward networks called relational networks (RNs) which were shown to successfully solve seemingly challenging visual question answering (VQA) challenges on the CLEVR datasets, suffer the same limitations. Motivated by the comparable success of biological vision, we argue that the incorporation of feedback mechanisms including working memory and attention will constitute a necessary step towards building machines that are capable of abstract visual reasoning.","pdf":"/pdf/3e5d19ffbed3d6c9a20186d1904f03d4ac42089a.pdf","TL;DR":"Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies.","paperhash":"anonymous|notsoclevr_visual_relations_strain_feedforward_neural_networks","_bibtex":"@article{\n  anonymous2018not-so-clevr:,\n  title={Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HymuJz-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper754/Authors"],"keywords":["Visual Relations","Visual Reasoning","SVRT","Attention","Working Memory","Convolutional Neural Network","Deep Learning","Relational Network"]}},{"tddate":null,"ddate":null,"tmdate":1509739121990,"tcdate":1509134187281,"number":754,"cdate":1509739119336,"id":"HymuJz-A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HymuJz-A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks","abstract":"The robust and efficient recognition of visual relations in images is a hallmark of biological vision. Here, we argue that, despite recent progresses in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity. We further show that another class of feedforward networks called relational networks (RNs) which were shown to successfully solve seemingly challenging visual question answering (VQA) challenges on the CLEVR datasets, suffer the same limitations. Motivated by the comparable success of biological vision, we argue that the incorporation of feedback mechanisms including working memory and attention will constitute a necessary step towards building machines that are capable of abstract visual reasoning.","pdf":"/pdf/3e5d19ffbed3d6c9a20186d1904f03d4ac42089a.pdf","TL;DR":"Using a novel, controlled, visual-relation challenge, we show that same-different tasks critically strain the capacity of CNNs; we argue that visual relations can be better solved using attention-mnemonic strategies.","paperhash":"anonymous|notsoclevr_visual_relations_strain_feedforward_neural_networks","_bibtex":"@article{\n  anonymous2018not-so-clevr:,\n  title={Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HymuJz-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper754/Authors"],"keywords":["Visual Relations","Visual Reasoning","SVRT","Attention","Working Memory","Convolutional Neural Network","Deep Learning","Relational Network"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}