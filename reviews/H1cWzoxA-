{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222626402,"tcdate":1512010368111,"number":3,"cdate":1512010368111,"id":"ryOYfeaef","invitation":"ICLR.cc/2018/Conference/-/Paper366/Official_Review","forum":"H1cWzoxA-","replyto":"H1cWzoxA-","signatures":["ICLR.cc/2018/Conference/Paper366/AnonReviewer2"],"readers":["everyone"],"content":{"title":"solid experiments, but the model is not very exciting","rating":"5: Marginally below acceptance threshold","review":"This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP. The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications. The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed.\n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.\n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR. First, there is not much innovation in the model architecture. The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level. I think this more counts as careful engineering of the SAN model rather than a main innovation. Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders. What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling","abstract":"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but dose not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","pdf":"/pdf/6d58eaccb8a70c884444342dfd9f6bb0b0552c45.pdf","TL;DR":"A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks","paperhash":"anonymous|bidirectional_block_selfattention_for_fast_and_memoryefficient_sequence_modeling","_bibtex":"@article{\n  anonymous2018bi-directional,\n  title={Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cWzoxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper366/Authors"],"keywords":["deep learning","attention mechanism","sequence modeling","natural language processing","sentence embedding"]}},{"tddate":null,"ddate":null,"tmdate":1512222626440,"tcdate":1511816497593,"number":2,"cdate":1511816497593,"id":"rkcETx9lf","invitation":"ICLR.cc/2018/Conference/-/Paper366/Official_Review","forum":"H1cWzoxA-","replyto":"H1cWzoxA-","signatures":["ICLR.cc/2018/Conference/Paper366/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Strong support for more efficient attention","rating":"9: Top 15% of accepted papers, strong accept","review":"This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks. The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed. It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves.\n\nThe paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading. The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling.\n\nI am curious how the story would look if one tried to push beyond two levels...? For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents? \n\nMinor points:\n- Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)}.\n- Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*.\n- Missing word in first sentence of S4.1: ... reason __ the ...","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling","abstract":"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but dose not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","pdf":"/pdf/6d58eaccb8a70c884444342dfd9f6bb0b0552c45.pdf","TL;DR":"A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks","paperhash":"anonymous|bidirectional_block_selfattention_for_fast_and_memoryefficient_sequence_modeling","_bibtex":"@article{\n  anonymous2018bi-directional,\n  title={Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cWzoxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper366/Authors"],"keywords":["deep learning","attention mechanism","sequence modeling","natural language processing","sentence embedding"]}},{"ddate":null,"tddate":1511806221587,"tmdate":1512222626479,"tcdate":1511806138108,"number":1,"cdate":1511806138108,"id":"SJz6VRFlG","invitation":"ICLR.cc/2018/Conference/-/Paper366/Official_Review","forum":"H1cWzoxA-","replyto":"H1cWzoxA-","signatures":["ICLR.cc/2018/Conference/Paper366/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The methodology of the paper is incremental; the evaluation is comprehensive and in general supports the claims. ","rating":"6: Marginally above acceptance threshold","review":"Pros: \nThe paper proposes a “bi-directional block self-attention network (Bi-BloSAN)” for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient. The paper is written clearly and is easy to follow. The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency. The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off. \n\nCons:\n- Methodology of the paper is very incremental compared with previous models.  \n- Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper. \n- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN.\n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart. This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks.\n- The main originality of paper is the block style. However, the paper doesn’t analyze how and why the block brings improvement. \n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling","abstract":"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but dose not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","pdf":"/pdf/6d58eaccb8a70c884444342dfd9f6bb0b0552c45.pdf","TL;DR":"A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks","paperhash":"anonymous|bidirectional_block_selfattention_for_fast_and_memoryefficient_sequence_modeling","_bibtex":"@article{\n  anonymous2018bi-directional,\n  title={Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cWzoxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper366/Authors"],"keywords":["deep learning","attention mechanism","sequence modeling","natural language processing","sentence embedding"]}},{"tddate":null,"ddate":null,"tmdate":1509739341169,"tcdate":1509106177744,"number":366,"cdate":1509739338501,"id":"H1cWzoxA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1cWzoxA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling","abstract":"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but dose not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","pdf":"/pdf/6d58eaccb8a70c884444342dfd9f6bb0b0552c45.pdf","TL;DR":"A self-attention network for RNN/CNN-free sequence encoding with small memory consumption, highly parallelizable computation and state-of-the-art performance on several NLP tasks","paperhash":"anonymous|bidirectional_block_selfattention_for_fast_and_memoryefficient_sequence_modeling","_bibtex":"@article{\n  anonymous2018bi-directional,\n  title={Bi-directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cWzoxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper366/Authors"],"keywords":["deep learning","attention mechanism","sequence modeling","natural language processing","sentence embedding"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}