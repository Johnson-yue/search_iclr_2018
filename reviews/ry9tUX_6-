{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222619837,"tcdate":1511836208162,"number":2,"cdate":1511836208162,"id":"r1dNqr9xf","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Review","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Weak Accept","rating":"6: Marginally above acceptance threshold","review":"1) I would like to ask for the clarification regarding the generalization guarantees. The original Entropy-SGD paper shows improved generalization over SGD using uniform stability, however the analysis of the authors rely on an unrealistic assumption regarding the eigenvalues of the Hessian (they are assumed to be away from zero, which is not true at least at local minima of interest). What is the enabling technique in this submission that avoids taking this assumption? (to clarify: the analysis is all-together different in both papers, however this aspect of the analysis is not fully clear to me).\n2) It is unclear to me what are the unrealistic assumptions made in the paper. Please, list them all in one place in the paper and discuss in details.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm for classifiers, optimizes a PAC-Bayes bound on the risk of the classifier, or more accurately, the Gibbs posterior, i.e., a risk-sensitive perturbation of the classifier. Entropy-SGD works by optimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an epsilon-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/a83f5ae92e6c3595355df5797520890dad101fc8.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgld_optimizes_the_prior_of_a_valid_pacbayes_bound","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222619874,"tcdate":1511828145939,"number":1,"cdate":1511828145939,"id":"ryq2cm9xG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Review","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper connects Entropy-SGD with PAC-Bayes learning. It shows that maximizing the local entropy during the execution of Entropy-SGD essentially minimize a PAC-Bayes bound on the risk of the Gibbs posterior. Despite this connection, Entropy-SGD could lead to dependence between prior and data and thus violate the requirement of PAC-Bayes theorem. The paper then proposes to use a differentially private prior to get a valid PAC-Bayes bound with SGLD. Experiments on MNIST shows such algorithm does generalize better.\n\nLinking Entropy-SGD to PAC-Bayes learning and making use of differential privacy to improve generalization is quite interesting. However, I'm not sure if the ideas and techniques used to solve the problem are novel enough.\nIt would be better if the presentation of the paper is improved. The result in Section 4 can be presented in a theorem, and any related analysis can be put into the proof. Section 5 about previous work on differentially private posterior sampling and stability could follow other preliminaries in Section 2. The figures are a bit hard to read. Adding sub-captions and re-scaling y-axis might help.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm for classifiers, optimizes a PAC-Bayes bound on the risk of the classifier, or more accurately, the Gibbs posterior, i.e., a risk-sensitive perturbation of the classifier. Entropy-SGD works by optimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an epsilon-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/a83f5ae92e6c3595355df5797520890dad101fc8.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgld_optimizes_the_prior_of_a_valid_pacbayes_bound","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739521006,"tcdate":1508550274397,"number":32,"cdate":1509739518349,"id":"ry9tUX_6-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm for classifiers, optimizes a PAC-Bayes bound on the risk of the classifier, or more accurately, the Gibbs posterior, i.e., a risk-sensitive perturbation of the classifier. Entropy-SGD works by optimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an epsilon-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/a83f5ae92e6c3595355df5797520890dad101fc8.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgld_optimizes_the_prior_of_a_valid_pacbayes_bound","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}