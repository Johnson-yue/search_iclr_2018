{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222557451,"tcdate":1511818269428,"number":3,"cdate":1511818269428,"id":"H1BQNZqgf","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Propose explicitly modeling the hidden state of the backward RNN used for inference in a sequential deep generative model. Good idea, good empirical performance; the writing, however, is confusing.","rating":"7: Good paper, accept","review":"This paper builds a sequential deep generative model with (1) an inference network parameterized by an RNN running from the future to the past and (2) an explicit representation of the hidden state of the backward RNN in the generative model. The model is validated on held-out likelihood via the ELBO on text, handwriting, speech and images. It presents good emprical results and works at par with or better than many other baselines considered.\n\nThe main source of novelty here the choice made in the transition function of z_t to also incorporate an explicit variable to models the hidden state of the backward RNN at inference time and use that random variable in the generative process. This is a choice of structural prior for the transition function of the generative model that I think lends it more expressivity realizing the empirical gains obtained.\n\nI found the presentation of both the model and learning objective to be confusing and had a hard time following it. The source of my confusion is is that \\tilde{b} (the following argument applies equivalently to \\tilde{h}) is argued to be a latent variable. Yet it is not inferred (via a variational distribution) during training.\n\nPlease correct me if I'm wrong but I believe that an easier to understand way to explain the model is as follows: both \\tilde{b} and \\tilde{h} should be presented as *observed* random variables during *training* and latent at inference time. Training then comprises maximizing the marginal likelihood of the data *and* maximizing the conditional likelihood of the two observed variables(via p_psi and p_eta; conditioned on z_t). Under this view, setting beta to 0 simply corresponds to not observing \\tilde{h_t}. alpha can be annealed but should never be set to anything less than 1 without breaking the semantics of the learned generative model.\n\nConsider Figure 1(b). It seems that the core difference between this work and [Chung et. al] is that this work parameterizes q(Z_t) using x_t....x_T (via a backward RNN). This choice of inference network can be motivated from the point of view of building a better approximation to the structure of the posterior distribution of Z_t under the generative model. Both [Fracarro et. al] and [Krishnan et. al] (https://arxiv.org/pdf/1609.09869.pdf) use RNNs from x_T to x_1 to train sequential state space models. [Gao et. al] (https://arxiv.org/pdf/1605.08454.pdf) derive an inference network with a block-diagonal structure motivated by correlations in the posterior distribution. Incorporating a discussion around this idea would provide useful context for where this work stands amongst the many sequential deep generative models in the\nliterature.\n\nQuestions for the authors:\n* How important is modeling \\tilde{h_t} in TIMIT, Blizzard and IMDB?\n* Did you try annealing the KL divergence in the PTB experiment. Based on the KL divergence you report it seems the latent variable is not necessary.\n\nOverall, I find the model to be interesting and it performs well empirically. However, the text of the paper lacks a bit of context and clarity that makes understanding it challenging to understand in its current form.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222557490,"tcdate":1511805112035,"number":2,"cdate":1511805112035,"id":"HJg6l0FxM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting paper, but not the clearest presentation.","rating":"7: Good paper, accept","review":"This paper proposes a particular form of variational RNN that uses a forward likelihood and a backwards posterior.  Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.\n\nMy first concern with this paper is that the derivation in Eq. 1 does not seem to be correct.  There is a p(z_1:T) term that should appear in the integrand.\n\nIt is not clear to me why h_t should depend on \\tilde{b}_t.  All paths from input to output through \\tilde{b}_t also pass through z_t so I don't see how this could be adding information.  It may add capacity to the decoder in the form of extra weights, but the same could be achieved by making z_t larger. Why not treat \\tilde{b}_t symmetrically to \\tilde{h}_t, and use it only as a regularizer?  \n\nIn the no reconstruction loss experiments do you still sample \\tilde{b}_t in the generative part?  Baselines where the \\tilde{b}_t -> h_t edge is removed would be very nice.\n\nIt seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.\n\nI don't understand the description of the \"Skip Gradient\" trick.  Exactly which gradients are you skipping at random?\n\nDo you have any intuition for why it is sometimes necessary to set beta=0?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222557534,"tcdate":1511643242576,"number":1,"cdate":1511643242576,"id":"Hy7uO8PlG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting ideas; paper could be improved by more ablation experiments, theoretical justifications, and evaluation methods","rating":"4: Ok but not good enough - rejection","review":"*Quality*\n\nThe paper is easy to parse, with clear diagrams and derivations at the start. The problem context is clearly stated, as is the proposed model.\n\nThe improvements in terms of average log-likelihood are clear. The model does improve over state-of-the-art in some cases, but not all.\n\nBased on the presented findings, it is difficult to determine the quality of the learned models overall, since they are only evaluated in terms of average log likelihood. It is also difficult to determine whether the improvements are due to the model change, or some difference in how the models themselves were trained (particularly in the case of Z-Forcing, a closely related technique). I would like to see more exploration of this point, as the section titled “ablation studies” is short and does not sufficiently address the issue of what component of the model is contributing to the observed improvements in average log-likelihood.\n\nHence, I have assigned a score of \"4\" for the following reasons: the quality of the generated models is unclear; the paper does not clearly distinguish itself from the closely-related Z-Forcing concept (published at NIPS 2017); and the reasons for the improvements shown in average log-likelihood are not explored sufficiently, that is, the ablation studies don't eliminate key parts of the model that could provide this information.\n\nMore information on this decision is given in the remainder.\n\n*Clarity*\n\nA lack of generated samples in the Experimental Results section makes it difficult to evaluate the performance of the models; log-likelihood alone can be an inadequate measure of performance without some care in how it is calculated and interpreted (refer, e.g., to Theis et al. 2016, “A Note on the Evaluation of Generative Models”).\n\nThere are some typos and organizational issues. For example, VAEs are reintroduced in the Related Works section, only to provide an explanation for an unrelated optimization challenge with the use of RNNs as encoders and decoders.\n\nI also find the motivations for the proposed model itself a little unclear. It seems unnatural to introduce a side-channel-cum-regularizer between a sequence moving forward in time and the same sequence moving backwards, through a variational distribution. In the introduction, improved regularization for LSTM models is cited as a primary motivation for introducing and learning two approximate distributions for latent variables between the forward and backward paths of a bi-LSTM. Is there a serious need for new regularization in such models? The need for this particular regularization choice is not particularly clear based on this explanation, nor are the improvements state-of-the-art in all cases. This weakens a possible theoretical contribution of the paper.\n\n*Originality*\n\nThe proposed modification appears to amount to a regularizer for bi-LSTMs which bears close similarity to Z-Forcing (cited in the paper). I recommend a more careful comparison between the two methods. Without such a comparison, they are a little hard to distinguish, and the originality of this paper is hard to evaluate. Both appear to employ the same core idea of regularizing an LSTM using a learned variational distributions. The differences *seem* to be in the small details, and these details appear to provide better performance in terms of average log-likelihood on all tasks compared to Z-Forcing--but, crucially, not compared to other models in all cases.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511408670479,"tcdate":1511408670479,"number":3,"cdate":1511408670479,"id":"rJU7Epmef","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"H1RmckMeM","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Further experiments for clarification","comment":"The analysis of our method with and without stochastic backprop as well as with and without reconstruction losses are provided in the ablation studies section (figure 2). The text around this figure is unfortunately missing in the submitted version but can be found in the latest (anonymous) version we have linked in our previous reply. This analysis shows how our model benefits from stochastic backprop.\n\nWe had also run experiments to see the effects of stochastic backprop on Z-forcing. We additionally also add a reconstruction cost on h_t in the Z-forcing model as another separate experiment. So for a detailed comparison, we show the evolution of BPC on PTB for four models:\n1. Z-forcing\n2. Z-forcing + stochastic backprop (on the auxiliary cost)\n3. Z-forcing + stochastic backprop (on the auxiliary cost) + reconstruction/auxiliary loss\n4. Variational Bi-LSTM\nThe plot can be found in this anonymous link https://anonfile.com/HdFdcadbb6/ptb_sdc_zf_rec.png . As can be seen there is a gradual improvement from model 1 to model 4. \n\nWe agree with your suggestion of exploring the usefulness of the latent variable z and we ourselves had given thought to it. However, this is not the focus of our work, and this analysis applies to all models that make use of a latent variable in LSTMs (including Z-forcing). So we leave this as separate future work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511408228310,"tcdate":1511408228310,"number":2,"cdate":1511408228310,"id":"SknDf6mxz","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"S1-LoAGxM","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Clarification for Eq. 10","comment":"We thank you for raising this question. Your first doubt regarding \\tilde{b}_t is because of ambiguity in our notation. We do sample \\tilde{b}_t from p_psi. These samples are used in the third term-- alpha log p_psi(b_t | z_t). To remove ambiguity, this term should be read as-- alpha log p_psi(\\tilde{b}_t = b_t | z_t).\n\nRegarding your second question about terms that should relate \\tilde{b}_t and h_t, we believe the notations are correct. Imagine if we were to write the objective for a simple LSTM, then this objective would simply contain a summation of terms p(x_{t+1} | h_t) over time steps t. The dependence of h_t on the previous time steps are implicit. Similarly, in our objective, the term p(x_{t+1} | h_t) implicitly contains the dependence on \\tilde{b}_t, z_t and the previous time step variables."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511349486857,"tcdate":1511349064998,"number":3,"cdate":1511349064998,"id":"S1-LoAGxM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some questions about Eq. 10 in latest version ","comment":"Dear authors,\n\nit's an interesting research, but I still have some questions about the objective, and I hope you can give me a help!\n\nIn Eq.10, the objective, \\tilde{b}_t is sampled from p_psi. However, there is no \\tilde{b}_t in the inside term, so it seems that there is no need to sample \\tilde{b}_t? Is it just a typo?\n\nNext, based on figure 1(a) and your answer, I think there may be some terms to stand for the direct connection between h_t and \\tilde{b}_t. However, it seems that, in Eq. 10 , there is no term to stand for the directly conditional dependence between h_t and \\tilde{b}_t( or b_t ). I guess maybe the term p(x_{t+1} | b_t) includes relations like p(x_{t+1} | h_t)p(h_t | b_t), is it true?\nThanks for your help!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511327904737,"tcdate":1511287334065,"number":2,"cdate":1511287334065,"id":"H1RmckMeM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"rkK4-0bgG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for clarification!","comment":"\"Another possible difference in your implementation could be that we suggest using stochastic back-propagation through the auxiliary costs. This entails that the gradients through the auxiliary cost be stochastically dropped during training\"\n\nI was not dropping these gradients, this is the only difference I could think of. Though, this raises the point, is all the benefit actually coming from this \"stochastic back-propagation\" over Z-Forcing ? As without using this stochastic back-propagation, results seems more or less same to Z-Forcing (https://arxiv.org/abs/1711.05411)\n\nI'd encourage the authors to add the results with/without \"stochastic back-propagation\"  and compare themselves to the results which Z-Forcing paper reports.\n\nAnother thing which would make this submission strong,  is to analyze how useful the latents (learned z's) are. For ex. may be for some classification task.   "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511280944639,"tcdate":1511280944639,"number":1,"cdate":1511280944639,"id":"rkK4-0bgG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"ByTDAogxG","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Probable causes of your issue","comment":"We appreciate your interest in our paper and your effort to reproduce our results. \nWe apologize for the lack of clarity in the submitted version. We have improved the model description in our current version. We would like to point out that while our model is similar in spirit to Z-forcing, there are notable differences in the derivation of the variational lower bound and the auxiliary costs that provide improvement in performance because the forward LSTM is implicitly directly fed the backward LSTM's state which is in contrast with Z-forcing.\n\nFrom your comment, it seems to us that you add a reconstruction cost on h_t on top of the Z-forcing objective. If this is true, then we would like to clarify that in addition to adding the reconstruction cost and feeding z_t to h_t, we also pass \\tilde{b}_t to h_t. Amongst other differences, this is a crucial difference between Z-forcing and our model. In other words, during training, we sample \\tilde{b}_t for a sampled z_t, and encourage this \\tilde{b}_t to be similar to b_t, and also feed this \\tilde{b}_t to h_t. In this way, our model learns to implicitly use b_t during training as an input to h_t. This is different from Z-forcing where the model passes z_t to h_t while minimizing the KL divergence difference between the prior and posterior over z_t.\n\nAnother possible difference in your implementation could be that we suggest using stochastic backpropagation through the auxiliary costs. This entails that the gradients through the auxiliary cost be stochastically dropped during training.\n\nWe hope these suggestions help in reproducing the results we report in our paper.\n\nFor further clarification, we have uploaded an anonymous copy of the latest version of our paper here: https://anonfile.com/W6i9bad3b4/ICLR18_VLM.pdf."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511213200479,"tcdate":1511206501364,"number":1,"cdate":1511206501364,"id":"ByTDAogxG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Difference to Z-Forcing ?","comment":"Hello Authors, \n\nVery interesting work! \n\nI have been trying to reproduce your experiments. As far as I understand it is straightforward extension to Z-Forcing(https://arxiv.org/abs/1711.05411). I tried to replicate your results using the Z-Forcing code(https://github.com/sordonia/zforcing) so far I have not been able to replicate your results. Adding the reconstruction cost (in the forward RNN, which was also missing from Z-Forcing) does not seem to have any impact on results. \n\nSo are you doing something which is not mentioned in the paper?\n\n    "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092380311,"tcdate":1509138743610,"number":1125,"cdate":1510092359696,"id":"SkqV-XZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}