{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222619083,"tcdate":1511834773291,"number":3,"cdate":1511834773291,"id":"BkpqEH5gf","invitation":"ICLR.cc/2018/Conference/-/Paper314/Official_Review","forum":"H18WqugAb","replyto":"H18WqugAb","signatures":["ICLR.cc/2018/Conference/Paper314/AnonReviewer2"],"readers":["everyone"],"content":{"title":"No Title","rating":"6: Marginally above acceptance threshold","review":"This paper argues about limitations of RNNs to learn models than exhibit a human-like compositional operation that facilitates generalization to unseen data, ex. zero-shot or one-shot applications. The paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas. To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case. They conduct a complete experimentation, testing different popular RNN architectures, as well as parameter and hyperparameters values. \n\nThe main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on \"memorizing\" a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality). Authors present a set of experiments designed to support this observation. \n\nWhile the experiments are compelling, as I explain below, I believe there is an underlying assumption that is not considered. Performance on training set by the best model is close to perfect (99.5%), so the model is really learning the task. Authors are then testing the model using test sets that do not follow the same distribution than training data, example,  longer sequences. By doing so, they are breaking one of the most fundamental assumptions of inductive machine learning, i.e., the distribution of train and test data should be equal. Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans. Importantly, this does not entail that using a better regularization a similar RNN model can indeed learn such a representation. In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities.   \n\n\nAnyway, I believe the paper is interesting and the authors are exposing interesting facts that might be worth to spread in our community, so I rate the paper as slightly over the acceptance threshold.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks","abstract":"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","pdf":"/pdf/a49276b162d7a7d610f7d74be1cee0c4fddb203f.pdf","TL;DR":"Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks.","paperhash":"anonymous|still_not_systematic_after_all_these_years_on_the_compositional_skills_of_sequencetosequence_recurrent_networks","_bibtex":"@article{\n  anonymous2018still,\n  title={Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H18WqugAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper314/Authors"],"keywords":["sequence-to-sequence recurrent networks","compositionality","systematicity","generalization","language-driven navigation"]}},{"tddate":null,"ddate":null,"tmdate":1512222619121,"tcdate":1511813311923,"number":2,"cdate":1511813311923,"id":"By_6glcgf","invitation":"ICLR.cc/2018/Conference/-/Paper314/Official_Review","forum":"H18WqugAb","replyto":"H18WqugAb","signatures":["ICLR.cc/2018/Conference/Paper314/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clearly written paper with informative results","rating":"7: Good paper, accept","review":"The paper analyzed the composition abilities of Recurrent Neural Networks.  The authors analyzed the the generalization for the following scenarios\n\n- the generalization ability of RNNs on random subset of SCAN commands\n- the generalization ability of RNNs on longer SCAN commands\n- The generalization ability of composition over primitive commands.\n\nThe experiments supported the hypothesis that the RNNs are able to \n\n- generalize zero-shot to new commands. \n- difficulty generalizing to longer sequence (compared to training sequences) of commands.\n- the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training. The model does not seem to generalize to completely new action and commands (like Jump), however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands)\n\nOverall, the paper is well written and easy to follow. The experiments are complete. The results and analysis are informative.\n\nAs for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables. For example, analyzing whether the latent stochastic variables may shown to actually help with generalization of composition of primitive commands. \n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks","abstract":"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","pdf":"/pdf/a49276b162d7a7d610f7d74be1cee0c4fddb203f.pdf","TL;DR":"Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks.","paperhash":"anonymous|still_not_systematic_after_all_these_years_on_the_compositional_skills_of_sequencetosequence_recurrent_networks","_bibtex":"@article{\n  anonymous2018still,\n  title={Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H18WqugAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper314/Authors"],"keywords":["sequence-to-sequence recurrent networks","compositionality","systematicity","generalization","language-driven navigation"]}},{"tddate":null,"ddate":null,"tmdate":1512222619156,"tcdate":1511694619285,"number":1,"cdate":1511694619285,"id":"S1m7bXulz","invitation":"ICLR.cc/2018/Conference/-/Paper314/Official_Review","forum":"H18WqugAb","replyto":"H18WqugAb","signatures":["ICLR.cc/2018/Conference/Paper314/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nicely written paper describing an interesting series of experiments.","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs.  Through a series of experiments and a newly defined dataset, it exposes the short-comings of current seq2seq RNN architectures.  The proposed dataset, called the SCAN dataset, is a selected subset of the CommonAI navigation tasks data set.  This subset is chosen such that each command sequence corresponds to exactly one target action sequence, making it possible to apply standard seq2seq methods.  Existing methods are then compared based on how accurately they can produce the target action sequence based on the command input sequence.\n\nThe introduction covers relevant literature and nicely describes the motivation for later experiments. Description of the model architecture is largely done in the appendix, this puts the focus of the paper on the experimental section. This choice seems to be appropriate, since standard methods are used. Figure 2 is sufficient to illustrate the model to readers familiar with the literature.\n\nThe experimental part establishes a baseline using standard seq2seq models on the new dataset, by exploring large variations of model architectures and a large part of the hyper-parameter space.   This papers experimentation sections sets a positive example by exploring a comparatively large space of standard model architectures on the problem it proposes. This search enables the authors to come to convincing conclusions regarding the shortcomings of current models. The paper explores in particular:\n1.) Model generalization to unknown data similar to the training set.\n2.) Model generalization to data-sequences longer than the training set.\n3.) Generalization to composite commands, where  a part of the command is never observed in sequence in the training set.\n4.) A recreation of a similar problem in the machine translation context.\nThese experiments show that modern sequence to sequence models do not solve the systematicity problem, while making clear by application to machine translation, why such a solution would be desirable. The SCAN data-set has the potential to become an interesting test-case for future research in this direction.\n\nThe experimental results shown in this paper are clearly compelling in exposing the weaknesses of current seq2seq RNN models.  However, where the paper falls a bit short is in the discussion / outlook in terms of suggestions of how one can go about tackling these shortcomings.  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks","abstract":"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","pdf":"/pdf/a49276b162d7a7d610f7d74be1cee0c4fddb203f.pdf","TL;DR":"Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks.","paperhash":"anonymous|still_not_systematic_after_all_these_years_on_the_compositional_skills_of_sequencetosequence_recurrent_networks","_bibtex":"@article{\n  anonymous2018still,\n  title={Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H18WqugAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper314/Authors"],"keywords":["sequence-to-sequence recurrent networks","compositionality","systematicity","generalization","language-driven navigation"]}},{"tddate":null,"ddate":null,"tmdate":1509739369059,"tcdate":1509095933746,"number":314,"cdate":1509739366396,"id":"H18WqugAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H18WqugAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks","abstract":"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","pdf":"/pdf/a49276b162d7a7d610f7d74be1cee0c4fddb203f.pdf","TL;DR":"Using a simple language-driven navigation task, we study the compositional capabilities of modern seq2seq recurrent networks.","paperhash":"anonymous|still_not_systematic_after_all_these_years_on_the_compositional_skills_of_sequencetosequence_recurrent_networks","_bibtex":"@article{\n  anonymous2018still,\n  title={Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H18WqugAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper314/Authors"],"keywords":["sequence-to-sequence recurrent networks","compositionality","systematicity","generalization","language-driven navigation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}