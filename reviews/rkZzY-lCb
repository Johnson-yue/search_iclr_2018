{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222590553,"tcdate":1512080091476,"number":3,"cdate":1512080091476,"id":"ByQ1mb0xM","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neat representation learning scheme for structured features","rating":"7: Good paper, accept","review":"This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Further, these features can be structured i.e. a set of them are of the same 'type'. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Therefore, this can be used as a multi-label classifier by using two feature types -- the input and the set of categories. This proposed scheme is evaluated on two datasets -- movies and education in a retrieval setting. \n\nI would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Features","abstract":"Methods that calculate dense vector representations for features in unstructured data - such as words in a document - have proven to be very successful for knowledge representation. Surprisingly, very little work has focused on methods for structured datasets where there is more than one type of feature - that is, datasets that have arbitrary features beyond words. We study how to estimate  dense representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space. Feat2Vec is a novel method that calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. We demonstrate our work on two datasets, and our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data.\n","pdf":"/pdf/85f667dfb4b4ae20f4acb2ee9b2f8873f7ce00ef.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222590591,"tcdate":1511778761747,"number":2,"cdate":1511778761747,"id":"HJfRKPFeM","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not clear what I can learn from this","rating":"2: Strong rejection","review":"SUMMARY.\n\nThe paper presents an extension of word2vec for structured features.\nThe authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features.\nThe learned representation of features is tested on a recommendation-like task.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is not clear and thus I am not sure what I can learn from it.\nFrom what is written on the paper I have trouble to understand the definition of the model the authors propose and also an actual NLP task where the representation induced by the model can be useful.\nFor this reason, I would suggest the authors make clear with a more formal notation, and the use of examples, what the model is supposed to achieve.\n\n----------\n\nDETAILED COMMENTS\nWhen the authors refer to word2vec is not clear if they are referring to skipgram or cbow algorithm, please make it clear.\nBottom of page one: \"a positive example is 'semantic'\", please, use another expression to describe observable examples, 'semantic' does not make sense in this context.\nLevi and Goldberg (2014)  do not say anything about factorization machines, could the authors clarify this point?\nEquation (4), what do i and j stand for? what does \\beta represent? is it the embedding vector? How is this formula related to skipgram or cbow?\nThe introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model.\nThe experimental section is rather poor, first, the authors only compare themselves with word2ve (cbow), it is not clear what the reader should learn from the results the authors got.\nFinally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Features","abstract":"Methods that calculate dense vector representations for features in unstructured data - such as words in a document - have proven to be very successful for knowledge representation. Surprisingly, very little work has focused on methods for structured datasets where there is more than one type of feature - that is, datasets that have arbitrary features beyond words. We study how to estimate  dense representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space. Feat2Vec is a novel method that calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. We demonstrate our work on two datasets, and our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data.\n","pdf":"/pdf/85f667dfb4b4ae20f4acb2ee9b2f8873f7ce00ef.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222590634,"tcdate":1511560368421,"number":1,"cdate":1511560368421,"id":"r1_2VGLlz","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper with convincing results, but the approach has limited novelty. Proposed method is based on an approach that is concurrently under review at ICLR18","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThis paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. The paper proposes an approach called Feat2vec that relies on Structured Deep-In Factorization machines-- a paper that is concurrently under review at ICLR2018, which I haven't read in depth. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. Results are shown on IMDB movies and a proprietary education platform datasets. In both the tasks, Feat2vec leads to significant reduction in error compared to Word2vec.\n\nComments:\n\nThe paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. It is a very practically relevant problem. The novelty of the proposed approach seems limited in light of the related paper that is concurrently under review at ICLR2018, on which this paper heavily relies. Perhaps the authors should consider combining the two papers into one complete paper? The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. The sampling approaches proposed seem pretty straightforward adaptations of existing methods and not novel enough.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Features","abstract":"Methods that calculate dense vector representations for features in unstructured data - such as words in a document - have proven to be very successful for knowledge representation. Surprisingly, very little work has focused on methods for structured datasets where there is more than one type of feature - that is, datasets that have arbitrary features beyond words. We study how to estimate  dense representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space. Feat2Vec is a novel method that calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. We demonstrate our work on two datasets, and our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data.\n","pdf":"/pdf/85f667dfb4b4ae20f4acb2ee9b2f8873f7ce00ef.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739423723,"tcdate":1509067016720,"number":218,"cdate":1509739421068,"id":"rkZzY-lCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Feat2Vec:  Dense Vector Representation for Data with Features","abstract":"Methods that calculate dense vector representations for features in unstructured data - such as words in a document - have proven to be very successful for knowledge representation. Surprisingly, very little work has focused on methods for structured datasets where there is more than one type of feature - that is, datasets that have arbitrary features beyond words. We study how to estimate  dense representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space. Feat2Vec is a novel method that calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. We demonstrate our work on two datasets, and our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data.\n","pdf":"/pdf/85f667dfb4b4ae20f4acb2ee9b2f8873f7ce00ef.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","knowledge representation","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}