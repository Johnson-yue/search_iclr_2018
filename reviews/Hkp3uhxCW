{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222634628,"tcdate":1511996806581,"number":3,"cdate":1511996806581,"id":"rk156h2gf","invitation":"ICLR.cc/2018/Conference/-/Paper396/Official_Review","forum":"Hkp3uhxCW","replyto":"Hkp3uhxCW","signatures":["ICLR.cc/2018/Conference/Paper396/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting change in inference but not clear why the modification helps","rating":"6: Marginally above acceptance threshold","review":"The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm.  In particular, the authors propose a new framework to \"sharpen\" the posterior.\n\nIn particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation.  In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why.  In particular, how does the variational posterior change as a result of the hierarchical prior?  It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening.  Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it.  As a final point, the authors state, \"as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L.\"  Do the authors mean that the negative log-likelihood will be improved in this case?  Or the actual optimization?  Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization.\n\nThe new evaluation metric proposed in Section 6.1.1 is confusing, and I do not understand what the metric is trying to capture.  This needs significantly more detail and explanation.  Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs.  It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them.  The authors should not compare to the uniform prior as a baseline for entropy.  It's much more revealing to compare it to the empirical likelihoods of the words.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Recurrent Neural Networks","abstract":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\nFirstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\nSecondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\nWe also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.","pdf":"/pdf/39eb1abfbd69410249062e33af85d79e7255e880.pdf","TL;DR":" Variational Bayes scheme for Recurrent Neural Networks","paperhash":"anonymous|bayesian_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkp3uhxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper396/Authors"],"keywords":["Bayesian","Deep Learning","Recurrent Neural Networks","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1511808528500,"tcdate":1511808496050,"number":1,"cdate":1511808496050,"id":"HkueR0FlG","invitation":"ICLR.cc/2018/Conference/-/Paper396/Public_Comment","forum":"Hkp3uhxCW","replyto":"Hkp3uhxCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Is it possible to revise the title?","comment":"Is it possible to revise the title, to better reflect the proposed variational technique for RNNs? \"Bayesian Recurrent Neural Networks\" have been proposed in several papers with different Bayesian learning methods. See below for examples:\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks, NIPS 2017\nScalable Bayesian Learning of Recurrent Neural Networks for Language Modeling, ACL 2017\nBayesian Recurrent Neural Network for Language Modeling, IEEE Trans Neural Netw Learn Syst. 2016"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Recurrent Neural Networks","abstract":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\nFirstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\nSecondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\nWe also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.","pdf":"/pdf/39eb1abfbd69410249062e33af85d79e7255e880.pdf","TL;DR":" Variational Bayes scheme for Recurrent Neural Networks","paperhash":"anonymous|bayesian_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkp3uhxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper396/Authors"],"keywords":["Bayesian","Deep Learning","Recurrent Neural Networks","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1512222634669,"tcdate":1511804020330,"number":2,"cdate":1511804020330,"id":"Hy2OnpKeG","invitation":"ICLR.cc/2018/Conference/-/Paper396/Official_Review","forum":"Hkp3uhxCW","replyto":"Hkp3uhxCW","signatures":["ICLR.cc/2018/Conference/Paper396/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting posterior sharpening idea","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an interesting variational posterior approximation for the weights of an RNN. The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN. \n\npros:\n--I liked the posterior sharpening idea. It was well motivated from a computational cost perspective hence the use of a hierarchical prior. \n--I liked the uncertainty analysis. There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights. These works can benefit from the uncertainty analysis scheme introduced in this paper.\n--The experiments were well carried through.\n\ncons:\n--Change the title! the title is too vague. \"Bayesian recurrent neural networks\" already exist and is rather vague for what is being described in this paper.\n--There were a lot of unanswered questions:\n (1) how does sharpening lead to lower variance? This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section\n(2) how is the level of uncertainty related to performance? It would have been insightful to see effect of \\sigma_0 on the performance rather than report the best result. \n(3) what was the actual computational cost for the BBB RNN and the baselines?\n--There were very minor typos and some unclear connotations. For example there is no such thing as a \"variational Bayes model\".\n\nI am willing to adjust my rating when the questions and remarks above get addressed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Recurrent Neural Networks","abstract":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\nFirstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\nSecondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\nWe also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.","pdf":"/pdf/39eb1abfbd69410249062e33af85d79e7255e880.pdf","TL;DR":" Variational Bayes scheme for Recurrent Neural Networks","paperhash":"anonymous|bayesian_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkp3uhxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper396/Authors"],"keywords":["Bayesian","Deep Learning","Recurrent Neural Networks","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1512222634706,"tcdate":1511755720608,"number":1,"cdate":1511755720608,"id":"BJZRkfFgG","invitation":"ICLR.cc/2018/Conference/-/Paper396/Official_Review","forum":"Hkp3uhxCW","replyto":"Hkp3uhxCW","signatures":["ICLR.cc/2018/Conference/Paper396/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Variational inference + reparameterisation trick for Bayesian recurrent neural networks","rating":"5: Marginally below acceptance threshold","review":"*Summary*\n\nThe paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs). The paper first considers the \"Bayes by Backprop\" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model. Several experiments demonstrate the quality of the prediction and the uncertainty over dropout.  \n\n*Originality + significance*\n\nTo my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs. However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. \n\nNevertheless, the parameterisation of the conditional variational distribution q(\\theta | \\phi, (x, y)) using recognition model is interesting and could be useful in other models. However, this has not been tested or concretely shown in this paper. The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. \n\n*Clarity*\n\nThe paper is, in general, well-written. However, the presentation in 4 is hard to follow. I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \\phi, a variational approximation over both \\theta and \\phi is needed, and a q that couples \\theta, \\phi and and the gradient of the log likelihood term wrt \\phi is chosen. \n\nAdditional comments:\n\nWhy is the variational approximation called \"sharpened\"?\n\nAt test time, normal VI just uses the fixed q(\\theta) after training. It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\\theta | \\phi, x) in eqs. 19-20 parameterised? The first paragraph of page 5 uses q(\\theta | \\phi, (x, y)), but y is not known at test time.\n\nWhat is C in eq. 9?\n\nThis comment \"variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty...\" is not precise. EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs. In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational. On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the \"Two problems with variational EM... \" paper by Turner and Sahani (2010).\n\nThere are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy? Was there any KL reweighting scheduling as done in the original BBB paper? \n\nWhat is the significance of the difference between BBB and BBB with sharpening in the language modelling task? Was sharpening used in the image caption generation task?\n\nWhat is the computational complexity of BBB with posterior sharpening? Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer? Would be interesting to see the time/accuracy frontier.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Recurrent Neural Networks","abstract":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\nFirstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\nSecondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\nWe also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.","pdf":"/pdf/39eb1abfbd69410249062e33af85d79e7255e880.pdf","TL;DR":" Variational Bayes scheme for Recurrent Neural Networks","paperhash":"anonymous|bayesian_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkp3uhxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper396/Authors"],"keywords":["Bayesian","Deep Learning","Recurrent Neural Networks","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1509739325636,"tcdate":1509111989148,"number":396,"cdate":1509739322975,"id":"Hkp3uhxCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hkp3uhxCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bayesian Recurrent Neural Networks","abstract":"In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.\nFirstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.\nSecondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.\nWe also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.","pdf":"/pdf/39eb1abfbd69410249062e33af85d79e7255e880.pdf","TL;DR":" Variational Bayes scheme for Recurrent Neural Networks","paperhash":"anonymous|bayesian_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkp3uhxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper396/Authors"],"keywords":["Bayesian","Deep Learning","Recurrent Neural Networks","LSTM"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}