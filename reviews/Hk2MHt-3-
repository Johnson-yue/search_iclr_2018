{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222616784,"tcdate":1511814958514,"number":2,"cdate":1511814958514,"id":"Hk8Nwx9xf","invitation":"ICLR.cc/2018/Conference/-/Paper3/Official_Review","forum":"Hk2MHt-3-","replyto":"Hk2MHt-3-","signatures":["ICLR.cc/2018/Conference/Paper3/AnonReviewer1"],"readers":["everyone"],"content":{"title":"simple approach, shows parameter-saving benefits of coupled ensembling","rating":"6: Marginally above acceptance threshold","review":"Strengths:\n* Very simple approach, amounting to coupled training of \"e\" identical copies  of a chosen net architecture, whose predictions are fused during training. This forces the different model instances to become more complementary.\n* Perhaps counterintuitively, experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training.\n* The practical advantages of the proposed approach are twofold:\n1. Given a fixed parameter budget, coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets.\n2. For the same accuracy, coupled ensembling yields significant parameter savings.\n\nWeaknesses:\n* Although results are very strong, the proposed models do not outperform the state-of-the-art, except for the models reported in Table 4, which however were obtained by *traditional* ensembling of coupled ensembles. \n* Coupled ensembling requires joint training of all nets in the ensemble and thus is limited by the size of the model that can be fit in memory. Conversely, traditional ensembling involves separate training of the different instances and this enables the learning of an arbitrary number of individual nets. \n* I am surprised by the results in Table 2, which suggest that the optimal number of nets in the ensemble is remarkably low (only 3!). It'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net.\n* Strictly speaking it is correct to refer to the individual nets in the ensembles as \"branches\" and \"basic blocks.\" Nevertheless, I find the use of these terms confusing in the context of the proposed approach, since they are commonly used to denote concepts different from those represented here.  I would recommend refraining from using these terms here.\n\nOverall, the paper provides limited technical novelty. Yet, it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coupled Ensembles of Neural Networks","abstract":"We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","pdf":"/pdf/15da2442af0730bdc007ec71877a1e08a7b64d08.pdf","TL;DR":"We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further.","paperhash":"anonymous|coupled_ensembles_of_neural_networks","_bibtex":"@article{\n  anonymous2018coupled,\n  title={Coupled Ensembles of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk2MHt-3-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper3/Authors"],"keywords":["Ensemble learning","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222616873,"tcdate":1511627322938,"number":1,"cdate":1511627322938,"id":"SJXrqMPgf","invitation":"ICLR.cc/2018/Conference/-/Paper3/Official_Review","forum":"Hk2MHt-3-","replyto":"Hk2MHt-3-","signatures":["ICLR.cc/2018/Conference/Paper3/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This work proposed a reconfiguration of the existing state-of-the-art CNN model using a new branching architecture.","rating":"6: Marginally above acceptance threshold","review":"This work proposed a reconfiguration of the existing state-of-the-art CNN model architectures including ResNet and DensNet. By introducing new branching architecture, coupled ensembles, they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget. Additionally, they also show that the proposed ensemble method results in better performance than other ensemble methods (For example, ensemble over independently trained models)  not only in combined mode but also in individual branches.\n\nPaper Strengths:\n* The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet-BC L = 118 k = 35 e = 3).\n* Detailed analysis on different ensemble fusion methods on both training time and testing time.\n* Simple but effective design to achieve a better result in testing time with same total parameter budget.\n\t\nPaper Weakness:\n* Some detail about different fusing method should be mentioned in the main paper instead of in the supplementary material.\n* In practice, how much more GPU memory is required to train the model with parallel branches (with same parameter budgets) because memory consumption is one of the main problems of networks with multiple branches.\n* At least one experiment should be carried out on a larger dataset such as ImageNet to further demonstrate the validity of the proposed method.\n* More analysis can be conducted on the training process of the model. Will it converge faster? What will be the total required training time to reach the same performance compared with single branch model with the same parameter budget?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coupled Ensembles of Neural Networks","abstract":"We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","pdf":"/pdf/15da2442af0730bdc007ec71877a1e08a7b64d08.pdf","TL;DR":"We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further.","paperhash":"anonymous|coupled_ensembles_of_neural_networks","_bibtex":"@article{\n  anonymous2018coupled,\n  title={Coupled Ensembles of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk2MHt-3-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper3/Authors"],"keywords":["Ensemble learning","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739536102,"tcdate":1507067156473,"number":3,"cdate":1509739533442,"id":"Hk2MHt-3-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk2MHt-3-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Coupled Ensembles of Neural Networks","abstract":"We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance. The use of branches brings an additional form of regularization. In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities. The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently. We refer to this branched architecture as \"coupled ensembles\". The approach is very generic and can be applied with almost any neural network architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","pdf":"/pdf/15da2442af0730bdc007ec71877a1e08a7b64d08.pdf","TL;DR":"We show that splitting a neural network into parallel branches improves performance and that proper coupling of the branches improves performance even further.","paperhash":"anonymous|coupled_ensembles_of_neural_networks","_bibtex":"@article{\n  anonymous2018coupled,\n  title={Coupled Ensembles of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk2MHt-3-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper3/Authors"],"keywords":["Ensemble learning","neural networks"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}