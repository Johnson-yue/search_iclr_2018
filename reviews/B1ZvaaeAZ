{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222652732,"tcdate":1512165411212,"number":3,"cdate":1512165411212,"id":"HJiml81-z","invitation":"ICLR.cc/2018/Conference/-/Paper434/Official_Review","forum":"B1ZvaaeAZ","replyto":"B1ZvaaeAZ","signatures":["ICLR.cc/2018/Conference/Paper434/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of WRPN","rating":"5: Marginally below acceptance threshold","review":"This paper presents an simple and interesting idea to improve the performance for neural nets. The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced). The paper is aiming to solve a practical problem, and has done some solid research work to validate that.  In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach.   \n\nIn addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet. The results are convincing to me. \n\nOn the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study. So the novelty is limited.\n\nSo overall given limited novelty but the paper presents useful results, I would recommend borderline leaning towards reject.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"WRPN: Wide Reduced-Precision Networks","abstract":"For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","pdf":"/pdf/bee63d65557938ab8bddf66c5e3ce69942edf18e.pdf","TL;DR":"Lowering precision (to 4-bits, 2-bits and even binary) and widening the filter banks gives as accurate network as those obtained with FP32 weights and activations.","paperhash":"anonymous|wrpn_wide_reducedprecision_networks","_bibtex":"@article{\n  anonymous2018wrpn:,\n  title={WRPN: Wide Reduced-Precision Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ZvaaeAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper434/Authors"],"keywords":["Low precision","binary","ternary","4-bits networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222652774,"tcdate":1511920724733,"number":2,"cdate":1511920724733,"id":"rJ6IEcsgM","invitation":"ICLR.cc/2018/Conference/-/Paper434/Official_Review","forum":"B1ZvaaeAZ","replyto":"B1ZvaaeAZ","signatures":["ICLR.cc/2018/Conference/Paper434/AnonReviewer1"],"readers":["everyone"],"content":{"title":"High accuracy at low-precision","rating":"9: Top 15% of accepted papers, strong accept","review":"This is a well-written paper with good comparisons to a number of earlier approaches. It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs. Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper.\n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction.\n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design. It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today. \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit)\n- Moving the state of the art in low precision forward\n- Strong potential impact, especially on constrained power environments (but not limited to them)\n- Uses same hyperparameters as original training, making the process of using this much simpler.\n\nCons/Questions\n- They mention not quantizing the first and last layer of every network. How much does that impact the overall compute? \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model? This could be interesting for low power case, even if the \"effective compute\" is larger than the baseline.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"WRPN: Wide Reduced-Precision Networks","abstract":"For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","pdf":"/pdf/bee63d65557938ab8bddf66c5e3ce69942edf18e.pdf","TL;DR":"Lowering precision (to 4-bits, 2-bits and even binary) and widening the filter banks gives as accurate network as those obtained with FP32 weights and activations.","paperhash":"anonymous|wrpn_wide_reducedprecision_networks","_bibtex":"@article{\n  anonymous2018wrpn:,\n  title={WRPN: Wide Reduced-Precision Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ZvaaeAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper434/Authors"],"keywords":["Low precision","binary","ternary","4-bits networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222652820,"tcdate":1511815853634,"number":1,"cdate":1511815853634,"id":"S1L25x5xz","invitation":"ICLR.cc/2018/Conference/-/Paper434/Official_Review","forum":"B1ZvaaeAZ","replyto":"B1ZvaaeAZ","signatures":["ICLR.cc/2018/Conference/Paper434/AnonReviewer2"],"readers":["everyone"],"content":{"title":"WRPN: Wide Reduced-Precision Networks","rating":"5: Marginally below acceptance threshold","review":"The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision. The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks. \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision.\n\nNegatives\n- While the exhaustive analysis is extremely useful the overall technical contribution of the paper that of widening the networks is fairly small. \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches. However, the results are more focused on compute cost. Also large batches are used mainly during training where memory is generally not a huge issue. Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes. It might help to emphasize the speed-up in compute more in the contributions.  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"WRPN: Wide Reduced-Precision Networks","abstract":"For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","pdf":"/pdf/bee63d65557938ab8bddf66c5e3ce69942edf18e.pdf","TL;DR":"Lowering precision (to 4-bits, 2-bits and even binary) and widening the filter banks gives as accurate network as those obtained with FP32 weights and activations.","paperhash":"anonymous|wrpn_wide_reducedprecision_networks","_bibtex":"@article{\n  anonymous2018wrpn:,\n  title={WRPN: Wide Reduced-Precision Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ZvaaeAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper434/Authors"],"keywords":["Low precision","binary","ternary","4-bits networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739305598,"tcdate":1509117273005,"number":434,"cdate":1509739302948,"id":"B1ZvaaeAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1ZvaaeAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"WRPN: Wide Reduced-Precision Networks","abstract":"For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","pdf":"/pdf/bee63d65557938ab8bddf66c5e3ce69942edf18e.pdf","TL;DR":"Lowering precision (to 4-bits, 2-bits and even binary) and widening the filter banks gives as accurate network as those obtained with FP32 weights and activations.","paperhash":"anonymous|wrpn_wide_reducedprecision_networks","_bibtex":"@article{\n  anonymous2018wrpn:,\n  title={WRPN: Wide Reduced-Precision Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ZvaaeAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper434/Authors"],"keywords":["Low precision","binary","ternary","4-bits networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}