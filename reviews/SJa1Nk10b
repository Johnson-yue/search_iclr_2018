{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222556497,"tcdate":1511904199879,"number":2,"cdate":1511904199879,"id":"rJeC7UixM","invitation":"ICLR.cc/2018/Conference/-/Paper111/Official_Review","forum":"SJa1Nk10b","replyto":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference/Paper111/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Similar to Prior Cascade Work, Unclear Weighing Schemes, and Lack of Experimental Comparison","rating":"5: Marginally below acceptance threshold","review":"1. Paper Summary\n\nThis paper adds a separate network at every layer of a residual network that performs classification. They minimize the loss of every classifier using two proposed weighting schemes. They also ensemble this model.\n\n\n2. High level paper\n\nThe organization of this paper is a bit confusing. Two weighing schemes are introduced in Section 3.1, then the ensemble model is described in Section 3.2, then the weighing schemes are justified in Section 4.1.\nOverall this method is essentially an cascade where each cascade classifier is a residual block. Every input is passed through as many stages as possible until the budget is reached. While this model is likely quite useful in industrial settings, I don't think the model itself is wholly original.\nThe authors have done extensive experiments evaluating their method in different settings. I would have liked to see a comparison with at least one other anytime method. I think it is slightly unfair to say that you are comparing with Xie & Tu, 2015 and Huang et al., 2017 just because they use the CONSTANT weighing schemes.\n\n\n3. High level technical\n\nI have a few concerns:\n- Why does AANN+LINEAR nearly match the accuracy of EANN+SIEVE near 3e9 FLOPS in Figure 4b but EANN+LINEAR does not in Figure 4a? Shouldn't EANN+LINEAR be strictly better than AANN+LINEAR?\n- Why do the authors choose these specific weighing schemes? Section 4.1 is devoted to explaining this but it is still unclear to me. They talk about there being correlation between the predictors near the end of the model so they don't want to distribute weight near the final predictors but this general observation doesn't obviously lead to these weighing schemes, they still seem a bit adhoc.\n\nA few other comments:\n- Figure 3b seems to contain strictly less information than Figure 4a, I would remove Figure 3b and draw lines showing the speedup you get for one or two accuracy levels.\n\nQuestions:\n- Section 3.1: \"Such an ideal Î¸* does not exist in general and often does not exist in practice.\" Why is this the case? \n- Section 3.1: \" In particular, spreading weights evenly as in (Lee et al., 2015) keeps all i away from their possible respective minimum\" Why is this true?\n- Section 3.1: \"Since we will evaluate near depth b3L/4e, and it\nis the center of L/2 low-weight layers, we increase it weight by 1/8.\" I am completely lost here, why do you do this?\n\n\n4. Review summary\n\nUltimately because the model itself resembles previous cascade models, the selected weighings have little justification, and there isn't a comparison with another anytime method, I think this paper isn't yet ready for acceptance at ICLR.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/3d61c0310479ecda3a0948856de1a4cfde4d800f.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222556536,"tcdate":1511782698094,"number":1,"cdate":1511782698094,"id":"HJMEt_FeM","invitation":"ICLR.cc/2018/Conference/-/Paper111/Official_Review","forum":"SJa1Nk10b","replyto":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference/Paper111/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Anytime neural network","rating":"7: Good paper, accept","review":"This paper proposes an anytime neural network, which can predict anytime while training. To achieve that, the model includes auxiliary predictions which can make early predictions. Specifically, the paper presents a loss weighting scheme that considers high correlation among nearby predictions, an oscillating loss weighting scheme for further improvement, and an ensemble of anytime neural networks. In the experiments, test error of the proposed model was shown to be comparable to the optimal one at each time budget. \n\nIt is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget. \n\n1. In Section 3.2, there are some discussions on the parallel computations of EANN. The parallel training is not clear to me and it would be great to have more explanation on this with examples.  \n\n2. It seems that EANN is not scalable because the depth is increasing exponentially. For example, given 10 machines, the model with the largest depth would have 2^10 layers, which is difficult to train. It would be great to discuss this issue.\n\n3. In the experiments, it would be great to add a few alternatives to be compared for anytime predictions. \n\n\n\n\n\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/3d61c0310479ecda3a0948856de1a4cfde4d800f.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]}},{"tddate":null,"ddate":null,"tmdate":1509739478097,"tcdate":1508991972780,"number":111,"cdate":1509739475443,"id":"SJa1Nk10b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/3d61c0310479ecda3a0948856de1a4cfde4d800f.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}