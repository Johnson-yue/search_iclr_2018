{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222575679,"tcdate":1512077591505,"number":3,"cdate":1512077591505,"id":"S117txRef","invitation":"ICLR.cc/2018/Conference/-/Paper145/Official_Review","forum":"B16yEqkCZ","replyto":"B16yEqkCZ","signatures":["ICLR.cc/2018/Conference/Paper145/AnonReviewer1"],"readers":["everyone"],"content":{"title":"DQN and catastrophic forgetting","rating":"7: Good paper, accept","review":"The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that “DQNs  are susceptible to periodically repeating mistakes”. I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues.\n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions. In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel. \n\nStill, many of the design choices appear quite arbitrary and can most likely be improved upon. In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal. Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest. The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong. \n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning. A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1511824367901,"tcdate":1511824345117,"number":2,"cdate":1511824345117,"id":"BybJ2MqxG","invitation":"ICLR.cc/2018/Conference/-/Paper145/Official_Comment","forum":"B16yEqkCZ","replyto":"S16Q2IXlM","signatures":["ICLR.cc/2018/Conference/Paper145/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper145/Authors"],"content":{"title":"Of course!","comment":"Happy to share details - pardon the delay due to holiday travel. Need to get back home to look up exact details on hyperparameter settings as the toy environment experiments were done a while ago.\n\nYes the hyper-parameters can make a big difference on many of these problems. Optimizer, number of exploration turns, etc. There's also a large amount of variance across runs. Especially on the toy environments. That's why we run every experiment multiple times and report averages.  \n\nThanks for the questions and for holding tight, more details on toy environments coming soon!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1512222576220,"tcdate":1511815729264,"number":2,"cdate":1511815729264,"id":"SkYNcg5xz","invitation":"ICLR.cc/2018/Conference/-/Paper145/Official_Review","forum":"B16yEqkCZ","replyto":"B16yEqkCZ","signatures":["ICLR.cc/2018/Conference/Paper145/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea, but some potential issues.","rating":"4: Ok but not good enough - rejection","review":"The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states. The authors propose to train a predictive ‘fear model’ that penalizes states that lead to catastrophes. The proposed technique is validated both empirically and theoretically. \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN. Nonetheless, there are some criticisms than can be made of both the method and the evaluations:\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning. Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally. There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable. No empirical results on the effect of the parameter are given.\n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning.The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies.  This problem does not seem to be really solved by this method. Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier. While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties. As such the method wouldn’t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state. It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited. \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method. The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]).\n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  ‘if the fear model reaches arbitrarily high accuracy’ or ‘if the probability is negligible’.\n\nIt is wasn’t clear to me that the properties described in Theorem 1 actually hold. The motivation in the appendix is very informal and no clear derivation is provided. The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states. However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states.  It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty. It is therefore not clear to me that any claims can be made about its performance without additional assumptions.\n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state. Any optimal policy would therefore need to spend some time e in the danger state, on average. A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards. E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax. By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin).  This seems to contradict the theorem. It wasn’t clear what assumptions the authors make to exclude situations like this.\n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babuška, \"Improved deep reinforcement learning for robotics through distribution-based experience retention,\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952.\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1511382117971,"tcdate":1511382053091,"number":2,"cdate":1511382053091,"id":"S16Q2IXlM","invitation":"ICLR.cc/2018/Conference/-/Paper145/Public_Comment","forum":"B16yEqkCZ","replyto":"B16yEqkCZ","signatures":["~Zhe_Du1"],"readers":["everyone"],"writers":["~Zhe_Du1"],"content":{"title":"Post Parameters?","comment":"Hi,\n\nThis is a nice paper and we like the ideas in it! I tried to implement the algorithm DQN-Fear by modifying the baseline DQN, and reproduce the simulation results in your paper. The thing is, based on our trials with different parameters so far, we have some difficulty to reproduce part of the result. For example, (1) for the CartPole test, the DQN runs more than 10000 episodes within 4e6 time steps, while in your paper, there are only 4000 episodes;  (2) (this is more weird) for Freeway, our DQN achieves better performance than the plot of DQN-Fear in your paper within just 300 episodes. I guess this may be largely due to the hyper parameters.\n\nWe really appreciate your code on GitHub, and we can see the parameters for Atari games. But the hyperparameters for Adventure seeker and Cartpole are still unclear.  So, I am wondering if it's possible you share the hyper-parameters for DQN and DQN-Fear on all three experiments? The hyper-paramters could include but not limit to the following:\n(1) AdamLearning rates for the two neural nets of DQN and fear model\n(2) Buffer sizes for all 3 buffers\n(3) How exploration rate is scheduled\n(4) Train frequency\n(5) Batch size\n(6) When do the the learning start for the two neural nets of DQN and fear model\n(7) discount factor gamma\n(8) Target network update frequency\n(9) fear factor\n(10) fear phase-in length\n(11) fear radius\n\nThank you!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1511134945918,"tcdate":1511134945918,"number":1,"cdate":1511134945918,"id":"rJckvqkgz","invitation":"ICLR.cc/2018/Conference/-/Paper145/Official_Comment","forum":"B16yEqkCZ","replyto":"rk3V5Hjkf","signatures":["ICLR.cc/2018/Conference/Paper145/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper145/Authors"],"content":{"title":"Posted Code","comment":"Hi Nick,\n\nThanks for your interest in our paper! The code is actually open sourced now, and already one group of researchers has re-implemented our algorithm from scratch and confirmed outperformance of DQN. To preserve double blind status, we won't post the GitHub link here but it's not too hard to find.\n\nCheers,\n\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1512222576271,"tcdate":1510960269304,"number":1,"cdate":1510960269304,"id":"SyHc3kp1f","invitation":"ICLR.cc/2018/Conference/-/Paper145/Official_Review","forum":"B16yEqkCZ","replyto":"B16yEqkCZ","signatures":["ICLR.cc/2018/Conference/Paper145/AnonReviewer3"],"readers":["everyone"],"content":{"title":"There could be many other base-line ideas that could avoid the catastrophic scenarios.","rating":"5: Marginally below acceptance threshold","review":"\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.  The fear model is trained in parallel to predict catastrophic states.  Its output is used to penalize the Q learning target.\n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states. Because it does not experience it any more.  Shouldn’t the agent stop learning at some point in time?  Why does it need to keep collecting good data?  How about giving more weight to catastrophic data (e.g., replicating it)\n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation?\n\nWhy not specify catastrophic states with a large negative reward?\n\nIt seems that catastrophe states need to be experienced at least once.\nIs that acceptable for the autonomous car hitting a pedestrian?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1510853171559,"tcdate":1510853171559,"number":1,"cdate":1510853171559,"id":"rk3V5Hjkf","invitation":"ICLR.cc/2018/Conference/-/Paper145/Public_Comment","forum":"B16yEqkCZ","replyto":"B16yEqkCZ","signatures":["~Nick_Linck1"],"readers":["everyone"],"writers":["~Nick_Linck1"],"content":{"title":"Source Code","comment":"I was wondering if you have the code open sourced so that we can more easily reproduce the results provided in the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]}},{"tddate":null,"ddate":null,"tmdate":1509739461372,"tcdate":1509037029107,"number":145,"cdate":1509739458717,"id":"B16yEqkCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B16yEqkCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Avoiding Catastrophic States with Intrinsic Fear","abstract":"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","pdf":"/pdf/933a9cef1adc8d2db5d62a4b20cad5824ae1b489.pdf","TL;DR":"Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.","paperhash":"anonymous|avoiding_catastrophic_states_with_intrinsic_fear","_bibtex":"@article{\n  anonymous2018avoiding,\n  title={Avoiding Catastrophic States with Intrinsic Fear},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16yEqkCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper145/Authors"],"keywords":["reinforcement learning","safe exploration","dqn"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}