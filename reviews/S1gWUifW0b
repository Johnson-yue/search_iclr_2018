{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222820487,"tcdate":1511889131268,"number":3,"cdate":1511889131268,"id":"Sy7lYfixz","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"- This paper considers next step model prediction error as an intrinsic reward signal for RL agents. It combines this with several auxiliary tasks such as inverse dynamics prediction, hindsight experience replay and so on. The main conclusion is that training with this intrinsic reward performs similar to some of the considered aux tasks. So therefore the authors conclude that the curiosity reward likely encourages bootstrapping of purposeful features. \n\n- The curiosity signal is basically the same as in Pathak et al. (2017) with minor implementation level differences. Defining intrinsic rewards as error in the transition model can lead to hairy issues. This has been extensively studied in Ouyeder et al (c.f. What is intrinsic motivation? A typology of computational approaches). So the basic premise of the curiosity considered here is riddled with conceptual problems. This is my biggest issue with the setup. \n\n- How robust are the empirical returns curve? (mean/std)\n\n- Does this algorithm help with better exploration when combined with rewards coming from the environment?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222820528,"tcdate":1511796122471,"number":2,"cdate":1511796122471,"id":"rJMoToYlz","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Limited novelty","rating":"4: Ok but not good enough - rejection","review":"The authors present a derivation of previous work of [1]. In particular they propose the method of using the error signal of a dynamics model as curiosity for exploration, such as [1], but without any additionaly auxiliary methods. This the author call Curiosity by Bootstrapping Feature (CBF).\n  \nIn particular they show over  a set of auxiliary learning methods (hindsight ER, inverse dynamics model[1]) there is\nnot a clear cut edge one method has over the other (or over using no auxilirary method all, that is CBF).\n\nOverall I think the novelty is too limited for acceptance. The main point of the authors (heterogeneous results\nover different auxilirary learning methods),  is not suprising at all, and to be expected. The method the authors introduce\nis just a submodule of already published results[1].\n\nFor instance, section 4 discusses challenges related to these class of approaches such as the presence of stochasticity. Had the authors proposed a solution to these challenges that would have benefited the paper greatly.\n\nMinor: The light green link color make the paper hard on the eye, I suggest using [hidelinks] for hyperref.\nFigure 2 is  very small and hard to read.\n\n\n[1] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by\nself-supervised prediction. In ICML, 2017","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222820569,"tcdate":1511591144213,"number":1,"cdate":1511591144213,"id":"SyelTYIgM","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Lacking interesting results. ","rating":"6: Marginally above acceptance threshold","review":"Clarity \nThe paper is well written and clear. \n\nOriginality\nThe proposed Curiosity by Bootstrapping Features can be viewed as a simplified version of Pathak et al., 2017. But no significant advantage of CBF is demonstrated.  \n\nSignificance\n- The empirical results might be helpful in a better understanding of Pathak et al., 2017. \n- The average return could be a better metric. And some plots require standard errors. \n  \nPros:\n- The paper is well written and clear. \n- The paper provides additional results for Pathak et al., 2017.\n\nCons:\n- No effective new method is demonstrated. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092385571,"tcdate":1509137233260,"number":933,"cdate":1510092362273,"id":"S1gWUifW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}