{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222626083,"tcdate":1512193723010,"number":2,"cdate":1512193723010,"id":"SyQTA31bG","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Review","forum":"BkIkkseAZ","replyto":"BkIkkseAZ","signatures":["ICLR.cc/2018/Conference/Paper363/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper studies the theoretical properties of the two-layer neural networks. \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data. \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. \n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global. But the paper cannot establish such a result. \n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al.\n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W. result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.  \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum. It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1512222626124,"tcdate":1511723806388,"number":1,"cdate":1511723806388,"id":"BJUmm5_lz","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Review","forum":"BkIkkseAZ","replyto":"BkIkkseAZ","signatures":["ICLR.cc/2018/Conference/Paper363/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution. I think there are some interesting arguments made in the paper. However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims.","rating":"6: Marginally above acceptance threshold","review":"This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution. I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3. However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims. Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here.\n\nDetailed comments:\n1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper\nExample 1 from abstract: \n“We show that for a wide class of differentiable activation functions (this class involved “almost” all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.”\n\nThis is certainly not proven and in fact not formally stated anywhere in the paper. Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this. \n\nExample 2 from intro when comparing with other results on page 2:\nThe authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity). However as explained above the final conclusions are also significantly weaker than this prior literature so it’s a bit of apples vs oranges comparison.\n\n2) Page 2 minor typos\nWe study training problem -->we study the training problem\nIn the regime training objective--> in the regime the training objective\n\n3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al\n\n4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W. It would also be nice to compare with Soudry et. al.\n\n5) Argument on top of page 6 is incorrect as the global optima is data dependent and hence lemma 4.1 (which is for a fixed matrix) does not apply\n\n6) Section 5 on page 6. Again the stated conclusion here that the iterates do not lead to singular W is much weaker than the claims made early on.\n \n7) I haven’t had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done!\n\n8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al \n\n9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W. By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1511373621585,"tcdate":1511373621585,"number":4,"cdate":1511373621585,"id":"ryRVo4QeM","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Comment","forum":"BkIkkseAZ","replyto":"rkQ3TFMxG","signatures":["ICLR.cc/2018/Conference/Paper363/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper363/Authors"],"content":{"title":"Global convergence","comment":"Yes, for global convergence we need to prove that smallest singular value of matrix D does not decrease at the rate faster than 1/N (It may go to zero but the rate should not be faster than 1/N). We were unable to show this for the general setting where a simple noise is added to theta. Any suggestion in that direction are most welcome."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1511329195530,"tcdate":1511329195530,"number":3,"cdate":1511329195530,"id":"rkQ3TFMxG","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Comment","forum":"BkIkkseAZ","replyto":"SJMDtxGlz","signatures":["ICLR.cc/2018/Conference/Paper363/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper363/AnonReviewer3"],"content":{"title":"order of quantifier","comment":"But then this lemma 5.2,5.3 is sensitive in the sense that the resulting matrix W_k can be full rank but it may be very ill-conditioned (e.g., the least singular value can converge to zero very fast?)?  I guess this is why you couldn't prove that it converges to a global minimum? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1511291225855,"tcdate":1511291225855,"number":2,"cdate":1511291225855,"id":"SJMDtxGlz","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Comment","forum":"BkIkkseAZ","replyto":"rypGH4bgG","signatures":["ICLR.cc/2018/Conference/Paper363/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper363/Authors"],"content":{"title":"Convergence results and Corollary 4.2","comment":"In corollary 4.2, matrix W is independent of data. That is indeed a problem we notice and address in section 5. Immediately after lemma 5.1, we discuss the exact problem you are referring to. We use the ideas developed in lemma 4.1/corollary 4.2 to show that even though you assume that data is fixed, using the randomness of stochastic theta, one can show that Algorithm 1 is robust and W_k generated by this algorithm achieves the same guarantees as that of W in corollary 4.2. Robustness is derived solely from lebesgue measure on theta."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1511240981498,"tcdate":1511240981498,"number":1,"cdate":1511240981498,"id":"rypGH4bgG","invitation":"ICLR.cc/2018/Conference/-/Paper363/Official_Comment","forum":"BkIkkseAZ","replyto":"BkIkkseAZ","signatures":["ICLR.cc/2018/Conference/Paper363/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper363/AnonReviewer3"],"content":{"title":"order of quantifier","comment":"What's the order of the quantifier in Corollary 4.2? It seems that the samples are sampled after W is fixed? Then the result doesn't seem to be useful, because the whole point of previous work such as SC, XLS is to prove the uniform result so that W can be chosen after samples are fixed.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]}},{"tddate":null,"ddate":null,"tmdate":1509739342797,"tcdate":1509105373983,"number":363,"cdate":1509739340147,"id":"BkIkkseAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkIkkseAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Theoretical properties of the global optimizer of two-layer Neural Network","abstract":"In this paper, we study the problem of optimizing a two-layer artificial neural network that best fits a training dataset. We look at this problem in the setting where the number of parameters is greater than the number of sampled points. We show that for a wide class of differentiable activation functions (this class involves \"almost\" all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular. Our results are easily extended to hidden layers given by a flat matrix from that of a square matrix. Results are applicable even if network has more than one hidden layer provided all hidden layers satisfy non-singularity, all activations are from the given \"good\" class of differentiable functions and optimization is only with respect to last hidden layer. We also study the smoothness properties of the objective function and show that it is actually Lipschitz smooth, i.e., its gradients do not change sharply. We use smoothness properties to guarantee asymptotic convergence of $O(1/\\text{number of iterations})$ to a first-order optimal solution. We also show that our algorithm will maintain non-singularity of hidden layer for any finite number of iterations.\n","pdf":"/pdf/84055a275c202618d619bbaadce6d161e3b122bc.pdf","TL;DR":"This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case","paperhash":"anonymous|theoretical_properties_of_the_global_optimizer_of_twolayer_neural_network","_bibtex":"@article{\n  anonymous2018theoretical,\n  title={Theoretical properties of the global optimizer of two-layer Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkIkkseAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper363/Authors"],"keywords":["Non-convex optimization","Two-layer Neural Network","global optimality","first-order optimality"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}