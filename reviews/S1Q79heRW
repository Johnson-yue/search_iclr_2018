{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222637023,"tcdate":1511830627151,"number":3,"cdate":1511830627151,"id":"BkjD4Eqxz","invitation":"ICLR.cc/2018/Conference/-/Paper400/Official_Review","forum":"S1Q79heRW","replyto":"S1Q79heRW","signatures":["ICLR.cc/2018/Conference/Paper400/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"This work proposes to learn word vectors that are intended to specifically model the lexical entailment relationship. This is achieved in an unsupervised manner from unstructured data, through an approach heavily influenced by recent work by Henderson and Popa, which \"reinterprets word2vec\" by modeling distributions over discrete latent \"pseudo-phrase\" vectors. That is, instead of using two vectors per word, as in word2vec, a latent representation is introduced that models the joint properties of the target and context words. While Henderson and Popa represent the latent vector as the evidence for the target and context, or the likelihood, this work suggests to represent it based on the posterior distribution instead. The resultant representations are evaluated on Weeds et al.'s (2014) version of BLESS, as well as the full BLESS dataset, where they do better than the original.\n\nThe paper is confusingly written, fails to mention a lot of related work, has a weak evaluation where it doesn't compare to related systems, and I feel that it would benefit from \"toning down\". Hence, I do not recommend it for acceptance. In more detail:\n\n1. The idea behind Henderson and Popa's model, as well as the suggested modification, should be easy to explain, but I really had to struggle to make sense of it. This work relies very heavily on that paper, and would be better off if it was more standalone. I think part of the confusion stems from using y for the latent representation but not specifying whether it is a word or latent representation in Equation 1 - that only becomes obvious later. The exposition clearly needs more work, and more precise technical writing.\n\n2. There is a lot of related work around word embeddings that is not mentioned, both on word2vec-style representation learning (e.g. it would be useful to relate this more to word2vec and what it learns, as in Omer Levy's work on \"interpreting\" word2vec, rather than reinterpreting) and word embeddings on hypernymy detection and lexical entailment (see e.g. Stephen Roller's thesis for references).\n\n3. There has been a lot of work on the Weeds BLESS dataset that is not mentioned, or compared against, including unsupervised approaches (e.g. Levy's work, Santus's work, Kiela's work, Roller's work, etc.), that perform better than the numbers in Table 1. There are many other datasets that measure lexical entailment, none of which are evaluated on (apart from the original BLESS set, which is mentioned in passing). It would make sense to show that the method works on more than one dataset, and to do a thorough comparison against other work; especially given that:\n\n4. The tone of the work appears to imply that word2vec was wrong and needs to be reinterpreted: the work leads to \"unprecedented results\" (not true), claims to be a completely novel method for inducing word representations (together with LSA, BOW and Word2Vec, third paragraph; not true), and suggests it has found \"the best way to extract information about the semantics of a word from this model\" (7th paragraph; not true). This, together with the \"reinterpretation of word2vec\" and the proposed \"new distributional semantic models\" almost makes it hard for me to take the work seriously.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Entailment-Vector Word Embeddings","abstract":"Entailment vectors are a principled way to encode in a vector what information is known and what is unknown.  They are designed to model relations where one vector should include all the information in another vector, called entailment.  This paper investigates the unsupervised learning of entailment vectors for the semantics of words.  Using simple entailment-based models of the semantics of words in text (distributional semantics), we induce entailment-vector word embeddings which outperform the best previous results for predicting entailment between words, in unsupervised and semi-supervised experiments on hyponymy.\n","pdf":"/pdf/c5ca07769b8a1f426a7c7dedca68b58f3396df87.pdf","TL;DR":"We train word embeddings based on entailment instead of similarity, successfully predicting lexical entailment.","paperhash":"anonymous|unsupervised_learning_of_entailmentvector_word_embeddings","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Entailment-Vector Word Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Q79heRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper400/Authors"],"keywords":["word embeddings","natural language semantics","entailment","unsupervised learning","distributional semantics"]}},{"tddate":null,"ddate":null,"tmdate":1512222637065,"tcdate":1511599676496,"number":2,"cdate":1511599676496,"id":"SJ4HCiUgz","invitation":"ICLR.cc/2018/Conference/-/Paper400/Official_Review","forum":"S1Q79heRW","replyto":"S1Q79heRW","signatures":["ICLR.cc/2018/Conference/Paper400/AnonReviewer2"],"readers":["everyone"],"content":{"title":"a word embedding algorithm for  lexical entailment. Good experimental results.  ","rating":"7: Good paper, accept","review":"The paper presents a word embedding algorithm for lexical entailment. The paper follows the work of Henderson and Popa (ACL,2016) that presented an interpretation of word2vec word representation in which each feature in a word vector corresponds to the probability of it being known/unknown, and suggested an operator to compute the degree of entailment between two words. In this paper, the authors train the word2vec algorithms to directly optimize the objective function suggested by Henderson and Popa (2016),\nI find the paper interesting. The proposed approach is novel and not standard and the paper reports significant improvement of entailment results  compared to previous state of the art\n \nThe method part of the paper (sections 2.1 and 2.2 ) which is the main contribution is not clearly written. The paper heavily relies on Henderson and Popa (2016). You dont need to restate in the current paper all the mathematical analysis that appears in the previous paper. You are expected, however, that the model description and the notation that is used here should be clearly explained. Maybe you can also add algorithm box. I think that the author should prepared a revised version of section 2.\n\nIn Word2vec, Levy and Goldberg provided an elegant analysis of the algorithm and showed that the global optimum is obtained at the PMI matrix. Can you derive a similar analysis for your variant of the word2vec algorithm?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Entailment-Vector Word Embeddings","abstract":"Entailment vectors are a principled way to encode in a vector what information is known and what is unknown.  They are designed to model relations where one vector should include all the information in another vector, called entailment.  This paper investigates the unsupervised learning of entailment vectors for the semantics of words.  Using simple entailment-based models of the semantics of words in text (distributional semantics), we induce entailment-vector word embeddings which outperform the best previous results for predicting entailment between words, in unsupervised and semi-supervised experiments on hyponymy.\n","pdf":"/pdf/c5ca07769b8a1f426a7c7dedca68b58f3396df87.pdf","TL;DR":"We train word embeddings based on entailment instead of similarity, successfully predicting lexical entailment.","paperhash":"anonymous|unsupervised_learning_of_entailmentvector_word_embeddings","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Entailment-Vector Word Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Q79heRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper400/Authors"],"keywords":["word embeddings","natural language semantics","entailment","unsupervised learning","distributional semantics"]}},{"tddate":null,"ddate":null,"tmdate":1512222637104,"tcdate":1511375926920,"number":1,"cdate":1511375926920,"id":"r1kr4BQgG","invitation":"ICLR.cc/2018/Conference/-/Paper400/Official_Review","forum":"S1Q79heRW","replyto":"S1Q79heRW","signatures":["ICLR.cc/2018/Conference/Paper400/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Model is unclear, evaluation needs expansion","rating":"3: Clear rejection","review":"I'm finding this paper really difficult to understand. The introduction is very abstract, and it is hard for me to understand the model as it is explained at the moment. Could the authors please clarify, perhaps in more algorithmic terms, how the model works?\n\nAs for the evaluation, BLESS is a nice dataset, but it certainly isn't enough to make a broad claim because it has certain artifacts in the way negative examples were constructed. I recommend looking at the collection of datasets used by Levy et al. [1] and Shwartz et al. [2], and evaluating on their union.\n\nAnother discrepancy that appears in the paper is that the authors cite Shwartz et al. [2] as achieving 44.1% average precision on BLESS, when in fact, this number reflects their performance on the WordNet-based dataset created by Weeds et al. [3].\n\n[1] http://www.aclweb.org/anthology/N15-1098\n[2] http://aclweb.org/anthology/E/E17/E17-1007.pdf \n[3] http://sro.sussex.ac.uk/53103/1/C14-1212.pdf \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Entailment-Vector Word Embeddings","abstract":"Entailment vectors are a principled way to encode in a vector what information is known and what is unknown.  They are designed to model relations where one vector should include all the information in another vector, called entailment.  This paper investigates the unsupervised learning of entailment vectors for the semantics of words.  Using simple entailment-based models of the semantics of words in text (distributional semantics), we induce entailment-vector word embeddings which outperform the best previous results for predicting entailment between words, in unsupervised and semi-supervised experiments on hyponymy.\n","pdf":"/pdf/c5ca07769b8a1f426a7c7dedca68b58f3396df87.pdf","TL;DR":"We train word embeddings based on entailment instead of similarity, successfully predicting lexical entailment.","paperhash":"anonymous|unsupervised_learning_of_entailmentvector_word_embeddings","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Entailment-Vector Word Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Q79heRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper400/Authors"],"keywords":["word embeddings","natural language semantics","entailment","unsupervised learning","distributional semantics"]}},{"tddate":null,"ddate":null,"tmdate":1509739323455,"tcdate":1509112347428,"number":400,"cdate":1509739320791,"id":"S1Q79heRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1Q79heRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unsupervised Learning of Entailment-Vector Word Embeddings","abstract":"Entailment vectors are a principled way to encode in a vector what information is known and what is unknown.  They are designed to model relations where one vector should include all the information in another vector, called entailment.  This paper investigates the unsupervised learning of entailment vectors for the semantics of words.  Using simple entailment-based models of the semantics of words in text (distributional semantics), we induce entailment-vector word embeddings which outperform the best previous results for predicting entailment between words, in unsupervised and semi-supervised experiments on hyponymy.\n","pdf":"/pdf/c5ca07769b8a1f426a7c7dedca68b58f3396df87.pdf","TL;DR":"We train word embeddings based on entailment instead of similarity, successfully predicting lexical entailment.","paperhash":"anonymous|unsupervised_learning_of_entailmentvector_word_embeddings","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Entailment-Vector Word Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Q79heRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper400/Authors"],"keywords":["word embeddings","natural language semantics","entailment","unsupervised learning","distributional semantics"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}