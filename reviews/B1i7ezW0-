{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222745152,"tcdate":1511696126619,"number":3,"cdate":1511696126619,"id":"HyvZDmueM","invitation":"ICLR.cc/2018/Conference/-/Paper763/Official_Review","forum":"B1i7ezW0-","replyto":"B1i7ezW0-","signatures":["ICLR.cc/2018/Conference/Paper763/AnonReviewer2"],"readers":["everyone"],"content":{"title":"SEMI-SUPERVISED LEARNING VIA NEW DEEP NETWORK INVERSION","rating":"7: Good paper, accept","review":"This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks. The numerical experiments show a significant improvement in accuracy of the approach.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-Supervised Learning via New Deep Network Inversion","abstract":"We exploit a recently derived inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that applies to a wide range of systems and problems. \nThe approach outperforms current state-of-the-art methods on MNIST for small label set. Experiments with one-dimensional signals highlight the generality of the method. Importantly, our approach is simple, efficient, and requires no change in the deep network architecture. ","pdf":"/pdf/6441881122da9bb96e9a323356a4da50816d8aea.pdf","TL;DR":"We exploit an inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that outperforms current state-of-the-art methods on MNIST.","paperhash":"anonymous|semisupervised_learning_via_new_deep_network_inversion","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-Supervised Learning via New Deep Network Inversion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1i7ezW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper763/Authors"],"keywords":["inversion scheme","deep neural networks","semi-supervised learning","MNIST"]}},{"tddate":null,"ddate":null,"tmdate":1512222745188,"tcdate":1511535677178,"number":2,"cdate":1511535677178,"id":"HkBSVnBxG","invitation":"ICLR.cc/2018/Conference/-/Paper763/Official_Review","forum":"B1i7ezW0-","replyto":"B1i7ezW0-","signatures":["ICLR.cc/2018/Conference/Paper763/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Some comments on the related work, motivation and experiments","rating":"4: Ok but not good enough - rejection","review":"In summary, the paper is based on a recent work Balestriero & Baraniuk 2017 to do semi-supervised learning. In Balestriero & Baraniuk, it is shown that any DNN can be approximated via a linear spline and hence can be inverted to produce the \"reconstruction\" of the input, which can be naturally used to do unsupervised or semi-supervised learning. This paper proposes to use automatic differentiation to compute the inverse function efficiently. The idea seems interesting. However, I think there are several main drawbacks, detailed as follows:\n\n1. The paper lacks a coherent and complete review of the semi-supervised deep learning. Herewith some important missing papers, which are the previous or current state-of-the-art.\n\n[1] Laine S, Aila T. Temporal Ensembling for Semi-Supervised Learning[J]. arXiv preprint arXiv:1610.02242, ICLR 2016.\n[2] Li C, Xu K, Zhu J, et al. Triple Generative Adversarial Nets[J]. arXiv preprint arXiv:1703.02291, NIPS 2017.\n[3] Dai Z, Yang Z, Yang F, et al. Good Semi-supervised Learning that Requires a Bad GAN[J]. arXiv preprint arXiv:1705.09783, NIPS 2017.\n\nBesides, some papers should be mentioned in the related work such as Kingma et. al. 2014. I'm not an expert of the network inversion and not sure whether the related work of this part is sufficient or not.\n\n2. The motivation is not sufficient and not well supported. \n\nAs stated in the introduction, the authors think there are several drawbacks of existing methods including \"training instability, lack of topology generalization and computational complexity.\" Based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not.  The generative approaches based on VAEs and GANs are time consuming, but according to my experience, the training of VAE-based methods are stable and the topology generalization ability of such methods are good. Besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures.  Overall, I think the drawbacks mentioned in the paper are not common in existing methods and I do not see clear benefits of the proposed method. Again, I strongly suggest the authors to provide a complete review of the literature.\n\nFurther, please explicitly support your claim via experiments. For instance, the proposed method should be compared  with the discriminative approaches including VAT and [1] in terms of the training efficiency. It's not fair to say GAN-based methods require more training time because these methods can do generation and style-class disentanglement while the proposed method cannot.\n\n3. The experimental results are not so convincing. \n\nFirst, please systematically compare your methods with existing methods on the widely adopted benchmarks including MNIST with 20, 100 labels and SVHN with 500, 1000 labels and CIFAR10 with 4000 labels. It is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting.\n\nSecond, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model. Resnet is powerful but previous methods did not use that.\n\nLast, show the sensitive results of the proposed method by tuning alpha and beta. For instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not?\n\nI think the quality of the paper should be further improved by addressing these problems and currently it should be rejected.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-Supervised Learning via New Deep Network Inversion","abstract":"We exploit a recently derived inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that applies to a wide range of systems and problems. \nThe approach outperforms current state-of-the-art methods on MNIST for small label set. Experiments with one-dimensional signals highlight the generality of the method. Importantly, our approach is simple, efficient, and requires no change in the deep network architecture. ","pdf":"/pdf/6441881122da9bb96e9a323356a4da50816d8aea.pdf","TL;DR":"We exploit an inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that outperforms current state-of-the-art methods on MNIST.","paperhash":"anonymous|semisupervised_learning_via_new_deep_network_inversion","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-Supervised Learning via New Deep Network Inversion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1i7ezW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper763/Authors"],"keywords":["inversion scheme","deep neural networks","semi-supervised learning","MNIST"]}},{"tddate":null,"ddate":null,"tmdate":1512222745226,"tcdate":1511394222631,"number":1,"cdate":1511394222631,"id":"BJDhjYXlz","invitation":"ICLR.cc/2018/Conference/-/Paper763/Official_Review","forum":"B1i7ezW0-","replyto":"B1i7ezW0-","signatures":["ICLR.cc/2018/Conference/Paper763/AnonReviewer3"],"readers":["everyone"],"content":{"title":"plausible idea needs better motivation and demonstration","rating":"5: Marginally below acceptance threshold","review":"This paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning.\n\nPros:\n\nThe intuition is that the ReLU network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in PCA. Realizing that the linear mapping is the derivative of network output w.r.t. the input (the Jacobian), the authors proposed to use the reconstruction loss defined in (8). Different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the \"derivative\".  This observation is neat in my opinion, and does suggest a different use of the Jacobian in deep learning.  The related work include auto-encoders where the weights of symmetric layers are tied. \n\nCons:\n\nThe motivation (Section 2) needs to be improved. In particular, the introduction/review of the work of Balestriero and Baraniuk 2017 not very useful to the readers.  Notations in eqns (2) and (3) are not fully explained (e.g., boldface c). Intuition and implications of Theorem 1 is not sufficiently discussed: what do you mean by optimal DNN, what is the criteria for optimality? is there a generative assumption of the data underlying the theorem? and the assumption of all samples being norm 1 seems too strong and perhaps limits its application? As far as I see, section 2 is somewhat detached from the rest of the paper.\n\nThe main contribution of this paper is supposed to be the reconstruction mapping (6) and its effect in semi-supervised learning. The introduction of entropy regularization in sec 2.3 seems somewhat odd and obscures the contribution. It also bears the questions that how important is the entropy regularization vs. the reconstruction loss. In experiments, results with beta=1.0 need to be presented to assess the importance of network inversion and the reconstruction loss. Also, a comparison against typical auto-encoders (which uses another decoder networks, with weights possibly tied with the encoder networks) is missing.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-Supervised Learning via New Deep Network Inversion","abstract":"We exploit a recently derived inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that applies to a wide range of systems and problems. \nThe approach outperforms current state-of-the-art methods on MNIST for small label set. Experiments with one-dimensional signals highlight the generality of the method. Importantly, our approach is simple, efficient, and requires no change in the deep network architecture. ","pdf":"/pdf/6441881122da9bb96e9a323356a4da50816d8aea.pdf","TL;DR":"We exploit an inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that outperforms current state-of-the-art methods on MNIST.","paperhash":"anonymous|semisupervised_learning_via_new_deep_network_inversion","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-Supervised Learning via New Deep Network Inversion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1i7ezW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper763/Authors"],"keywords":["inversion scheme","deep neural networks","semi-supervised learning","MNIST"]}},{"tddate":null,"ddate":null,"tmdate":1509739116934,"tcdate":1509134371493,"number":763,"cdate":1509739114262,"id":"B1i7ezW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1i7ezW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Semi-Supervised Learning via New Deep Network Inversion","abstract":"We exploit a recently derived inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that applies to a wide range of systems and problems. \nThe approach outperforms current state-of-the-art methods on MNIST for small label set. Experiments with one-dimensional signals highlight the generality of the method. Importantly, our approach is simple, efficient, and requires no change in the deep network architecture. ","pdf":"/pdf/6441881122da9bb96e9a323356a4da50816d8aea.pdf","TL;DR":"We exploit an inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that outperforms current state-of-the-art methods on MNIST.","paperhash":"anonymous|semisupervised_learning_via_new_deep_network_inversion","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-Supervised Learning via New Deep Network Inversion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1i7ezW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper763/Authors"],"keywords":["inversion scheme","deep neural networks","semi-supervised learning","MNIST"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}