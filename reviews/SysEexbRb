{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222683191,"tcdate":1511831813439,"number":3,"cdate":1511831813439,"id":"SJ6btV9gz","invitation":"ICLR.cc/2018/Conference/-/Paper549/Official_Review","forum":"SysEexbRb","replyto":"SysEexbRb","signatures":["ICLR.cc/2018/Conference/Paper549/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting work on the characterization of critical points of neural networks","rating":"6: Marginally above acceptance threshold","review":"This paper mainly focuses on the square loss function of linear networks. It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks. Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks. As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed. In general, this manuscript is well written.   \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks. \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn’t require assumptions on parameter dimensions and data matrices. The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community.\n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation.\n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental. The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016).  \n2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted. This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions. The behaviors of linear networks and practical (deep and nonlinear) networks are very different. Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community.\n\nMinors:\nThere are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Points of Neural Networks: Analytical Forms and Landscape Properties","abstract":"Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.","pdf":"/pdf/7f726b3b3c58b75c823b405574a777cfedecca50.pdf","TL;DR":"We provide full analytical forms for the critical points of the square loss functions for various neural networks, and exploit the analytical forms to characterize the landscape properties for the loss functions of these neural networks.","paperhash":"anonymous|critical_points_of_neural_networks_analytical_forms_and_landscape_properties","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SysEexbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper549/Authors"],"keywords":["neural networks","critical points","analytical form","landscape"]}},{"tddate":null,"ddate":null,"tmdate":1512222683231,"tcdate":1511724032087,"number":2,"cdate":1511724032087,"id":"ryOWEcdlM","invitation":"ICLR.cc/2018/Conference/-/Paper549/Official_Review","forum":"SysEexbRb","replyto":"SysEexbRb","signatures":["ICLR.cc/2018/Conference/Paper549/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper studies the critical points of shallow and deep linear networks. The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima. While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.","rating":"7: Good paper, accept","review":"This paper studies the critical points of shallow and deep linear networks. The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima. Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices. I have not checked the proofs in detail but the general strategy seems sound. While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR. The authors also study the analytic form of critical points of a single-hidden layer ReLU network. However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear.\n\n\nDetailed comments:\n\n- I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear. This paper is mostly about linear networks. While a result has been stated for single-hidden ReLU networks. In my view this particular result is an immediate corollary of the result for linear networks. As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear. I would suggest rewording title/abstract/intro\n\n- Theorem 1 is neat, well done!\n\n- Page 4 p_i’s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly). \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc?\n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful. What insights do you gain by knowing these theorems etc. Are less sufficient conditions that is more intuitive or useful. (an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one).\n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications? Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound?\n\n- Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful. e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Points of Neural Networks: Analytical Forms and Landscape Properties","abstract":"Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.","pdf":"/pdf/7f726b3b3c58b75c823b405574a777cfedecca50.pdf","TL;DR":"We provide full analytical forms for the critical points of the square loss functions for various neural networks, and exploit the analytical forms to characterize the landscape properties for the loss functions of these neural networks.","paperhash":"anonymous|critical_points_of_neural_networks_analytical_forms_and_landscape_properties","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SysEexbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper549/Authors"],"keywords":["neural networks","critical points","analytical form","landscape"]}},{"tddate":null,"ddate":null,"tmdate":1512222683268,"tcdate":1511150133464,"number":1,"cdate":1511150133464,"id":"S1aEzCJxG","invitation":"ICLR.cc/2018/Conference/-/Paper549/Official_Review","forum":"SysEexbRb","replyto":"SysEexbRb","signatures":["ICLR.cc/2018/Conference/Paper549/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks.","rating":"7: Good paper, accept","review":"Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks. The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points.\n\nThe paper is well organized and well written. Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points. This technique generalizes existing work under full relaxation of assumptions. In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices; In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices. Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work.\n\nThe results obtained from the analytical forms of the critical points are interesting, but one problem is that how to obtain the proper solution of equation (3)? In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12). It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Points of Neural Networks: Analytical Forms and Landscape Properties","abstract":"Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.","pdf":"/pdf/7f726b3b3c58b75c823b405574a777cfedecca50.pdf","TL;DR":"We provide full analytical forms for the critical points of the square loss functions for various neural networks, and exploit the analytical forms to characterize the landscape properties for the loss functions of these neural networks.","paperhash":"anonymous|critical_points_of_neural_networks_analytical_forms_and_landscape_properties","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SysEexbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper549/Authors"],"keywords":["neural networks","critical points","analytical form","landscape"]}},{"tddate":null,"ddate":null,"tmdate":1509739241839,"tcdate":1509126195401,"number":549,"cdate":1509739239175,"id":"SysEexbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SysEexbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Critical Points of Neural Networks: Analytical Forms and Landscape Properties","abstract":"Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.","pdf":"/pdf/7f726b3b3c58b75c823b405574a777cfedecca50.pdf","TL;DR":"We provide full analytical forms for the critical points of the square loss functions for various neural networks, and exploit the analytical forms to characterize the landscape properties for the loss functions of these neural networks.","paperhash":"anonymous|critical_points_of_neural_networks_analytical_forms_and_landscape_properties","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SysEexbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper549/Authors"],"keywords":["neural networks","critical points","analytical form","landscape"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}