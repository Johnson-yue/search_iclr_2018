{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222676848,"tcdate":1511810827232,"number":2,"cdate":1511810827232,"id":"Hy7GD19gM","invitation":"ICLR.cc/2018/Conference/-/Paper503/Official_Review","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["ICLR.cc/2018/Conference/Paper503/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper is utilizing reinforcement learning to search new activation function. The search space is combination of a set of unary and binary functions. The search result is a new activation function named Swish function. The authors also run a number of ImageNet experiments, and one NTM experiment.\n\nComments:\n\n1. The search function set and method is not novel. \n2. There is no theoretical depth in the searched activation about why it is better.\n3. For leaky ReLU, use larger alpha will lead better result, eg, alpha = 0.3 or 0.5. I suggest to add experiment to leak ReLU with larger alpha. This result has been shown in previous work.\n\nOverall, I think this paper is not meeting ICLR novelty standard. I recommend to submit this paper to ICLR workshop track. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1512222676889,"tcdate":1511500825553,"number":1,"cdate":1511500825553,"id":"Sy-QnQHef","invitation":"ICLR.cc/2018/Conference/-/Paper503/Official_Review","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["ICLR.cc/2018/Conference/Paper503/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Another approach for arriving at proven concepts on activation functions","rating":"4: Ok but not good enough - rejection","review":"Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators. The best one found is termed Swish unit; x * sigmoid(b*x). \n\nThe properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU, PLReLU etc. As pointed out by the authors themselves for b=1 Swish is equivalent to SiL proposed in Elfwing et. al. (2017).\n\nIn terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Again, the authors do state that \"our results may not be directly comparable to the results in the corresponding works due to differences in our training steps.\"   \n\nBased on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. More explanation is required on why is it important and how does it help optimization. Distribution of learned b in Swish for different layers of a network can interesting to observe.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1510862598227,"tcdate":1510862598227,"number":5,"cdate":1510862598227,"id":"HkC-JdjkG","invitation":"ICLR.cc/2018/Conference/-/Paper503/Public_Comment","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Insights from Learnable Swish parameter(β)","comment":"Figure 7 shows an interesting feature that the β=1 is the most prevalent single β value after training.  Since Swish smoothly varies with β, one can only assume that the reason for this inconsistency was that β was initialized to 1 and that during training this parameter was not adjusted in many cases.  The text of the paper should clearly state the initialization value of β.\n\nThe more interesting aspect of this distribution is that over 2x more β values were learned to be better in the range of (0.0 to 0.9) than at the (assumed) starting value of β=1.  β’s in this range suggests that larger negative values must have some advantage.  \n\nIt would be very interesting to see understand if distribution of β values changes in the different layers of the neural network. Are the β in the range (0.0 to 0.9) more important at higher levels or lower levels.  It would also be instructive to see the effects of starting with β at another initial starting value.\n\nSwish approaches x/2 as β approaches inf, why is this better than approaching x in the manner that PReLU does?\n\nWhile the paper asserts the non-monotonic feature of Swish as an important aspect of Swish, but there is nothing that explains why this could be an advantage. In fact for Figure 6 show most negative preactivations are between -6 and 0 and given that Figure 7 shows most β between 0 and 1 most negative values will not be effected by non-monotonic behavior. Might the real lesson of the paper be that a smooth activation function with a smooth and continuous derivative function with a \"learnable\" small domain of negative values is more important for learning and generalization than non-montonicity?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1510860387876,"tcdate":1510860035065,"number":4,"cdate":1510860035065,"id":"S1jZrPjyG","invitation":"ICLR.cc/2018/Conference/-/Paper503/Public_Comment","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Figure 8 should show PReLU given data in Table 6","comment":"Figure 8 plot should show PReLU not ReLU since given data in Table 6, PReLU is better than ReLU in every case.\n\nin addition, in many of the other results in the paper LReLU is slightly better than  PReLU.  The two differences are that LReLU has α=0.01 and PReLU at α=.25 and that α in PReLU is learnable. Looking closely at Swish and PReLU plots, a more comparable starting initialization for PReLU would be α=.10 and it would be somewhat closer to the value the you use for LReLU.\n\nWe suggest rerunning PReLU with α=.10 and putting this result in Figure 8 and Table 6.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1510858670123,"tcdate":1510858670123,"number":3,"cdate":1510858670123,"id":"S1UnJvoyz","invitation":"ICLR.cc/2018/Conference/-/Paper503/Public_Comment","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Figure 7 would be more helpful if more typical beta values were shown","comment":"Given the distribution of actual learned β values for Swish the were presented in Figure 7, it would be more instructive to show β=0, β=0.3, β=0.5, β=1.0 in Figures 4&5. While β=10.0 is interesting to look at in the 1st derivative plot, it doesn’t seem to have been learned as useful value for β."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1510856979383,"tcdate":1510856979383,"number":2,"cdate":1510856979383,"id":"B1sGYLokG","invitation":"ICLR.cc/2018/Conference/-/Paper503/Public_Comment","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related work","comment":"You mention this in the body, but it would be helpful in the related work if you pointed out that (Hendrycks & Gimpel, 2016) considered this activation function but found a slightly different version to be better, and that Elfwing et. al already proposed Swish-1 under a different name. \n\nI see you went from sigmoid(x) -> sigmoid(beta * x) to avoid outright duplication, but empirically it looks like Swish-1 is equal or better than Swish? \n\nTable 3 is a little misleading - the magnitude of the differences is what we really care about, and those magnitudes are quite small.\n\nFigure 8 is a little misleading - ReLU's are far and away the worst on that particular dataset+model, I imagine the plot for existing work like PReLU, which gives basically the same performance, would look very different. \n\nIn the original version, you bolded the non-ReLU activations which provide basically the same perf, but you don't in the new version - why not? PReLU is often the same as Swish, but without the bolding it's a lot harder to read.\n\nThe differences in perf are small enough to make me think this is just hyperparameter noise. For instance, you try 2 learning rates for the NMT results, why only 2? What 2 did you choose? Why did you choose them? If you had introduced PReLU, would it's numbers be higher? Concrete questions aside, I have a very hard time trusting this paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1509810747118,"tcdate":1509810747118,"number":1,"cdate":1509810747118,"id":"SkQHfvoA-","invitation":"ICLR.cc/2018/Conference/-/Paper503/Public_Comment","forum":"SkBYYyZRZ","replyto":"SkBYYyZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"non-monotonic vs. small negative negative for negative pre-activations","comment":"You state: \"In Figure 6, a large percentage of preactivations fall inside the domain of the bump (−5 ≤ x ≤ 0), which indicates that the non-monotonic bump is an important aspect of Swish.\" \n\nIt seems that non-monotonic behavior is an artifact of your function that could have negative consequences by making a \"bumpier\" loss surface for optimizers. What is the value of Swish approaching 0 as x heads to -inf? Why wouldn't small negative values be sufficient for all negative pre-actiations (x ≤ -5)?  \n\nWouldn't something like CELU with small alpha in the long run be better?  CELU paper:\nhttps://arxiv.org/pdf/1704.07483.pdf"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]}},{"tddate":null,"ddate":null,"tmdate":1509739267328,"tcdate":1509124477536,"number":503,"cdate":1509739264666,"id":"SkBYYyZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkBYYyZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Searching for Activation Functions","abstract":"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. ","pdf":"/pdf/1c6e9cf33ec0b8f38d2638509ea5ee4c2099c24d.pdf","TL;DR":"We use search techniques to discover novel activation functions, and our best discovered activation function, f(x) = x * sigmoid(beta * x), outperforms ReLU on a number of challenging tasks like ImageNet.","paperhash":"anonymous|searching_for_activation_functions","_bibtex":"@article{\n  anonymous2018searching,\n  title={Searching for Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkBYYyZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper503/Authors"],"keywords":["meta learning","activation functions"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}