{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222736138,"tcdate":1511990786091,"number":3,"cdate":1511990786091,"id":"HJ9W8iheM","invitation":"ICLR.cc/2018/Conference/-/Paper725/Official_Review","forum":"S1CChZ-CZ","replyto":"S1CChZ-CZ","signatures":["ICLR.cc/2018/Conference/Paper725/AnonReviewer3"],"readers":["everyone"],"content":{"title":"RL formulation for question rewrite, but with weak justification","rating":"4: Ok but not good enough - rejection","review":"This paper formulates the Jeopardy QA as a query reformulation task that leverages a search engine.  In particular, a user will try a sequence of alternative queries based on the original question in order to find the answer.  The RL formulation essentially tries to mimic this process.  Although this is an interesting formulation, as promoted by some recent work, this paper does not provide compelling reasons why it's a good formulation.  The lack of serious comparisons to baseline methods makes it hard to judge the value of this work.\n\nDetailed comments/questions:\n\t1. I am actually quite confused on why it's a good RL setting. For a human user, having a series of queries to search for the right answer is a natural process, but it's not natural for a computer program.  For instance, each query can be viewed as different formulation of the same question and can be issued concurrently. Although formulated as an RL problem,  it is not clear to me whether the search result after each episode has been used as the immediate environment feedback. As a result, the dependency between actions seems rather weak.\n\t2. I also feel that the comparisons to other baselines (not just the variation of the proposed system) are not entirely fair. For instance, the baseline BiDAF model has only one shot, namely using the original question as query.  In this case, AQA should be allowed to use the same budget -- only one query.  Another more realistic baseline is to follow the existing work on query formulation in the IR community.  For example, 20 shorter queries generated by methods like [1] can be used to compare the queries created by AQA.\n\n[1] Kumaran & Carvalho. \"Reducing Long Queries Using Query Quality Predictors\". SIGIR-09\n\t\nPros:\n\t1. An interesting RL formulation for query reformulation\n\nCons:\n\t1. The use of RL is not properly justified\n\t2. The empirical result is not convincing that the proposed method is indeed advantageous ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","abstract":"We frame Question Answering as a Reinforcement Learning task, an approach that\nwe call Active Question Answering. We propose an agent that sits between\nthe user and a black box question-answering system and which learns to\nreformulate questions to elicit the best possible answers. The agent probes the\nsystem with, potentially many, natural language reformulations of an initial\nquestion and aggregates the returned evidence to yield the best answer.\nThe reformulation system is trained end-to-end to maximize answer quality using\npolicy gradient.\nWe evaluate on SearchQA, a dataset of complex questions\nextracted from Jeopardy!.\nOur agent improves F1 by 11.4% over a state-of-the-art base model that\nuses the original question/answer pairs. Based on a qualitative analysis of\nthe language that the agent has learned while interacting with the\nquestion answering system, we propose that the agent has discovered basic\ninformation retrieval techniques such as term re-weighting and stemming.","pdf":"/pdf/07877c2979d41113689c419199799e11cc6239cd.pdf","TL;DR":"We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers","paperhash":"anonymous|ask_the_right_questions_active_question_reformulation_with_reinforcement_learning","_bibtex":"@article{\n  anonymous2018ask,\n  title={Ask the Right Questions: Active Question Reformulation with Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1CChZ-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper725/Authors"],"keywords":["machine translation","paraphrasing","question answering","reinforcement learning","agents"]}},{"tddate":null,"ddate":null,"tmdate":1512222736191,"tcdate":1511797615848,"number":2,"cdate":1511797615848,"id":"Hydu7nFeG","invitation":"ICLR.cc/2018/Conference/-/Paper725/Official_Review","forum":"S1CChZ-CZ","replyto":"S1CChZ-CZ","signatures":["ICLR.cc/2018/Conference/Paper725/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written paper that clearly describes a RL-based active approach for question reformulation and answer selection with interesting experimental results. ","rating":"8: Top 50% of accepted papers, clear accept","review":"This article clearly describes how they designed and actively trained 2 models for question reformulation and answer selection during question answering episodes. The reformulation component is trained using a policy gradient over a sequence-to-sequence model (original vs. reformulated questions). The model is first pre-trained using a bidirectional LSTM on multilingual pairs of sentences. A small monolingual bitext corpus is the uses to improve the quality of the results. A CNN binary classifier performs answer selection. \n\nThe paper is well written and the approach is well described. I was first skeptical by the use of this technique but as the authors mention in their paper, it seems that the sequence-to-sequence translation model generate sequence of words that enables the black box environment to find meaningful answers, even though the questions are not semantically correct. Experimental clearly indicates that training both selection and reformulation components with the proposed active scheme clearly improves the performance of the Q&A system. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","abstract":"We frame Question Answering as a Reinforcement Learning task, an approach that\nwe call Active Question Answering. We propose an agent that sits between\nthe user and a black box question-answering system and which learns to\nreformulate questions to elicit the best possible answers. The agent probes the\nsystem with, potentially many, natural language reformulations of an initial\nquestion and aggregates the returned evidence to yield the best answer.\nThe reformulation system is trained end-to-end to maximize answer quality using\npolicy gradient.\nWe evaluate on SearchQA, a dataset of complex questions\nextracted from Jeopardy!.\nOur agent improves F1 by 11.4% over a state-of-the-art base model that\nuses the original question/answer pairs. Based on a qualitative analysis of\nthe language that the agent has learned while interacting with the\nquestion answering system, we propose that the agent has discovered basic\ninformation retrieval techniques such as term re-weighting and stemming.","pdf":"/pdf/07877c2979d41113689c419199799e11cc6239cd.pdf","TL;DR":"We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers","paperhash":"anonymous|ask_the_right_questions_active_question_reformulation_with_reinforcement_learning","_bibtex":"@article{\n  anonymous2018ask,\n  title={Ask the Right Questions: Active Question Reformulation with Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1CChZ-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper725/Authors"],"keywords":["machine translation","paraphrasing","question answering","reinforcement learning","agents"]}},{"tddate":null,"ddate":null,"tmdate":1512222736235,"tcdate":1511635846344,"number":1,"cdate":1511635846344,"id":"r10KoNDgf","invitation":"ICLR.cc/2018/Conference/-/Paper725/Official_Review","forum":"S1CChZ-CZ","replyto":"S1CChZ-CZ","signatures":["ICLR.cc/2018/Conference/Paper725/AnonReviewer2"],"readers":["everyone"],"content":{"title":"good, but could be better with more details and experiments","rating":"6: Marginally above acceptance threshold","review":"This paper proposes active question answering via a reinforcement learning approach that can learn to rephrase the original questions in a way that can provide the best possible answers. Evaluation on the SearchQA dataset shows significant improvement over the state-of-the-art model that uses the original questions. \n\nIn general, the paper is well-written (although there are a lot of typos and grammatical errors that need to be corrected), and the main ideas are clear. It would have been useful to provide some more details and carry out additional experiments to strengthen the merit of the proposed model. \n\nEspecially, in Section 4.2, more details about the quality of paraphrasing after training with the multilingual, monolingual, and refined models would be helpful. Which evaluation metrics were used to evaluate the quality? Also, more monolingual experiments could have been conducted with state-of-the-art neural paraphrasing models on WikiQA and Quora datasets (e.g. see https://arxiv.org/pdf/1610.03098.pdf and https://arxiv.org/pdf/1709.05074.pdf). \n\nMore details with examples should be provided about the variants of AQA along with the oracle model. Especially, step-by-step examples (for all alternative models) from input (original question) to question reformulations to output (answer/candidate answers) would be useful to understand how each module/variation is having an impact towards the best possible answer/ground truth.\n\nAlthough experiments on SearchQA demonstrate good results, I think it would be also interesting to see the results on additional datasets e.g. MS MARCO (Nguyen et al., 2016), which is very similar to the SearchQA dataset, in order to confirm the generalizability of the proposed approach. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","abstract":"We frame Question Answering as a Reinforcement Learning task, an approach that\nwe call Active Question Answering. We propose an agent that sits between\nthe user and a black box question-answering system and which learns to\nreformulate questions to elicit the best possible answers. The agent probes the\nsystem with, potentially many, natural language reformulations of an initial\nquestion and aggregates the returned evidence to yield the best answer.\nThe reformulation system is trained end-to-end to maximize answer quality using\npolicy gradient.\nWe evaluate on SearchQA, a dataset of complex questions\nextracted from Jeopardy!.\nOur agent improves F1 by 11.4% over a state-of-the-art base model that\nuses the original question/answer pairs. Based on a qualitative analysis of\nthe language that the agent has learned while interacting with the\nquestion answering system, we propose that the agent has discovered basic\ninformation retrieval techniques such as term re-weighting and stemming.","pdf":"/pdf/07877c2979d41113689c419199799e11cc6239cd.pdf","TL;DR":"We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers","paperhash":"anonymous|ask_the_right_questions_active_question_reformulation_with_reinforcement_learning","_bibtex":"@article{\n  anonymous2018ask,\n  title={Ask the Right Questions: Active Question Reformulation with Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1CChZ-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper725/Authors"],"keywords":["machine translation","paraphrasing","question answering","reinforcement learning","agents"]}},{"tddate":null,"ddate":null,"tmdate":1509739138905,"tcdate":1509133526323,"number":725,"cdate":1509739136245,"id":"S1CChZ-CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1CChZ-CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","abstract":"We frame Question Answering as a Reinforcement Learning task, an approach that\nwe call Active Question Answering. We propose an agent that sits between\nthe user and a black box question-answering system and which learns to\nreformulate questions to elicit the best possible answers. The agent probes the\nsystem with, potentially many, natural language reformulations of an initial\nquestion and aggregates the returned evidence to yield the best answer.\nThe reformulation system is trained end-to-end to maximize answer quality using\npolicy gradient.\nWe evaluate on SearchQA, a dataset of complex questions\nextracted from Jeopardy!.\nOur agent improves F1 by 11.4% over a state-of-the-art base model that\nuses the original question/answer pairs. Based on a qualitative analysis of\nthe language that the agent has learned while interacting with the\nquestion answering system, we propose that the agent has discovered basic\ninformation retrieval techniques such as term re-weighting and stemming.","pdf":"/pdf/07877c2979d41113689c419199799e11cc6239cd.pdf","TL;DR":"We propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers","paperhash":"anonymous|ask_the_right_questions_active_question_reformulation_with_reinforcement_learning","_bibtex":"@article{\n  anonymous2018ask,\n  title={Ask the Right Questions: Active Question Reformulation with Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1CChZ-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper725/Authors"],"keywords":["machine translation","paraphrasing","question answering","reinforcement learning","agents"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}