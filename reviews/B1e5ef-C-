{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222762799,"tcdate":1511880934735,"number":3,"cdate":1511880934735,"id":"H1kgYgogM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. ","rating":"6: Marginally above acceptance threshold","review":"My review reflects more from the compressive sensing perspective, instead that of deep learners.\n\nIn general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.\n\nThe paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review.\nFinally, extensive experiments are conducted and they are in accordance with the theory.\n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5.\n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery.\n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC.\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.\n\nSeveral minor comments:\n\n1. Please avoid the use of “information theory”, especially “classical information theory”, in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there’s no need to mention information theory here.\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen.\n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs.\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?\n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017].\n\n6. Page 2, first paragraph of related work, the sentence “Our method also closely related to ...” is incomplete.\n\n7. Page 2, second paragraph of related work, “Pagliardini also introduceD a linear ...”\n\n8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous.\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, “The Convex Geometry of Linear Inverse Problems”, Foundations of Computational Mathematics, 2012.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful in downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of BonG representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to establish. Our experimental results support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of pre- trained word embeddings (e.g. GloVe, word2vec): they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/aeef1bcfa96888c6faa06b2f9bda0fb1ebc27465.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["LSTM","unsupervised learning","word embeddings","compressed sensing","document representation","sparse recovery","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222762845,"tcdate":1511784653592,"number":2,"cdate":1511784653592,"id":"SyURgFFlG","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper","rating":"7: Good paper, accept","review":"The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized.\n\n-In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful in downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of BonG representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to establish. Our experimental results support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of pre- trained word embeddings (e.g. GloVe, word2vec): they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/aeef1bcfa96888c6faa06b2f9bda0fb1ebc27465.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["LSTM","unsupervised learning","word embeddings","compressed sensing","document representation","sparse recovery","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222762887,"tcdate":1511715690041,"number":1,"cdate":1511715690041,"id":"SkzuQ_dlG","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper applies techniques from compressed sensing to analyze the classification performance of LSTM word embeddings","rating":"7: Good paper, accept","review":"The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition.  Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.\n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful in downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of BonG representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to establish. Our experimental results support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of pre- trained word embeddings (e.g. GloVe, word2vec): they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/aeef1bcfa96888c6faa06b2f9bda0fb1ebc27465.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["LSTM","unsupervised learning","word embeddings","compressed sensing","document representation","sparse recovery","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739111437,"tcdate":1509134472178,"number":772,"cdate":1509739108778,"id":"B1e5ef-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful in downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of BonG representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to establish. Our experimental results support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of pre- trained word embeddings (e.g. GloVe, word2vec): they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/aeef1bcfa96888c6faa06b2f9bda0fb1ebc27465.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["LSTM","unsupervised learning","word embeddings","compressed sensing","document representation","sparse recovery","text classification"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}