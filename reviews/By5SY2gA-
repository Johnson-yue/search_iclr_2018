{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222634905,"tcdate":1512010016963,"number":3,"cdate":1512010016963,"id":"H1YXWe6gM","invitation":"ICLR.cc/2018/Conference/-/Paper398/Official_Review","forum":"By5SY2gA-","replyto":"By5SY2gA-","signatures":["ICLR.cc/2018/Conference/Paper398/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lack of novelty and unconvincing results","rating":"4: Ok but not good enough - rejection","review":"This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings. The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection.\n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning. The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work. The main originality seems to be captured in Algorithm 1, which computes the strength between two words. Unlike prior work, this is a real-valued instead of a binary quantity. Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques. In this light, the performance relative to the baselines is particularly important. From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods. I therefore cannot recommend this paper for publication.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Building Affect sensitive Word Distributions","abstract":"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings. Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge. Our work addresses the question: can affect lexica improve the word representations learnt from a corpus? In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach. We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus. Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection. We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","pdf":"/pdf/a41e9a7b828e731604079f953ab5e4bf1e44e202.pdf","TL;DR":"Enriching word embeddings with affect information improves their performance on sentiment prediction tasks.","paperhash":"anonymous|towards_building_affect_sensitive_word_distributions","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Building Affect sensitive Word Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5SY2gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper398/Authors"],"keywords":["Affect lexicon","word embeddings","Word2Vec","GloVe","WordNet","joint learning","sentiment analysis","word similarity","outlier detection","affect prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222634975,"tcdate":1511887676230,"number":2,"cdate":1511887676230,"id":"ByVH7MslM","invitation":"ICLR.cc/2018/Conference/-/Paper398/Official_Review","forum":"By5SY2gA-","replyto":"By5SY2gA-","signatures":["ICLR.cc/2018/Conference/Paper398/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A curious example of adding structured knowledge into embedding spaces","rating":"4: Ok but not good enough - rejection","review":"This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm. \n\nThe affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment. \n\nThe justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me:\n\n- The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives).\n\n- Affect in language seems to me to be a very contextual phenomenon. Only a tiny subset of words have intrinsic and context-free affect. Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'...\n\nThe model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements. However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful. \n\nIt is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails. \n\nTo understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines. For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Building Affect sensitive Word Distributions","abstract":"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings. Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge. Our work addresses the question: can affect lexica improve the word representations learnt from a corpus? In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach. We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus. Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection. We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","pdf":"/pdf/a41e9a7b828e731604079f953ab5e4bf1e44e202.pdf","TL;DR":"Enriching word embeddings with affect information improves their performance on sentiment prediction tasks.","paperhash":"anonymous|towards_building_affect_sensitive_word_distributions","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Building Affect sensitive Word Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5SY2gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper398/Authors"],"keywords":["Affect lexicon","word embeddings","Word2Vec","GloVe","WordNet","joint learning","sentiment analysis","word similarity","outlier detection","affect prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222635014,"tcdate":1511812708920,"number":1,"cdate":1511812708920,"id":"HyaDR19xG","invitation":"ICLR.cc/2018/Conference/-/Paper398/Official_Review","forum":"By5SY2gA-","replyto":"By5SY2gA-","signatures":["ICLR.cc/2018/Conference/Paper398/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper with promising results","rating":"6: Marginally above acceptance threshold","review":"This paper proposed to use affect lexica to improve word embeddings. They extended the training objective functions of Word2vec and Glove with the affect information. The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis. Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove. In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups.\n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new. Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016.\n-\tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best. Do the authors have any intuition why?\n-\tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me.  I would like to see to the top k nearest neighbors of each of those words.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Building Affect sensitive Word Distributions","abstract":"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings. Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge. Our work addresses the question: can affect lexica improve the word representations learnt from a corpus? In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach. We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus. Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection. We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","pdf":"/pdf/a41e9a7b828e731604079f953ab5e4bf1e44e202.pdf","TL;DR":"Enriching word embeddings with affect information improves their performance on sentiment prediction tasks.","paperhash":"anonymous|towards_building_affect_sensitive_word_distributions","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Building Affect sensitive Word Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5SY2gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper398/Authors"],"keywords":["Affect lexicon","word embeddings","Word2Vec","GloVe","WordNet","joint learning","sentiment analysis","word similarity","outlier detection","affect prediction"]}},{"tddate":null,"ddate":null,"tmdate":1509739324553,"tcdate":1509112129715,"number":398,"cdate":1509739321872,"id":"By5SY2gA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By5SY2gA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Building Affect sensitive Word Distributions","abstract":"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings. Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge. Our work addresses the question: can affect lexica improve the word representations learnt from a corpus? In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach. We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus. Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection. We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","pdf":"/pdf/a41e9a7b828e731604079f953ab5e4bf1e44e202.pdf","TL;DR":"Enriching word embeddings with affect information improves their performance on sentiment prediction tasks.","paperhash":"anonymous|towards_building_affect_sensitive_word_distributions","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Building Affect sensitive Word Distributions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5SY2gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper398/Authors"],"keywords":["Affect lexicon","word embeddings","Word2Vec","GloVe","WordNet","joint learning","sentiment analysis","word similarity","outlier detection","affect prediction"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}