{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222555213,"tcdate":1511781456418,"number":2,"cdate":1511781456418,"id":"H1O8NOKeM","invitation":"ICLR.cc/2018/Conference/-/Paper1096/Official_Review","forum":"ry831QWAb","replyto":"ry831QWAb","signatures":["ICLR.cc/2018/Conference/Paper1096/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A very interesting and important paper","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper illustrates the benefits of using normalized gradients when training deep models.\nBeyond exploring the \"vanilla\" normalized gradient algorithm they also consider adaptive versions, i.e., methods that employ per block (adaptive) learning rates using ideas from AdaGrad and Adam.\nFinally, the authors provide a theoretical analysis of NG with adaptive step-size, showing convergence guarantees in the stochastic convex optimization setting.\n\nI find this paper both very interesting and important. \nThe normalized gradient method was previously shown to overcome some non-convex phenomena which are hurdles to SGD, yet there was still the gap of  combining NG with methods which automatically tune the learning rate.\n\nThe current paper addresses this gap by a very simple (yet clever) combination of NG with AdaGrad and Adam, and the authors do a great job by illustrating the benefits of their scheme by testing it over a very wide span of deep learning \nmodels. In light of their experiments it seems like AdamNG and NG should be adopted as the new state-of-the-art methods in deep-learning applications.\n\nAdditional comments:\n-In the experiments the authors use the same parameters as is used by Adam/AdaGrad, etc..\nDid the authors also try to fine tune the parameters of their NG versions? If so what is the benefit that they get by doing so?\n-It will be useful if the authors can provide some intuition about why is the learning rate  chosen per block for NG?\nDid the authors also try to choose a learning rate per weight vector rather than per block? If so, what is the behaviour that they see.\n-I find the theoretical analysis a bit incomplete. The authors should spell out the choice of the learning rate in Thm. 1 and compare to AdaGrad.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK","abstract":"In this paper, we propose a generic and simple strategy for utilizing stochastic gradient information in optimization. The technique essentially contains two con- secutive steps in each iteration: 1) computing and normalizing each block (layer) of the mini-batch stochastic gradient; 2) selecting appropriate step size to update the decision variable (parameter) towards the negative of the block-normalized gradient. We conduct extensive empirical studies on various non-convex neu- ral network optimization problems, including multi layer perceptron, convolution neural networks and recurrent neural networks. The results indicate the block- normalized gradient can help accelerate the training of neural networks. In partic- ular, we observe that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum, have better performance in the deep convolution neural networks, while those with adaptive step sizes, such as Adam, perform better in recurrent neural networks. Besides, we also observe this line of methods can lead to solutions with better generalization properties, which is confirmed by the performance improvement over strong baselines.","pdf":"/pdf/0f2afdd3f1854921fca69289a77b0a0b34922fe1.pdf","paperhash":"anonymous|blocknormalized_gradient_method_an_empirical_study_for_training_deep_neural_network","_bibtex":"@article{\n  anonymous2018block-normalized,\n  title={BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry831QWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1096/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222555260,"tcdate":1511756325040,"number":1,"cdate":1511756325040,"id":"BkTXGMKlf","invitation":"ICLR.cc/2018/Conference/-/Paper1096/Official_Review","forum":"ry831QWAb","replyto":"ry831QWAb","signatures":["ICLR.cc/2018/Conference/Paper1096/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clearly written paper, but experiments are not compelling and theoretical result is suboptimal","rating":"3: Clear rejection","review":"This paper proposes a family of first-order stochastic optimization schemes based on (1)  normalizing (batches of) stochastic gradient descents and (2) choosing from a step size updating scheme. The authors argue that iterative first-order optimization algorithms can be interpreted as a choice of an update direction and a step size, so they suggest that one should always normalize the gradient when computing the direction and then choose a step size using the normalized gradient. \n\nThe presentation in the paper is clear, and the exposition is easy to follow. The authors also do a good job of presenting related work and putting their ideas in the proper context. The authors also test their proposed method on many datasets, which is appreciated.\n\nHowever, I didn't find the main idea of the paper to be particularly compelling. The proposed technique is reasonable on its own, but the empirical results do not come with any measure of statistical significance. The authors also do not analyze the sensitivity of the different optimization algorithms to hyperparameter choice, opting to only use the default. Moreover, some algorithms were used as benchmarks on some datasets but not others. For a primarily empirical paper, every state-of-the-art algorithm should be used as a point of comparison on every dataset considered. These factors altogether render the experiments uninformative in comparing the proposed suite of algorithms to state-of-the-art methods. The theoretical result in the convex setting is also not data-dependent, despite the fact that it is the normalized gradient version of AdaGrad, which does come with a data-dependent convergence guarantee.\n\nGiven the suite of optimization algorithms in the literature and in use today, any new optimization framework should either demonstrate improved (or at least matching) guarantees in some common (e.g. convex) settings or definitively outperform state-of-the-art methods on problems that are of widespread interest. Unfortunately, this paper does neither. \n\nBecause of these points, I do not feel the quality, originality, and significance of the work to be high enough to merit acceptance. \n\nSome specific comments:\np. 2: \"adaptive feature-dependent step size has attracted lots of attention\". When you apply feature-dependent step sizes, you are effectively changing the direction of the gradient, so your meta learning formulation, as posed, doesn't make as much sense.\np. 2: \"we hope the resulting methods can benefit from both techniques\". What reason do you have to hope for this? Why should they be complimentary? Existing optimization techniques are based on careful design and coupling of gradients or surrogate gradients, with specific learning rate schedules. Arbitrarily mixing the two doesn't seem to be theoretically well-motivated.\np. 2: \"numerical results shows that normalized gradient always helps to improve the performance of the original methods when the network structure is deep\". It would be great to provide some intuition for this.  \np. 2: \"we also provide a convergence proof under this framework when the problem is convex and the stepsize is adaptive\". The result that you prove guarantees a \\theta(\\sqrt{T}) convergence rate. On the other hand, the AdaGrad algorithm guarantees a data-dependent bound that is O(\\sqrt{T}) but can also be much smaller. This suggests that there is no theoretical motivation to use NGD with an adaptive step size over AdaGrad.\np. 2-3: \"NGD can find a \\eps-optimal solution....when the objective function is quasi-convex. ....extended NGD for upper semi-continuous quasi-convex objective functions...\". This seems like a typo. How are results that go from quasi-convex to upper semi-continuous quasi-convex an extension?\np. 3: There should be a reference for RMSProp.\np. 3: \"where each block of parameters x^i can be viewed as parameters associated to the ith layer in the network\". Why is layer parametrization (and later on normalization) a good way idea? There should be either a reference or an explanation.\np. 4: \"x=(x_1, x_2, \\ldots, x_B)\". Should these subscripts be superscripts?\np. 4: \"For all the algorithms, we use their default settings.\" This seems insufficient for an empirical paper, since most problems often involve some amount of hyperparameter tuning. How sensitive is each method to the choice of hyperparameters? What about the impact of initialization?\np. 4-8: None of the experimental results have error bars or any measure of statistical significance.\np. 5: \"NG... is a variant of the NG_{UNIT} method\". This method is never motivated.\np. 5-6: Why are SGD and Adam used for MNIST but not on CIFAR? \np. 5: \"we chose the best heyper-paerameter from the 56 layer residual network.\" Apart from the typos, are these parameters chosen from the training set or the test set? \np. 6: Why isn't Adam tested on ImageNet?\n\n  \n\n\n\n\n\n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK","abstract":"In this paper, we propose a generic and simple strategy for utilizing stochastic gradient information in optimization. The technique essentially contains two con- secutive steps in each iteration: 1) computing and normalizing each block (layer) of the mini-batch stochastic gradient; 2) selecting appropriate step size to update the decision variable (parameter) towards the negative of the block-normalized gradient. We conduct extensive empirical studies on various non-convex neu- ral network optimization problems, including multi layer perceptron, convolution neural networks and recurrent neural networks. The results indicate the block- normalized gradient can help accelerate the training of neural networks. In partic- ular, we observe that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum, have better performance in the deep convolution neural networks, while those with adaptive step sizes, such as Adam, perform better in recurrent neural networks. Besides, we also observe this line of methods can lead to solutions with better generalization properties, which is confirmed by the performance improvement over strong baselines.","pdf":"/pdf/0f2afdd3f1854921fca69289a77b0a0b34922fe1.pdf","paperhash":"anonymous|blocknormalized_gradient_method_an_empirical_study_for_training_deep_neural_network","_bibtex":"@article{\n  anonymous2018block-normalized,\n  title={BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry831QWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1096/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092380772,"tcdate":1509138468317,"number":1096,"cdate":1510092360005,"id":"ry831QWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry831QWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK","abstract":"In this paper, we propose a generic and simple strategy for utilizing stochastic gradient information in optimization. The technique essentially contains two con- secutive steps in each iteration: 1) computing and normalizing each block (layer) of the mini-batch stochastic gradient; 2) selecting appropriate step size to update the decision variable (parameter) towards the negative of the block-normalized gradient. We conduct extensive empirical studies on various non-convex neu- ral network optimization problems, including multi layer perceptron, convolution neural networks and recurrent neural networks. The results indicate the block- normalized gradient can help accelerate the training of neural networks. In partic- ular, we observe that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum, have better performance in the deep convolution neural networks, while those with adaptive step sizes, such as Adam, perform better in recurrent neural networks. Besides, we also observe this line of methods can lead to solutions with better generalization properties, which is confirmed by the performance improvement over strong baselines.","pdf":"/pdf/0f2afdd3f1854921fca69289a77b0a0b34922fe1.pdf","paperhash":"anonymous|blocknormalized_gradient_method_an_empirical_study_for_training_deep_neural_network","_bibtex":"@article{\n  anonymous2018block-normalized,\n  title={BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry831QWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1096/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}