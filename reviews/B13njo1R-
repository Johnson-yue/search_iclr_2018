{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222582760,"tcdate":1511819981446,"number":2,"cdate":1511819981446,"id":"H1BAcZ9eG","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Review","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice continual learning study","rating":"7: Good paper, accept","review":"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum.\n\nPros:\n- PLAID masters several distinct tasks in sequence, building up “skills” by learning “related” tasks of increasing difficulty.\n- Although the main focus of this paper is on continual learning of “related” tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain.\n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case.\n- Are the experiments single runs? Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour.\n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks?\n- How were the network architecture and network size chosen, especially for the multitasker? Would policies generalize to later tasks better with larger, or smaller networks?\n- Was any kind of regularization used, how does it influence task performance vs. transfer?\n- I find figure 1 (c) somewhat confusing. Is performance maintained only on the last 2 tasks, or all previously seen tasks? That’s what the figure suggests at first glance, but that’s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst two alternative baselines.","pdf":"/pdf/df473dfe04a2e78084868ce62ed37aa580716529.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222582812,"tcdate":1511648042339,"number":1,"cdate":1511648042339,"id":"ByMViwPef","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Review","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good approach, needs more details.","rating":"6: Marginally above acceptance threshold","review":"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially. The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task. The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously. \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?\n- What data do you use for the distillation? Section 4.1 states\"We use a method similar to the DAGGER algorithm\", but what is your method. If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?\n- I do not understand the purpose of \"input injection\" nor where it is used in the paper. \n\nStrengths:\n- The method is simple but novel. The results support the method's utility.\n- The testbed is nice; the tasks seem significantly different from each other. It seems that no reward shaping is used.\n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.\n\nWeaknesses:\n- Figure 2: the plots are too small.\n- Distilling may hurt performance ( Figure 2.d)\n- The method lacks details (see Questions above)\n- No comparisons with prior work are provided. The paper cites many previous approaches to this but does not compare against any of them. \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch. \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work. The method is clear but not precisely described. The results are promising. I think that this is a good approach to the problem that could be used in real-world scenarios. With some filling out, this could be a great paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst two alternative baselines.","pdf":"/pdf/df473dfe04a2e78084868ce62ed37aa580716529.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739451054,"tcdate":1509043124096,"number":162,"cdate":1509739448397,"id":"B13njo1R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B13njo1R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst two alternative baselines.","pdf":"/pdf/df473dfe04a2e78084868ce62ed37aa580716529.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}