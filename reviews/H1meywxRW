{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222611789,"tcdate":1512007506468,"number":3,"cdate":1512007506468,"id":"rJ9UPyaeG","invitation":"ICLR.cc/2018/Conference/-/Paper282/Official_Review","forum":"H1meywxRW","replyto":"H1meywxRW","signatures":["ICLR.cc/2018/Conference/Paper282/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Significant improvement of DCN answer selection models using mixed objectives and 2 stacked levels of coattention","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR. First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions. The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions. Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance. \n\nThe paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers. It seems to me that this paper is a significant contribution to the field of question answering systems. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DCN+: Mixed Objective And Deep Residual Coattention for Question Answering","abstract":"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.","pdf":"/pdf/b7c199be8412c1e46db225723ee4e5125c699a2c.pdf","TL;DR":"We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.","paperhash":"anonymous|dcn_mixed_objective_and_deep_residual_coattention_for_question_answering","_bibtex":"@article{\n  anonymous2018dcn+:,\n  title={DCN+: Mixed Objective And Deep Residual Coattention for Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1meywxRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper282/Authors"],"keywords":["question answering","deep learning","natural language processing","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222611830,"tcdate":1511841274323,"number":2,"cdate":1511841274323,"id":"rkf-R8qlz","invitation":"ICLR.cc/2018/Conference/-/Paper282/Official_Review","forum":"H1meywxRW","replyto":"H1meywxRW","signatures":["ICLR.cc/2018/Conference/Paper282/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An improvement of DCN model","rating":"6: Marginally above acceptance threshold","review":"This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. \n\nOne question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DCN+: Mixed Objective And Deep Residual Coattention for Question Answering","abstract":"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.","pdf":"/pdf/b7c199be8412c1e46db225723ee4e5125c699a2c.pdf","TL;DR":"We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.","paperhash":"anonymous|dcn_mixed_objective_and_deep_residual_coattention_for_question_answering","_bibtex":"@article{\n  anonymous2018dcn+:,\n  title={DCN+: Mixed Objective And Deep Residual Coattention for Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1meywxRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper282/Authors"],"keywords":["question answering","deep learning","natural language processing","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222611874,"tcdate":1511653944197,"number":1,"cdate":1511653944197,"id":"HyeSGKwgf","invitation":"ICLR.cc/2018/Conference/-/Paper282/Official_Review","forum":"H1meywxRW","replyto":"H1meywxRW","signatures":["ICLR.cc/2018/Conference/Paper282/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"Summary:\nThis paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention. It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span. The resulting DCN+ model achieved significant improvement over DCN.\n\nStrengths:\nThe model and the mixed objective is well-motivated and clearly explained.\nNear state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard).\n\nOther questions and comments:\nThe ablation shows 0.7 improvement on EM with mixed objective. It is interesting that the mixed objective (which targets F1) also brings improvement on EM. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DCN+: Mixed Objective And Deep Residual Coattention for Question Answering","abstract":"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.","pdf":"/pdf/b7c199be8412c1e46db225723ee4e5125c699a2c.pdf","TL;DR":"We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.","paperhash":"anonymous|dcn_mixed_objective_and_deep_residual_coattention_for_question_answering","_bibtex":"@article{\n  anonymous2018dcn+:,\n  title={DCN+: Mixed Objective And Deep Residual Coattention for Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1meywxRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper282/Authors"],"keywords":["question answering","deep learning","natural language processing","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510340061023,"tcdate":1510340032421,"number":1,"cdate":1510340032421,"id":"rJdarOXyG","invitation":"ICLR.cc/2018/Conference/-/Paper282/Official_Comment","forum":"H1meywxRW","replyto":"H1meywxRW","signatures":["ICLR.cc/2018/Conference/Paper282/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper282/Authors"],"content":{"title":"Errata","comment":"In Equation 17 (page 5), we made a typo in that we did not include the regularization terms $$\\log \\sigma_{ce}^2 + \\log \\sigma_{rl}^2$$."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DCN+: Mixed Objective And Deep Residual Coattention for Question Answering","abstract":"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.","pdf":"/pdf/b7c199be8412c1e46db225723ee4e5125c699a2c.pdf","TL;DR":"We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.","paperhash":"anonymous|dcn_mixed_objective_and_deep_residual_coattention_for_question_answering","_bibtex":"@article{\n  anonymous2018dcn+:,\n  title={DCN+: Mixed Objective And Deep Residual Coattention for Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1meywxRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper282/Authors"],"keywords":["question answering","deep learning","natural language processing","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739387259,"tcdate":1509089003345,"number":282,"cdate":1509739384606,"id":"H1meywxRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1meywxRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DCN+: Mixed Objective And Deep Residual Coattention for Question Answering","abstract":"Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.","pdf":"/pdf/b7c199be8412c1e46db225723ee4e5125c699a2c.pdf","TL;DR":"We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.","paperhash":"anonymous|dcn_mixed_objective_and_deep_residual_coattention_for_question_answering","_bibtex":"@article{\n  anonymous2018dcn+:,\n  title={DCN+: Mixed Objective And Deep Residual Coattention for Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1meywxRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper282/Authors"],"keywords":["question answering","deep learning","natural language processing","reinforcement learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}