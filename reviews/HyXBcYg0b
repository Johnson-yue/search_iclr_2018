{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222622599,"tcdate":1511822713456,"number":3,"cdate":1511822713456,"id":"r1-tSfqeG","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer3"],"readers":["everyone"],"content":{"title":"the relation between the two proposed models is not very clear","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets. There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture. The Related work part of this paper makes a good description of both topics. \n\nI would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets). The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM. The presentation may raise some misunderstanding. A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting. \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks. These two proposed neural network models seem performing well empirically. \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as functional brain networks, social networks, gene regulatory networks, communications networks have brought the interest in generalizing neural networks to graph domains. In this paper, we are interested to design efficient neural network architectures for graphs with variable length. Several existing works such as Scarselli et al. (2009); Li et al. (2016) have focused on recurrent neural networks (RNNs) to solve this task. A recent different approach was proposed in Sukhbaatar et al. (2016), where a vanilla graph convolutional neural network (ConvNets) was introduced. We believe the latter approach to be a better paradigm to solve graph learning problems because ConvNets are more pruned to deep networks than RNNs. For this reason, we propose the most generic class of residual multi-layer graph ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani & Titov (2017). Gated edges appear to be a natural property in the context of graph learning tasks, as the system has the ability to learn which edges are important or not for the task to solve. We apply several graph neural models to two basic network science tasks; subgraph matching and semi-supervised clustering for graphs with variable length. Numerical results show the performances of the new model.","pdf":"/pdf/1406315424bea1012e73619a998f6f5442ba7d26.pdf","TL;DR":"We propose a generic class of graph ConvNets with edge gating mechanism and residual formulation.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1512222622678,"tcdate":1511778075312,"number":2,"cdate":1511778075312,"id":"Sy7QPPYxM","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Residual Gated Graph ConvNets","rating":"3: Clear rejection","review":"The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community. In particular, a tree structured LSTM is taken and modified. The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality.\n\nMy biggest concern is novelty, as the modifications are minor. In particular, the formulation can be seen in a different way. As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate). This is a minor modification. Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper.\n\nA second concern is the presentation of the paper, which can be confusing at some points. A major example is the mathematical description of the methods. When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent.\n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies. \n\nGenerally, a clear introduction of the problem is also missing. What are the inputs, what are the outputs, what kind of problems should be solved? The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?\n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality.\n\nA couple of details :\n\n- the length of a graph is not defined. The size of the set of nodes might be meant.\n\n- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing. RNNs are not restricted to NLP and I think there is no need to introduce an application at this point.\n\n- It is unclear what does the following sentence means: \"ConvNets are more pruned to deep networks than RNNs\"?\n\n- What are \"heterogeneous graph domains\"?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as functional brain networks, social networks, gene regulatory networks, communications networks have brought the interest in generalizing neural networks to graph domains. In this paper, we are interested to design efficient neural network architectures for graphs with variable length. Several existing works such as Scarselli et al. (2009); Li et al. (2016) have focused on recurrent neural networks (RNNs) to solve this task. A recent different approach was proposed in Sukhbaatar et al. (2016), where a vanilla graph convolutional neural network (ConvNets) was introduced. We believe the latter approach to be a better paradigm to solve graph learning problems because ConvNets are more pruned to deep networks than RNNs. For this reason, we propose the most generic class of residual multi-layer graph ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani & Titov (2017). Gated edges appear to be a natural property in the context of graph learning tasks, as the system has the ability to learn which edges are important or not for the task to solve. We apply several graph neural models to two basic network science tasks; subgraph matching and semi-supervised clustering for graphs with variable length. Numerical results show the performances of the new model.","pdf":"/pdf/1406315424bea1012e73619a998f6f5442ba7d26.pdf","TL;DR":"We propose a generic class of graph ConvNets with edge gating mechanism and residual formulation.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1512222622718,"tcdate":1511773057124,"number":1,"cdate":1511773057124,"id":"ryFFX8Yxf","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting approach that should be better presented ","rating":"6: Marginally above acceptance threshold","review":"Summary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs. It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees. The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms. They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering.  \n\nMain comments:\nIt would strengthen the paper to also compare all these network learning based approaches to variational ones. For instance, to a spectral clustering method for the semi supervised clustering, or\nsolving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006.\n\nThe abstract and the conclusion should be revised, they are very vague.\n- The abstract should be self contained and should not contain citations.\n- The authors should clarify which problem they are dealing with.\n- instead of the \"numerical result show the performance of the new model\", give some numerical results here, otherwise, this sentence is useless.\n- we propose ... as propose -> unclear: what do you propose?\n \n\nMinor comments:\n- You should make sentences when using references with the author names format. Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997)\n- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)\n- Don't start sentences with And, or But","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as functional brain networks, social networks, gene regulatory networks, communications networks have brought the interest in generalizing neural networks to graph domains. In this paper, we are interested to design efficient neural network architectures for graphs with variable length. Several existing works such as Scarselli et al. (2009); Li et al. (2016) have focused on recurrent neural networks (RNNs) to solve this task. A recent different approach was proposed in Sukhbaatar et al. (2016), where a vanilla graph convolutional neural network (ConvNets) was introduced. We believe the latter approach to be a better paradigm to solve graph learning problems because ConvNets are more pruned to deep networks than RNNs. For this reason, we propose the most generic class of residual multi-layer graph ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani & Titov (2017). Gated edges appear to be a natural property in the context of graph learning tasks, as the system has the ability to learn which edges are important or not for the task to solve. We apply several graph neural models to two basic network science tasks; subgraph matching and semi-supervised clustering for graphs with variable length. Numerical results show the performances of the new model.","pdf":"/pdf/1406315424bea1012e73619a998f6f5442ba7d26.pdf","TL;DR":"We propose a generic class of graph ConvNets with edge gating mechanism and residual formulation.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1509739355519,"tcdate":1509100090864,"number":340,"cdate":1509739352856,"id":"HyXBcYg0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as functional brain networks, social networks, gene regulatory networks, communications networks have brought the interest in generalizing neural networks to graph domains. In this paper, we are interested to design efficient neural network architectures for graphs with variable length. Several existing works such as Scarselli et al. (2009); Li et al. (2016) have focused on recurrent neural networks (RNNs) to solve this task. A recent different approach was proposed in Sukhbaatar et al. (2016), where a vanilla graph convolutional neural network (ConvNets) was introduced. We believe the latter approach to be a better paradigm to solve graph learning problems because ConvNets are more pruned to deep networks than RNNs. For this reason, we propose the most generic class of residual multi-layer graph ConvNets that make use of an edge gating mechanism, as proposed in Marcheggiani & Titov (2017). Gated edges appear to be a natural property in the context of graph learning tasks, as the system has the ability to learn which edges are important or not for the task to solve. We apply several graph neural models to two basic network science tasks; subgraph matching and semi-supervised clustering for graphs with variable length. Numerical results show the performances of the new model.","pdf":"/pdf/1406315424bea1012e73619a998f6f5442ba7d26.pdf","TL;DR":"We propose a generic class of graph ConvNets with edge gating mechanism and residual formulation.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","pattern matching","semi-supervised clustering"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}