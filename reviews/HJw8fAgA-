{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222654204,"tcdate":1511761152767,"number":1,"cdate":1511761152767,"id":"H1KbSmYeM","invitation":"ICLR.cc/2018/Conference/-/Paper444/Official_Review","forum":"HJw8fAgA-","replyto":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference/Paper444/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple idea that seems to work well on Ms Pacman, but paper needs more quantitative results/qualitative inspections","rating":"6: Marginally above acceptance threshold","review":"Summary:\n\nThis paper studies how to learn (hidden)-state-space models of environment dynamics, and integrate them with Imagination-Augmented Agents (I2A). The paper considers single-agent problems and tests on Ms Pacman etc.\n\nThere are several variations of the hidden-state space [ds]SSM model: using det/stochastic latent variables + using det/stochastic decoders. In the stochastic case, learning is done using variational methods. \n\n[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model. The rollouts are then fed into the agent's policy / value function.\n\nMain results seem to be:\n1. Experiments on learning the forward model, show that latent forward models work better and faster than naive AR models on several Atari games, and better than fully model-free baselines. \n2. I2A agents with latent codes work better than model-free models or I2A from pixels. Deterministic latent models seem to work better than stochastic ones.\n\nPro:\n- Relatively straightforward idea: learn the forward model on hidden states, rather than raw states.\n- Writing is clear, although a bit dense in places.\n\nCon:\n- Paper only shows training curves for MS Pacman. What about the other games from Table 1?\n- The paper lacks any visualization of the latent codes. What do they represent? Can we e.g. learn a raw-state predictor from the latent codes?\n- Are the latent codes relevant in the stochastic model? See e.g. the discussion in \"Variational Lossy Autoencoder\" (Chen et al. 2016)\n- Experiments are not complete (e.g. for AR, as noted in the paper).\n- The games used are fairly reactive (i.e. do not require significant long-term planning), and so the sequential hidden-state-space model does not have to capture long-term dependencies. It would be nice to see how this technique fares on Montezuma's revenge, for instance.\n\nOverall:\nThe paper proposes a simple idea that seems to work well on reactive 1-agent games. However, the paper could give more insights into *how* this works: e.g. a better qualitative inspection of the learned latent model, and how existing questions surrounding sequential stochastic model affect the proposed method. Also, not all baseline experiments are done, and the impact on training is only evaluated on 1 game. \n\nDetailed:\n-\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]}},{"tddate":null,"ddate":null,"tmdate":1509739300652,"tcdate":1509118542685,"number":444,"cdate":1509739297992,"id":"HJw8fAgA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]},"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}