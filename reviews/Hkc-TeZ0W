{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222703831,"tcdate":1511781580632,"number":3,"cdate":1511781580632,"id":"rkSREOYgM","invitation":"ICLR.cc/2018/Conference/-/Paper629/Official_Review","forum":"Hkc-TeZ0W","replyto":"Hkc-TeZ0W","signatures":["ICLR.cc/2018/Conference/Paper629/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Elegant method with impressive results","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes a device placement algorithm to place operations of tensorflow on devices. \n\nPros:\n\n1. It is a novel approach which trains the placement end to end.\n2. The experiments are solid to demonstrate this method works very well.\n3. The writing is easy to follow.\n4. This would be a very useful tool for the community if open sourced.\n\nCons:\n\n1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models. The latter would be more exciting. The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph? However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case.\n2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.\n3. It is not clear how the adjacency information is used.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Hierarchical Model for Device Placement","abstract":"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. The algorithm learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed algorithm is trained by a policy gradient method and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow (TF) computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves reductions in runtime of up to 60.6% per training step when applied to models such as Neural Machine Translation.","pdf":"/pdf/d526ad489860ce5d15b4e64611bcf9ee6e6e21d5.pdf","TL;DR":"We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices.","paperhash":"anonymous|a_hierarchical_model_for_device_placement","_bibtex":"@article{\n  anonymous2018a,\n  title={A Hierarchical Model for Device Placement},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkc-TeZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper629/Authors"],"keywords":["deep learning","device placement","policy gradient optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222703869,"tcdate":1511758728550,"number":2,"cdate":1511758728550,"id":"Sk-qjGYlz","invitation":"ICLR.cc/2018/Conference/-/Paper629/Official_Review","forum":"Hkc-TeZ0W","replyto":"Hkc-TeZ0W","signatures":["ICLR.cc/2018/Conference/Paper629/AnonReviewer1"],"readers":["everyone"],"content":{"title":"In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model’s runtime. However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step.","rating":"5: Marginally below acceptance threshold","review":"In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model’s runtime. However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step. In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation. Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder. \n\nOverall speaking, this work is quite interesting. However, it also has several limitations, as explained below.\n\nFirst, the computational cost of the proposed method seems very high. It may take more than one day on 320-640 GPUs for training (I did not find enough details in this paper, but the training complexity will be no less than the in [1]). This makes it very hard to reproduce the experimental results (in order to verify it), and its practical value becomes quite restrictive (very few organizations can afford such a cost).\n\nSecond, as the author mentioned, it’s hard to compare the experimental results in this paper wit those in [1] because different hardware devices and software versions were used. However, this is not a very sound excuse. I would encourage the authors to implement colocRL [1] on their own hardware and software systems, and make direct comparison. Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is. In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements.\n\n [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. https://arxiv.org/pdf/1706.04972.pdf \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Hierarchical Model for Device Placement","abstract":"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. The algorithm learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed algorithm is trained by a policy gradient method and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow (TF) computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves reductions in runtime of up to 60.6% per training step when applied to models such as Neural Machine Translation.","pdf":"/pdf/d526ad489860ce5d15b4e64611bcf9ee6e6e21d5.pdf","TL;DR":"We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices.","paperhash":"anonymous|a_hierarchical_model_for_device_placement","_bibtex":"@article{\n  anonymous2018a,\n  title={A Hierarchical Model for Device Placement},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkc-TeZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper629/Authors"],"keywords":["deep learning","device placement","policy gradient optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222703909,"tcdate":1511333135817,"number":1,"cdate":1511333135817,"id":"BJuGT9zez","invitation":"ICLR.cc/2018/Conference/-/Paper629/Official_Review","forum":"Hkc-TeZ0W","replyto":"Hkc-TeZ0W","signatures":["ICLR.cc/2018/Conference/Paper629/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes to jointly learn groups of operators to colocate and to place learned groups on devices so as to distribute operations for deep learning via reinforcement learning.","rating":"5: Marginally below acceptance threshold","review":"The paper seems clear enough and original enough. The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit. Where the paper falls short is motivating the problem setting. Traditionally, for determining optimal execution plans, one may resort to cost-based optimization (e.g., database management systems). This paper's introduction provides precisely 1 statement to suggest that may not work for deep learning. Here's the relevant phrase: \"the cost function is typically non-stationary due to the interactions between multiple devices\". Unfortunately, this statement raises more questions than it answers. Why are the cost functions non-stationary? What exactly makes them dynamic? Are we talking about a multi-tenancy setting where multiple processes execute on the same device? Unlikely, because GPUs are involved. Without a proper motivation, its difficult to appreciate the methods devised.\n\nPros:\n- Jointly optimizing forming of groups and placing these seems to have merit\n- Experiments show improvements over placement by human \"experts\"\n- Targets an important problem\n\nCons:\n- Related work seems inadequately referenced. There exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed setting. This paper should at least cite those papers and qualitatively compare against those approaches. Here's one reference (others should be easy to find): \"SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs\" by Boehm et al, IEEE Data Engineering Bulletin, 2014.\n- The methods are not well motivated. There are many approaches to devising optimal execution plans, e.g., rule-based, cost-based, learning-based. In particular, what makes cost-based optimization inapplicable? Also, please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic, optimally forming groups and placing them is learn-able.\n- The template seems off. I don't see the usual two lines under the title (\"Anonymous authors\", \"Paper under double-blind review\").\n- The title seems misleading. \".... Device Placement\" seems to suggest that one is placing devices when in fact, the operators are being placed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Hierarchical Model for Device Placement","abstract":"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. The algorithm learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed algorithm is trained by a policy gradient method and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow (TF) computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves reductions in runtime of up to 60.6% per training step when applied to models such as Neural Machine Translation.","pdf":"/pdf/d526ad489860ce5d15b4e64611bcf9ee6e6e21d5.pdf","TL;DR":"We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices.","paperhash":"anonymous|a_hierarchical_model_for_device_placement","_bibtex":"@article{\n  anonymous2018a,\n  title={A Hierarchical Model for Device Placement},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkc-TeZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper629/Authors"],"keywords":["deep learning","device placement","policy gradient optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739192497,"tcdate":1509129474351,"number":629,"cdate":1509739189830,"id":"Hkc-TeZ0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hkc-TeZ0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Hierarchical Model for Device Placement","abstract":"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. The algorithm learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed algorithm is trained by a policy gradient method and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow (TF) computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves reductions in runtime of up to 60.6% per training step when applied to models such as Neural Machine Translation.","pdf":"/pdf/d526ad489860ce5d15b4e64611bcf9ee6e6e21d5.pdf","TL;DR":"We introduce a hierarchical model for efficient, end-to-end placement of computational graphs onto hardware devices.","paperhash":"anonymous|a_hierarchical_model_for_device_placement","_bibtex":"@article{\n  anonymous2018a,\n  title={A Hierarchical Model for Device Placement},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkc-TeZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper629/Authors"],"keywords":["deep learning","device placement","policy gradient optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}