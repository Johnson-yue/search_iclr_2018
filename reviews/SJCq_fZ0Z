{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222800023,"tcdate":1511912901473,"number":2,"cdate":1511912901473,"id":"SJppHuogG","invitation":"ICLR.cc/2018/Conference/-/Paper874/Official_Review","forum":"SJCq_fZ0Z","replyto":"SJCq_fZ0Z","signatures":["ICLR.cc/2018/Conference/Paper874/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Sparse attention backtracking, an alternative to (T)BPTT","rating":"8: Top 50% of accepted papers, clear accept","review":"re. Introduction, page 2: Briefly explain here how SAB is different from regular Attention?\n\nGood paper. There's not that much discussion of the proposed SAB compared to regular Attention, perhaps that could be expanded. Also, I suggest summarizing the experimental findings in the Conclusion.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks","abstract":"A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic.  However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored.  Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights.  This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.   ","pdf":"/pdf/17aa0f1cb740cd2e0bc8da2c8f614a7cba5a6678.pdf","TL;DR":"Towards Efficient Credit Assignment in Recurrent Networks without Backpropagation Through Time","paperhash":"anonymous|sparse_attentive_backtracking_longrange_credit_assignment_in_recurrent_networks","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJCq_fZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper874/Authors"],"keywords":["recurrent neural networks","long-term dependencies","back-propagation through time","truncated back-propagation","biological inspiration","self-attention"]}},{"tddate":null,"ddate":null,"tmdate":1512222801331,"tcdate":1511851329656,"number":1,"cdate":1511851329656,"id":"H1qBHY5eM","invitation":"ICLR.cc/2018/Conference/-/Paper874/Official_Review","forum":"SJCq_fZ0Z","replyto":"SJCq_fZ0Z","signatures":["ICLR.cc/2018/Conference/Paper874/AnonReviewer3"],"readers":["everyone"],"content":{"title":"SAB Review","rating":"5: Marginally below acceptance threshold","review":"The paper proposes sparse attentive backtracking, essentially an attention mechanism that performs truncated BPTT around a subset of the selected states.\n\nThe early claims regarding biological plausibility seem stretched, at least when applying to this work. The \"waiting for life to end to learn\" and student study / test analogies were not helpful from an understanding point of view and indeed raised more questions than insight. The latter hippocampal discussion was at least more grounded.\n\nWhile a strong motivator for this work would be in allowing for higher efficiency on longer BPTT sequences, potentially capturing longer term dependencies, this aspect was not explored to this reviewer's understanding. To ensure clarity, in the character level PTB or Text8 examples, SAB's previous attention was limited to sequences of T = 100 or 180 respectively?\nLimiting the truncation to values below the sequence length for the LSTM baselines also appears strange given the standard within the literature is setting sequence length equal to BPTT length. I presume this was done to keep the number of optimizer updates equal?\nAnother broader question is whether longer term dependencies could be caught at all given the model doesn't feature \"exploration\" in the reinforcement learning sense, especially for non-trivial longer term dependencies.\n\nWhen noting the speed of generating a sparsely sourced summary vector (equation 3), it is worth pointing out that weighted summation over vectors in traditional attention is not a limiting factor as it's a very rapid element-wise only operation over already computed states.\n\nFor the experiments, I was looking for comparisons to attention over the \"LSTM (full BPTT)\" window. This experiment would provide an upper bound and an understanding of how much of SAB's improvement may be as a result of simply adding attention to the underlying LSTM models. Even a simpler and fast (cuDNN compatible) attention mechanism such as [a single cuDNN LSTM layer over the input, an attentional mechanism over the results of the first layer (masked to avoid observing timesteps from the future), summed, and then passed into a softmax] would be informative.\n\nFinally, whilst not a deal breaker for introducing new techniques, stronger LSTM baselines help to further underline the efficacy of the technique. For sequential MNIST, a relatively small dataset, previous papers have LSTM models that achieve 98.2% test accuracy (Arjovsky et al, https://arxiv.org/abs/1511.06464) and the IRNN example included as part of the Keras framework achieves 93% out of the box.\n\nNoting similarities to the Transformer architecture and other similar architectures would also be useful. Both are using attention to minimize the length of a gradient's path, though in Transformers it eliminates the RNN entirely. If a Transformer network performed a k=5 convolution or limited RNN run to produce the initial inputs to the Transformer, it would share many similarities to SAB, though without the sparsity.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks","abstract":"A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic.  However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored.  Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights.  This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.   ","pdf":"/pdf/17aa0f1cb740cd2e0bc8da2c8f614a7cba5a6678.pdf","TL;DR":"Towards Efficient Credit Assignment in Recurrent Networks without Backpropagation Through Time","paperhash":"anonymous|sparse_attentive_backtracking_longrange_credit_assignment_in_recurrent_networks","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJCq_fZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper874/Authors"],"keywords":["recurrent neural networks","long-term dependencies","back-propagation through time","truncated back-propagation","biological inspiration","self-attention"]}},{"tddate":null,"ddate":null,"tmdate":1509739054452,"tcdate":1509136534488,"number":874,"cdate":1509739051783,"id":"SJCq_fZ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJCq_fZ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks","abstract":"A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic.  However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored.  Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights.  This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.   ","pdf":"/pdf/17aa0f1cb740cd2e0bc8da2c8f614a7cba5a6678.pdf","TL;DR":"Towards Efficient Credit Assignment in Recurrent Networks without Backpropagation Through Time","paperhash":"anonymous|sparse_attentive_backtracking_longrange_credit_assignment_in_recurrent_networks","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJCq_fZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper874/Authors"],"keywords":["recurrent neural networks","long-term dependencies","back-propagation through time","truncated back-propagation","biological inspiration","self-attention"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}