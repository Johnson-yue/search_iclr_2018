{"notes":[{"tddate":null,"ddate":null,"tmdate":1512232960298,"tcdate":1512232913457,"number":1,"cdate":1512232913457,"id":"S1FRPUgWG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"rJm-vMaxG","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Connection with Message Passing Neural Network","comment":"Thanks for pointing out this paper!\nWe do think we can rephrase our model as a message passing neural network (MPNNs) except some subtle differences, like the message function could take representations of both head and tail of an edge as input arguments in MPNNs whereas ours only takes representation of head to compute the message.\nWe will add the discussion of these connections in the final version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512202786300,"tcdate":1512202733926,"number":2,"cdate":1512202733926,"id":"SJ8gfJebG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Public_Comment","forum":"S1sqHMZCb","replyto":"rJm-vMaxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Message passing neural networks vs. graph neural networks","comment":"I think it is fair to frame their model as a graph neural network, as it closely resembles the \"local transition function\" proposed in the original graph neural network paper (Gori et al., 2009): http://ieeexplore.ieee.org/document/4700287/\n\nThe only architectural difference that the authors propose here is to use a gated per-node update function right after the local transition function is evaluated - this mostly resembles the work from Li et al., 2015: https://arxiv.org/abs/1511.05493 (which is cited)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512019707293,"tcdate":1512019707293,"number":1,"cdate":1512019707293,"id":"rJm-vMaxG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Public_Comment","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["~George_Edward_Dahl1"],"readers":["everyone"],"writers":["~George_Edward_Dahl1"],"content":{"title":"Can you write your model as a message passing neural network?","comment":"Is it possible to write your model as a message passing neural network, as in http://proceedings.mlr.press/v70/gilmer17a.html ? It looks closely related and readers might benefit from any explicit connections that can be made."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222794889,"tcdate":1512005171833,"number":3,"cdate":1512005171833,"id":"Hy24AAnlM","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Though occasionally unclear, the authors present an interesting approach to solving a well-scoped problem.","rating":"7: Good paper, accept","review":"The authors present an interesting application of Graph Neural Networks to learning policies for controlling \"centipede\" robots of different lengths. They leverage the non-parametric nature of graph neural networks to show that their approach is capable of transferring policies to different robots more quickly than other approaches. The significance of this work is in its application of GNNs to a potentially practical problem in the robotics domain. The paper suffers from some clarity/presentation issues that will need to be improved. Ultimately, the contribution of this paper is rather specific, yet the authors show the clear advantage of their technique for improved performance and transfer learning on some agent types within this domain.\n\nSome comments:\n- Significant: A brief statement of the paper's \"contributions\" is also needed; it is unclear at first glance what portions of the work are the authors' own contributions versus prior work, particularly in the section describing the GNN theory.\n- Abstract: I take issue with the phrase \"are significantly better than policies learned by other models\", since this is not universally true. While there is a clear benefit to their technique for the centipede and snake models, the performance on the other agents is mostly comparable, rather than \"significantly better\"; this should be reflected in the abstract.\n- Figure 1 is instructive, but another figure is needed to better illustrate the algorithm (including how the state of the world is mapped to the graph state h, how these \"message\" are passed between nodes, and how the final graph states are used to develop a policy). This would greatly help clarity, particularly for those who have not seen GNNs before, and would make the paper more self-contained and easier to follow. The figure could also include some annotated examples of the input spaces of the different joints, etc. Relatedly, Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example (an example might help the reader understand the procedure without having to develop an intuition for GNNs).\n- There is almost certainly a typo in Eq. (4), since it does not contain the aggregated message \\bar{m}_u^t.\n\nSmaller issues / typos:\n- Abstract: please spell out spell out multi-layer perceptrons (MLP).\n- Sec 2.2: \"servers\" should be \"serves\"\n- \"performance By\" on page 4 is missing a \".\"\n\nPros:\n- The paper presents an interesting application of GNNs to the space of reinforcement learning and clearly show the benefits of their approach for the specific task of transfer learning.\n- To the best of my knowledge, the paper presents an original result and presents a good-faith effort to compare to existing, alternative systems (showing that they outperform on the tasks of interest).\n\nCons:\n- The contributions of the paper should be more clearly stated (see comment above).\n- The section describing their approach is not \"self contained\" and is difficult for an unlearned reader to follow.\n- The problem the authors have chosen to tackle is perhaps a bit \"specific\", since the performance of their approach is only really shown to exceed the performance on agents, like centipedes or snakes, which have this \"modular\" quality.\n\nI certainly hope the authors improve the quality of the theory section; the poor presentation here brings down the rest of the paper, which is otherwise an easy read.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222794937,"tcdate":1511846941136,"number":2,"cdate":1511846941136,"id":"r1r7Vd9gf","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Investigates an under-explored idea, but evaluation could be more compelling","rating":"6: Marginally above acceptance threshold","review":"The submission proposes incorporation of additional structure into reinforcement learning problems. In particular, the structure of the agent's morphology. The policy is represented as a graph neural network over the agent's morphology graph and message passing is used to update individual actions per joint.\n\nThe exposition is fairly clear and the method is well-motivated. I see no issues with the mathematical correctness of the claims made in the paper. However, the paper could benefit from being shorter by moving some details to the appendix (such as much of section 2.1 and PPO description).\n\nRelated work section could consider the following papers:\n\n\"Discrete Sequential Prediction of Continuous Actions for Deep RL\"\nAnother approach that outputs actions per joint, although in a general manner that does not require morphology structure\n\n\"Generalized Biped Walking Control\"\nConsiders the task of interactively changing limb lengths (your size transfer task) in a zero-shot manner, albeit with a non-neural network controller\n\nThe experimental results investigate the effects of various algorithm parameters, which is appreciated. However, a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation. In addition to robustness to limb length and disability perturbations, it would have been very nice to see multi-task learning that takes advantage of body structure (such as learning to reach for target with arms while walking with legs and being able to learn those independently, for example).\n\nHowever, I do think using agent morphology is an under-explored idea and one that is general, since we tend to have access to this structure in continuous control tasks for the time being. As a result, I believe this submission would be of interest to ICLR community.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222794980,"tcdate":1511591442882,"number":1,"cdate":1511591442882,"id":"BkjfRYLxz","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A nice paper that learns structured policy for control","rating":"7: Good paper, accept","review":"This paper proposes NerveNet to represent and learn structured policy for continuous control tasks. Instead of using the widely adopted fully connected MLP, this paper uses Graph Neural Networks to learn a structured controller for various MuJoco environments. It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine-tuning of transfer.\n\nThe idea to build structured policy is novel for continuous control tasks. It is an exciting direction since there are inherent structures that should be exploited in many control tasks, especially for locomotion. This paper explores this less-studied area and demonstrates promising results.\n\nThe presentation is mostly clear. Here are some questions and a list of minor suggestions:\n1) In the Output Model section, I am not sure how the controller is shared. It first says that \"Nodes with the same node type should share the instance of MLP\", which means all the \"joint\" nodes should share the same controller. But later it says \"Two LeftHip should have a shared controller.\" What about RightHip? or Ankle? They all belongs to the same node type \"joint\". Am I missing something here? It seems that in this paper, weights sharing is an essential part of the structured policy, it would be great if it can be described in more details.\n\n2) In States Update of Propagation Model Section, it is not clear how the aggregated message is used in eq. (4).\n\n3) Typo in Caption of Table 1: CentipedeFour not CentipedeSix.\n\n4) If we just use MLP but share weights among joints (e.g. the weights from observation to action of all the LeftHips are constrained to be same), how would it compare to the method proposed in this paper?\n\nIn summary, I think that it is worthwhile to develop structured representation of policies for control tasks. It is analogue to use CNN that share weights between kernels for computer vision tasks. I believe that this paper could inspire many follow-up work. For this reason, I would recommend accepting this paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1509739072182,"tcdate":1509135763462,"number":840,"cdate":1509739069520,"id":"S1sqHMZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/72d866176b6da175e4a7373672c1da5fdcca08b4.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}