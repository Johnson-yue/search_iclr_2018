{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222560245,"tcdate":1511892959911,"number":3,"cdate":1511892959911,"id":"SJ_kd7ixf","invitation":"ICLR.cc/2018/Conference/-/Paper115/Official_Review","forum":"HJSA_e1AW","replyto":"HJSA_e1AW","signatures":["ICLR.cc/2018/Conference/Paper115/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper extended the Adam optimization algorithm to preserve the update direction. Instead of using the un-centered variance of individual weights, the proposed method adapts the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector. The authors empirically demonstrated the method works well on CIFAR-10/100 tasks.\n\nComments:\n\n- I found the paper very hard to follow. The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest. The authors also combined the proposed method with a few existing deep learning tricks in the paper. All those tricks that, ie. section 3.3 and 4, should go into the background section.\n\n- Overall, the only contribution of the paper seems to be the ad-hoc modification to Adam in Eq. (9). Why is this a reasonable modification? Do we expect this modification to fail in any circumstances? The experiments on CIFAR dataset and one CNN architecture do not provide enough evidence to show the proposed method work well in general.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1511817138959,"tcdate":1511817138959,"number":2,"cdate":1511817138959,"id":"Bks3kZqgM","invitation":"ICLR.cc/2018/Conference/-/Paper115/Official_Comment","forum":"HJSA_e1AW","replyto":"HJSA_e1AW","signatures":["ICLR.cc/2018/Conference/Paper115/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper115/Authors"],"content":{"title":"Code","comment":"Code can be found at https://github.com/zj10/ND-Adam."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1512222560291,"tcdate":1511813753101,"number":2,"cdate":1511813753101,"id":"S1-Kfe5lM","invitation":"ICLR.cc/2018/Conference/-/Paper115/Official_Review","forum":"HJSA_e1AW","replyto":"HJSA_e1AW","signatures":["ICLR.cc/2018/Conference/Paper115/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A variant of ADAM optimization algorithm that normalizes the weights of each hidden unit","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a variant of ADAM optimization algorithm that normalizes the weights of each hidden unit. They further suggest using batch normalization on the output of the network before softmax to improve the generalization. The main ideas are new to me and the paper is well-written. The arguments and derivations are very clear. However, the experimental results suggest that the proposed method is not superior to SGD and ADAM.\n\nPros: \n\n- The idea of optimizing the direction while ignoring the magnitude is interesting and make sense.\n- Using batch normalization before softmax is interesting.\n\nCons:\n\n- In the abstract, authors claim that the proposed method has good optimization performance of ADAM and good generalization performance of SGD. Such a method could be helpful if one can get to the same level of generalization faster (less number of epochs). However, the experiments suggest that optimization advantages of the proposed method do not translate to faster generalization. Figures 2,3 and Table 1 indicate that the generalization performance of this method is very similar to SGD.\n\n- The paper is not coherent. In particular, direction-preserving ADAM and batch-normalized softmax trick are completely orthogonal ideas. \n\n- In the introduction and Section 2.2, authors claim that weight decay has a significant effect on the generalization performance of DNNs. I wonder if authors can refer to any work on this. My own experience and several empirical works have suggested that weight decay does not improve generalization significantly.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1512222560349,"tcdate":1511809823709,"number":1,"cdate":1511809823709,"id":"Bk_mQkcgM","invitation":"ICLR.cc/2018/Conference/-/Paper115/Official_Review","forum":"HJSA_e1AW","replyto":"HJSA_e1AW","signatures":["ICLR.cc/2018/Conference/Paper115/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Some related works should be analyzed. The experimental validation should to be revised. ","rating":"5: Marginally below acceptance threshold","review":"Method:\n\nThe paper is missing analysis of some important related works such as\n\n\"Beyond convexity: Stochastic quasi-convex optimization\" by E. Hazan et al. (2015) \n\nwhere Stochastic Normalized Gradient Descent (SNGD) was proposed. \n\nThen, normalized gradient versions of AdaGrad and Adam were proposed in \n\n\"Normalized Gradient with Adaptive Stepsize Method for Deep\nNeural Network Training\" by A. W. Yu et al. (2017).\n\nAnother work which I find to be relevant is \n\n\"Follow the Signs for Robust Stochastic Optimization\" by L. Balles and P. Hennig (2017).\n\nFrom my personal experiments, restricting w_i to have L2 norm of 1, i.e., to be +-1 \nleads to worse generalization. One reason for this is that weight decay is not \nreally functioning since it cannot move w_i to 0 or make its amplitude any smaller. \nPlease correct me if I misunderstand something here. \n\nThe presence of +-1 weights moves us to the area of low-precision NNs, \nor more specifically, NNs with binary / binarized weights as in \n\n\"BinaryConnect: Training Deep Neural Networks with\nbinary weights during propagations\" by M. Courbariaux et al. (2015)\n\nand \n\n\"Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1\" by M. Courbariaux et al. (2016). \n\nRegarding\n\"Moreover, the magnitude of each update does not depend on themagnitude of the gradient. Thus, ND-Adam is more robust to improper initialization, and vanishing or exploding gradients.\"\n\nIf the magnitude of each update does not depend on the magnitude of the gradient, then the algorithm heavily depends on the learning rate. Otherwise, it does not have any means to approach the optimum in a reasonable number of steps *when* it is initialized very / unreasonably far from it. The claim of your second sentence is not supported by the paper. \n\nEvaluation:\n\nI am not confident that the presented experimental validation is fair. First, the original WRN paper and many other papers with ResNets used weight decay of 0.0005 and not 0.001 or 0.002 as used for SGD in this paper. It is unclear why this setting was changed. One could just use \\alpha_0 = 0.05 and \\lambda = 0.0005.\n\nThen, I don't see why the authors use WRN-22-7.5 which is different from WRN-28-10 which was suggested in the original study and used in several follow-up works. The difference between WRN-22-7.5 and WRN-28-10 is unlikely to be significant, \nthe former might have about only 2 times less parameters which should barely change the final validation errors. However, the use of WRN-22-7.5 makes it impossible to easily compare the presented results to the results of Zagoruyko who had 3.8\\% with WRN-28-10. I believe that the use of the setup of Zagoruyko for WRN-22-7.5 would allow to get much better results than 4.5\\% and 4.49\\% shown for SGD and likely better 4.14\\% shown for ND-Adam. I note that the use of WRN-22-7.5 is unlikely to be due to the used hardware because later in paper the authors refer to WRN-34-7.5.\n\nMy intuition is that the proposed ND-Adam moves the algorithm back to SGD but with potentially harmful constraints of w_i=+-1. Even the values of \\alpha^v_0 found for ND-Adam (e.g., \\alpha^v_0=0.05 in Figure 1B) are in line of what would be optimal values of \\alpha_0 for SGD. \n\nI find it uncomfortable that BN-Softmax is introduced here to support the use of an optimization algorithm, moreover, that the values of \\gamma_c are different for CIFAR-10 and CIFAR-100. I wonder if the proposed values are optimal (and therefore selected) for all three tested algorithms  or only for Adam-ND. I expect that hyperparameters of SGD and Adam would also need to be revised to account for BN-Softmax.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1511809506306,"tcdate":1511809506306,"number":1,"cdate":1511809506306,"id":"SJ9yMkqlf","invitation":"ICLR.cc/2018/Conference/-/Paper115/Official_Comment","forum":"HJSA_e1AW","replyto":"r1864VYlG","signatures":["ICLR.cc/2018/Conference/Paper115/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper115/Authors"],"content":{"title":"Responses","comment":"Thank you for your comments.\n\nIn https://arxiv.org/pdf/1707.04822.pdf, they normalize the gradient of each step to keep only its direction. However, the gradient is further multiplied by individually adapted step sizes to form the actual updates, thus changing the direction of the gradient.\n\nIn this work, we normalized the input weight vector of each hidden unit, rather than the gradient, at the end of each step. We also adapt the learning rate to weight vectors instead of individual weights, in order to preserve the direction of gradient."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1511765182148,"tcdate":1511765182148,"number":1,"cdate":1511765182148,"id":"r1864VYlG","invitation":"ICLR.cc/2018/Conference/-/Paper115/Public_Comment","forum":"HJSA_e1AW","replyto":"HJSA_e1AW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related work","comment":"There is probably a missing related work: https://arxiv.org/pdf/1707.04822.pdf\n\nIn that work, the gradient is normalized to preserve the direction, while the adaptive step size (Adam) is used. This sounds exactly match your paper title. Could you elaborate the difference if there is any?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1509739475940,"tcdate":1508997325439,"number":115,"cdate":1509739473281,"id":"HJSA_e1AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJSA_e1AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Normalized Direction-preserving Adam","abstract":"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of the models. While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs). In this work, we identify two problems of Adam that may degrade the generalization performance. As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which combines the best of both worlds, i.e., the good optimization performance of Adam, and the good generalization performance of SGD. In addition, we further improve the generalization performance in classification tasks, by using batch-normalized softmax. This study suggests the need for more precise control over the training process of DNNs.","pdf":"/pdf/5e8506a6a4eca30566f6e4c67b2a0203bc45f013.pdf","TL;DR":"A tailored version of Adam for training DNNs, which combines the good optimization performance of Adam, with the good generalization performance of SGD.","paperhash":"anonymous|normalized_directionpreserving_adam","_bibtex":"@article{\n  anonymous2018normalized,\n  title={Normalized Direction-preserving Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJSA_e1AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper115/Authors"],"keywords":["optimization","generalization","Adam","SGD"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}