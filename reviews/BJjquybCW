{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222670099,"tcdate":1511820607119,"number":2,"cdate":1511820607119,"id":"rkvS6-9gG","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/275af91e971bc27b247854f3fb628ad5c63a6548.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1512222670143,"tcdate":1511759101582,"number":1,"cdate":1511759101582,"id":"BkIW6fYxz","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"The loss surface and expressivity of deep convolutional neural networks\"","rating":"4: Ok but not good enough - rejection","review":"This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks. I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \"wide“ hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples.\n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data.\n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum. These solutions achieve zero squared-loss.\n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1). Intuitively, (1) is an easy result. Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent. The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small.\n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima; (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure; (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero. If an optimization algorithm falls onto these solutions, it will be hard to escape.\n\nOverall, the paper presents several incremental improvement over existing theories. However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/275af91e971bc27b247854f3fb628ad5c63a6548.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1509739272819,"tcdate":1509124242758,"number":493,"cdate":1509739270159,"id":"BJjquybCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJjquybCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/275af91e971bc27b247854f3fb628ad5c63a6548.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}