{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222695361,"tcdate":1511938455642,"number":1,"cdate":1511938455642,"id":"BkeoFCjgG","invitation":"ICLR.cc/2018/Conference/-/Paper593/Official_Review","forum":"rkN2Il-RZ","replyto":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference/Paper593/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A neural network that learns visual concepts and basic operators over them.","rating":"5: Marginally below acceptance threshold","review":"This paper proposed a novel neural net architecture that learns object concepts by combining a beta-VAE and SCAN. The SCAN is actually another beta-VAE with an additional term that minimizes the KL between the distribution of its latent representation and the first beta-VAE’s latent distribution. The authors also explored how this structure could be further expanded to incorporate another neural net that learns operators (and, in common, ignore), and demonstrated that the proposed system is able to generate accurate and diverse scenes given the visual descriptions.\n\nIn general, I think this paper is interesting. It’s studying an important problem with a newly proposed neural net structure. The experimental results are good and the model is compared with very recent baselines.\n\nI am, however, still lukewarm on this submission for its limited technical innovation and over-simplified experimental setup.\n\nThis paper does have technical innovations: the SCAN architecture and the way they learn “recombination operators” are newly proposed. However, there are in essence very straightforward extensions of VAE and beta-VAE (this is based on the fact that beta-VAE itself is a simple modification of VAE and the effect was discussed in a number of concurrent papers).\n\nThis would still be fine, as many small modifications of neural net architecture turn out to reveal fundamental insights that push the field forward. This is, however, not the case in this paper (at least not in the current manuscript) due to its over-simplified experiments. The authors are using images as input, but the images are all synthetic, and further, they are all synthesized to have highly regular structure. This suggests the network is likely to overfit the data and learn a straightforward mapping from input to the code. It’s unclear how well the system is able to generalize to real-world scenarios. Note that even datasets like MNIST has much higher complexity than the dataset used in this paper (though the dataset in this paper is more colorful).\n\nI agree that the proposed method performs better that its recent competitors. However, many of those methods like TripleELBO are not explicitly designed for these ‘recombination operators’. In contrast, they seem to perform well on real datasets. I would strongly suggest the authors perform additional experiments on standard benchmarks for a fair comparison.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]}},{"tddate":null,"ddate":null,"tmdate":1509739212795,"tcdate":1509127851938,"number":593,"cdate":1509739210138,"id":"rkN2Il-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]},"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}