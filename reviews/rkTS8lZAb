{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222695087,"tcdate":1511811160075,"number":3,"cdate":1511811160075,"id":"H1lP_k5ez","invitation":"ICLR.cc/2018/Conference/-/Paper591/Official_Review","forum":"rkTS8lZAb","replyto":"rkTS8lZAb","signatures":["ICLR.cc/2018/Conference/Paper591/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Why should I use your method?","rating":"4: Ok but not good enough - rejection","review":"The paper introduces a new method for training GANs with discrete data. To this end, the output of the discriminator is interpreted as importance weight and REINFORCE-like updates are used to train the generator.\n\nDespite making interesting connections between different ideas in GAN training, I found the paper to be disorganized and hard to read. My main concern is the fact that the paper does not make any comparison with other methods for handling of discrete data in GANs. In particular, (Gulrajani et al.â€™17) show that it is possible to train Wasserstein GANs without sampling one-hot vectors for discrete variables during the training. Is there a reason to use REINFORCE-like updates when such a direct approach works?\n\nMinor: \ncomplex conjugate => convex conjugate ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boundary Seeking GANs","abstract":"Generative adversarial networks are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation.  In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.","pdf":"/pdf/2ce0e40a65f5f7a4ff3f38bdf644b782eeee3914.pdf","TL;DR":"We address training GANs with discrete data by formulating a policy gradient that generalizes across f-divergences","paperhash":"anonymous|boundary_seeking_gans","_bibtex":"@article{\n  anonymous2018boundary,\n  title={Boundary Seeking GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTS8lZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper591/Authors"],"keywords":["Generative adversarial networks","generative learning","deep learning","neural networks","adversarial learning","discrete data"]}},{"tddate":null,"ddate":null,"tmdate":1512222695125,"tcdate":1511798249158,"number":2,"cdate":1511798249158,"id":"rJegU3tef","invitation":"ICLR.cc/2018/Conference/-/Paper591/Official_Review","forum":"rkTS8lZAb","replyto":"rkTS8lZAb","signatures":["ICLR.cc/2018/Conference/Paper591/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review for boundary seeking GAN","rating":"6: Marginally above acceptance threshold","review":"Summary of the paper:\n\nThe paper presents a method based on importance sampling and reinforcement learning to learn discrete generators in the GAN framework. The GAN uses an  f-divergence cost function for  training the discriminator. The generator is trained to minimize the KL distance between the  discrete generator q_{\\theta}(x|z), and the importance weight discrete real distribution estimator w(x|z)q(\\theta|z). where w(x|z) is estimated in turn using the discriminator. \nThe methodology is also extended to the continuous case. Experiments are conducted on quantized image generation, and text generation.\n\nQuality:\n\nthe paper is overall well written and supported with reasonable experiments.\n\nClarity:\n\nThe paper has a lot of typos that make sometimes the paper harder to follow:\n- page (2) Eq 3 max , min should be min, max if we want to keep working with f-divergence\n- Definition 2.1 \\mathbb{Q}_{\\theta} --> \\mathbb{Q}\n- page 5 the definition of \\tilde{w}(x^{(m})) in the normalization it is missing \\tilde{w}\n- Equation (10) \\nabla_{\\theta}\\log(x|z) --> \\nabla_{\\theta}\\log(x^{(m)}|z)\n- In algorithm 1, again missing indices in the update of theta  --> \\nabla_{\\theta}\\log(x^{(m|n)}|z^{n})\n \nOriginality:\n\nThe main ingredients of the paper are well known and already used in the literature (Reinforce for discrete GAN with Disc as a reward for e.g GAN for image captioning Dai et al). The perspective from importance sampling coming from f-divergence for discrete GAN has some novelty although the foundations of this work relate also to previous work:\n- Estimating ratios using the discriminator is well known for e.g learning implicit models , Mohamed et al \n- The relation of  importance sampling to  reinforce is also well known\" On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient,\" Tang and Abbeel.\n\nGeneral Review:\n\n- when the generator is producing only *one* discrete distribution the theory is presented in Section 2.3. When we move to experiments, for image generation for example, we need to have a generator that produces a distribution by pixel. It would be important for 1) understanding the work 2) the reproducibility of the work to parallel algorithm 1 and have it *in the paper*, for this 'multi discrete distribution ' generation case.  If we have N pixels    \\log(p(x_1,...x_N|z))= \\Pi_i g_{\\theta}(x_i|z) (this should be mentioned in the paper if it is the case ), it would be instructive to comment on the assumptions on independence/conditional dependence of this model, also to state clearly how the generator is updated in this case and what are importance sampling weights. \n\n- Would it make sense in this N pixel discrete case generation to have also the discriminator produce N probabilities of real and fake as in PixelGAN in Isola et al? then see in this case what are the importance sampling weights this would parallel the instantaneous reward in RL?\n\n\n\n \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boundary Seeking GANs","abstract":"Generative adversarial networks are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation.  In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.","pdf":"/pdf/2ce0e40a65f5f7a4ff3f38bdf644b782eeee3914.pdf","TL;DR":"We address training GANs with discrete data by formulating a policy gradient that generalizes across f-divergences","paperhash":"anonymous|boundary_seeking_gans","_bibtex":"@article{\n  anonymous2018boundary,\n  title={Boundary Seeking GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTS8lZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper591/Authors"],"keywords":["Generative adversarial networks","generative learning","deep learning","neural networks","adversarial learning","discrete data"]}},{"tddate":null,"ddate":null,"tmdate":1512222695162,"tcdate":1511789521390,"number":1,"cdate":1511789521390,"id":"H1FAQ9Flz","invitation":"ICLR.cc/2018/Conference/-/Paper591/Official_Review","forum":"rkTS8lZAb","replyto":"rkTS8lZAb","signatures":["ICLR.cc/2018/Conference/Paper591/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Contains some interesting ideas","rating":"6: Marginally above acceptance threshold","review":"Thank you for an interesting read.\n\nMy understanding of the paper is that:\n\n1. the paper proposes a density-ratio estimator via the f-gan approach;\n2. the paper proposes a training criterion that matches the generator's distribution to a self-normalised importance sampling (SIS) estimation of the data distribution;\n3. in order to reduce the variance of the REINFORCE gradient, the paper seeks out to do matching between conditionals instead.\n\nThere are a few things that I expect to see explanations, which are not included in the current version:\n\n1. Can you justify your variance reduction technique either empirically or experimentally? Because your method requires sampling multiple x for a single given z, then in the same wall-clock time I should be able to obtain more samples for the vanilla version eq (8). How do they compare?\n\n2. Why your density ratio estimation methods work in high dimensions, even when at the beginning p and q are so different?\n\n3. It's better to include some quantitative metrics for the image and NLP experiments rather than just showing the readers images and sentences!\n\n4. Over-optimising generators is like solving a max-min problem instead. You showed your method is more robust in this case, can you explain it from the objective you use, e.g. the convex/concavity of your approach in general?\n\nTypo: eq (3) should be min max I believe?\n\nBTW I'm not an expert of NLP so I won't say anything about the quality of the NLP experiment.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boundary Seeking GANs","abstract":"Generative adversarial networks are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation.  In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.","pdf":"/pdf/2ce0e40a65f5f7a4ff3f38bdf644b782eeee3914.pdf","TL;DR":"We address training GANs with discrete data by formulating a policy gradient that generalizes across f-divergences","paperhash":"anonymous|boundary_seeking_gans","_bibtex":"@article{\n  anonymous2018boundary,\n  title={Boundary Seeking GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTS8lZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper591/Authors"],"keywords":["Generative adversarial networks","generative learning","deep learning","neural networks","adversarial learning","discrete data"]}},{"tddate":null,"ddate":null,"tmdate":1509739214696,"tcdate":1509127749454,"number":591,"cdate":1509739212039,"id":"rkTS8lZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkTS8lZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Boundary Seeking GANs","abstract":"Generative adversarial networks are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation.  In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.","pdf":"/pdf/2ce0e40a65f5f7a4ff3f38bdf644b782eeee3914.pdf","TL;DR":"We address training GANs with discrete data by formulating a policy gradient that generalizes across f-divergences","paperhash":"anonymous|boundary_seeking_gans","_bibtex":"@article{\n  anonymous2018boundary,\n  title={Boundary Seeking GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTS8lZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper591/Authors"],"keywords":["Generative adversarial networks","generative learning","deep learning","neural networks","adversarial learning","discrete data"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}