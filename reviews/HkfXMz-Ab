{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222766949,"tcdate":1512190838480,"number":3,"cdate":1512190838480,"id":"SyhuQnyZz","invitation":"ICLR.cc/2018/Conference/-/Paper785/Official_Review","forum":"HkfXMz-Ab","replyto":"HkfXMz-Ab","signatures":["ICLR.cc/2018/Conference/Paper785/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Sketch Learning for Program Generation","rating":"7: Good paper, accept","review":"This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved. This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps. The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously.\n\nThis paper has many strengths:\n1) The writing is clear, and the paper is well-motivated\n2) The proposed algorithm is described in excellent detail, which is essential to reproducibility\n3) As stated previously, the approach is validated with a large number of real Android projects\n4) The fact that the language generated is non-trivial (Java-like) is a substantial plus\n5) Good discussion of limitations\n\nOverall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Sketch Learning for Conditional Program Generation","abstract":"We study the problem of generating source code in a strongly typed,\nJava-like programming language, given a label (for example a set of\nAPI calls or types) carrying a small amount of information about the\ncode that is desired. The generated programs are expected to respect a\n`\"realistic\" relationship between programs and labels, as exemplified\nby a corpus of labeled programs available during training.\n\nTwo challenges in such *conditional program generation* are that\nthe generated programs must satisfy a rich set of syntactic and\nsemantic constraints, and that source code contains many low-level\nfeatures that impede learning.  We address these problems by training\na neural generator not on code but on *program sketches*, or\nmodels of program syntax that abstract out names and operations that\ndo not generalize across programs. During generation, we infer a\nposterior distribution over sketches, then concretize samples from\nthis distribution into type-safe programs using combinatorial\ntechniques.  We implement our ideas in a system for generating\nAPI-heavy Java code, and show that it can often predict the entire\nbody of a method given just a few API calls or data types that appear\nin the method.","pdf":"/pdf/b01498119b8cd154d2b2f6636326997b0b4505ab.pdf","TL;DR":"We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.","paperhash":"anonymous|neural_sketch_learning_for_conditional_program_generation","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Sketch Learning for Conditional Program Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkfXMz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper785/Authors"],"keywords":["Program generation","Source code","Program synthesis","Deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222766992,"tcdate":1511747220622,"number":2,"cdate":1511747220622,"id":"rJ69A1Kxf","invitation":"ICLR.cc/2018/Conference/-/Paper785/Official_Review","forum":"HkfXMz-Ab","replyto":"HkfXMz-Ab","signatures":["ICLR.cc/2018/Conference/Paper785/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well executed, moving towards more realistic program synthesis tasks.","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program. The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts. First, the description is mapped to a \"sketch\" (Y) containing high level program structure but no concrete details about, e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations.\n\nThe paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs. Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST. The data is derived from an online repository of ~1500 Android apps, and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale. This is one of the strongest points of the paper.\n\nOne point I found confusing is how exactly the Combinatorial Concretization step works. Am I correct in understanding that this step depends only on Y, and that given Y, Prog is conditionally independent of X? If this is correct, how many Progs are consistent with a typical Y? Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated.\n\nI'm also curious whether using a stochastic latent variable (Z) is necessary. Would the approach work as well using a more standard encoder-decoder model with determinstic Z?\n\nSome discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate.\n\nOverall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found \"in the wild\". While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel, I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Sketch Learning for Conditional Program Generation","abstract":"We study the problem of generating source code in a strongly typed,\nJava-like programming language, given a label (for example a set of\nAPI calls or types) carrying a small amount of information about the\ncode that is desired. The generated programs are expected to respect a\n`\"realistic\" relationship between programs and labels, as exemplified\nby a corpus of labeled programs available during training.\n\nTwo challenges in such *conditional program generation* are that\nthe generated programs must satisfy a rich set of syntactic and\nsemantic constraints, and that source code contains many low-level\nfeatures that impede learning.  We address these problems by training\na neural generator not on code but on *program sketches*, or\nmodels of program syntax that abstract out names and operations that\ndo not generalize across programs. During generation, we infer a\nposterior distribution over sketches, then concretize samples from\nthis distribution into type-safe programs using combinatorial\ntechniques.  We implement our ideas in a system for generating\nAPI-heavy Java code, and show that it can often predict the entire\nbody of a method given just a few API calls or data types that appear\nin the method.","pdf":"/pdf/b01498119b8cd154d2b2f6636326997b0b4505ab.pdf","TL;DR":"We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.","paperhash":"anonymous|neural_sketch_learning_for_conditional_program_generation","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Sketch Learning for Conditional Program Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkfXMz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper785/Authors"],"keywords":["Program generation","Source code","Program synthesis","Deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222767034,"tcdate":1511718210332,"number":1,"cdate":1511718210332,"id":"Sy9Sau_xf","invitation":"ICLR.cc/2018/Conference/-/Paper785/Official_Review","forum":"HkfXMz-Ab","replyto":"HkfXMz-Ab","signatures":["ICLR.cc/2018/Conference/Paper785/AnonReviewer3"],"readers":["everyone"],"content":{"title":"sketch learning ","rating":"7: Good paper, accept","review":"The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language. In this setting, they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names. Conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketches. \n \nPositives:\n \n\t•\tNovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed\n\t•\tVery good abstract: It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results.\n\t•\tExcellently structured and presented paper\n \n\t•\tMotivation given in form of relevant applications and mention that it is relatively unstudied\n\t•\tThe hypothesis/ the papers goal is clearly stated. It is introduced with 'We ask' followed by two well formulated lines that make up the hypothesis. It is repeated multiple times throughout the paper. Every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners, clearly stating discrepancies.\n\t•\tExplanations are exceptionally well done: terms that might not be familiar to the reader are explained. This is true for mathematical aspects as well as program generating specific terms. Examples are given where appropriate in a clear and coherent manner\n\t•\tProblem statement well defined mathematically and understandable for a broad audience\n\t•\tMentioning of failures and limitations demonstrates a realistic  view on the project\n\t•\tComplexity and time analysis provided\n\t•\tPaper written so that it's easy for a reader to implement the methods\n\t•\tDetailed descriptions of all instantiations even parameters and comparison methods\n\t•\tSystem specified\n\t•\tValidation method specified\n\t•\tData and repository, as well as cleaning process provided\n\t•\tEvery figure and plot is well explained and interpreted\n\t•\tLarge successful evaluation section provided\n\t•\tMany different evaluation measures defined to measure different properties of the project\n\t•\tDifferent observability modes\n\t•\tEvaluation against most compatible methods from other sources \n\t•\t Results are in line with hypothesis\n\t•\tThorough appendix clearing any open questions \n \nIt would have been good to have a summary/conclusion/future work section\n \nSUMMARY: ACCEPT.  The authors present a very intriguing novel approach that  in a clear and coherent way. The approach is thoroughly explained for a large audience. The task itself is interesting and novel. The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising. Even though no conclusive section is provided, the paper is not missing any information.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Sketch Learning for Conditional Program Generation","abstract":"We study the problem of generating source code in a strongly typed,\nJava-like programming language, given a label (for example a set of\nAPI calls or types) carrying a small amount of information about the\ncode that is desired. The generated programs are expected to respect a\n`\"realistic\" relationship between programs and labels, as exemplified\nby a corpus of labeled programs available during training.\n\nTwo challenges in such *conditional program generation* are that\nthe generated programs must satisfy a rich set of syntactic and\nsemantic constraints, and that source code contains many low-level\nfeatures that impede learning.  We address these problems by training\na neural generator not on code but on *program sketches*, or\nmodels of program syntax that abstract out names and operations that\ndo not generalize across programs. During generation, we infer a\nposterior distribution over sketches, then concretize samples from\nthis distribution into type-safe programs using combinatorial\ntechniques.  We implement our ideas in a system for generating\nAPI-heavy Java code, and show that it can often predict the entire\nbody of a method given just a few API calls or data types that appear\nin the method.","pdf":"/pdf/b01498119b8cd154d2b2f6636326997b0b4505ab.pdf","TL;DR":"We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.","paperhash":"anonymous|neural_sketch_learning_for_conditional_program_generation","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Sketch Learning for Conditional Program Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkfXMz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper785/Authors"],"keywords":["Program generation","Source code","Program synthesis","Deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509739104201,"tcdate":1509134874067,"number":785,"cdate":1509739101543,"id":"HkfXMz-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkfXMz-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Sketch Learning for Conditional Program Generation","abstract":"We study the problem of generating source code in a strongly typed,\nJava-like programming language, given a label (for example a set of\nAPI calls or types) carrying a small amount of information about the\ncode that is desired. The generated programs are expected to respect a\n`\"realistic\" relationship between programs and labels, as exemplified\nby a corpus of labeled programs available during training.\n\nTwo challenges in such *conditional program generation* are that\nthe generated programs must satisfy a rich set of syntactic and\nsemantic constraints, and that source code contains many low-level\nfeatures that impede learning.  We address these problems by training\na neural generator not on code but on *program sketches*, or\nmodels of program syntax that abstract out names and operations that\ndo not generalize across programs. During generation, we infer a\nposterior distribution over sketches, then concretize samples from\nthis distribution into type-safe programs using combinatorial\ntechniques.  We implement our ideas in a system for generating\nAPI-heavy Java code, and show that it can often predict the entire\nbody of a method given just a few API calls or data types that appear\nin the method.","pdf":"/pdf/b01498119b8cd154d2b2f6636326997b0b4505ab.pdf","TL;DR":"We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.","paperhash":"anonymous|neural_sketch_learning_for_conditional_program_generation","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Sketch Learning for Conditional Program Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkfXMz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper785/Authors"],"keywords":["Program generation","Source code","Program synthesis","Deep generative models"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}