{"notes":[{"tddate":null,"ddate":null,"tmdate":1512257689203,"tcdate":1512257689203,"number":3,"cdate":1512257689203,"id":"rJbiu3lbM","invitation":"ICLR.cc/2018/Conference/-/Paper563/Official_Review","forum":"rJUYGxbCW","replyto":"rJUYGxbCW","signatures":["ICLR.cc/2018/Conference/Paper563/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting experimental results, but no definitive argument","rating":"7: Good paper, accept","review":"The authors propose to use a generative model of images to detect and defend against adverarial examples. White-box attacks against standard models for image recognition (Resnet and VGG) are considered, and a generative model (a PixelCNN) is trained on the same data as the classifiers. The authors first show that adversarial examples created by the white-box attacks correspond to low likelihood region (according to the pixelCNN), which first gives a classification rule for detecting adversarial examples.\n\nThen, to turn the genrative model into a defensive algorithm, the authors propose to preprocess test images by approximately maximizing the likelihood under similar constraints as the attacker of images, to \"project\" adversarial examples back to high-density regions (as estimated by the generative model). As a heuristic method, the authors propose to greedily maximize the likelihood of the incoming images pixel-by-pixel, which is possible because of the specific form of the PixelCNN likelihood in the context of l-infty attacks. An \"adaptive\" version of the algorithm, in which the preprocessing is used only when the likelihood of an example is below a certain threshold, is also proposed.\n\nExperiments are carried out on Fashion MNIST and CIFAR-10. At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples. The main result is that this approach based on generative models seems to work even on against the strongest attacks.\n\nOverall, the idea proposed in the paper, using a generative model to detect and filter out spurious patterns that can appear in adversarial examples, is rather intuitive. The experimental result that adversarial examples can somehow be corrected by a generative model is also interesting. The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting.\n\nWhereas the paper is an interesting step forward, the paper still doesn't provide definitive arguments in favor of using such approaches in practice. There is a significant loss in accuracy on clean examples (2% on CIFAR-10 for a resnet), and more generally against weaker opponents such as the fast gradient sign. Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models. This somewhat weakens the result, since robustness against these methods that do not transfer well is achieved by changing the model. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples","abstract":"Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.","pdf":"/pdf/0d4ce57a71d03574168a087da01ebbc568cf512f.pdf","paperhash":"anonymous|pixeldefend_leveraging_generative_models_to_understand_and_defend_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018pixeldefend:,\n  title={PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUYGxbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper563/Authors"],"keywords":["Adversarial Examples","Generative Models","Purification","Hypothesis Testing"]}},{"tddate":null,"ddate":null,"tmdate":1512222691592,"tcdate":1511690603617,"number":2,"cdate":1511690603617,"id":"rJ4_WfuxM","invitation":"ICLR.cc/2018/Conference/-/Paper563/Official_Review","forum":"rJUYGxbCW","replyto":"rJUYGxbCW","signatures":["ICLR.cc/2018/Conference/Paper563/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A convincing new way to defend image models against adversarial examples.","rating":"7: Good paper, accept","review":"The paper describes the creative application of a density estimation model to clean up adversarial examples before applying and image model (for classification, in this setup). The basic idea is that the image is first moved back to the probable region of images before applying the classifier. For images, the successful PiexlCNN model is used as a density estimator and is applied to clean up the image before the classification is attempted.\n\nThe proposed method is very intuitive, but might be expensive if a naive implementation of PixelCNN is used for the cleaning. The approach is novel. It is useful that the density estimator model does not have to rely on the labels. Also, it might even be trained on a different dataset potentially.\n\nThe con is that the proposed methodology still does not solve the problem of adversarial examples completely.\n\nMinor nitpick: In section 2.1, it is suggested that DeepFool was the first optimization based attack to minimize the perturbation wrt the original image. In fact the much earler (2013) \"Intriguing Propoerties ... \" paper relied on the same formulation (minimizing perturbation under several constraints: changed detection and pixel intensities are being in the given range).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples","abstract":"Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.","pdf":"/pdf/0d4ce57a71d03574168a087da01ebbc568cf512f.pdf","paperhash":"anonymous|pixeldefend_leveraging_generative_models_to_understand_and_defend_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018pixeldefend:,\n  title={PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUYGxbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper563/Authors"],"keywords":["Adversarial Examples","Generative Models","Purification","Hypothesis Testing"]}},{"tddate":null,"ddate":null,"tmdate":1512222691630,"tcdate":1510961922339,"number":1,"cdate":1510961922339,"id":"HJ9WQx6JG","invitation":"ICLR.cc/2018/Conference/-/Paper563/Official_Review","forum":"rJUYGxbCW","replyto":"rJUYGxbCW","signatures":["ICLR.cc/2018/Conference/Paper563/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of PixelDefend","rating":"6: Marginally above acceptance threshold","review":"The authors describe a method for detecting adversarial examples by measuring the likelihood in terms of a generative model of an image. Furthermore, the authors prescribe a method for cleaning or 'santizing' an adversarial image through employing a generative model. The authors demonstrate some success in restoring images that have been adversarially perturbed with this technique.\n\nThe idea of using a generative model (PixelCNN) to assess whether a given image has been adversarially perturbed is a very interesting and understandable finding that may contribute quite nicely to the adversarial literature. One limitation of this method, however, is our ability to build successful generative models for high resolution images. However, I would be curious to know if the authors tried their method on high resolution images, regardless?\n\nMajor comments:\n1) Cross validation. Figure 2a is quite interesting and compelling. It is not clear from the figure if the 'clean' (nor the other data for that matter) is from the *training* or  *testing* data for the PixelCNN model. I would *hope* that this is from the *testing* data indicating that these are the likelihood on unseen images?\n\nThat said, it would be interesting to see the *training* data on this plot as well to see if there are any systematic shifts that might make the distribution of adversarial examples less discernible.\n\n2) Adversary to PixelCNN. It is not clear why a PixelCNN may not be adversarially attacked, nor if such a model would be able to guard against an adversarial attack. I am not sure how well viable of strategy this may be but it is worth understanding or addressing to determine how viable this method for guarding actually is.\n\n3) Restorative effects of PixelDefend. I would like to see individual examples of (a) adversarial perturbation for a given image and (b) PixelDefend perturbation for that adversarial image. In particular, I would like to see how close (a) is the negative of (b). This would give me more confidence that this techniques is successfully guarding against the original attack.\n\nI am willing to adjust my rating upward if the authors are able to address some of the points above in a substantive manner. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples","abstract":"Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.","pdf":"/pdf/0d4ce57a71d03574168a087da01ebbc568cf512f.pdf","paperhash":"anonymous|pixeldefend_leveraging_generative_models_to_understand_and_defend_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018pixeldefend:,\n  title={PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUYGxbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper563/Authors"],"keywords":["Adversarial Examples","Generative Models","Purification","Hypothesis Testing"]}},{"tddate":null,"ddate":null,"tmdate":1509739234012,"tcdate":1509126781636,"number":563,"cdate":1509739231337,"id":"rJUYGxbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJUYGxbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples","abstract":"Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.","pdf":"/pdf/0d4ce57a71d03574168a087da01ebbc568cf512f.pdf","paperhash":"anonymous|pixeldefend_leveraging_generative_models_to_understand_and_defend_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018pixeldefend:,\n  title={PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUYGxbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper563/Authors"],"keywords":["Adversarial Examples","Generative Models","Purification","Hypothesis Testing"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}