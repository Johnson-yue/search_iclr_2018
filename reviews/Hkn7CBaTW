{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222733941,"tcdate":1511821019170,"number":3,"cdate":1511821019170,"id":"BJ711zqxG","invitation":"ICLR.cc/2018/Conference/-/Paper72/Official_Review","forum":"Hkn7CBaTW","replyto":"Hkn7CBaTW","signatures":["ICLR.cc/2018/Conference/Paper72/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting, but premature contribution on interpretability ","rating":"6: Marginally above acceptance threshold","review":"I found this paper an interesting read for two reasons: First, interpretability is an increasingly important problem as machine learning models grow more and more complicated. Second, the paper aims at generalization of previous work on confounded linear model interpretation in neuroimaging (the so-called filter versus patterns problem). The problem is relevant for discriminative problems: If the objective is really to visualize the generative process,  the \"filters\" learned by the discriminative process need to be transformed to correct for spatial correlated noise. \n\nGiven the focus on extracting visualization of the generative process, it would have been meaningful to place the discussion in a greater frame of generative model deep learning (VAEs, GANs etc etc). At present the \"state of the art\" discussion appears quite narrow, being confined to recent methods for visualization of discriminative deep models.\n\nThe authors convincingly demonstrate for the linear case, that their \"PatternNet\" mechanism can produce the generative process (i.e. discard spatially correlated \"distractors\"). The PatternNet is generalized to multi-layer ReLu networks by construction of node-specific pattern vectors and back-propagating these through the network. The \"proof\" (eqs. 4-6) is sketchy and involves uncontrolled approximations. The back-propagation mechanism is very briefly introduced and depicted in figure 1.\n\nYet, the results are rather convincing. Both the anecdotal/qualitative examples and the more quantitative patch elimination experiment figure 4a (?number missing) \n\nI do not understand the remark: \"However, our method has the advantage that it is not only applicable to image models but is a generalization of the theory commonly used in neuroimaging Haufe et al. (2014).\"  what ??\n\nOverall, I appreciate the general idea. However, the contribution could have been much stronger based on a detailed derivation with testable assumptions/approximations, and if based on a clear declaration of the aim.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222733985,"tcdate":1511816701870,"number":2,"cdate":1511816701870,"id":"ByUZ0g5lG","invitation":"ICLR.cc/2018/Conference/-/Paper72/Official_Review","forum":"Hkn7CBaTW","replyto":"Hkn7CBaTW","signatures":["ICLR.cc/2018/Conference/Paper72/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Important framework, tools, and criterion for understanding deep neural networks","rating":"9: Top 15% of accepted papers, strong accept","review":"summary of article: \nThis paper organizes existing methods for understanding and explaining deep neural networks into three categories based on what they reveal about a network: functions, signals, or attribution. “The function extracts the signal from the data by removing the distractor. The attribution of output values to input dimensions shows how much an individual component of the signal contributes to the output…” (p. 5). The authors propose a novel quality criterion for signal estimators, inspired by the analysis of linear models. They also propose two new explanatory methods, PatternNet (for signal estimation) and PatternAttribution (for relevance attribution), based on optimizing their new quality criterion. They present quantitative and qualitative analyses comparing PatternNet and PatternAttribution to several existing explanation methods on VGG-19.\n\n* Quality: The claims of the paper are well supported by quantitative results and qualitative visualizations. \n* Clarity: Overall the paper is clear and well organized. There are a few points that could benefit from clarification.\n* Originality: The paper puts forth an original framing of the problem of explaining deep neural networks. Related work is appropriately cited and compared. The authors's quality criterion for signal estimators allows them to do a quantitative analysis for a problem that is often hard to quantify.\n* Significance: This paper justifies PatternNet and PatternAttribution as good methods to explain predictions made by neural networks. These methods may now serve as an important tool for future work which may lead to new insights about how neural networks work. \n\nPros:\n* Helps to organize existing methods for understanding neural networks in terms of the types of descriptions they provide: functions, signals or attribution.\n* Creative quantitative analyses that evaluate their signal estimator at the level of single units and entire networks.\n\nCons:\n* Experiments consider only the pre-trained VGG-19 model trained on ImageNet. Results may not generalize to other architectures/datasets.\n* Limited visualizations are provided. \n\nComments:\n* Most of the paper is dedicated to explaining these signal estimators and quality criterion in case of a linear model. Only one paragraph is given to explain how they are used to estimate the signal at each layer in VGG-19. On first reading, there are some ambiguities about how the estimators scale up to deep networks. It would help to clarify if you included the expression for the two-component estimator and maybe your quality criterion for an arbitrary hidden unit. \n* The concept of signal is somewhat unclear. Is the signal \n    * (a) the part of the input image that led to a particular classification, as described in the introduction and suggested by the visualizations, in which case there is one signal per image for a given trained network?\n    *  (b) the part of the input that led to activation of a particular unit, as your unit wise signal estimators are applied, in which case there is one signal for every unit of a trained network? You might benefit from two terms to separate the unit-level signal (what caused the activation of a particular unit?) from the total signal (what caused all activations in this network?).\n* Assuming definition (b) I think the visualizations would be more convincing if you showed the signal for several output units. One would like to see that the signal estimation is doing more than separating foreground from background but is actually semantically specific. For instance, for the mailbox image, what does the signal look like if you propagate back from only the output unit for umbrella compared to the output unit for mailbox? \n* Do you have any intuition about why your two-component estimator doesn’t seem to be working as well in the convolutional layers? Do you think it is related to the fact that you are averaging within feature maps? Is it strictly necessary to do this averaging? Can you imagine a signal estimator more specifically designed for convolutional layers?\n\nMinor issues: \n* The label \"Figure 4\" is missing. Only subcaptions (a) and (b) are present.\n* Color scheme of figures: Why two oranges? It’s hard to see the difference.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222734029,"tcdate":1511798005713,"number":1,"cdate":1511798005713,"id":"Hk0lS3teG","invitation":"ICLR.cc/2018/Conference/-/Paper72/Official_Review","forum":"Hkn7CBaTW","replyto":"Hkn7CBaTW","signatures":["ICLR.cc/2018/Conference/Paper72/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very interesting and clear work","rating":"7: Good paper, accept","review":"The authors analyze show theoretical shortcomings in previous methods of explaining neural networks and propose an elegant way to remove these shortcomings in their methods PatternNet and PatternAttribution.\n\nThe quest of visualizing neural network decision is now a very active field with many contributions. The contribution made by the authors stands out due to its elegant combination of theoretical insights and improved performance in application. The work is very detailed and reads very well.\n\nI am missing at least one figure with comparison with more state-of-the-art methods (e.g. I would love to see results from the method by Zintgraf et al. 2017 which unlike all included prior methods seems to produce much crisper visualizations and also is very related because it learns from the data, too).\n\nMinor questions and comments:\n* Fig 3: Why is the random method so good at removing correlation from fc6? And the S_w even better? Something seems special about fc6.\n* Fig 4: Why is the identical estimator better than the weights estimator and that one better than S_a?\n* It would be nice to compare the image degradation experiment with using the ranking provided by the work from Zintgraf which should by definition function as a kind of gold standard\n* Figure 5, 4th row (mailbox): It looks like the umbrella significantly contributes to the network decision to classify the image as \"mailbox\" which doesn't make too much sense. Is is a problem of the visualization  (maybe there is next to no weight on the umbrella), of PatternAttribution or a strange but interesting a artifact of the analyzed network?\n* page 8 \"... closed form solutions (Eq (4) and Eq. (7))\" The first reference seems to be wrong. I guess Eq 4. should instead reference the unnumbered equation after Eq. 3.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509747092062,"tcdate":1509744010175,"number":3,"cdate":1509744010175,"id":"HyG5aL9C-","invitation":"ICLR.cc/2018/Conference/-/Paper72/Public_Comment","forum":"Hkn7CBaTW","replyto":"B12BY_ATZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"I don't think it does","comment":"In a linear model the gradient is constant, so the integrated gradients method produces the element-wise product of the gradient and the image (assuming an all-zero baseline).\n\nMy understanding is that the authors here make the point that the image is generally signal + distractors (see Fig. 2), and the correct attribution would be the element-wise product of the gradient and the signal (without distractors). Thus, you need to first estimate what's the signal, which is what the authors address here."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509329982250,"tcdate":1509329982250,"number":2,"cdate":1509329982250,"id":"rJUSnbE0b","invitation":"ICLR.cc/2018/Conference/-/Paper72/Public_Comment","forum":"Hkn7CBaTW","replyto":"Hkn7CBaTW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Baselines?","comment":"Why don't you baseline against prior work in your evaluation?\n\nThere's a big problem with clutter in this space, with lots of other approaches to doing this, even that have been published just this year. It's incumbent upon you to demonstrate that your method is better than the 10 other methods you cite, rather than tucking away comparisons into a one-paragraph section at the end of your results, which reads more like a related work section."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1508964676008,"tcdate":1508964676008,"number":1,"cdate":1508964676008,"id":"B12BY_ATZ","invitation":"ICLR.cc/2018/Conference/-/Paper72/Public_Comment","forum":"Hkn7CBaTW","replyto":"Hkn7CBaTW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Doesn't integrated gradients already do this?","comment":"Doesn't integrated gradients, which you cite (https://arxiv.org/abs/1703.01365, ICML 2017), produce the theoretically correct explanation for a linear model, and also prove that their method is the only method which can do so under pretty reasonable assumptions?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739502179,"tcdate":1508888100303,"number":72,"cdate":1509739499520,"id":"Hkn7CBaTW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hkn7CBaTW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning how to explain neural networks: PatternNet and PatternAttribution","abstract":"DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.\n","pdf":"/pdf/1f44f76251aa023e93c2b427145be62a12ff771e.pdf","TL;DR":"Without learning, it is impossible to explain a machine learning model's decisions.","paperhash":"anonymous|learning_how_to_explain_neural_networks_patternnet_and_patternattribution","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning how to explain neural networks: PatternNet and PatternAttribution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkn7CBaTW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper72/Authors"],"keywords":["machine learning","interpretability","deep learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}