{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222819780,"tcdate":1511933099936,"number":3,"cdate":1511933099936,"id":"ByV24asxM","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer2"],"readers":["everyone"],"content":{"title":"MCTS is promising, but should be evaluated in a more standard way","rating":"4: Ok but not good enough - rejection","review":"Monte-Carlo Tree Search is a reasonable and promising approach to hyperparameter optimization or algorithm configuration in search spaces that involve conditional structure.\n\nThis paper must acknowledge more explicitly that it is not the first to take a graph-search approach. The cited work related to SMAC and Hyperopt / TPE addresses this problem similarly. The technique of separating a description language from the optimization algorithm is also used in both of these projects / lines of research. The [mis-cited] paper titled “Making a science of model search …” is about using TPE to configure 1, 2, and 3 layer convnets for several datasets, including CIFAR-10. SMAC and Hyperopt have been used to search large search spaces involving pre-processing and classification algorithms (e.g. auto-sklearn, autoweka, hyperopt-sklearn). There have been near-annual workshops on AutoML and Bayesian optimization at NIPS and ICML (see e.g. automl.org).\n\nThere is a benchmark suite of hyperparameter optimization problems that would be a better way to evaluate MCTS as a hyperparameter optimization algorithm: http://www.ml4aad.org/automl/hpolib/","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1512222819823,"tcdate":1511515276341,"number":2,"cdate":1511515276341,"id":"Sy4q4vBgf","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose to automatically design and train deep architectures. ","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a DeepArchitect framework to build and train deep models automatically. Specifically, the authors proposes three components, i.e., model search specification language, model search algorithm, model evaluation algorithm. The paper is written well, and the proposed framework provides us with a systematical way to design deep models.\n\nHowever, my concern is mainly about its importance in practice. The experiments and computational modules are basic and small-scale, i.e., it may be restricted for large-scale computer vision problems. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1512222819865,"tcdate":1511292901250,"number":1,"cdate":1511292901250,"id":"Skp1e-zgM","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The novelty in this paper is beyond what is expected for a publication at ICLR. I recommend rejection.","rating":"4: Ok but not good enough - rejection","review":"The author present a language for expressing hyperparameters (HP) of a network. This language allows to define a tree structure search space to cover the case where some HP variable exists only if some previous HP variable took some specific value. Using this tool, they explore the depth of the network, when to apply batch-normalization, when to apply dropout and some optimization variables. They compare the search performance of random search, monte carlo tree search and a basic implementation of a Sequential Model Based Search. \n\nThe novelty in this paper is beyond what is expected for a publication at ICLR. I recommend rejection.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1510092385799,"tcdate":1509137221794,"number":926,"cdate":1510092362372,"id":"rkTBjG-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}