{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222709203,"tcdate":1511802101348,"number":3,"cdate":1511802101348,"id":"SyTgr6FlM","invitation":"ICLR.cc/2018/Conference/-/Paper652/Official_Review","forum":"HypNZW-Rb","replyto":"HypNZW-Rb","signatures":["ICLR.cc/2018/Conference/Paper652/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Are the MN the best architecture to tackle this task?","rating":"4: Ok but not good enough - rejection","review":"This article tackles the extraction of sentiments at a fine-grained level. Thus, the authors insist on context modeling to obtain a relevant analysis of a word's meaning. The authors propose a first modeling called ASC. Identifying some weakness in the formulation, the authors propose 5 solutions.\nThe authors apply their different models on a small dataset (semeval 2014), they compare basic memory network implementations with their approaches.\n\n==\n\nThe authors do not put into perspective their approach. Given the literature in topics / sentiment modeling, it is a real weakness of this article.\n\nTo improve readability, the authors should propose a diagram of the network, summarizing all notations\n\nDimension of c_i / o ? d\n\nDefinition of u (eq. 3) => v?\n\nAt the beginning of section 3, the discussion about the independence of the terms in the decomposition (6) is not completely relevant: alpha terms embedded the relation between the target and the context i\n\nAmong the different solutions, IT and CI are very redundant. A kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in M- : why not using those terms instead of ai <di,dt>?\nWe see that the authors build models that become more and more complex, but their motivation in combining attention and IT/CI is not clear: they learn the relation between context and target twice without any factorization.\n\nIn section 5, the authors mention briefly some works from the RNN domain & the classical use case of memory networks. They claim that:\n\"The above studies cannot be directly applied to the ASC task as they are not capable of mining finer-grained aspect dependent sentiments.\"\nI do not agree with them: latent representations from RNN are fine-grained & context dependent; latent representations from Socher et al. are also fine-grained & target dependent: the position in the latent space -modeling context- has an impact on the estimated sentiment.\n\nIn the experimental section, there is no discussion about the ration between the dataset size and the number of parameters to estimate. However: there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to \"lucky trial\".\nIt is true that the authors use strong regularization techniques (drop out, external knowledge of words embedding...). However, the validation procedure is not clear in the article.\n\nThe main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis: none of the state-of-the-art approaches are implemented here (RNN, basic models on W2V aggregations, ...). That makes  the contribution very difficult to evaluate.\n\nThe analysis of the results are interesting, both from the quantitative & qualitative point of view.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Target-Aware Memory Networks for Aspect Sentiment Classification","abstract":"Memory networks with external memories and the attention mechanism have been used in numerous applications and achieved superior results. Many recent studies have also made great efforts to capture more accurate attentions. In this paper, we focus on solving an important sentiment analysis problem, called aspect sentiment classification (ASC), which requires us to extend the traditional memory networks because in ASC, the classification can be dependent on the target aspect. For example, given two target aspects (which are product features) resolution and price, “higher resolution” and “lower price” express positive opinions while “higher price” and “lower resolution” express negative opinions. We observe that the same word higher (or lower) can indicate different sentiments for different target aspects. Traditional memory networks have difficulty in capturing such target-aware (dependent) sentiment signals. In this paper, we propose the target-aware memory networks to tackle the problem, which is able to model sentiment interaction between target aspects and the context words jointly with the attention mechanism. Our experimental results demonstrate its effectiveness.\n","pdf":"/pdf/684e67abed4ce8ac06385fa4beccf0de4aa2a647.pdf","paperhash":"anonymous|targetaware_memory_networks_for_aspect_sentiment_classification","_bibtex":"@article{\n  anonymous2018target-aware,\n  title={Target-Aware Memory Networks for Aspect Sentiment Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypNZW-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper652/Authors"],"keywords":["Target-Aware Memory Network","Target-Aware Sentiment","Memory Network","Aspect Sentiment Classification","Sentiment Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222709240,"tcdate":1511718240672,"number":2,"cdate":1511718240672,"id":"SkYvp_ueM","invitation":"ICLR.cc/2018/Conference/-/Paper652/Official_Review","forum":"HypNZW-Rb","replyto":"HypNZW-Rb","signatures":["ICLR.cc/2018/Conference/Paper652/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of \"Target-Aware Memory Networks for Aspect Sentiment Classification\"","rating":"5: Marginally below acceptance threshold","review":"This paper considers the task of aspect sentiment classification, which entails categorizing texts with respect to the sentiment expressed concerning particular aspects (e.g., television resolution or price). Toward this end, the authors adopt a memory-network based approach. The idea is to explicitly model interactions between aspects and words expressing sentiment about them. This is achieved via an attention mechanism. \n\nOverall, this paper does seem to identify a concrete problem, and I liked the use of explicit aspect embeddings for sentiment analysis. However, the contribution is relatively minor here. Furthermore, the approaches did not seem particularly well motivated in my view; inconsistent and seemingly erroneous notation in places complicate the picture. Moreover, the experimental evaluation is small, considering only two datasets. My feeling is that this work is a bit preliminary at present, but could be expanded into a nice contribution eventually. \n\nSpecific comments\n---\n- I had some trouble following the notation in places, and I think this is due to a bit of sloppiness. Specifically:\n\n    1. Target aspect vectors live in R^V, but it's not clear to me what V represents; this is the number of distinct aspects? This should be clarified, ideally with concrete examples. \n\n    2. Are the x_1 ... x_n here word embeddings or one-hot encodings? I believe the latter, but this implies that the vocabulary dimension (V) is the same as the number of aspects, since A is apparently shared. This seems a bit surprising, or at least would seem to warrant further explanation. In Eq. 2, below, it actually seems words are embedded separately via C, although again the dimensions are not provided. Regardless, more discussion regarding what the Ax_i embeddings are meant to capture (in contrast to the Cx_i vectors) would be appreciated. \n\n    3. What is u is equation 3? As far as I can tell, this term is undefined. In Equation 5 this is mysteriously replaced with v_t, which is the target aspect embedding and so may have been the intention for u all along? \n\n    4. In Equation 6, on the RHS in the expansion, the \\alpha_1 W c_i should be \\alpha_1 W c_1. The mistake is repeated for the following two terms. \n\n- Equations 3 and 4 suggest that despite their ordinal structure, sentiment labels are treated as unstructured at predict time. This seems like a missed opportunity to capitalize on structure to bias predictions (neutral sentiment is closer to positive than is negative, after all, but the model does not know this as currently specified).\n\n- The authors write \"\\alpha_i W c_i does not explicitly depend on the target word t.\" I'm not sure I follow though, because the alpha terms do indeed depend on the word $t$, as per equation (1), which includes v_t, a vector representation of the target aspect. I think what the authors intend to note here is that the W parameters are independent of this, or as expounded upon in the concrete example that follows, that context words and weights W both are. In any case, this statement should be clarified. \n\n- It would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of Section 3 would be to add an additional layer, which would facilitate interactions between the attention-weighted word embedding (\\alpha_i c_i) and aspect embedding (v_t). However, the authors do not seem to have considered this straight-forward approach. Why? \n\n- Surely \"d_t = Dt\" should be \"d_t = D v_t\" in the \"Interaction Term (IT)\" subsection? \n\n- The authors write \"diWdt generates slightly better results and we adopt it in our experiments.\" -- please clarify; better results on a development set, I hope? \n\n- The evaluation is small. The authors use only two datasets comprising only a few thousand data points (and hence the test sets comprise 500-1000 instances). It is therefore hard to draw anything conclusive from these results, especially given how many moving parts there are here (from model initializations to training procedures). Regardless, the authors should report the micro-F1 in addition to the macro-F1. \n\nSmaller comments\n---\n- As a stylistic thing, I would suggest not pluralizing \"attention\" (i.e., remove \"Attentions\"). \n\n- In Eq 3, please be explicit as to whether the '+' is here a concatentation or an actual (element-wise) sum? Eq. 5 clarifies this implicitly, but would be good to state outright. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Target-Aware Memory Networks for Aspect Sentiment Classification","abstract":"Memory networks with external memories and the attention mechanism have been used in numerous applications and achieved superior results. Many recent studies have also made great efforts to capture more accurate attentions. In this paper, we focus on solving an important sentiment analysis problem, called aspect sentiment classification (ASC), which requires us to extend the traditional memory networks because in ASC, the classification can be dependent on the target aspect. For example, given two target aspects (which are product features) resolution and price, “higher resolution” and “lower price” express positive opinions while “higher price” and “lower resolution” express negative opinions. We observe that the same word higher (or lower) can indicate different sentiments for different target aspects. Traditional memory networks have difficulty in capturing such target-aware (dependent) sentiment signals. In this paper, we propose the target-aware memory networks to tackle the problem, which is able to model sentiment interaction between target aspects and the context words jointly with the attention mechanism. Our experimental results demonstrate its effectiveness.\n","pdf":"/pdf/684e67abed4ce8ac06385fa4beccf0de4aa2a647.pdf","paperhash":"anonymous|targetaware_memory_networks_for_aspect_sentiment_classification","_bibtex":"@article{\n  anonymous2018target-aware,\n  title={Target-Aware Memory Networks for Aspect Sentiment Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypNZW-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper652/Authors"],"keywords":["Target-Aware Memory Network","Target-Aware Sentiment","Memory Network","Aspect Sentiment Classification","Sentiment Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222709278,"tcdate":1511707662779,"number":1,"cdate":1511707662779,"id":"ByDzN8dxG","invitation":"ICLR.cc/2018/Conference/-/Paper652/Official_Review","forum":"HypNZW-Rb","replyto":"HypNZW-Rb","signatures":["ICLR.cc/2018/Conference/Paper652/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Motivation is clear, but the proposed method is trivial, and the experimental results are not convincing","rating":"4: Ok but not good enough - rejection","review":"Overall strength:\nIn this paper, the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions. This work has a well-established motivation: traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predictions. To solve this problem, the authors proposed five formulations in the final prediction layer. The illustration about the problem is clear, as well as the explanation for the formulations.\n\nMajor concerns:\n1.\tThis work brings some modifications to the prediction layer, which is a bit trivial. Although the effect has been shown, the model is too specific to a narrow area, and is not general to be applied in a broad sense. It could have more contribution if the authors model the interactions within the attention model itself, instead of a simple prediction layer, which is problem-dependent.\n2.\tThe experiments are insufficient to show the effectiveness. It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4. Only two examples are not convincing.\n3.\tIn section 3, the authors claimed that (5) models the target and context independently. However, in section 4, in (7), the authors claimed the target vector v_t will affect the context shifting their representation to c’_i. This should also work for (5). \n4.\tThere are too many typos in the paper, e.g., \\alpha is replaced by a, etc.\n\nOther concerns:\n1.\tIt seems that one needs to train at least three embedding matrices: A, C, D which represent input embeddings, output embeddings, and interactive embeddings, respectively. I wonder if this brings redundant parameters that do not guarantee convergence. Why not use one matrix instead? Did the authors try experiments with less embedding matrices?\n2.\tThere is another work that also considers the target-context interaction using interactive attention model. Please refer to this paper “Interactive Attention Networks for Aspect-Level Sentiment Classification”. A comparison is needed.\n3.\tIt is better to provide results in terms of accuracy for both datasets, as previous methods usually use accuracy for comparison. How’s the score of the proposed model compared with the above paper as well as [Tang et al. 2016]?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Target-Aware Memory Networks for Aspect Sentiment Classification","abstract":"Memory networks with external memories and the attention mechanism have been used in numerous applications and achieved superior results. Many recent studies have also made great efforts to capture more accurate attentions. In this paper, we focus on solving an important sentiment analysis problem, called aspect sentiment classification (ASC), which requires us to extend the traditional memory networks because in ASC, the classification can be dependent on the target aspect. For example, given two target aspects (which are product features) resolution and price, “higher resolution” and “lower price” express positive opinions while “higher price” and “lower resolution” express negative opinions. We observe that the same word higher (or lower) can indicate different sentiments for different target aspects. Traditional memory networks have difficulty in capturing such target-aware (dependent) sentiment signals. In this paper, we propose the target-aware memory networks to tackle the problem, which is able to model sentiment interaction between target aspects and the context words jointly with the attention mechanism. Our experimental results demonstrate its effectiveness.\n","pdf":"/pdf/684e67abed4ce8ac06385fa4beccf0de4aa2a647.pdf","paperhash":"anonymous|targetaware_memory_networks_for_aspect_sentiment_classification","_bibtex":"@article{\n  anonymous2018target-aware,\n  title={Target-Aware Memory Networks for Aspect Sentiment Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypNZW-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper652/Authors"],"keywords":["Target-Aware Memory Network","Target-Aware Sentiment","Memory Network","Aspect Sentiment Classification","Sentiment Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1509739179750,"tcdate":1509130549158,"number":652,"cdate":1509739177083,"id":"HypNZW-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HypNZW-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Target-Aware Memory Networks for Aspect Sentiment Classification","abstract":"Memory networks with external memories and the attention mechanism have been used in numerous applications and achieved superior results. Many recent studies have also made great efforts to capture more accurate attentions. In this paper, we focus on solving an important sentiment analysis problem, called aspect sentiment classification (ASC), which requires us to extend the traditional memory networks because in ASC, the classification can be dependent on the target aspect. For example, given two target aspects (which are product features) resolution and price, “higher resolution” and “lower price” express positive opinions while “higher price” and “lower resolution” express negative opinions. We observe that the same word higher (or lower) can indicate different sentiments for different target aspects. Traditional memory networks have difficulty in capturing such target-aware (dependent) sentiment signals. In this paper, we propose the target-aware memory networks to tackle the problem, which is able to model sentiment interaction between target aspects and the context words jointly with the attention mechanism. Our experimental results demonstrate its effectiveness.\n","pdf":"/pdf/684e67abed4ce8ac06385fa4beccf0de4aa2a647.pdf","paperhash":"anonymous|targetaware_memory_networks_for_aspect_sentiment_classification","_bibtex":"@article{\n  anonymous2018target-aware,\n  title={Target-Aware Memory Networks for Aspect Sentiment Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypNZW-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper652/Authors"],"keywords":["Target-Aware Memory Network","Target-Aware Sentiment","Memory Network","Aspect Sentiment Classification","Sentiment Analysis"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}