{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222718364,"tcdate":1511986808127,"number":3,"cdate":1511986808127,"id":"HkkF8qngG","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Attention masks for diagnosing neural nets","rating":"7: Good paper, accept","review":"The paper presents the formulation of Latent Attention Masks, which is a framework for understanding the importance of input structure in neural networks. The framework takes a pre-trained network F as target of the analysis, and trains another network A that generates masks for inputs. The goal of these masks is to remove parts of the input without changing the response of F. Generated masks are helpful to interpret the preferred patterns of neural networks as well as diagnose modes of error.\n\nThe paper is very well motivated and the formulation and experiments are well presented too. The experiments are conducted in small benchmarks and using simple fully connected networks. It would be interesting to report and discuss convergence properties of the proposed framework. Also, insights of what are the foreseeable challenges on scaling up the framework to real world scenarios.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/81308288f62953f9495ba27f0a725de11fa1d8d1.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1512222718403,"tcdate":1511899075938,"number":2,"cdate":1511899075938,"id":"Sk3pyHoez","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The authors of this paper proposed a data-driven black-box visualization scheme. The paper primarily focuses on neural network models in the experiment section. The proposed method iteratively optimize learnable masks for each training example to find the most relevant content in the input that was \"attended\" by the neural network.  The authors empirically demonstrated their method on image and text classification tasks. \n\nStrength:\n           - The paper is well-written and easy to follow. \n           - The qualitative analysis of the experimental results nicely illustrated how the learnt latent attention masks match with our intuition about how neural networks make its classification predictions.\n\n        Weakness:\n           - Most of the experiments in the paper are performed on small neural networks and simple datesets. I found the method will be more compiling if the authors can show visualization results on ImageNet models. Besides simple object recognition tasks, other more interesting tasks to test out the proposed visualization method are object detection models like end-to-end fast R-CNN, video classification models, and image-captioning models. Overall, the current set of experiments are limited to showcase the effectiveness of the proposed method.\n           - It is unclear how the hyperparameter is chosen for the proposed method. How does the \\beta affect the visualization quality? It would be great to show a range of samples from high to low beta values. Does it require tuning for different visualization samples? Does it vary over different datasets?\n  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/81308288f62953f9495ba27f0a725de11fa1d8d1.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1512222718448,"tcdate":1511840388443,"number":1,"cdate":1511840388443,"id":"Hy3t5U5gz","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper presented a general method for visualizing an arbitrary neural network's inner mechanisms.","rating":"4: Ok but not good enough - rejection","review":"The main contribution of the paper is to propose to learn a Latent Attention Network (LAN) that can help to visualize the inner structure of a deep neural network. To this end, the paper propose a novel training objective that can learn to tell the importance of each dimension of input. It is very interesting. However, one question is what is the potential usage of the model? Since the model need to train an another network to visualize the structure of a trained neural network, it is expensive, and I don't think the model can help use to design a better structure (at least the experiments did not show this point). And maybe different structures of LAN will produce different understanding of the trained model. Hence people are still not sure what kind of structure is the most helpful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/81308288f62953f9495ba27f0a725de11fa1d8d1.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1509739166518,"tcdate":1509131733416,"number":676,"cdate":1509739163859,"id":"SJ60SbW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/81308288f62953f9495ba27f0a725de11fa1d8d1.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}