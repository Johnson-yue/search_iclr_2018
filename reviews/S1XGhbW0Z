{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222732905,"tcdate":1511882876763,"number":3,"cdate":1511882876763,"id":"ByrFebjgM","invitation":"ICLR.cc/2018/Conference/-/Paper718/Official_Review","forum":"S1XGhbW0Z","replyto":"S1XGhbW0Z","signatures":["ICLR.cc/2018/Conference/Paper718/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting idea but experiments are weak.","rating":"5: Marginally below acceptance threshold","review":"This paper presents an active learning method for deep models. The method queries consecutive points using farthest-first traversals in the space of neural activation over a representation layer.\n\nEmploying core-set for active selection is interesting and reasonable. The idea is clearly presented.\n\nThe method assumes that a deep model has already been initially trained with reasonable accuracy. However, such a reasonable model usually requires a large set of training examples, which is less practical in the case of active learning.\n\nAuthors performed experiments on MNIST, CIFAR10 and CIFAR100. The results are quite mixed. I can hardly observe significant superiority of the proposed method from the figures. Given that the baseline (uncertainty sampling) is not a strong one, I have doubt on the effectiveness of the proposed method.\n\nAuthors are suggested to perform more experiments on various datasets and compare to more state-of-the-art active learning methods.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning over the Long Tail","abstract":"This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, Cifar-10, and Cifar-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.","pdf":"/pdf/0f64fbf40277a1f145ca8637b984db73e07c23b2.pdf","paperhash":"anonymous|deep_active_learning_over_the_long_tail","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning over the Long Tail},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XGhbW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper718/Authors"],"keywords":["Active Learning","Deep Learning","Coreset","Deep Representation","Compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222732946,"tcdate":1511762949309,"number":2,"cdate":1511762949309,"id":"Sy6WnmFef","invitation":"ICLR.cc/2018/Conference/-/Paper718/Official_Review","forum":"S1XGhbW0Z","replyto":"S1XGhbW0Z","signatures":["ICLR.cc/2018/Conference/Paper718/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Deep active learning","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a novel active learning method for deep neural networks. A seed subset is randomly selected from the whole set at the beginning, and then additional batches of training data are consecutively selected by using the farthest-first algorithm. The distances between data points are measured using the outputs in the top representation layer of neural networks. Experiments on MNIST, CIFAR-10 and CIFAR-100 show improvements of the proposed method over softmax response and random method.\n\nThe research of efficient active learning algorithms for deep learning models is important and valuable. However, this paper is in a draft version. The algorithm is simple but the experiments are not convincing. The competing algorithms are too limited. The experiments does not run long enough to see the performance of all competing algorithm till they obtain similar performance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning over the Long Tail","abstract":"This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, Cifar-10, and Cifar-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.","pdf":"/pdf/0f64fbf40277a1f145ca8637b984db73e07c23b2.pdf","paperhash":"anonymous|deep_active_learning_over_the_long_tail","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning over the Long Tail},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XGhbW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper718/Authors"],"keywords":["Active Learning","Deep Learning","Coreset","Deep Representation","Compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222732997,"tcdate":1510179746137,"number":1,"cdate":1510179746137,"id":"ByqiXbbyf","invitation":"ICLR.cc/2018/Conference/-/Paper718/Official_Review","forum":"S1XGhbW0Z","replyto":"S1XGhbW0Z","signatures":["ICLR.cc/2018/Conference/Paper718/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The novelty of this paper is not enough","rating":"4: Ok but not good enough - rejection","review":"This paper introduces the coreset sampling idea for active learning of neural networks. The idea of coreset comes from geometric algorithms, and heuristically produce representative samples. The paper focuses on the long-tail setting, where we start with a set of random examples, and apply active learning on top of it. Experiments show that the proposed algorithm is favorable compared to uncertainty sampling and random sampling.\n\nClarity: The paper is mostly well-written. Some comments:\n1. I think the connection between Section 2 and Section 3 is a bit weak - Algorithm 1 collects examples such that each class share the same portion. However, if the labels are hidden (as in Algorithm 2), then this property is not perserved.\n\n2. In section 4.4, the problem setup is favorable to the FF algorithm, hence I think it is inappropriate to present. The reason is, FF automatically ignores repeating unlabeled examples, while other algorithms seem not. \n\nQuality: I think the comparison done in this paper is not extensive enough. Although the focus of this paper is on the long tail setting, why the setting is interesting is questionable. Is it because active learning algorithm, if start from scratch, will fall into local optima? It would be great if the paper can provide comparisons between this model and the original active learning model.\n\nOriginality: This is the part I am most concerned about. The paper (Baram et al. 2004) has already proposed the furthest-first traversal heuristic for active learning. I think this makes Section 2 less valuable. \n\nThe example in Section 5 is useful for illustration purposes, although I think the idea has already appeared in (Baram et al. 2004).\n\nSignificance: Although I think using representative sampling is a good idea, I think the contribution of this paper is limited for publication.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning over the Long Tail","abstract":"This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, Cifar-10, and Cifar-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.","pdf":"/pdf/0f64fbf40277a1f145ca8637b984db73e07c23b2.pdf","paperhash":"anonymous|deep_active_learning_over_the_long_tail","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning over the Long Tail},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XGhbW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper718/Authors"],"keywords":["Active Learning","Deep Learning","Coreset","Deep Representation","Compression"]}},{"tddate":null,"ddate":null,"tmdate":1509578947926,"tcdate":1509578947926,"number":1,"cdate":1509578947926,"id":"BJnT_0vCW","invitation":"ICLR.cc/2018/Conference/-/Paper718/Public_Comment","forum":"S1XGhbW0Z","replyto":"S1XGhbW0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Nice paper!","comment":"I really enjoyed reading this paper :)\n\nGiven a reasonable model, AL requires much less data (which is expected) to achieve high performance. In order for AL to be really practical in DL (or applied ML), I would like to ask:\n\nDoes this paper provide any wisdom on whether AL can be combined with hyper-parameter search? Most NNs are trained on massive dataset, and hyper-parameter search takes a LONG time. The paper seems to take a \"reasonable\" model from the hyper-parameter search done by passive learning, which does not save people much time in terms of model search..."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning over the Long Tail","abstract":"This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, Cifar-10, and Cifar-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.","pdf":"/pdf/0f64fbf40277a1f145ca8637b984db73e07c23b2.pdf","paperhash":"anonymous|deep_active_learning_over_the_long_tail","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning over the Long Tail},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XGhbW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper718/Authors"],"keywords":["Active Learning","Deep Learning","Coreset","Deep Representation","Compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739143315,"tcdate":1509133322794,"number":718,"cdate":1509739140658,"id":"S1XGhbW0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1XGhbW0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Active Learning over the Long Tail","abstract":"This paper is concerned with pool-based active learning for deep neural networks. Motivated by coreset dataset compression ideas, we present a novel active learning algorithm that queries consecutive points from the pool using farthest-first traversals in the space of neural activation over a representation layer. We show consistent and overwhelming improvement in sample complexity over passive learning (random sampling) for three datasets: MNIST, Cifar-10, and Cifar-100. In addition, our algorithm outperforms the traditional uncertainty sampling technique (obtained using softmax activations), and we identify cases where uncertainty sampling is only slightly better than random sampling.","pdf":"/pdf/0f64fbf40277a1f145ca8637b984db73e07c23b2.pdf","paperhash":"anonymous|deep_active_learning_over_the_long_tail","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning over the Long Tail},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XGhbW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper718/Authors"],"keywords":["Active Learning","Deep Learning","Coreset","Deep Representation","Compression"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}