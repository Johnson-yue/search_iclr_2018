{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222683460,"tcdate":1511931795050,"number":3,"cdate":1511931795050,"id":"Sks9k6olz","invitation":"ICLR.cc/2018/Conference/-/Paper550/Official_Review","forum":"HJLPel-CW","replyto":"HJLPel-CW","signatures":["ICLR.cc/2018/Conference/Paper550/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Language style transfer using cycle-consistency loss","rating":"5: Marginally below acceptance threshold","review":"This paper is an extension of the recent language style transfer method without parallel training pairs (Shen et al. 2017). The author extend the  original algorithm from two-folds:\n\n1. A style discrepancy loss that pushes the style vector of the sentences away from target style\n2. A cycle consistency loss that makes sure the content vector of transferred sentences and style vector of the original sentence should be able to reconstruct the original sentences. \n\nBoth of the extensions are reasonable to me. The experiments further verify the performance gain compared against the baseline. \n\n\nI have several concerns:\n\nMetrics are not very reasonable to me: \n- It does not measure how well the content is preserved. Style transfer has two key components, the first is how well it is transferred to the target style; second is how well it preserves the original contents. However, the metric that using a pre-trained style classification network has issues in terms of evaluating how well the original content is preserved. In fact, from the qualitative examples of Yelp, I do not think that the contents are well preserved. Also it is not clear how good the classifier is. \n- It seems there are only two styles in Yelp dataset, and it's not clear how many styles are there in the \"On Chat\" dataset. \n\nThe paper does not clearly describe how D_s is obtained. Also in the formulation D_S is not optimised. I am wondering how D_S is trained, how the GT labels are obtained and whether it is trained jointly. Moreover, is this D_S same as the style classifier used in the metric? These are the crucial questions related with fair comparisons, which I would like to know specific answers to make my final decisions. \n\nMy final decision is dependent on the author's response to my questions. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Language Style Transfer from Non-Parallel Text with Arbitrary Styles","abstract":"Language style transfer is the problem of migrating the content of a source sentence to a target style. In many applications, parallel training data are not available and source sentences to be transferred may have arbitrary and unknown styles. In this paper, we present an encoder-decoder framework under this problem setting. Each sentence is encoded into its content and style latent representations. By recombining the content with the target style, we can decode a sentence aligned in the target domain. To adequately constrain the encoding and decoding functions, we couple them with two loss functions. The first is a style discrepancy loss, enforcing that the style representation accurately encodes the style information guided by the discrepancy between the sentence style and the target style. The second is a cycle consistency loss, which ensures that the transferred sentence should preserve the content of the original sentence disentangled from its style. We validate the effectiveness of our proposed model on two tasks: sentiment modification of restaurant reviews, and dialog response revision with a romantic style.","pdf":"/pdf/d8b9efe839a46b06a7cf4fa2ada38499ef077a70.pdf","TL;DR":"We present an encoder-decoder framework for language style transfer, which allows for the use of non-parallel data and source data with various unknown language styles.","paperhash":"anonymous|language_style_transfer_from_nonparallel_text_with_arbitrary_styles","_bibtex":"@article{\n  anonymous2018language,\n  title={Language Style Transfer from Non-Parallel Text with Arbitrary Styles},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJLPel-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper550/Authors"],"keywords":["style transfer","text generation","non-parallel data"]}},{"tddate":null,"ddate":null,"tmdate":1512222683500,"tcdate":1511810486192,"number":2,"cdate":1511810486192,"id":"S10nr1cgf","invitation":"ICLR.cc/2018/Conference/-/Paper550/Official_Review","forum":"HJLPel-CW","replyto":"HJLPel-CW","signatures":["ICLR.cc/2018/Conference/Paper550/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well written, additive contribution, and good results.","rating":"7: Good paper, accept","review":"This paper introduces two new loss functions which can be used along with the existing reconstruction and adversarial losses for language style transfer. The first one is a style discrepancy loss to enforce that the discrepancy between the learnt style representation for a source sentence and the target style representation is consistent with a pre-trained discriminator. The second one is a cycle consistency loss to make sure that the the original sentence can be recovered from the content of the transferred sentence and the style of the original sentence. The proposed method does not assume that the source sentences have only one style and allows them to have unknown styles.\n\nGenerally speaking, this paper is well written and easy to follow. The ideas are straightforward and make sense given the current trends in the field. This paper does not bring completely new perspectives for the task, but the contribution is additive to the community. \n\nThe experimental results show the effectiveness of the approach. It can be trained with source sentences having various styles and it can produce sentences in a different style without changing the content much. These are pros of the approach.\n\nThe cons of the approach may be the fact that it needs a pre-trained classifier for the style discrepancy loss. If it can be integrated into the training end-to-end, it might be better.\n\nThe experimental results show the results without the cyclic loss. The experimental results could also include results without style discrepancy loss. \n\nIt looks that this paper denotes the style of the target domain as y^* and assume that it is shared by all samples in X_2. However, y^* is also estimated by the encoder as shown in Eq. (2) and it varies depending on the input to the encoder. It is not very clear to me how y^* is chosen in Section 3.3. Please make that part clear.\n\nPlease explain the reason why STB is better than the proposed method with 100k positive samples.\n\nThe difference between STB and the proposed method is not clear without reading the reference. The algoroithm of STB should be briefly explained in Section 4.2.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Language Style Transfer from Non-Parallel Text with Arbitrary Styles","abstract":"Language style transfer is the problem of migrating the content of a source sentence to a target style. In many applications, parallel training data are not available and source sentences to be transferred may have arbitrary and unknown styles. In this paper, we present an encoder-decoder framework under this problem setting. Each sentence is encoded into its content and style latent representations. By recombining the content with the target style, we can decode a sentence aligned in the target domain. To adequately constrain the encoding and decoding functions, we couple them with two loss functions. The first is a style discrepancy loss, enforcing that the style representation accurately encodes the style information guided by the discrepancy between the sentence style and the target style. The second is a cycle consistency loss, which ensures that the transferred sentence should preserve the content of the original sentence disentangled from its style. We validate the effectiveness of our proposed model on two tasks: sentiment modification of restaurant reviews, and dialog response revision with a romantic style.","pdf":"/pdf/d8b9efe839a46b06a7cf4fa2ada38499ef077a70.pdf","TL;DR":"We present an encoder-decoder framework for language style transfer, which allows for the use of non-parallel data and source data with various unknown language styles.","paperhash":"anonymous|language_style_transfer_from_nonparallel_text_with_arbitrary_styles","_bibtex":"@article{\n  anonymous2018language,\n  title={Language Style Transfer from Non-Parallel Text with Arbitrary Styles},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJLPel-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper550/Authors"],"keywords":["style transfer","text generation","non-parallel data"]}},{"tddate":null,"ddate":null,"tmdate":1512222683538,"tcdate":1511760766556,"number":1,"cdate":1511760766556,"id":"rJ8FQQYxf","invitation":"ICLR.cc/2018/Conference/-/Paper550/Official_Review","forum":"HJLPel-CW","replyto":"HJLPel-CW","signatures":["ICLR.cc/2018/Conference/Paper550/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A borderline paper","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors proposed a method to transfer the text style to a specific target style. To this end, the authors combined a text reconstruction loss, an adversarial decoding loss, a cyclic consistency loss and a style discrepancy loss. The method seems solid, and the writing is pretty good. But there are a few key issues that are not clearly addressed and the experimental results are not convincing.\n\nPros:\nThis method combines the contributions of a few previous works, and obtains a stronger and more general model. Especially it borrows the cyclic loss from the image style transfer, which provides a reasonable regularization to the text style transfer model. \n\nCons:\nThere are a few key technical issues that are not clearly addressed. Hence I'm not fully convinced that this model indeed works as claimed.\n1. Why use CNN for the style representation layer? CNN is known to be usually unable to capture long-range correlations in natural language (unless enhanced with attentions). Are long-range correlations irrelevant to the text style?\n2. The authors made an essential assumption that all target samples have the same style embedding y*. But seems none of the 4 loss functions incorporates this constraint. If y* is simply fixed somewhere in the model, then I'm worried that it may cause mode collapse (i.e. the encoder always output similar values), and one possible bad consequence is that y1_i of different sentences in the source domain may have very similar values. \n3. The experiments are toyish and not convincing. The given examples seem to exhibit certain kind of mode collapse, i.e. different examples have similar wording from a very limited vocabulary. It is possible that the generator just learned to overfit the sentiment classifier, so that the classifier thought the transferred sentences have the desired sentiment, but the transferred sentences may lack variations and hence lacks practical use. It would be convincing if the authors could transfer an English paragraph into the style of a certain author, such as Shakespeare, which can be easily evaluated by a human instead of a trained classifier.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Language Style Transfer from Non-Parallel Text with Arbitrary Styles","abstract":"Language style transfer is the problem of migrating the content of a source sentence to a target style. In many applications, parallel training data are not available and source sentences to be transferred may have arbitrary and unknown styles. In this paper, we present an encoder-decoder framework under this problem setting. Each sentence is encoded into its content and style latent representations. By recombining the content with the target style, we can decode a sentence aligned in the target domain. To adequately constrain the encoding and decoding functions, we couple them with two loss functions. The first is a style discrepancy loss, enforcing that the style representation accurately encodes the style information guided by the discrepancy between the sentence style and the target style. The second is a cycle consistency loss, which ensures that the transferred sentence should preserve the content of the original sentence disentangled from its style. We validate the effectiveness of our proposed model on two tasks: sentiment modification of restaurant reviews, and dialog response revision with a romantic style.","pdf":"/pdf/d8b9efe839a46b06a7cf4fa2ada38499ef077a70.pdf","TL;DR":"We present an encoder-decoder framework for language style transfer, which allows for the use of non-parallel data and source data with various unknown language styles.","paperhash":"anonymous|language_style_transfer_from_nonparallel_text_with_arbitrary_styles","_bibtex":"@article{\n  anonymous2018language,\n  title={Language Style Transfer from Non-Parallel Text with Arbitrary Styles},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJLPel-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper550/Authors"],"keywords":["style transfer","text generation","non-parallel data"]}},{"tddate":null,"ddate":null,"tmdate":1509739241270,"tcdate":1509126237976,"number":550,"cdate":1509739238611,"id":"HJLPel-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJLPel-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Language Style Transfer from Non-Parallel Text with Arbitrary Styles","abstract":"Language style transfer is the problem of migrating the content of a source sentence to a target style. In many applications, parallel training data are not available and source sentences to be transferred may have arbitrary and unknown styles. In this paper, we present an encoder-decoder framework under this problem setting. Each sentence is encoded into its content and style latent representations. By recombining the content with the target style, we can decode a sentence aligned in the target domain. To adequately constrain the encoding and decoding functions, we couple them with two loss functions. The first is a style discrepancy loss, enforcing that the style representation accurately encodes the style information guided by the discrepancy between the sentence style and the target style. The second is a cycle consistency loss, which ensures that the transferred sentence should preserve the content of the original sentence disentangled from its style. We validate the effectiveness of our proposed model on two tasks: sentiment modification of restaurant reviews, and dialog response revision with a romantic style.","pdf":"/pdf/d8b9efe839a46b06a7cf4fa2ada38499ef077a70.pdf","TL;DR":"We present an encoder-decoder framework for language style transfer, which allows for the use of non-parallel data and source data with various unknown language styles.","paperhash":"anonymous|language_style_transfer_from_nonparallel_text_with_arbitrary_styles","_bibtex":"@article{\n  anonymous2018language,\n  title={Language Style Transfer from Non-Parallel Text with Arbitrary Styles},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJLPel-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper550/Authors"],"keywords":["style transfer","text generation","non-parallel data"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}