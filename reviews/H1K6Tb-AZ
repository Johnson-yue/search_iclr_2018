{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222737755,"tcdate":1512157628434,"number":3,"cdate":1512157628434,"id":"r1VT-4J-z","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A potentially useful method, but not well motivated or explained","rating":"4: Ok but not good enough - rejection","review":"The authors propose a method for reducing the computational burden when performing inference in deep neural networks. The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients. The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product. \n\nUnfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not. While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure.\n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets.\n\nNegative points:\n- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches.\n- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task.\n\nMinor point:\n- Figure 1 is unclear and requires a better caption. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP used hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that achieves 94.7% accuracy on average, and especially outperforms the original IDP by 32% when only 10% of dot products we used. We further introduce trainable profile coefficients, with which TESLA further improves the accuracy to 95.5% without specifying coefficients in advance. We also apply TESLA to the VGG-16 model on CIFAR-10 and it achieves 80% accuracy using only 20% of dot products, where the original IDP does not work. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/bd6b4dc3c6962b705863b934a22da7857dd65171.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222737790,"tcdate":1511982967229,"number":2,"cdate":1511982967229,"id":"rJyYwFhlz","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer3"],"readers":["everyone"],"content":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference ","rating":"5: Marginally below acceptance threshold","review":"This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints. The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized. These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity. \n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities. This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting. They also present results for a 'harmonic' combination which was not explained in the paper at all.\n\nOverall the paper has very good motivation and significance. \nHowever the writing is not very clear and the paper is not self-contained at all. I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting. \n\nAdditionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3. As is, while results are promising, I can't fully assess that the paper has major contributions.  ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP used hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that achieves 94.7% accuracy on average, and especially outperforms the original IDP by 32% when only 10% of dot products we used. We further introduce trainable profile coefficients, with which TESLA further improves the accuracy to 95.5% without specifying coefficients in advance. We also apply TESLA to the VGG-16 model on CIFAR-10 and it achieves 80% accuracy using only 20% of dot products, where the original IDP does not work. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/bd6b4dc3c6962b705863b934a22da7857dd65171.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222737835,"tcdate":1511755472701,"number":1,"cdate":1511755472701,"id":"SJF0AbKgG","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated.\n\nThe approach is based on `profile coefficientsâ€™ which are learned for every channel in a convolution layer, or for every column in the fully connected layer. Based on the magnitude of this profile coefficient, which determines the importance of this `filter,â€™ individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner.\n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesnâ€™t fully utilize the deep net performance. To address this issue a `loss aggregationâ€™ is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used.\n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime.\n\nSummary:\nâ€”â€”\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method. The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing. In addition, the writing should be improved as it is often ambiguous. See below for details.\n\nReview:\nâ€”â€”â€”â€”â€”\n1. Experiments are only provided on very small datasets. According to my opinion, this isnâ€™t sufficient to illustrate the effectiveness of the proposed approach. As a reader I wouldnâ€™t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2. Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent. More specifically, while the profile coefficient is applied for every input element in Eq. (1), itâ€™s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layerâ€™ is more confusing than clarifying.\n\n3. The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot productsâ€™. While sentences like those can be deciphered, they arenâ€™t that appealing.\n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc.\n\n5. Algorithm 1 and Algorithm 2 call functions that arenâ€™t described/defined.\n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP used hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that achieves 94.7% accuracy on average, and especially outperforms the original IDP by 32% when only 10% of dot products we used. We further introduce trainable profile coefficients, with which TESLA further improves the accuracy to 95.5% without specifying coefficients in advance. We also apply TESLA to the VGG-16 model on CIFAR-10 and it achieves 80% accuracy using only 20% of dot products, where the original IDP does not work. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/bd6b4dc3c6962b705863b934a22da7857dd65171.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739133487,"tcdate":1509133761070,"number":735,"cdate":1509739130821,"id":"H1K6Tb-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP used hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that achieves 94.7% accuracy on average, and especially outperforms the original IDP by 32% when only 10% of dot products we used. We further introduce trainable profile coefficients, with which TESLA further improves the accuracy to 95.5% without specifying coefficients in advance. We also apply TESLA to the VGG-16 model on CIFAR-10 and it achieves 80% accuracy using only 20% of dot products, where the original IDP does not work. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/bd6b4dc3c6962b705863b934a22da7857dd65171.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}