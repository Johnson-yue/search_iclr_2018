{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222599447,"tcdate":1511839753836,"number":3,"cdate":1511839753836,"id":"SkzzOIcez","invitation":"ICLR.cc/2018/Conference/-/Paper248/Official_Review","forum":"SkFEGHx0Z","replyto":"SkFEGHx0Z","signatures":["ICLR.cc/2018/Conference/Paper248/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Rejection of paper with potential if reframed as revisiting the NCA loss.","rating":"4: Ok but not good enough - rejection","review":"The authors propose a loss that is based on a RBF loss for metric learning and incorporates additional per exemplar weights in the index for classification. Significant improvements over softmax are shown on several datasets.\n\nIMHO, this could be a worthwhile paper, but the framing of the paper into existing literature is lacking and thus it appears as if the authors are re-inventing the wheel (NCA loss) under a different name (RBF solver).\n\nThe specific problems are:\n- The authors completely miss the connection to NCA loss (https://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf) and thus appear to be re-inventing the wheel.\n  - The proposed metric learning scenario is exactly as proposed in the NCA loss works, while the classification approach adds an interesting twist by learning per exemplar weights. I haven't encountered this before and it could make an interesting proposal. Of course the benefit of this should be evaluated in ablation studies( Tab 3 shows one experiment with marginal improvements).\n- The authors' use of 'solver' seems uncommon and confusing. What is proposed is a loss in addition to building a weighted index in the case of classification.\n- In the metric learning comparison with softmax (end of page 9) the authors mentions that a Gaussian standard deviation for softmax is learned. It appears as if the authors use the softmax logits as embedding whereas the more common approach is to use the bottleneck layer. This is also indicated by the discussion at the end of page 10 where the authors mention that softmax is restricted to axis aligned embeddings. All softmax metric learning experiments should be carried out on appropriately sized bottleneck layers.\n- Some of the motivations of what the various methods learn seem flawed, e.g. triplet loss CAN learn multiple modes per class and there is nothing in the Softmax loss that encourages the classes to fill a large region of the space.\n- Why don't the authors compare on ImageNet?\n\nSome positive points:\n- The authors mention in Sec 3.3 that updating the RBF centres is not required. This is a crucial point that should be made a centerpiece of this work, as there are many metric learning works that struggle with this. Additional experiments that can investigate this point would greatly contribute to a well rounded paper.\n- The numbers reported in Tab 1 show very significant improvements\n\nIf the paper was re-framed and builds on top of the already existing NCA loss, there could be valuable contributions in this paper. The experimental comparisons are lacking in some respect, as the comparison with Softmax as a metric learning method seems uncommon, i.e. using the logits instead of the bottleneck layer. I encourage the authors to extend the paper and flesh out some of the experiments and then submit it again.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks","abstract":"We present a radial basis function solver for convolutional neural networks that can be directly applied to both distance metric learning and classification problems. Our method treats all training features from a deep neural network as radial basis function centres and computes loss by summing the influence of a feature's nearby centres in the embedding space. Having a radial basis function centred on each training feature is made scalable by treating it as an approximate nearest neighbour search problem. End-to-end learning of the network and solver is carried out, mapping high dimensional features into clusters of the same class. This results in a well formed embedding space, where semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. The same loss function is used for both the metric learning and classification problems. We show that our radial basis function solver outperforms state-of-the-art embedding approaches on the Stanford Cars196 and CUB-200-2011 datasets. Additionally, we show that when used as a classifier, our method outperforms a conventional softmax classifier on the CUB-200-2011, Stanford Cars196, Oxford 102 Flowers and Leafsnap fine-grained classification datasets.","pdf":"/pdf/3e3becd68f1c4326b53ce2d8f8c6847e51f086bf.pdf","paperhash":"anonymous|nearest_neighbour_radial_basis_function_solvers_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018nearest,\n  title={Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFEGHx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper248/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222599489,"tcdate":1511825271029,"number":2,"cdate":1511825271029,"id":"BJ1FJQ5lG","invitation":"ICLR.cc/2018/Conference/-/Paper248/Official_Review","forum":"SkFEGHx0Z","replyto":"SkFEGHx0Z","signatures":["ICLR.cc/2018/Conference/Paper248/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes a non-linear RBF kernel based layer on top of standard CNNs for improving classification. The presentation quality is not good, and the novelty is low. ","rating":"3: Clear rejection","review":"- The paper proposes to use RBF kernel based neurons with each training data point as a center of\n  one of the RBF kernel neuron. (i) Kernel based neural networks have been explored before [A] and\n  (ii) ideas similar to the nearest neighbour based efficient but approximate learning for mixture\n  of Gaussians like settings have also been around, e.g. in traning GMMs [B]. Hence I would consider\n  the novelty to be very low \n- The paper says that the method can be applied to embedding learning and classification, which were\n  previously separate problems. This is largely incorrect as many methods for classification,\n  especially in zero- and few-shots settings (on some of the datasets used in the paper) are using\n  embedding learning [C], one of the cited and compared with paper (Sohn 2016) also does both\n  (mostly these methods use k-NN classifier with Euclidean distance between learned embeddings)\n- It seems that the method thus is adding a kernel neuron layer, with the number equal to the number\n  of training samples, centers initialized with the training samples, followed by a normalized\n  voting based on the distance of the test example with training examples of different classes\n  (approximately a weighted k-NN classifier)\n- The number of neurons in the last layer thus scales with the number of training examples, which\n  can be prohibitively large \n- It is difficult to understand what exactly is the embedding; if the number of neurons in the\n  RBF layer is equal to the number of training examples then it seems the embedding is the activation\n  of the layer before that (Fig1 also seems to suggest this). But the evaluation is done with\n  different embedding sizes, which suggests that another layer was inserted between the last FC\n  layer of the base network and the RBF layer. In that case the empirical validation is not fair as\n  the network was made deeper.\n- Also, it is a bit confusing that as training proceeds the centers change (Sec3.3 first few lines),\n  so the individual RBF neurons, eventually, do not necessarily correspond to the training examples\n  they were initialized with, but the final decision (Eq4) seems to be taken assuming that the\n  neurons do correspond to the training examples (and their classes). While the training might\n  ensure that the centers do not move so much, this should be explicitly discussed and clarified.\n  \nOverall, the novelty of the paper seems to be low and it is difficult to understand what exactly is\nbeing done.  \n\n[A] Xu et al., Kernel neuron and its training algorithm, ICONIP 2001\n[B] Verbeek et al., Efficient greedy learning of gaussian mixture models, Neural Computation 2003\n[C] Xian et al., Latent Embeddings for Zero-shot Classification, CVPR 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks","abstract":"We present a radial basis function solver for convolutional neural networks that can be directly applied to both distance metric learning and classification problems. Our method treats all training features from a deep neural network as radial basis function centres and computes loss by summing the influence of a feature's nearby centres in the embedding space. Having a radial basis function centred on each training feature is made scalable by treating it as an approximate nearest neighbour search problem. End-to-end learning of the network and solver is carried out, mapping high dimensional features into clusters of the same class. This results in a well formed embedding space, where semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. The same loss function is used for both the metric learning and classification problems. We show that our radial basis function solver outperforms state-of-the-art embedding approaches on the Stanford Cars196 and CUB-200-2011 datasets. Additionally, we show that when used as a classifier, our method outperforms a conventional softmax classifier on the CUB-200-2011, Stanford Cars196, Oxford 102 Flowers and Leafsnap fine-grained classification datasets.","pdf":"/pdf/3e3becd68f1c4326b53ce2d8f8c6847e51f086bf.pdf","paperhash":"anonymous|nearest_neighbour_radial_basis_function_solvers_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018nearest,\n  title={Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFEGHx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper248/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222599532,"tcdate":1511783922897,"number":1,"cdate":1511783922897,"id":"r1jeC_Kgf","invitation":"ICLR.cc/2018/Conference/-/Paper248/Official_Review","forum":"SkFEGHx0Z","replyto":"SkFEGHx0Z","signatures":["ICLR.cc/2018/Conference/Paper248/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"(Summary)\nThis paper proposes weighted RBF distance based loss function where embeddings for cluster centroids and data are learned and used for class probabilities (eqn 3). The authors experiment on CUB200-2011, Cars106, Oxford 102 Flowers datasets.\n\n(Pros)\nThe citations and related works cover fairly comprehensive and up-to-date literatures on deep metric learning.\n\n(Cons)\nThe proposed method is unlikely to scale with respect to the number of classes. \"..our approach is also free to create multiple clusters for each class..\" This makes it unfair to deep metric learning baselines in figures 2 and 3 because DMP baselines has memory footprint constant in the number of classes. In contrast, the proposed method have linear memory footprint in the number of classes. Furthermore, the authors ommit how many centroids are used in each experiments.\n\n(Assessment)\nMarginally below acceptance threshold. The method is unlikely to scale and the important details on how many centroids the authors used in each experiments is omitted.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks","abstract":"We present a radial basis function solver for convolutional neural networks that can be directly applied to both distance metric learning and classification problems. Our method treats all training features from a deep neural network as radial basis function centres and computes loss by summing the influence of a feature's nearby centres in the embedding space. Having a radial basis function centred on each training feature is made scalable by treating it as an approximate nearest neighbour search problem. End-to-end learning of the network and solver is carried out, mapping high dimensional features into clusters of the same class. This results in a well formed embedding space, where semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. The same loss function is used for both the metric learning and classification problems. We show that our radial basis function solver outperforms state-of-the-art embedding approaches on the Stanford Cars196 and CUB-200-2011 datasets. Additionally, we show that when used as a classifier, our method outperforms a conventional softmax classifier on the CUB-200-2011, Stanford Cars196, Oxford 102 Flowers and Leafsnap fine-grained classification datasets.","pdf":"/pdf/3e3becd68f1c4326b53ce2d8f8c6847e51f086bf.pdf","paperhash":"anonymous|nearest_neighbour_radial_basis_function_solvers_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018nearest,\n  title={Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFEGHx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper248/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739405269,"tcdate":1509081648717,"number":248,"cdate":1509739402612,"id":"SkFEGHx0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkFEGHx0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks","abstract":"We present a radial basis function solver for convolutional neural networks that can be directly applied to both distance metric learning and classification problems. Our method treats all training features from a deep neural network as radial basis function centres and computes loss by summing the influence of a feature's nearby centres in the embedding space. Having a radial basis function centred on each training feature is made scalable by treating it as an approximate nearest neighbour search problem. End-to-end learning of the network and solver is carried out, mapping high dimensional features into clusters of the same class. This results in a well formed embedding space, where semantically related instances are likely to be located near one another, regardless of whether or not the network was trained on those classes. The same loss function is used for both the metric learning and classification problems. We show that our radial basis function solver outperforms state-of-the-art embedding approaches on the Stanford Cars196 and CUB-200-2011 datasets. Additionally, we show that when used as a classifier, our method outperforms a conventional softmax classifier on the CUB-200-2011, Stanford Cars196, Oxford 102 Flowers and Leafsnap fine-grained classification datasets.","pdf":"/pdf/3e3becd68f1c4326b53ce2d8f8c6847e51f086bf.pdf","paperhash":"anonymous|nearest_neighbour_radial_basis_function_solvers_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018nearest,\n  title={Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFEGHx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper248/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}