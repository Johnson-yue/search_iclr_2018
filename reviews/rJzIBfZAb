{"notes":[{"tddate":null,"ddate":null,"tmdate":1511938535534,"tcdate":1511938535534,"number":2,"cdate":1511938535534,"id":"HJJe50sez","invitation":"ICLR.cc/2018/Conference/-/Paper835/Official_Comment","forum":"rJzIBfZAb","replyto":"rkj4yGsgf","signatures":["ICLR.cc/2018/Conference/Paper835/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper835/Authors"],"content":{"title":"Re: Lyu et al. (2015)","comment":"We thank the reviewer for bringing the work of Lyu et al. to our attention. We will cite it and discuss it in the \"Related Work\" section of our updated paper. As we already point out in our paper, both the general min-max framework, as well as its application to the problem of adversarial examples, are not new. Min-max formulations have been used extensively in the context of robust optimization and statistics, going back at least to the work of Abraham Wald in the 1930s and 40s. In the context of adversarial examples, we already cite the work of Shaham et al. (https://arxiv.org/abs/1511.05432) and Huang et al. (https://arxiv.org/abs/1511.03034), which consider a similar min-max formulation and appeared on arXiv nearly concurrently with the work of Lyu et al.\n\nTo clarify our contributions: the min-max formulation is part of the approach and *not* claimed as a contribution (see our introduction and the reply to \"Certified Defenses for Data Poisoning Attacks\" above). Instead, one of our main contributions is to study the loss landscape of the saddle point problem, *without replacing the loss by its first-order approximation*. It is known that solving the saddle point problem with a first-order approximation of the loss (see Figure 6 of Appendix B in our paper) produces networks that are vulnerable to more sophisticated (multi-step) attacks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1512222783608,"tcdate":1511900038038,"number":3,"cdate":1511900038038,"id":"SyRt7SoxG","invitation":"ICLR.cc/2018/Conference/-/Paper835/Official_Review","forum":"rJzIBfZAb","replyto":"rJzIBfZAb","signatures":["ICLR.cc/2018/Conference/Paper835/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The unreasonable effectiveness of gradient descent","rating":"7: Good paper, accept","review":"This paper consolidates and builds on recent work on adversarial examples and adversarial training for image classification. Its contributions:\n\n - Making the connection between adversarial training and robust optimization more explicit.\n\n - Empirical evidence that:\n   * Projected gradient descent (PGD) (as proposed by Kurakin et al. (2016)) reasonably approximates the optimal attack against deep convolutional neural networks\n   * PGD finds better adversarial examples, and training with it yields more robust models, compared to FGSM \n\n - Additional empirical analysis:\n   * Comparison of weights in robust and non-robust MNIST classifiers\n   * Vulnerability of L_infty-robust models to to L_2-bounded attacks\n\nThe evidence that PGD consistently finds good examples is fairly compelling -- when initialized from 10,000 random points near the example to be disguised, it usually finds examples of similar quality. The remaining variance that's present in those distributions shouldn't hurt learning much, as long as a significant fraction of the adversarial examples are close enough to optimal.\n\nGiven the consistent effectiveness of PGD, using PGD for adversarial training should yield models that are reliably robust (for a specific definition of robustness, such as bounded L_infinity norm). This is an improvement over purely heuristic approaches, which are often less robust than claimed.\n\nThe comparison to R+FGSM is interesting, and could be extended in a few small ways. What would R+FGSM look like with 10,000 restarts? The distribution should be much broader, which would further demonstrate how PGD works better on these models. Also, when generating adversarial examples for testing, how well would R+FGSM work if you took the best of 2,000 random restarts? This would match the number of gradient computations required by PGD with 100 steps and 20 restarts. Again, I expect that PGD would be better, but this would make that point clearer. I think this analysis would make the paper stronger, but I don't think it's required for acceptance, especially since R+FGSM itself is such a recent development.\n\nOne thing not discussed is the high computational cost: performing a 40-step optimization of each training example will be ~40 times slower than standard stochastic gradient descent. I suspect this is the reason why there are results on MNIST and CIFAR, but not ImageNet. It would be very helpful to add some discussion of this.\n\nThe title seems unnecessarily vague, since many papers have been written with the same goal -- make deep learning models resistant to adversarial attacks. (This comment does not affect my opinion about whether or not the paper should be accepted, and is merely a suggestion for the authors.)\n\nAlso, much of the paper's content is in the appendices. This reads like a journal article where the references were put in the middle. I don't know if that's fixable, given conference constraints.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1511886647766,"tcdate":1511886647766,"number":3,"cdate":1511886647766,"id":"rkj4yGsgf","invitation":"ICLR.cc/2018/Conference/-/Paper835/Public_Comment","forum":"rJzIBfZAb","replyto":"rJzIBfZAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Lyu et al. (2015)","comment":"Lyu et al. (2015) has made the connection to robust optimization (and proposed a new regularization). \nPlease cite their work https://arxiv.org/pdf/1511.06385.pdf when you introduce the minimax formulation.\n\nHaving said this, I understand that the main contribution of this paper is a systematic empirical study of the minimax formulation. The abstract and the intro seem to give the impression that this is the first paper that suggests a unifying minimax framework. Citing other works and attributing credit accordingly would help emphasize the true contributions of the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1512222783653,"tcdate":1511814821624,"number":2,"cdate":1511814821624,"id":"Hy0j8ecgz","invitation":"ICLR.cc/2018/Conference/-/Paper835/Official_Review","forum":"rJzIBfZAb","replyto":"rJzIBfZAb","signatures":["ICLR.cc/2018/Conference/Paper835/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting experimental results, but insufficient to support some of the strong claims made in the paper","rating":"5: Marginally below acceptance threshold","review":"- The authors investigate a minimax formulation of deep network learning to increase their robustness, using projected gradient descent as the main adversary. The idea of formulating the threat model as the inner maximization problem is an old one. Many previous works on dealing with uncertain inputs in classification apply this minimax approach using robust optimization, e.g.: \n\nhttps://www2.eecs.berkeley.edu/Pubs/TechRpts/2003/CSD-03-1279.pdf\nhttp://www.jmlr.org/papers/volume13/ben-tal12a/ben-tal12a.pdf\n\nIn the case of convex uncertainty sets, many of these problems can be solved efficiently to a global minimum. Generalization bounds on the adversarial losses can also be proved. Generalizing this approach to non-convex neural network learning makes sense, even when it is hard to obtain any theoretical guarantees. \n\n- The main novelty is the use of projected gradient descent (PGD) as the adversary. From the experiments it seems training with PGD is very robust against a set of adversaries including fast gradient sign method (FGSM), and the method proposed in Carlini & Wagner (CW). Although the empirical results are promising, in my opinion they are not sufficient to support the bold claim that PGD is a 'universal' first order adversary (on p2, in the contribution list) and provides broad security guarantee (in the abstract). For example, other adversarial example generation methods such as DeepFool and Jacobian-based Saliency Map approach are missing from the comparison. Also it is not robust to generalize from two datasets MNIST and CIFAR alone.  \n\n- Another potential issue with using projected gradient descent as adversary is the quality of the adversarial example generated. The authors show empirically that PGD finds adversarial examples with very similar loss values on multiple runs. But this does not exclude the possibility that PGD with different step sizes or line search procedure, or the use of randomization strategies such as annealing, can find better adversarial examples under the same threat model. This could make the robustness of the network rather dependent on the specific implementation of PGD for the inner maximization problem. \n\n- In Tables 3, 4, and 5 in the appendix, in most cases models trained with PGD are more robust than models trained with FGSM as adversary, modulo the phenomenon of label leakage when using FGSM as attack. However in the bottom right corner of Table 4, FGSM training seems to be more robust than PGD training against black box PGD attacks. This raises the question on whether PGD is truly 'universal' and provides broad security guarantees, once we add more first order attacks methods to the mix. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1512222783696,"tcdate":1511709840036,"number":1,"cdate":1511709840036,"id":"rkO53U_ez","invitation":"ICLR.cc/2018/Conference/-/Paper835/Official_Review","forum":"rJzIBfZAb","replyto":"rJzIBfZAb","signatures":["ICLR.cc/2018/Conference/Paper835/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to look at making neural networks resistant to adversarial loss through the framework of saddle-point problems. They show that, on MNIST, a PGD adversary fits this framework and allows the authors to train very robust models. They also show encouraging results for robust CIFAR-10 models, but with still much room for improvement. Finally, they suggest that PGD is an optimal first order adversary, and leads to optimal robustness against any first order attack.\n\nThis paper is well written, brings new ideas and perfoms interesting experiments, but its claims are somewhat bothering me, considering that e.g. your CIFAR-10 results are somewhat underwhelming. All you've really proven is that PGD on MNIST seems to be the ultimate adversary. You contrast this to the fact that the optimization is non-convex, but we know for a fact that MNIST is fairly simple in that regime; iirc a linear classifier gets something like 91% accuracy on MNIST. So my guess is that the optimization problem on MNIST is in fact pretty convex and mostly respects the assumptions of Danskin's theorem, but not so much for CIFAR-10 (maybe even less so for e.g. ImageNet, which is what Kurakin et al. seem to find).\n\nConsidering your CIFAR-10 results, I don't think anyone should \"suggest that secure neural networks are within reach\", because 1) there is still room for improvement 2) it's a safe bet that someone will always just come up with a better attack than whatever defense we have now. It has been this way in many disciplines (crypto, security) for centuries, I don't see why deep learning should be exempt. Simply saying \"we believe that our robust models are significant progress on the defense side\" was enough, because afaik you did improve on CIFAR-10's SOTA; don't overclaim. \nYou make these kinds of claims in a few other places in this paper, please be careful with that.\n\nThe contributions in your appendix are interesting. \nAppendix A somewhat confirms one of the postulates in Goodfellow et al. (2014): \"The direction of perturbation, rather than the specific point in space, matters most. Space is not full of pockets of adversarial examples that finely tile the reals like the rational numbers\".\nAppendix B and C are not extremely novel in my mind, but definitely add more evidence. \nAppendix E is quite nice since it gives an insight into what actually makes the model resistant to adversarial examples.\n\n\nRemarks:\n- The update for PGD should be using \\nabla_{x_t} L(\\theta,x_t,y), (rather than only \\nabla_x)?\n- In table 2, attacking a with 20-step PGD is doing better than 7-step.  When you say \"other hyperparameter choices didn’t offer a significant decrease in accuracy\", does that include the number of steps? If not why stop there? What happens for more steps? (or is it too computationally intensive?)\n- You only seem to consider adversarial examples created from your dataset + adv. noise. What about rubbish class examples? (e.g. rgb noise)\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1510956319429,"tcdate":1510956319429,"number":2,"cdate":1510956319429,"id":"rJw7T03yG","invitation":"ICLR.cc/2018/Conference/-/Paper835/Public_Comment","forum":"rJzIBfZAb","replyto":"ryQ33hmkf","signatures":["~Jacob_Steinhardt1"],"readers":["everyone"],"writers":["~Jacob_Steinhardt1"],"content":{"title":"Re: re: data poisoning paper","comment":"As the author of the data poisoning paper that is mentioned, I just wanted to agree with the authors of the current paper that these papers (in my opinion) are quite different. It is of course interesting to compare the similarities/differences in the min-max formulation, but the problems studied in the two papers are so different that this would (again, in my opinion) have no affect whatsoever on novelty of the current submission."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1510358186674,"tcdate":1510358186674,"number":1,"cdate":1510358186674,"id":"ryQ33hmkf","invitation":"ICLR.cc/2018/Conference/-/Paper835/Official_Comment","forum":"rJzIBfZAb","replyto":"rJsu3TGyG","signatures":["ICLR.cc/2018/Conference/Paper835/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper835/Authors"],"content":{"title":"Re: Certified Defenses for Data Poisoning Attacks","comment":"We thank the reviewer for inquiring about the novelty of our work. As we point out in our paper (see Page 3), the min-max formulation itself is not new. In fact, problem formulations of this form have been studied for multiple decades (c.f. the work of Abraham Wald). Moreover, there is a rich literature concerning min-max problems in robust optimization. Claiming a min-max formulation as new would ignore a significant body of prior work.\n\nAs we point out in the introduction, our main contribution is *how* we employ the min-max formulation to study adversarially robust machine learning. To the best of our knowledge, our paper is the first detailed study of the min-max formulation for robust neural networks. From a scientific point of view, the question is not only whether adversarial robustness can be described with a min-max formulation, but whether such a formulation actually matches the computational reality we face in the practice of deep learning.\n\nOne concrete contribution is our thorough experiments exploring the adversarial loss landscape. Combined with Danskin’s theorem, they give evidence to the theory that adversarial training is indeed a principled way to solve the aforementioned min-max problem. Furthermore, our paper conducted the first public attack challenge to ascertain the robustness of a proposed defense. The challenge showed that our MNIST model is the first deep network that could not easily be broken with a new attack (subject to l_infinity constraints).\n\nFinally, we would also like to point out that our paper appeared publicly nearly concurrently with the NIPS paper mentioned by the reviewer. Moreover, there are several important differences compared to this paper. For instance, their focus is on robustness to corrupt training data, while our paper is about robustly classifying new unseen examples. Overall, we believe that reducing the comparison with the cited NIPS paper to the min-max formulation is an oversimplification of both their work and our work.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1510296691401,"tcdate":1510296691401,"number":1,"cdate":1510296691401,"id":"rJsu3TGyG","invitation":"ICLR.cc/2018/Conference/-/Paper835/Public_Comment","forum":"rJzIBfZAb","replyto":"rJzIBfZAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Certified Defenses for Data Poisoning Attacks","comment":"This NIPS paper seems to have the same formulation for defending against attacks: https://arxiv.org/pdf/1706.03691.pdf\n\nThey consider the same min-max formulation and try to build a defense against attacks (similar idea to current paper?). \n\nThey consider data-poisoning, but one could imagine the same ideas applied to adversarial attacks. Does the current paper propose something very new from what was described in the NIPS paper? I'm not a specialist in analysis or large-scale optimization, hence my question. \n\nThanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]}},{"tddate":null,"ddate":null,"tmdate":1509739075674,"tcdate":1509135690444,"number":835,"cdate":1509739073020,"id":"rJzIBfZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJzIBfZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Deep Learning Models Resistant to Adversarial Attacks","abstract":"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.","pdf":"/pdf/063919f3e0237592fdc368c73b405c9d26c4cc73.pdf","TL;DR":"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.","paperhash":"anonymous|towards_deep_learning_models_resistant_to_adversarial_attacks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Deep Learning Models Resistant to Adversarial Attacks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJzIBfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper835/Authors"],"keywords":["adversarial examples","robust optimization","ML security"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}