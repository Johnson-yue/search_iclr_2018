{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222565464,"tcdate":1511871686185,"number":3,"cdate":1511871686185,"id":"HJ0pVRqxM","invitation":"ICLR.cc/2018/Conference/-/Paper124/Official_Review","forum":"rytNfI1AZ","replyto":"rytNfI1AZ","signatures":["ICLR.cc/2018/Conference/Paper124/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a single bit for each weight","rating":"6: Marginally above acceptance threshold","review":"The paper trains wide ResNets for 1-bit per weight deployment.\nThe experiments are conducted on CIFAR-10, CIFAR-100, SVHN and ImageNet32.\n\n+the paper reads well\n+the reported performance is compelling \n\nPerhaps the authors should make it clear in the abstract by replacing:\n\"Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight\"\nwith\n\"Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for wide ResNets deployed using a single bit for each weight\"\n\nI am curious how the proposed approach compares with SqueezeNet (Iandola et al.,2016) in performance and memory savings.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training wide residual networks for deployment using a single bit for each weight","abstract":"For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware,  each learnt weight parameter should ideally be represented and stored using a single bit.  Error-rates usually increase when this requirement is imposed. Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight. The main contribution is to replace learnt scaling factors applied to the sign of weights in training, by a constant scaling factor that reflects a common initialisation method. For models with 1-bit per weight, and  20 convolutional layers, requiring only ~4 MB of parameter memory, for CIFAR-10, CIFAR-100 we achieve error rates of 3.74%, 18.41%. We also considered MNIST, SVHN, Imagenet32, achieving single-bit weight test results of 0.27%, 1.93%, and 42.92/19.95% (Top-1/Top-5) respectively. These error rates are about half those of previously reported error rates for CIFAR-10/100, and are within 1-3% of our error-rates for the same network with full-precision weights.  Using a warm-restart learning-rate schedule, we found training for single-bit weights just as fast as full-precision networks, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR 10/100.","pdf":"/pdf/0c0aba9dca13099f1fd9d19eac55915043bd0074.pdf","TL;DR":"We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.","paperhash":"anonymous|training_wide_residual_networks_for_deployment_using_a_single_bit_for_each_weight","_bibtex":"@article{\n  anonymous2018training,\n  title={Training wide residual networks for deployment using a single bit for each weight},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytNfI1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper124/Authors"],"keywords":["wide residual networks","model compression","quantization","1-bit weights"]}},{"tddate":null,"ddate":null,"tmdate":1512222565511,"tcdate":1511798137959,"number":2,"cdate":1511798137959,"id":"SkGtH2Kxf","invitation":"ICLR.cc/2018/Conference/-/Paper124/Official_Review","forum":"rytNfI1AZ","replyto":"rytNfI1AZ","signatures":["ICLR.cc/2018/Conference/Paper124/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Solid work","rating":"6: Marginally above acceptance threshold","review":"The authors propose to train neural networks with 1bit weights by storing and updating full precision weights in training, but using the reduced 1bit version of the network to compute predictions and gradients in training. They add a few tricks to keep the optimization numerically efficient. Since right now more and more neural networks are deployed to end users, the authors make an interesting contribution to a very relevant question.\n\nThe approach is precisely described although the text sometimes could be a bit clearer (for example, the text contains many important references to later sections).\n\nThe authors include a few other methods for comparision, but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint. For example, weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32. Additionally, for practical applications, methods like weight pruning might be more promising since they reduce both the memory load and the computational load.\n\nSide mark: the manuscript has quite a few typos.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training wide residual networks for deployment using a single bit for each weight","abstract":"For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware,  each learnt weight parameter should ideally be represented and stored using a single bit.  Error-rates usually increase when this requirement is imposed. Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight. The main contribution is to replace learnt scaling factors applied to the sign of weights in training, by a constant scaling factor that reflects a common initialisation method. For models with 1-bit per weight, and  20 convolutional layers, requiring only ~4 MB of parameter memory, for CIFAR-10, CIFAR-100 we achieve error rates of 3.74%, 18.41%. We also considered MNIST, SVHN, Imagenet32, achieving single-bit weight test results of 0.27%, 1.93%, and 42.92/19.95% (Top-1/Top-5) respectively. These error rates are about half those of previously reported error rates for CIFAR-10/100, and are within 1-3% of our error-rates for the same network with full-precision weights.  Using a warm-restart learning-rate schedule, we found training for single-bit weights just as fast as full-precision networks, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR 10/100.","pdf":"/pdf/0c0aba9dca13099f1fd9d19eac55915043bd0074.pdf","TL;DR":"We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.","paperhash":"anonymous|training_wide_residual_networks_for_deployment_using_a_single_bit_for_each_weight","_bibtex":"@article{\n  anonymous2018training,\n  title={Training wide residual networks for deployment using a single bit for each weight},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytNfI1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper124/Authors"],"keywords":["wide residual networks","model compression","quantization","1-bit weights"]}},{"tddate":null,"ddate":null,"tmdate":1512222565550,"tcdate":1511751402736,"number":1,"cdate":1511751402736,"id":"BJyxkbFxz","invitation":"ICLR.cc/2018/Conference/-/Paper124/Official_Review","forum":"rytNfI1AZ","replyto":"rytNfI1AZ","signatures":["ICLR.cc/2018/Conference/Paper124/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Mixed ideas","rating":"6: Marginally above acceptance threshold","review":"This paper introduces several ideas: scaling, warm-restarting learning rate, cutout augmentation. \n\nI would like to see detailed ablation studies: how the performance is influenced by the warm-restarting learning rates, how the performance is influenced by cutout. Is the scaling scheme helpful for existing single-bit algorithms?\n\nQuestion for Table 3: 1-bit WRN 20-10 (this paper) outperforms WRN 22-10 with the same #parameters on C100. I would like to see more explanations. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training wide residual networks for deployment using a single bit for each weight","abstract":"For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware,  each learnt weight parameter should ideally be represented and stored using a single bit.  Error-rates usually increase when this requirement is imposed. Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight. The main contribution is to replace learnt scaling factors applied to the sign of weights in training, by a constant scaling factor that reflects a common initialisation method. For models with 1-bit per weight, and  20 convolutional layers, requiring only ~4 MB of parameter memory, for CIFAR-10, CIFAR-100 we achieve error rates of 3.74%, 18.41%. We also considered MNIST, SVHN, Imagenet32, achieving single-bit weight test results of 0.27%, 1.93%, and 42.92/19.95% (Top-1/Top-5) respectively. These error rates are about half those of previously reported error rates for CIFAR-10/100, and are within 1-3% of our error-rates for the same network with full-precision weights.  Using a warm-restart learning-rate schedule, we found training for single-bit weights just as fast as full-precision networks, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR 10/100.","pdf":"/pdf/0c0aba9dca13099f1fd9d19eac55915043bd0074.pdf","TL;DR":"We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.","paperhash":"anonymous|training_wide_residual_networks_for_deployment_using_a_single_bit_for_each_weight","_bibtex":"@article{\n  anonymous2018training,\n  title={Training wide residual networks for deployment using a single bit for each weight},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytNfI1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper124/Authors"],"keywords":["wide residual networks","model compression","quantization","1-bit weights"]}},{"tddate":null,"ddate":null,"tmdate":1509739471530,"tcdate":1509020208736,"number":124,"cdate":1509739468874,"id":"rytNfI1AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rytNfI1AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Training wide residual networks for deployment using a single bit for each weight","abstract":"For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware,  each learnt weight parameter should ideally be represented and stored using a single bit.  Error-rates usually increase when this requirement is imposed. Here, we report methodological innovations that result in large reductions in error rates across multiple datasets for deep convolutional neural networks deployed using a single bit for each weight. The main contribution is to replace learnt scaling factors applied to the sign of weights in training, by a constant scaling factor that reflects a common initialisation method. For models with 1-bit per weight, and  20 convolutional layers, requiring only ~4 MB of parameter memory, for CIFAR-10, CIFAR-100 we achieve error rates of 3.74%, 18.41%. We also considered MNIST, SVHN, Imagenet32, achieving single-bit weight test results of 0.27%, 1.93%, and 42.92/19.95% (Top-1/Top-5) respectively. These error rates are about half those of previously reported error rates for CIFAR-10/100, and are within 1-3% of our error-rates for the same network with full-precision weights.  Using a warm-restart learning-rate schedule, we found training for single-bit weights just as fast as full-precision networks, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR 10/100.","pdf":"/pdf/0c0aba9dca13099f1fd9d19eac55915043bd0074.pdf","TL;DR":"We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.","paperhash":"anonymous|training_wide_residual_networks_for_deployment_using_a_single_bit_for_each_weight","_bibtex":"@article{\n  anonymous2018training,\n  title={Training wide residual networks for deployment using a single bit for each weight},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytNfI1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper124/Authors"],"keywords":["wide residual networks","model compression","quantization","1-bit weights"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}