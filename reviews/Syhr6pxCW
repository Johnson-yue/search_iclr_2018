{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222652433,"tcdate":1512182688724,"number":3,"cdate":1512182688724,"id":"SJt9X9kWz","invitation":"ICLR.cc/2018/Conference/-/Paper432/Official_Review","forum":"Syhr6pxCW","replyto":"Syhr6pxCW","signatures":["ICLR.cc/2018/Conference/Paper432/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple and effective baseline for conditional image generation","rating":"7: Good paper, accept","review":"This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets. \n\nPros:\n- Simple approach based on nearest-neighbors, likely easier to train compared to GANs.\n- Scales to high-resolution images.\n\nCons:\n- Requires a potentially costly search procedure to generate images.\n- Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelNN: Example-based Image Synthesis","abstract":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. Importantly, pixel-wise matching allows our method to compose novel high-frequency content by cutting-and-pasting pixels from different training exemplars.  We demonstrate our approach for various input modalities, and for various domains ranging from human faces, pets, shoes, and handbags.","pdf":"/pdf/aa118e4dd54dac476182c27b200a708a8d79dc99.pdf","TL;DR":"Pixel-wise nearest neighbors used for generating multiple images from incomplete priors such as a low-res images, surface normals, edges etc.","paperhash":"anonymous|pixelnn_examplebased_image_synthesis","_bibtex":"@article{\n  anonymous2018pixelnn:,\n  title={PixelNN: Example-based Image Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syhr6pxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper432/Authors"],"keywords":["conditional image synthesis","nearest neighbors"]}},{"tddate":null,"ddate":null,"tmdate":1512222652476,"tcdate":1511903947598,"number":2,"cdate":1511903947598,"id":"BJ4AfUoeG","invitation":"ICLR.cc/2018/Conference/-/Paper432/Official_Review","forum":"Syhr6pxCW","replyto":"Syhr6pxCW","signatures":["ICLR.cc/2018/Conference/Paper432/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Shines Light on Deficiencies in Conditional GAN: borderline accept","rating":"6: Marginally above acceptance threshold","review":"This paper presents a pixel-matching based approach to synthesizing RGB images from input edge or normal maps. The approach is compared to Isola et al’s conditional adversarial networks, and unlike the conditional GAN, is able to produce a diverse set of outputs.\n\nOverall, the paper describes a computer visions system based on synthesizing images, and not necessarily a new theoretical framework to compete with GANs. With the current focus of the paper being the proposed system, it is interesting to the computer vision community. However, if one views the paper in a different light, namely showing some “blind-spots” of current conditional GAN approaches like lack of diversity, then it can be of much more interest to the broader ICLR community.\n\nPros: \nOverall the paper is well-written\nMakes a strong case that random noise injection inside conditional GANs does not produce enough diversity\nShows a number of qualitative and quantitative results\n\nConcerns about the paper:\n1.) It is not clear how well the proposed approach works with CNN architectures other than PixelNet\n2.) Since the paper used “the pre-trained PixelNet to extract surface normal and edge maps” for ground-truth generation, it is not clear whether the approach will work as well when the input is a ground-truth semantic segmentation map.\n3.) Since the paper describes a computer-vision image synthesis system and not a new theoretical result, I believe reporting the actual run-time of the system will make the paper stronger. Can PixelNN run in real-time? How does the timing compare to Isola et al’s Conditional GAN?\n\nMinor comments:\n1.) The paper mentions making predictions from “incomplete” input several times, but in all experiments, the input is an edge map, normal map, or low-resolution image. When reading the manuscript the first time, I was expecting experiments on images that have regions that are visible and regions that are masked out. However, I am not sure if the confusion is solely mine, or shared with other readers.\n\n2.) Equation 1 contains the norm operator twice, and the first norm has no subscript, while the second one has an l_2 subscript. I would expect the notation style to be consistent within a single equation (i.e., use ||w||_2^2, ||w||^2, or ||w||_{l_2}^2)\n\n3.) Table 1 has two sub-tables: left and right. The sub-tables have the AP column in different places.\n\n4.) “Dense pixel-level correspondences” are discussed but not evaluated.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelNN: Example-based Image Synthesis","abstract":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. Importantly, pixel-wise matching allows our method to compose novel high-frequency content by cutting-and-pasting pixels from different training exemplars.  We demonstrate our approach for various input modalities, and for various domains ranging from human faces, pets, shoes, and handbags.","pdf":"/pdf/aa118e4dd54dac476182c27b200a708a8d79dc99.pdf","TL;DR":"Pixel-wise nearest neighbors used for generating multiple images from incomplete priors such as a low-res images, surface normals, edges etc.","paperhash":"anonymous|pixelnn_examplebased_image_synthesis","_bibtex":"@article{\n  anonymous2018pixelnn:,\n  title={PixelNN: Example-based Image Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syhr6pxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper432/Authors"],"keywords":["conditional image synthesis","nearest neighbors"]}},{"tddate":null,"ddate":null,"tmdate":1512222652525,"tcdate":1511752112648,"number":1,"cdate":1511752112648,"id":"SJt3bbKgz","invitation":"ICLR.cc/2018/Conference/-/Paper432/Official_Review","forum":"Syhr6pxCW","replyto":"Syhr6pxCW","signatures":["ICLR.cc/2018/Conference/Paper432/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice approach on conditional image generation","rating":"8: Top 50% of accepted papers, clear accept","review":"Overall I like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image, super-resolution, etc. Unlike the generative distribution sampling of GANs, the method provides an interesting compositional scheme, where the low frequencies are regressed and the high frequencies are obtained by \"copying\" patches from the training set. In some cases the results are similar to pix-to-pix (also in the numerical evaluation) but the method allows for one-to-many image generation, which is a important contribution. Another positive aspect of the paper is that the synthesis results can be analyzed, providing insights for the generation process. \n\nWhile most of the paper is well written, some parts are difficult to parse. For example, the introduction has some parts that look more like related work (that is mostly a personal preference in writting). Also in Section 3, the paragraph for distance functions do not provide any insight about what is used, but it is included in the next paragraph (I would suggest either merging or not highlighting the paragraphs).\n\nQ: The spatial grouping that is happening in the compositional stage, is it solely due to the multi-scale hypercolumns?  Would the result be more inconsistent if the hypercolumns had smaller receptive field?\n\nQ: For the multiple outputs, the k neighbor is selected at random?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PixelNN: Example-based Image Synthesis","abstract":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. Importantly, pixel-wise matching allows our method to compose novel high-frequency content by cutting-and-pasting pixels from different training exemplars.  We demonstrate our approach for various input modalities, and for various domains ranging from human faces, pets, shoes, and handbags.","pdf":"/pdf/aa118e4dd54dac476182c27b200a708a8d79dc99.pdf","TL;DR":"Pixel-wise nearest neighbors used for generating multiple images from incomplete priors such as a low-res images, surface normals, edges etc.","paperhash":"anonymous|pixelnn_examplebased_image_synthesis","_bibtex":"@article{\n  anonymous2018pixelnn:,\n  title={PixelNN: Example-based Image Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syhr6pxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper432/Authors"],"keywords":["conditional image synthesis","nearest neighbors"]}},{"tddate":null,"ddate":null,"tmdate":1509739306698,"tcdate":1509117252213,"number":432,"cdate":1509739304038,"id":"Syhr6pxCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Syhr6pxCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"PixelNN: Example-based Image Synthesis","abstract":"We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds:  the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. Importantly, pixel-wise matching allows our method to compose novel high-frequency content by cutting-and-pasting pixels from different training exemplars.  We demonstrate our approach for various input modalities, and for various domains ranging from human faces, pets, shoes, and handbags.","pdf":"/pdf/aa118e4dd54dac476182c27b200a708a8d79dc99.pdf","TL;DR":"Pixel-wise nearest neighbors used for generating multiple images from incomplete priors such as a low-res images, surface normals, edges etc.","paperhash":"anonymous|pixelnn_examplebased_image_synthesis","_bibtex":"@article{\n  anonymous2018pixelnn:,\n  title={PixelNN: Example-based Image Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syhr6pxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper432/Authors"],"keywords":["conditional image synthesis","nearest neighbors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}