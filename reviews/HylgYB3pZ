{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222693674,"tcdate":1511776289340,"number":2,"cdate":1511776289340,"id":"H1tXgwtgG","invitation":"ICLR.cc/2018/Conference/-/Paper58/Official_Review","forum":"HylgYB3pZ","replyto":"HylgYB3pZ","signatures":["ICLR.cc/2018/Conference/Paper58/AnonReviewer2"],"readers":["everyone"],"content":{"title":"more disadvantages vs few advatages as of now.","rating":"5: Marginally below acceptance threshold","review":"The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article). The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.   Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same. \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU. When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input. While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features. Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias","abstract":"In this paper, we first identify \\textit{angle bias}, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose \\textit{linearly constrained weights (LCW)} to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.","pdf":"/pdf/94467c7407bb0ed3f4fd978b6d3e524e2a3c60d9.pdf","TL;DR":"We identify angle bias that causes the vanishing gradient problem in deep nets and propose an efficient method to reduce the bias.","paperhash":"anonymous|linearly_constrained_weights_resolving_the_vanishing_gradient_problem_by_reducing_angle_bias","_bibtex":"@article{\n  anonymous2018linearly,\n  title={Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HylgYB3pZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper58/Authors"],"keywords":["vanishing gradient problem","multilayer perceptron","angle bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222693715,"tcdate":1511243307264,"number":1,"cdate":1511243307264,"id":"SymE04bxf","invitation":"ICLR.cc/2018/Conference/-/Paper58/Official_Review","forum":"HylgYB3pZ","replyto":"HylgYB3pZ","signatures":["ICLR.cc/2018/Conference/Paper58/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Studies an interesting phenomenon but with preliminary results","rating":"5: Marginally below acceptance threshold","review":"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.  Test accuracy is not improved, however.\n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.\n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case.  There can be many other approaches to mitigate the impact of angle bias.  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?  When the mean of input is zero, there is no angle bias in the first layer.  Also, what about if we include the bias term so that b + w a is the preactivation value?\n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.\n\nMinor comments:\n\nIn Section 2.2, is Layer 1 the input layer or the next?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias","abstract":"In this paper, we first identify \\textit{angle bias}, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose \\textit{linearly constrained weights (LCW)} to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.","pdf":"/pdf/94467c7407bb0ed3f4fd978b6d3e524e2a3c60d9.pdf","TL;DR":"We identify angle bias that causes the vanishing gradient problem in deep nets and propose an efficient method to reduce the bias.","paperhash":"anonymous|linearly_constrained_weights_resolving_the_vanishing_gradient_problem_by_reducing_angle_bias","_bibtex":"@article{\n  anonymous2018linearly,\n  title={Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HylgYB3pZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper58/Authors"],"keywords":["vanishing gradient problem","multilayer perceptron","angle bias"]}},{"tddate":null,"ddate":null,"tmdate":1509739509318,"tcdate":1508821223956,"number":58,"cdate":1509739506661,"id":"HylgYB3pZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HylgYB3pZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias","abstract":"In this paper, we first identify \\textit{angle bias}, a simple but remarkable phenomenon that causes the vanishing gradient problem in a multilayer perceptron (MLP) with sigmoid activation functions. We then propose \\textit{linearly constrained weights (LCW)} to reduce the angle bias in a neural network, so as to train the network under the constraints that the sum of the elements of each weight vector is zero. A reparameterization technique is presented to efficiently train a model with LCW by embedding the constraints on weight vectors into the structure of the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a mechanism to correct angle bias. Preliminary experiments show that LCW helps train a 100-layered MLP more efficiently than does batch normalization.","pdf":"/pdf/94467c7407bb0ed3f4fd978b6d3e524e2a3c60d9.pdf","TL;DR":"We identify angle bias that causes the vanishing gradient problem in deep nets and propose an efficient method to reduce the bias.","paperhash":"anonymous|linearly_constrained_weights_resolving_the_vanishing_gradient_problem_by_reducing_angle_bias","_bibtex":"@article{\n  anonymous2018linearly,\n  title={Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HylgYB3pZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper58/Authors"],"keywords":["vanishing gradient problem","multilayer perceptron","angle bias"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}