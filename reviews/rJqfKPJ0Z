{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222566398,"tcdate":1511897933126,"number":3,"cdate":1511897933126,"id":"BkrIo4ixG","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental but interesting results for adversarial examples","rating":"5: Marginally below acceptance threshold","review":"In this paper the authors present a new method for generating adversarial examples by constraining the perturbations to fall in a bounded region.  Further, experimentally, they demonstrate that learning the perturbations to balance errors against multiple classifiers can overcome many common defenses used against adversarial examples.\n\nPros:\n- Simple, easy to apply technique\n- Positive results in a wide variety of settings.\n\nCons:\n- Writing is a bit awkward at points.\n- Approach seems fairly incremental.\n\nOverall, the results are interesting but the technique seems relatively incremental.\n\nDetails:\n\n\"To find the center of domain definition...\" paragraph should probably go after the cases are described.  Confusing as to what is being referred to where it currently is written.\n\nTable 1: please use periods not commas (as in Table 2), e.g. 96.1 not 96,1\n\ninexistent --> non-existent\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1512222566890,"tcdate":1511887133620,"number":2,"cdate":1511887133620,"id":"ryU7ZMsgf","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting reparametrization, but too little experimental support","rating":"4: Ok but not good enough - rejection","review":"This paper presents a reparametrization of the perturbation applied to features in adversarial examples based attacks. It tests this attack variation on against Inception-family classifiers on ImageNet. It shows some experimental robustness to JPEG encoding defense.\n\nSpecifically about the method: Instead of perturbating a feature x_i by delta_i, as in other attacks, with delta_i in range [-Delta_i, Delta_i], they propose to perturbate x_i^*, which is recentered in the domain of x_i through a heuristic ((x_i Â± Delta_i + domain boundary that would be clipped)/2), and have a similar heuristic for computing a Delta_i^*. Instead of perturbating x_i^* directly by delta_i, they compute the perturbed x by x_i^* + Delta_i^* * g(r_i), so they follow the gradient of loss to misclassify w.r.t. r (instead of delta). \n\n+/-:\n+ The presentation of the method is clear.\n+ ImageNet is a good dataset to benchmark on.\n- (!) The (ensemble) white-box attack is effective but the results are not compared to anything else, e.g. it could be compared to (vanilla) FGSM nor C&W.\n- The other attack demonstrated is actually a grey-box attack, as 4 out of the 5 classifiers are known, they are attacking the 5th, but in particular all the 5 classifiers are Inception-family models.\n- The experimental section is a bit sloppy at times (e.g. enumerating more than what is actually done, starting at 3.1.1.).\n- The results on their JPEG approximation scheme seem too explorative (early in their development) to be properly compared.\n\nI think that the paper need some more work, in particular to make more convincing experiments that the benefit lies in CIA (baselines comparison), and that it really is robust across these defenses shown in the paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1512222566927,"tcdate":1511826937781,"number":1,"cdate":1511826937781,"id":"B1fZIQcxM","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Clipping Free Attacks Against Neural Networks ","rating":"3: Clear rejection","review":"The paper is not anonymized. In page 2, the first line, the authors revealed [15] is a self-citation and [15] is not anonumized in the reference list.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1509739467401,"tcdate":1509026065718,"number":133,"cdate":1509739464751,"id":"rJqfKPJ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}