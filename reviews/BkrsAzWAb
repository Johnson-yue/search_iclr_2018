{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222548253,"tcdate":1511833189294,"number":3,"cdate":1511833189294,"id":"BJ6v0V9ef","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Official_Review","forum":"BkrsAzWAb","replyto":"BkrsAzWAb","signatures":["ICLR.cc/2018/Conference/Paper1073/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting idea, but weak experiments","rating":"7: Good paper, accept","review":"\nThis paper revisits an interesting and important trick to automatically adapt the stepsize. They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize. Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice.\n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance. But before that, there are several issues need to be addressed. \n\n1, the derivation of the update of \\alpha relies on the expectation formulation. I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick. \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing. \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments. Moreover, the empirical comparisons are only conducted on MNIST. To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet. \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes. Could you please explain why such phenomenon happens?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511526943610,"tcdate":1511526943610,"number":2,"cdate":1511526943610,"id":"BydQzcHxf","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Official_Comment","forum":"BkrsAzWAb","replyto":"r1HU3l1kf","signatures":["ICLR.cc/2018/Conference/Paper1073/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1073/Authors"],"content":{"title":"Thank you","comment":"Hi, both are very interesting potential applications! \n\nI think an application to non-stationary data, where the learning rate varies on the fly as new data comes in, would be very interesting indeed. We will keep this in mind. \n\nWe're also looking at adaptive filter theory.\n\nThank you very much for the pointers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511526063526,"tcdate":1511293955195,"number":1,"cdate":1511293955195,"id":"S1sZVWMlz","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Official_Comment","forum":"BkrsAzWAb","replyto":"rkaXMT-lz","signatures":["ICLR.cc/2018/Conference/Paper1073/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1073/Authors"],"content":{"title":"why multiplicative adaptation is in general faster than the additive adaptation？","comment":"You only need a logaritmic number of iterations to shift your current learning rate to another value, instead of a linear number of them. We have also seen in practice that with good hyperparameters for both implementations, the multiplicative rule adapts faster. There is also a theoretical reason that comes from the formal derivation of the rule that suggests that the multiplicative rule makes more sense than the additive one."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511277093098,"tcdate":1511277093098,"number":2,"cdate":1511277093098,"id":"rkaXMT-lz","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Public_Comment","forum":"BkrsAzWAb","replyto":"BkrsAzWAb","signatures":["~Kai_Li2"],"readers":["everyone"],"writers":["~Kai_Li2"],"content":{"title":"why multiplicative adaptation is in general faster than the additive adaptation？","comment":" One of the practical advantages of this multiplicative\nrule is that it is invariant up to rescaling and that the multiplicative adaptation is in general faster than\nthe additive adaptation. Why?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222548297,"tcdate":1510948435021,"number":2,"cdate":1510948435021,"id":"r1jLC23Jf","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Official_Review","forum":"BkrsAzWAb","replyto":"BkrsAzWAb","signatures":["ICLR.cc/2018/Conference/Paper1073/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Somewhat weak novelty, but well written, complete, and potentially impactful.","rating":"7: Good paper, accept","review":"The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update. The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha.\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization).\n\nThe experiments are well-presented, and appear to convincingly show a benefit. Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two).\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one. This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222548340,"tcdate":1510554372703,"number":1,"cdate":1510554372703,"id":"H1pbs28kG","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Official_Review","forum":"BkrsAzWAb","replyto":"BkrsAzWAb","signatures":["ICLR.cc/2018/Conference/Paper1073/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good, but not perfect","rating":"6: Marginally above acceptance threshold","review":"SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent. The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.\n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty. The authors are well aware of this. They still manage to provide added value.\nDespite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum. However, it is not clear why the method is tested only on a single data set: MNIST. Since it is entirely general, I would rather expect a test on a dozen different data sets. That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.\n\nThe extensions in section 5 don't seem to be very useful. In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem. Analyzing the actual adaptive algorithm would be very interesting. In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011).\n\nNone of the figures is legible on a grayscale printout of the paper. Please do not use color as the only cue to identify a curve.\n\nIn figure 2, top row, please display the learning rate on a log scale.\n\npage 8, line 7 in section 4.3: \"the the\" (unintended repetition)\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510046797377,"tcdate":1510046797377,"number":1,"cdate":1510046797377,"id":"r1HU3l1kf","invitation":"ICLR.cc/2018/Conference/-/Paper1073/Public_Comment","forum":"BkrsAzWAb","replyto":"BkrsAzWAb","signatures":["~Ricardo_Pio_Monti1"],"readers":["everyone"],"writers":["~Ricardo_Pio_Monti1"],"content":{"title":"Nice paper!","comment":"Dear authors,\n\nThank you for this paper, I really enjoyed it! :)\n\nI have two small comments:\n\n - A related field which may provide additional insights in that of Adaptive filter theory [1]. A particularly relevant example would be the use of adaptive forgetting factors, where gradient information is used to tune a forgetting factor recursively.\n\n - A further interesting application for the proposed method could be in the context of non-stationary data. In such a setting, it may be desirable to allow the learning to rate to increase if necessary (as would be the case if, for example, the underlying data distribution changed). Potential scenarios where this could happen are streaming data applications (where model parameters are constantly updated to take into consideration new observations/drifts in the distribution) or transfer learning applications. \n\nBest wishes and good luck!\n\nReferences:\n1. Adaptive Filter Theory, Simon Haykin, Prentice Hall, 2008"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092381165,"tcdate":1509138151755,"number":1073,"cdate":1510092360190,"id":"BkrsAzWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkrsAzWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Online Learning Rate Adaptation with Hypergradient Descent","abstract":"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.","pdf":"/pdf/99866a78dad33243f7634f3657ba2bbbc7825f79.pdf","paperhash":"anonymous|online_learning_rate_adaptation_with_hypergradient_descent","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Learning Rate Adaptation with Hypergradient Descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrsAzWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1073/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}