{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222559431,"tcdate":1512034101571,"number":3,"cdate":1512034101571,"id":"SkA4yLpxf","invitation":"ICLR.cc/2018/Conference/-/Paper1142/Official_Review","forum":"BJoBfQ-0b","replyto":"BJoBfQ-0b","signatures":["ICLR.cc/2018/Conference/Paper1142/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very relevant paper for NLP researchers. ","rating":"5: Marginally below acceptance threshold","review":"\nThe paper does a very interesting and much relevant job at systematically investigating the various architectural choices and optimization settings of sequence labeling tasks for natural language processing. \n\nInvestigation concernes the use of CNN and LSTM, the various strategies for embedding refinement etc. \n\nBesides investigating systematically the different choices one has to make when designing a sequence labeling system for NLP the paper finally ends with new state of the art systems for the two tasks handled here, named entity recognition and pos tagging. This study is extensive and well explained and of clear relevance for any NLP practitioner.\n\nThe real strength of the paper is linked to the genericity of the conclusions. May we generalize this study’s conclusions to other datasets and other NLP tasks ?   \n\nActually, the conclusions are drawn from the experimental results gained in two datasets. These datasets are well known ones in the NLP field for POS tagging and for Named Entity Recognition. Yet, not being a specialist of natural language processing, i am not able to evaluate if conclusions drawn from results on these two datasets and on these two tasks may be trusted for other datasets and other NLP tasks. Which should be a strength of the paper. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Representation Methods for Sequence Labeling","abstract":"Sequence labeling is a general task encompassing a variety of applications in natural language processing, such as part-of-speech tagging and named entity recognition. Recent advances in representation learning can automatically encode word-level and character-level information. They allow neural networks to achieve the state-of-the-art without domain-specific feature engineering. However, the effectiveness of character-level representation modules is not clear, and how to leverage pre-trained embeddings have not been well studied. Therefore we first compare popular character-level representation modules with controlled experiments. From the results, we observed LSTM-based modules achieved better performance than CNN-based modules. Such comparison allows us to better understand existing representation modules and achieve further improvements. Also, we proposed a novel word-wise dropout strategy to carefully fine-tune pre-trained embeddings without shifting the whole semantic space. The resulting representation components help the sequence labeling model achieve the new state-of-the-art on three benchmark datasets. For further studies and improvements, we would release all implementations and codes to the public.","pdf":"/pdf/efa84800de59a703122ea1f328a6a3c1031e1cfa.pdf","paperhash":"anonymous|exploring_representation_methods_for_sequence_labeling","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Representation Methods for Sequence Labeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJoBfQ-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1142/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222559474,"tcdate":1511814405901,"number":2,"cdate":1511814405901,"id":"r10bSl9gf","invitation":"ICLR.cc/2018/Conference/-/Paper1142/Official_Review","forum":"BJoBfQ-0b","replyto":"BJoBfQ-0b","signatures":["ICLR.cc/2018/Conference/Paper1142/AnonReviewer1"],"readers":["everyone"],"content":{"title":"comparison of CRF sequence tagging architectures","rating":"4: Ok but not good enough - rejection","review":"This paper does a systematic comparison of different architectures for learning representations for bigram CRF tagging models, looking at NER mostly. Additionally, several strategies for fine-tuning word type embeddings are explored. The findings help show that LSTMs are slightly better on this task than CNNs, and that fine tuning can be productively included.\n\nSome questions I had were how the gradients were scaled during dropout (or at test time when dropout is presumably disabled). Normally neurons are dropped out to zero, but here they were frequently reverted back to the original word embedding. In this case, the standard gradient transforms seem inappropriate. Were they done? Also, which embeddings were used at test time?\n\nAlso, I’m confused about the updating for common word embeddings. If the goal is to only update common word embeddings, I don’t see the point of the group lasso penalty. Also, do the settings of lambda that you provide actually zero out the proper name words and not the others? Also, what list is used to determine which group a word belongs to?\n\nOverall though, this is a useful set of results that will be helpful for people implementing chunking sequence models.\n\nSome suggestions for clarity:\nEq1 and 2 are computed using the forward algorithm, not the Viterbi algorithm. The generic term might be to say “a polynomial time dynamic programming algorithm”.\n\nSome missing citations for sequence labeling with character models (some of the original work):\ndos Santos and Zadrozny. (2014 ICML) http://proceedings.mlr.press/v32/santos14.pdf\nLing et al. (2015 EMNLP) Finding Function in Form. https://arxiv.org/abs/1508.02096","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Representation Methods for Sequence Labeling","abstract":"Sequence labeling is a general task encompassing a variety of applications in natural language processing, such as part-of-speech tagging and named entity recognition. Recent advances in representation learning can automatically encode word-level and character-level information. They allow neural networks to achieve the state-of-the-art without domain-specific feature engineering. However, the effectiveness of character-level representation modules is not clear, and how to leverage pre-trained embeddings have not been well studied. Therefore we first compare popular character-level representation modules with controlled experiments. From the results, we observed LSTM-based modules achieved better performance than CNN-based modules. Such comparison allows us to better understand existing representation modules and achieve further improvements. Also, we proposed a novel word-wise dropout strategy to carefully fine-tune pre-trained embeddings without shifting the whole semantic space. The resulting representation components help the sequence labeling model achieve the new state-of-the-art on three benchmark datasets. For further studies and improvements, we would release all implementations and codes to the public.","pdf":"/pdf/efa84800de59a703122ea1f328a6a3c1031e1cfa.pdf","paperhash":"anonymous|exploring_representation_methods_for_sequence_labeling","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Representation Methods for Sequence Labeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJoBfQ-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1142/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222559514,"tcdate":1511733528310,"number":1,"cdate":1511733528310,"id":"H1g7FnueM","invitation":"ICLR.cc/2018/Conference/-/Paper1142/Official_Review","forum":"BJoBfQ-0b","replyto":"BJoBfQ-0b","signatures":["ICLR.cc/2018/Conference/Paper1142/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Minor experimental findings on existing sequence labeling architectures","rating":"3: Clear rejection","review":"The paper conducts empirical comparisons between well-known architectures for sequence labeling. Specifically, it compares LSTM-CNN-CRF (Ma and Hovy, 2016) and LSTM-CRF (Lample et al., 2016), combined with character-based architectures based on CNN or LSTM, and also different regularization techniques including a new word-level dropout scheme. \n\nQuality: Low, minor/incremental contribution due to lack of novelty and lack of significantly useful findings. \n\nClarity: Readable with some typos (e.g., \"let the model being aware...\"). Experimental results are thoroughly tabulated. \n\nOriginality: Very low. The finding that LSTMs have advantages over CNNs in modeling characters is not new (see the last paragraph of Section 4.1 of Lample et al.), though it's good to have empirical verification. One of the only novel components is the word-wise dropout (Table 5), but the improvement is minor and the technique is too thin to be considered substantial novelty. \n\nSignificance: The reported findings will be useful for practitioners who are specifically interested in incrementally improving sequence labeling performance. Otherwise, there isn't much depth. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Representation Methods for Sequence Labeling","abstract":"Sequence labeling is a general task encompassing a variety of applications in natural language processing, such as part-of-speech tagging and named entity recognition. Recent advances in representation learning can automatically encode word-level and character-level information. They allow neural networks to achieve the state-of-the-art without domain-specific feature engineering. However, the effectiveness of character-level representation modules is not clear, and how to leverage pre-trained embeddings have not been well studied. Therefore we first compare popular character-level representation modules with controlled experiments. From the results, we observed LSTM-based modules achieved better performance than CNN-based modules. Such comparison allows us to better understand existing representation modules and achieve further improvements. Also, we proposed a novel word-wise dropout strategy to carefully fine-tune pre-trained embeddings without shifting the whole semantic space. The resulting representation components help the sequence labeling model achieve the new state-of-the-art on three benchmark datasets. For further studies and improvements, we would release all implementations and codes to the public.","pdf":"/pdf/efa84800de59a703122ea1f328a6a3c1031e1cfa.pdf","paperhash":"anonymous|exploring_representation_methods_for_sequence_labeling","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Representation Methods for Sequence Labeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJoBfQ-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1142/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092379840,"tcdate":1509139011021,"number":1142,"cdate":1510092359497,"id":"BJoBfQ-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJoBfQ-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring Representation Methods for Sequence Labeling","abstract":"Sequence labeling is a general task encompassing a variety of applications in natural language processing, such as part-of-speech tagging and named entity recognition. Recent advances in representation learning can automatically encode word-level and character-level information. They allow neural networks to achieve the state-of-the-art without domain-specific feature engineering. However, the effectiveness of character-level representation modules is not clear, and how to leverage pre-trained embeddings have not been well studied. Therefore we first compare popular character-level representation modules with controlled experiments. From the results, we observed LSTM-based modules achieved better performance than CNN-based modules. Such comparison allows us to better understand existing representation modules and achieve further improvements. Also, we proposed a novel word-wise dropout strategy to carefully fine-tune pre-trained embeddings without shifting the whole semantic space. The resulting representation components help the sequence labeling model achieve the new state-of-the-art on three benchmark datasets. For further studies and improvements, we would release all implementations and codes to the public.","pdf":"/pdf/efa84800de59a703122ea1f328a6a3c1031e1cfa.pdf","paperhash":"anonymous|exploring_representation_methods_for_sequence_labeling","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Representation Methods for Sequence Labeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJoBfQ-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1142/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}