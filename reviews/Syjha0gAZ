{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222659549,"tcdate":1511927793213,"number":3,"cdate":1511927793213,"id":"SJYge3sxG","invitation":"ICLR.cc/2018/Conference/-/Paper469/Official_Review","forum":"Syjha0gAZ","replyto":"Syjha0gAZ","signatures":["ICLR.cc/2018/Conference/Paper469/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Technical contribution of the paper is marginal because of the lack of reliable mathematical discussion or investigation.","rating":"4: Ok but not good enough - rejection","review":"Summary: \nThe paper considers the prediction problem where labels are given as multisets. The authors give a definition of a loss function for multisets and show experimental results. The results show that the proposed methods optimizing the loss function perform better than other alternatives.\n\nComments: \nThe problem of predicting multisets looks challenging and interesting. The experimental results look nice. On the other hand, I have several concerns about writing and technical discussions. \n\nFirst of all, the goal of the problem is not exactly stated. After I read the experimental section, I realized that the goal is to optimize the exact match score (EM) or F1 measure w.r.t. the ground truth multisets. This goal should be explicitly stated in the paper. Now then, the approach of the paper is to design surrogate loss functions to optimize these criteria. \n\nThe technical discussions for defining the proposed loss function seems not reliable for the reasons below. Therefore, I do not understand the rationale of the definition of the proposed loss function.:  \n- An exact definition of the term multiset is not given. If I understand it correctly, a multiset is a “set” of instances allowing duplicated ones. \n- There is no definition of Prec or Rec (which look like Precision and Recall) in Remark 1. The definitions appear in Appendix, which might not be well-defined. For example, let y, Y be mutisets , y=[a, a, a] and Y = [a, b]. Then, by definition, Prec(y,Y)=3/3 =1. Is this what you meant? (Maybe, the ill-definedness  comes from the lack of definition of inclusion in a mutiset.) \n- I cannot follow the proof of Remark 1 since it does not seem to take account of the randomness by the distribution \\pi^*. \n- I do not understand the definition of the oracle policy exactly. It seems to me that, the oracle policy knows the correct label (multi-set) \\calY for each instance x and use it to construct \\calY_t. But, this implicit assumption is not explicitly mentioned. \n- In (1), (2) and Definition 3, what defines \\calY_t? If \\calY_t is determined by some “optimal” oracle, you cannot define the loss function in Def. 3 since it is not known a priori. Or, if the learner determines \\calY_t, I don’t understand why the oracle policy is optimal since it depends on the learner’s choices. \n\nAlso, I expect an investigation of theoretical properties of the proposed loss function, e.g., relationship to EM or F1 or other loss functions. Without understanding the theoretical properties and the rationale, I cannot judge the goodness of the experimental results (look good though). In other words, I cannot judge the paper in a qualitative perspective, not in a quantitative view. \n\nAs a summary, I think the technical contribution of the paper is marginal because of the lack of reliable mathematical discussion or investigation.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss Functions for Multiset Prediction","abstract":"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.","pdf":"/pdf/88bb5a8fbbf8a891189d62df636875735b72378b.pdf","TL;DR":"We study the problem of multiset prediction and propose a novel multiset loss function, providing analysis and empirical evidence that demonstrates its effectiveness.","paperhash":"anonymous|loss_functions_for_multiset_prediction","_bibtex":"@article{\n  anonymous2018loss,\n  title={Loss Functions for Multiset Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syjha0gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper469/Authors"],"keywords":["machine learning","deep learning","structured prediction","sequential prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222659586,"tcdate":1511821581870,"number":2,"cdate":1511821581870,"id":"SJVGWfceM","invitation":"ICLR.cc/2018/Conference/-/Paper469/Official_Review","forum":"Syjha0gAZ","replyto":"Syjha0gAZ","signatures":["ICLR.cc/2018/Conference/Paper469/AnonReviewer2"],"readers":["everyone"],"content":{"title":"multiset prediction as sequential decision making","rating":"7: Good paper, accept","review":"This is an interesting paper, in the sense of looking at a problem such as multiset prediction in the context of sequential decision making (using a policy).  \n\nIn more detail, the authors construct an oracle policy, shown to be optimal (in terms of precision and recall).  A parametrized policy instead of the oracle policy is utilized in the proposed multiset loss function, while furthermore, a termination policy facilitates the application on variable-sized multiset targets.  The authors also study other loss functions, ordered sequence prediction as well as reinforcement learning.\n\nResults show that the proposed order-invariant loss outperforms other losses, along with a set if experiments evaluating choice of rank function for sequence prediction and selection strategies.  The experiments seem rather comprehensive, as well as the theoretical analysis.   The paper describes an interesting approach to the problem.\n\nWhile the paper is comprehensive it could be improved in terms of clarity & flow (e.g., by better preparing the reader on what is to follow)","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss Functions for Multiset Prediction","abstract":"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.","pdf":"/pdf/88bb5a8fbbf8a891189d62df636875735b72378b.pdf","TL;DR":"We study the problem of multiset prediction and propose a novel multiset loss function, providing analysis and empirical evidence that demonstrates its effectiveness.","paperhash":"anonymous|loss_functions_for_multiset_prediction","_bibtex":"@article{\n  anonymous2018loss,\n  title={Loss Functions for Multiset Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syjha0gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper469/Authors"],"keywords":["machine learning","deep learning","structured prediction","sequential prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222661199,"tcdate":1511487440960,"number":1,"cdate":1511487440960,"id":"rktAPxrlG","invitation":"ICLR.cc/2018/Conference/-/Paper469/Official_Review","forum":"Syjha0gAZ","replyto":"Syjha0gAZ","signatures":["ICLR.cc/2018/Conference/Paper469/AnonReviewer3"],"readers":["everyone"],"content":{"title":"While the proposed solution is intuitive, and can possibly be useful, the technical part of this paper is a bit weak.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a type of loss functions for the problem of multiset prediction. A detailed discussion on the intuition is provided and extensive experiments are conducted to show that this loss function indeed provides some performance gain in terms of Exact Matching and F1-score.\n\nThe idea of this paper is as follows: instead of viewing the multiset prediction task as a classification problem, this paper models it as a sequential decision problem (this idea is not new, see Welleck et al., 2017, as pointed out by the authors).  Define pi* to be the optimal policy that outputs the labels of input x in a certain way. We learn a parameterized policy pi(theta) which takes x and all previous predictions as input, and outputs a label as a new prediction. At each time step t, pi* and pi(theta) can be viewed as distributions over all remaining labels; the KL divergence is then used to calculate the difference between those two distributions. Finally, the loss function sums those KL divergences over all t. Computing this loss function directly can be intractable, so the author suggested that one can compute the entire trajectory of predictions and then do aggregation.\n\nAdmitted that such construction is quite intuitive, and can possibly be useful, the technical part of this paper seems to be rather straightforward. In particular, I think the solution to the issue of unknown T is very heuristic, making the proposed loss function less principled. \n\n\nOther Detailed Comments/Issues:\n\n-- It seems to me that models that can utilize this loss function must support varying-length inputs, e.g. LSTM. Any idea how to apply it to models with fixed-length inputs?\n\n-- The proposed loss function assumes that T (i.e. the size of the output set) is known, which is unrealistic. To handle this, the authors simply trained an extra binary classifier that takes x and all previous predictions as the input at each time step, and decides whether or not to terminate. I think this solution is rather hacky and I’d like to see a more elegant solution, e.g. incorporate the loss on T into the proposed loss function.\n\n-- Could you formally define “Exact Match”?\n\n-- In Section 4.4, maybe it is better to run the stochastic sampling multiple times to reduce the variance? I would expect that the result can be quite different in different runs.\n\n-- Any discussion on how will the classifier for T affect the experimental results? \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss Functions for Multiset Prediction","abstract":"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.","pdf":"/pdf/88bb5a8fbbf8a891189d62df636875735b72378b.pdf","TL;DR":"We study the problem of multiset prediction and propose a novel multiset loss function, providing analysis and empirical evidence that demonstrates its effectiveness.","paperhash":"anonymous|loss_functions_for_multiset_prediction","_bibtex":"@article{\n  anonymous2018loss,\n  title={Loss Functions for Multiset Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syjha0gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper469/Authors"],"keywords":["machine learning","deep learning","structured prediction","sequential prediction"]}},{"tddate":null,"ddate":null,"tmdate":1509739285022,"tcdate":1509121459098,"number":469,"cdate":1509739282357,"id":"Syjha0gAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Syjha0gAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Loss Functions for Multiset Prediction","abstract":"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.","pdf":"/pdf/88bb5a8fbbf8a891189d62df636875735b72378b.pdf","TL;DR":"We study the problem of multiset prediction and propose a novel multiset loss function, providing analysis and empirical evidence that demonstrates its effectiveness.","paperhash":"anonymous|loss_functions_for_multiset_prediction","_bibtex":"@article{\n  anonymous2018loss,\n  title={Loss Functions for Multiset Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Syjha0gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper469/Authors"],"keywords":["machine learning","deep learning","structured prediction","sequential prediction"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}