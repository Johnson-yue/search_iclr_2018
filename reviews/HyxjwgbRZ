{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222699817,"tcdate":1511785177912,"number":3,"cdate":1511785177912,"id":"rkMJQKYxz","invitation":"ICLR.cc/2018/Conference/-/Paper601/Official_Review","forum":"HyxjwgbRZ","replyto":"HyxjwgbRZ","signatures":["ICLR.cc/2018/Conference/Paper601/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Preliminary work that requires further investigation","rating":"4: Ok but not good enough - rejection","review":"\n The paper explores SignGD --- an algorithm that uses the sign of the gradients instead of actual gradients for training deep models. The authors provide some guarantees regarding the convergence of SignGD to local minima in the stochastic optimization setting, and later compare SignSG to GD in two deep learning tasks.\n\nExploring signSGD is an important and interesting line of research, and this paper provides some preliminary result in this direction.\nHowever, in my view, this work is too preliminary and not ready for publish. This is since the authors do not illustrate any clear benefits of signSGD over SGD neither in theory nor in practice. I elaborate on this below:\n\n-The theory part shows that under some conditions, signGD  finds a local minima. Yet, as the authors themselves \nmention, the dependence on the dimension is much worse compared to SGD.\nMoreover, the authors do not mention that if the noise variance does not scale with the dimension (as is often the case), then the convergence of SGD will not depend on the dimension, while it seems that the convergence of signGD will still depend on the dimension.\n\n-The experiments are nice as a preliminary investigation, but not enough in order to illustrate the benefits of signSGD over SGD. In order to do so, the authors should make a more extensive experimental study.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convergence rate of sign stochastic gradient descent for non-convex functions","abstract":"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates. Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors. For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions. We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension. We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","pdf":"/pdf/ef31306ae2508e5857c25b9fb03c6809fedfe2a9.pdf","TL;DR":"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning.","paperhash":"anonymous|convergence_rate_of_sign_stochastic_gradient_descent_for_nonconvex_functions","_bibtex":"@article{\n  anonymous2018convergence,\n  title={Convergence rate of sign stochastic gradient descent for non-convex functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyxjwgbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper601/Authors"],"keywords":["sign","stochastic","gradient","non-convex","optimization","gradient","quantization","convergence","rate"]}},{"tddate":null,"ddate":null,"tmdate":1512222699860,"tcdate":1511648756921,"number":2,"cdate":1511648756921,"id":"Sy6g0wDxz","invitation":"ICLR.cc/2018/Conference/-/Paper601/Official_Review","forum":"HyxjwgbRZ","replyto":"HyxjwgbRZ","signatures":["ICLR.cc/2018/Conference/Paper601/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Would rate more confidently, if stronger numerical experiments are present :) and Assumption 3 is more explained and defended","rating":"5: Marginally below acceptance threshold","review":"In this paper, authors provided a convergence analysis of Sign SGD algorithm for non-covex case.\nThe crucial assumption for the proof was Assumption 3, otherwise, the proof technique is following a standard path in non-convex optimization.   \n\nIn general, the paper is written nicely, easy to follow.\n\n==============================================\n\"The major issue\":\nWhy Assumption 3 can be problematic in practice is given below:\nLet us assume just a convex case and assume we have just 2 kids of function in 2D:  f_1(x) = 0.5 x_1^2 and f_2(x) = 0.5 x_2^2.\nThen define the function f(x) = E [ f_i(x)  ].   where $i =1$  with prob 0.5 and $i=2$ with probability 0.5. \nWe have that   g(x) = 0.5 [ x_1, x_2 ]^T.\nLet us choose $i=1$ and choose $x = [a,a]^T$, where $a$ is some parameter.\n\nThen (4) says, that there has to exist a $\\sigma$ such that\nP [   | \\bar g_i(x) - g_i(x) | > t ] \\leq 2 exp( - t^2 / 2\\sigma^2).  forall \"x\".\n\nplugging our function inside it should be true that\n\nP [   | [ B ] - 0.5 a | > t ] \\leq 2 exp( - t^2 / 2\\sigma^2).  forall \"x\".\nwhere B is a random variable which has value \"a\" with probability 0.5 and value \"0\" with probability 0.5.\n\nIf we choose $t = 0.1a$ then we have that it has to be true that\n\n1 = P [   | [ B ] - 0.5 a | > 0.1a ] \\leq 2 exp( - 0.01 a^2 / 2\\sigma^2)   ---->  0 as $a \\to \\infty$.\n\nHence, even in this simple example, one can show that this assumption is violated unless $\\sigma = \\infty$.\n\nOne way to ho improve this is to put more assumption + maybe put some projection into a compact set?\n==============================================\n\nHence, I think the theory should be improved.\n\nIn terms of experiments, I like the discussion about escaping saddle points, it is indeed a good discussion. However, it would be nicer to have more numerical experiments.\nOne thing I am also struggling is the \"advantage\" of using signSGD: one saves on communication (instead of sending 4*8 bits per dimension, one just send only 1 bit, however, one needs \"d\"times more iterations, hence, the theory shows that it is much worse then SGD (see (11) ).\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convergence rate of sign stochastic gradient descent for non-convex functions","abstract":"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates. Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors. For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions. We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension. We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","pdf":"/pdf/ef31306ae2508e5857c25b9fb03c6809fedfe2a9.pdf","TL;DR":"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning.","paperhash":"anonymous|convergence_rate_of_sign_stochastic_gradient_descent_for_nonconvex_functions","_bibtex":"@article{\n  anonymous2018convergence,\n  title={Convergence rate of sign stochastic gradient descent for non-convex functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyxjwgbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper601/Authors"],"keywords":["sign","stochastic","gradient","non-convex","optimization","gradient","quantization","convergence","rate"]}},{"tddate":null,"ddate":null,"tmdate":1512222699903,"tcdate":1511458940447,"number":1,"cdate":1511458940447,"id":"S1CO_KVez","invitation":"ICLR.cc/2018/Conference/-/Paper601/Official_Review","forum":"HyxjwgbRZ","replyto":"HyxjwgbRZ","signatures":["ICLR.cc/2018/Conference/Paper601/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not correct","rating":"2: Strong rejection","review":"The paper presents convergence rate of a quantized SGD, with biased quantization - simply taking a sign of each element of gradient.\n\nThe stated Theorem 1 is incorrect. Even if the stated result was correct, it presents much worse rate for a weaker notion of convergence.\n\nMajor flaws:\n1. As far as I can see, Theorem 1 should depend on 4th root of N_K, the last (omitted) step from the proof is done incorrectly. This makes it much worse than presented.\n2. Even if this was correct, the main point is that this is \"only\" d times worse - see eq (11). That is enormous difference, particularly in settings where such gradient compression can be relevant. Also, it is lot more worse than just d times:\n3. Again in eq (11), you compare different notions of convergence - E[||g||_1]^2 vs. E[||g||_2^2]. In particular, the one for signSGD is the weaker notion - squared L1 norm can be d times bigger again. If this is not the case for some reason, more detailed explanation is needed.\n\nOther than that, the paper contains several attempts at intuitive explanation, which I don't find correct. Inclusion of Assumption 3 would in particular require better justification.\n\nExperiments are also inconclusive, as the plots show convergence to significantly worse accuracy than what the models converged to in original contributions.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convergence rate of sign stochastic gradient descent for non-convex functions","abstract":"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates. Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors. For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions. We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension. We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","pdf":"/pdf/ef31306ae2508e5857c25b9fb03c6809fedfe2a9.pdf","TL;DR":"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning.","paperhash":"anonymous|convergence_rate_of_sign_stochastic_gradient_descent_for_nonconvex_functions","_bibtex":"@article{\n  anonymous2018convergence,\n  title={Convergence rate of sign stochastic gradient descent for non-convex functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyxjwgbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper601/Authors"],"keywords":["sign","stochastic","gradient","non-convex","optimization","gradient","quantization","convergence","rate"]}},{"tddate":null,"ddate":null,"tmdate":1509739208248,"tcdate":1509128087753,"number":601,"cdate":1509739205593,"id":"HyxjwgbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyxjwgbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Convergence rate of sign stochastic gradient descent for non-convex functions","abstract":"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates. Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors. For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions. We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension. We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","pdf":"/pdf/ef31306ae2508e5857c25b9fb03c6809fedfe2a9.pdf","TL;DR":"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning.","paperhash":"anonymous|convergence_rate_of_sign_stochastic_gradient_descent_for_nonconvex_functions","_bibtex":"@article{\n  anonymous2018convergence,\n  title={Convergence rate of sign stochastic gradient descent for non-convex functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyxjwgbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper601/Authors"],"keywords":["sign","stochastic","gradient","non-convex","optimization","gradient","quantization","convergence","rate"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}