{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222569613,"tcdate":1511862000971,"number":3,"cdate":1511862000971,"id":"HyYek2cgM","invitation":"ICLR.cc/2018/Conference/-/Paper138/Official_Review","forum":"S1680_1Rb","replyto":"S1680_1Rb","signatures":["ICLR.cc/2018/Conference/Paper138/AnonReviewer1"],"readers":["everyone"],"content":{"title":"We find this work is interesting, timely, and of good quality to be presented in ICLR","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper is on construction graph CNN using spectral techniques. The originality of this work is the use of Cayley polynomials to compute spectral filters on graphs, related to the work of Defferrard et al. (2016) and Monto et al. (2017) where Chebyshev filters were used. Theoretical and experimental results show the relevance of the Cayley polynomials as filters for graph CNN.\n\nThe paper is well written, and connections to related works are highlighted. We recommend the authors to talk about some future work.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS","abstract":"The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. \nIn this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach on spectral image classification, community detection, vertex classification and matrix completion tasks.","pdf":"/pdf/383bfb1a55ba0a1d0478d51b0476504b4ac70a3c.pdf","TL;DR":"A spectral graph convolutional neural network with spectral zoom properties.","paperhash":"anonymous|cayleynets_spectral_graph_cnns_with_complex_rational_filters","_bibtex":"@article{\n  anonymous2018cayleynets:,\n  title={CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1680_1Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper138/Authors"],"keywords":["Deep Learning","Spectral Graph Convolutional Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222569652,"tcdate":1511841432821,"number":2,"cdate":1511841432821,"id":"BJWjA85xz","invitation":"ICLR.cc/2018/Conference/-/Paper138/Official_Review","forum":"S1680_1Rb","replyto":"S1680_1Rb","signatures":["ICLR.cc/2018/Conference/Paper138/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Propose new filters based on Cayley transform -- interesting filter, but unconvincing theory / experiments","rating":"4: Ok but not good enough - rejection","review":"Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results.\n\nOpinion: Though the Cayley filters seem to have interesting properties,  I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below:\n1. The comparison to Chebyshev filters  (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly).\n\n2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison.\n\n3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them.\nAlso, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter.\n\nAmong the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. \n\nOverall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS","abstract":"The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. \nIn this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach on spectral image classification, community detection, vertex classification and matrix completion tasks.","pdf":"/pdf/383bfb1a55ba0a1d0478d51b0476504b4ac70a3c.pdf","TL;DR":"A spectral graph convolutional neural network with spectral zoom properties.","paperhash":"anonymous|cayleynets_spectral_graph_cnns_with_complex_rational_filters","_bibtex":"@article{\n  anonymous2018cayleynets:,\n  title={CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1680_1Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper138/Authors"],"keywords":["Deep Learning","Spectral Graph Convolutional Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222569692,"tcdate":1511693216833,"number":1,"cdate":1511693216833,"id":"rJKsozOxM","invitation":"ICLR.cc/2018/Conference/-/Paper138/Official_Review","forum":"S1680_1Rb","replyto":"S1680_1Rb","signatures":["ICLR.cc/2018/Conference/Paper138/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting new filter for graph CNNs, although experiments are not fully convincing","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new filter for spectral analysis on graphs for graph CNNs. The filter is a rational function based on the Cayley transform. Unlike other popular variants, it is not strictly supported on a small graph neighborhood, but the paper proves an exponential-decay property on the norm of a filtered vertex indicator function.\n\nThe paper argues that Cayley filters allow better spectral localization than Chebyshev filters. While Chebyshev filters can be applied efficiently using a recursive method, evaluation of a  Cayley filter of order r requires solving r linear system in dimension corresponding to the number of vertices, which is expensive. The paper proposes to stop after a small number of iterations of Jacobi's method to alleviate this problem.\n\nThe paper is clear and well written.\n\nThe proposed method seems of interest, although I find the experimental section only partly convincing. \n\nThere seems to be a tradeoff here. The paper demonstrates that CayleyNet achieves similar efficiency as ChebNet in multiple experiments while using smaller filter orders. Although using smaller filter orders (and better-localized filters) is an interesting property, it is not necessarily a key objective, especially as this seems to come at the cost of a significantly increased computational complexity. \n\nThe paper could help us understand this tradeoff better. For instance:\n- Middle and right panels of Figure 4 could use a more precise Y scale. How much slower is CayleyNet here with respect to the ChebNet?\n\n- Figure 4 mentions time corresponds to \"test times on batches of 100 samples\". Is this an average value over multiple 100-sample batches? What is the standard deviation? How do the training times compare?\n\n- MNIST accuracies are very similar (and near perfect) -- how did the training and testing time compare? Same for the MovieLens experiment. The improvement in performance is rather small, what is the corresponding computational cost?\n\n- CORA results are a bit confusing to me. The filter orders used here are very small, and the best amongst the values considered seems to be r=1. Is there a reason only such small values have been considered? Is this a fair evaluation of ChebNet which may possibly perform better with larger filter orders?\n\n- The paper could provide some insights as to why ChebNet is unable to work with unnormalized Laplacians while CayleyNet is (and why the ChebNet performance seems to get worse and worse as r increases?).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS","abstract":"The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. \nIn this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach on spectral image classification, community detection, vertex classification and matrix completion tasks.","pdf":"/pdf/383bfb1a55ba0a1d0478d51b0476504b4ac70a3c.pdf","TL;DR":"A spectral graph convolutional neural network with spectral zoom properties.","paperhash":"anonymous|cayleynets_spectral_graph_cnns_with_complex_rational_filters","_bibtex":"@article{\n  anonymous2018cayleynets:,\n  title={CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1680_1Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper138/Authors"],"keywords":["Deep Learning","Spectral Graph Convolutional Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739464657,"tcdate":1509031509079,"number":138,"cdate":1509739462000,"id":"S1680_1Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1680_1Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS","abstract":"The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. \nIn this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach on spectral image classification, community detection, vertex classification and matrix completion tasks.","pdf":"/pdf/383bfb1a55ba0a1d0478d51b0476504b4ac70a3c.pdf","TL;DR":"A spectral graph convolutional neural network with spectral zoom properties.","paperhash":"anonymous|cayleynets_spectral_graph_cnns_with_complex_rational_filters","_bibtex":"@article{\n  anonymous2018cayleynets:,\n  title={CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1680_1Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper138/Authors"],"keywords":["Deep Learning","Spectral Graph Convolutional Neural Networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}