{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222589107,"tcdate":1512193205301,"number":3,"cdate":1512193205301,"id":"r1ahhhJWM","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer3"],"readers":["everyone"],"content":{"title":"topic modeling + seq2seq","rating":"6: Marginally above acceptance threshold","review":"I enjoyed this paper a lot. The paper addresses the issue of enduring topicality in conversation models. The model proposed here is basically a mash-up between a neural topic model and a seq2seq-based dialog system. The exposition is relatively clear and a reader with sufficient background in ML should have no following the model. My only concern about the paper is that is very incremental in nature -- the authors combine two separate models into a relatively straight-forward way. The results do are good and validate the approach, but the paper has little to offer beyond that.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to the baseline models.","pdf":"/pdf/464db64959951ba5c86eddb57d695c7f25ac6f3b.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1512222589146,"tcdate":1511850357212,"number":2,"cdate":1511850357212,"id":"rypOZF5eG","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting combination of seq2seq and neural topic models, but weak evaluation","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a conversational model with topical information, by combining seq2seq model with neural topic models. The experiments and human evaluation show the model outperform some the baseline model seq2seq and the other latent variable model variant of seq2seq.\n\nThe paper is interesting, but it also has certain limitations:\n\n1) To my understanding, it is a straightforward combination of seq2seq and one of the neural topic models without any justification.\n2) The evaluation doesn't show how the topic information could influence word generation. No of the metrics in table 2 could be used to justify the effect of topical information.\n3) There is no analysis about the model behavior, therefore there is no way we could get a sense about how the model actually works. One possible analysis is to investigate the values $l_t$ and the corresponding words, which to some extent will tell us how the topical information be used in generation. In addition, it could be even better if there are some analysis about topics extracted by this model.\n\nThis paper also doesn't pay much attention to the existing work on topic-driven conversational modeling. For example \"Topic Aware Neural Response Generation\" from Xing et al., 2017.\n\nSome additional issues:\n\n1) In the second line under equation 4, y_{t-1} -> y_{t}\n2) In the first paragraph of section 3, two \"MLP\"'s are confusing\n3) In the first paragraph of page 6, words with \"highest inverse document frequency\" are used as stop words?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to the baseline models.","pdf":"/pdf/464db64959951ba5c86eddb57d695c7f25ac6f3b.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1512222589185,"tcdate":1511833163874,"number":1,"cdate":1511833163874,"id":"H1E8RNcxz","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The evaluation and details of experiments are not sufficient.","rating":"4: Ok but not good enough - rejection","review":"This paper proposed the combination of topic model and seq2seq conversational model.\nThe idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.\nHowever, I'm not sure that the empirical evaluation shows the really impressive results.\nIn particular, the difference between LV-S2S and LTCM seem to be trivial.\nThere are many configurations in the LSTM-based model.\nCan you say that there is no configuration of LV-S2S that outperforms your model?\nMoreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to the baseline models.","pdf":"/pdf/464db64959951ba5c86eddb57d695c7f25ac6f3b.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1509739429381,"tcdate":1509060682464,"number":207,"cdate":1509739426720,"id":"S1GUgxgCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to the baseline models.","pdf":"/pdf/464db64959951ba5c86eddb57d695c7f25ac6f3b.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}