{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222693873,"tcdate":1511817965695,"number":3,"cdate":1511817965695,"id":"ry8emW9gf","invitation":"ICLR.cc/2018/Conference/-/Paper581/Official_Review","forum":"H1sUHgb0Z","replyto":"H1sUHgb0Z","signatures":["ICLR.cc/2018/Conference/Paper581/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novel approach and a well written paper. Need more details on theoretical guarantees for multi-class settings and detailed analysis on real world data.","rating":"7: Good paper, accept","review":"The authors proposed a supervised learning algorithm for modeling label and worker quality. Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels.\n\nOverall the paper was well written. The motivation of the work is clearly explained and supported with relevant related work. The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion. Though limited to binary classification, the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk. The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results. \n\nMore detailed comments\n1. Instead of considering multi-class classification as one-vs-all binary classification, can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays.\n2. Can you introduce the Risk -R in the paper before using it in Theorem 4.1\n3. Is there any limit on how many examples each worker has to label? Can you comment more on how to pick that value in real-world settings? Just saying sufficiently many (Section 4.2) is not sufficient.\n4. Under the experiments, different variations of Majority Vote, EM and Oracle correction were used as baselines. Can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work section.\n5. For the experiments on synthetic datasets, workers are randomly sampled with replacements. Were the scores reported based on average of multiple runs. If yes, can you please report the error bars.\n6. For the MS-COCO, examples can you provide more detailed results as shown for synthetic datasets? Majority vote is a very weak baseline. \n\nFor the novel approach and the theoretical backing, I consider the paper to be a good one. The paper has scope for improvement.\n\n   ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning From Noisy Singly-labeled Data","abstract":"Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply). Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. ","pdf":"/pdf/24501d72afdf0c546b76d8632072343b8a2c6cf9.pdf","TL;DR":"A new approach for learning a model from noisy crowdsourced annotations.","paperhash":"anonymous|learning_from_noisy_singlylabeled_data","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning From Noisy Singly-labeled Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1sUHgb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper581/Authors"],"keywords":["crowdsourcing","noisy annotations","deep leaerning"]}},{"tddate":null,"ddate":null,"tmdate":1512222693914,"tcdate":1511745605526,"number":2,"cdate":1511745605526,"id":"SJpS_JYgz","invitation":"ICLR.cc/2018/Conference/-/Paper581/Official_Review","forum":"H1sUHgb0Z","replyto":"H1sUHgb0Z","signatures":["ICLR.cc/2018/Conference/Paper581/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A good paper with relatively pool experiments.","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item. The main framework is based on the Dawid-Skene model. By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels. The paper discusses the influence of the label redundancy both theoretically and empirically. Results show that with a fixed budget, it’s better to label many examples once rather than fewer examples multiple times.\n\nThe model and algorithm in this paper are simple and straightforward. However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy. The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed. The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing.\n\nAbout the technique details, this paper is clearly written, but some experimental comparisons and claims are not very convincing. Here I list some of my questions:\n+About the MBEM algorithm, it’s better to make clear the difference between MBEM and a standard EM. Will it always converge? What’s its objective?\n+The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical?\n+When n = O(m log m), the result that \\epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this?\n+During CIFAR-10 experiments when r=1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels?\n+For ImageNet and MS-COCO experiments with a fixed budget, you reduced the training set when increasing the redundancy, which is unfair. The reduction of performance could mainly cause by seeing fewer raw images, but not the labels. It’s better to train some semi-supervised model to make the settings more comparable.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning From Noisy Singly-labeled Data","abstract":"Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply). Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. ","pdf":"/pdf/24501d72afdf0c546b76d8632072343b8a2c6cf9.pdf","TL;DR":"A new approach for learning a model from noisy crowdsourced annotations.","paperhash":"anonymous|learning_from_noisy_singlylabeled_data","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning From Noisy Singly-labeled Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1sUHgb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper581/Authors"],"keywords":["crowdsourcing","noisy annotations","deep leaerning"]}},{"tddate":null,"ddate":null,"tmdate":1512222693958,"tcdate":1511622739648,"number":1,"cdate":1511622739648,"id":"BJhUuZDgf","invitation":"ICLR.cc/2018/Conference/-/Paper581/Official_Review","forum":"H1sUHgb0Z","replyto":"H1sUHgb0Z","signatures":["ICLR.cc/2018/Conference/Paper581/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-written paper with extensive experiments, but missing a realistic non-simulated experiment, as well as some comparisons and related work.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators). The authors provide both theoretical and experimental validation of their idea. \n\nPros:\n+ The paper is generally very clearly written. The motivation, notation, and method are clear.\n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. \n+ The approach is a novel twist on an existing method for learning from noisy data. \n\nCons: \n- All experiments use simulated workers; this is probably common but still not very convincing.\n- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. \"To re (label), or not to re (label).\" HCOMP 2014.\n- The authors should have compared their approach to the \"base\" approach of Natarajan et al. \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.\n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture.\n\nQuestions: \n- How would the model need to change to account for example difficulty? \n- Why are Joulin 2016, Krause 2016 not relevant?\n- Best to clarify what the weights in the weighted sum of Natarajan are. \n- \"large training error on wrongly labeled examples\" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels? Where does this ground truth come from?\n- Not clear what \"Ensure\" means in the algorithm description.\n- In Sec. 4.4, why is it important that the samples are fresh?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning From Noisy Singly-labeled Data","abstract":"Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply). Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. ","pdf":"/pdf/24501d72afdf0c546b76d8632072343b8a2c6cf9.pdf","TL;DR":"A new approach for learning a model from noisy crowdsourced annotations.","paperhash":"anonymous|learning_from_noisy_singlylabeled_data","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning From Noisy Singly-labeled Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1sUHgb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper581/Authors"],"keywords":["crowdsourcing","noisy annotations","deep leaerning"]}},{"tddate":null,"ddate":null,"tmdate":1509739223318,"tcdate":1509127507068,"number":581,"cdate":1509739220657,"id":"H1sUHgb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1sUHgb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning From Noisy Singly-labeled Data","abstract":"Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply). Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. ","pdf":"/pdf/24501d72afdf0c546b76d8632072343b8a2c6cf9.pdf","TL;DR":"A new approach for learning a model from noisy crowdsourced annotations.","paperhash":"anonymous|learning_from_noisy_singlylabeled_data","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning From Noisy Singly-labeled Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1sUHgb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper581/Authors"],"keywords":["crowdsourcing","noisy annotations","deep leaerning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}