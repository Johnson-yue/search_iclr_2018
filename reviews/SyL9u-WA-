{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222721661,"tcdate":1512198282662,"number":3,"cdate":1512198282662,"id":"H1Z5gRJZf","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN. \n\nMy concerns with the paper include: \n\na) the paper says that the same method works for convolutional neural networks but I couldn't find anything about convolution. \n\nb) the theoretical analysis might be misleading --- clearly section 6.2 shouldn't have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it's not a global minimum. Theorem 5 should be phrased as \n\nall critical points of the population risk that is non-singular are global minima.\n\nc) the paper should run some experiments on language applications where RNN is widely used\n\nd) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it's kind of impossible to scale to large datasets? \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/7865267a324dfcd873d101339d8b50569aae3c89.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1512222721704,"tcdate":1511840487132,"number":2,"cdate":1511840487132,"id":"Bkyxj89lM","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer1"],"readers":["everyone"],"content":{"title":"SVD reparametrization of the transition matrix","rating":"5: Marginally below acceptance threshold","review":"This paper suggests a reparametrization of the transition matrix. The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.\n\nThe paper is well-written and authors explain related work adequately. The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary. The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact. \n\nI have two comments on the experiment section:\n\n- Choice of experiments. Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.\n\n- Stopping condition. The plots suggest that the optimization has stopped earlier for some models. Is this because of some stopping condition or because of gradient explosion? Is there a way to avoid this?\n\n- Quality of figures. Figures are very hard to read because of small font. Also, the captions need to describe more details about the figures.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/7865267a324dfcd873d101339d8b50569aae3c89.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1512222721742,"tcdate":1511730067422,"number":1,"cdate":1511730067422,"id":"Syi9ojdgf","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training. The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017),\n2) faster gradient update than Vorontsov et al. (2017).\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power. The experimental results also look promising. \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe). If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum. \n\nOverall this is a strong paper and I recommend to accept.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/7865267a324dfcd873d101339d8b50569aae3c89.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1509739153314,"tcdate":1509132429987,"number":699,"cdate":1509739150659,"id":"SyL9u-WA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/7865267a324dfcd873d101339d8b50569aae3c89.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}