{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222775558,"tcdate":1511842908209,"number":2,"cdate":1511842908209,"id":"BkVvEP5gM","invitation":"ICLR.cc/2018/Conference/-/Paper817/Official_Review","forum":"rJm7VfZA-","replyto":"rJm7VfZA-","signatures":["ICLR.cc/2018/Conference/Paper817/AnonReviewer2"],"readers":["everyone"],"content":{"title":"ICLR may not be the right venue; Technical questions: Unclear how to deal with stochastic dynamics, etc.","rating":"4: Ok but not good enough - rejection","review":"Summary:\nThis paper studies multi-agent sequential decision making problems that belong to the class of games called Markov Potential Games (MPG). It considers finding the optimal policy within a parametric space of policies, which can be represented by a function approximator such as a DNN.\nA main contribution of this work is that it shows that for MPG, instead of solving a multi-objective optimization problem (Eq. 8), which is difficult, it is sufficient to solve a scalar-valued optimization problem (Eq. 16).  Theorem 1 shows that under certain conditions on the reward function, the game is MPG. It also shows how one might find the potential function J, which is used in the single objective optimization problem.\nFinding J can be computationally expensive in general. So the paper provides some properties that lead to finding J easier. For example, obtaining J is easy if we have a cooperative game (Corollary 1) or the reward can be decomposed/decoupled in a certain way (Theorem 2).\n\n\nEvaluation:\n\nThis is a well-written paper that studies an important problem, but I don’t think ICLR is the right venue for it. There is not much about (representation) learning in this work. The use of TRPO as an RL algorithm in the Experiment does not play a critical role in this work either. Aside this general comment, I have several other more specific comments.\n\n\n- There is a significant literature on the use of RL for multi-agent systems. The paper does not do a good job comparing and positioning with respect to them. For example, refer to the following recent paper and references therein:\n\nPerolat, Strub, et al., “Learning Nash Equilibrium for General-Sum Markov Games from Batch Data,” AISTATS, 2017.\n\n\n- If I understand correctly, the policies are considered to be functions from the state of the system to a continuous action. So it is a function, and not a probability distribution. This means that the space of considered policies correspond to the space of pure strategies. We know that for some games, the Nash equilibrium is a mixed strategy. Isn’t this a big limitation of this approach?\n\n\n- I am unclear how this approach can handle stochastic dynamics. For example, the optimization (P1) depends on the realization of (theta_i)_i. But this is not available. The dependence is not only in the objective, but also in the constraints, which makes things more difficult.\n\nI understand that in the experiments the authors used two models (either the average of random realization, or solving a different optimization for each realization), but none of them is an appropriate solution for a stochastic system.\n\n\n- How large is the MPG class? Is there any structural result that positions them compared to other Markov Games? For example, is the class of zero-sum games an example of MPG?\n\n\n- There is a comment close to the end of Section 5 that when there is no prior knowledge of the dynamics and the reward, one can use the proposed approach to learn PCL-NE by using any DRL.\nThis is questionable because if the reward is not known, the conditions of Theorems 1 or 2 cannot be verifies, so it is not possible to use (P1) instead of (G2).\n\n\n- What comments can you make about the computational complexity? It seems that depending on the dynamics, the optimization problem P1 can be non-convex, hence computationally difficult to solve.\n\n\n- How is the work related to the following paper?\nMacua, Zazo, Zazo, “Learning in Constrained Stochastic Dynamic Potential Games,” ICASSP, 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Approximate Closed-Loop Policies for Markov Potential Games","abstract":"Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We consider a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications, where the agents share or compete for some common resource, the state-action sets are continuous, rewards might be nonconvex functions, and there might be coupled constraints. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided open-loop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider policies that depend on the current state and where agents adapt to stochastic transitions. Following state-of-the-art results for single-agent problems obtained with deep-reinforcement-learning, we consider the agents' policies belong to some parametric class (e.g., deep neural networks). We provide sufficient and necessary, easily verifiable conditions for a stochastic game to be an MPG, and show that a closed-loop Nash equilibrium can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP---which is a single-objective problem---is usually much simpler than solving the original set of coupled OCPs that form the game---which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no Nash equilibrium belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning (DRL) algorithm and learn a set of policies (one per agent) that closely approximates an exact variational Nash equilibrium of the game.","pdf":"/pdf/db5f02eca6f3c74ec9a9c411db46cb4fec131e2d.pdf","TL;DR":"We present general closed loop analysis for Markov potential games and show that deep reinforcement learning can be used for learning approximate closed-loop Nash equilibrium.","paperhash":"anonymous|learning_approximate_closedloop_policies_for_markov_potential_games","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Approximate Closed-Loop Policies for Markov Potential Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJm7VfZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper817/Authors"],"keywords":["Stochastic games","potential games","closed loop","reinforcement learning","multiagent systems"]}},{"tddate":null,"ddate":null,"tmdate":1512222775602,"tcdate":1511820984723,"number":1,"cdate":1511820984723,"id":"BJZ6A-clG","invitation":"ICLR.cc/2018/Conference/-/Paper817/Official_Review","forum":"rJm7VfZA-","replyto":"rJm7VfZA-","signatures":["ICLR.cc/2018/Conference/Paper817/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting work on Markov potential games, from the viewpoint of someone without any prior knowledge on the topic","rating":"6: Marginally above acceptance threshold","review":"This manuscript considers a subclass of stochastic games named Markov potential games. It provides some assumptions that guarantee that a game is a Markov potential game and leads to some nice properties to solve the problem to approximately a Nash equilibrium. It is claimed that the work extends the state of the art by analysing the closed-loop version in a different manner, firstly constraining policies to a parametric family and then deriving conditions for that, instead of the other way around. As someone with no knowledge in the topic, I find the paper interesting to read, but I have not followed any proofs. The experimental setup is quite limited, even though I believe that the intention of the authors is to provide some theoretical ideas rather than applying them. Minor point: there are a few sentences with small errors, this could be improved.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Approximate Closed-Loop Policies for Markov Potential Games","abstract":"Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We consider a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications, where the agents share or compete for some common resource, the state-action sets are continuous, rewards might be nonconvex functions, and there might be coupled constraints. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided open-loop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider policies that depend on the current state and where agents adapt to stochastic transitions. Following state-of-the-art results for single-agent problems obtained with deep-reinforcement-learning, we consider the agents' policies belong to some parametric class (e.g., deep neural networks). We provide sufficient and necessary, easily verifiable conditions for a stochastic game to be an MPG, and show that a closed-loop Nash equilibrium can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP---which is a single-objective problem---is usually much simpler than solving the original set of coupled OCPs that form the game---which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no Nash equilibrium belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning (DRL) algorithm and learn a set of policies (one per agent) that closely approximates an exact variational Nash equilibrium of the game.","pdf":"/pdf/db5f02eca6f3c74ec9a9c411db46cb4fec131e2d.pdf","TL;DR":"We present general closed loop analysis for Markov potential games and show that deep reinforcement learning can be used for learning approximate closed-loop Nash equilibrium.","paperhash":"anonymous|learning_approximate_closedloop_policies_for_markov_potential_games","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Approximate Closed-Loop Policies for Markov Potential Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJm7VfZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper817/Authors"],"keywords":["Stochastic games","potential games","closed loop","reinforcement learning","multiagent systems"]}},{"tddate":null,"ddate":null,"tmdate":1509739084890,"tcdate":1509135386830,"number":817,"cdate":1509739082214,"id":"rJm7VfZA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJm7VfZA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Approximate Closed-Loop Policies for Markov Potential Games","abstract":"Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We consider a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications, where the agents share or compete for some common resource, the state-action sets are continuous, rewards might be nonconvex functions, and there might be coupled constraints. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided open-loop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider policies that depend on the current state and where agents adapt to stochastic transitions. Following state-of-the-art results for single-agent problems obtained with deep-reinforcement-learning, we consider the agents' policies belong to some parametric class (e.g., deep neural networks). We provide sufficient and necessary, easily verifiable conditions for a stochastic game to be an MPG, and show that a closed-loop Nash equilibrium can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP---which is a single-objective problem---is usually much simpler than solving the original set of coupled OCPs that form the game---which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no Nash equilibrium belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning (DRL) algorithm and learn a set of policies (one per agent) that closely approximates an exact variational Nash equilibrium of the game.","pdf":"/pdf/db5f02eca6f3c74ec9a9c411db46cb4fec131e2d.pdf","TL;DR":"We present general closed loop analysis for Markov potential games and show that deep reinforcement learning can be used for learning approximate closed-loop Nash equilibrium.","paperhash":"anonymous|learning_approximate_closedloop_policies_for_markov_potential_games","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Approximate Closed-Loop Policies for Markov Potential Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJm7VfZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper817/Authors"],"keywords":["Stochastic games","potential games","closed loop","reinforcement learning","multiagent systems"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}