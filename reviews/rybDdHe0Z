{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222603718,"tcdate":1511818671426,"number":3,"cdate":1511818671426,"id":"S1D3Hb9eM","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Application of LSTM to decoding of neural signals, limited novelty, inconclusive","rating":"3: Clear rejection","review":"This work addresses brain state  decoding (intent to move) based on intra-cranial \"electrocorticography (ECoG) grids\". ECoG signals are generally of much higher quality than more conventional EEG signals acquired on the skalp, hence it appears meaningful to invest significant effort to decode.  \nPreprocessing is only descibed in a few lines in Section 2.1, and the the feature space is unclear (number of variables etc)\n\nLinear discriminants, \"1-state and 2-state\" hidden markov models, and LSTMs are considered for classification (5 classes, unclear if prior odds are uniform). Data involves multiple subjects (4 selected from a larger pool). Total amount of data unclear. \"A validation set is not used due to the limited data size.\"  The LSTM setup and training follows conventional wisdom.\n\"The model used for our analyses was constructed with 100 hidden units with no performance gain identified using larger or stacked networks.\"\nA simplistic but interesting  transfer scheme is proposed amounting to an affine transform of features(??) - the complexity of this transform is unclear.\n\nWhile limited novelty is found in the methodology/engineering - novelty being mainly related to the affine transfer mechanism, results are disappointing.  \nThe decoding performance of the LSTMs does not convincingly exceed that of the simple baselines. \n\nWhen analyzing the transfer mechanism only the LSTMs are investigated and it remains unclear how well trans works.\n\nThere is an interesting visualization (t-SNE) of the latent representations. But very limited discussion of what we learn from it, or how such visualization could  be used to provide neuroscience insights.\n\nIn the discussion we find the claim: \"In this work, we have shown that LSTMs can model the variation within a neural sequence and are a good alternative to state-of-the-art decoders.\"  I fail to see how it can be attractive to obtain similar performance with a model of 100x (?) the complexity\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/6f2f7deab4987cbc31d764b87984e186372d621e.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1512222603760,"tcdate":1511804475119,"number":2,"cdate":1511804475119,"id":"HJmBCpKeG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer1"],"readers":["everyone"],"content":{"title":"LSTMs for ECoG","rating":"6: Marginally above acceptance threshold","review":"The ms applies an LSTM on ECoG data and studies tranfer between subjects etc. \n\nThe data includes only few samples per class. The validation procedure to obtain the model accuray is a bit iffy. \nThe ms says: The test data contains 'at least 2 samples per class'. Data of the type analysed is highly dependend, so it is not unclear whether this validation procedure will not provide overoptimistic results. Currently, I do not see evidence for a stable training procedure in the ms. I would be curious also to see a comparison to a k-NN classifier using embedded data to gauge the problem difficulty. \nAlso, the paper does not really decide whether it is a neuroscience contribution or an ML one. If it were a neuroscience contribution, then it would be important to analyse and understand the LSTM representation and to put it into a biological context fig 5B is a first step in this direction. \nIf it where a ML contribution, then there should be a comprehensive analysis that indeed the proposed architecture using the 2 steps is actually doing the right thing, i.e. that the method converges to the truth if more and more data is available. \nThere is also some initial experiments in fig 3A. Currently, I find the paper somewhat unsatisfactory and thus preliminary. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/6f2f7deab4987cbc31d764b87984e186372d621e.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1512222603807,"tcdate":1511631616458,"number":1,"cdate":1511631616458,"id":"HJ_bsmPxG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Difficult problem, some aspects are unclear, evaluation could be improved","rating":"4: Ok but not good enough - rejection","review":"The paper describes an approach to use LSTMâ€™s for finger classification based on ECOG. and a transfer learning extension of which two variations exists. From the presented results, the LSTM model is not an improvement over a basic linear model. The transfer learning models performs better than subject specific models on a subset of the subjects. Overall, I think the problem Is interesting but the technical description and the evaluation can be improved. I am not confident in the analysis of the model. Additionally, the citations are not always correct and some related work is not referenced at all. For the reasons above, I am not willing to recommend the paper for acceptance at his point.\n\nThe paper tackles a problem that is challenging and interesting. Unfortunately, the dataset size is limited. \nThis is common for brain data and makes evaluation much more difficult.\n The paper states that all hyper-parameters were optimized on 75% of subject B data.\nThe actual model training was done using cross-validation. \nSo far this approach seems more or less correct but in this case I would argue that subject B should not be considered for evaluation since its data is heavily used for hyper-parameter optimization and the results obtained on this subject are at risk of being biased.\nOmitting subject B from the analysis, each non-transfer learning method  performs best on one of the remaining subjects.\nTherefore it is not clear that an LSTM model is an improvement. \nFor transfer learning (ignoring B again) only C and D are improved but it is unclear what the variance is.\nIn the BCI community there are many approaches that use transfer learning with linear models. I think that it would be interesting how linear model transfer learning would fare in this task. \n\nA second issue that might inflate the results is the fact that the data is shuffled randomly. While this is common practice for most machine learning tasks, it is dangerous when working with brain data due to changes in the signal over time. As a result, selecting random samples might inflate the accuracy compared to having a proper train and test set that are separated in time. Ideally the cross-validation should be done using contiguous folds. \n\nI am not quite sure whether it should be possible to have an accuracy above chance level half a second before movement onset? How long does motor preparation take? I am not familiar with this specific subject, but a quick search gave me a reaction time for sprinters of .15 seconds. Is it possible that cue processing activity was used to obtain the classification result? Please discuss this effect because I am do not understand why it should be possible to get above chance level accuracy half a second before movement onset. \n\nThere are also several technical aspects that are not clear to me. I am confident that I am unable to re-implement the proposed method and their baseline given the information provided.\n\nLDA baseline:\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”\nFor the LDA baseline, how is the varying sequence length treated? \nLedoit wolf analytic  regularization is used, but it isn not referenced. If you use that method, cite the paper. \nThe claim that LDA works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true. However, it is never empirically verified.  Therefore this statement should not be in the paper. \n\nHMM baseline\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”\nHow are the 1 and the 2 state HMM used w.r.t. the 5 classes? It is unclear to me how they are used exactly. Is there a single HMM per class? Please be specific. \n\nLSTM Model\nâ€”â€”â€”â€”â€”\nWhat is the random and language model initialization scheme? I can only find the sequence auto-encoder in the Dai and Le paper. \n\n\nModel analysis\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-\nIt is widely accepted in the neuroimaging community that linear weight vectors should not be interpreted directly. It is actually impossible to do this.  Therefore this section should be completely re-done. Please read the following paper on this subject.\nHaufe, Stefan, et al. \"On the interpretation of weight vectors of linear models in multivariate neuroimaging.\"Â NeuroimageÂ 87 (2014): 96-110.\n\nReferences\nâ€”â€”â€”â€” \nLedoit wolf regularization is used but not cited. Fix this.\nThere is no citation for the random/language model initialization of the LSTM model. I have no clue how to do this without proper citation.\nLe at al (2011) are referenced for auto-encoders. This is definitely not the right citation. \nRumelhart, Hinton, & Williams, 1986a; Bourlard & Kamp, 1988; Hinton & Zemel, 1994 and Bengio, Lamblin, Popovici, & Larochelle, 2007; Ranzato, Poultney, Chopra, & LeCun, 2007 are probably all more relevant.\nPlease cite the relevant work on affine transformations for transfer learning especially the work by morioka et al who also learn an input transferm.\nMorioka, Hiroshi, et al. \"Learning a common dictionary for subject-transfer decoding with resting calibration.\" NeuroImage 111 (2015): 167-178.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/6f2f7deab4987cbc31d764b87984e186372d621e.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1509739399210,"tcdate":1509083224816,"number":259,"cdate":1509739396560,"id":"rybDdHe0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/6f2f7deab4987cbc31d764b87984e186372d621e.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}