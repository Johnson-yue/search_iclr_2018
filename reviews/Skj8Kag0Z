{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222650916,"tcdate":1511712056423,"number":2,"cdate":1511712056423,"id":"HJgHrDugM","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Review","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good work","rating":"7: Good paper, accept","review":"This work proposes a framework for stabilizing adversarial nets using a prediction step. The prediction step is motivated by primal-dual algorithms in convex optimization where the term having both variables is bi-linear.  \n\nThe authors prove a convergence result when the function is convex in one variable and concave in the other. This problem is more general than the previous one in convex optimization.  Then this prediction step is applied in many recent applications in training adversarial nets and compared with state-of-the-art solvers. The better performance of this simple step is shown in most of the numerical experiments. \n\nThough this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance,  there is more work to be done. Whether there is a better generalization of this prediction step? There are also other variants of primal-dual algorithms in convex optimization; can other modification including the accelerated variants be applied?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/4a76a5a116445ef0aef046ccc781abfa68760ff3.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222650958,"tcdate":1511688841272,"number":1,"cdate":1511688841272,"id":"ry-q9ZOlf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Review","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple modification to alternating stochastic gradient for GAN training, which stabilizes training, essentially for free. Clever and useful idea, solid and insightful analysis, good presentation. ","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. \n\nThis is a clever and useful idea, and the paper is very well written. The proposed method is very clearly motivated, both intuitively and mathematically, and the authors also provide theoretical guarantees on its convergence behavior. I particularly liked the analogy with the damped harmonic oscillator.  \n\nThe experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training.\n\nI have only one minor question: in the prediction step, why not use a step size, say \n$\\bar{u}_k+1 = u_{k+1} + \\gamma_k (u_{k+1} âˆ’ u_k)$, such that the \"amount of predition\" may be adjusted?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/4a76a5a116445ef0aef046ccc781abfa68760ff3.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739312175,"tcdate":1509116243358,"number":422,"cdate":1509739309510,"id":"Skj8Kag0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/4a76a5a116445ef0aef046ccc781abfa68760ff3.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}