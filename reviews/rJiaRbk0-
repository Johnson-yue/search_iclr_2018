{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222564704,"tcdate":1511836598499,"number":3,"cdate":1511836598499,"id":"HyA3jBqgG","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper propose a new \"gate\" function for LSTM to enable the values of the gates towards 0 or 1. The motivation behind is a  flat region of the loss surface is likely to generalize well. It shows the experimental results are comparable or better than vanilla LSTM and much more robust to low-precision approximation and low-rank approximation.\n\nIn section 3.2, the paper claimed using a smaller temperature cannot guarantee the outputs to be close to the boundary. Is there any experimental evidence to show it's not working? It also claimed pushing output gate to 0/1 will drop the performance. It actually quite interesting because there are bunch of paper claimed output gate is not important for language modeling, e.g. https://openreview.net/pdf?id=HJOQ7MgAW . \n\nIn the sensitive analysis, what if apply rounding / low-rank for all the parameters? \n\nHow was this approach compare to binarynet https://arxiv.org/abs/1602.02830 ? Applying the same idea, but only for forget gate/ input gate. Also, can we apply this idea to the binarynet? \n\nOverall, I think it's an interesting paper but I feel it should compare with some simple baseline to binarized the gate function.  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/43f39282b4be9f6f8244e5785ab7f01bf5fbd9e0.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1512222564743,"tcdate":1511828226587,"number":2,"cdate":1511828226587,"id":"Syo-smqgf","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but not impressive","rating":"6: Marginally above acceptance threshold","review":"The paper argues for pushing the input and forget gate’s output toward 0 or 1, i.e., the LSTM tends to reside in flat region of surface loss, which is likely to generalize well. To achieve that, the sigmoid function in the original LSTM is replaced by a function G that is continuous and differentiable with respect to the parameters (by applying the Gumbel-Softmax trick). As a result, the model is still differentiable while the output gate is approximately binarized.  \n\nPros:\n-\tThe paper is clearly written\n-\tThe method is new and somehow theoretically guaranteed by the proof of the Proposition 1\n-\tThe experiments are clearly explained with detailed configurations\n-\tThe performance of the method in the model compression task is promising \n\nCons:\n-\tThe “simple deduction” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis\n-\tIt is confusing whether the output of the gate is sampled based on or computed directly by the function G  \n-\tThe experiments lack many recent baselines on the same dataset (Penn Treebank: Melis et al. (2017) – On the State of the Art of Evaluation in Neural Language Models; WMT: Ashish et.al. (2017) – Attention Is All You Need) \n-\tThe experiment’s result is only slightly better than the baseline’s\n-\tTo be more persuasive, the author should include in the baselines other method that can “binerize” the gate values such as the one sharpening the sigmoid function. \n\n\nIn short, this work is worth a read. Although the experimental results are not quite persuasive, the method is nice and promising. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/43f39282b4be9f6f8244e5785ab7f01bf5fbd9e0.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1512222564780,"tcdate":1511683954491,"number":1,"cdate":1511683954491,"id":"S15OPlugz","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The technical novelty is limited and experiments do not show much benefits of the proposed model.","rating":"4: Ok but not good enough - rejection","review":"This paper aims to push the LSTM gates to be binary. To achieve this, the paper proposes to employ the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution (taking 0 or 1 value). The resulted G2-LSTM is applied for language model and machine translation in the experiments. \n\nThe novelty of this paper is limited. Just directly apply the Gumbel-Softmax trick. \n\nThe motivation is not explained clearly and convincingly. Why need to pursue binary gates? According to the paper, it may give better generalization performance. But there is no theoretical or experimental evidence provided by this paper to support this argument. \n\nThe results of the new G2-LSTM are not significantly better than baselines in the experiments.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/43f39282b4be9f6f8244e5785ab7f01bf5fbd9e0.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1509739474249,"tcdate":1509002946844,"number":118,"cdate":1509739471595,"id":"rJiaRbk0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/43f39282b4be9f6f8244e5785ab7f01bf5fbd9e0.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}