{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222620359,"tcdate":1511823778293,"number":3,"cdate":1511823778293,"id":"r1wiKz5ef","invitation":"ICLR.cc/2018/Conference/-/Paper323/Official_Review","forum":"SkOb1Fl0Z","replyto":"SkOb1Fl0Z","signatures":["ICLR.cc/2018/Conference/Paper323/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors present an interesting framework to search for new RNN models, with some promising results.","rating":"7: Good paper, accept","review":"The authors introduce a new method to generate RNNs architectures. The authors propose a domain-specific language two types of generators (random and RL-based) together with a ranking function and evaluator. The results are promising, and this research introduces a framework that might enable the community to find new interesting models. However, it's not clear how these framework compare with previous ones (see below). Also, the clarity of the text could be improved.\n\nPros:\n1. An interesting automatic method for generating RNNs is introduced by the authors (but is not entirely clear how does ir compare with previous different approaches)\n2. The approach is tested in a number of tasks: Language modelling (PTB and wikipedia-2) and machine translation (\n3. In these work the authors tested a wide range of different RNNs\n3. It is interesting that some of the best performing architectures (e.g. LSTM, residual nets) are found during the automatic search\n\n\nCons:\n1. It would be nice if the method didn’t rely on defining a specific set of functions and operators upon which the proposed method works.\n2. The text has some typos: for example: “that were used to optimize the generator each batch”\n3. In section 5, the authors briefly discuss other techniques using RL and neuroevolution, but they never contrast these approaches with theirs. Overall, it would be nice if the authors had made a more direct comparison with other methods for generating RNNs.\n4. The description of the ranking function is not clear. What kind of networks were used? This appears to introduce a ranking-network-specific bias in the search process.\n\nMinor comments:\n1. The authors study the use of subtractive operators. Recently a new model has considered the use of subtractive gates in LSTMs as a more biologically plausible implementation (Cortical microcircuits as gated-recurrent neural networks, NIPS 2017).\n2. Figure 4 missing label on x-axis\n3. End of Related Work period is missing.\n3. The authors state that some of the networks generated do not follow human intuition, but this doesn’t appear to discussed. What exactly do the authors mean?\n4. Not clear what happens in Figure 4 in epoch 19k or so, why such an abrupt change?\n5. Initial conditions are key for such systems, could the init itself be included in this framework?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Flexible Approach to Automated RNN Architecture Generation","abstract":"The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","pdf":"/pdf/0ff12b0243b01ce916abac8a40673b2dce7a6997.pdf","TL;DR":"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.","paperhash":"anonymous|a_flexible_approach_to_automated_rnn_architecture_generation","_bibtex":"@article{\n  anonymous2018a,\n  title={A Flexible Approach to Automated RNN Architecture Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkOb1Fl0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper323/Authors"],"keywords":["reinforcement learning","architecture search","ranking function","recurrent neural networks","recursive neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222620404,"tcdate":1511820480055,"number":2,"cdate":1511820480055,"id":"BkuT3b9ef","invitation":"ICLR.cc/2018/Conference/-/Paper323/Official_Review","forum":"SkOb1Fl0Z","replyto":"SkOb1Fl0Z","signatures":["ICLR.cc/2018/Conference/Paper323/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper investigates meta-learning strategy for automated architecture search in the context of RNN. To constraint the architecture search space, authors propose a DSL that specifies the RNN recurrent operations. This DSL allows to explore RNN architectures using either random search or a reinforcement-learning strategy. Candidate architectures are ranked using a TreeLSTM that tries to predict the architecture performances. The top-k architectures are then evaluated by fully training them on a given task.\n\nAuthors evaluate their approach on  PTB/Wikitext 2 language modeling and Multi30k/IWSLT'16  machine translation. In both experiments, authors show that their approach obtains competitive results and can sometime outperforms RNN cells such as GRU/LSTM. In the PTB experiment, their architecture however underperforms other LSTM variant in the literatures.\n\n\n- Quality/Clarity\nThe paper is overall well written and pleasant to read.\n\nFew details can be clarified. In particular how did you initialize the weight and bias for both the LSTM/GRU baselines and the found architectures? Is there other works leveraging RNN that report results on the Multi30k/IWSLT datasets?\n\nYou state in paragraph 3.2 that human experts can inject the previous best known architecture when training the ranking networks. Did you use this in the experiments? If yes, what was the impact of this online learning strategy on the final results? \n\n\n- Originality\nThe idea of using DSL + ranking for architecture search seems novel.\n\n\n- Significance\nAutomated architecture search is a promising way to design new networks. However, it is not clear why the proposed approach is not able to outperforms other LSTM-based architectures on the PTB task. Could the problem arise from the DSL that constraint too much the search space ? It would be nice to have other tasks that are commonly used as benchmark for RNN to see where this approach stand.\n\nIn addition, authors propose both a DSL, a random and RL generator and a ranking function. It would be nice to disentangle the contributions of the different components. In particular, did the authors compare the random search vs the RL based generator or the performances of the RL-based generator when the ranking network is not used?\n\nAlthough authors do show that they outperform NAScell in one setting, it would be nice to have an extended evaluation (using character level PTB for instance).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Flexible Approach to Automated RNN Architecture Generation","abstract":"The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","pdf":"/pdf/0ff12b0243b01ce916abac8a40673b2dce7a6997.pdf","TL;DR":"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.","paperhash":"anonymous|a_flexible_approach_to_automated_rnn_architecture_generation","_bibtex":"@article{\n  anonymous2018a,\n  title={A Flexible Approach to Automated RNN Architecture Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkOb1Fl0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper323/Authors"],"keywords":["reinforcement learning","architecture search","ranking function","recurrent neural networks","recursive neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222620447,"tcdate":1510337512447,"number":1,"cdate":1510337512447,"id":"S1exhDQJf","invitation":"ICLR.cc/2018/Conference/-/Paper323/Official_Review","forum":"SkOb1Fl0Z","replyto":"SkOb1Fl0Z","signatures":["ICLR.cc/2018/Conference/Paper323/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice work, but limited in scope and does not appear to generalize well","rating":"4: Ok but not good enough - rejection","review":"This work tries to cast the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and are either 1. Sampled randomly and scored based on a learnt function OR 2. Generated by a RL agent.\nWhile the overall approach appears to generalize previous work, I see a few serious flaws in this work:\n\nLimited scope\nAs far as I can tell this work only tries to come up with a design for a single RNN cell, and then claims that the optimality of the design will carry over to stacking of such modules, not to mention more complicated network designs. \nEven the design of a single cell is heavily biased by human intuition (section 4.1) It would have been more convincing to see that the system learn heuristics such as “don’t stack two matrix mults” rather than have these hard coded.\n\nNo generalization guarantees:\nNo attempt is made to optimize hyperparameters of the candidate architectures. This leads one to wonder if the winning architecture has won only because of the specific parameters that were used in the evaluation.\nIndeed, the experiments in the paper show that a cell which was successful on one task isn’t necessary successful on a different one, which questions the competence of the scoring function / RL agent.\n\nOn the experimental side:\nNo comparison is made between the two optimization strategies, which leaves the reader wondering which one is better.\nControl for number of network variables is missing when comparing candidate architectures. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Flexible Approach to Automated RNN Architecture Generation","abstract":"The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","pdf":"/pdf/0ff12b0243b01ce916abac8a40673b2dce7a6997.pdf","TL;DR":"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.","paperhash":"anonymous|a_flexible_approach_to_automated_rnn_architecture_generation","_bibtex":"@article{\n  anonymous2018a,\n  title={A Flexible Approach to Automated RNN Architecture Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkOb1Fl0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper323/Authors"],"keywords":["reinforcement learning","architecture search","ranking function","recurrent neural networks","recursive neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739364186,"tcdate":1509097215895,"number":323,"cdate":1509739361529,"id":"SkOb1Fl0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkOb1Fl0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Flexible Approach to Automated RNN Architecture Generation","abstract":"The process of designing neural architectures requires expert knowledge and extensive trial and error.\nWhile automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\nWe propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\nThe DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\nThe resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","pdf":"/pdf/0ff12b0243b01ce916abac8a40673b2dce7a6997.pdf","TL;DR":"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures.","paperhash":"anonymous|a_flexible_approach_to_automated_rnn_architecture_generation","_bibtex":"@article{\n  anonymous2018a,\n  title={A Flexible Approach to Automated RNN Architecture Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkOb1Fl0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper323/Authors"],"keywords":["reinforcement learning","architecture search","ranking function","recurrent neural networks","recursive neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}