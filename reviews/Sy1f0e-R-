{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222706674,"tcdate":1512141872561,"number":3,"cdate":1512141872561,"id":"HyK44e1bM","invitation":"ICLR.cc/2018/Conference/-/Paper634/Official_Review","forum":"Sy1f0e-R-","replyto":"Sy1f0e-R-","signatures":["ICLR.cc/2018/Conference/Paper634/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An empirical comparison of 4 metrics for GANs","rating":"5: Marginally below acceptance threshold","review":"The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). \n\nThe paper is well written, clear, organized and easy to follow.\n\nGiven that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons. They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting. \n\nAlthough this work and its results are very useful for practitioners, it lacks in two aspects. First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions. Some of the conclusions could be further clarified with additional experiments (e.g., Sec 3.6 ‘while the reason that RMS also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the ImageNet dataset’).\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1512222706713,"tcdate":1511847282452,"number":2,"cdate":1511847282452,"id":"BJquSu9eM","invitation":"ICLR.cc/2018/Conference/-/Paper634/Official_Review","forum":"Sy1f0e-R-","replyto":"Sy1f0e-R-","signatures":["ICLR.cc/2018/Conference/Paper634/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Evaluation of GAN evaluation metrics","rating":"7: Good paper, accept","review":"Thanks for an interesting paper. \n\nThe paper evaluates popular GAN evaluation metrics to better understand their properties. The \"novelty\" of this paper is a bit hard to assess. However, I found their empirical evaluation and experimental observations to be very interesting. If the authors release their code as promised, the off-the-shelf tool would be a very valuable contribution to the GAN community. \n\nIn addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM). \n\nHave you considered approximations to Wasserstein distance? E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs: \nComparison of Maximum Likelihood and GAN-based training of Real NVPs\nhttps://arxiv.org/pdf/1705.05263.pdf\n\nHow sensitive are the results to hyperparameters? It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Appendix G in https://arxiv.org/pdf/1706.04987.pdf)\n\nDo you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics? Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511727315304,"tcdate":1511727315304,"number":5,"cdate":1511727315304,"id":"BkoCeoOef","invitation":"ICLR.cc/2018/Conference/-/Paper634/Public_Comment","forum":"Sy1f0e-R-","replyto":"H1bj91Ixz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Mentioning all of missing measures","comment":"If this were true then why not mention \"all\" of the potential metrics that are missing from the paper and instead only cite one paper that is missing.  \n\nOther works not covered or mentioned as a potential metric:\n\nRevisiting Classifier Two-Sample Tests\nhttps://arxiv.org/abs/1610.06545\n\nGENERATIVE ADVERARIAL METRIC\nhttps://openreview.net/pdf?id=wVqzLo88YsG0qV7mtLq7\n\nMode Regularized Generative Adversarial Networks\nhttps://arxiv.org/abs/1612.02136\n(They introduce the mode score as a modification to the inception score)\n\nAgain the original \"review\" only mentioned one particular paper for perhaps obvious reasons."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511662750315,"tcdate":1511662750315,"number":2,"cdate":1511662750315,"id":"HkLsEjDlf","invitation":"ICLR.cc/2018/Conference/-/Paper634/Official_Comment","forum":"Sy1f0e-R-","replyto":"HJ27tABef","signatures":["ICLR.cc/2018/Conference/Paper634/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper634/Authors"],"content":{"title":"Reply","comment":"1) This is an interesting question. For CycleGAN, the mode dropping/collapsing problem might be less several due to the reconstruction loss. Therefore, we may prefer those metrics, e.g., MMD, that have better discriminability (between generated distribution and target distribution).\n\n2) The critical part might be how to define the distance between two sentences/documents. Once we have a well-defined distance, all the metrics investigated in this paper can be applied. The word mover’s distance (WMD) [1] appears to be an ideal candidate.\n[1] Kusner et al, From Word Embeddings To Document Distances, ICML, 2015\n\n3) We plan to include FID in the updated version. Please refer to our reply to the other comments for some preliminary results.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511662659577,"tcdate":1511662659577,"number":1,"cdate":1511662659577,"id":"BJhrEjDgf","invitation":"ICLR.cc/2018/Conference/-/Paper634/Official_Comment","forum":"Sy1f0e-R-","replyto":"Sy2r0j_yG","signatures":["ICLR.cc/2018/Conference/Paper634/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper634/Authors"],"content":{"title":"Preliminary Results and Thoughts","comment":"Thanks for pointing this paper to us. The FID is essentially a distance metric for probability distributions, thus it fits into the evaluation framework we investigated. We will include it in our updated version. \n\nFollowing [1], we make the Gaussian distribution assumption on the real/generated data, and test the discriminability of FID. Our preliminary results show that it behaves similarly to the Wasserstein distance under this test. For other properties, we speculate that FID will have a better time/sample complexity than the Wasserstein distance due to the simplified Gaussian distribution assumption. But it might be less sensitive to mode collapse.\n\nOur paper aims to provide a framework to investigate different properties of GAN evaluation metrics. Thus researchers can use it to analyze any sample-based evaluation metric that fits into the framework. As there exist many metrics for probability distributions, we can only focus on several most typical ones (especially those already been used by the GAN community) in our paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511549592757,"tcdate":1511549592757,"number":4,"cdate":1511549592757,"id":"H1bj91Ixz","invitation":"ICLR.cc/2018/Conference/-/Paper634/Public_Comment","forum":"Sy1f0e-R-","replyto":"r1E16Vmxz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"IMHO","comment":"This review, in particular it's a public comment, makes a valid point. A paper titled \"An empirical study on evaluation metrics of generative adversarial networks\" should consider all evaluation metrics out there or at least give reasons as to why some were not considered."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511545124190,"tcdate":1511545124190,"number":3,"cdate":1511545124190,"id":"HJ27tABef","invitation":"ICLR.cc/2018/Conference/-/Paper634/Public_Comment","forum":"Sy1f0e-R-","replyto":"Sy1f0e-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Questions","comment":"1) This may not be a very relevant question. But from your current knowledge of evaluation metrics of GANs, what kind of evaluation metrics are relevant for using for other GAN tasks such as unpaired translation (e.g. CycleGAN)?   \n\n2) Can you say anything with evaluation metrics for text generation?\n\n3) I'm also curious as to why FID was omitted, and I'd like to know which one would be better, FID or 1-NN, and in addition the sample efficiency of FID in particular.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1512222706752,"tcdate":1511407689617,"number":1,"cdate":1511407689617,"id":"S1zLl6mlz","invitation":"ICLR.cc/2018/Conference/-/Paper634/Official_Review","forum":"Sy1f0e-R-","replyto":"Sy1f0e-R-","signatures":["ICLR.cc/2018/Conference/Paper634/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A good survey of GAN evaluation metrics with exhaustive experimental evaluations.","rating":"8: Top 50% of accepted papers, clear accept","review":"In the paper, the authors discuss several GAN evaluation metrics.\nSpecifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy.\nFor those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not.\nSection 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used.\n\nI think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs.\nIn particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons.\nThe result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable.\nThis would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution.\nHowever, the result is supported by exhaustive experiments making the result highly convincing.\n\nOverall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1511374044083,"tcdate":1511374044083,"number":2,"cdate":1511374044083,"id":"r1E16Vmxz","invitation":"ICLR.cc/2018/Conference/-/Paper634/Public_Comment","forum":"Sy1f0e-R-","replyto":"Sy2r0j_yG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Shameless","comment":"This \"review\" appears to be self-promotion."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1510682180138,"tcdate":1510682180138,"number":1,"cdate":1510682180138,"id":"Sy2r0j_yG","invitation":"ICLR.cc/2018/Conference/-/Paper634/Public_Comment","forum":"Sy1f0e-R-","replyto":"Sy1f0e-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Fréchet Inception Distance (FID) for evaluating GANs","comment":"[1] proposed the Fréchet Inception Distance (FID) for evaluating GANs which is not mentioned here. To gain a broader insight into evaluation metrics of GANs the authors should also discuss this quality measure.\n\n[1] https://arxiv.org/abs/1706.08500"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]}},{"tddate":null,"ddate":null,"tmdate":1509739189724,"tcdate":1509129735263,"number":634,"cdate":1509739187064,"id":"Sy1f0e-R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy1f0e-R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"An empirical study on evaluation metrics of generative adversarial networks","abstract":"Despite the widespread interest in generative adversarial networks (GANs), few works have studied the metrics that quantitatively evaluate GANs' performance. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the important problem of \\emph{how to evaluate the evaluation metrics}. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. Then with a series of carefully designed experiments,  we are able to comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbour (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far these state-of-the-art GANs are from perfect.","pdf":"/pdf/d7a4b1653cb089ddb4ce2246da47ac7931e3e898.pdf","paperhash":"anonymous|an_empirical_study_on_evaluation_metrics_of_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018an,\n  title={An empirical study on evaluation metrics of generative adversarial networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy1f0e-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper634/Authors"],"keywords":["generative adversarial networks","evaluation metric"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}