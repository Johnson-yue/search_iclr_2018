{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222828294,"tcdate":1512076068770,"number":3,"cdate":1512076068770,"id":"Hy6mmeCgf","invitation":"ICLR.cc/2018/Conference/-/Paper963/Official_Review","forum":"r1uOhfb0W","replyto":"r1uOhfb0W","signatures":["ICLR.cc/2018/Conference/Paper963/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A useful approach for making model averaging more feasible","rating":"6: Marginally above acceptance threshold","review":"The authors note that several recent papers have shown that bayesian model averaging is an effective and universal way to improve hold-out performance, but unfortunately are limited by increased computational costs.   Towards that end, the authors of this manuscript propose several modifications to this procedure to make it computationally feasible and indeed improve performance.\n\nPros:\nThe authors demonstrate an effective procedure for FNN and LSTMs that makes model averaging improve performance.\nEmpirical evidence is convincing on the utility of the approach.\n\nCons:\nNot clear how this approach would be used with convolutional structures\nMuch of the benefit appears to come from the sparse prior, pruning, and retraining (Figure 3).  The model averaging seems to have a smaller contribution.  Due to that, it seems that the nature of the contribution needs to be clarified compared to the large literature on sparsifying neural networks, and the introductory comments of the paper should be rewritten to reflect that reality.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning","abstract":"A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. \nThis is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 12\\% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40\\% of model parameters and 90\\% of computations in total, as compared to a state-of-the-art large LSTM LM.","pdf":"/pdf/d2c875aa64cf80b7259ff1219d9b6450536bbcdb.pdf","TL;DR":"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with greater performance and significantly lower cost than traditional methods. ","paperhash":"anonymous|learning_sparse_structured_ensembles_with_sgmcmc_and_network_pruning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1uOhfb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper963/Authors"],"keywords":["ensemble learning","SG-MCMC","group sparse prior","network pruning"]}},{"tddate":null,"ddate":null,"tmdate":1512222828339,"tcdate":1511814577120,"number":2,"cdate":1511814577120,"id":"BJt3Bg5gM","invitation":"ICLR.cc/2018/Conference/-/Paper963/Official_Review","forum":"r1uOhfb0W","replyto":"r1uOhfb0W","signatures":["ICLR.cc/2018/Conference/Paper963/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A more principled way for training ensemble neural networks, nice idea and experiments","rating":"6: Marginally above acceptance threshold","review":"In this paper, the authors present a new framework for training ensemble of neural networks. The approach is based on the recent scalable MCMC methods, namely the stochastic gradient Langevin dynamics.\n\nThe paper is overall well-written and ideas are clear. The main contributions of the paper, namely using SG-MCMC methods within deep learning, and then increasing the computational efficiency by group sparsity+pruning are valuable and can have a significant impact in the domain. Besides, the proposed approach is more elegant the competing ones, while still not being theoretically justified completely. \n\nI have the following minor comments:\n\n1) The authors mention that retraining significantly improves the performance, even without pruning. What is the explanation for this? If there is no pruning, I would expect that all the samples would converge to the same minimum after retraining. Therefore, the reason why retraining improves the performance in all cases is not clear to me.\n\n2) The notation |\\theta_g| is confusing, the authors should use a different symbol.\n\n3) After section 4, the language becomes quite informal sometimes, the authors should check the sentences once again.\n\n4) The results with SGD (1 model) + GSP + PR should be added in order to have a better understanding of the improvements provided by the ensemble networks. \n\n5) Why does the performance get worse \"obviously\" when the pruning is 95% and why is it not obvious when the pruning is 90%?\n\n6) There are several typos\n\npg7: drew -> drawn\npg7: detail -> detailed\npg7: changing -> challenging\npg9: is strongly depend on -> depends on\npg9: two curve -> two curves","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning","abstract":"A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. \nThis is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 12\\% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40\\% of model parameters and 90\\% of computations in total, as compared to a state-of-the-art large LSTM LM.","pdf":"/pdf/d2c875aa64cf80b7259ff1219d9b6450536bbcdb.pdf","TL;DR":"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with greater performance and significantly lower cost than traditional methods. ","paperhash":"anonymous|learning_sparse_structured_ensembles_with_sgmcmc_and_network_pruning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1uOhfb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper963/Authors"],"keywords":["ensemble learning","SG-MCMC","group sparse prior","network pruning"]}},{"tddate":null,"ddate":null,"tmdate":1512222828383,"tcdate":1511811365999,"number":1,"cdate":1511811365999,"id":"B1A7YkceM","invitation":"ICLR.cc/2018/Conference/-/Paper963/Official_Review","forum":"r1uOhfb0W","replyto":"r1uOhfb0W","signatures":["ICLR.cc/2018/Conference/Paper963/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Justification for the proposed algorithm is weak + weak experiments.","rating":"4: Ok but not good enough - rejection","review":"The authors propose a procedure to generate an ensemble of sparse structured models. To do this, the authors propose to (1) sample models using SG-MCMC with group sparse prior, (2) prune hidden units with small weights, (3) and retrain weights by optimizing each pruned model. The ensemble is applied to MNIST classification and language modelling on PTB dataset. \n\nI have two major concerns on the paper. First, the proposed procedure is quite empirically designed. So, it is difficult to understand why it works well in some problems. Particularly. the justification on the retraining phase is weak. It seems more like to use SG-MCMC to *initialize* models which will then be *optimized* to find MAP with the sparse-model constraints. The second problem is about the baselines in the MNIST experiments. The FNN-300-100 model without dropout, batch-norm, etc. seems unreasonably weak baseline. So, the results on Table 1 on this small network is not much informative practically. Lastly, I also found a significant effort is also desired to improve the writing. \n\nThe following reference also needs to be discussed in the context of using SG-MCMC in RNN.\n- \"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\", Zhe Gan*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning","abstract":"A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. \nThis is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 12\\% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40\\% of model parameters and 90\\% of computations in total, as compared to a state-of-the-art large LSTM LM.","pdf":"/pdf/d2c875aa64cf80b7259ff1219d9b6450536bbcdb.pdf","TL;DR":"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with greater performance and significantly lower cost than traditional methods. ","paperhash":"anonymous|learning_sparse_structured_ensembles_with_sgmcmc_and_network_pruning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1uOhfb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper963/Authors"],"keywords":["ensemble learning","SG-MCMC","group sparse prior","network pruning"]}},{"tddate":null,"ddate":null,"tmdate":1509557426706,"tcdate":1509557426706,"number":1,"cdate":1509557426706,"id":"HysnNFwA-","invitation":"ICLR.cc/2018/Conference/-/Paper963/Public_Comment","forum":"r1uOhfb0W","replyto":"r1uOhfb0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"DNNs with structured sparsity learning","comment":"In one category of your related work -- \"Sparse structure learning\", some previous works [1][2] used group Lasso regularization during SGD to directly learn structurally sparse DNNs for computation efficiency and memory saving. Compare or clarify the difference might make this work more comprehensive.\n\n[1] http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf\n[2] http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning","abstract":"A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. \nThis is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 12\\% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40\\% of model parameters and 90\\% of computations in total, as compared to a state-of-the-art large LSTM LM.","pdf":"/pdf/d2c875aa64cf80b7259ff1219d9b6450536bbcdb.pdf","TL;DR":"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with greater performance and significantly lower cost than traditional methods. ","paperhash":"anonymous|learning_sparse_structured_ensembles_with_sgmcmc_and_network_pruning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1uOhfb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper963/Authors"],"keywords":["ensemble learning","SG-MCMC","group sparse prior","network pruning"]}},{"tddate":null,"ddate":null,"tmdate":1510092383450,"tcdate":1509137520566,"number":963,"cdate":1510092361115,"id":"r1uOhfb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1uOhfb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning","abstract":"A ensemble of neural networks is known to be more robust and accurate than an individual network, however with linearly-increased cost in both training and testing. \nIn this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\nIn the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\nIn this way of learning SSEs with SG-MCMC and pruning, we reduce memory and computation cost significantly in both training and testing of NN ensembles, while maintaining high prediction accuracy. \nThis is empirically verified in the experiments of learning SSE ensembles of both FNNs and LSTMs.\nFor example, in LSTM based language modeling (LM), we obtain 12\\% relative improvement in LM perplexity by learning a SSE of 4 large LSTM models, which has only 40\\% of model parameters and 90\\% of computations in total, as compared to a state-of-the-art large LSTM LM.","pdf":"/pdf/d2c875aa64cf80b7259ff1219d9b6450536bbcdb.pdf","TL;DR":"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with greater performance and significantly lower cost than traditional methods. ","paperhash":"anonymous|learning_sparse_structured_ensembles_with_sgmcmc_and_network_pruning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1uOhfb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper963/Authors"],"keywords":["ensemble learning","SG-MCMC","group sparse prior","network pruning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}