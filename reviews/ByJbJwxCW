{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222611935,"tcdate":1511933887797,"number":3,"cdate":1511933887797,"id":"Hyu6DTogG","invitation":"ICLR.cc/2018/Conference/-/Paper283/Official_Review","forum":"ByJbJwxCW","replyto":"ByJbJwxCW","signatures":["ICLR.cc/2018/Conference/Paper283/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A new MIL formulation with limited technical innovation and unconvincing experimental results.","rating":"4: Ok but not good enough - rejection","review":"This paper proposed a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. The paper also explored integrating RMIL with various attention mechanisms, and demonstrated its usage on medical concept prediction from time series data.\n\nThe biggest technical innovation in this paper is it combines recurrent networks like Bi-LSTM with MIL to model the relations among instances. Other than that, the paper has limited technical innovations: the pooling functions were proposed earlier and their integration with MIL was widely studied before (as cited by the authors); the attention mechanisms are also proposed by others.\n\nHowever, I am doubtful whether it’s appropriate to use LSTM to model the relations among instances. In general MIL, there exists no temporal order among instances, so modeling them with a LSTM is unjustified. It might be acceptable is the authors are focusing on time-series data; but in this case, it’s unclear why the authors are applying MIL on it. It seems other learning paradigm could be more appropriate.\n\nThe biggest concern I have with this paper is the unconvincing experiments. First, the baselines are very weak. Both MISVM and DPMIL are MIL methods without using deep learning features. It them becomes very unclear how much of the gain on Table 3 is from the use of deep learning, and how much is from the proposed RMIL.\n\nAlso, although the authors conducted a number of ablation studies, they don’t really tell us much. Basically, all variants of the algorithm perform as well, so it’s confusing why we need so many of them, or whether they can be integrated as a better model.\n\nThis could also be due to the small dataset. As the authors are proposing a new MIL learning paradigm, I feel they should experiment on a number of MIL tasks, not limited to analyzing time series medical data. The current experiments are quite narrow in terms of scope.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relational Multi-Instance Learning for Concept Annotation from Medical Time Series","abstract":"Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.","pdf":"/pdf/37347268bab99866c7147958a2779fef15eb6118.pdf","TL;DR":"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks.","paperhash":"anonymous|relational_multiinstance_learning_for_concept_annotation_from_medical_time_series","_bibtex":"@article{\n  anonymous2018relational,\n  title={Relational Multi-Instance Learning for Concept Annotation from Medical Time Series},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJbJwxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper283/Authors"],"keywords":["Multi-instance learning","Medical Time Series","Concept Annotation"]}},{"tddate":null,"ddate":null,"tmdate":1512222612800,"tcdate":1511826177767,"number":2,"cdate":1511826177767,"id":"rkcWXX9gf","invitation":"ICLR.cc/2018/Conference/-/Paper283/Official_Review","forum":"ByJbJwxCW","replyto":"ByJbJwxCW","signatures":["ICLR.cc/2018/Conference/Paper283/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Relational Multi-Instance Learning for Concept Annotation from Medical Time Series","rating":"3: Clear rejection","review":"This paper proposes a framework called 'multi-instance learning', in which a time series is treated as a 'set' of observations, and label is assigned to the full set, rather than individual observations. In this framework, authors propose to do set-level prediction (using pooling) and observation level predictions (using various attention mechanisms). \nThey test their approach in a medical setting, where the goal is to annotate vital signs time series by clinical events. Their cohort is 2014 adults time series (average length 4 time steps), and their time series has dimension of 21 and their clinical events have dimension of 26. Their baselines are other 'multi-instance learning' prior work and results are achieved through cross-validation. A few of the relevant hyper-parameters are tuned and some important hyper-parameters (i.e. number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. \n\nOriginality - I find the paper to be very incremental in terms of originality of the method. \n\nQuality and Significance - Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments. Given that results are reported via cross-validation and without a true held-out dataset, and given that a number of hyperparameters are not even tuned, it is difficult to be confident that the differences of all the methods reported are significant. \n\nClarity - The writing has good clarity.\n\nMajor issues with the paper: \n- Lack of reliable experiment section. Dataset is too small (2000 total samples), and model training is not described with enough details in terms of hyper-parameters tuned. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relational Multi-Instance Learning for Concept Annotation from Medical Time Series","abstract":"Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.","pdf":"/pdf/37347268bab99866c7147958a2779fef15eb6118.pdf","TL;DR":"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks.","paperhash":"anonymous|relational_multiinstance_learning_for_concept_annotation_from_medical_time_series","_bibtex":"@article{\n  anonymous2018relational,\n  title={Relational Multi-Instance Learning for Concept Annotation from Medical Time Series},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJbJwxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper283/Authors"],"keywords":["Multi-instance learning","Medical Time Series","Concept Annotation"]}},{"tddate":null,"ddate":null,"tmdate":1512222612842,"tcdate":1511220259794,"number":1,"cdate":1511220259794,"id":"Hk2mNy-gG","invitation":"ICLR.cc/2018/Conference/-/Paper283/Official_Review","forum":"ByJbJwxCW","replyto":"ByJbJwxCW","signatures":["ICLR.cc/2018/Conference/Paper283/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall, this is a reasonable paper with no obvious major flaws.  The novelty and impact may be greater on the application side than on the methodology side.","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the classification of medical time-series data by formulating the problem as a multi-instance learning (MIL) task, where there is an instance for each timestep of each time series, labels are observed at the time-series level (i.e. for each bag), and the goal is to perform instance-level and series-level (i.e. bag-level) prediction.  The main difference from the typical MIL setup is that there is a temporal relationship between the instances in each bag.  The authors propose to model this using a recurrent neural network architecture.  The aggregation function which maps instance-level labels to bag-level labels is modeled using a pooling layer (this is actually a nice way to describe multi-instance classification assumptions using neural network terminology).  An attention mechanism is also used.\n\nThe proposed time-series MIL problem formulation makes sense.  The RNN approach is novel to this setting, if somewhat incremental.  One very positive aspect is that results are reported exploring the impact of the choice of recurrent neural network architecture, pooling function, and attention mechanism.  Results on a second dataset are reported in the appendix, which greatly increases confidence in the generalizability of the experiments.  One or more additional datasets would have helped further solidify the results, although I appreciate that medical datasets are not always easy to obtain.  Overall, this is a reasonable paper with no obvious major flaws.  The novelty and impact may be greater on the application side than on the methodology side.\n\nMinor suggestions:\n\n-The term \"relational multi-instance learning\" seems to suggest a greater level of generality than the work actually accomplishes.  The proposed methods can only handle time-series / longitudinal dependencies, not arbitrary relational structure.  Moreover, multi-instance learning is typically viewed as an intermediary level of structure \"in between\" propositional learning (i.e. the standard supervised learning setting) and fully relational learning, so the \"relational multi-instance learning\" terminology sounds a little strange. Cf.:\nDe Raedt, L. (2008). Logical and relational learning. Springer Science & Business Media.\n\n-Pg 3, a capitalization typo: \"the Multi-instance learning framework\"\n\n-The equation for the bag classifier on page 4 refers to the threshold-based MI assumption, which should be attributed to the following paper:\nWeidmann, N., Frank, E. & Pfahringer, B. 2003. A two-level learning method for generalized multi-instance problems. In Proceedings of the 14th European Conference on Machine Learning,\nSpringer, 468-479.\n(See also: J. R. Foulds and E. Frank. A review of multi-instance learning assumptions. Knowledge Engineering Review, 25(1):1-25, 2010. )\n\n- Pg 5, \"Table 1\" vs \"table 1\" - be consistent.\n\n-A comparison to other deep learning MIL methods, i.e. those that do not exploit the time-series nature of the problem, would be valuable.  I wouldn't be surprised if other reviewers insist on this.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relational Multi-Instance Learning for Concept Annotation from Medical Time Series","abstract":"Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.","pdf":"/pdf/37347268bab99866c7147958a2779fef15eb6118.pdf","TL;DR":"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks.","paperhash":"anonymous|relational_multiinstance_learning_for_concept_annotation_from_medical_time_series","_bibtex":"@article{\n  anonymous2018relational,\n  title={Relational Multi-Instance Learning for Concept Annotation from Medical Time Series},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJbJwxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper283/Authors"],"keywords":["Multi-instance learning","Medical Time Series","Concept Annotation"]}},{"tddate":null,"ddate":null,"tmdate":1509739386724,"tcdate":1509089014786,"number":283,"cdate":1509739384076,"id":"ByJbJwxCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByJbJwxCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Relational Multi-Instance Learning for Concept Annotation from Medical Time Series","abstract":"Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.","pdf":"/pdf/37347268bab99866c7147958a2779fef15eb6118.pdf","TL;DR":"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks.","paperhash":"anonymous|relational_multiinstance_learning_for_concept_annotation_from_medical_time_series","_bibtex":"@article{\n  anonymous2018relational,\n  title={Relational Multi-Instance Learning for Concept Annotation from Medical Time Series},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJbJwxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper283/Authors"],"keywords":["Multi-instance learning","Medical Time Series","Concept Annotation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}