{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222614968,"tcdate":1512033219594,"number":3,"cdate":1512033219594,"id":"HJ2pirpxG","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A novel contribution to sequence generation","rating":"7: Good paper, accept","review":"This paper considers the problem of improving sequence generation by learning better metrics. Specifically, it focuses on addressing the exposure bias problem, where traditional methods such as SeqGAN uses GAN framework and reinforcement learning. Different from these work, this paper does not use GAN framework. Instead, it proposed an expert-based reward function training, which trains the reward function (the discriminator) from data that are generated by randomly modifying parts of the expert trajectories. Furthermore, it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data. This is similar to the idea of hierarchical RL, which divide the problem into potential subtasks, which could alleviate the difficulty of reinforcement learning from sparse rewards. The idea of the paper is novel. However, there are a few points to be clarified.\n\nIn Section 3.2 and in (4) and (5), the authors explains how the action value Q_{D_i} is modeled and estimated for the partial reward function D_i of length L_{D_i}. But the authors do not explain how the rewards (or action value functions) of different lengths are aggregated together to update the model using policy gradient. Is it a simple sum of all of them?\n\nIt is not clear why the future subsequences that do not contain y_{t+1} are ignored for estimating the action value function Q in (4) and (5). The authors stated that it is for reducing the computation complexity. But it is not clear why specifically dropping the sequences that do not contain y_{t+1}. Please clarify more on this point.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222615010,"tcdate":1511539169294,"number":2,"cdate":1511539169294,"id":"SkY1f6Hlf","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces an RL approach to generating time series data without the difficult training of GANs. Unfortunately, the paper is too poorly written to be clear or effective.","rating":"3: Clear rejection","review":"This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value.  Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient.\n\nUnfortunately, this description is a little vague, because the paper's details are quite difficult to understand.  Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled.  Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained.\n\nThis paper could be interesting, but substantial editing is needed before it is sufficient for publication.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222615050,"tcdate":1511278646169,"number":1,"cdate":1511278646169,"id":"H104OpbgM","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Using IRL techniques instead of GANs for sequence generation","rating":"7: Good paper, accept","review":"This article is a follow-up from recent publications (especially the one on \"seqGAN\" by Yu et al. @ AAAI 2017) which tends to assimilate Generative Adversarial Networks as an Inverse Reinforcement Learning task in order to obtain a better stability.\nThe adversarial learning is replaced here by a combination of policy gradient and a learned reward function.\n\nIf we except the introduction which is tainted with a few typos and English mistakes, the paper is clear and well written. The experiments made on both synthetic and real text data seems solid.\nBeing not expert in GANs I found it pleasant to read and instructive.\n\n\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1509739383476,"tcdate":1509090134984,"number":289,"cdate":1509739380822,"id":"r1kP7vlRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}