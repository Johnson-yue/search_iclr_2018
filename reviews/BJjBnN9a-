{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222652026,"tcdate":1511836976349,"number":2,"cdate":1511836976349,"id":"S1_ETHcef","invitation":"ICLR.cc/2018/Conference/-/Paper43/Official_Review","forum":"BJjBnN9a-","replyto":"BJjBnN9a-","signatures":["ICLR.cc/2018/Conference/Paper43/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes an extension of CNN to the case where data features are continuous functions in a RKHS. The paper is well written and provides some new insights on incorporating kernels in CNN.","rating":"6: Marginally above acceptance threshold","review":"This paper aims to provide a continuous variant of CNN. The main idea is to apply CNN on Hilbert maps of the data. The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix. A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets.\n\nThe paper is well written and provides some new insights on incorporating kernels in CNN.\n\nThe kernel matrix in Eq. 5 is not symmetric and the kernel function in Eq. 3 is not defined over a pair of inputs. In this case, the projections of the data via the kernel are not necessarily in a RKHS. The connection between Hilbert maps and RKHS in that sense is not clear in the paper.\n\nThe size of a kernel matrix depends on the sample size. In large scale situations, working with the kernel matrix can be computational expensive. It is not clear how this issue is addressed in this paper.\n\nIn section 2.2, how \\mu_i and \\sigma_i are computed?\n\nHow the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Convolutional Neural Networks for Image Classification","abstract":"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases. We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS. Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters. This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity. Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","pdf":"/pdf/b4f4bb1f9b15a20e61a304985ac7532e402a8319.pdf","TL;DR":"This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space.","paperhash":"anonymous|continuous_convolutional_neural_networks_for_image_classification","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Convolutional Neural Networks for Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjBnN9a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper43/Authors"],"keywords":["convolutional neural networks","image classification","deep learning","feature representation","hilbert maps","reproducing kernel hilbert space"]}},{"tddate":null,"ddate":null,"tmdate":1512222652064,"tcdate":1511822039632,"number":1,"cdate":1511822039632,"id":"rklk7M9ez","invitation":"ICLR.cc/2018/Conference/-/Paper43/Official_Review","forum":"BJjBnN9a-","replyto":"BJjBnN9a-","signatures":["ICLR.cc/2018/Conference/Paper43/AnonReviewer1"],"readers":["everyone"],"content":{"title":"nice idea. lack in motivation and insights","rating":"5: Marginally below acceptance threshold","review":"The paper introduces the notion of continuous convolutional neural networks. \nThe main idea of the paper is to project examples into an RK Hilbert space\nand performs convolution and filtering into that space. Interestingly, the\nfilters defined in the Hilbert space  have parameters that are learnable.\n\nWhile the idea may be novel and interesting, its motivation is not clear for\nme. Is it for space? for speed? for expressivity of hypothesis spaces? \nMost data that are available for learning are in discrete forms and hopefully,\nthey have been digitalized according to Shannon theory. This means that they bring\nall necessary information for rebuilding their continuous counterpart. Hence, it is\nnot clear why projecting them back into continuous functions is of interest. \n\nAnother point that is not clear or at least misleading is the so-called Hilbert Maps.\nAs far as I understand, Equation (4) is not an embedding into an Hilbert space but\nis more a proximity space representation [1]. Hence, the learning framework of the\nauthors can be casted more as a learning with similarity function than learning\ninto a RKHS [2]. A proper embedding would have mapped $x$ into a function\nbelonging to $\\mH$. In addition, it seems that all computations are done\ninto a \\ell^2 space instead of in the RKHS (equations 5 and 11). \nLearning good similarity functions is also not novel [3] and Equations\n(6) and (7) corresponds to learning these similarity functions.\nAs far as I remember, there exists also some paper from the nineties that\nlearn the parameters of RBF networks but unfortunately I have not been able to\ngoogle some of them.\n\n\nPart 3 is the most interesting part of the paper, however it would have been\ngreat if the authors provide other kernel functions with closed-form convolution \nformula that may be relevant for learning.\nThe proposed methodology is evaluated on some standard benchmarks in vision. While\nresults are pretty good, it is not clear how the various cluster sets have been obtained\nand what are their influence on the performances (if they are randomly initialized, it \nwould be great to see standard deviation of performances with respect to initializations).\nI would also be great to have intuitions on why a single continuous filter works betters\nthan 20 discrete ones (if this behaviour is consistent accross initialization).\n\nOn the overall, while the idea may be of interested, the paper lacks in motivations\nin connecting to relevant previous works and in providing insights on why it works.\nHowever, performance results seem to be competitive and that's the reader may\nbe eager for insights.\n\n\nminor comments\n---------------\n\n* the paper employs vocabulary that is not common in ML. eg. I am not sure what\noccupancy values, or inducing points are. \n\n* Supposingly that the authors properly consider computation in RKHS, then \\Sigma_i\nshould be definite positive right? how update in (7) is guaranteed to be DP? \nThis constraints may not be necessary if instead they used proximity space representation.\n\n\n\n\n\n[1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf\n[2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf\n[3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Convolutional Neural Networks for Image Classification","abstract":"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases. We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS. Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters. This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity. Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","pdf":"/pdf/b4f4bb1f9b15a20e61a304985ac7532e402a8319.pdf","TL;DR":"This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space.","paperhash":"anonymous|continuous_convolutional_neural_networks_for_image_classification","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Convolutional Neural Networks for Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjBnN9a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper43/Authors"],"keywords":["convolutional neural networks","image classification","deep learning","feature representation","hilbert maps","reproducing kernel hilbert space"]}},{"tddate":null,"ddate":null,"tmdate":1508700085212,"tcdate":1508699988940,"number":1,"cdate":1508699988940,"id":"ByTIk_5pZ","invitation":"ICLR.cc/2018/Conference/-/Paper43/Public_Comment","forum":"BJjBnN9a-","replyto":"BJjBnN9a-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related Literature on Continuous Neural Networks","comment":"Very interesting approach to continuously relax convolutional filters. There's been substantial literature on infinite dimensional neural networks, such as your proposal, since the early 90s, that you might want to cite in your work. This paper (https://openreview.net/forum?id=H1pri9vTZ) outlines all of these approaches and gives some theoretical justification for your relaxation and others like it.\n\n\nBest of luck!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Convolutional Neural Networks for Image Classification","abstract":"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases. We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS. Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters. This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity. Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","pdf":"/pdf/b4f4bb1f9b15a20e61a304985ac7532e402a8319.pdf","TL;DR":"This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space.","paperhash":"anonymous|continuous_convolutional_neural_networks_for_image_classification","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Convolutional Neural Networks for Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjBnN9a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper43/Authors"],"keywords":["convolutional neural networks","image classification","deep learning","feature representation","hilbert maps","reproducing kernel hilbert space"]}},{"tddate":null,"ddate":null,"tmdate":1509739515304,"tcdate":1508686914660,"number":43,"cdate":1509739512652,"id":"BJjBnN9a-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJjBnN9a-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous Convolutional Neural Networks for Image Classification","abstract":"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases. We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS. Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters. This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity. Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","pdf":"/pdf/b4f4bb1f9b15a20e61a304985ac7532e402a8319.pdf","TL;DR":"This paper proposes a novel convolutional layer that operates in a continuous Reproducing Kernel Hilbert Space.","paperhash":"anonymous|continuous_convolutional_neural_networks_for_image_classification","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Convolutional Neural Networks for Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjBnN9a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper43/Authors"],"keywords":["convolutional neural networks","image classification","deep learning","feature representation","hilbert maps","reproducing kernel hilbert space"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}