{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222563927,"tcdate":1511781686147,"number":3,"cdate":1511781686147,"id":"B1RNB_tlG","invitation":"ICLR.cc/2018/Conference/-/Paper1170/Official_Review","forum":"Hymt27b0Z","replyto":"Hymt27b0Z","signatures":["ICLR.cc/2018/Conference/Paper1170/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A proposed new method to estimate mutual information which requires more thorough experiments.","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new method for estimation of mutual information (MI) based on the Donsker-Varhan (DV) representation of KL-divergence. This representation requires the calculation of a supremum over a set of functions and a lower bound is achieved when a neural network is used for the maximisation of it. Computing the DV representation also requires evaluating expectations wrt to the distributions of interest, the proposed method uses Monte-Carlo estimates based on the empirical distributions.\n\nThe experiments evaluating the quality of the OMIE estimator for mutual information should be more thorough to make a point that OMIE beats competing estimators. The bivariate Gaussian case presented in Figure 1 is not a very relevant test case as estimating MI is especially difficult in higher dimensions. It would also be interesting to know the number of samples used as the ratio nbr dimensions/samples matters for estimation quality. The caption for Figure 2 mentions “bivariate Gaussians of dimension 50”, do the author mean two Gaussians of dimension 50 each?\n\nThe results of the proposed method on the swiss-roll dataset look good, however the authors only provide a comparison to a classic GAN where it seems more natural to compare with the other works on mode-dropping for GAN cited in the related works section. A comparison with InfoGAN and Dai et al. would be especially relevant to evaluate the effectiveness of OMIE. \n\nOn the application of OMIE to the Information Bottleneck (IB) problem:\nHow was the optimization of the objective exactly performed? How are gradients calculated? Is the reparametrisation trick used? More details should be provided on the results presented in table 3. Are the results obtained on the test set? What was the value of beta and to which values of I(X,Z) and I(Z,Y) does it correspond? Was the misclassification rate averaged over multiple runs?\n\nThe generalization to f-divergences is interesting but seems rather straightforward. \n\nThe second line of equation (20) does not make sense to me, it is not equivalent to the first line.\n\nThe methods proposed in Alemi et al. and Chalk et al. differ also in the way the bounds are estimated, not only in the choice of the marginal distribution.\n\nThe authors mention that strong consistency and convergence properties (page 3) are proven in the appendix, however I could not find them.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"OMIE: The Online Mutual Information Estimator","abstract":"This paper presents OMIE, an Online Mutual Information Estimator(OMIE) that\nis linearly scalable both on the dimension of the data as well as sample size. OMIE\nis back-propable and we prove that it is strongly consistent. We apply the estimation\nprinciple underlying the mutual information estimator to general dependency\nf-divergence as well as integral probability metrics dependency measures. We illustrate a\nhandful of applications in which OMIE is succesfully applied to enhance\nthe property of generative models in both unsupervised and supervised settings.\nWe apply our framework to estimate the information bottleneck and apply it in\ntasks related to supervised classification problem and show that there is substantial\nadded flexibility and improvement in these settings","pdf":"/pdf/0d736ada7e156b950fdd5eb287d9f95a22d9c54c.pdf","TL;DR":"A scalable in sample size and dimensions mutual information estimator.","paperhash":"anonymous|omie_the_online_mutual_information_estimator","_bibtex":"@article{\n  anonymous2018omie:,\n  title={OMIE: The Online Mutual Information Estimator},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hymt27b0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1170/Authors"],"keywords":["Deep Learning","Neural Networks","Information Theory","Generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222563965,"tcdate":1511746928633,"number":2,"cdate":1511746928633,"id":"SkFdaJtgf","invitation":"ICLR.cc/2018/Conference/-/Paper1170/Official_Review","forum":"Hymt27b0Z","replyto":"Hymt27b0Z","signatures":["ICLR.cc/2018/Conference/Paper1170/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Apply existing variational representations of f-divergences to build a MI estimator mostly for GANs","rating":"5: Marginally below acceptance threshold","review":"The authors present an estimator for the mutual information (MI) based on the Donsker-Varadhan representation for the KL divergence and its generalization to arbitrary f-divergences by Ruderman et al. While that last work introduced an estimator based on optimization over the unit ball in an RKHS, the current work propose to use a parametric function class given by a neural network (I'd suggest that the authors make this point more explicit, as currently it's not totally clear what their actual contribution is and how their work compares to the prior art they cite). The authors show that such an estimator can be used to train models with less mode-dropping in adversarial models. \n\nThe work is quite straightforward, but improves over similar work in the GAN space by Nowozin et al. by using Ruderman's tighter variational representation instead of Nguyen's one.\n\nThe paper contains many typos and grammatical errors and the authors should do an exhaustive proof-reading. More problematic is that, right after eq. 10, the authors mention \"We show in the Appendix that OMIE has the desirable strong consistency and convergence properties\". However, the appendix doesn't contain such a proof. Is it missing from the submitted version? I don't think that such a consistency proof is strictly necessary for a paper like this, but for the review to be accurate I need to see the proof. Since I can't find it, I assume it does not exist. In that case, the authors should give less emphasis to the MI estimator itself and more to the empirical properties and applications.\n\nThe authors present some experiments comparing different estimators of MI applied to synthetic data. Figure 1 is hard to read, I suggest the authors try to come up with a more legible plot. Figure 2 is also a bit surprising, why show error for 50 dimensions but estimates for 2 dimensions? Since these experiments are quick to run, it would be helpful to get more information on how the gap between the methods change as the dimensionality increases (e.g. a surface plot with d and # of iterations on the x and y axes). Also it would be highly beneficial to compare with the method in Ruderman at al., so that people interested in MI estimation but who don't plan on using the estimator as part of a neural net architecture can get some idea on how the inductive bias of NNs compare to RKHS.\n\nIn the caption to Fig. 3 the authors state \"The OMIEGAN generator learns a distribution with a high amount of structured noise\", which I find hard to understand. Probably the authors can be a bit more precise than saying \"structured noise\".\n\nI would recommend dropping the Information Bottleneck section to focus on showing more convincingly the impact of OMIE in GANs. The experiments section currently looks rushed and lacking in depth.\n\nIn summary, this work provides value by introducing a (previously known) superior f-divergence variational representation to the GAN community. The mode-collapse prevention via MI maximisation is also interesting and deserves more experimental attention to make the paper stronger.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"OMIE: The Online Mutual Information Estimator","abstract":"This paper presents OMIE, an Online Mutual Information Estimator(OMIE) that\nis linearly scalable both on the dimension of the data as well as sample size. OMIE\nis back-propable and we prove that it is strongly consistent. We apply the estimation\nprinciple underlying the mutual information estimator to general dependency\nf-divergence as well as integral probability metrics dependency measures. We illustrate a\nhandful of applications in which OMIE is succesfully applied to enhance\nthe property of generative models in both unsupervised and supervised settings.\nWe apply our framework to estimate the information bottleneck and apply it in\ntasks related to supervised classification problem and show that there is substantial\nadded flexibility and improvement in these settings","pdf":"/pdf/0d736ada7e156b950fdd5eb287d9f95a22d9c54c.pdf","TL;DR":"A scalable in sample size and dimensions mutual information estimator.","paperhash":"anonymous|omie_the_online_mutual_information_estimator","_bibtex":"@article{\n  anonymous2018omie:,\n  title={OMIE: The Online Mutual Information Estimator},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hymt27b0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1170/Authors"],"keywords":["Deep Learning","Neural Networks","Information Theory","Generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222564007,"tcdate":1511499477779,"number":1,"cdate":1511499477779,"id":"ByA0IXBlz","invitation":"ICLR.cc/2018/Conference/-/Paper1170/Official_Review","forum":"Hymt27b0Z","replyto":"Hymt27b0Z","signatures":["ICLR.cc/2018/Conference/Paper1170/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Potentially workable idea, but needs a lot more work","rating":"3: Clear rejection","review":"Summary\n======================================================================\nThe authors propose an estimator for the Shannon Mutual Information that is based on\nthe Donsker Varadhan lower bound. The idea is to choose an expressive class of functions\n(in this case, parametrized by a NN) and maximise a statistic. The authors present\nsome applications for the proposed estimator.\n\nWhile the idea of using the Donsker-Varadhan lower bound is interesting and potentially\nworkable, the theory is not strong and the experiments are far from compelling to warrant\nacceptance.\n\nDetailed Review\n======================================================================\n\nWhy is this called an online MI estimator? Nothing about the formalism or the estimator\nuses an online learning approach.\n\nDespite the authors' claims, the estimator does not come with any theoretical guarantees.\n- What is your definition of strongly consistent? Since the MI is a scalar quantity,\n  strong consistency is the same as (weak) consistency.\n- The proof is not given, and I don't think the proposed estimator would be consistent if\n  you use a fixed class of networks T. There is a necessarily a bias between the class of\n  functions the network can approximate and all bounded functions.\n\nMissing citations: There is a ton of recent work on estimating mutual information\nthat the authors have missed. These are a few but you should also look at papers that\ncite these and are cited by these.\n- Kandasamy et al 2015. Nonparametric von Mises estimators for entropies, divergences and\n  mutual informations.\n- Singh & Poczos 2016. Finite-sample analysis of fixed kNN density functional estimators.\n- Moon et al 2017. Ensemble estimation of Mutual Information. \n\nIn Algorithm 1, why do the samples have to be inside the loop? What is wrong with\napplying the last two lines on the same dataset? On the same note, do you really need\n\\bar{z}^(i) to be different from z^(i)?\n\nThe authors claims in the introduction that the non-parametric methods make critical\nassumptions while GANs do not is misleading. Many of the methods make assumptions for the\ntheoretical analysis - in practice, some, if not most of them work well even when the\nassumptions do not hold. Similarly, if you want to prove something about GANs you\nprobably have to make assumptions too.\n\nExperiments:\nThe authors present 4 use cases. All of them are toy settings and none of them make a\ncompelling case for the proposed estimator.\n- In the GAN setting, I am failing to see why one would use a MI regularizer over an\n  entropic regularizer. It seems like what you need is entropy, and it is not clear what\n  happens to the conditional entropy term when you maximize MI.\n- Section 4.3: The bound on the reconstruction error is dropping a KL(p(z)||q(z)) term\n  and the authors don't really discuss how lose this is.\n- The authors make claims about scalability with n and d but none of the experiments\n  show the evaluation times compared to simpler estimators.\n\nMinor\n- I thought there were several unnecessary tangential discussions that didn't really add\n  much to the paper. For instance, Section 3.3 was unnecessary given that all the\n  experiments solely focused on the Shannon case. The para after theorem 1 on the\n  compression lemma doesn't add much. Even the definitions of the Shannon MI and Theorem\n  1 could have been stated without appealing to measure theory constructs.\n- Figure 1: This is perhaps not the cleanest way to present this graph. Perhaps consider\n  plotting the error in a log-scale might work better.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"OMIE: The Online Mutual Information Estimator","abstract":"This paper presents OMIE, an Online Mutual Information Estimator(OMIE) that\nis linearly scalable both on the dimension of the data as well as sample size. OMIE\nis back-propable and we prove that it is strongly consistent. We apply the estimation\nprinciple underlying the mutual information estimator to general dependency\nf-divergence as well as integral probability metrics dependency measures. We illustrate a\nhandful of applications in which OMIE is succesfully applied to enhance\nthe property of generative models in both unsupervised and supervised settings.\nWe apply our framework to estimate the information bottleneck and apply it in\ntasks related to supervised classification problem and show that there is substantial\nadded flexibility and improvement in these settings","pdf":"/pdf/0d736ada7e156b950fdd5eb287d9f95a22d9c54c.pdf","TL;DR":"A scalable in sample size and dimensions mutual information estimator.","paperhash":"anonymous|omie_the_online_mutual_information_estimator","_bibtex":"@article{\n  anonymous2018omie:,\n  title={OMIE: The Online Mutual Information Estimator},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hymt27b0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1170/Authors"],"keywords":["Deep Learning","Neural Networks","Information Theory","Generative models"]}},{"tddate":null,"ddate":null,"tmdate":1510092378961,"tcdate":1509141627174,"number":1170,"cdate":1510092359134,"id":"Hymt27b0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hymt27b0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"OMIE: The Online Mutual Information Estimator","abstract":"This paper presents OMIE, an Online Mutual Information Estimator(OMIE) that\nis linearly scalable both on the dimension of the data as well as sample size. OMIE\nis back-propable and we prove that it is strongly consistent. We apply the estimation\nprinciple underlying the mutual information estimator to general dependency\nf-divergence as well as integral probability metrics dependency measures. We illustrate a\nhandful of applications in which OMIE is succesfully applied to enhance\nthe property of generative models in both unsupervised and supervised settings.\nWe apply our framework to estimate the information bottleneck and apply it in\ntasks related to supervised classification problem and show that there is substantial\nadded flexibility and improvement in these settings","pdf":"/pdf/0d736ada7e156b950fdd5eb287d9f95a22d9c54c.pdf","TL;DR":"A scalable in sample size and dimensions mutual information estimator.","paperhash":"anonymous|omie_the_online_mutual_information_estimator","_bibtex":"@article{\n  anonymous2018omie:,\n  title={OMIE: The Online Mutual Information Estimator},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hymt27b0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1170/Authors"],"keywords":["Deep Learning","Neural Networks","Information Theory","Generative models"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}