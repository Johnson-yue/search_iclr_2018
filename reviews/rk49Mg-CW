{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222691840,"tcdate":1511696028828,"number":3,"cdate":1511696028828,"id":"S1riI7OxM","invitation":"ICLR.cc/2018/Conference/-/Paper565/Official_Review","forum":"rk49Mg-CW","replyto":"rk49Mg-CW","signatures":["ICLR.cc/2018/Conference/Paper565/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Convincing demonstration of stochastic video predictions on real data","rating":"7: Good paper, accept","review":"The submission presents a method or video prediction from single (or multiple) frames, which is capable of producing stochastic predictions by means of training a variational encoder-decoder model. Stochastic video prediction is a (still) somewhat under-researched direction, due to its inherent difficulty.\n\nThe method can take on several variants: time-invariant [latent variable] vs. time-variant, or action-conditioned vs unconditioned. The generative part of the method is mostly borrowed from Finn et al. (2016). Figure 1 clearly motivates the problem. The method itself is fairly clearly described in Section 3; in particular, it is clear why conditioning on all frames during training is helpful. As a small remark, however, it remains unclear what the action vector a_t is comprised of, also in the experiments.\n\nThe experimental results are good-looking, especially when looking at the provided web site images. \nThe main goal of the quantitative comparison results (Section 5.2) is to determine whether the true future is among the generated futures. While this is important, a question that remains un-discussed is whether all generated stochastic samples are from realistic futures. The employed metrics (best PSNR/SSIM among multiple samples) can only capture the former, and are also pixel-based, not perceptual.\n\nThe quantitative comparisons are mostly convincing, but Figure 6 needs some further clarification. It is mentioned in the text that \"time-varying latent sampling is more stable beyond the time horizon used during training\". While true for Figure 6b), this statement is contradicted by both Figure 6a) and 6c), and Figure 6d) seems to be missing the time-invariant version completely (or it overlaps exactly, which would also need explanation). As such, I'm not completely clear on whether the time variant or invariant version is the stronger performer.\n\nThe qualitative comparisons (Section 5.3) are difficult to assess in the printed material, or even on-screen. The animated images on the web site provide a much better impression of the true capabilities, and I find them convincing.\n\nThe experiments only compare to Reed et al. (2017)/Kalchbrenner et al. (2017), with Finn at al. (2016) as a non-stochastic baseline, but no comparisons to, e.g., Vondrick et al. (2016) are given. Stochastic prediction with generative adversarial networks is a bit dismissed in Section 2 with a mention of the mode-collapse problem.\n\nOverall the submission makes a significant enough contribution by demonstrating a (mostly) working stochastic prediction method on real data.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222691880,"tcdate":1511644215569,"number":2,"cdate":1511644215569,"id":"S1gH28vgM","invitation":"ICLR.cc/2018/Conference/-/Paper565/Official_Review","forum":"rk49Mg-CW","replyto":"rk49Mg-CW","signatures":["ICLR.cc/2018/Conference/Paper565/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"1) Summary\nThis paper proposed a new method for predicting multiple future frames in videos. A new formulation is proposed where the frames’ inherent noise is modeled separate from the uncertainty of the future. This separation allows for directly modeling the stochasticity in the sequence through a random variable z ~ p(z)  where the posterior  q(z | past and future frames) is approximated by a neural network, and as a result, sampling of a random future is possible through sampling from the prior p(z) during testing. The random variable z can be modeled in a time-variant and time-invariant way. Additionally, this paper proposes a training procedure to prevent their method from ignoring the stochastic phenomena modeled by z. In the experimental section, the authors highlight the advantages of their method in 1) a synthetic dataset of shapes meant to clearly show the stochasticity in the prediction, 2) two robotic arm datasets for video prediction given and not given actions, and 3) A challenging human action dataset in which they perform future prediction only given previous frames.\n\n\n\n2) Pros:\n+ Novel/Sound future frame prediction formulation and training for modeling the stochasticity of future prediction.\n+ Experiments on the synthetic shapes and robotic arm datasets highlight the proposed method’s power of multiple future frame prediction possible.\n+ Good analysis on the number of samples improving the chance of outputting the correct future, the modeling power of the posterior for reconstructing the future, and a wide variety of qualitative examples.\n+ Work is significant for the problem of modeling the stochastic nature of future frame prediction in videos.\n\n\n\n\n3) Cons:\nApproximate posterior in non-synthetic datasets:\nThe variable z seems to not be modeling the future very well. In the robot arm qualitative experiments, the robot motion is well modeled, however, the background is not. Given that for the approximate posterior computation the entire sequence is given (e.g. reconstruction is performed), I would expect the background motion to also be modeled well. This issue is more evident in the Human 3.6M experiments, as it seems to output blurriness regardless of the true future being observed. This problem may mean the method is failing to model a large variety of objects and clearly works for the robotic arm because a very similar large shape (e.g. robot arm) is seen in the training data. Do you have any comments on this?\n\n\n\nFinn et al 2016 PNSR performance on Human 3.6M:\nIs the same exact data, pre-processing, training, and architecture being utilized? In her paper, the PSNR for the first timestep on Human 3.6M is about 41 (maybe 42?)  while in this paper it is 38.\n\n\n\nAdditional evaluation on Human 3.6M:\nPSNR is not a good evaluation metric for frame prediction as it is biased towards blurriness, and also SSIM does not give us an objective evaluation in the sense of semantic quality of predicted frames. It would be good if the authors present additional quantitative evaluation to show that the predicted frames contain useful semantic information [1, 2, 3, 4]. For example, evaluating the predicted frames for the Human 3.6M dataset to see if the human is still detectable in the image or if the expected action is being predicted could be useful to verify that the predicted frames contain the expected meaningful information compared to the baselines.\n\n\n\nAdditional comments:\nAre all 15 actions being used for the Human 3.6M experiments? If so, the fact of the time-invariant model performs better than the time-variant one may not be the consistent action being performed (last sentence of 5.2). The motion performed by the actors in each action highly overlaps (talking on the phone action may go from sitting to walking a little to sitting again, and so on). Unless actions such as walking and discussion were only used, it is unlikely the time-invariant z is performing better because of consistent action. Do you have any comments on this?\n\n\n\n4) Conclusion\nThis paper proposes an interesting novel approach for predicting multiple futures in videos, however, the results are not fully convincing in all datasets. If the authors can provide additional quantitative evaluation besides PSNR and SSIM (e.g. evaluation on semantic quality), and also address the comments above, the current score will improve.\n\n\n\nReferences:\n[1] Emily Denton and Vighnesh Birodkar. Unsupervised Learning of Disentangled Representations from Video. In NIPS, 2017.\n[2] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In ICML, 2017.\n[3] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive Growing of GANs for Improved Quality, Stability, and Variation. arXiv preprint arXiv:1710.10196, 2017.\n[4] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved Techniques for Training GANs. In NIPS, 2017.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1511288615363,"tcdate":1511288615363,"number":1,"cdate":1511288615363,"id":"HJ1NygMgM","invitation":"ICLR.cc/2018/Conference/-/Paper565/Official_Comment","forum":"rk49Mg-CW","replyto":"H1i2t1egM","signatures":["ICLR.cc/2018/Conference/Paper565/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper565/Authors"],"content":{"title":"re: prior work","comment":"We apologize for missing that highly-relevant reference. We will include a reference in the next revision.\n\nNote that Walker et al. '16 does not predict video frames; thus, we cannot compare to the approach.  Unlike Walker et al. '16, our work does not require optical flow supervision nor an optical flow solver, which tend to not work consistently on real videos (as optical flow is an open research problem [1,2,3]). Our method only uses raw videos. Furthermore, we show that a CVAE trained from scratch does not work consistently, and propose a pre-training scheme which, in our experiments, consistently finds a good solution.\n\n[1] https://arxiv.org/abs/1705.01352\n[2] https://arxiv.org/abs/1612.01925\n[3] https://arxiv.org/abs/1604.01827"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1511156146935,"tcdate":1511156146935,"number":1,"cdate":1511156146935,"id":"H1i2t1egM","invitation":"ICLR.cc/2018/Conference/-/Paper565/Public_Comment","forum":"rk49Mg-CW","replyto":"rk49Mg-CW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Prior Work","comment":"The authors make the following claim:\n\n\"We believe, our approach is the first latent variable model to successfully demonstrate stochastic multi-frame video prediction on real world datasets.\"\n\nHowever, variational methods have been used before to forecast multiple frames in static images (An Uncertain Future: \nForecasting from Static Images using Variational Autoencoders, Walker et al., ECCV 2016). In this ECCV paper, the output space\nare dense pixel trajectories instead of direct pixels, but the model is trained on realistic videos of human activities. What distinguishes this proposed approach from prior work? The paper has been cited in references of other papers cited by the authors.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222691919,"tcdate":1510529018561,"number":1,"cdate":1510529018561,"id":"r17bOI8yG","invitation":"ICLR.cc/2018/Conference/-/Paper565/Official_Review","forum":"rk49Mg-CW","replyto":"rk49Mg-CW","signatures":["ICLR.cc/2018/Conference/Paper565/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Suggest an accept but it requires further revision","rating":"7: Good paper, accept","review":"Quality: above threshold\nClarity: above threshold, but experiment details are missing.\nOriginality: slightly above threshold.\nSignificance: above threshold\n\nPros:\n\nThis paper proposes a stochastic variational video prediction model. It can be used for prediction in optionally available external action cases. The inference network is a convolution net and the generative network is using a previously structure with minor modification. The result shows its ability to sample future frames and outperforms with methods in qualitative and quantitive metrics.\n\nCons:\n\n1. It is a nice idea and it seems to perform well in practice, but are there careful experiments justifying the 3-stage training scheme? For example, compared with other schemes like alternating between 3 stages, dynamically soft weighting terms. \n\n2. It is briefly mentioned in the context, but has there any attempt towards incorporating previous frames context for z, instead of sampling from prior? This piece seems much important in the scenarios which this paper covers.\n\n3. No details about training (training data size, batches, optimization) are provided in the relevant section, which greatly reduces the reproducibility and understanding of the proposed method. For example, it is not clear whether the model can generative samples that are not previously seen in the training set. It is strongly suggested training details be provided. \n\n4. Minor, If I understand correctly, in equation in the last paragraph above 3.1,  z instead of z_t \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739232891,"tcdate":1509126795685,"number":565,"cdate":1509739230225,"id":"rk49Mg-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk49Mg-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stochastic Variational Video Prediction","abstract":"Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.","pdf":"/pdf/92c9409ebde4427f8086eaffed2f1679f034c620.pdf","TL;DR":"Stochastic variational video prediction in real-world settings.","paperhash":"anonymous|stochastic_variational_video_prediction","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Variational Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk49Mg-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper565/Authors"],"keywords":["video prediction","stochastic prediction","variational inference","unsupervised learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}