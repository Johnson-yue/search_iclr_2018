{"notes":[{"tddate":null,"ddate":null,"tmdate":1512256845519,"tcdate":1512256309069,"number":4,"cdate":1512256309069,"id":"Hya4QnxWz","invitation":"ICLR.cc/2018/Conference/-/Paper395/Public_Comment","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Recall simple recurrent network","comment":"Nice work! But isn't the name simple recurrent unit (SRU) a bit similar to the classic name \"simple recurrent network\" which often refers to both Jordan & Elman networks.\nhttps://en.wikipedia.org/wiki/Recurrent_neural_network"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222634531,"tcdate":1511819427355,"number":2,"cdate":1511819427355,"id":"SyjjOZ5gM","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Review","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["ICLR.cc/2018/Conference/Paper395/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very useful RNN cell with ok results but over-hyped presentation.","rating":"7: Good paper, accept","review":"The authors introduce SRU, the Simple Recurrent Unit that can be used as a substitute for LSTM or GRU cells in RNNs. SRU is much more parallel than the standard LSTM or GRU, so it trains much faster: almost as fast as a convolutional layer with properly optimized CUDA code. Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below).\n\nOn the positive side, the paper is very clear and well-written, the SRU is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace LSTMs in most cases but yield fast training. On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art. Some of this has been pointed out in the comments below already. As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21. But already a long time ago Wu et. al. presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762). While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation. In conclusion: the authors seem to avoid discussing the problem that current non-RNN architectures  could be both faster and yield better results on some of the studied problems. That's bad presentation of related work and should be improved in the next versions (at which point this reviewer is willing to revise the score). But in all cases, this is a significant contribution to deep learning and deserves acceptance.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222634570,"tcdate":1511287058999,"number":1,"cdate":1511287058999,"id":"BJsMKkGgf","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Review","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["ICLR.cc/2018/Conference/Paper395/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice idea, tested extensively","rating":"7: Good paper, accept","review":"This work presents the Simple Recurrent Unit architecture which allows more parallelism than the LSTM architecture while maintaining high performance.\n\nSignificance, Quality and clarity:\nThe idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense. \nThe idea is well explained and the experiments convince that the new architecture is indeed much faster yet performs very well.\n\nA few constructive comments:\n- The experiment’s tables alternate between “time” and “speed”, It will be good to just have one of them.\n- Table 4 has time/epoch yet only time is stated","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510785941809,"tcdate":1510785941809,"number":5,"cdate":1510785941809,"id":"SyAqQBqyz","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Comment","forum":"rJBiunlAW","replyto":"BJABIzckG","signatures":["ICLR.cc/2018/Conference/Paper395/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper395/Authors"],"content":{"title":"Results on Switchboard","comment":"Thank you! We will update it."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510785877793,"tcdate":1510785877793,"number":4,"cdate":1510785877793,"id":"H10L7B91G","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Comment","forum":"rJBiunlAW","replyto":"r1XmNCKkz","signatures":["ICLR.cc/2018/Conference/Paper395/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper395/Authors"],"content":{"title":"Depth vs. width","comment":"In general we found that increasing depth is more helpful than increasing width as long as the width is in a reasonable size. I think this is because we drop \"the dependency\" between \"h\" and this context needs to be recovered by adding more layers. But since SWB training takes about 4 days, we didn't try all the configuration. That's why we didn't draw a conclusion on depth vs. width. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510774342149,"tcdate":1510774342149,"number":3,"cdate":1510774342149,"id":"BJABIzckG","invitation":"ICLR.cc/2018/Conference/-/Paper395/Public_Comment","forum":"rJBiunlAW","replyto":"HkeDMwt1f","signatures":["~Dmitriy_Serdyuk1"],"readers":["everyone"],"writers":["~Dmitriy_Serdyuk1"],"content":{"title":"Results on Switchboard","comment":"Thanks for a quick response.\n\nOk, I see now. Maybe you need to describe the setup more thoroughly and state which work you are basing your experiments on."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510757403355,"tcdate":1510757403355,"number":3,"cdate":1510757403355,"id":"r1XmNCKkz","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Comment","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["ICLR.cc/2018/Conference/Paper395/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper395/AnonReviewer2"],"content":{"title":"Nice work","comment":"In Tables 6 / 9: \nIt is not clear why SRU model capacity was increased in depth (to 12 layers) and not in width, which would give an even faster model I would think. As you mention for LSTM 5 layers appear to be optimal, so it is surprising that 12 were needed for SRU."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510733503987,"tcdate":1510728280426,"number":2,"cdate":1510728280426,"id":"HkeDMwt1f","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Comment","forum":"rJBiunlAW","replyto":"r1Yz8IYkG","signatures":["ICLR.cc/2018/Conference/Paper395/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper395/Authors"],"content":{"title":"Results on Switchboard","comment":"Thanks for your comments.\n\nSorry for the confusing, we didn't use RNN-LM here (only N-gram). So the number we should compare with is 10.0 in Table 8. I think JHU recently have better number using the same language model with lattice-free MMI training. We will try this new loss later. But similar to RNN-LM, this is orthogonal to this paper, we are trying to compare with LSTM only for acoustic modeling.\n\nWe haven't try it on 2000hrs. (1) To my understanding, there still lots of institute use 300hrs setup especially at school. If you check last year ICASSP, there are still many paper use 300hrs set, e.g. http://danielpovey.com/files/2017_spl_tdnnlstm.pdf. (2) In my experiences, 20000hrs vs. 300hrs do make a difference, especially for end-to-end system. But 2000hrs set and 300hrs usually don't have significant difference in term of testing the trend of the model quality (especially for HMM-NN hybrid system, model A > model B for 300hrs usually also hold for the full fisher set). Also, 300hrs usually take 4 days on a single GPU which is a reasonable setup for reproduce results.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510725136836,"tcdate":1510725136836,"number":2,"cdate":1510725136836,"id":"r1Yz8IYkG","invitation":"ICLR.cc/2018/Conference/-/Paper395/Public_Comment","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["~Dmitriy_Serdyuk1"],"readers":["everyone"],"writers":["~Dmitriy_Serdyuk1"],"content":{"title":"This is not SOTA on Switchboard","comment":"You cannot claim state of the art on Switchboard. https://arxiv.org/pdf/1610.05256.pdf showed 7.7% WER (Table 8, first row). Unless you are using no LM here (you need to describe LM you used), you don't have SOTA. \n\nSecond, 300h training set is just not very interesting for current research on ASR, therefore not many paper publish results on it. Have you run your model on 2000h set? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510419125611,"tcdate":1510419125611,"number":1,"cdate":1510419125611,"id":"BJR39j4yM","invitation":"ICLR.cc/2018/Conference/-/Paper395/Official_Comment","forum":"rJBiunlAW","replyto":"B1HgFDfJf","signatures":["ICLR.cc/2018/Conference/Paper395/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper395/Authors"],"content":{"title":"activation and highway bias","comment":"Thank you for the comment. \n\nThe identity activation (use_tanh=0) and non-zero highway bias are applied only on language modeling following a few of recent papers such as \n  - language modeling via gated convolutional network: https://arxiv.org/pdf/1612.08083.pdf\n  - recurrent highway network: https://arxiv.org/abs/1607.03474\n\nWe expect the model to perform better on other tasks as well by initializing a non-zero highway bias, since it can help to balance gradient propagation and model complexity (non-linearity) from layer stacking. This is recommended in the original highway network paper (https://arxiv.org/abs/1505.00387). However, we choose to use zero highway bias on other tasks for simplicity. \n\nRegarding the choice of activation function:\n  - this could be an empirical question since the best activation varies across tasks / datasets (Appendix A)\n  - identity already works since the pre-activation state (i.e. c[t]) readily encapsulates sequence similarity computation. see the discussed related work (Lei et al 2017; section 2.1 & 2.2) https://arxiv.org/pdf/1705.09037.pdf\n\nThank you again for bringing up the questions.\n  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1510271212657,"tcdate":1510271212657,"number":1,"cdate":1510271212657,"id":"B1HgFDfJf","invitation":"ICLR.cc/2018/Conference/-/Paper395/Public_Comment","forum":"rJBiunlAW","replyto":"rJBiunlAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Good paper","comment":"If the original result (arxiv) was already pretty surprising, this result seems to be even better? It seems a solid 3x speed-up is expected, and it can train a crazy number of layers (10 layers in MT). \n\nIn the actual code on github, it says \"use_tanh=0\" and set highway bias to \"-3\". These intuitions are not explained in the paper. Can the author offer some understanding into them? It seems that identity is better than tanh in the appendix...but then again...some explanation?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1509739326211,"tcdate":1509111965352,"number":395,"cdate":1509739323541,"id":"rJBiunlAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJBiunlAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Training RNNs as Fast as CNNs","abstract":"Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ","pdf":"/pdf/45dee06780354eb8a13d670863abc2edac145f55.pdf","paperhash":"anonymous|training_rnns_as_fast_as_cnns","_bibtex":"@article{\n  anonymous2018training,\n  title={Training RNNs as Fast as CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJBiunlAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper395/Authors"],"keywords":["recurrent neural networks","natural language processing"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}