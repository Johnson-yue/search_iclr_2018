{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222676956,"tcdate":1511823646098,"number":3,"cdate":1511823646098,"id":"ryImKM5lG","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice work","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.\n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility.\n\nThe presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1512222677001,"tcdate":1511811210010,"number":2,"cdate":1511811210010,"id":"Bkg5_kcxG","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer1"],"readers":["everyone"],"content":{"title":" Nice extensions to FederatedAveraging, with strong experimental setup.","rating":"7: Good paper, accept","review":"\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee. The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent\n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point.\n\nCorrectness: The paper is technically correct.\n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline.\n2. Clipping is supposed to help with the exploding gradients problem. Do you have an idea why a low threshold hurts the performances? Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?\n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee.\n2. Strong experimental setup that analyses in details the proposed extensions.\n3. Experiments performed on public datasets.\n\nCons\n-------\n\nNone\n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \"is given in Figure 1\" -> \"is given in Algorithm 1\"\n\nNote\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1512222677042,"tcdate":1511740950669,"number":1,"cdate":1511740950669,"id":"BJ1XIR_ef","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer2"],"readers":["everyone"],"content":{"title":"I like the experimental strength of the paper, but I have mild concerns about the new algorithmic ideas in the paper.","rating":"7: Good paper, accept","review":"Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy. The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example. The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework.\n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale. The paper uses the right composition tools like moments accountant to get strong privacy guarantees. The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm. Both these contributions are important in the effectiveness of the overall algorithm.\n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models. I did not find strong algorithmic ideas in the paper. I found the paper to be lacking in that respect.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1509739266766,"tcdate":1509124534502,"number":504,"cdate":1509739264101,"id":"BJ0hF1Z0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}