{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222621429,"tcdate":1512167949124,"number":2,"cdate":1512167949124,"id":"SyrG5UJ-G","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. It also adopts an interesting multicodebook approach for encoding than binary embeddings. \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy.\n\nThe experiments are convincing and solid. Overall I am weakly inclined to accept this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222621468,"tcdate":1511815094006,"number":1,"cdate":1511815094006,"id":"rk0hvx5xf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Effective work","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices. The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks.\n\nOverall, the paper is well-written. The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation.\n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer. For example, the model does not guarantee to be able to convert resulting \"indices\" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector. As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task.\nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well.\nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer. There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies.\n\nFurthermore, I would like to see two additional analysis. First, if we trained the proposed model with starting from \"zero\" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739359887,"tcdate":1509097989677,"number":331,"cdate":1509739357229,"id":"BJRZzFlRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}