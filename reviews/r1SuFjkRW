{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222578333,"tcdate":1511892154920,"number":3,"cdate":1511892154920,"id":"S1m6NXjlM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper describes a new RL technique for high dimensional action spaces.  It discretizes each dimension of the action space, but to avoid an exponential blowup, it selects the action for each dimension in sequence.  This is an interesting approach.  The paper reformulates the MDP with a high dimensional action space into an equivalent MDP with more time steps (one per dimension) that each selects the action in one dimension.  This makes sense.\n\nWhile I do like very much the model, I am perplex about the training technique.  The lower MDP is precisely the new proposed model with unidimensional actions and therefore it should be sufficient.  However, the paper also describes an upper MDP that seems to be superfluous.  The two MDPs are mathematically equivalent, but their Q-values are obtained differently (TD-0 for the upper MDP and Q-learning for the lower MDP) and yet the paper tries to minimize the Euclidean distance between them.  This is really puzzling since the different training algorithms suggest that the Q-values should be different while minimizing the Euclidean distance between them tries to make them equal.  The paper suggests that divergence occurs without the upper MDP.  This is really suspicious. The approach feels like a band-aid solution to cover a problem that the authors could not identify.  While the empirical results are good, I don't think the paper should be published until the authors figure out a principled way of training.\n\nThe proposed approach reformulates the MDP with high dimensional actions into an equivalent one with uni dimensional actions.  There is a catch.  This approach effectively hides the exponential action space into the state space which becomes exponential.  Since u contains all the actions of the previous dimensions, we are effectively increasing the state space by an exponential factor.  The paper should discuss this and explain what are the consequences in practice.  In the end, the MDP does not become simpler.\n\nOverall, this is an interesting paper with a good idea, but the training technique is not mature enough for publication.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/0d0e767af68d1479cef7580de0dd5e1f2ff1ef3e.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222578378,"tcdate":1511889477514,"number":2,"cdate":1511889477514,"id":"rkaH5MsgM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A useful method to handle continuous action spaces; however comes with additional cost of training as many networks as the number of actions","rating":"5: Marginally below acceptance threshold","review":"Originality\n--------------\nWhen the action space is N-dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1-D actions. \n\nClarity\n---------\n1) Explicitly writing down DDPG will be helpful\n2) The number of actions in each of the domains will also be useful\n\nQuality\n----------\n1) The paper reports experimental results on order of actions as well as binning, and the results confirm with what one would expect from intuition. \n2) It will be important to talk about the case when the action dimension N is very large, what happens in that case? Does the proposed method would work in such a scenario? A discussion is needed.\n3) Given that the ordering of actions does not matter, what is the real take away of looking at them as 'sequence' (which has not temporal structure because action order could be arbitrary)?\n\n\nSignificance\n----------------\nWhile the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i=1,..N) for Q^L, which is a limitation. Further, since the actions could be arbitrary, it is unclear where 'sequence' approach helps. These limit the understand and hence significance.\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/0d0e767af68d1479cef7580de0dd5e1f2ff1ef3e.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222578440,"tcdate":1510741011803,"number":1,"cdate":1510741011803,"id":"H13MV5tkM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents a correct, reasonably effective, but not groundbreaking approach to Q value approximation in MDPs with high-dimensional action spaces","rating":"7: Good paper, accept","review":"The paper presents Sequential Deep Q-Networks (SDQNs), which select actions from discretized high-dimensional action spaces.  This is done by introducing another, undiscounted MDP in which each action dimension is chosen sequentially by an agent.  By training a Q network to best choose these action dimensions, and loosely enforcing equality between the original and new MDPs at points where they are equivalent, the new MDP can be successfully navigated, resulting in good action selection for the original MDP.  This is experimentally compared against DDPG in several domains.  There are no theoretical results.\n\nThis work is correct and clearly written.  Experiments do demonstrate improved effectiveness in the chosen domains, and the authors do a nice job of illustrating the range of performance by their approach (which has low variance in some domains, but high variance in others).  Because of the clarity of the paper, the effectiveness of the approach, and the high quality experiments, I encourage acceptance.\n\nIt doesn't strike me as world-changing, however.  The MDP-within-an-MDP approach is quite similar to the Pazis and Lagoudakis MDP decomposition for the same problem (work which is appropriately cited, but maybe too briefly compared against).  In other words, it strikes me as merely being P&L plus networks, dampening my enthusiasm.\n\nMy one question for the authors is how much the order of action dimension selection matters.  This seems probably quite important practically, but is undiscussed.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/0d0e767af68d1479cef7580de0dd5e1f2ff1ef3e.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739453199,"tcdate":1509042541346,"number":158,"cdate":1509739450544,"id":"r1SuFjkRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/0d0e767af68d1479cef7580de0dd5e1f2ff1ef3e.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}