{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222619930,"tcdate":1511918426284,"number":2,"cdate":1511918426284,"id":"SJMDsFilM","invitation":"ICLR.cc/2018/Conference/-/Paper320/Official_Review","forum":"SksY3deAW","replyto":"SksY3deAW","signatures":["ICLR.cc/2018/Conference/Paper320/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper contains nice ideas and experimental results are promising, but has non-negligible mistakes in theoretical parts which degrade the contribution.","rating":"4: Ok but not good enough - rejection","review":"Summary:\nThis paper considers a learning method for the ResNet using the boosting framework. More precisely, the authors view the structure of the ResNet as a (weighted) sum of base networks (weak hypotheses) and apply the boosting framework. The merit of this approach is to decompose the learning of complex networks to that of small to large networks in a moderate way and it uses less computational costs. The experimental results are good. The authors also show training and generalization error bounds for the proposed approach.\n\nComments: \nThe idea of the paper is natural and interesting. Experimental results are somewhat impressive. However, I am afraid that theoretical results in the paper contain several mistakes and does not hold. The details are below.\n\nI think the proof of Theorem 4.2 is wrong. More precisely, there are several possibly wrong arguments as follows:\n- In the proof, \\alpha_t+1 is chosen so as to minimize an upper bound of Z_t, while the actual algorithm is chosen to minimize Z_t. The minimizer of Z_t and that of an upper bound are different in general. So, the obtained upper bound does not hold for the training error of the actual algorithm. \n- It is not a mistake, but, there is no explanation why the equality between (27) and (28) holds. Please add an explanation. Indeed, equation (21) matters. \n\nAlso, the statement of Theorem 4.2 looks somewhat cheating: The statement seems to say that it holds for any iteration T and the training error decays exponentially w.r.t. T. However, the parameter T is determined by the parameter gamma, so it is some particular iteration, which might be small and the bound could be large. \n\nThe generalization error bound Corollary 4.3 seems to be wrong, too. More precisely, Lemma 2 of Cortes et al. is OK, but the application of Lemma 2 is not. In particular, the proof does not take into account of the function \\sigma. In other words, the proof considers the Rademacher complexity R_m(\\calF_t), of the class \\calF_t, but, acutually, I think it should consider R_m(\\sigma(\\calF_t)), where the class \\sigma(\\calF_t) consists of the composition of functions \\sigma and f_t in \\calF_t. Talagrand’s lemma (see, e.g., Mohri et al.’ s book: Foundation of Machine Learning) can be used to analyze the complexity of the composite class. But, the resulting bound would depend on the Lipschizness of \\sigma in an exponential way. \n\nThe explanation of the generalization ability is not sufficient. While the latter weak hypotheses are complex enough and would have large edges, the complexity of the function class of weak hypotheses grows exponentially w.r.t. the iteration T, which should be mentioned. \n\nAs a summary, the paper contains nice ideas and experimental results are promising, but has non-negligible mistakes in theoretical parts which degrade the contribution of the paper.\n\nMinor Comments:\n-In Algorithm 1, \\gamma_t is not defined when a while-loop starts. So, the condition of the while-loop cannot be checked.\n\n \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep ResNet Blocks Sequentially using Boosting Theory","abstract":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T \"shallow ResNets\". We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","pdf":"/pdf/0a6dc2f2538631a7cba056687a3c4b1e5b157cbc.pdf","TL;DR":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.","paperhash":"anonymous|learning_deep_resnet_blocks_sequentially_using_boosting_theory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep ResNet Blocks Sequentially using Boosting Theory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SksY3deAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper320/Authors"],"keywords":["residual network","boosting theory","training error guarantee"]}},{"tddate":null,"ddate":null,"tmdate":1512222619966,"tcdate":1511759421589,"number":1,"cdate":1511759421589,"id":"BkUBAzFlG","invitation":"ICLR.cc/2018/Conference/-/Paper320/Official_Review","forum":"SksY3deAW","replyto":"SksY3deAW","signatures":["ICLR.cc/2018/Conference/Paper320/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An ineresting approach to boosting residual networks but problems from NIPS reviews are still not resolved.","rating":"5: Marginally below acceptance threshold","review":"Disclaimer: I reviewed this paper for NIPS as well and many of comments made by reviewers at that time still apply to this version of the paper as well, although presentation has overall improved.\n\nThe paper presents a boosting-style algorithm for training deep residual networks. Convergence analysis for training error is presented and analysis of generalization ability is also provided. Paper concludes with some experimental results.\n\nThe main contribution of this work is interpretation of ResNet as a telescoping sum of differences between the intermediate layers and treating these differences as weak learners that are then boosted. This indeed appears to an interesting insight about ResNet training.\n\nOn the other hand, one of the main objections during NIPS reviews was the relation of this work to work of Cortes et al. on Adanet. In particular, generalization bounds presented in this work are results taken from that paper (which authors admit). What is less clear is the distinction between the algorithmic approaches which makes it hard to judge the novelty of this work. There is a paragraph at the end of section 2 but it seems rather vague.\n\nOne other objection during NIPS reviews was experimental setup explanation of which is omitted from the current version. In particular, same learning rate and mini-batch size was used both for boosting and backprop algorithms which seems strange since boosting is supposed to train much smaller classifiers.\n\nAnother concern is practicality of the proposed method which seems to require maintaining explicit distribution over all examples which would not be practical for modern datasets where NNs are typically applied.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep ResNet Blocks Sequentially using Boosting Theory","abstract":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T \"shallow ResNets\". We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","pdf":"/pdf/0a6dc2f2538631a7cba056687a3c4b1e5b157cbc.pdf","TL;DR":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.","paperhash":"anonymous|learning_deep_resnet_blocks_sequentially_using_boosting_theory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep ResNet Blocks Sequentially using Boosting Theory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SksY3deAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper320/Authors"],"keywords":["residual network","boosting theory","training error guarantee"]}},{"tddate":null,"ddate":null,"tmdate":1509739365798,"tcdate":1509096579402,"number":320,"cdate":1509739363139,"id":"SksY3deAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SksY3deAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep ResNet Blocks Sequentially using Boosting Theory","abstract":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T \"shallow ResNets\". We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","pdf":"/pdf/0a6dc2f2538631a7cba056687a3c4b1e5b157cbc.pdf","TL;DR":"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.","paperhash":"anonymous|learning_deep_resnet_blocks_sequentially_using_boosting_theory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep ResNet Blocks Sequentially using Boosting Theory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SksY3deAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper320/Authors"],"keywords":["residual network","boosting theory","training error guarantee"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}