{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222745684,"tcdate":1512166146777,"number":3,"cdate":1512166146777,"id":"BJiW7IkZM","invitation":"ICLR.cc/2018/Conference/-/Paper767/Official_Review","forum":"BkUHlMZ0b","replyto":"BkUHlMZ0b","signatures":["ICLR.cc/2018/Conference/Paper767/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting point of view on analysis of robustness","rating":"7: Good paper, accept","review":"In this work, the objective is to analyze the robustness of a neural network to any sort of attack.\n\nThis is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function. This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community.\n\nThis is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach","abstract":"The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and is computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed give better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifiers.\n\n","pdf":"/pdf/e1ba4ec3c5d6f46fb5f233bd5add374df2b81ecc.pdf","TL;DR":"We propose the first attack-independent robustness metric, a.k.a CLEVER, that can be applied to any neural network classifier.","paperhash":"anonymous|evaluating_the_robustness_of_neural_networks_an_extreme_value_theory_approach","_bibtex":"@article{\n  anonymous2018evaluating,\n  title={Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUHlMZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper767/Authors"],"keywords":["robustness","adversarial machine learning","neural network","extreme value theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222746448,"tcdate":1511819853701,"number":2,"cdate":1511819853701,"id":"rk8Ucb5gf","invitation":"ICLR.cc/2018/Conference/-/Paper767/Official_Review","forum":"BkUHlMZ0b","replyto":"BkUHlMZ0b","signatures":["ICLR.cc/2018/Conference/Paper767/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Critical problem and important claims, with experimental justification.","rating":"7: Good paper, accept","review":"The work claims a measure of robustness of networks that is attack-agnostic. Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function. That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples. Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation.\n\nThe paper closely follows Hein and Andriushchenko (2017). There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3). As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples. This is a rather simple idea that is shown to be effective in Figure 3. The following section (the part starting from 5.3) presents the key to the success of the proposed measure. \n\nThis is an important problem and the paper attempts to tackle it in a computationally efficient way. The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3). It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score.\n\nFinally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1. I believe this is just a typo.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach","abstract":"The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and is computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed give better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifiers.\n\n","pdf":"/pdf/e1ba4ec3c5d6f46fb5f233bd5add374df2b81ecc.pdf","TL;DR":"We propose the first attack-independent robustness metric, a.k.a CLEVER, that can be applied to any neural network classifier.","paperhash":"anonymous|evaluating_the_robustness_of_neural_networks_an_extreme_value_theory_approach","_bibtex":"@article{\n  anonymous2018evaluating,\n  title={Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUHlMZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper767/Authors"],"keywords":["robustness","adversarial machine learning","neural network","extreme value theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222746493,"tcdate":1510323177518,"number":1,"cdate":1510323177518,"id":"B1ZlEVXyf","invitation":"ICLR.cc/2018/Conference/-/Paper767/Official_Review","forum":"BkUHlMZ0b","replyto":"BkUHlMZ0b","signatures":["ICLR.cc/2018/Conference/Paper767/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors present an \"attack-independent\" method for evaluating the robustness of a network. The idea is interesting, but the theoretical is very weak.","rating":"3: Clear rejection","review":"Summary\n========\n\nThe authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point. This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network.\n\nThe method proposed in the paper already exists for classical function, they only transpose it to neural networks. Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions.\n\n\nClarity\n=====\n\nThe paper is clear and well-written.\n\n\nOriginality\n=========\n\nThis idea is not new: if we search for \"Lipschitz constant estimation\" in google scholar, we get for example\nWood, G. R., and B. P. Zhang. \"Estimation of the Lipschitz constant of a function.\" (1996)\nwhich presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull).\n\n\nTechnical quality\n==============\n\nThe main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on\na data point to fool the network. This result is obtained almost directly by writing the bound on Lipschitz-continuous function\n | f(y)-f(x) | < L || y-x ||\nwhere x = x_0 and y = x_0 + \\delta.\n\nComments:\n- Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity? Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0). Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement.\n\n- (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g. However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be \ng(x) = min_{k \\neq c} f_c(x) - f_k(x).\nThus its Lipschitz constant is different, potentially equal to\nL_q = max_{k} \\| L_q^k \\|, \nwhere L_q^k is the Lipschitz constant of f_c-f_k. If the theorem remains unchanged after this modification, you should clarify the proof. Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened.\n\n- Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated.\n\n\nNumerical experiments\n====================\n\nGlobally, the numerical experiments are in favor of the presented method. The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example.\n\nMoreover, the numerical experiments look to be realized in the context of targeted attack. To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach","abstract":"The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and is computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed give better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifiers.\n\n","pdf":"/pdf/e1ba4ec3c5d6f46fb5f233bd5add374df2b81ecc.pdf","TL;DR":"We propose the first attack-independent robustness metric, a.k.a CLEVER, that can be applied to any neural network classifier.","paperhash":"anonymous|evaluating_the_robustness_of_neural_networks_an_extreme_value_theory_approach","_bibtex":"@article{\n  anonymous2018evaluating,\n  title={Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUHlMZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper767/Authors"],"keywords":["robustness","adversarial machine learning","neural network","extreme value theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739114155,"tcdate":1509134398000,"number":767,"cdate":1509739111485,"id":"BkUHlMZ0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkUHlMZ0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach","abstract":"The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and is computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed give better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifiers.\n\n","pdf":"/pdf/e1ba4ec3c5d6f46fb5f233bd5add374df2b81ecc.pdf","TL;DR":"We propose the first attack-independent robustness metric, a.k.a CLEVER, that can be applied to any neural network classifier.","paperhash":"anonymous|evaluating_the_robustness_of_neural_networks_an_extreme_value_theory_approach","_bibtex":"@article{\n  anonymous2018evaluating,\n  title={Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUHlMZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper767/Authors"],"keywords":["robustness","adversarial machine learning","neural network","extreme value theory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}