{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222695700,"tcdate":1512147964590,"number":3,"cdate":1512147964590,"id":"rkx-2-y-f","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very interesting principled analysis of robust learning","rating":"9: Top 15% of accepted papers, strong accept","review":"In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\n\nSince the min-max problem is intractable in general, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem. If the duality parameter is large enough, the functions become convex given that the initial losses are smooth. \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions. Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training  procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. We match or outperform heuristic approaches on supervised and reinforcement learning tasks.\n","pdf":"/pdf/981ec3b4ce7c272758dcc23a5ff038926c862741.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222695773,"tcdate":1511887853232,"number":2,"cdate":1511887853232,"id":"HySlNfjgf","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training  procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. We match or outperform heuristic approaches on supervised and reinforcement learning tasks.\n","pdf":"/pdf/981ec3b4ce7c272758dcc23a5ff038926c862741.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222697348,"tcdate":1511800281907,"number":1,"cdate":1511800281907,"id":"HJ-1AnFlM","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer2"],"readers":["everyone"],"content":{"title":"a very interesting approach to adversarial training based on robustness over Wasserstein balls","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution. In particular, the authors adopt the Wasserstein distance to define the ambiguity sets. This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost. The theoretical results in the paper are supported by experiments.\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training  procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. We match or outperform heuristic approaches on supervised and reinforcement learning tasks.\n","pdf":"/pdf/981ec3b4ce7c272758dcc23a5ff038926c862741.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739211074,"tcdate":1509127909457,"number":596,"cdate":1509739208417,"id":"Hk6kPgZA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training  procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. We match or outperform heuristic approaches on supervised and reinforcement learning tasks.\n","pdf":"/pdf/981ec3b4ce7c272758dcc23a5ff038926c862741.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}