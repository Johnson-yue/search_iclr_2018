{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222610559,"tcdate":1511783403932,"number":3,"cdate":1511783403932,"id":"r1EghdKeM","invitation":"ICLR.cc/2018/Conference/-/Paper274/Official_Review","forum":"ry1arUgCW","replyto":"ry1arUgCW","signatures":["ICLR.cc/2018/Conference/Paper274/AnonReviewer2"],"readers":["everyone"],"content":{"title":"E-counters are value functions that must be decayed to zero via learning. Some evidence they help with exploration.","rating":"6: Marginally above acceptance threshold","review":"The paper proposes an approach to exploration based on initializing a value function to 1 everywhere, then letting the value decay back toward zero as the state space is explored. I like the idea a lot. I don't really like the paper, though. I'd really like to see a strong theoretical and/or empirical justification for it, and both are lacking. On the theoretical side, can a bound be proven for this approach, even in the tabular case? On the empirical side, there are more (and more recent!) testbeds that have come to define the field---the mountain car problem is just not sufficient to convincingly argue that the method scales and generalizes. My intuition is that such an approach ought to be effective, but I really want to see additional evidence. Given the availability of so many RL testbeds, I worry that it had been tried but failed.\n\nDetailed comments:\n\n\"Where γ is\" -> \", <newline> where γ is\".\n\n\"The other alternative\" -> \"The alternative\"?\n\n\"without learning a model (Mongillo et al., 2014).\": Seems like an odd choice for a citation for model-free RL. Perhaps select \nthe paper that first used the term? Or an RL survey?\n\nRight before Section 1.1, put a period after the Q-learning update equation.\n\n\"new states may\" -> \"new states, may\".\n\n\"such approaches leads\" -> \"such approaches lead\".\n\n\"they still fails\" -> \"they still fail\".\n\n\"evaluated with respect only to its immediate outcome\": Not so. Several of the cited papers use counters to determine which \nstates are \"known\" and then solve an MDP to direct exploration past immediate outcomes.\n\n\" exploration bonus(Little & Sommer, 2014)\" -> \" exploration bonus (Little & Sommer, 2014)\".\n\n\"n a model-free settings.\" -> \"n model-free settings.\".\n\n\" Therefore, a satisfying approach for propagating directed exploration in model-free reinforcement learning is still missing. \": I think you should cite http://research.cs.rutgers.edu/~nouri/papers/nips08mre.pdf , which also combines a kind of counter \nidea with function approximation to improve exploration.\n\n\"initializing E-values to 1\": I like this idea. I wonder if one could prove bounds similar to the delayed Q-learning algorithm with \nthis approach. It is reminiscent of https://arxiv.org/pdf/1205.2606.pdf , which also drives exploration by beginning with an \noverly optimistic estimate and letting the data (in a function approximation setting) decay the influence of this initialization.\n\n\"So after visited n times\" -> \"So after being visited n times\".\n\n\"figure 1a\" -> \"Figure 1a\". (And, in other places.)\n\n\"An important property of E-values is that it decreases over repetition\" -> \"An important property of E-values is that they decrease over repetition\".\n\n\"t utilize counters, can\" -> \"t utilize counters can\".\n\n\" hence we were interested a convergence measure\": Multiple problems in this sentence, please fix.\n\nFigure 2: How many states are in this environment? Some description is needed.\n\nFigure 3: The labels in this figure (and all the figures) are absurdly small and, hence, unreadable.\n\n\"now turn to show that by using counters,\" -> \"now turn to showing that, by using counters,\".\n\nTheorem 3.1: I'm not quite getting why we want to take a stochastic rule and make it deterministic. Note that standard PAC-MDP algorithms choose deterministically. It's not clear why we'd want to start from a stochastic rule.\n\n\" models(Bellemare\" -> \" models (Bellemare\".\n\n\"Efficient memory-based learning for robot control\": This reference is incomplete. (I'm skeptical that it represents the first use of this problem, but I can't check it.)\n\n\"Softmax exploration fail\" -> \"Softmax exploration fails\".\n\n\"whom also analyzed\" -> \"who also analyzed\".\n\n\"non-Markovity\" -> \"non-Markovianness\"?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DORA The Explorer: Directed Outreaching Reinforcement Action-Selection","abstract":"Exploration is a fundamental aspect of Reinforcement Learning. Two key challenges are how to focus exploration on more valuable states, and how to direct exploration toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality, considering only the immediate one step exploration value. While there are a few model-based solutions to this difficulty, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-value improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to learn continuous MDPs.","pdf":"/pdf/b647056322f7a4f171719f72bc8907bfbce3d343.pdf","TL;DR":"We propose a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL","paperhash":"anonymous|dora_the_explorer_directed_outreaching_reinforcement_actionselection","_bibtex":"@article{\n  anonymous2018dora,\n  title={DORA The Explorer: Directed Outreaching Reinforcement Action-Selection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry1arUgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper274/Authors"],"keywords":["Reinforcement Learning","Exploration","Model-Free"]}},{"tddate":null,"ddate":null,"tmdate":1512222610603,"tcdate":1511735432245,"number":2,"cdate":1511735432245,"id":"BJeqeaOgM","invitation":"ICLR.cc/2018/Conference/-/Paper274/Official_Review","forum":"ry1arUgCW","replyto":"ry1arUgCW","signatures":["ICLR.cc/2018/Conference/Paper274/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead.","rating":"5: Marginally below acceptance threshold","review":"This paper presents an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. This generalization is achieved by relying on the convergence rate of SARSA updates on an auxiliary MDP.\n\nThe method presented in the paper trains a parallel \"E-value\" MDP, with initial value of 1 for all state-action pairs. It applies SARSA (on-policy) update rule to the E-value MDP, where the acting policy is selected on the original MDP. While the E-value MDP is training, the proposed method uses a 1/log transformation applied to E-values to get the corresponding exploration bonus term for the original MDP. This bonus term is shown to be equivalent counter-based methods for finite MDPs when the discount factor of the E-MDP is set to 0. The paper has minimal theoretical analysis of the proposed algorithm, essentially only showing convergence with infinite visiting. In that regard, the presented method seems like a useful heuristic with anecdotal empirical benefits.\n\nWhat is crucially lacking from the paper is any reference to model-free Bayesian methods that have very similar intuition behind them: taking into account the long term exploratory benefits of actions (passed on through the Bayesian inference). A comparison would have been trivial to do (with a generic non-informative prior) for the finite MPD setting (section 3.4). Even for the function approximation case one could use Gaussian process methods as the Bayesian baseline. There are also several computationally tractable approximations of Bayesian RL that can be used as baseline for empirical analysis.\n\nIt would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. Unlike the finite case, updates to the value of one E-value can change the value for another state-action pair and the convergence could be faster than (1-alpha)^n. Given the lack of any theory on this, an empirical analysis is certainly valuable.\n\nNotes:\n- The plots are horrible in a print. I had to zoom 400% into the PDF file to be able to read the plots. Please scale them at least by 200% and use a larger font for the legends.\n\n- Add a minimal description of the initial setup for E-value neural network to section 4.1 (i.e. how the initializing is achieved to have a constant value for all state-action pairs as described in the appendix).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DORA The Explorer: Directed Outreaching Reinforcement Action-Selection","abstract":"Exploration is a fundamental aspect of Reinforcement Learning. Two key challenges are how to focus exploration on more valuable states, and how to direct exploration toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality, considering only the immediate one step exploration value. While there are a few model-based solutions to this difficulty, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-value improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to learn continuous MDPs.","pdf":"/pdf/b647056322f7a4f171719f72bc8907bfbce3d343.pdf","TL;DR":"We propose a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL","paperhash":"anonymous|dora_the_explorer_directed_outreaching_reinforcement_actionselection","_bibtex":"@article{\n  anonymous2018dora,\n  title={DORA The Explorer: Directed Outreaching Reinforcement Action-Selection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry1arUgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper274/Authors"],"keywords":["Reinforcement Learning","Exploration","Model-Free"]}},{"tddate":null,"ddate":null,"tmdate":1512222610649,"tcdate":1511628790221,"number":1,"cdate":1511628790221,"id":"Hy0gemvxf","invitation":"ICLR.cc/2018/Conference/-/Paper274/Official_Review","forum":"ry1arUgCW","replyto":"ry1arUgCW","signatures":["ICLR.cc/2018/Conference/Paper274/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple but nice idea to tackle the exploration-exploitation tradeoff in model-free reinforcement learning that seems to work well","rating":"7: Good paper, accept","review":"\n\nThe paper proposes a novel way for trading of exploration and exploitation in model-free reinforcement learning. The idea is to learn a second (kind of) Q function, which could be called E-function, which captures the value of exploration (E-value). In contrast to the Q-function of the problem at hand, the E-function assumes no preferences among actions. \nThis is makes sense in my opinion as exploration is exactly “no preferences among actions”. \n\nActually, to be more precise, the paper shows that the logarithm of E-Values can be thought of as a generalization of visit counters, with propagation\nof the values along state-action pairs. This is important, as E-values should actually decrease with repetition. Moreover, the paper shows that by using counters for stochastic action-selection rules commonly employed within the RL community, for every stochastic rule there exist equivalent deterministic rules. Once turned to deterministic counter-based rules, it is again possible improve them using E-values. This provides a nice story for a simple (in a positive sense) approach to tackle the exploration-exploitation tradeoff. The experimental results demonstrate this is a sufficient number of domains. To summarize, for an informed outsider such as the reviewer, the paper makes a simple but strong contribution to an important problem. Overall the paper is well writing and structured. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DORA The Explorer: Directed Outreaching Reinforcement Action-Selection","abstract":"Exploration is a fundamental aspect of Reinforcement Learning. Two key challenges are how to focus exploration on more valuable states, and how to direct exploration toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality, considering only the immediate one step exploration value. While there are a few model-based solutions to this difficulty, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-value improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to learn continuous MDPs.","pdf":"/pdf/b647056322f7a4f171719f72bc8907bfbce3d343.pdf","TL;DR":"We propose a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL","paperhash":"anonymous|dora_the_explorer_directed_outreaching_reinforcement_actionselection","_bibtex":"@article{\n  anonymous2018dora,\n  title={DORA The Explorer: Directed Outreaching Reinforcement Action-Selection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry1arUgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper274/Authors"],"keywords":["Reinforcement Learning","Exploration","Model-Free"]}},{"tddate":null,"ddate":null,"tmdate":1509739391587,"tcdate":1509086646656,"number":274,"cdate":1509739388920,"id":"ry1arUgCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry1arUgCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DORA The Explorer: Directed Outreaching Reinforcement Action-Selection","abstract":"Exploration is a fundamental aspect of Reinforcement Learning. Two key challenges are how to focus exploration on more valuable states, and how to direct exploration toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality, considering only the immediate one step exploration value. While there are a few model-based solutions to this difficulty, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-value improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to learn continuous MDPs.","pdf":"/pdf/b647056322f7a4f171719f72bc8907bfbce3d343.pdf","TL;DR":"We propose a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL","paperhash":"anonymous|dora_the_explorer_directed_outreaching_reinforcement_actionselection","_bibtex":"@article{\n  anonymous2018dora,\n  title={DORA The Explorer: Directed Outreaching Reinforcement Action-Selection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry1arUgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper274/Authors"],"keywords":["Reinforcement Learning","Exploration","Model-Free"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}