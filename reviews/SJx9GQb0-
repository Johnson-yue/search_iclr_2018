{"notes":[{"tddate":null,"ddate":null,"tmdate":1512145700642,"tcdate":1512145700642,"number":6,"cdate":1512145700642,"id":"SkTmQbkbz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"BJhxQVieM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"The dual roles of that formulation","comment":"That’s a great question! We were actually wondering about a similar one. We can certainly interpret the formulation as that it encourages the network to be resilient to the dropout noise --- one of the notions that motivates the temporal ensembling semi-supervised learning method. In addition to that, however, it also enforces the Lipschitz continuity over the discriminator because of equation (3). Thanks to the dual roles of this formulation, we are able to use it to both improve the training of WGANs and connect GAN with the temporal ensembling method. \n\nWe have re-run the experiments using margin $M’=0.2$. To show that the margin, albeit small, plays an active role, we have got some statistics of the $d(D,D)+0.1 d(D_,D_)$ term over the last 10 epochs. We can see that the median values of that term are smaller and the max values are larger than the margin. \n\nMin          0.0162    0.0153    0.0149    0.0171    0.0170    0.0159    0.0140    0.0146    0.0159    0.0144\nMedian    0.1130    0.1133     0.1124    0.1138    0.1114      0.1123    0.1124     0.1122    0.1125     0.1111\nMax          7.1718    6.1229    7.1985    7.3505    4.9636    5.2252   5.2559   5.3058    5.9905    4.8519\n\n\n\n\n\n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511895796243,"tcdate":1511895796243,"number":15,"cdate":1511895796243,"id":"BJhxQVieM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does the intuition agree with what you are doing in the end?","comment":"Section 1.2 and Figure 1 outline the general idea behind the approach.\n\nI wonder however, how much of the intuitive explanation is still valid in the actual CT loss.\n\nTo dissect this a little:\nd(x1, x2) being a metric should always be positive. that means all instances of max(0, d(x1,x2)) reduce to d(x1,x2)\n\nLooking at Eq 4 and 5, given M=0, d(x1,x2) is assumed to be constant, it reduces to \nCT_(x1,x2) = E_{x1,x2} d(D(x1),D(x2))\nwith the constant d(x1,x2) absorbed into d.\n\nHowever, since the input is not changed but rather the network, a better notation would probably be\n\nCT = E_(x, \\theta_1, \\theta_2) d(D(x, \\theta_1), D(x, \\theta_2))\n\nwhere \\theta_1, \\theta_2 are the noise vectors used for dropout.\n\nLooking at that formulation, does this still mean it penalizes the gradient in the original input space or would it be more appropriate to say it encourages the resilience to dropout (or is that actually the same thing)?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222559572,"tcdate":1511844555829,"number":3,"cdate":1511844555829,"id":"BJNCqPqxM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Official review for paper 1144","rating":"4: Ok but not good enough - rejection","review":"This paper presented an improved approach for training WGANs, by applying some Lipschitz constraint close to the real manifold in the pixel level.  The framework can also be integrated to boost the SSL performances. In experiments, the generated data showed very good qualities, measured by inception score. Meanwhile, the SSL-GANs results were impressive on MNIST and CIFAR-10, demonstrating its effectiveness. \n\nHowever, the paper has the following weakness: \n\nMissing citations: the most related work of this one is the DRAGAN work. However, it did not cite it. I think the author should cite it, make a clear justification for the comparison and emphasize the main contribution of the method. Also, it suggested that the paper should discuss its relation to other important work, [Arjovsky & Bottou 2017], [Wu et al. 2016].\n\nExperiments: as for the experimental part, it is not solid. Firstly, although the SSL results are very good, it is guaranteed the proposed GAN is good [Dai & Almahairi, et al. 2017]. Secondly, the paper missed several details, such as settings, model configuration, hyper-parameters, making it is difficult to justify which part of the model works. Since the paper using the temporal ensembling trick [Samuli & Timo, 2017],  most of the gain might be from there. Data augmentation might also help to improve. Finally, except CIFAR-10, it is better to evaluate it on more datasets. \n\nGiven the above reason, I think this paper is not ready to be published in ICLR. The author can submit it to the workshop and prepare for next conference. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222559617,"tcdate":1511772853534,"number":2,"cdate":1511772853534,"id":"ryT2f8KgM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper continues a line of improvement to Wasserstein GANs, and suggests an approach based a double perturbation of each data point, penalizing deviations from Lipshitz-ness. Empirical results demonstrate the effectiveness of the proposal. ","rating":"7: Good paper, accept","review":"This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al.  [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty \"encouraging\" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning.  \n\nIn terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator’s response on the perturbed points.  The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10.  They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. \n\nThe authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. \n\nWhere the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. \n[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan.\n\n[2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222559658,"tcdate":1511654601307,"number":1,"cdate":1511654601307,"id":"HkbRNKwef","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review for \"Improving the Improved Training of Wasserstein GANs\"","rating":"6: Marginally above acceptance threshold","review":"Summary:\n\nThe paper proposes a new regularizer for wgans, to be combined with the traditional gradient penalty. The theoretical motivation is bleak, and the analysis contains some important mistakes. The results are very good, as noticed by the comments, the fact that the method is also less susceptible to overfitting is also an important result, though this might be purely due to dropout. One of the main problems is that the largest dataset used is CIFAR, which is small. Experiments on something like bedrooms or imagenet would make the paper much stronger. \n\nIf the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score.\n\nDetailed comments:\n\n- The motivation of 1.2 and the sentence \"Arguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions\" are incredibly wrong. First of all, it should be noted that the duality uses 1-Lip in the entire space between Pr and Pg, not in Pr alone. If the manifolds are not extremely close (such as in the beginning of training), then the discriminator can be almost exactly 1 in the real data, and 0 on the fake. Thus the discriminator would be almost exactly constant (0-Lip) near the real manifold, but will fail to be 1-lip in the decision boundary, this is where interpolations fix this issue. See Figure 2 of the wgan paper for example, in this simple example an almost perfect discriminator would have almost 0 penalty.\n\n- In the 'Potential caveats' section, the implication that 1-Lip may not be enforced in non-examined samples is checkable by an easy experiment, which is to look for samples that have gradients of the critic wrt the input with norm > 1. I performed the exp in figure 8 and saw that by taking a slightly higher lambda, one reaches gradients that are as close to 1 as with ct-gan. Since ct-gan uses an extra regularizer, I think the authors need some stronger evidence to support the claim that ct-gan better battles this 'potential caveat'.\n\n- It's important to realize that the CT regularizer with M' = 1 (1-Lip constraint) will only be positive for an almost 1-Lip function if x and x' are sampled when x - x' has a very similar direction than the gradient at x. This is very hard in high dimensional spaces, and when I implemented a CT regularizer indeed the ration of eq (4) was quite less than the norm of the gradient. It would be useful to plot the value of the CT regularizer (the eq 4 version) as the training iterations progresses. Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant. This difference is non-trivial and should be discussed.\n\n- Line 11 of the algorithm is missing L^(i) inside the sum.\n\n- One shouldn't use MNIST for anything else than deliberately testing an overfitting problem. Figure 4 is thus relevant, but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method.\n\n- The overfitting result is very important, but one should disambiguate this from being due to dropout. Comparing with wgangp + dropout is thus important in this experiment.\n\n- The authors should provide experiments in at least one larger dataset like bedrooms or imagenet (not faces, which is known to be very easy). This would strengthen the paper quite a bit.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511633678365,"tcdate":1511633678365,"number":14,"cdate":1511633678365,"id":"SJLzXVwlz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"r1s7vBrCb","signatures":["~Naveen_Kodali1"],"readers":["everyone"],"writers":["~Naveen_Kodali1"],"content":{"title":"An extension of DRAGAN","comment":"After a close look at your consistency penalty, I think you are basically proposing a small extension to DRAGAN (published in May'17). There are two differences - \n1. WGAN-GP, DRAGAN use stronger Lipschitz constraint, (=) rather than (<=) as you do.\n1. In DRAGAN, they apply the Lipschitz (stronger) constraint close to real manifold using small noise in the pixel space. The authors leave open the possibility of other mechanisms to do this. In your paper, you propose to add noise in discriminator's layers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511389406380,"tcdate":1511389406380,"number":13,"cdate":1511389406380,"id":"HkLJt_QxG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BynjqwxJf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Ablated study results","comment":"The table below shows the ablated study results:\n\nMethod                                                                          |                  Test Error\nOURS w/o CT                                                               |                 14.98+-0.43\nOURS w/o GAN *                                                         |                  11.98+-0.32\nOURS w batch norm **                                                |                           --\nOURS w/o D_(.,.) over the second-to-last layer        |                  10.70+-0.24\nOURS                                                                             |                  9.98+-0.21\n\n* This almost reduces to TE (Laine & Aila, 2016). All the settings are exactly the same as in TE  (Laine & Aila, 2016) except that we use the extra regularization (D_(.,.) in CT) over the second-to-last layer.\n** We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in  (Salimans et al., 2016).\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprintarXiv:1610.02242, 2016.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans. In Advances in Neural Information Processing Systems,pp. 2234–2242, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510795715364,"tcdate":1510795715364,"number":10,"cdate":1510795715364,"id":"BJopYw5yM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Code","comment":"Here is the code for this paper: https://github.com/biuyq/CT-GAN\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510140580230,"tcdate":1510140580230,"number":5,"cdate":1510140580230,"id":"BynjqwxJf","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"r1mlEqyyz","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Ablated studies","comment":"We have done some ablated studies but they are not as thorough as you suggested. We will complete them and then get back to you soon. Thanks! \n\nObservations thus far: Both the consistent regularization and GAN are necessary to arrive at the report results, and the results without the consistency drop more than those without GAN.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510140148720,"tcdate":1510140148720,"number":4,"cdate":1510140148720,"id":"SkpgYPxyz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"rJm5I9kJM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Will add a column about the data agumentations","comment":"Hello Hongyi, \n\nWe will add a column or a new table about the results with and without the data augmentations. Thank you for the pointer! \n\nBest,\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510086283416,"tcdate":1510086283416,"number":9,"cdate":1510086283416,"id":"rJm5I9kJM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BJzbrHC0-","signatures":["~Hongyi_Zhang1"],"readers":["everyone"],"writers":["~Hongyi_Zhang1"],"content":{"title":"Thanks (and additional comments)","comment":"Thanks for your clarification, it's very helpful.\n\nI think it is good to explicitly compare the data augmentation used by different methods in Table 2, so that the interested readers don't assume they all use the same augmentation, or don't have to look up each paper to figure out what augmentation each method used. For example, AFAIK, the Ladder Networks paper (Table 3, https://arxiv.org/pdf/1507.02672.pdf) reported results without data augmentation."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510085610893,"tcdate":1510085610893,"number":8,"cdate":1510085610893,"id":"r1mlEqyyz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Source of semi-supervised learning gains","comment":"I'm impressed by your semi-supervised learning results. However, without an ablation study it's hard to tell why your method works so well. Do you have any ideas about what's causing the improvement? It could be\n(1) You use both a GAN and consistency regularization (prior work uses one or the other).\n(2) Your GAN works better.\n(3) Your consistency regularization is better (either because dropout is better than Gaussian noise or because the second-to-last layer consistency term helps).\n(4) Improvements to the architecture/hyperparameters (e.g., using weight-norm instead of batch-norm as you mention in the appendix).\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509999866432,"tcdate":1509999866432,"number":7,"cdate":1509999866432,"id":"BJzbrHC0-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"Syb85wYR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for your concern.","comment":"Dear Hongyi,\n\nSorry for the late response. We did not receive or had missed the notification from Openreview about your comment. Following (Laine & Aila, 2016, Miyato et al., 2017, Tarvainen & Valpola, 2017), we do not apply any augmentation to MNIST and yet augment the CIFAR10 images in the following way. We flip the images horizontally and randomly translate the images within [-2,2] pixels horizontally. \n\nSamuli Laine and Timo Aila.  Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.\nAntti Tarvainen and Harri Valpola.  Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509681736930,"tcdate":1509681736930,"number":6,"cdate":1509681736930,"id":"Syb85wYR-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["~Hongyi_Zhang1"],"readers":["everyone"],"writers":["~Hongyi_Zhang1"],"content":{"title":"Any data augmentation?","comment":"In Figure 2, there is an \"augmentation\" process before feeding the input x into the network D. Could you clarify what \"augmentation\" means here? In particular, what kind of data preprocessing did you use in the semi-supervised learning experiments?\n\nThanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509410645475,"tcdate":1509410595464,"number":5,"cdate":1509410595464,"id":"r1s7vBrCb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"Hk9rHlS0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for the clarification","comment":"Thanks for the clarification. Comparing with DRAGAN in your experiments would have helped to understand where the benefit is coming from. They show improvements over WGAN-GP as well but use only DCGAN architecture. \n\nGood luck!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509397858477,"tcdate":1509397858477,"number":4,"cdate":1509397858477,"id":"B1cwHGBA-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BkWTPbHCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for your reply.","comment":"Now I can understand the experimental details. Thanks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427332,"tcdate":1509394361112,"number":3,"cdate":1509394361112,"id":"BkWTPbHCW","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"HJ-LeemC-","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"More experiment details","comment":"Thank you for the interest in our work. Please see below for the answers to your questions. \n\n(1) Dropout ratio: \n\n## The network used to learn from 1000 labeled CIFAR10 images only\nDiscriminator                                                                             Generator\nInput: 3*32*32 Image x                                                            Input: Noise z 128\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                        MLP 8192, ReLU, Batch norm\n0.5 Dropout                                                                               Reshape     512*4*4\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                       5*5 deconv. 256*8*8,\n0.5 Dropout                                                                              ReLU, Bach norm\n5*5 conv. 512, Pad = same Stride = 2, lReLU                       5*5 deconv. 128*16*16\n0.5 Dropout                                                                              ReLU, Batch norm\nReshape 512*4*4      (D_)                                                        5*5 deconv. 3*32*32\nMLP 1                          (D)                                                         Tanh\n\n## ResNet:\nDiscriminator                                                           |                Generator\nInput: 3*32*32 Image x                                          |                Input: Noise z 128\n[3*3]*2 Residual Block, Resample = DOWN        |                MLP 2048\n128*16*16                                                               |                Reshape 128*4*4\n[3*3]*2 Residual Block, Resample = DOWN        |                [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.2 Dropout                                             |                 128*8*8\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*16*16\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*32*32\nReLU, Global mean pool    (D_)                              |                 3*3 conv.  3*32*32\nMLP 1                                  (D)                                |                 Tanh\n\n## The network for MNIST\nDiscriminator                                                                               Generator\nInput: 1*28*28 Image x                                                              Input: Noise z 128\n5*5 conv. 64, Pad = same, Stride = 2, lReLU                            MLP 4096, ReLU\n0.5 Dropout                                                                                 Reshape 256*4*4\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                         5*5 deconv. 128*8*8\n0.5 Dropout                                                                                 ReLU, Cut 128*7*7\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                         5*5 deconv. 64*14*14\n0.5 Dropout                                                                                 ReLU\nReshape 256*4*4       (D_)                                                          5*5 deconv. 1*28*28\nMLP 1                         (D)                                                             Sigmoid\n\n(2) No. There are only two perturbations, as denoted by x' and x’’, for a data point x in each iteration. They are independently generated by the dropout as shown in my answer to your question (1). In other words, the two terms equation (5) are actually calculated over the same pair of x' and x'' for each draw x ~ P_r.\n\nWe will try to release the code in one or two weeks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427376,"tcdate":1509389716127,"number":2,"cdate":1509389716127,"id":"BJhcHerRb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"HkMNKONR-","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Thank you and we will correct it in the updated version","comment":"Sorry about that and Thank you for noting it! We will correct it in the updated version. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427414,"tcdate":1509389634003,"number":1,"cdate":1509389634003,"id":"Hk9rHlS0-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"ry2iCPQRb","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Thank you for directing us to DRAGAN","comment":"Thank you for directing us to DRAGAN. Sorry for missing it in our paper. We will include it in the updated version.\n\nGoing back to your question, the short answer is no because we do not actually aim to smooth the discriminator though the approach may have that effect. The long answer below clarifies it further and additionally highlights some differences between ours and DRAGAN.\n\nMotivations: DRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs by drawing results from the minimax theorem for zero-sum game. In sharp contrast, we propose an alternative way of enforcing the 1-Lipschitz continuity over the “critic” of WGANs thanks to the recent results by Arjovsky & Bottou (2017). \n\nHow to add the perturbations: One of the key observations in our experiments is that it reduces the quality of the generated samples if we add noise directly to the data points, as what is done in DRAGAN. Similar observations are reported by Arjovsky & Bottou (2017) and Wu et al. (2016). After many painstaking trials, we find good results by perturbing the hidden layers of the discriminator instead (as opposed to perturbing the original data). Besides, DRAGAN perturbs a data point once while we do it twice in an iteration. \n\nHow to use the perturbation: Similar to the gradient penalty proposed in (Gulrajani et al., 2017), DRGAN introduces a same regularization whereas for different reasons. In contrast, ours is a consistent regularization derived from the basic definition of Lipschitz continuous functions. \n\nSemi-supervised learning: One of the most notable features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila (2016) with GANs. \n\nFinally, here is the DRAGAN paper we found on ArXiv: https://arxiv.org/abs/1705.07215 just to confirm it with you. Going back to the DRGAN work, it would be interesting to investigate whether it generates blurry images too, for example by comparing the results of different amount of noise including no noise. It may do not because it constraints the gradient as oppose to the discriminator’s output. \n \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509357866515,"tcdate":1509357866515,"number":3,"cdate":1509357866515,"id":"HkMNKONR-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["~Antti_Tarvainen1"],"readers":["everyone"],"writers":["~Antti_Tarvainen1"],"content":{"title":"Citation typo","comment":"Looks like an interesting paper!\n\nI noticed you accidentally cited Salimans et al. on the fourth row of Table 2 when you (probably) meant to cite our work: https://arxiv.org/abs/1703.01780"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509290271795,"tcdate":1509289636439,"number":2,"cdate":1509289636439,"id":"ry2iCPQRb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Smoothing the discriminator near data manifold using local perturbations sounds familiar ","comment":"Your contribution looks like a relaxed version of DRAGAN's regularization scheme, which you don't cite anywhere. Is that correct? \n\nKodali, N., Abernethy, J., Hays, J. and Kira, Z., 2017. How to Train Your DRAGAN. arXiv preprint arXiv:1705.07215."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509257321034,"tcdate":1509257289032,"number":1,"cdate":1509257289032,"id":"HJ-LeemC-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Missing Experiment Parameters","comment":"I appreciate your contribution to generative model research. The results seem to be great.\nI was trying to reproduce the paper's result, but I think it is difficult to get some details:\n(1) Dropout ratio for generative models is not specified. It might be better to have Tables like Tables 4 and 5 for generative modeling tasks.\n(2) I could not understand the meaning of \"We find that it slightly improves the performance by further controlling the second-to-last layer D_(.) of the discriminator.\" Are we generating two more perturbed points x''' and x'''' by inserting a dropout layer at second-to-last layer -- as opposed to perturbed points x' and x''?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092379803,"tcdate":1509139082615,"number":1144,"cdate":1510092359481,"id":"SJx9GQb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/98bba828944f13faf32019e9400c7ce9615e175e.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]},"nonreaders":[],"replyCount":22,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}