{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222588778,"tcdate":1511901864415,"number":3,"cdate":1511901864415,"id":"Syx39Bsgf","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"],"readers":["everyone"],"content":{"title":"does not actual say why using Rademacher as a regularizer is theoretically justified, and why the loose bound is reasonable","rating":"4: Ok but not good enough - rejection","review":"==Main comments\n\nThe authors connect dropout parameters to a bound of the Rademacher complexity (Rad) of the network. It is cool to be able to optimize dropout parameters, and having a reasonable regularizer so that the optimal training retain rate is non-zero. I find the following leaps problematic and could not find where it was addressed in the paper:\n\n1) why is is adding Rad as a regularizer reasonable? Rad is usually hard to compute, and most useful for bounding the generalization error. It would be interesting if it turns out to be a good regularizer, but the authors are reasonable to say why or cite\n2) why is it reasonable to go from a regularizer based on RC to a loose bound of Rad? The actual resulting regularizer turns out to be a weight penalty but this seems to be a rather loose bound that might not have too much to do with Rad anymore. There should be some analysis on how loose this bound is, and if this looseness matter at all.\n\nThe empirical results themselves seem reasonable, but the results are not actually better than simpler methods in the corresponding tasks, the interpretation is less confident. Afterall, it seems that the proposed method had several parameters that were turned, where the analogous parameters are not present in the competing methods. And the per unit dropout rates are themselves additional parameters, but are they actually good use of parameters?\n\n==Minor comments\n\nThe optimization is perhaps also not quite right, since this requires taking the gradient of the dropout parameter in the original objective. While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. Afterall, the gradient used for regular learning is not based on the mean prediction, but rather the samples.\n\ntiny columns surrounding figures are ugly and hard to read\n\ndropout rate is perhaps more common than retain rate\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/7e27b5fe4d5a641b188866b2510200f3f6e3f26d.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1512222588823,"tcdate":1511805775704,"number":2,"cdate":1511805775704,"id":"HkOUXRFlz","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is an important piece of work that relates complexity of networks' learnability to dropout rates in backpropagation. This paper answers some critical questions about dropout learning.","rating":"7: Good paper, accept","review":"An important contribution. The paper is well written. Some questions that needs to be better answered are listed here.\n1. The theorem is difficult to decipher. Some remarks needs to be included explaining the terms on the right and what they mean with respect to learnability or complexity. \n2. How does the regularization term in eq (2) relate to the existing (currently used) norm based regularizers in deep network learning? It may be straight forward but some small simulation/plots explaining this is important. \n3. Apart from the accuracy results, the change in computational time for working with eq (2), rather than using existing state-of-the-art deep network optimization needs to be reported? How does this change vary with respect to dataset and network size (beyond the description of scaled regularization in section 4)?\n4. Confidence intervals needs to be computed for the retain-rates (reported as a function of epoch). This is critical both to evaluate the stability of regularizers as well as whether the bound from theorem is strong. \n5. Did the evaluations show some patterns on the retain rates across different layers? It seems from Figure 3,4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases. Is this a general pattern? \n6. It has been long known that dropout relates to non-negative weighted averaging of partially learned neural networks and dropout rate of 0.5 provides best dymanics. The evaluations say that clearly 0.5 for all units/layers us not correct. What does this mean in terms of network architecture? Is it that some layers are easy to average (nothing is learned there, so dropped networks have small variance), while some other layers are sensitive? \n7. What are some simple guidelines for choosing the values of p and q? Again it appears p=q=2 is the best, but need confidence intervals here to say anything substantial. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/7e27b5fe4d5a641b188866b2510200f3f6e3f26d.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1512222588863,"tcdate":1510752390645,"number":1,"cdate":1510752390645,"id":"Bk15lpF1G","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"],"readers":["everyone"],"content":{"title":"mathematical analysis seems not sound","rating":"5: Marginally below acceptance threshold","review":"This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory.\n\nMajor comments:\n(1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix.\n(2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of \\sum\\sigma_if^L(x_i,w) in (5).\n(3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there.\n(4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency?\n\nMinor comments:\n(1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5}\n(2) Abstract: there should be a space before \"Experiments\".\n(3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1\n\nSummary:\nThe mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/7e27b5fe4d5a641b188866b2510200f3f6e3f26d.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1509739431041,"tcdate":1509059311749,"number":204,"cdate":1509739428381,"id":"S1uxsye0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/7e27b5fe4d5a641b188866b2510200f3f6e3f26d.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}