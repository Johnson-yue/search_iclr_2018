{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222671273,"tcdate":1511831420627,"number":2,"cdate":1511831420627,"id":"S1rKPVcgz","invitation":"ICLR.cc/2018/Conference/-/Paper496/Official_Review","forum":"HkUR_y-RZ","replyto":"HkUR_y-RZ","signatures":["ICLR.cc/2018/Conference/Paper496/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good ideas, but lack of comparison against previous work and unclear experiments","rating":"5: Marginally below acceptance threshold","review":"This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text. In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore.\n\nPros:\n- Good literature review. But the future work on bandits is already happening:\nPaper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler.\n\n\nCons:\n- The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones. However there is no direct comparison either theoretical or empirical against them. In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method. Furthermore, given its simplicity, I would expect a comparison against scheduled sampling.\n\n- A lot of important experimental details are in the appendices and they differ among experiments. For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically. Also,  no details are given on how the mixing in the rollouts was tuned. Finally, in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work, this is not the case eventually, as it is acknowledged at least in the case of MIXER. I would have expected the same encoder-decoder architecture to have been used for all the methods considered.\n \n- the two losses introduced are not really new. The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one. Note that the notion of dynamic expert is present in the SEARN paper too. Goldberg and Nivre just adapted it to transition-based dependency parsing. Similarly, since the KL loss is the same as XENT, why give it a new name?\n\n- the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite. Thus it is not a novel contribution.\n  \n- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7. They both could be mini-batched similarly. Also, not sure I see why SEARNN can be used on any task, in comparison to other methods. They all seem to be equally capable.\n\nMinor comments:\n- Figure 1: what is the difference between \"cost-sensitive loss\" and just \"loss\"?\n- local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs.\n- Can't see why SEARNN can help with the vanishing gradient problem. Seem to be rather orthogonal.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SEARNN: Training RNNs with global-local losses","abstract":"We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the \"learning to search\" (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We first demonstrate improved performance over MLE on two different tasks: OCR and spelling correction. Then, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits of our approach on a machine translation task.","pdf":"/pdf/617d813f02fe4c3f3d27b08d1d5ba573eb5a04c6.pdf","TL;DR":"We introduce SeaRNN, a novel algorithm for RNN training, inspired by the learning to search approach to structured prediction, in order to avoid the limitations of MLE training.","paperhash":"anonymous|searnn_training_rnns_with_globallocal_losses","_bibtex":"@article{\n  anonymous2018searnn:,\n  title={SEARNN: Training RNNs with global-local losses},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkUR_y-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper496/Authors"],"keywords":["Structured prediction","RNNs"]}},{"tddate":null,"ddate":null,"tmdate":1512222671321,"tcdate":1511816192723,"number":1,"cdate":1511816192723,"id":"S1KZ3x5ef","invitation":"ICLR.cc/2018/Conference/-/Paper496/Official_Review","forum":"HkUR_y-RZ","replyto":"HkUR_y-RZ","signatures":["ICLR.cc/2018/Conference/Paper496/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Fascinating and well investigated extension of L2S to RNNs","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN. Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model.\n\nThe underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique.\n\nI am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset. As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset. For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component.\n\nThis reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains.\n\nOverall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights.\n\n(extremely minor typo: \"One popular possibility from L2S is go the full reduction route down to binary classification\")","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SEARNN: Training RNNs with global-local losses","abstract":"We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the \"learning to search\" (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We first demonstrate improved performance over MLE on two different tasks: OCR and spelling correction. Then, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits of our approach on a machine translation task.","pdf":"/pdf/617d813f02fe4c3f3d27b08d1d5ba573eb5a04c6.pdf","TL;DR":"We introduce SeaRNN, a novel algorithm for RNN training, inspired by the learning to search approach to structured prediction, in order to avoid the limitations of MLE training.","paperhash":"anonymous|searnn_training_rnns_with_globallocal_losses","_bibtex":"@article{\n  anonymous2018searnn:,\n  title={SEARNN: Training RNNs with global-local losses},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkUR_y-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper496/Authors"],"keywords":["Structured prediction","RNNs"]}},{"tddate":null,"ddate":null,"tmdate":1509739271189,"tcdate":1509124302476,"number":496,"cdate":1509739268530,"id":"HkUR_y-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkUR_y-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SEARNN: Training RNNs with global-local losses","abstract":"We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the \"learning to search\" (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We first demonstrate improved performance over MLE on two different tasks: OCR and spelling correction. Then, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits of our approach on a machine translation task.","pdf":"/pdf/617d813f02fe4c3f3d27b08d1d5ba573eb5a04c6.pdf","TL;DR":"We introduce SeaRNN, a novel algorithm for RNN training, inspired by the learning to search approach to structured prediction, in order to avoid the limitations of MLE training.","paperhash":"anonymous|searnn_training_rnns_with_globallocal_losses","_bibtex":"@article{\n  anonymous2018searnn:,\n  title={SEARNN: Training RNNs with global-local losses},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkUR_y-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper496/Authors"],"keywords":["Structured prediction","RNNs"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}