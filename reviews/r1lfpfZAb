{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222834995,"tcdate":1512071307870,"number":3,"cdate":1512071307870,"id":"HkN9lyRxG","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neat contribution that integrates previous work","rating":"7: Good paper, accept","review":"This paper proposes to bring together multiple inductive biases that hope to correct for inconsistencies in sequence decoding. Building on previous works that utilize modified objectives to generate sequences, this work proposes to optimize for the parameters of a pre-defined combination of various sub-objectives. The human evaluation is straight-forward and meaningful to compensate for the well-known inaccuracies of automatic evaluation. \n\nWhile the paper points out that they introduce multiple inductive biases that are useful to produce human-like sentences, it is not entirely correct that the objective is being learnt as claimed in portions of the paper. I would like this point to be clarified better in the paper. \n\nI think showing results on grounded generation tasks like machine translation or image-captioning would make a stronger case for evaluating relevance. I would like to see comparisons on these tasks. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/944843774a69d1f438a216e893ed052c9298b83b.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222835038,"tcdate":1511834849508,"number":2,"cdate":1511834849508,"id":"BJFJrHcgz","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-motivated goals, but the methods don't achieve them.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to improve RNN language model generation using augmented objectives inspired by Grice's maxims of communication. The idea is to combine the standard word-by-word decoding objective with additional objectives that reward sentences following these maxims. The proposed decoding objective is not new; reseachers in machine translation \n have worked on it referring to it as loss-augmented decoding: http://www.cs.cmu.edu/~nasmith/papers/gimpel+smith.naacl12.pdf\nThe use of RNNs in this context might be novel though.\n\nPros:\n- Well-motivated and ambitious goals\n\n- Human evaluation conducted on the outputs.\n\nCons:\n- My main concern is that it is unclear whether the models introduced are indeed implementing the Gricean maxims. For eaxample, the repetition model would not only discourage the same word occurring twice, but also a similar word (according to the word vectors used) to follow another one. \n\n- Similary, for the entailment model, what is an \"obvious\" entailment\"? Not sure we have training data for this in particular. Also, entailment suggests textual cohesion, which is conducive to the relation maxim. If this kind of model is what we need, why not take a state-of-the-art model?\n\n- The results seem to be inconsistent. The working vocabulary doesn't help in the tripAdvior experiment, while the RNN seems to work very well on the ROCstory data. While there might be good reasons for these, the point for me is that we cannot trust that the models added to the objective do what they are supposed to do.\n\n- Are the negative examples generated for the repetition model checked that they contain repetitions? Shouldn't be difficult to do. \n\n- Would be better to give the formula for the length model, the description is intuition but it is difficult to know exactly what the objective is\n\n- In algorithm 1, it seems like we fix in advance the max length of the sentence (max-step).  Is this the case? If so why? Also, the proposed learning algorithm only learns how to mix pre-trained models, not sure I agree they learn the objective. It is more of an ensembling.\n\n- As far as I can tell these ideas could have been more simply implemented by training a re-ranker to score the n-best outputs of the decoder. Why not try it? They are very popular in text generation tasks.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/944843774a69d1f438a216e893ed052c9298b83b.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222835080,"tcdate":1511765128833,"number":1,"cdate":1511765128833,"id":"ByWqV4YlG","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper combines RNN language model with several discriminatively trained models to improve the language generation. I like the idea of using Grice’s Maxims of communication to improve the language generation. However, some parts need to be further clarified and it would be nice to see more related analysis. ","rating":"5: Marginally below acceptance threshold","review":"This paper argues that the objective of RNN is not expressive enough to capture the good generation quality. In order to address the problems of RNN in generating languages, this paper combines the RNN language model with several other discriminatively trained models, and the weight for each sub model is learned through beam search. \n\nI like the idea of using Grice’s Maxims of communication to improve the language generation. Human evaluation shows significant improvement over the baseline. I have some detailed comments as follows:\n\n- The repetition model uses the samples from the base RNNs as negative examples. More analysis is needed to show it is a good negative sampling method.\n\n- As Section 3.2.3 introduced, “the unwanted entailment cases include repetitions and paraphrasing”. Does it mean the entailment model also handles repetition problem? Do we still need a separate repetition model? How about a separate paraphrasing model?\n\n- Equation 6 and the related text are not very clearly represented. It would be better to add more intuition and better explained. \n\n- In the Table 2, the automated bleu scores of L2W algorithm for Tripadvisor is very low (0.34 against 24.11). Is this normal? More explanation is needed here.\n\n- For human judgement, how many scores does each example get? It would be better to get multiple workers on M-Turk to label the same example, and compute the mean and variance. One score per example may not be reliable. \n\n- It would be interesting to see deeper analysis about how each model in the objectives influence the actual language generation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/944843774a69d1f438a216e893ed052c9298b83b.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092382739,"tcdate":1509137685713,"number":993,"cdate":1510092360822,"id":"r1lfpfZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/944843774a69d1f438a216e893ed052c9298b83b.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}