{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222634092,"tcdate":1511800376880,"number":3,"cdate":1511800376880,"id":"ByZSRnteG","invitation":"ICLR.cc/2018/Conference/-/Paper392/Official_Review","forum":"ryZ283gAZ","replyto":"ryZ283gAZ","signatures":["ICLR.cc/2018/Conference/Paper392/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Useful experiment results and not so clear insights","rating":"5: Marginally below acceptance threshold","review":"Originality\n--------------\nThe paper takes forward the idea of correspondence between  ResNets and discretization of ODEs. Introducing multi-step discretization is novel.\n\nClarity\n---------\n1)  The paper does not define the meaning of u_n=f(u).\n2) The stochastic control problem (what is the role of controller, how is connected to the training procedure) is not defined\n\nQuality\n---------\nWhile the experiments are done in CIFAR-10 and 100,  ImageNet and improvements are reported, however, connection/insights to why the improvement is obtained is still missing. Thus the evidence is only partial, i.e., we still don't know why the connection between ODE and ResNet is helpful at all.\n\nSignificance\n-----------------\nStrength: LM architectures reduce the layers in some cases and achieve the same level of accuracy.\nWeakness: Agreed that LM methods are better approximations of the ODEs. Where do we gain? (a) It helps if we faithfully discretize the ODE. Why does (a) help? We don't have a clear answer; which takes back to the lack of what the underlying stochastic control problem is.\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations","abstract":"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.","pdf":"/pdf/69629338cc0a9366665689398f9332b147d1c1e5.pdf","TL;DR":"This paper bridges deep network architectures with numerical (stochastic) differential equations. This new perspective enables new designs of more effective deep neural networks.","paperhash":"anonymous|beyond_finite_layer_neural_networks_bridging_deep_architectures_and_numerical_differential_equations","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ283gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper392/Authors"],"keywords":["deep convolutional network","residual network","dynamic system","stochastic dynamic system","modified equation"]}},{"tddate":null,"ddate":null,"tmdate":1512222634131,"tcdate":1511501162440,"number":2,"cdate":1511501162440,"id":"ryMdpXref","invitation":"ICLR.cc/2018/Conference/-/Paper392/Official_Review","forum":"ryZ283gAZ","replyto":"ryZ283gAZ","signatures":["ICLR.cc/2018/Conference/Paper392/AnonReviewer3"],"readers":["everyone"],"content":{"title":"further comparisons & inconsistent reported baselines","rating":"5: Marginally below acceptance threshold","review":"The authors cast some of the most recent CNN designs as approximate solutions to discretized ODEs. On that basis, they propose a new type of block architecture which they evaluate on CIFAR and ImageNet. They show small gains when applying their design on the ResNet architectures. They also draw a comparison between a stochastic learning process and approximation to stochastic dynamic systems.\n\nPros:\n(+) The paper presents a way to connect NN design with principled approximations to systems\n(+) Experiments are shown on compelling benchmarks such as ImageNet\nCons:\n(-) It is not clear why the proposed approach is superior to the other designs\n(-) Gains are relatively small and at a price of a more complicated design\n(-) Incosistent baselines reported\n\nWhile the effort of presenting recent CNN designs as plausible approximations to ODEs, the paper does not try to draw connections among the different approaches, compare them or prove the limits of their related approximations. In addition, it is unclear from the paper how the proposed approach (LM-architecture) compares to the recent works, what are the benefits and gains from casting is as a direct relative to the multi-step scheme in numerical ODEs. How do the different approximations relate in terms of convergence rates, error bounds etc.?\n\nExperimentwise, the authors show some gains on CIFAR 10/100, or 0.5% (see ResNeXt Table1), while also introducing slightly more parameters. On ImageNet1k, comparisons to ResNeXt are missing from Table3, while the comparison with the ResNets show gains in the order of 1% for top-1 accuracy. \n\nTable3 is concerning. With a single crop testing scheme, ResNet101 is yielding top-1 error of 22% and top-5 error of 6% (see Table 5 of Xie et al, 2017 (aka ResNeXt)). However, the authors report 23.6% and 7.1% respectively for their ResNet101. The performance stated by the authors of ResNe(X)t weakens the empirical results of LM-architecture.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations","abstract":"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.","pdf":"/pdf/69629338cc0a9366665689398f9332b147d1c1e5.pdf","TL;DR":"This paper bridges deep network architectures with numerical (stochastic) differential equations. This new perspective enables new designs of more effective deep neural networks.","paperhash":"anonymous|beyond_finite_layer_neural_networks_bridging_deep_architectures_and_numerical_differential_equations","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ283gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper392/Authors"],"keywords":["deep convolutional network","residual network","dynamic system","stochastic dynamic system","modified equation"]}},{"tddate":null,"ddate":null,"tmdate":1512222634172,"tcdate":1511282723299,"number":1,"cdate":1511282723299,"id":"ByjXORWlG","invitation":"ICLR.cc/2018/Conference/-/Paper392/Official_Review","forum":"ryZ283gAZ","replyto":"ryZ283gAZ","signatures":["ICLR.cc/2018/Conference/Paper392/AnonReviewer2"],"readers":["everyone"],"content":{"title":" [Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations]","rating":"6: Marginally above acceptance threshold","review":"The authors proposed to bridge deep neural network design with numerical differential equations. They found that many effective networks can be interpreted as different numerical discretization of differential equations and provided a new perspective on the design of effective deep architectures. \n\nThis paper is interesting in general and it will be useful to design new and potentially more effective deep networks. Regarding the technical details, the reviewer has the following comments:\n\n- The authors draw a relatively comprehensive connection between the architecture of popular deep networks and discretization schemes of ODEs. Is it possible to show stability of the architecture of deep networks based on their associated ODEs? Related to this, can we choose step size or the number of layers to guarantee numerical stability?\n\n- It is very interesting to consider networks as stochastic dynamic systems. Are there any limitations of this interpretation or discrepancy due to the weak approximation? ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations","abstract":"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.","pdf":"/pdf/69629338cc0a9366665689398f9332b147d1c1e5.pdf","TL;DR":"This paper bridges deep network architectures with numerical (stochastic) differential equations. This new perspective enables new designs of more effective deep neural networks.","paperhash":"anonymous|beyond_finite_layer_neural_networks_bridging_deep_architectures_and_numerical_differential_equations","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ283gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper392/Authors"],"keywords":["deep convolutional network","residual network","dynamic system","stochastic dynamic system","modified equation"]}},{"tddate":null,"ddate":null,"tmdate":1509739328520,"tcdate":1509111465405,"number":392,"cdate":1509739325857,"id":"ryZ283gAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryZ283gAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations","abstract":"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.","pdf":"/pdf/69629338cc0a9366665689398f9332b147d1c1e5.pdf","TL;DR":"This paper bridges deep network architectures with numerical (stochastic) differential equations. This new perspective enables new designs of more effective deep neural networks.","paperhash":"anonymous|beyond_finite_layer_neural_networks_bridging_deep_architectures_and_numerical_differential_equations","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ283gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper392/Authors"],"keywords":["deep convolutional network","residual network","dynamic system","stochastic dynamic system","modified equation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}