{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222544260,"tcdate":1511817231988,"number":3,"cdate":1511817231988,"id":"S1ufxZqlG","invitation":"ICLR.cc/2018/Conference/-/Paper1045/Official_Review","forum":"ryZERzWCZ","replyto":"ryZERzWCZ","signatures":["ICLR.cc/2018/Conference/Paper1045/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not clear what specific insights exist or what problem this solves","rating":"4: Ok but not good enough - rejection","review":"The authors propose an objective whose Lagrangian dual admits a variety of modern objectives from variational auto-encoders and generative adversarial networks. They describe tradeoffs between flexibility and computation in this objective leading to different approaches. Unfortunately, I'm not sure what specific contributions come out, and the paper seems to meander in derivations and remarks that I didn't understand what the point was.\n\nFirst, it's not clear what this proposed generalization offers. It's a very nuanced and not insightful construction (eq. 3) and with a specific choice of a weighted sum of mutual informations subject to a combinatorial number of divergence measure constraints, each possibly held in expectation (eq. 5) to satisfy the chosen subclass of VAEs and GANs; and with or without likelihoods (eq. 7). What specific insights come from this that isn't possible without the proposed generalization?\n\nIt's also not clear with many GAN algorithms that reasoning with their divergence measure in the limit of infinite capacity discriminators is even meaningful (e.g., Arora et al., 2017; Fedus et al., 2017). It's only true for consistent objectives such as MMD-GANs.\n\nSection 4 seems most pointed in explaining potential insights.  However, it only introduces hyperparameters and possible combinatorial choices with no particular guidance in mind. For example, there are no experiments demonstrating the usefulness of this approach except for a toy mixture of Gaussians and binarized MNIST, explaining what is already known with the beta-VAE and infoGAN. It would be useful if the authors could make the paper overall more coherent and targeted to answer specific problems in the literature rather than try to encompass all of them.\n\nMisc\n+ The \"feature marginal\" is also known as the aggregate posterior (Makhzani et al., 2015) and average encoding distribution (Hoffman and Johnson, 2016); also see Tomczak and Welling (2017).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling","abstract":"A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.","pdf":"/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf","paperhash":"anonymous|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling","_bibtex":"@article{\n  anonymous2018the,\n  title={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZERzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1045/Authors"],"keywords":["Generative Models","Variational Autoencoder","Generative Adversarial Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222544304,"tcdate":1511768815907,"number":2,"cdate":1511768815907,"id":"SkugmHtgf","invitation":"ICLR.cc/2018/Conference/-/Paper1045/Official_Review","forum":"ryZERzWCZ","replyto":"ryZERzWCZ","signatures":["ICLR.cc/2018/Conference/Paper1045/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good framework for learning generative models, but significance/consequence of the results is unclear","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. Using this framework, the paper tries to generalize the optimization problems by defining computationally-tractable family which can be expressed in terms of existing objective functions. \n\nThe paper has interesting elements and the results are original. The main issue is that the significance is unclear. The writing in Section 3 is unclear for me, which further made it challenging to understand the consequences of the theorems presented in that section. \n\nHere is a big-picture question that I would like to know answer for. Do the results of sec 3 help us identify a more useful/computationally tractable model than exiting approaches? Clarification on this will help me evaluate the significance of the paper.\n\nI have three main clarification points. First, what is the importance of T1, T2, and T3 classes defined in Def. 7, i.e., why are these classes useful in solving some problems? Second, is the opposite relationship in Theorem 1, 2, and 3 true as well, e.g., is every linear combination of beta-ELBO and VMI is equivalent to a likelihood-based computable-objective of KL info-encoding family? Is the same true for other theorems?\n\nThird, the objective of section 3 is to show that \"only some choices of lambda lead to a dual with a tractable equivalent form\". Could you rewrite the theorems so that they truly reflect this, rather than stating something which only indirectly imply the main claim of the paper.\n\nSome small comments:\n- Eq. 4. It might help to define MI to remind readers.\n- After Eq. 7, please add a proof (may be in the Appendix). It is not that straightforward to see this. Also, I suppose you are saying Eq. 3 but with f from Eq. 4.\n- Line after Eq. 8, D_i is \"one\" of the following... Is it always the same D_i for all i or it could be different? Make this more clear to avoid confusion.\n- Last line in Para after Eq. 15, \"This neutrality corresponds to the observations made in..\" It might be useful to add a line explaining that particular \"observation\"\n- Def. 7, the names did not make much sense to me. You can add a line explaining why this name is chosen.\n- Def. 8, the last equation is unclear. Does the first equivalence impy the next one? \n- Writing in Sec. 3.3 can be improved. e.g., \"all linear operations on log prob.\" is very unclear, \"stated computational constraints\" which constraints?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling","abstract":"A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.","pdf":"/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf","paperhash":"anonymous|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling","_bibtex":"@article{\n  anonymous2018the,\n  title={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZERzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1045/Authors"],"keywords":["Generative Models","Variational Autoencoder","Generative Adversarial Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222544347,"tcdate":1511717117835,"number":1,"cdate":1511717117835,"id":"BJ8bKuOlM","invitation":"ICLR.cc/2018/Conference/-/Paper1045/Official_Review","forum":"ryZERzWCZ","replyto":"ryZERzWCZ","signatures":["ICLR.cc/2018/Conference/Paper1045/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Contains some interesting results but the presentation is not focused","rating":"6: Marginally above acceptance threshold","review":"Thank you for an interesting read.\n\nThe paper presented a unifying framework for many existing generative modelling techniques, by first considering constrained optimisation problem of mutual information, then addressing the problem using Lagrange multipliers.\n\nI see the technical contribution to be the three theorems, in the sense that it gives a closure of all possible objective functions (if using the KL divergences). This can be useful: I'm tired of reading papers which just add some extra \"regularisation terms\" and claim they work. I did not check every equation of the proof, but it seems correct to me.\n\nHowever, an imperfection is, the paper did not provide a convincing explanation on why their view should be preferred compared to the original papers' intuition.  For example in VAE case, why this mutual information view is better than the traditional view of approximate MLE, where q is known to be the approximate posterior? A better explanation on this (and similarly for say infoGAN/infoVAE) will significantly improve the paper.\n\nContinuing on the above point, why in section 4 you turn to discuss relationship between mutual information and test-LL?  How does that relate to the main point you want to present in the paper, which is to prefer MI interpretation if I understand it correctly?\n\nTerm usage: we usually *maximize* the ELBO and *minimise* the variational free-energy (VFE). ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling","abstract":"A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.","pdf":"/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf","paperhash":"anonymous|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling","_bibtex":"@article{\n  anonymous2018the,\n  title={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZERzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1045/Authors"],"keywords":["Generative Models","Variational Autoencoder","Generative Adversarial Network"]}},{"tddate":null,"ddate":null,"tmdate":1510092381764,"tcdate":1509137965386,"number":1045,"cdate":1510092360422,"id":"ryZERzWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryZERzWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling","abstract":"A variety of learning objectives have been recently proposed for training generative models. We show that many of them, including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, VAE, $\\beta$-VAE, adversarial autoencoders, AVB, and InfoVAE, are Lagrangian duals of the same primal optimization problem. This generalization reveals the implicit modeling trade-offs  between flexibility and  computational requirements being made by these models. Furthermore, we characterize the class of all objectives that can be optimized under certain computational constraints.\nFinally, we show how this new Lagrangian perspective can explain undesirable behavior of existing methods and provide new principled solutions.","pdf":"/pdf/27ef4955b028c7a77d5b76170b017d35d32e5569.pdf","paperhash":"anonymous|the_informationautoencoding_family_a_lagrangian_perspective_on_latent_variable_generative_modeling","_bibtex":"@article{\n  anonymous2018the,\n  title={The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZERzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1045/Authors"],"keywords":["Generative Models","Variational Autoencoder","Generative Adversarial Network"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}