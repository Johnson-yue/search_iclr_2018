{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222829150,"tcdate":1511887339386,"number":3,"cdate":1511887339386,"id":"HyXxfzsxM","invitation":"ICLR.cc/2018/Conference/-/Paper972/Official_Review","forum":"rJl63fZRb","replyto":"rJl63fZRb","signatures":["ICLR.cc/2018/Conference/Paper972/AnonReviewer3"],"readers":["everyone"],"content":{"title":".","rating":"6: Marginally above acceptance threshold","review":"Summary of paper: The goal of this paper is to be able to construct programs given data consisting of program input and program output pairs. Previous works by Reed & Freitas (2016) (using the paper's references) and Cai et al. (2017) used fully supervised trace data. Li et al. (2017) used a mixture of fully supervised and weakly supervised trace data. The supervision helps with discovering the hierarchical structure in the program which helps generalization to other program inputs. The method is heavily based on the \"Discovery of Deep Options\" (DDO) algorithm by Fox et al. (2017).\n\n---\n\nQuality: The experiments are chosen to compare the method that the paper is proposing directly with the method from Li et al. (2017).\nClarity: The connection between learning a POMDP policy and program induction could be made more explicit. In particular, section 3 describes the problem statement but in terms of learning a POMDP policy. The only sentence with some connection to learning programs is the first one.\nOriginality: This line of work is very recent (as far as I know), with Li et al. (2017) being the other work tackling program learning from a mixture of supervised and weakly supervised program trace data.\nSignificance: The problem that the paper is solving is significant. The paper makes good progress in demonstrating this on toy tasks.\n\n---\n\nSome questions/comments:\n- Is the Expectation-Gradient trick also known as the reinforce/score function trick?\n- This paper could benefit from being rewritten so that it is in one language instead of mixing POMDP language used by Fox et al. (2017) and program learning language. It is not exactly clear, for example, how are memory states m_t and states s_t related to the program traces.\n- It would be nice if the experiments in Figure 2 could compare PHP and NPL on exactly the same total number of demonstrations.\n\n---\n\nSummary: The problem under consideration is important and experiments suggest good progress. However, the clarity of the paper could be made better by making the connection between POMDPs and program learning more explicit or if the algorithm was introduced with one language.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parameterized Hierarchical Procedures for Neural Programming","abstract":"Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite its potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parameterized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, that uses a program counter, along with the observation, to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a mixture of annotated and unannotated demonstrations, and demonstrate efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of strong and weak supervision.","pdf":"/pdf/4007b99586e4f95de8cd1a8d192f374e1559aa2b.pdf","TL;DR":"We introduce the PHP model for hierarchical representation of neural programs, and an algorithm for learning PHPs from a mixture of strong and weak supervision.","paperhash":"anonymous|parameterized_hierarchical_procedures_for_neural_programming","_bibtex":"@article{\n  anonymous2018parameterized,\n  title={Parameterized Hierarchical Procedures for Neural Programming},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl63fZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper972/Authors"],"keywords":["Neural programming","Hierarchical Control"]}},{"tddate":null,"ddate":null,"tmdate":1512222829194,"tcdate":1511819495888,"number":2,"cdate":1511819495888,"id":"SylxFWcgG","invitation":"ICLR.cc/2018/Conference/-/Paper972/Official_Review","forum":"rJl63fZRb","replyto":"rJl63fZRb","signatures":["ICLR.cc/2018/Conference/Paper972/AnonReviewer2"],"readers":["everyone"],"content":{"title":"NPI with less supervision","rating":"6: Marginally above acceptance threshold","review":"This paper extends an existing thread of neural computation research focused on learning resuable subprocedures (or options in RL-speak).  Instead of simply input and output examples, as in most of the work in neural computation, they follow in the vein of the Neural Programmer-Interpreter (Reed and de Freitas, 2016) and Li et. al., 2017, where the supervision contains the full sequence of elementary actions in the domain for all samples, and some samples also contain the hierarchy of subprocedure calls.\n\nThe main focus of their work is learning from fewer fully annotated samples than prior work.  They introduce two new ideas in order to enable this:\n1.  They limit the memory state of each level in the program heirarchy to simply a counter indicating the number of elementary actions/subprocedure calls taken so far (rather than a full RNN embedded hidden/cell state as in prior work).  They also limit the subprocedures such that they do not accept any arguments.\n2.  By considering this very limited set of possible hidden states, they can compute the gradients using a dynamic program that seems to be more accurate than the approximate dynamic program used in Li et. al., 2017.  \n\nThe main limitation of the work is this extremely limited memory state, and the lack of arguments.  Without arguments, everything that parameterizes the subprocedures must be in the visible world state.  In both of their domains, this is true, but this places a significant limitation on the algorithms which can be modeled with this technique.  Furthermore, the limited memory state means that the only way a subprocedure can remember anything about the current observation is to call a different subprocedure.  Again, their two evalation tasks fit into this paradigm, but this places very significant limitations on the set of applicable domains.  I would have like to see more discussion on how constraining these limitations would be in practice.  For example, it seems it would be impossible for this architecture to perform the Nanocraft task if the parameters of the task (width, height, etc.) were only provided in the first observation, rather than every observation.  \n\nNone-the-less I think this work is an important step in our understanding of the learning dynamics for neural programs.  In particular, while the RNN hidden state memory used by the prior work enables the learning of more complicted programs *in theory*, this has not been shown in practice. So, it's possible that all the prior work is doing is learning to approixmate a much simpler architecture of this form.  Specifically, I think this work can act as a great base-line by forcing future work to focus on domains which cannot be easily solved by a simpler architecture of this form.  This limitation will also force the community to think about which tasks require a more complicated form of memory, and which can be solved with a very simple memory of this form.\n\n\nI also have the following additional concerns about the paper:\n\n1.  I found the current explanation of the algorithm to be very difficult to understand.  It's extremely difficult to understand the core method without reading the appendix, and even with the appendix I found the explanation of the level-by-level decomposition to be too terse.\n\n2.  It's not clear how their gradient approximation compares to the technique used by Li et. al.  They obviously get better results on the addition and Nanocraft domains, but I would have liked a more clear explanation and/or some experiments providing insights into what enables these improvements (or at least an admission by the authors that they don't really understand what enabled the performance improvements).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parameterized Hierarchical Procedures for Neural Programming","abstract":"Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite its potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parameterized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, that uses a program counter, along with the observation, to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a mixture of annotated and unannotated demonstrations, and demonstrate efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of strong and weak supervision.","pdf":"/pdf/4007b99586e4f95de8cd1a8d192f374e1559aa2b.pdf","TL;DR":"We introduce the PHP model for hierarchical representation of neural programs, and an algorithm for learning PHPs from a mixture of strong and weak supervision.","paperhash":"anonymous|parameterized_hierarchical_procedures_for_neural_programming","_bibtex":"@article{\n  anonymous2018parameterized,\n  title={Parameterized Hierarchical Procedures for Neural Programming},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl63fZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper972/Authors"],"keywords":["Neural programming","Hierarchical Control"]}},{"tddate":null,"ddate":null,"tmdate":1512222829239,"tcdate":1511662818865,"number":1,"cdate":1511662818865,"id":"SkiyHjDlf","invitation":"ICLR.cc/2018/Conference/-/Paper972/Official_Review","forum":"rJl63fZRb","replyto":"rJl63fZRb","signatures":["ICLR.cc/2018/Conference/Paper972/AnonReviewer1"],"readers":["everyone"],"content":{"title":"a good paper","rating":"6: Marginally above acceptance threshold","review":"In the paper titled \"Parameterized Hierarchical Procedures for Neural Programming\", the authors proposed \"Parametrized Hierarchical Procedure (PHP)\", which is a representation of a hierarchical procedure by differentiable parametrization. Each PHP is represented with two multi-layer perceptrons with ReLU activation, one for its operation statement and one for its termination statement. With two benchmark tasks (NanoCraft and long-hand addition), the authors demonstrated that PHPs are able to learn neural programs accurately from smaller amounts of strong/weak supervision. \n\nOverall the paper is well-written with clear logic and accurate narratives. The methodology within the paper appears to be reasonable to me. Because this is not my research area, I cannot judge its technical contribution. ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parameterized Hierarchical Procedures for Neural Programming","abstract":"Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite its potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parameterized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, that uses a program counter, along with the observation, to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a mixture of annotated and unannotated demonstrations, and demonstrate efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of strong and weak supervision.","pdf":"/pdf/4007b99586e4f95de8cd1a8d192f374e1559aa2b.pdf","TL;DR":"We introduce the PHP model for hierarchical representation of neural programs, and an algorithm for learning PHPs from a mixture of strong and weak supervision.","paperhash":"anonymous|parameterized_hierarchical_procedures_for_neural_programming","_bibtex":"@article{\n  anonymous2018parameterized,\n  title={Parameterized Hierarchical Procedures for Neural Programming},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl63fZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper972/Authors"],"keywords":["Neural programming","Hierarchical Control"]}},{"tddate":null,"ddate":null,"tmdate":1510092383252,"tcdate":1509137594509,"number":972,"cdate":1510092361028,"id":"rJl63fZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJl63fZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Parameterized Hierarchical Procedures for Neural Programming","abstract":"Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite its potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parameterized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, that uses a program counter, along with the observation, to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a mixture of annotated and unannotated demonstrations, and demonstrate efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of strong and weak supervision.","pdf":"/pdf/4007b99586e4f95de8cd1a8d192f374e1559aa2b.pdf","TL;DR":"We introduce the PHP model for hierarchical representation of neural programs, and an algorithm for learning PHPs from a mixture of strong and weak supervision.","paperhash":"anonymous|parameterized_hierarchical_procedures_for_neural_programming","_bibtex":"@article{\n  anonymous2018parameterized,\n  title={Parameterized Hierarchical Procedures for Neural Programming},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl63fZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper972/Authors"],"keywords":["Neural programming","Hierarchical Control"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}