{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222707473,"tcdate":1511819013035,"number":2,"cdate":1511819013035,"id":"ByT-vbcgM","invitation":"ICLR.cc/2018/Conference/-/Paper64/Official_Review","forum":"S1_6Tana-","replyto":"S1_6Tana-","signatures":["ICLR.cc/2018/Conference/Paper64/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting method and the formulation is precisely given. Experimental results are not strong.","rating":"5: Marginally below acceptance threshold","review":"This paper presents a novel algorithm for unsupervised learning based on information theoretic co-training. The objective is to find parameters that maximize the mutual information between x and y through hidden variable z without computing the probabilities of x and y directly. This means that this technique allows to find a structure from data in an unsupervised way without modeling x and y explicitly. The proposed method is applied to unsupervised learning of phonemes on TIMIT corpus. The experimental results show that the learnt representations correspond to the labeled phonemes 35.1% on all frames.\n\nThe idea looks original and the formulation is presented precisely. Comprehensive comparisons to related models are given. The experimental results are not very strong, but the authors claim that they are initial experiments and can be improved with just a little more experimentation.\n\nThe experiments could be improved to show the effectiveness of the proposed approach better. The experimental results show that it can learn something but it is difficult to know how good the number is. Probably, it is useful to compare the method with some simple baseline even if it is not the objective of the paper to achieve high accuracy. Also, I am not sure if the phoneme discovery is the best task for the experiments. If the purpose is to discover phoneme as accurate as possible, probably, one might take another approach. Probably, it is better to emphasize that this is just to show the ability of the approach. It will be useful to show a list of possible applications. If more experiments can be added with such applications, it will be great.\n\nI would recommend to have a figure to explain the procedure of the algorithm, i.e., how to update phi and psi.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Information Theoretic Co-Training","abstract":"This paper introduces an information theoretic co-training objective for unsupervised learning.  We consider the problem of predicting the future. Rather than predict future sensations (image pixels or sound waves) we predict ``hypotheses'' to be confirmed by future sensations. More formally, we assume a population distribution on pairs $(x,y)$ where we can think of $x$ as a past sensation and $y$ as a future sensation. We train both a predictor model $P_\\Phi(z|x)$ and a confirmation model $P_\\Psi(z|y)$ where we view $z$ as hypotheses (when predicted) or facts (when confirmed). For a population distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual information between $x$ and $y$. By the data processing inequality this mutual information is at least as large as the mutual information between $x$ and $z$ under the distribution on triples $(x,z,y)$ defined by the confirmation model $P_\\Psi(z|y)$. The information theoretic training objective for $P_\\Phi(z|x)$ and $P_\\Psi(z|y)$ can be viewed as a form of co-training where we want the prediction from $x$ to match the confirmation from $y$. We give experiments on applications to learning phonetics on the TIMIT dataset.","pdf":"/pdf/926a35bd48dc9b9a8d88d3a80bb070d7ccfed74a.pdf","TL;DR":"Presents an information theoretic training objective for co-training and demonstrates its power in unsupervised learning of phonetics.","paperhash":"anonymous|information_theoretic_cotraining","_bibtex":"@article{\n  anonymous2018information,\n  title={Information Theoretic Co-Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1_6Tana-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper64/Authors"],"keywords":["co-training","phonetics","unsupervised learning","mutual information"]}},{"tddate":null,"ddate":null,"tmdate":1512222707512,"tcdate":1511805972934,"number":1,"cdate":1511805972934,"id":"Bk6zECKgf","invitation":"ICLR.cc/2018/Conference/-/Paper64/Official_Review","forum":"S1_6Tana-","replyto":"S1_6Tana-","signatures":["ICLR.cc/2018/Conference/Paper64/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but more experiments needed","rating":"4: Ok but not good enough - rejection","review":"In this work, the authors propose an information theoretic objective for co-training, wherein two learners are trained together to predict the same label, z, from two \"views\", x and y, of the data. Unlike the classical co-training framework, the focus here is on the fully unsupervised setting. Also, here there is an asymmetry between the two views: it is easy to predict z from y, and the goal is to learn to closely match this prediction given only x. This is enforced using an information theoretic loss function.\n\nI liked the formulation of the problem and the loss function itself. I can imagine that this is an idea that will find use in various scenarios. However, as of now, this is just a nugget of an idea that needs to be developed much more. The only experiment given in the paper gives somewhat underwhelming results. (Of course, this may not be the fault of the main idea itself, and may have to do with the other details in the experiment.) But to meet the bar for ICLR, I feel that the authors should present more convincing evidence. Also, in the process, they may discover crucial details that are important to get the idea to work well.\n\nThe authors themselves note that the experiments are preliminary and that \"with just a little more experimentation\" they hope to get stronger results. I share their optimism, and would encourage them to carry out these experiments and make a stronger submission.\n\nPros:\nPromising/interesting idea\n\nCons:\nNot fully developed\nThin experimental section","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Information Theoretic Co-Training","abstract":"This paper introduces an information theoretic co-training objective for unsupervised learning.  We consider the problem of predicting the future. Rather than predict future sensations (image pixels or sound waves) we predict ``hypotheses'' to be confirmed by future sensations. More formally, we assume a population distribution on pairs $(x,y)$ where we can think of $x$ as a past sensation and $y$ as a future sensation. We train both a predictor model $P_\\Phi(z|x)$ and a confirmation model $P_\\Psi(z|y)$ where we view $z$ as hypotheses (when predicted) or facts (when confirmed). For a population distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual information between $x$ and $y$. By the data processing inequality this mutual information is at least as large as the mutual information between $x$ and $z$ under the distribution on triples $(x,z,y)$ defined by the confirmation model $P_\\Psi(z|y)$. The information theoretic training objective for $P_\\Phi(z|x)$ and $P_\\Psi(z|y)$ can be viewed as a form of co-training where we want the prediction from $x$ to match the confirmation from $y$. We give experiments on applications to learning phonetics on the TIMIT dataset.","pdf":"/pdf/926a35bd48dc9b9a8d88d3a80bb070d7ccfed74a.pdf","TL;DR":"Presents an information theoretic training objective for co-training and demonstrates its power in unsupervised learning of phonetics.","paperhash":"anonymous|information_theoretic_cotraining","_bibtex":"@article{\n  anonymous2018information,\n  title={Information Theoretic Co-Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1_6Tana-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper64/Authors"],"keywords":["co-training","phonetics","unsupervised learning","mutual information"]}},{"tddate":null,"ddate":null,"tmdate":1509739506034,"tcdate":1508855231963,"number":64,"cdate":1509739503370,"id":"S1_6Tana-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1_6Tana-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Information Theoretic Co-Training","abstract":"This paper introduces an information theoretic co-training objective for unsupervised learning.  We consider the problem of predicting the future. Rather than predict future sensations (image pixels or sound waves) we predict ``hypotheses'' to be confirmed by future sensations. More formally, we assume a population distribution on pairs $(x,y)$ where we can think of $x$ as a past sensation and $y$ as a future sensation. We train both a predictor model $P_\\Phi(z|x)$ and a confirmation model $P_\\Psi(z|y)$ where we view $z$ as hypotheses (when predicted) or facts (when confirmed). For a population distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual information between $x$ and $y$. By the data processing inequality this mutual information is at least as large as the mutual information between $x$ and $z$ under the distribution on triples $(x,z,y)$ defined by the confirmation model $P_\\Psi(z|y)$. The information theoretic training objective for $P_\\Phi(z|x)$ and $P_\\Psi(z|y)$ can be viewed as a form of co-training where we want the prediction from $x$ to match the confirmation from $y$. We give experiments on applications to learning phonetics on the TIMIT dataset.","pdf":"/pdf/926a35bd48dc9b9a8d88d3a80bb070d7ccfed74a.pdf","TL;DR":"Presents an information theoretic training objective for co-training and demonstrates its power in unsupervised learning of phonetics.","paperhash":"anonymous|information_theoretic_cotraining","_bibtex":"@article{\n  anonymous2018information,\n  title={Information Theoretic Co-Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1_6Tana-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper64/Authors"],"keywords":["co-training","phonetics","unsupervised learning","mutual information"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}