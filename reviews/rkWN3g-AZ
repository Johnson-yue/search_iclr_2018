{"notes":[{"tddate":null,"ddate":null,"tmdate":1511794440027,"tcdate":1511794440027,"number":2,"cdate":1511794440027,"id":"SygfwiYgM","invitation":"ICLR.cc/2018/Conference/-/Paper626/Public_Comment","forum":"rkWN3g-AZ","replyto":"HkcKlK_1f","signatures":["~R_Devon_Hjelm1"],"readers":["everyone"],"writers":["~R_Devon_Hjelm1"],"content":{"title":"many to many","comment":"I disagree that extending to conditioning on noise is \"trivial\", as this is a well-known alignment problem in unsupervised domain mapping. Please see \nhttps://arxiv.org/pdf/1709.00074.pdf\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]}},{"tddate":null,"ddate":null,"tmdate":1512222703441,"tcdate":1511756262966,"number":2,"cdate":1511756262966,"id":"BJklMMFez","invitation":"ICLR.cc/2018/Conference/-/Paper626/Official_Review","forum":"rkWN3g-AZ","replyto":"rkWN3g-AZ","signatures":["ICLR.cc/2018/Conference/Paper626/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The new task and new dataset are nice contributions. Technical novelty is limited. Experiment design could be improved.","rating":"4: Ok but not good enough - rejection","review":"This paper proposed an X-shaped GAN for the so called semantic style transfer task, in which the goal is to transfer the style of an image from one domain to another without altering the semantic content of the image. Here, a domain is collectively defined by the images of the same style, e.g., cartoon faces. \n\nThe cost function used to train the network consists of five terms of which four are pretty standard: a reconstruction loss, two regular GAN-type losses, and an imitation loss. The fifth term, called the semantic consistency loss, is one of the main contributions of this paper. This loss ensures that the translated images should be encoded into about the same location as the embedding of the original image, albeit by different encoders. \n\nStrengths:\n1. The new CartoonSet dataset is carefully designed and compiled. It could facilitate the future research on style transfer. \n2. The paper is very well written. I enjoyed reading the paper. The text is concise and also clear enough and the figures are illustrative.\n3. The semantic consistency loss is reasonable, but I do not think this is significantly novel. \n\nWeaknesses:\n1. Although “the key aim of XGAN is to learn a joint meaningful and semantically consistent embedding”, the experiments are actually devoted to the qualitative style transfer only. A possible experiment design for evaluating “the key aim of XGAN” may be the facial attribute prediction. The CartoonSet contains attribute labels but the authors may need collect such labels for the VGG-face set.\n2. Only one baseline is considered in the style transfer experiments. Both CycleGAN and UNIT are very competitive methods and would be better be included in the comparison. \n3. The “many-to-many” is ambiguous. Style transfer in general is not a one-to-one or many-to-one mapping. It is not necessary to stress the many-to-many property of the proposed new task, i.e., semantic style transfer. \n\nThe CartoonSet dataset and the new task, which is called semantic style transfer between two domains, are nice contributions of this paper. In terms of technical contributions, it is not significant to have the X-shaped GAN or the straightforward semantic consistency loss. The experiments are somehow mismatched with the claimed aim of the paper. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]}},{"tddate":null,"ddate":null,"tmdate":1512222703484,"tcdate":1511738825026,"number":1,"cdate":1511738825026,"id":"rybCT6Olf","invitation":"ICLR.cc/2018/Conference/-/Paper626/Official_Review","forum":"rkWN3g-AZ","replyto":"rkWN3g-AZ","signatures":["ICLR.cc/2018/Conference/Paper626/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Due to limited novelty, lack of clarity in presentation, and poor experimental validation, the reviewer recommends rejecting the paper.","rating":"3: Clear rejection","review":"\n\n- Lack of novelty\n\nThe paper has very limited novelty since the proposed method is a straightforward combination of two prior works on the same topic (unpair/unsupervised image translation or cross-domain image generation) where the two prior works are the DTN work [a] and the UNIT [b] work. To be more precise, the proposed method utilizes the weight-sharing design for enforcing the shared latent space constraint proposed in the UNIT work [b] and the feature consistency loss term for ensuring common embedding in the DTN work [a] for solving the ill-posed unpaired/unsupervised image-to-image translation problem. Since the ideas are already published in the prior work, the paper does not contribute additional knowledge to the problem. \n\nIn addition, the combination is done in a careless manner. First of all, the paper proposes jointly minimizing the common embedding loss [a] and the domain adversarial loss [b]. However, minimizing the common embedding loss [a] also results in minimizing the domain adversarial loss [c]. This can be easily seen as when the embeddings are the same, no discriminators can tell them apart. This suggests that the paper fails to see the connection and blindly put the two things together. Moreover, given the generators, minimizing the common embedding loss also results in minimizing the cycle-consistency loss [d]. As the UNIT work [b] utilize both the weight-sharing constraint and cycle-consistency loss, the proposed method becomes a close variant to the UNIT work [b].\n\n- Poor experimental verification\n\nThe paper only shows visualization results on translating frontal face images to cartoon images in the resolution of 64x64. This is apparently short as compared to the experimental validations done in several prior works [a,b,d]. In the CycleGAN work [d], the results are shown on several translation tasks (picture to painting, horse to zebra, map to image, and different scenarios) in a resolution of 256x256. In the UNIT work [b], the results are shown in various street scene (sunny to rainy, day to night, winter to summer, synthetic to real) and animal portraits (cat species and dog breeds) where the resolution is up to 640x480. In the DTN [a] and UNIT [b] work, promising domain adaptation results (SVHN to MNIST) are reported. Due to the shortage of results, the credibility of the paper is damaged. \n\n- Lack of clarity in presentation\n\nThe paper tends to introduces new key words for existing one. For example, the \"semantic style transfer\" is exactly the unpaired/unsupervised image-to-image translation or cross-domain image generation. It is not clear why the paper needs to introduce the new keyword. Also, the Coupled GAN work [e] is the first work that utilizes both weight-sharing (shared latent space assumption) and GAN for unpaired/unsupervised image-to-image translation. It is unfortunately that the paper fails to refer to this closely related prior work.\n\n[a] Yaniv Taigman, Adam Polyak, Lior Wolf \"Unsupervised Cross-Domain Image Generation\", ICLR 2017\n\n[b] Ming-Yu Liu, Thomas Breuel, Jan Kautz \"Unsupervised Image-to-Image Translation Networks\", NIPS 2017 \n\n[c] YaroslavGanin et al. \"Domain-adversarial Training of Neural Networks\" JMLR 2016\n\n[d] Jun-Yan Zhu, Taesung Park, Philip Isola, and Alexei A. Efros \"Unpaired Image-to-Image Translation Using Cycle-consistent Adversarial Networks\" ICCV 2017\n\n[e] Ming-Yu Liu, Oncel Tuzle \"Coupled Generative Adversarial Networks\", NIPS 2016","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]}},{"tddate":null,"ddate":null,"tmdate":1510670465777,"tcdate":1510670465777,"number":1,"cdate":1510670465777,"id":"HkcKlK_1f","invitation":"ICLR.cc/2018/Conference/-/Paper626/Official_Comment","forum":"rkWN3g-AZ","replyto":"rJ8xTtx1f","signatures":["ICLR.cc/2018/Conference/Paper626/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper626/Authors"],"content":{"title":"Clarification on 'many-to-many'","comment":"Thank you for the question. To clarify, the *tasks* we consider are many-to-many in the sense there is no pre-defined one-to-one mapping between the domains, eg one face maps to many possible cartoons and vice-versa. However, the face/cartoon-to-latent-space mapping is many-to-one. We indeed only report results from deterministic models, which can be trivially extended to be conditional to a noise vector as well. An example of how this can be done is outlined in the CVPR 2017 PixelDA paper: Unsupervised Pixel-level Domain Adaptation with GANs by Bousmalis et al. In this and other papers, they found that introducing such noise does not affect the quality of the generated samples."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]}},{"tddate":null,"ddate":null,"tmdate":1510149358496,"tcdate":1510149358496,"number":1,"cdate":1510149358496,"id":"rJ8xTtx1f","invitation":"ICLR.cc/2018/Conference/-/Paper626/Public_Comment","forum":"rkWN3g-AZ","replyto":"rkWN3g-AZ","signatures":["~R_Devon_Hjelm1"],"readers":["everyone"],"writers":["~R_Devon_Hjelm1"],"content":{"title":"Many to many?","comment":"It's unclear how this model is many-to-many. The mappings are deterministic as far as I can tell, no?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]}},{"tddate":null,"ddate":null,"tmdate":1509739194171,"tcdate":1509129257219,"number":626,"cdate":1509739191499,"id":"rkWN3g-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkWN3g-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings","abstract":"Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions.  We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose will also be released as a new benchmark for semantic style transfer.","pdf":"/pdf/41c88b2dabfd3920efcc3a59d0a0499cd72ede5c.pdf","TL;DR":"XGAN is an unsupervised model for feature-level image-to-image translation applied to semantic style transfer problems such as the face-to-cartoon task, for which we introduce a new dataset.","paperhash":"anonymous|xgan_unsupervised_imagetoimage_translation_for_manytomany_mappings","_bibtex":"@article{\n  anonymous2018xgan:,\n  title={XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkWN3g-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper626/Authors"],"keywords":["unsupervised","gan","domain adaptation","style transfer","semantic","image translation","dataset"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}