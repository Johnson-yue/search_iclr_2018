{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222545210,"tcdate":1512000116480,"number":3,"cdate":1512000116480,"id":"HJ3OcT3gG","invitation":"ICLR.cc/2018/Conference/-/Paper1055/Official_Review","forum":"SyZI0GWCZ","replyto":"SyZI0GWCZ","signatures":["ICLR.cc/2018/Conference/Paper1055/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written paper with interesting results. ","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors propose a novel method for generating adversarial examples when the model is a black-box and we only have access to its decisions (and a positive example).  It iteratively takes steps along the decision boundary while trying to minimize the distance to the original positive example.\n\n\nPros:\n- Novel method that works under much stricter and more realistic assumptions.\n- Fairly thorough evaluation.\n- The paper is clearly written.\n\n\nCons:\n- Need a fair number of calls to generate a small perturbation.  Would like to see more analysis of this.\n- Attack works for making something outside the boundary (not X), but is less clear how to generate image to meet a specific classification (X).  3.2 attempts this slightly by using an image in the class, but is less clear for something like FaceID.\n- Unclear how often the images generated look reasonable.  Do different random initializations given different quality examples?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]}},{"tddate":null,"ddate":null,"tmdate":1512222545248,"tcdate":1511816984997,"number":2,"cdate":1511816984997,"id":"SyWXJWqgf","invitation":"ICLR.cc/2018/Conference/-/Paper1055/Official_Review","forum":"SyZI0GWCZ","replyto":"SyZI0GWCZ","signatures":["ICLR.cc/2018/Conference/Paper1055/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Old problem formulation, but interesting empirical results in a new application domain","rating":"7: Good paper, accept","review":"This is a nice paper proposing a simple but effective heuristic for generating adversarial examples from class labels with no gradient information or class probabilities. Highly relevant prior work was overlooked and there is no theoretical analysis, but I think this paper still makes a valuable contribution worth sharing with a broader audience.\n\nWhat this paper does well:\n- Suggests a type of attack that hasn't been applied to image classifiers\n- Proposes a simple heuristic method for performing this attack\n- Evaluates the attack on both benchmark neural networks and a commercial system \n\nProblems and limitations:\n\n1. No theoretical analysis. Under what conditions does the boundary attack succeed or fail? What geometry of the classification boundaries is necessary? How likely are those conditions to hold? Can we measure how well they hold on particular networks?\n\nSince there is no theoretically analysis, the evidence for effectiveness is entirely empirical. That weakens the paper and suggests an important area of future work, but I think the empirical evidence is sufficient to show that there's something interesting going. Not a fatal flaw.\n\n2. Poor framing. The paper frames the problem in terms of \"machine learning models\" in general (beginning with the first line of the abstract), but it only investigates image classification. There's no particular reason to believe that all machine learning algorithms will behave like convolutional neural network image classifiers. Thus, there's an implicit claim of generality that is not supported.\n\nThis is a presentation issue that is easily fixed. I suggest changing the title to reflect this, or at least revising the abstract and introduction to make the scope clearer.\n\nA minor presentation quibble/suggestion: \"adversarial\" is used in this paper to refer to any class that differs from the true class of the instance to be disguised. But an image of a dalmation that's labeled as a dalmation isn't adversarial -- it's just a different image that's labeled correctly. The adversarial process is about constructing something that will be mislabeled, exploiting some kind of weakness that doesn't show up on a natural distribution of inputs. I suggest rewording some of the mentions of adversarial.\n\n3. Ignorance of prior work. Finding deceptive inputs using only the classifier output has been done by Lowd and Meek (KDD 2005) for linear classifiers and Nelson et al. (AISTATS 2010, JMLR 2012) for convex-inducing classifiers. Both works include theoretical bounds on the number of queries required for near-optimal adversarial examples. Biggio et al. (ECML 2013) further propose training a surrogate classifier on similar training data, using the predictions of the target classifier to relabel the training data. In this way, decision information from the target model is used to help train a more similar surrogate, and then attacks can be transferred from the surrogate to the target.\n\nThus, \"decision-based attacks\" are not new, although the algorithm and experiments in this paper are. \n\n\nOverall, I think this paper makes a worthwhile contribution, but needs to revise the claims to match what's done in the paper and what's been done before.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]}},{"tddate":null,"ddate":null,"tmdate":1512222545288,"tcdate":1511812350607,"number":1,"cdate":1511812350607,"id":"BkP-T1qgM","invitation":"ICLR.cc/2018/Conference/-/Paper1055/Official_Review","forum":"SyZI0GWCZ","replyto":"SyZI0GWCZ","signatures":["ICLR.cc/2018/Conference/Paper1055/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Innovative new class of attacks on neural networks, which pose a threat to distillation defenses","rating":"7: Good paper, accept","review":"The authors identify a new security threat for deep learning: Decision-based adversarial attacks. This new class of attacks on deep learning systems requires from an attacker only the knowledge of class labels (previous attacks required more information, e.g., access to a gradient oracle). Unsurprisingly, since the attacker has so few information, such kind of attacks involves quite a lot trial and error. The authors propose one specific attack instance out of this class of attacks. It works as follows.\n\nFirst, an initial point outside of the benign region is guessed. Then multiple steps towards the decision boundary is taken, finally reaching the boundary (I am not sure about the precise implementation, but it seems not crucial; the author may please check whether their description of the algorithm is really reproducable). Then, in a nutshell, a random walk on a sphere centered around the original, benign point is performed, where after each step, the radius of the sphere is slightly reduced (drawing the point closer to the original point), if and only if the resulting point still is outside of the benign region.\n\nThe algorithm is evaluated on the following datasets: MNIST, CIFAR, VGG19, ResNet50, and InceptionV3.\n\nThe paper is rather well written and structured. The text was easy to follow. I suggest that a self-contained description of the problem setting (assumptions on attacker and defender; aim?) shall be added to the camera-ready version (being not familiar with the area, I had to read a couple of papers to get a feeling for the setting, before reviewing this paper). As in many DL papers these days, there really isn't any math in it worth a mention; so no reason here to say anything about mathematical soundness. The authors employ a reasonable evaluation criterion in their experiments: the median squared Euclidean distance between the original and adversarially modified data point. The results show consistent improvement for most data sets. \n\nIn summary, this is an innovative paper, proposing a new class of attacks that totally makes sense in my opinion. Apart from some minor weaknesses in the presentation that can be easily fixed for the camera ready, this is a nice, fresh paper, that might spur more attacks (and of course new defenses) from the new class of decision-based attacks. It is worth to note that the authors show that distillation is not a useful defense against such attacks, so we may expect follow-up proposing useful defenses against the new attack (which BTW is shown to be about a factor of 10 in terms of iterations more costly than the SOTA).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]}},{"tddate":null,"ddate":null,"tmdate":1511540975087,"tcdate":1511540975087,"number":1,"cdate":1511540975087,"id":"SJvgtpHeM","invitation":"ICLR.cc/2018/Conference/-/Paper1055/Official_Comment","forum":"SyZI0GWCZ","replyto":"SySvBkSJf","signatures":["ICLR.cc/2018/Conference/Paper1055/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1055/Authors"],"content":{"title":"Implementation will be released soon","comment":"Hi,\n\nthe Reproducibility Challenge is a great idea!\nWe will make our implementation available on GitHub as soon as we have cleaned it up in a way that makes it easy to apply.\n\nWe will also post a link here and update the paper accordingly once the double-blind review period has ended."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]}},{"tddate":null,"ddate":null,"tmdate":1510434140661,"tcdate":1510434140661,"number":1,"cdate":1510434140661,"id":"SySvBkSJf","invitation":"ICLR.cc/2018/Conference/-/Paper1055/Public_Comment","forum":"SyZI0GWCZ","replyto":"SyZI0GWCZ","signatures":["~Claude_Chen1"],"readers":["everyone"],"writers":["~Claude_Chen1"],"content":{"title":"Implementation Request ","comment":"Hi:\nWe are students at Carnegie Mellon University participating the ICLR 2018 Reproducibility Challenge. We find this paper quite interesting and were wondering if it is possible for you to share the implementation of boundary attack.\n\nMuch appreciated!\n\n\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]}},{"tddate":null,"ddate":null,"tmdate":1510092381688,"tcdate":1509138019073,"number":1055,"cdate":1510092360373,"id":"SyZI0GWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyZI0GWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models","abstract":"Most modern machine learning algorithms are vulnerable to minimal and almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most attack methods rely either on detailed model information (gradient-based attacks) or on confidence scores such as probability or logit outputs of the model (score-based attacks), neither of which are available in most real-world scenarios. In such cases, currently the only option are transfer-based attacks, however they need access to the training data, rely on cumbersome substitute models and are much easier to defend against. Here we introduce a new category of attacks - decision-based attacks - that rely solely on the final decision of a model. Decision-based attacks are therefore (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-based or score-based attacks. As a first effective representative of this category we introduce the Boundary Attack. This attack is conceptually simple, extremely flexible, requires close to no hyperparameter tuning and yields adversarial perturbations that are competitive with those produced by the best gradient-based attacks in standard computer vision applications. We demonstrate the attack on two real-world black-box models from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general thus open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the Boundary Attack is available at XXXXXX.","pdf":"/pdf/be866f82b1ca2c8155aae2c1f1db9998cf7745ef.pdf","TL;DR":"A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.","paperhash":"anonymous|decisionbased_adversarial_attacks_reliable_attacks_against_blackbox_machine_learning_models","_bibtex":"@article{\n  anonymous2018decision-based,\n  title={Decision-Based Adversarial Attacks: reliable attacks against Black-Box Machine Learning Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZI0GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1055/Authors"],"keywords":["adversarial attacks","adversarial examples","adversarials"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}