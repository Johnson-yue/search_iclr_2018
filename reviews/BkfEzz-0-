{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222767252,"tcdate":1511879053397,"number":2,"cdate":1511879053397,"id":"HJSqWxjez","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Review","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Requires major editing to be fit for publication. ","rating":"3: Clear rejection","review":"The authors consider a Neural Network where the neurons are treated as rational agents. In this model, the neurons must pay to observe the activation of neurons upstream. Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task).  \n\nWhile this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. It would require very major editing to be fit for publication. \n\nThe major problem with this paper is its clarity. See detailed comments below for problems just in the introduction. More generally, the paper is riddled with non sequiturs. The related work section mentions Generative Adversarial Nets. As far as I can tell, this paper has nothing to do with GANs. The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells. \n\nThere is also a general lack of attention to detail. For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task. This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}). It is never explained how this reward is allocated even in the authors’ own experiments. The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology. None of this is discussed. The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader’s understanding.  \n\nWhile this might be lost because of the clarity problems described above, the model itself is also never really motivated. Why is this an interesting problem? There are many ways to create rational incentives for neurons in a neural net. Why is paying to observe activations the one chosen here? The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is there an economic motivation? Is it just a different way to train a NN? \n\nDetailed Comments:\n“In the of NaaA” => remove “of”?\n“passing its activation to the unit as cost” => Unclear. What does this mean?\n“performance decreases if we naively consider units as agents” => Performance on what?\n“.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium“ => Perhaps, this should be rewritten as “Agents maximize their counterfactual return in equilibrium. \n“Subsequently, we present that learning counterfactual return leads the model to learning optimal topology” => Do you mean  “maximizing” instead of learning. Optimal with respect to what task?\n“pure-randomly” => “randomly”\n “with adaptive algorithm” => “with an adaptive algorithm”\n“the connection” => “connections”\n“In game theory, the outcome maximizing overall reward is named Pareto optimality.” => This is simply incorrect. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1512222767294,"tcdate":1511820853935,"number":1,"cdate":1511820853935,"id":"H12VRW9gM","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Review","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good work. Presentation needs further polishing.","rating":"7: Good paper, accept","review":"In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline.\n\nThe idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Many equations were unclear to me for similar reasons to the point I decided to only skim those parts. Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. Like, what does it mean when we say, “environment that the multi-agent system itself touches”?\n\nOverall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1511084185266,"tcdate":1511083528139,"number":3,"cdate":1511083528139,"id":"BJxz0TA1f","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"BkTpRSRkf","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Demo and sample code","comment":"Thank you for being interested in.\n\n> Demo\n\nYou can see our demo for the multi-agent Doom in the following URL.\nhttps://youtu.be/paT2n40QHOA\n\n> Implementation of Adaptive Dropconnect\n\nImplementation is easy because you can use it by just replace a layer.\n\nHere is a sample vanilla code w/o Adaptive DropConnect in pytorch:\n\n 1  class Net(nn.Module):                                                           \n 2     def __init__(self):                                           \n 3         super(Net, self).__init__()                                             \n 4         self.conv1 = nn.Conv2d(3, 10, kernel_size=5)                            \n 5         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)                           \n 6         self.fc1 = nn.Linear(500,100)                                 \n 7         self.fc2 = nn.Linear(100, 10)                                           \n 8                                                                                 \n 9     def forward(self, x):                                                       \n 10         x = F.relu(F.max_pool2d(self.conv1(x), 2))                              \n 11         x = F.relu(F.max_pool2d(self.conv2(x), 2))                              \n 12         x = x.view(-1, 500)                                                     \n 13         x = F.relu(self.fc1(x, training=self.training))                         \n 14         x = F.dropout(x, training=self.training)                                \n 15         x = self.fc2(x)                                                                                                                                                                                                                   \n 16         return F.log_softmax(x)   \n\nYou can turn on Adaptive DropConnnect by just replace a line with\n  6         self.fc1 = nn.Linear(500,100)                                 \n                                  vvv\n  6         self.fc1 = TradeLinear(500,100,eps=0.2)                                 \nTradeLinear is contained in our provided library, which supports Adaptive DropConnnect and NaaA."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1511075637330,"tcdate":1511074917224,"number":2,"cdate":1511074917224,"id":"BJTvnoCJG","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"SJPHs661G","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Re: External/internal","comment":"Good questions.\n\n> External/internal\n\nIn reinforcement learning (RL), there are two parts: an environment and an agent.\nIn \"deep\" RL, there is a neural network inside the agent as a value/policy function approximator.\nThe network contains bunch of units,\nand NaaA considers the network as a multi-agent system, and each unit as an agent.\nFrom perspective of the unit, the other units are considered as an environment.\nTo distinguish from the original environment, we call it an internal environment,\nand call the original environment an external environment.\n\nHere is a quick reference which can also be helpful.\n\n                   Environment for a unit         State for a unit                               Observation for a unit                   \n                   -----------------------------------   -------------------------------------------   ---------------------------------------\nExternal    original environment            original state                                   original observation            \nInternal     other units                              activation of all the other units   activation of allocated units\nBoth           -                                                -                                                         be used to predict o_{ijt}\n\n                   Reward per unit                     Total reward over units\n                   -----------------------------------   -------------------------------------------------------------\nExternal    original reward                       total original reward (designer's objective)\nInternal     revenue from units - cost     0\nBoth           units' objective                       total original reward (designer's objective)\n\n\n> Why not use simple neural network\n\nSuppose AIs had ego. That is, they maximize not total reward but their own reward in a multi-agent system.\nAlthough recent works such as CommNet supposed cooperate setting, say, all the agents have obtain total reward R,\nif the agents were selfish, there would be no incentive to cooperate, and hence they would not communicate each other.\nThe problem is known as social dilemma (e.g., prisoner's dilemma), and leads the overall reward.\nNaaA enables us to design such a multi-agent setting.\nAlso, NaaA can be used multi-agent setting in which the agents are made by other people."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1511050949046,"tcdate":1511050949046,"number":3,"cdate":1511050949046,"id":"BkTpRSRkf","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["~Luis_Garcia1"],"readers":["everyone"],"writers":["~Luis_Garcia1"],"content":{"title":"Cool work!","comment":"The method is general and that makes it widely applicable for many problems.\nI hope that the design concept will be a new basis of studies such as GAN.\n \nI found very challenging trying to find alternative AI patterns or routines\nbased on the cooperation of two AIs. I would like to see this happening more often in videogames.\n Are there any demo videos?\n \nHowever, your idea, Adaptive Dropconnect seems to be complicated to Implement.\nHow can we implement it?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1511017279511,"tcdate":1511017279511,"number":2,"cdate":1511017279511,"id":"SJPHs661G","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"External/internal","comment":"Why do you just divide the environment into two types: external/internal?\nThere is also another way to simply use neural network."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1510403363870,"tcdate":1510403327633,"number":1,"cdate":1510403327633,"id":"rkdWpPEJz","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"ByRdDLV1z","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Thank you for being interested in our work","comment":"\n1) The objective function is return (discounted cumulative reward). That is,\n                  Σ_{t=0}^T [ γ^t R_t^{ex} ],\n    where R_{ex,t} := Σ_i R_{it}^{ex} is overall reward from the external environment.\n\n   > POMDP/MDP\n   The actual problem we want to solve is POMDP.\n   However, we extended it to POSG, multi-agent problem, because we consider bunch of neurons as agents.\n   That's why we distinguished it external/internal environment in the paper.\n\n2) Yes, we used DQN-like architecture (Q-learning with neural net and experience replay) to predict counterfactual return of j for i  at t o_{ijt}. The detail is as below.\n     - The input is a state s_{it}, a coupled vector made of an external state, input vector, and parameter (weight and bias).\n     - The output is Q-value Q(s_{it}, g_{ijt}), where g_{ijt} \\in {0, 1} is allocation. Hence, there are 2 |N_i^{in}| Q-values per unit, where |N_i^{in}| is number of j's (indices of connected units from a unit v_i).\n     - o_{ijt} is calculated from a pair of scalars from the output: Q(s_{it}, 1) - Q(s_{it}, 0).\n     - The model made of one layer, but also deeper architectures can be introduced.\n\n3) ViZDoom partially supports multi-agent environment, but it does not supports communication among the agents.\n    So, we extended it with writing the original code which supports communication.\n\n> def of (s_it^ex, \\tilde{x}, \\theta_i)\n\nThe coupled vectors are designed as a state to predict Q-values for o_{ijt}. \nHere is the definition of the each notation.\n- s_it^ex: external state\n- \\tilde{x}: the predicted input vector from limited information. \\tilde{x} := x * g + \\bar{x} * (1-g). \n   \\bar{x} is mean value of x.\n- \\theta_i: the parameter of v_i. For example, weight and bias for linear unit.\nPlease also see our answer (2) in this post."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1510397814415,"tcdate":1510397814415,"number":1,"cdate":1510397814415,"id":"ByRdDLV1z","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["~Xin_Yang1"],"readers":["everyone"],"writers":["~Xin_Yang1"],"content":{"title":"Interesting paper. I have three questions","comment":"Very interesting paper. It shows a novel framework to consider all the units as agents.\nEven though the problem setting is challenging, the paper solved it by converting it into a scheme of counterfactual return maximization using an elegant trick from auction theory.\n\nNonetheless, I have several questions about the paper.\n1. What is the objective function?  While the author states the problem is POSG, I guess the problem is POMDP/MDP since the paper introduced a Doom-based environment as the experiment. I'm not sure to what the algorithm want to maximize actually.\n2. I'm unsure how to predict o_it actually. Though it seems to use Q-learn according to the paper, I want you to provide detail information of the architecture.\n3. As I guess ViZDoom is a single-agent platform, how do you realize the multi-agent setting?  I mean, are there some special implementations?\n\nThere are minor comments which may improve your paper:\n - Definition of R and \\pi is missing. I supposed they are a reward function and a policy. \n - Provide definition of (s_it^ex, \\tilde x, \\theta_i) in line 12, algo 1."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]}},{"tddate":null,"ddate":null,"tmdate":1509739103087,"tcdate":1509134889628,"number":787,"cdate":1509739100430,"id":"BkfEzz-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neuron as an Agent","abstract":"We propose Neuron as an Agent (NaaA) as a novel framework for reinforcement learning (RL), and explain its optimization method.\nNaaA incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.\nFirst, showing optimization of NaaA, this report describes the negative result that the performance decreases if we naively consider the units as agents.\nTo resolve that difficulty, we introduce a mechanism from game theory.\nAs a theoretical result, we demonstrate that the agent obeys the system to maximize its counterfactual return as the Nash equilibrium of the mechanism.\nSubsequently, we show that learning counterfactual returns leads the model to learning optimal topology among units.\nWe propose adaptive dropconnect, a natural extension of dropconnect.\nFinally, we confirm that optimization with the framework of NaaA leads to better performance of RL, with numerical experiments.\nSpecifically, we use a single-agent environment from Open AI gym, and a multi-agent environment from ViZDoom.","pdf":"/pdf/1bf6d8a770aeeb57805cce67cd3347a151112957.pdf","TL;DR":"Neuron as an Agent (NaaA) incorporates all neural network units as agents and optimizes the reward distribution as a multi-agent RL problem.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Reinforcement Learning","Multi-agent","Adaptive DropConnect"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}