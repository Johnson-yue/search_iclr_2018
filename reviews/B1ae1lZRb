{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222682108,"tcdate":1512099183027,"number":3,"cdate":1512099183027,"id":"Byv_pHRlM","invitation":"ICLR.cc/2018/Conference/-/Paper540/Official_Review","forum":"B1ae1lZRb","replyto":"B1ae1lZRb","signatures":["ICLR.cc/2018/Conference/Paper540/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good paper","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation.\nScheme A consists of training a high precision teacher jointly with a low precision student. Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode.\n\nReview:\nThe paper is well written. The experiments are clear and the three different schemes provide good analytical insights.\nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\nComments:\nTensorflow citation is missing.\nConclusion is short and a few directions for future research would have been useful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy","abstract":"Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems -- the models (often deep networks or wide networks or both) are compute and memory intensive. Low precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low precision networks can be significantly improved by using knowledge distillation techniques. We call our approach Apprentice and show state-of-the-art accuracies using ternary precision and 4-bit precision for many variants of ResNet architecture on ImageNet dataset. We study three schemes in which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.","pdf":"/pdf/9e49c6b54183fa35744c16b608d3a632788bbb9b.pdf","TL;DR":"We show that knowledge transfer techniques can improve the accuracy of low precision networks and set new state-of-the-art accuracy for ternary and 4-bits precision. ","paperhash":"anonymous|apprentice_using_knowledge_distillation_techniques_to_improve_lowprecision_network_accuracy","_bibtex":"@article{\n  anonymous2018apprentice:,\n  title={Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ae1lZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper540/Authors"],"keywords":["Ternary","4-bits","low precision","knowledge distillation","knowledge transfer","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222682149,"tcdate":1511782798917,"number":2,"cdate":1511782798917,"id":"rkPqK_tef","invitation":"ICLR.cc/2018/Conference/-/Paper540/Official_Review","forum":"B1ae1lZRb","replyto":"B1ae1lZRb","signatures":["ICLR.cc/2018/Conference/Paper540/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting results but very limited contribution","rating":"5: Marginally below acceptance threshold","review":"The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network. Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly. The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.\n\nI found the paper interesting but the contribution seems quite limited.\n\nPros:\n1. The paper is well written and easy to read.\n2. The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally.\n\nCons:\n1. The name Apprentice seems a bit confusing with apprenticeship learning.\n2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights).\n3. It is unclear how the proposed method outperforms other methods based on fine-tuning. It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy","abstract":"Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems -- the models (often deep networks or wide networks or both) are compute and memory intensive. Low precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low precision networks can be significantly improved by using knowledge distillation techniques. We call our approach Apprentice and show state-of-the-art accuracies using ternary precision and 4-bit precision for many variants of ResNet architecture on ImageNet dataset. We study three schemes in which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.","pdf":"/pdf/9e49c6b54183fa35744c16b608d3a632788bbb9b.pdf","TL;DR":"We show that knowledge transfer techniques can improve the accuracy of low precision networks and set new state-of-the-art accuracy for ternary and 4-bits precision. ","paperhash":"anonymous|apprentice_using_knowledge_distillation_techniques_to_improve_lowprecision_network_accuracy","_bibtex":"@article{\n  anonymous2018apprentice:,\n  title={Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ae1lZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper540/Authors"],"keywords":["Ternary","4-bits","low precision","knowledge distillation","knowledge transfer","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222682187,"tcdate":1511480235171,"number":1,"cdate":1511480235171,"id":"B1QhiA4eG","invitation":"ICLR.cc/2018/Conference/-/Paper540/Official_Review","forum":"B1ae1lZRb","replyto":"B1ae1lZRb","signatures":["ICLR.cc/2018/Conference/Paper540/AnonReviewer1"],"readers":["everyone"],"content":{"title":"speed & memory gains, transferability","rating":"7: Good paper, accept","review":"The authors investigate knowledge distillation as a way to learn low precision networks. They propose three training schemes to train a low precision student network from a teacher network. They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\nPros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\nCons:\n(-) The gap with full precision models is still large \n(-) Transferability of the learned low precision models to other tasks is not discussed\n\nThe authors tackle a very important problem, the one of learning low precision models without comprosiming performance. For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks. One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3). As a matter of fact, under some scenarios performance even decreases. \n\nThe authors do not discuss the gains of their best low-precision regime in terms of computation and memory.\n\nFinally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k). How good are the low-precision models trained by the authors at transferring to other tasks? Is it possible to transfer student-teacher training practices to other tasks?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy","abstract":"Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems -- the models (often deep networks or wide networks or both) are compute and memory intensive. Low precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low precision networks can be significantly improved by using knowledge distillation techniques. We call our approach Apprentice and show state-of-the-art accuracies using ternary precision and 4-bit precision for many variants of ResNet architecture on ImageNet dataset. We study three schemes in which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.","pdf":"/pdf/9e49c6b54183fa35744c16b608d3a632788bbb9b.pdf","TL;DR":"We show that knowledge transfer techniques can improve the accuracy of low precision networks and set new state-of-the-art accuracy for ternary and 4-bits precision. ","paperhash":"anonymous|apprentice_using_knowledge_distillation_techniques_to_improve_lowprecision_network_accuracy","_bibtex":"@article{\n  anonymous2018apprentice:,\n  title={Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ae1lZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper540/Authors"],"keywords":["Ternary","4-bits","low precision","knowledge distillation","knowledge transfer","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739246804,"tcdate":1509125876698,"number":540,"cdate":1509739244142,"id":"B1ae1lZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1ae1lZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy","abstract":"Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems -- the models (often deep networks or wide networks or both) are compute and memory intensive. Low precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low precision networks can be significantly improved by using knowledge distillation techniques. We call our approach Apprentice and show state-of-the-art accuracies using ternary precision and 4-bit precision for many variants of ResNet architecture on ImageNet dataset. We study three schemes in which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.","pdf":"/pdf/9e49c6b54183fa35744c16b608d3a632788bbb9b.pdf","TL;DR":"We show that knowledge transfer techniques can improve the accuracy of low precision networks and set new state-of-the-art accuracy for ternary and 4-bits precision. ","paperhash":"anonymous|apprentice_using_knowledge_distillation_techniques_to_improve_lowprecision_network_accuracy","_bibtex":"@article{\n  anonymous2018apprentice:,\n  title={Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1ae1lZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper540/Authors"],"keywords":["Ternary","4-bits","low precision","knowledge distillation","knowledge transfer","model compression"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}