{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222565185,"tcdate":1511839980562,"number":3,"cdate":1511839980562,"id":"rJrltU5gf","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good work but much similarity with an existing work","rating":"5: Marginally below acceptance threshold","review":"This paper inherits the framework proposed by Han[1]. A pruning, weight sharing, quantization pipeline is refined at each stage. At the pruning stage, by taking into account difference in the distribution across the layers, this paper propose a dynamic threshold pruning, which partially avoids mistakenly pruning important connections. As for the weight sharing stage, this paper explores several ways to initialize the clustering method. The introduction of error tolerance gives us more fine-grained control over the compression process.\n\nHere are some issues to be paid attention to:\n\n1. The overall pipeline including the last two stage looks quite similar to Han[1]. Though different initialization methods are tested in this paper, final conclusion does not change.\n\n2. The dynamic threshold pruning seems to be very time-consuming. As indicated from the paper, only 42 iterations for MNIST and 32 iterations for Cityscapes are required. Whether these number works for each layer or total network should be clarified.\n\n3. Fig 7(a) says it's error rate while it plots accuracy rate.\n\n4. Experiments on popular network structure such as residual connection should be conducted, as they are widely used nowadays.\n\n\nReferences:\n[1] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015 \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.007 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/8a47c56c42d0929498eefac44df1df58b23c7e67.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222565222,"tcdate":1511790464992,"number":2,"cdate":1511790464992,"id":"BkYtPqKez","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer1"],"readers":["everyone"],"content":{"title":"iterative deep compression","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method for iteratively pruning redundant weights in deep networks. The method is primarily based on a 3-step pipeline to achieve this objective. These three steps consist of pruning, weight sharing and quantization. The authors demonstrate reduction in model size and number of parameters significantly while only undergoing minor decrease in accuracy.\n\nSome of the main points of concern are below :\n\n - Computational complexity - The proposed method of iterative pruning seems quite computationally expensive. In the conclusion, it is mentioned that it takes 35 days of training for MNIST. This seems extremely high, and given this, it is unclear if there is much benefit in further reduction in model sizes and parameters (by the proposed method) than those obtained by existing method such as Han etal.\n\n - The novelty in the paper is quite limited and is mainly based on combining existing methods for pruning, weight sharing and quantization. The main difference from existing method seems to be the inclusion of layerwise threshold for weight pruning instead of using a single global threshold.\n\n - The results shown in Table 2 do not indicate much difference in terms of number of parameters between the proposed method and that of Han etal. For instance, the number of overall remaining parameters is 6.5% for the proposed method versus 8% for Deep Compression. As a result, the impact of the proposed method seems quite limited. \n\n - The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.007 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/8a47c56c42d0929498eefac44df1df58b23c7e67.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222565260,"tcdate":1511739796237,"number":1,"cdate":1511739796237,"id":"Syn5-0_lM","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Pruning NNs with layer-dependent parametrization, combined with other approaches for space reduction (weight sharing, quantization) significantly decrease space requirements without hurting accuracy. However, this comes at the cost of significant pruning time.","rating":"6: Marginally above acceptance threshold","review":"quality: this paper is of good quality\nclarity: this paper is very clear\noriginality: this paper combines original ideas with existing approaches for pruning to obtain dramatic space reduction in NN parameters.\nsignificance: this paper seems significant.\n\nPROS\n- a new approach to sparsifying that considers different thresholds for each layer\n- a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task.\n- Very interesting and extensive experiments that validate the reasoning behind the described approach, with a detailed analysis of each step of the algorithm.\n\nCONS\n- Pruning time. Although the authors argue that the pruning algorithm is not prohibitive, I would argue that >1 month to prune LeNet-5 for MNIST is certainly daunting in many settings. It would benefit the experimental section to use another dataset than MNIST (e.g. CIFAR-10) for the image recognition experiment.\n- It is unclear whether this approach will always work well; for some neural nets, the currently used sparsification method (thresholding) may not perform well, leading to very little final sparsification to maintain good performance.\n- The search for the optimal sparsity in each level seems akin to a brute-force search. Although possibly inevitable, it would be valuable to discuss whether or not this approach can be refined.\n\nMain questions\n- You mention removing \"unimportant and redundant weights\" in the pruning step; in this case, do unimportant and redundant have the same meaning (smaller than a given threshold), or does redundancy have another meaning (e.g. (Mariet, Sra, 2016))?\n- Algorithm 1 finds the best sparsity for a given layer that maintains a certain accuracy. Have you tried using a binary search for the best sparsity instead of simply decreasing the sparsity by 1% at each step? If there is a simple correlation between sparsity and accuracy, that might be faster; if there isn't (which would be believable given the complexity of neural nets), it would be valuable to confirm this with an experiment.\n- Have you tried other pruning methods than thresholding to decide on the optimal sparsity in each layer?\n- Could you please report the final accuracy of both models in Table 2?\n\nNitpicks:\n- paragraph break in page 4 would be helpful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.007 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/8a47c56c42d0929498eefac44df1df58b23c7e67.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739472619,"tcdate":1509011097527,"number":122,"cdate":1509739469966,"id":"SJZsR7kCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.007 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/8a47c56c42d0929498eefac44df1df58b23c7e67.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}