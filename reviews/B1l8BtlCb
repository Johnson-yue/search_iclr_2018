{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222621809,"tcdate":1511955553224,"number":3,"cdate":1511955553224,"id":"rJKwhzhxM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Fast inference for transformer nmt","rating":"6: Marginally above acceptance threshold","review":"This paper can be seen as an extension of the paper \"attention is all you need\" that will be published at nips in a few weeks (at the time I write this review). \n\nThe goal here is to make the target sentence generation non auto regressive. The authors propose to introduce a set of latent variables to represent the fertility of each source words. The number of target words can be then derived and they're all predicted in parallel.\n\nThe idea is interesting and trendy. However, the paper is not really stand alone. A lot of tricks are stacked to reduce the performance degradation. However, they're sometimes to briefly described to be understood by most readers. \n\nThe training process looks highly elaborate with a lot of hyper parameters. Maybe you could comment on this. \n\nFor instance, the use fertility supervision during training could be better motivated and explained. Your choice of IBM 2 is wired since it doesn't include fertility. Why not IBM 4, for instance ? How you use IBM model for supervision. This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1512222621846,"tcdate":1511824553213,"number":2,"cdate":1511824553213,"id":"B1Zh3McgM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review of \"Non-autoregressive neural machine translation\"","rating":"7: Good paper, accept","review":"This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models). The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described). The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search. \n\nOverall, this is an interesting paper. It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores. Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant.  While the Ro->En results are good, this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more. Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1512222621882,"tcdate":1511784943295,"number":1,"cdate":1511784943295,"id":"HJ-eMYYxz","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting work","rating":"7: Good paper, accept","review":"This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs. To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations. For training, apply knowledge distilation for better training followed by fine tuning by reinforce. Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.\n\nThe motivation is clear and proposed methods are very sound. Experiments are carried out very carefully.\n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency. I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough.\n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment.\n\n- What is the impact of the external word aligner quality? For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner. \n\n- The positional attention is rather unclear and it would be better to revise it. Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1511299081549,"tcdate":1511299081549,"number":1,"cdate":1511299081549,"id":"Sk-GdGflM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Comment","forum":"B1l8BtlCb","replyto":"SJ7QvnZlM","signatures":["ICLR.cc/2018/Conference/Paper335/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper335/Authors"],"content":{"title":"Thanks!","comment":"You're right about the first error; that's a typo. For the second point, the fertility sequence is [2, 0, 1] because our analysis (and the network) counts the period/full stop as a third source/target token."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1511274267393,"tcdate":1511274267393,"number":2,"cdate":1511274267393,"id":"SJ7QvnZlM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Public_Comment","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["~Ozan_Caglayan1"],"readers":["everyone"],"writers":["~Ozan_Caglayan1"],"content":{"title":"Two small glitches","comment":"Page 3, Paragraph 1: \"The factorization by length introduced .... first and third property but not the **first.**\"\nPage 6,  4.1.: Why is the fertility sequence [2,0,1] ? If I understood fertility correctly, I think it should have been [2,0] since number of tokens in the source sentence is 2."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1509739358259,"tcdate":1509098824198,"number":335,"cdate":1509739355600,"id":"B1l8BtlCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}