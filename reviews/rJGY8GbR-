{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222795950,"tcdate":1512086676931,"number":2,"cdate":1512086676931,"id":"SJTc3MAgf","invitation":"ICLR.cc/2018/Conference/-/Paper848/Official_Review","forum":"rJGY8GbR-","replyto":"rJGY8GbR-","signatures":["ICLR.cc/2018/Conference/Paper848/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Difficult to follow for someone not familiar with all the terminologies used in deep nets. ","rating":"5: Marginally below acceptance threshold","review":"The authors study mean field theory for deep neural nets. \n\nTo the best of my knowledge we do not have a good understanding of mean field theory for neural networks and this paper  and some references therein are starting to address some of it. \n\nHowever, my concern about the paper is in readability. I am very familiar with the literature on mean field theory but less so on deep nets. I found it difficult to follow many parts because the authors assume that the reader will have the knowledge of all the terminology in the paper, which there is a lot of. ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion","abstract":"\tA recent line of work has studied the statistical properties of neural networks to great success from a {\\it mean field theory} perspective, making and verifying very precise predictions of neural network behavior and test time performance.\n\tIn this paper, we build upon these previous works to explore two methods for taming the behaviors of random residual networks (with only fully connected layers and no batchnorm).\n\n\tThe first method is {\\it width variation (WV)}, i.e. varying the widths of layers as a function of depth.\n\tWe show that width decay reduces gradient explosion without affecting the mean forward dynamics of the random network.\n\t\n\tThe second method is {\\it variance variation (VV)}, i.e. changing the initialization variances of weights and biases over depth.\n\tWe show VV, used appropriately, can reduce gradient explosion of tanh and ReLU resnets from $\\exp(\\Theta(\\sqrt L))$ and $\\exp(\\Theta(L))$ respectively to constant $\\Theta(1)$.\n\tA complete phase-diagram is derived for how variance decay affects different dynamics, such as those of gradient and activation norms.\n\tIn particular, we show the existence of many phase transitions where these dynamics switch between exponential, polynomial, logarithmic, and even constant behaviors.\n\t\n\tUsing the obtained mean field theory, we are able to track surprisingly well how VV at initialization time affects training and test time performance on MNIST after a set number of epochs: the level set of test/train set accuracies coincide with the level sets of certain gradient norms.","pdf":"/pdf/5d708130e210d723741937f51a0ee2cbc266e0ca.pdf","TL;DR":"By setting the width or the initialization variance of each layer differently, we can actually subdue gradient explosion problems in residual networks (with fully connected layers and no batchnorm). A mathematical theory is developed that not only tells you how to do it, but also surprisingly is able to predict, after you apply such tricks, how fast your network trains to achieve a certain test set performance. This is some black magic stuff, and it's called \"Deep Mean Field Theory.\"","paperhash":"anonymous|deep_mean_field_theory_variance_and_width_variation_by_layer_as_methods_to_control_gradient_explosion","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJGY8GbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper848/Authors"],"keywords":["mean field","dynamics","residual network","variance variation","width variation","initialization"]}},{"tddate":null,"ddate":null,"tmdate":1512222795996,"tcdate":1511857487458,"number":1,"cdate":1511857487458,"id":"rkDLp95lG","invitation":"ICLR.cc/2018/Conference/-/Paper848/Official_Review","forum":"rJGY8GbR-","replyto":"rJGY8GbR-","signatures":["ICLR.cc/2018/Conference/Paper848/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice addition to the mean-field-theory subfield","rating":"7: Good paper, accept","review":"This paper further develops the research program using mean field theory to predict generalization performance of deep neural networks. As with all recent mean-field papers, the main query here is to what extent the assumptions (Axioms 1+2, which basically define the asymptotic parameters of interest to be the quantities defined in Sec. 2.; and also the fully connected residual structure of the network) apply in practice. This is answered using the same empirical standard as in [Yang and Schoenholz, Schoenholz et al.], i.e. showing that the dynamics of initialization predict generalization behavior on MNIST according to theory.\n\nAs with the earlier papers in this recent program, the paper is notation-heavy but generally written well, though there is some overreliance on the readers' knowledge of previous work, for instance in presenting the evidence as above. Try as I might, I cannot find a detailed explanation of the color scale for the important Fig. 4. A small notation issue: the current Hebrew letter for the gradient quantity does not go with the other Greek letters and is typographically poor choice because of underlining, etc.). Also, several of the citations should be fixed to reflect peer-reviewed publication of Arxiv papers. I was not able to review all the proofs, but what I checked was sound. Finally, the techniques of WV and VV would be more applicable if it were not for the very tenuous relationship between gradient explosion and performance, which should be mentioned more than the one time it appears in the paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion","abstract":"\tA recent line of work has studied the statistical properties of neural networks to great success from a {\\it mean field theory} perspective, making and verifying very precise predictions of neural network behavior and test time performance.\n\tIn this paper, we build upon these previous works to explore two methods for taming the behaviors of random residual networks (with only fully connected layers and no batchnorm).\n\n\tThe first method is {\\it width variation (WV)}, i.e. varying the widths of layers as a function of depth.\n\tWe show that width decay reduces gradient explosion without affecting the mean forward dynamics of the random network.\n\t\n\tThe second method is {\\it variance variation (VV)}, i.e. changing the initialization variances of weights and biases over depth.\n\tWe show VV, used appropriately, can reduce gradient explosion of tanh and ReLU resnets from $\\exp(\\Theta(\\sqrt L))$ and $\\exp(\\Theta(L))$ respectively to constant $\\Theta(1)$.\n\tA complete phase-diagram is derived for how variance decay affects different dynamics, such as those of gradient and activation norms.\n\tIn particular, we show the existence of many phase transitions where these dynamics switch between exponential, polynomial, logarithmic, and even constant behaviors.\n\t\n\tUsing the obtained mean field theory, we are able to track surprisingly well how VV at initialization time affects training and test time performance on MNIST after a set number of epochs: the level set of test/train set accuracies coincide with the level sets of certain gradient norms.","pdf":"/pdf/5d708130e210d723741937f51a0ee2cbc266e0ca.pdf","TL;DR":"By setting the width or the initialization variance of each layer differently, we can actually subdue gradient explosion problems in residual networks (with fully connected layers and no batchnorm). A mathematical theory is developed that not only tells you how to do it, but also surprisingly is able to predict, after you apply such tricks, how fast your network trains to achieve a certain test set performance. This is some black magic stuff, and it's called \"Deep Mean Field Theory.\"","paperhash":"anonymous|deep_mean_field_theory_variance_and_width_variation_by_layer_as_methods_to_control_gradient_explosion","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJGY8GbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper848/Authors"],"keywords":["mean field","dynamics","residual network","variance variation","width variation","initialization"]}},{"tddate":null,"ddate":null,"tmdate":1509739068300,"tcdate":1509135993584,"number":848,"cdate":1509739065627,"id":"rJGY8GbR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJGY8GbR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion","abstract":"\tA recent line of work has studied the statistical properties of neural networks to great success from a {\\it mean field theory} perspective, making and verifying very precise predictions of neural network behavior and test time performance.\n\tIn this paper, we build upon these previous works to explore two methods for taming the behaviors of random residual networks (with only fully connected layers and no batchnorm).\n\n\tThe first method is {\\it width variation (WV)}, i.e. varying the widths of layers as a function of depth.\n\tWe show that width decay reduces gradient explosion without affecting the mean forward dynamics of the random network.\n\t\n\tThe second method is {\\it variance variation (VV)}, i.e. changing the initialization variances of weights and biases over depth.\n\tWe show VV, used appropriately, can reduce gradient explosion of tanh and ReLU resnets from $\\exp(\\Theta(\\sqrt L))$ and $\\exp(\\Theta(L))$ respectively to constant $\\Theta(1)$.\n\tA complete phase-diagram is derived for how variance decay affects different dynamics, such as those of gradient and activation norms.\n\tIn particular, we show the existence of many phase transitions where these dynamics switch between exponential, polynomial, logarithmic, and even constant behaviors.\n\t\n\tUsing the obtained mean field theory, we are able to track surprisingly well how VV at initialization time affects training and test time performance on MNIST after a set number of epochs: the level set of test/train set accuracies coincide with the level sets of certain gradient norms.","pdf":"/pdf/5d708130e210d723741937f51a0ee2cbc266e0ca.pdf","TL;DR":"By setting the width or the initialization variance of each layer differently, we can actually subdue gradient explosion problems in residual networks (with fully connected layers and no batchnorm). A mathematical theory is developed that not only tells you how to do it, but also surprisingly is able to predict, after you apply such tricks, how fast your network trains to achieve a certain test set performance. This is some black magic stuff, and it's called \"Deep Mean Field Theory.\"","paperhash":"anonymous|deep_mean_field_theory_variance_and_width_variation_by_layer_as_methods_to_control_gradient_explosion","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJGY8GbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper848/Authors"],"keywords":["mean field","dynamics","residual network","variance variation","width variation","initialization"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}