{"notes":[{"tddate":null,"ddate":null,"tmdate":1512256330392,"tcdate":1512255784157,"number":1,"cdate":1512255784157,"id":"S1g4W3x-f","invitation":"ICLR.cc/2018/Conference/-/Paper672/Public_Comment","forum":"BJJLHbb0-","replyto":"BJJLHbb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some related works are not cited","comment":"This work obviously ignored several important related previous work from the speech recognition community. All these works use GMMs model features produced by a bottle-neck feature extractor (trained in a way sort of similar to an autoencoder), and jointly trained the feature extractor & GMMs. For example,\n\n1. M.Paulik,\"Lattice-based training of bottleneck feature extraction neural networks\", Proc. Interspeech 2013.\nwhich trains GMMs using EM and bottle-neck features using SGD in an interleaved fashion.\n\n2. E. Variani, E. McDermott, and G. Heigold, \"A Gaussian mixture model layer jointly optimized with discriminative features within a deep neural network architecture\", ICASSP 2015.\n3. C. Zhang and P.C. Woodland, \"Joint optimisation of tandem systems using Gaussian mixture density neural network discriminative sequence training\", ICASSP 2017.\nThese two papers jointly trained GMMs and their bottle-neck features using SGD and different criterion. \n\n4. Z. Tuuske, M. Sundermeyer, R. Schluuter, and H. Ney, \"Integrating Gaussian mixtures into deep neural networks: Softmax layer with hidden variables\", ICASSP 2015.\n5. Z. Tuuske, P. Golik, R. Schluuter, and H. Ney, \"Speaker adaptive joint training of Gaussian mixture models and bottleneck features\", ASRU 2015.\nThese two papers trained a log-linear mixture model (seen as an extension to softmax) together with the features.\n\nThe major difference between the neural network architectures in this paper and those cited above (esp. those in 2 & 3) is perhaps mainly whether to use a separate network to estimate the membership of the sample. It is not certain if such a membership estimation network is useful given sufficient computational power.  \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","pdf":"/pdf/805b7f64d746b79938780ebd29379996e5152c2d.pdf","TL;DR":"An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.","paperhash":"anonymous|deep_autoencoding_gaussian_mixture_model_for_unsupervised_anomaly_detection","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJLHbb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper672/Authors"],"keywords":["Density estimation","unsupervised anomaly detection","high-dimensional data","Deep autoencoder","Gaussian mixture modeling","latent low-dimensional space"]}},{"tddate":null,"ddate":null,"tmdate":1512222717795,"tcdate":1511978533123,"number":3,"cdate":1511978533123,"id":"B1aQ8_2ef","invitation":"ICLR.cc/2018/Conference/-/Paper672/Official_Review","forum":"BJJLHbb0-","replyto":"BJJLHbb0-","signatures":["ICLR.cc/2018/Conference/Paper672/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Strong anomaly detection by end-to-end architecture that predicts GMM parameters with tailored loss function","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space. The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples.\n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture. The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training.\n\nThe paper demonstrates improvements in a number of public datasets. Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field. Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection.\n\nCriticisms\n\nBased on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method. Little to no detail about these features is included. Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled.\n\nSince this is so important to the results, more analysis would be helpful. Why did the choices that were made in the paper yield this success? How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results?\n\nQuality\n\nThis paper does not set out to produce a novel network architecture. Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM. This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","pdf":"/pdf/805b7f64d746b79938780ebd29379996e5152c2d.pdf","TL;DR":"An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.","paperhash":"anonymous|deep_autoencoding_gaussian_mixture_model_for_unsupervised_anomaly_detection","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJLHbb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper672/Authors"],"keywords":["Density estimation","unsupervised anomaly detection","high-dimensional data","Deep autoencoder","Gaussian mixture modeling","latent low-dimensional space"]}},{"tddate":null,"ddate":null,"tmdate":1512222717836,"tcdate":1511791457275,"number":2,"cdate":1511791457275,"id":"r1tvocFgf","invitation":"ICLR.cc/2018/Conference/-/Paper672/Official_Review","forum":"BJJLHbb0-","replyto":"BJJLHbb0-","signatures":["ICLR.cc/2018/Conference/Paper672/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clear paper on joint optimization of dimension reduction and density estimation, more work needed on the justification and the experimental setup.","rating":"6: Marginally above acceptance threshold","review":"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized. The paper is rigorous and ideas are clearly stated. The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods. My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation. I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.\n\n1. The framework uses the class information, i.e., “only data samples from the normal class are used for training”, but it is still considered unsupervised. Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information. I would like to see a plot of the sample energy as a function of the number of data points. Is there an elbow that indicates the threshold cut? Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 – LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data).\n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network? \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results?\n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model). Those approaches should at least be discussed in the related work, if not compared against.\n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better. Could you provide a comparison with EM?\n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant? Does it well describe the new space? Do you normalize the features (the output of the dimension reduction and the representation error are quite different)? Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians.\n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets.\n8. The authors mention that “we can clearly see from Fig. 3a that DAGMM is able to well separate ...” - it is not clear to me, it does look better than the other ones, but not clear. If there is a clear separation from a different view, show that one instead. We don't need the same view for all methods. \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them. This seems very drastic!\n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2. “DAGMM preserves the key information of an input sample” - what does key information mean?\n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE. Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined. They are the best in terms of precision.\n4. Is the error in Table 2 averaged over multiple runs? If yes, how many?\n\nQuality – The paper is thoroughly written, and the ideas are clearly presented. It can be further improved as mentioned in the comments.\n\nClarity – The paper is very well written with clear statements, a pleasure to read.\n\nOriginality – Fairly original, but it still needs some work to justify it better.\n\nSignificance – Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","pdf":"/pdf/805b7f64d746b79938780ebd29379996e5152c2d.pdf","TL;DR":"An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.","paperhash":"anonymous|deep_autoencoding_gaussian_mixture_model_for_unsupervised_anomaly_detection","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJLHbb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper672/Authors"],"keywords":["Density estimation","unsupervised anomaly detection","high-dimensional data","Deep autoencoder","Gaussian mixture modeling","latent low-dimensional space"]}},{"tddate":null,"ddate":null,"tmdate":1512222717873,"tcdate":1511732778342,"number":1,"cdate":1511732778342,"id":"S1f48huxz","invitation":"ICLR.cc/2018/Conference/-/Paper672/Official_Review","forum":"BJJLHbb0-","replyto":"BJJLHbb0-","signatures":["ICLR.cc/2018/Conference/Paper672/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents a joint deep learning framework for dimension reduction-clustering, leads to competitive anomaly detection","rating":"8: Top 50% of accepted papers, clear accept","review":"1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection\n2. It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods\n3. Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process? can it be a trainable parameter?\n4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","pdf":"/pdf/805b7f64d746b79938780ebd29379996e5152c2d.pdf","TL;DR":"An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.","paperhash":"anonymous|deep_autoencoding_gaussian_mixture_model_for_unsupervised_anomaly_detection","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJLHbb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper672/Authors"],"keywords":["Density estimation","unsupervised anomaly detection","high-dimensional data","Deep autoencoder","Gaussian mixture modeling","latent low-dimensional space"]}},{"tddate":null,"ddate":null,"tmdate":1509739168697,"tcdate":1509131591295,"number":672,"cdate":1509739166032,"id":"BJJLHbb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJJLHbb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection","abstract":"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","pdf":"/pdf/805b7f64d746b79938780ebd29379996e5152c2d.pdf","TL;DR":"An end-to-end trained deep neural network that leverages Gaussian Mixture Modeling to perform density estimation and unsupervised anomaly detection in a low-dimensional space learned by deep autoencoder.","paperhash":"anonymous|deep_autoencoding_gaussian_mixture_model_for_unsupervised_anomaly_detection","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJLHbb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper672/Authors"],"keywords":["Density estimation","unsupervised anomaly detection","high-dimensional data","Deep autoencoder","Gaussian mixture modeling","latent low-dimensional space"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}