{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222577161,"tcdate":1512175263720,"number":3,"cdate":1512175263720,"id":"HkJjUOybz","invitation":"ICLR.cc/2018/Conference/-/Paper150/Official_Review","forum":"ByWBpcJAZ","replyto":"ByWBpcJAZ","signatures":["ICLR.cc/2018/Conference/Paper150/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review of \"A cluster-to-cluster framework for neural machine translation\"","rating":"6: Marginally above acceptance threshold","review":"This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences. The idea of using similar sentence pairs as cluster-to-cluster translation is interesting. \nThe experimental results seem promising, but the presentation can be improved. Some parts of the paper are hard to read.\n \n \nMajor\n1. What is the model/baseline in Tables 3, 4, 5?\n2. What is the intuition in adding target cluster entropy in Eq. 3?\n3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from? Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster?\n4. In section 4.2, the name cluster is a bit confusing with the one in section 3.1. What's the relationship? The symbols C(Y*) and C(X*) are not used afterward.\n5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by \"efficient\"?\n6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs?\n7. What's the reward used in the experiments?\n8. In the Monte-Carlo sampling, how many pairs are sampled? \n \nMinor \n1. In Table 1, where is sigma defined?\n2. The notation D for a dataset in Section 3.3 is confusing with D in system D.\n3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified.\n4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y).\n5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10.\n6. Some recent results on WMT DE-EN are missing, such ashttps://arxiv.org/abs/1706.03762.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A cluster-to-cluster framework for neural machine translation","abstract":"The quality of a machine translation system depends largely on the availability of sizable parallel corpora. For the recently popular Neural Machine Translation (NMT) framework, data sparsity problem can become even more severe. With large amount of tunable parameters, the NMT model may overfit to the existing language pairs while failing to understand the general diversity in language. In this paper, we advocate to broadcast every sentence pair as two groups of similar sentences to incorporate more diversity in language expressions, which we name as parallel cluster. Then we define a more general cluster-to-cluster correspondence score and train our model to maximize this score. Since direct maximization is difficult, we derive its lower-bound as our surrogate objective, which is found to generalize point-point Maximum Likelihood Estimation (MLE) and point-to-cluster Reward Augmented Maximum Likelihood (RAML) algorithms as special cases. Based on this novel objective function, we delineate four potential systems to realize our cluster-to-cluster framework and test their performances in three recognized translation tasks, each task with forward and reverse translation directions. In each of the six experiments, our proposed four parallel systems have consistently proved to outperform the MLE baseline, RL (Reinforcement Learning) and RAML systems significantly. Finally, we have performed case study to empirically analyze the strength of the cluster-to-cluster NMT framework. ","pdf":"/pdf/e5353dc88792a5f479111808d79f9ac87a6d2a31.pdf","TL;DR":"We invent a novel cluster-to-cluster framework for NMT training, which can better understand the both source and target language diversity.","paperhash":"anonymous|a_clustertocluster_framework_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018a,\n  title={A cluster-to-cluster framework for neural machine translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByWBpcJAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper150/Authors"],"keywords":["Natural Language Processing","Machine Translation","Deep Learning","Data Augmentation"]}},{"tddate":null,"ddate":null,"tmdate":1512222577205,"tcdate":1511909718650,"number":2,"cdate":1511909718650,"id":"r11DYPolG","invitation":"ICLR.cc/2018/Conference/-/Paper150/Official_Review","forum":"ByWBpcJAZ","replyto":"ByWBpcJAZ","signatures":["ICLR.cc/2018/Conference/Paper150/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review of \"A cluster-to-cluster framework for neural machine translation\"","rating":"3: Clear rejection","review":"This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples. The paper claims superior results using the described method.\n\nOverall, there are a few problems with the paper. 1) The arguments for using clusters instead of single sentences are questionable.  The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that). It is not clear why that is not used or at least compared to the method presented. 2) The writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (What is 'Golden Centroid Augmentation\" or \"Model Centroid Augmentation\"? The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input. The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy. There are lots more examples of unclear statements in this paper -- it should be heavily improved. 3) Section 3 and 4 are very hard/impossible to understand, it is not clear how the formulas help the reader to better understand the concept in any way. 5) The results presented in this paper given the complexity of the method are just not great -- for example, WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU (Google's Neural Machine Translation System) -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that? .  6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A cluster-to-cluster framework for neural machine translation","abstract":"The quality of a machine translation system depends largely on the availability of sizable parallel corpora. For the recently popular Neural Machine Translation (NMT) framework, data sparsity problem can become even more severe. With large amount of tunable parameters, the NMT model may overfit to the existing language pairs while failing to understand the general diversity in language. In this paper, we advocate to broadcast every sentence pair as two groups of similar sentences to incorporate more diversity in language expressions, which we name as parallel cluster. Then we define a more general cluster-to-cluster correspondence score and train our model to maximize this score. Since direct maximization is difficult, we derive its lower-bound as our surrogate objective, which is found to generalize point-point Maximum Likelihood Estimation (MLE) and point-to-cluster Reward Augmented Maximum Likelihood (RAML) algorithms as special cases. Based on this novel objective function, we delineate four potential systems to realize our cluster-to-cluster framework and test their performances in three recognized translation tasks, each task with forward and reverse translation directions. In each of the six experiments, our proposed four parallel systems have consistently proved to outperform the MLE baseline, RL (Reinforcement Learning) and RAML systems significantly. Finally, we have performed case study to empirically analyze the strength of the cluster-to-cluster NMT framework. ","pdf":"/pdf/e5353dc88792a5f479111808d79f9ac87a6d2a31.pdf","TL;DR":"We invent a novel cluster-to-cluster framework for NMT training, which can better understand the both source and target language diversity.","paperhash":"anonymous|a_clustertocluster_framework_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018a,\n  title={A cluster-to-cluster framework for neural machine translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByWBpcJAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper150/Authors"],"keywords":["Natural Language Processing","Machine Translation","Deep Learning","Data Augmentation"]}},{"tddate":null,"ddate":null,"tmdate":1512222577256,"tcdate":1511788030072,"number":1,"cdate":1511788030072,"id":"r1UZAYKgM","invitation":"ICLR.cc/2018/Conference/-/Paper150/Official_Review","forum":"ByWBpcJAZ","replyto":"ByWBpcJAZ","signatures":["ICLR.cc/2018/Conference/Paper150/AnonReviewer2"],"readers":["everyone"],"content":{"title":"\"cluster-to-cluster\" is unclear","rating":"5: Marginally below acceptance threshold","review":"This work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster, which represents a set of similar data point, e.g., sentence, according to a metric. By employing the cluster, this work propose a joint source/target modeling by varying how sampling is performed, e.g., draw independently or conditionally, and how the cluster are constructed, e.g., model-wise or non-model. Experiments on German/English and Chinese/English show gains over other reinforcement learning methods.\n\nIf my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and model-based scoring. However one of the problems of this paper is clarity.\n\n- The notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology, e.g., clustering. Also, cluster-to-cluster might not fit well. \n\n- It is hard to map System-{ABCD} to the underlying proposed methods described in Table 2. Also, I feel algorithm 1 is spurious given that it merely switch by systems. Probably better to introduce branch for key methods, parallel sampling/ translation broadcasting and inadaptive or adaptive model.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A cluster-to-cluster framework for neural machine translation","abstract":"The quality of a machine translation system depends largely on the availability of sizable parallel corpora. For the recently popular Neural Machine Translation (NMT) framework, data sparsity problem can become even more severe. With large amount of tunable parameters, the NMT model may overfit to the existing language pairs while failing to understand the general diversity in language. In this paper, we advocate to broadcast every sentence pair as two groups of similar sentences to incorporate more diversity in language expressions, which we name as parallel cluster. Then we define a more general cluster-to-cluster correspondence score and train our model to maximize this score. Since direct maximization is difficult, we derive its lower-bound as our surrogate objective, which is found to generalize point-point Maximum Likelihood Estimation (MLE) and point-to-cluster Reward Augmented Maximum Likelihood (RAML) algorithms as special cases. Based on this novel objective function, we delineate four potential systems to realize our cluster-to-cluster framework and test their performances in three recognized translation tasks, each task with forward and reverse translation directions. In each of the six experiments, our proposed four parallel systems have consistently proved to outperform the MLE baseline, RL (Reinforcement Learning) and RAML systems significantly. Finally, we have performed case study to empirically analyze the strength of the cluster-to-cluster NMT framework. ","pdf":"/pdf/e5353dc88792a5f479111808d79f9ac87a6d2a31.pdf","TL;DR":"We invent a novel cluster-to-cluster framework for NMT training, which can better understand the both source and target language diversity.","paperhash":"anonymous|a_clustertocluster_framework_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018a,\n  title={A cluster-to-cluster framework for neural machine translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByWBpcJAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper150/Authors"],"keywords":["Natural Language Processing","Machine Translation","Deep Learning","Data Augmentation"]}},{"tddate":null,"ddate":null,"tmdate":1509739458694,"tcdate":1509039417037,"number":150,"cdate":1509739456038,"id":"ByWBpcJAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByWBpcJAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A cluster-to-cluster framework for neural machine translation","abstract":"The quality of a machine translation system depends largely on the availability of sizable parallel corpora. For the recently popular Neural Machine Translation (NMT) framework, data sparsity problem can become even more severe. With large amount of tunable parameters, the NMT model may overfit to the existing language pairs while failing to understand the general diversity in language. In this paper, we advocate to broadcast every sentence pair as two groups of similar sentences to incorporate more diversity in language expressions, which we name as parallel cluster. Then we define a more general cluster-to-cluster correspondence score and train our model to maximize this score. Since direct maximization is difficult, we derive its lower-bound as our surrogate objective, which is found to generalize point-point Maximum Likelihood Estimation (MLE) and point-to-cluster Reward Augmented Maximum Likelihood (RAML) algorithms as special cases. Based on this novel objective function, we delineate four potential systems to realize our cluster-to-cluster framework and test their performances in three recognized translation tasks, each task with forward and reverse translation directions. In each of the six experiments, our proposed four parallel systems have consistently proved to outperform the MLE baseline, RL (Reinforcement Learning) and RAML systems significantly. Finally, we have performed case study to empirically analyze the strength of the cluster-to-cluster NMT framework. ","pdf":"/pdf/e5353dc88792a5f479111808d79f9ac87a6d2a31.pdf","TL;DR":"We invent a novel cluster-to-cluster framework for NMT training, which can better understand the both source and target language diversity.","paperhash":"anonymous|a_clustertocluster_framework_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018a,\n  title={A cluster-to-cluster framework for neural machine translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByWBpcJAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper150/Authors"],"keywords":["Natural Language Processing","Machine Translation","Deep Learning","Data Augmentation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}