{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222577702,"tcdate":1512168376891,"number":3,"cdate":1512168376891,"id":"S1-ToUJWz","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. \n\nThe experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too.\n\nOverall the paper is a descent one, but with limited novelty. I am a weak reject","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/ff157069fcbc59b8cf0f7504e5a20939f8815912.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222577739,"tcdate":1511899630648,"number":2,"cdate":1511899630648,"id":"B1wlzrslf","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded.\n\nThe experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model. \nRelated to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%. Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy. Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN.\n\nOTHER COMMENTS:\n\n- the list of contributions is a bit strange. It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function. To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/ff157069fcbc59b8cf0f7504e5a20939f8815912.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222577817,"tcdate":1511751927638,"number":1,"cdate":1511751927638,"id":"BkgW-ZteG","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Is the idea strong enough?","rating":"5: Marginally below acceptance threshold","review":"The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then,  PACT is combined with quantizing the activations. \n\nThe proposed technique sounds. The performance improvement is expected and validated by experiments. \n\nBut I am not sure if the novelty is strong enough for an ICLR paper. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/ff157069fcbc59b8cf0f7504e5a20939f8815912.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1509739456678,"tcdate":1509040242005,"number":154,"cdate":1509739454019,"id":"By5ugjyCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/ff157069fcbc59b8cf0f7504e5a20939f8815912.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}