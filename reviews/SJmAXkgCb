{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222588230,"tcdate":1511911099731,"number":3,"cdate":1511911099731,"id":"BJ46Rwjez","invitation":"ICLR.cc/2018/Conference/-/Paper200/Official_Review","forum":"SJmAXkgCb","replyto":"SJmAXkgCb","signatures":["ICLR.cc/2018/Conference/Paper200/AnonReviewer1"],"readers":["everyone"],"content":{"title":"high compression rate for marginal accuracy loss, but approach requires specialized architecture","rating":"4: Ok but not good enough - rejection","review":"Strengths:\n- Unlike most previous approaches that suffer from significant accuracy drops for good feature map compression, the proposed method achieves reductions in feature map sizes of 1 order of magnitude at effectively no loss in accuracy.\n- Technical approach relates closely to some of the prior approaches (e.g., Iandola et al. 2016) but can be viewed as learning the quantization rather than relying on a predefined one.\n- Good results on both large-scale classification and object detection.\n- Technical approach is clearly presented.\n\nWeaknesses:\n- The primary downside is that the approach requires a specialized architecture to work well (all experiments are done with SqueezeNets). Thus, the approach is less general than prior work, which can be applied to arbitrary architectures.\n- From the experiments it is not fully clear what is the performance loss due to having to use the SqueezeNet architecture rather than state-of-the-art models. For example, for the image categorization experiment, the comparative baselines are for AlexNet and NIN, which are outdated and do not represent the state-of-the-art in this field. The object detection experiments are based on a variant of Faster R-CNN where the VGG16 feature extractor is replaced with a SqueezeNet model. However, the drop in accuracy caused by this modification is not discussed in the paper and, in any case, there are now much better models for object detection than Faster R-CNN.\n- In my view the strengths of the approach would be more convincingly conveyed visually with a plot reporting accuracy versus memory usage, rather than by the many numerical tables in the paper.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DNN Feature Map Compression using Learned Representation over GF(2)","abstract":"In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architecture to the tasks of ImageNet classification and KITTI object detection. When combined with conventional methods, the conducted experiments show two orders of magnitude decrease in memory requirements while adding only bitwise computations.","pdf":"/pdf/72dfe008f4b2a1d67953c2527940db53fe39722a.pdf","TL;DR":"Feature map compression method that converts quantized activations into binary vectors followed by nonlinear dimensionality reduction layers embedded into a DNN","paperhash":"anonymous|dnn_feature_map_compression_using_learned_representation_over_gf2","_bibtex":"@article{\n  anonymous2018dnn,\n  title={DNN Feature Map Compression using Learned Representation over GF(2)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJmAXkgCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper200/Authors"],"keywords":["feature map","representation","compression","quantization","finite-field"]}},{"tddate":null,"ddate":null,"tmdate":1512222588269,"tcdate":1511867081566,"number":2,"cdate":1511867081566,"id":"SJG0Ga5ef","invitation":"ICLR.cc/2018/Conference/-/Paper200/Official_Review","forum":"SJmAXkgCb","replyto":"SJmAXkgCb","signatures":["ICLR.cc/2018/Conference/Paper200/AnonReviewer3"],"readers":["everyone"],"content":{"title":"DNN Feature Map Compression using Learned Representation","rating":"5: Marginally below acceptance threshold","review":"In order to compress DNN intermediate feature maps the authors covert fixed-point activations into vectors over the smallest finite field, the Galois field of two elements (GF(2)) and use nonlinear dimentionality reduction layers.\n\nThe paper reads well and the methods and experiments are generally described in sufficient detail.\n\nMy main concern with this paper and approach is the performance achieved. According to Table 1 and Table 2 there is a small accuracy benefit from using the proposed approach over the \"quantized\" SqueezeNet baseline. If I am weighing in the need to alter the network for the proposed approach in comparison with the \"quantized\" setting then, from practical point of view, I would prefer the later \"quantized\" approach.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DNN Feature Map Compression using Learned Representation over GF(2)","abstract":"In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architecture to the tasks of ImageNet classification and KITTI object detection. When combined with conventional methods, the conducted experiments show two orders of magnitude decrease in memory requirements while adding only bitwise computations.","pdf":"/pdf/72dfe008f4b2a1d67953c2527940db53fe39722a.pdf","TL;DR":"Feature map compression method that converts quantized activations into binary vectors followed by nonlinear dimensionality reduction layers embedded into a DNN","paperhash":"anonymous|dnn_feature_map_compression_using_learned_representation_over_gf2","_bibtex":"@article{\n  anonymous2018dnn,\n  title={DNN Feature Map Compression using Learned Representation over GF(2)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJmAXkgCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper200/Authors"],"keywords":["feature map","representation","compression","quantization","finite-field"]}},{"tddate":null,"ddate":null,"tmdate":1512222588305,"tcdate":1511534345075,"number":1,"cdate":1511534345075,"id":"rJbz1nrgM","invitation":"ICLR.cc/2018/Conference/-/Paper200/Official_Review","forum":"SJmAXkgCb","replyto":"SJmAXkgCb","signatures":["ICLR.cc/2018/Conference/Paper200/AnonReviewer2"],"readers":["everyone"],"content":{"title":"initial review","rating":"7: Good paper, accept","review":"The method of this paper minimizes the memory usage of the activation maps of a CNN. It starts from a representation where activations are compressed with a uniform scalar quantizer and fused to reduce intermediate memory usage. This looses some accuracy, so the contribution of the paper is to add a pair of convolution layers in the binary domain (GF(2)) that are trained to restore the lost precision. \n\nOverall, this paper seems to be a nice addition to the body of works on network compression. \n\n+ : interesting approach and effective results. \n\n+ : well related to the state of the art and good comparison with other works. \n\n- : somewhat incremental. Most of the claimed 100x compression is due to previous work.\n\n- : impact on runtime is not reported. Since there is a caffe implementation it would be interesting to have an additional column with the comparative execution speeds, even if only on CPU. I would expect the FP32 timings to be hard to beat, despite the claims that it uses only binary operations.\n\n- : the paper is sometimes difficult to understand (see below)\n\ndetailed comments: \n\nEquations (3)-(4) are difficult to understand. If I understand correctly, b just decomposes a \\hat{x} in {0..2^B-1} into its B bits \\tilda{x} \\in {0,1}^B, which can be then considered as an additional dimension in the activation map where \\hat{x} comes from. \n\nIt is not stated clearly whether P^l and R^l have binary weights. My understanding is that P^l has but R^l not.\n\n4.1 --> a discussion of the large mini-batch size (1024) could be useful. My understanding is that large mini-batches are required to use averaged gradients and get smooth updates. \n\nend of 4.1 --> unclear what \"equivalent bits\" means\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DNN Feature Map Compression using Learned Representation over GF(2)","abstract":"In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architecture to the tasks of ImageNet classification and KITTI object detection. When combined with conventional methods, the conducted experiments show two orders of magnitude decrease in memory requirements while adding only bitwise computations.","pdf":"/pdf/72dfe008f4b2a1d67953c2527940db53fe39722a.pdf","TL;DR":"Feature map compression method that converts quantized activations into binary vectors followed by nonlinear dimensionality reduction layers embedded into a DNN","paperhash":"anonymous|dnn_feature_map_compression_using_learned_representation_over_gf2","_bibtex":"@article{\n  anonymous2018dnn,\n  title={DNN Feature Map Compression using Learned Representation over GF(2)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJmAXkgCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper200/Authors"],"keywords":["feature map","representation","compression","quantization","finite-field"]}},{"tddate":null,"ddate":null,"tmdate":1509739433181,"tcdate":1509057483067,"number":200,"cdate":1509739430518,"id":"SJmAXkgCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJmAXkgCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DNN Feature Map Compression using Learned Representation over GF(2)","abstract":"In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architecture to the tasks of ImageNet classification and KITTI object detection. When combined with conventional methods, the conducted experiments show two orders of magnitude decrease in memory requirements while adding only bitwise computations.","pdf":"/pdf/72dfe008f4b2a1d67953c2527940db53fe39722a.pdf","TL;DR":"Feature map compression method that converts quantized activations into binary vectors followed by nonlinear dimensionality reduction layers embedded into a DNN","paperhash":"anonymous|dnn_feature_map_compression_using_learned_representation_over_gf2","_bibtex":"@article{\n  anonymous2018dnn,\n  title={DNN Feature Map Compression using Learned Representation over GF(2)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJmAXkgCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper200/Authors"],"keywords":["feature map","representation","compression","quantization","finite-field"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}