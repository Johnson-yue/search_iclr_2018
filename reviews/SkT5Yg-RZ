{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222701533,"tcdate":1511986229524,"number":3,"cdate":1511986229524,"id":"SkaVNc2gz","invitation":"ICLR.cc/2018/Conference/-/Paper612/Official_Review","forum":"SkT5Yg-RZ","replyto":"SkT5Yg-RZ","signatures":["ICLR.cc/2018/Conference/Paper612/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Compelling approach with solid results on a reasonable set of baselines","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper presents a method for learning a curriculum for reinforcement learning tasks.The approach revolves around splitting the personality of the agent into two parts. The first personality learns to generate goals for other personality for which the second agent is just barely capable--much in the same way a teacher always pushes just past the frontier of a student’s ability. The second personality attempts to achieve the objectives set by the first as well as achieve the original RL task.  \n\nThe novelty of the proposed method is introduction of a teacher that learns to generate a curriculum for the agent.The formulation is simple and elegant as the teacher is incentivised to widen the gap between bob but pays a price for the time it takes which balances the adversarial behavior. \n\nPrior and concurrent work on learning curriculum and intrinsic motivation in RL rely on GANs (e.g., automatic goal generation by Held et al.), adversarial agents (e.g., RARL by Pinto et al.), or algorithmic/heuristic methods (e.g., reverse curriculum by Florensa et al. and HER Andrychowicz et al.).  In the context of this work, the contribution is the insight that an agent can be learned to explore the immediate reachable space but that is just within the capabilities of the agent. HER and goal generation share the core insight on training to reach goals. However, HER does generate goals beyond the reachable it instead relies on training on existing reached states or explicitly consider the capabilities of the agent on reaching a goal. Goal generation while learning to sample from the achievable frontier does not ensure the goal is reachable and may not be as stable to train. \n\nAs noted by the authors the above mentioned prior work is closely related to the proposed approach. However, the paper only briefly mentions this corpus of work. A more thorough comparison with these techniques should be provided even if somewhat concurrent with the proposed method. The authors should consider additional experiments on the same domains of this prior work to contrast performance.\n\nQuestions:\nDo the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob? \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play","abstract":"We describe a simple scheme that allows an agent to learn about its\n  environment in an unsupervised manner. Our scheme pits two versions\n  of the same agent, Alice and Bob, against one another. Alice\n  proposes a task for Bob to complete; and then Bob attempts to\n  complete the task.  In this work we will focus on two kinds of environments: (nearly)\n  reversible environments and environments that can be reset.\n  Alice will ``propose'' the task by doing a sequence of actions and then\n  Bob must undo or repeat them, respectively.  Via an\n  appropriate reward structure, Alice and Bob automatically generate a\n  curriculum of exploration, enabling unsupervised training of the\n  agent. When Bob is deployed on an RL task within the environment, this\n  unsupervised training reduces the number of supervised episodes needed to\n  learn, and in some cases converges to a higher reward.","pdf":"/pdf/4dd88df14081add8282c0f416a701250337d83da.pdf","TL;DR":"Unsupervised learning for reinforcement learning using an automatic curriculum of self-play","paperhash":"anonymous|intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay","_bibtex":"@article{\n  anonymous2018intrinsic,\n  title={Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkT5Yg-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper612/Authors"],"keywords":["self-play","automatic curriculum","intrinsic motivation","unsupervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222701575,"tcdate":1511915756366,"number":2,"cdate":1511915756366,"id":"H14gZYsgG","invitation":"ICLR.cc/2018/Conference/-/Paper612/Official_Review","forum":"SkT5Yg-RZ","replyto":"SkT5Yg-RZ","signatures":["ICLR.cc/2018/Conference/Paper612/AnonReviewer2"],"readers":["everyone"],"content":{"title":"baseline","rating":"5: Marginally below acceptance threshold","review":"This paper proposes an interesting model of self-play where one agent learns to propose tasks that are easy for her but difficult for an opponent. This creates a moving target of self-play objectives and learning curriculum.\n\nThe idea is certainly elegant and clearly described. \nI don't really feel qualified to comment on the novelty, since this paper is somewhat out of my area of expertise, but I did notice that the authors' own description of Baranes and Oudeyer (2013) seems very close to the proposal in this paper. Given the existence of similar forms of self-play the key issue with paper I see is that there is no strong self-play baseline in the experimental evaluation. It is hard to tell whether this neat idea is really an improvement.\n\nIs progress guaranteed? Is it not possible for Alice to imemdiately find an easy task for her where Bob times out, gets no reward signal, and therefore is unable to learn anything? Then repeating that task will loop forever without progress. This suggests that the adversarial setting is quite brittle.\n\nI also find that the paper is a little light on the technical side.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play","abstract":"We describe a simple scheme that allows an agent to learn about its\n  environment in an unsupervised manner. Our scheme pits two versions\n  of the same agent, Alice and Bob, against one another. Alice\n  proposes a task for Bob to complete; and then Bob attempts to\n  complete the task.  In this work we will focus on two kinds of environments: (nearly)\n  reversible environments and environments that can be reset.\n  Alice will ``propose'' the task by doing a sequence of actions and then\n  Bob must undo or repeat them, respectively.  Via an\n  appropriate reward structure, Alice and Bob automatically generate a\n  curriculum of exploration, enabling unsupervised training of the\n  agent. When Bob is deployed on an RL task within the environment, this\n  unsupervised training reduces the number of supervised episodes needed to\n  learn, and in some cases converges to a higher reward.","pdf":"/pdf/4dd88df14081add8282c0f416a701250337d83da.pdf","TL;DR":"Unsupervised learning for reinforcement learning using an automatic curriculum of self-play","paperhash":"anonymous|intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay","_bibtex":"@article{\n  anonymous2018intrinsic,\n  title={Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkT5Yg-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper612/Authors"],"keywords":["self-play","automatic curriculum","intrinsic motivation","unsupervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222701618,"tcdate":1511820769302,"number":1,"cdate":1511820769302,"id":"S1Fy0bqlG","invitation":"ICLR.cc/2018/Conference/-/Paper612/Official_Review","forum":"SkT5Yg-RZ","replyto":"SkT5Yg-RZ","signatures":["ICLR.cc/2018/Conference/Paper612/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good work.","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors describe a new formulation for exploring the environment in an unsupervised way to aid a specific task later. Using two “minds”, Alice and Bob, where the former proposes increasingly difficult tasks and the latter tries to accomplish them as fast as possible, the learning agent Bob can later perform a given task faster having effectively learned the environment dynamics from playing the game with Alice. \n\nThe idea of unsupervised exploration has been visited before. However, the paper presents a novel way to frame the problem, and shows promising results on several tasks. The ideas are well-presented and further expounded in a systematic way. Furthermore, the crux of the proposal and simple and elegant yet leading to some very interesting results. My only complaint is that some of the finer implementation details seems to have been omitted. For example, the parameter update equation is section 4 is somewhat opaque and requires more discussion than the motivation presented in the preceding paragraph.\n\nTypos and grammatical errors: let assume (section 2.2), it is possible show (section 5).\n\nOverall, I think the paper presents a novel and unique idea that would be interesting to the wider research community. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play","abstract":"We describe a simple scheme that allows an agent to learn about its\n  environment in an unsupervised manner. Our scheme pits two versions\n  of the same agent, Alice and Bob, against one another. Alice\n  proposes a task for Bob to complete; and then Bob attempts to\n  complete the task.  In this work we will focus on two kinds of environments: (nearly)\n  reversible environments and environments that can be reset.\n  Alice will ``propose'' the task by doing a sequence of actions and then\n  Bob must undo or repeat them, respectively.  Via an\n  appropriate reward structure, Alice and Bob automatically generate a\n  curriculum of exploration, enabling unsupervised training of the\n  agent. When Bob is deployed on an RL task within the environment, this\n  unsupervised training reduces the number of supervised episodes needed to\n  learn, and in some cases converges to a higher reward.","pdf":"/pdf/4dd88df14081add8282c0f416a701250337d83da.pdf","TL;DR":"Unsupervised learning for reinforcement learning using an automatic curriculum of self-play","paperhash":"anonymous|intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay","_bibtex":"@article{\n  anonymous2018intrinsic,\n  title={Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkT5Yg-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper612/Authors"],"keywords":["self-play","automatic curriculum","intrinsic motivation","unsupervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739201372,"tcdate":1509128597405,"number":612,"cdate":1509739198650,"id":"SkT5Yg-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkT5Yg-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play","abstract":"We describe a simple scheme that allows an agent to learn about its\n  environment in an unsupervised manner. Our scheme pits two versions\n  of the same agent, Alice and Bob, against one another. Alice\n  proposes a task for Bob to complete; and then Bob attempts to\n  complete the task.  In this work we will focus on two kinds of environments: (nearly)\n  reversible environments and environments that can be reset.\n  Alice will ``propose'' the task by doing a sequence of actions and then\n  Bob must undo or repeat them, respectively.  Via an\n  appropriate reward structure, Alice and Bob automatically generate a\n  curriculum of exploration, enabling unsupervised training of the\n  agent. When Bob is deployed on an RL task within the environment, this\n  unsupervised training reduces the number of supervised episodes needed to\n  learn, and in some cases converges to a higher reward.","pdf":"/pdf/4dd88df14081add8282c0f416a701250337d83da.pdf","TL;DR":"Unsupervised learning for reinforcement learning using an automatic curriculum of self-play","paperhash":"anonymous|intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay","_bibtex":"@article{\n  anonymous2018intrinsic,\n  title={Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkT5Yg-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper612/Authors"],"keywords":["self-play","automatic curriculum","intrinsic motivation","unsupervised learning","reinforcement learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}