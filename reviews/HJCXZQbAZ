{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222557156,"tcdate":1511885236561,"number":3,"cdate":1511885236561,"id":"rk63KZixz","invitation":"ICLR.cc/2018/Conference/-/Paper1122/Official_Review","forum":"HJCXZQbAZ","replyto":"HJCXZQbAZ","signatures":["ICLR.cc/2018/Conference/Paper1122/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper introduces a novel method for modeling hierarchical data. The work builds on previous approaches, such as Vilnis and McCallum's Word2Gauss and Vendrov's Order Embeddings, to establish a partial order over probability densities via encapsulation, which allows it to model hierarchical information. The aim is to learn embeddings from supervised structured data, such as WordNet. The work also investigates various schemes for selecting negative samples. The evaluation consists of hypernym detection on WordNet and graded lexical entailment, in the shape of HyperLex. This is good work: it is well written, the experiments are thorough and the proposed method is original and works well.\n\nSection 3 could use some more signposting. Especially for 3.3 it would be good to explain (either at the beginning of section 3, or the beginning of section 3.3) why these measures matter and what is going to be done with them.\n\nIt's good that LEAR is mentioned and compared against, even though it was very recently published. Please do note that the authors' names are misspelled: Vuli\\'c not Vulic, Mrk\\v{s}i\\'c instead of Mrksic.\n\nIf I am not mistaken, the Vendrov WordNet test set is a set of positive pairs. I would like to see more details on how the evaluation is done here: presumably, the lower I set the threshold, the higher my score? Or am I missing something?\n\nIt would be useful to describe exactly the extent to which supervision is used - the method only needs positive and negative links, and does not require any additional order information (i.e., WordNet strictly contains more information than what is being used).\n\nI don't see what Socher et al. (2013) has to do with the loss in equation (7). Or did they invent the margin loss?\n\nWord2gauss also evaluates on similarity and relatedness datasets. Did you consider doing that here too?\n\n\"hypothesis proposed by Santus et al. which says\" is not a valid reference.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES","abstract":"By representing words with probability densities rather than point vectors, probabilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). For example, such embeddings trained on an unlabelled corpus can represent lexical entailment, where the learned distribution for a general concept such as “animal” can contain the distributions for more specific words such as “dog” or “cat”. However, for some words such as “mammal”, the entailment signal is often weak since “mammal” is usually not used in place of ”dog” or “cat” in natural sentences. In this paper, we develop the use of density representations to specifically model hierarchical data. We introduce simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our methods outperform the original methodology proposed by Vilnis & McCallum (2014) by a significant margin and provide state-of-the-art performance on the WORDNET hypernym relationship prediction task and the challenging HYPERLEX lexical entailment dataset.","pdf":"/pdf/a09f1ca6968a32ebc27f80d50c9cf7afcdeaaca5.pdf","paperhash":"anonymous|on_modeling_hierarchical_data_via_encapsulation_of_probability_densities","_bibtex":"@article{\n  anonymous2018on,\n  title={ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCXZQbAZ}\n}","keywords":["embeddings"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1122/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222557194,"tcdate":1511816166088,"number":2,"cdate":1511816166088,"id":"SyRynl9eM","invitation":"ICLR.cc/2018/Conference/-/Paper1122/Official_Review","forum":"HJCXZQbAZ","replyto":"HJCXZQbAZ","signatures":["ICLR.cc/2018/Conference/Paper1122/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Potentially a good paper, badly presented","rating":"6: Marginally above acceptance threshold","review":"The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one. The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data.\n+++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!)\n---cons: 1) Badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves\n\nCOMMENTS:\nThe introduction puzzled me:  the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t. their hyponyms), proposed a solution which seems not to directly solve this problem. So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem.\n\nThis aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset.\n\nHow the 4000 hypernyms have been selected? Why a diagonal covariance has been estimated, and not a full covariance one? \n\nn Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets. It shouldn't be, but the authors should spend a little on this.\n\nApart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out. \n\nAs a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES","abstract":"By representing words with probability densities rather than point vectors, probabilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). For example, such embeddings trained on an unlabelled corpus can represent lexical entailment, where the learned distribution for a general concept such as “animal” can contain the distributions for more specific words such as “dog” or “cat”. However, for some words such as “mammal”, the entailment signal is often weak since “mammal” is usually not used in place of ”dog” or “cat” in natural sentences. In this paper, we develop the use of density representations to specifically model hierarchical data. We introduce simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our methods outperform the original methodology proposed by Vilnis & McCallum (2014) by a significant margin and provide state-of-the-art performance on the WORDNET hypernym relationship prediction task and the challenging HYPERLEX lexical entailment dataset.","pdf":"/pdf/a09f1ca6968a32ebc27f80d50c9cf7afcdeaaca5.pdf","paperhash":"anonymous|on_modeling_hierarchical_data_via_encapsulation_of_probability_densities","_bibtex":"@article{\n  anonymous2018on,\n  title={ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCXZQbAZ}\n}","keywords":["embeddings"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1122/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222557239,"tcdate":1511594618375,"number":1,"cdate":1511594618375,"id":"HyGYccUxz","invitation":"ICLR.cc/2018/Conference/-/Paper1122/Official_Review","forum":"HJCXZQbAZ","replyto":"HJCXZQbAZ","signatures":["ICLR.cc/2018/Conference/Paper1122/AnonReviewer2"],"readers":["everyone"],"content":{"title":"KL is not symmetric but its directional aspect was not found to  be significant for lexical entailment","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks.Each word is represented  by a diagonal Gaussian and the KL divergence is used as a directional distance measure. if D(f||g) < gamma then the concept represented by f entails the concept represented by g.\n\nThe main technical difference of the present work compared from the  main prior work (Vendrov, 2015) is that in addition to mean vector representation they use here also a variance component. The main modeling challenge here to to define a  good directional measure that can be suitable for lexical entailment. in Vendrov work they defined a partial ordering. Here,  the KL is not symmetric but its directional aspect is not significant.\nFor example if we set all the variances to be a unit matrix than the KL is collapsed to be a simple symmetrical Euclidean distance. We can also see from Table 1 that if we replace KL by its symmetrical variant we get similar results. Hence, I was not convinced that the propose KL+Gaussian modeling is suitable for directional relations.\n\nThe paper also presents several methods for negative samplings and according to table 4 there is a lot of performance variability based on the method that is used for selecting negative sampling. I find this component of the proposed algorithm  very heuristic.\n\nTo summarize, I don't think there is enough interesting novelty in this paper. If the focus of the  paper is on  obtaining good entailment results, maybe an NLP conference can be a more suitable venue.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES","abstract":"By representing words with probability densities rather than point vectors, probabilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). For example, such embeddings trained on an unlabelled corpus can represent lexical entailment, where the learned distribution for a general concept such as “animal” can contain the distributions for more specific words such as “dog” or “cat”. However, for some words such as “mammal”, the entailment signal is often weak since “mammal” is usually not used in place of ”dog” or “cat” in natural sentences. In this paper, we develop the use of density representations to specifically model hierarchical data. We introduce simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our methods outperform the original methodology proposed by Vilnis & McCallum (2014) by a significant margin and provide state-of-the-art performance on the WORDNET hypernym relationship prediction task and the challenging HYPERLEX lexical entailment dataset.","pdf":"/pdf/a09f1ca6968a32ebc27f80d50c9cf7afcdeaaca5.pdf","paperhash":"anonymous|on_modeling_hierarchical_data_via_encapsulation_of_probability_densities","_bibtex":"@article{\n  anonymous2018on,\n  title={ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCXZQbAZ}\n}","keywords":["embeddings"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1122/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092380391,"tcdate":1509138733360,"number":1122,"cdate":1510092359732,"id":"HJCXZQbAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJCXZQbAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES","abstract":"By representing words with probability densities rather than point vectors, probabilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). For example, such embeddings trained on an unlabelled corpus can represent lexical entailment, where the learned distribution for a general concept such as “animal” can contain the distributions for more specific words such as “dog” or “cat”. However, for some words such as “mammal”, the entailment signal is often weak since “mammal” is usually not used in place of ”dog” or “cat” in natural sentences. In this paper, we develop the use of density representations to specifically model hierarchical data. We introduce simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our methods outperform the original methodology proposed by Vilnis & McCallum (2014) by a significant margin and provide state-of-the-art performance on the WORDNET hypernym relationship prediction task and the challenging HYPERLEX lexical entailment dataset.","pdf":"/pdf/a09f1ca6968a32ebc27f80d50c9cf7afcdeaaca5.pdf","paperhash":"anonymous|on_modeling_hierarchical_data_via_encapsulation_of_probability_densities","_bibtex":"@article{\n  anonymous2018on,\n  title={ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCXZQbAZ}\n}","keywords":["embeddings"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1122/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}