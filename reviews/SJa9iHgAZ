{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222604176,"tcdate":1511930694914,"number":3,"cdate":1511930694914,"id":"HJyUi3sez","invitation":"ICLR.cc/2018/Conference/-/Paper262/Official_Review","forum":"SJa9iHgAZ","replyto":"SJa9iHgAZ","signatures":["ICLR.cc/2018/Conference/Paper262/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"Residual Connections Encourage Iterative Inference\"","rating":"7: Good paper, accept","review":"\nThis paper shows that residual networks can be viewed as doing a sort of iterative inference, where each layer is trained to use its “nonlinear part” to push its values in the negative direction of the loss gradient.  The authors demonstrate this using a Taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference.  Overall the strength of this paper is that the main insight is quite interesting — though many people have informally thought of residual networks as having this interpretation — this paper is the first one to my knowledge to explain the intuition in a more precise way.  \n\nSome weaknesses of the paper on the other hand — some of the parts of the paper (e.g. on weight sharing) are only somewhat related to the main topic of the paper. In fact, the authors moved the connection to SGD to the appendix, which I thought would be *more* related.   Additionally, parts of the paper are not as clearly written as they could be and lack rigor.  This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly.  The explanation following is also handwavey despite claims to being formal.   \n\nSome other lower level thoughts:\n* Regarding weight sharing for residual layers, I don’t understand why we can draw the conclusion that the initial gradient explosion is responsible for the lower generalization capability of the model with shared weights.  Are there other papers in literature that have shown this connection?\n* The name “cosine loss” suggests that this function is actually being minimized by a training procedure, but it is just a value that is being plotted… perhaps just call it the cosine?\n* I recommend that the authors also check out Figurnov et al CVPR 2017 (\"Spatially Adaptive Computation Time for Residual Networks\") which proposes an “adaptive” version of ResNet based on the intuition of adaptive inference.\n* The plots in the later parts of the paper are quite small and hard to read.  They are also spaced together too tightly (horizontally), making it difficult to immediately see what each plot is supposed to represent via the y-axis label.\n* Finally, the citations need to be fixed (use \\citep{} instead of \\cite{})\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Connections Encourage Iterative Inference","abstract":"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem.","pdf":"/pdf/4b024f06e4073a1b4214a93eb09744816b9adf7a.pdf","TL;DR":"Residual connections really perform iterative inference","paperhash":"anonymous|residual_connections_encourage_iterative_inference","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Connections Encourage Iterative Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa9iHgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper262/Authors"],"keywords":["residual network","iterative inference","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222604214,"tcdate":1511872104536,"number":2,"cdate":1511872104536,"id":"HkeOU0qgf","invitation":"ICLR.cc/2018/Conference/-/Paper262/Official_Review","forum":"SJa9iHgAZ","replyto":"SJa9iHgAZ","signatures":["ICLR.cc/2018/Conference/Paper262/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Evidences are not solid enough","rating":"5: Marginally below acceptance threshold","review":"The author unveils some properties of the resnets, for example, the cosine loss and l2 ratio of the layers. \nI think the author should place more focus to study \"real\" iterative inference with shared parameters rather than analyzing original resnets.\n\nIn resnet without sharing parameters, it is quite ambiguous to say whether it is doing representation learning or iterative refinement.\n\n1. The cosine loss is not meaningful in the sense that the classification layer is trained on the output of the last residual block and fixed. Moving the classification layer to early layers will definitely result in accuracy loss. Even in non-residual network, we can always say that the vector h_{i+1} - h_i is refining h_i towards the negative gradient direction. The motivation of iterative inference would be to generate a feature that is easier to classify rather than to match the current fixed classifier. Thus the final classification layer should be retrained for every addition or removal of residual blocks.\n\n2. The l2 ratio. The l2 ratio is small for higher residual layers, I'm not sure how much this phenomenon can prove that resnet is actually doing iterative inference.\n\n3. In section 4.4 it is shown that unrolling the layers can improve the performance of the network. However, the same can be achieved by adding more unshared layers. I think the study should focus more on whether shared or unshared is better.\n\n4. Section 4.5 is a bit weak in experiments, my conclusion is that currently it is still limited by batch normalization and optimization, the evidence is still not strong enough to show that iterative inference is advantageous / disadvantageous.\n\nThe the above said, I think the more important thing is how we can benefit from iterative inference interpretation, which is relatively weak in this paper.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Connections Encourage Iterative Inference","abstract":"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem.","pdf":"/pdf/4b024f06e4073a1b4214a93eb09744816b9adf7a.pdf","TL;DR":"Residual connections really perform iterative inference","paperhash":"anonymous|residual_connections_encourage_iterative_inference","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Connections Encourage Iterative Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa9iHgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper262/Authors"],"keywords":["residual network","iterative inference","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222604253,"tcdate":1511669852151,"number":1,"cdate":1511669852151,"id":"H1EPgaweG","invitation":"ICLR.cc/2018/Conference/-/Paper262/Official_Review","forum":"SJa9iHgAZ","replyto":"SJa9iHgAZ","signatures":["ICLR.cc/2018/Conference/Paper262/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper investigates residual networks (ResNets) in an empirical way. The authors argue that shallow layers are responsible for learning important feature representations, while deeper layers focus on refining the features. They validate this point by performing a series of lesion study on ResNet.\n\nOverall, the experiments and discussions in the first part of Section 4.2 and 4.3 appears to be interesting, while other observations are not quite surprising. I have two questions:\n1)\tWhat is the different between the layer-dropping experiment in sec 4.2 and that in [Veit, et al, Residual networks are exponential ensembles of relatively shallow networks] ? What is the main point here? \n2)\tI don't quite understand the first paragraph of sec 4.5. Could you elaborate more on this?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Connections Encourage Iterative Inference","abstract":"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem.","pdf":"/pdf/4b024f06e4073a1b4214a93eb09744816b9adf7a.pdf","TL;DR":"Residual connections really perform iterative inference","paperhash":"anonymous|residual_connections_encourage_iterative_inference","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Connections Encourage Iterative Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa9iHgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper262/Authors"],"keywords":["residual network","iterative inference","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739398138,"tcdate":1509084052920,"number":262,"cdate":1509739395465,"id":"SJa9iHgAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJa9iHgAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Residual Connections Encourage Iterative Inference","abstract":"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem.","pdf":"/pdf/4b024f06e4073a1b4214a93eb09744816b9adf7a.pdf","TL;DR":"Residual connections really perform iterative inference","paperhash":"anonymous|residual_connections_encourage_iterative_inference","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Connections Encourage Iterative Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa9iHgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper262/Authors"],"keywords":["residual network","iterative inference","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}