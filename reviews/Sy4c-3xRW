{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222628456,"tcdate":1511813308157,"number":3,"cdate":1511813308157,"id":"ryl6gl5xM","invitation":"ICLR.cc/2018/Conference/-/Paper382/Official_Review","forum":"Sy4c-3xRW","replyto":"Sy4c-3xRW","signatures":["ICLR.cc/2018/Conference/Paper382/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Needs more clarity and a deterministic baseline","rating":"5: Marginally below acceptance threshold","review":"Pros\n- The proposed model is a nice way of multiplicatively combining two features :\n  one which determines which classes to pay attention to, and other that\nprovides useful features for discrimination.\n\n- The adaptive component seems to provide improvements for small dataset sizes\n  and large number of classes.\n\nCons\n- \"One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the\n  classification and the gradients are not back-propagated from it.\" : This does\nnot seem to be true. Even if the logits are zero, the class would have a\nnon-zero probability and would receive gradients. Do the authors mean\nexp(o_t(x;w)) = 0 ?\n\n- Related to the above, it should be clarified what is meant by dropping a\n  class. Is its logit set to zero or -\\infty ? Excluding a class from the\nsoftmax is equivalent to having a logit of -\\infty, not zero. However, from the\nequations in the paper it seems that the logit is set to zero. This would not\nresult in excluding the unit. The overall effect would just be to raise the\nmagnitude of logits across the entire softmax.\n\n- It seems that the model benefits from at least two separate effects - one is\n  the attention mechanism provided by the sigmoids, and the other is the\nstochasticity during training. Presently, it is not clear if only one of the\ncomponents is providing most of the benefits, or if both things are useful. It\nwould be great to compare this model to a non-stochastic one which just has the\nmultiplicative effects applied in a deterministic way (during both training and\ntesting).\n\n- The objective of the attention mechanism that sets the dropout mask seems to\n  be the same as the primary objective of classifying the input, and the\nattention mechanism is prevented from solving the task by adding an extra\nentropy regularization. It would be useful to explain more why this is needed.\nWould it not be fine if the attention mechanism did a perfect job of selecting\nthe class ?\n\nQuality\nThe paper makes relevant comparisons and is overall well-motivated. However,\nsome aspects of the paper can be improved by adding more explanations.\n\nClarity\nSome crucial aspects of the paper are unclear as mentioned above.\n\nOriginality\nThe main contribution of the paper is similar to multiplicative gating. The\nadded stochasticity and the model ensembling interpretation is probably novel.\nHowever, experiments are insufficient to determine whether it is this novelty\nthat contributes to improved performance or just the gating.\n\nSignificance\nThis paper makes incremental improvements and would be of moderate interest to\nthe machine learning community.\n\nTypos :\n- In Eq 3, the numerator has z_t. Should that be z_y ?\n- In Eq 5, the denominator has z_y. Should that be z_t ?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DropMax: Adaptive Stochastic Softmax","abstract":"We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes with some probability, for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are learned based on the input via regularized variational inference. This stochastic regularization has an effect of building an ensemble classifier out of combinatorial number of classifiers with different decision boundaries. Moreover, the learning of dropout probabilities for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains improved accuracy over regular softmax classifier and other baselines. Further analysis of the learned dropout masks shows that our model indeed selects confusing classes more often when it performs classification.","pdf":"/pdf/4566c443e0e52944dac66c5573ed7b201f506599.pdf","paperhash":"anonymous|dropmax_adaptive_stochastic_softmax","_bibtex":"@article{\n  anonymous2018dropmax:,\n  title={DropMax: Adaptive Stochastic Softmax},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy4c-3xRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper382/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222632740,"tcdate":1511725284906,"number":2,"cdate":1511725284906,"id":"Bkp1F5OlG","invitation":"ICLR.cc/2018/Conference/-/Paper382/Official_Review","forum":"Sy4c-3xRW","replyto":"Sy4c-3xRW","signatures":["ICLR.cc/2018/Conference/Paper382/AnonReviewer1"],"readers":["everyone"],"content":{"title":" A relevant idea, but not especially innovative and not brilliantly carried out.","rating":"4: Ok but not good enough - rejection","review":"The paper discusses dropping out the pre-softmax logits in an adaptive manner. This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout. In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference. The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour. A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1. I would have liked to have seen results on ImageNet. I don't find (the too small) Figure 2 to be compelling evidence that \"our dropmax effectively prevents\noverfiting by converging to much lower test loss\". The test loss in question looks like a noisy version of the base test loss with a slightly lower mean. There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage. Figure 3 illustrates the idea nicely. Which of the MNIST models from Table 1 was used?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DropMax: Adaptive Stochastic Softmax","abstract":"We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes with some probability, for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are learned based on the input via regularized variational inference. This stochastic regularization has an effect of building an ensemble classifier out of combinatorial number of classifiers with different decision boundaries. Moreover, the learning of dropout probabilities for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains improved accuracy over regular softmax classifier and other baselines. Further analysis of the learned dropout masks shows that our model indeed selects confusing classes more often when it performs classification.","pdf":"/pdf/4566c443e0e52944dac66c5573ed7b201f506599.pdf","paperhash":"anonymous|dropmax_adaptive_stochastic_softmax","_bibtex":"@article{\n  anonymous2018dropmax:,\n  title={DropMax: Adaptive Stochastic Softmax},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy4c-3xRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper382/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222632780,"tcdate":1511506829003,"number":1,"cdate":1511506829003,"id":"rkBcQHBgG","invitation":"ICLR.cc/2018/Conference/-/Paper382/Official_Review","forum":"Sy4c-3xRW","replyto":"Sy4c-3xRW","signatures":["ICLR.cc/2018/Conference/Paper382/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Adaptively zero out class logits based on the input","rating":"7: Good paper, accept","review":"This paper propose an adaptive dropout strategy for class logits. They learn a distribution q(z | x, y) that randomly throw class logits. By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks. They learn the dropout distribution by variational inference with concrete relaxation. \n\nOverall I think this is a good paper. The technique sounds, the presentation is clear and I have not seen similar paper elsewhere (not 100% sure about the originality of the work though). \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak. Only on CIFAR100 the proposed approach is much better than other approaches. I would like to see the results on more datasets. Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DropMax: Adaptive Stochastic Softmax","abstract":"We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes with some probability, for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are learned based on the input via regularized variational inference. This stochastic regularization has an effect of building an ensemble classifier out of combinatorial number of classifiers with different decision boundaries. Moreover, the learning of dropout probabilities for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains improved accuracy over regular softmax classifier and other baselines. Further analysis of the learned dropout masks shows that our model indeed selects confusing classes more often when it performs classification.","pdf":"/pdf/4566c443e0e52944dac66c5573ed7b201f506599.pdf","paperhash":"anonymous|dropmax_adaptive_stochastic_softmax","_bibtex":"@article{\n  anonymous2018dropmax:,\n  title={DropMax: Adaptive Stochastic Softmax},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy4c-3xRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper382/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739333534,"tcdate":1509110155554,"number":382,"cdate":1509739330876,"id":"Sy4c-3xRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy4c-3xRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DropMax: Adaptive Stochastic Softmax","abstract":"We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes with some probability, for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are learned based on the input via regularized variational inference. This stochastic regularization has an effect of building an ensemble classifier out of combinatorial number of classifiers with different decision boundaries. Moreover, the learning of dropout probabilities for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains improved accuracy over regular softmax classifier and other baselines. Further analysis of the learned dropout masks shows that our model indeed selects confusing classes more often when it performs classification.","pdf":"/pdf/4566c443e0e52944dac66c5573ed7b201f506599.pdf","paperhash":"anonymous|dropmax_adaptive_stochastic_softmax","_bibtex":"@article{\n  anonymous2018dropmax:,\n  title={DropMax: Adaptive Stochastic Softmax},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy4c-3xRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper382/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}