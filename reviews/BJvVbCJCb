{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222586544,"tcdate":1511743465593,"number":3,"cdate":1511743465593,"id":"H1flxytxf","invitation":"ICLR.cc/2018/Conference/-/Paper186/Official_Review","forum":"BJvVbCJCb","replyto":"BJvVbCJCb","signatures":["ICLR.cc/2018/Conference/Paper186/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting work but lack of detailed discussions and thorough quantitative results","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a neural clustering model following the \"Noise as Target\" technique. Combining with an reconstruction objective and \"delete-and-copy\" trick, it is able to cluster the data points into different groups and is shown to give competitive results on different benchmarks.\n\nIt is nice that the authors tried to extend the \"noise as target\" to the clustering problem, and proposed the simple \"delete-and-copy\" technique to group different data points into clusters. Even tough a little bit ad-hoc, it seems promising based on the experiment results. However, it is unclear to me why it is necessary to have the optimal matching here and why the simple nearest target would not work. After all, the cluster membership is found based on the nearest target in the test stage. \n\nAlso, the authors should provide more detailed description regarding the scheduling of the alpha and lambda values during training, and how sensitive it is to the final clustering performance. The authors cited the no requirement of \"a predefined number of clusters\" as one of the contributions, but the tuning of alpha seems more concerning.\n\nI like the authors experimented with different benchmarks, but lack of comparisons with existing deep clustering techniques is definitely a weakness. The only baseline comparison provided is the k-means clustering, but the comparisons were somewhat unfair. For all the text datasets, there were no comparisons with k-means on the features learned from the auto-encoders or clusterings learned from similar number of clusters. The comparisons for the Twitter dataset were even based on character-level with word-level. It is more convincing to show the superiority of the proposed method than existing ones on the same ground.\n\nSome other issues regarding quantitative results:\n- In Table 1, there are 152 clusters for 10-d latent space after convergence, but there are 61 clusters for 10-d latent space in Table 2 for the same MNIST dataset. Are they based on different alpha and lambda values? \n- Why does NATAC perform much better than NATAC-k? Would NATAC-k need a different number of clusters than the one from NATAC? The number of centroids learned from NATAC may not be good for k-means clustering.\n- It seems like the performance of AE-k is increasing with increase of dimensionality of latent space for Fashion-MNIST. Would AE-k beat NATAC with a different dimensionality of latent space and k?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222586584,"tcdate":1511520579695,"number":2,"cdate":1511520579695,"id":"Hk2HKdrlz","invitation":"ICLR.cc/2018/Conference/-/Paper186/Official_Review","forum":"BJvVbCJCb","replyto":"BJvVbCJCb","signatures":["ICLR.cc/2018/Conference/Paper186/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting method while less satisfactory results","rating":"5: Marginally below acceptance threshold","review":"This ms presents a new clustering method which combines deep autoencoder and a recent unsupervised representation learning approach (NAT; Bojanowski and Joujin 2017). The proposed method can jointly learn latent features and the cluster assignments. Then the method is tested in several image and text data sets.\n\nI have the following concerns:\n\n1) The paper is not self-contained. The review of NAT is too brief and makes it too hard to understand the remaining of the paper. Because NAT is a fundamental starting point of the work, it will be nice to elaborate the NAT method to be more understandable.\n\n2) Predicting the noise has no guarantee that the data items are better clustered in the latent space. Especially, projecting the data points to a uniform sphere can badly blur the cluster boundaries.\n\n3) How should we set the parameter lambda? Is it data dependent?\n\n4) The experimental results are a bit less satisfactory:\na) It is known that unsupervised clustering methods can achieve 0.97 accuracy for MNIST. See for example [Ref1, Ref2, Ref3].\nb) Figure 3 is not satisfactory. Actually t-SNE on raw MNIST pixels is not bad at all. See https://sites.google.com/site/neighborembedding/mnist\nc) For 20 Newsgroups dataset, NATAC achieves 0.384 NMI. By contrast, the DCD method in [Ref3] can achieve 0.54.\n\n5) It is not clear how to set the number of clusters. More explanations are appreciated.\n\n[Ref1] Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, Erkki Oja. Clustering by Nonnegative Matrix Factorization Using Graph Random Walk. In NIPS 2012.\n[Ref2] Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht. Multiclass Total Variation Clustering. In NIPS 2013.\n[Ref3] Zhirong Yang, Jukka Corander and Erkki Oja. Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis. Journal of Machine Learning Research, 17(187): 1-25, 2016.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222586625,"tcdate":1511414811023,"number":1,"cdate":1511414811023,"id":"rkX7hRmeM","invitation":"ICLR.cc/2018/Conference/-/Paper186/Official_Review","forum":"BJvVbCJCb","replyto":"BJvVbCJCb","signatures":["ICLR.cc/2018/Conference/Paper186/AnonReviewer3"],"readers":["everyone"],"content":{"title":"overall algorithm is somewhat heuristic","rating":"5: Marginally below acceptance threshold","review":"This paper presents an algorithm for clustering using DNNs. The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs. The second step also shrinks the number of targets over time to achieve clustering. Intuitively, the randomness in target may achieve certain regularization effect.\n\nMy concerns:\n1. There is no analysis on what the regularization effect is. What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot?\n2. The delete-and-copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout. Without a consistent objective function, the algorithm seems somewhat heuristic.\n3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it. If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case.\n4. Does the  Hungarian algorithm used for matching scales to much larger datasets?\n5. While the algorithm empirically improve over k-means, I believe at this point combinations of DNN with classical clustering algorithms already exist and comparisons with such stronger baselines are missing. The authors have listed a few related algorithms in the last paragraph on page 1. I think the following one is also relevant:\n-- Law et al. Deep spectral clustering learning. ICML 2015.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510853214279,"tcdate":1510853214279,"number":2,"cdate":1510853214279,"id":"SyUwqSoJf","invitation":"ICLR.cc/2018/Conference/-/Paper186/Official_Comment","forum":"BJvVbCJCb","replyto":"SkrcjjVJM","signatures":["ICLR.cc/2018/Conference/Paper186/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper186/Authors"],"content":{"title":"RE: Request for citation","comment":"We will certainly compare the results of the paper, along with other related clustering papers in the ICLR review, to our approach."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510853175079,"tcdate":1510853175079,"number":1,"cdate":1510853175079,"id":"HJyr5Hokf","invitation":"ICLR.cc/2018/Conference/-/Paper186/Official_Comment","forum":"BJvVbCJCb","replyto":"BJvVbCJCb","signatures":["ICLR.cc/2018/Conference/Paper186/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper186/Authors"],"content":{"title":"Including Related Work","comment":"Since submission, several papers have come to our attention which we would like to include and discuss:\n\n* Learning Discrete Representations via Information Maximizing Self-Augmented Training\n* Deep Continuous Clustering (ICLR 2018 submission)\n* Spectralnet Spectral Clustering Using Deep Neural Networks (ICLR 2018 submission)\n* Learning Latent Representations In Neural Networks For Unsupervised Clustering Through Pseudo Supervision And Graph Based Activity Regularization (ICLR 2018 submission)\n\nAdditionally, some of the above report higher NMI scores than our model (although they require a set number of clusters). We will adapt the paper respectively."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510419341528,"tcdate":1510419341528,"number":1,"cdate":1510419341528,"id":"SkrcjjVJM","invitation":"ICLR.cc/2018/Conference/-/Paper186/Public_Comment","forum":"BJvVbCJCb","replyto":"BJvVbCJCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Request for citation","comment":"I believe that you should also cite “Learning Discrete Representations via Information Maximizing Self-Augmented Training” (ICML 2017) http://proceedings.mlr.press/v70/hu17b.html.\nThis paper is closely related to your work and is also about unsupervised clustering using deep neural networks.\nAs far as I know, the proposed method, IMSAT, is the current state-of-the-art method in deep clustering (November 2017). Could you compare your results against their result?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739438627,"tcdate":1509052719227,"number":186,"cdate":1509739435965,"id":"BJvVbCJCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJvVbCJCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Clustering By Predicting And Copying Noise","abstract":"We propose a neural clustering model that jointly learns both latent features and how they cluster. Unlike similar methods our model does not require a predefined number of clusters. Using a supervised approach, we agglomerate latent features towards randomly sampled targets within the same space whilst progressively removing the targets until we are left with only targets which represent cluster centroids. To show the behavior of our model across different modalities we apply our model on both text and image data and achieve state-of-the-art results on MNIST. Finally, we also provide results against baseline models for fashion-MNIST, the 20 newsgroups dataset, and a Twitter dataset we ourselves create.","pdf":"/pdf/f8c230dc00b942c536e25b3573cb4274066d0682.pdf","TL;DR":"State of the art neural clustering without needing a number of clusters","paperhash":"anonymous|neural_clustering_by_predicting_and_copying_noise","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Clustering By Predicting And Copying Noise},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvVbCJCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper186/Authors"],"keywords":["unsupervised learning","clustering","deep learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}