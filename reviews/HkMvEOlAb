{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222617968,"tcdate":1512207743911,"number":3,"cdate":1512207743911,"id":"H1_YBgxZz","invitation":"ICLR.cc/2018/Conference/-/Paper307/Official_Review","forum":"HkMvEOlAb","replyto":"HkMvEOlAb","signatures":["ICLR.cc/2018/Conference/Paper307/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. ","rating":"4: Ok but not good enough - rejection","review":"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. Pipeline: -Data are augmented with domain-specific transformations. For instance, in the case of MNIST, rotations with different degrees are applied. All data are then labelled as \"original\" or \"transformed by ...(specific transformation)\". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels. -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion. -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer. \n\nDetailed Comments:\n(*) Pros\n-The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST.\n-Use of ACOL and GAR is interesting, also the idea to make \"labeled\" data from unlabelled ones by using data augmentation.\n\n(*) Cons\n-minor: in the title, I find the expression \"unsupervised clustering\" uselessly redundant since clustering is by definition unsupervised.\n-Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits. This is not a very challenging task.\nAnd just because something works on MNIST, does not mean it works in general. \nWhat are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)?\n-This is not clear what is novel here since ACOL and GAR already exist. The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]}},{"tddate":null,"ddate":null,"tmdate":1512222618009,"tcdate":1511991196446,"number":2,"cdate":1511991196446,"id":"r1Eovo2gf","invitation":"ICLR.cc/2018/Conference/-/Paper307/Official_Review","forum":"HkMvEOlAb","replyto":"HkMvEOlAb","signatures":["ICLR.cc/2018/Conference/Paper307/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very Incremental ","rating":"5: Marginally below acceptance threshold","review":"This paper utilizes ACOL algorithm for unsupervised learning. ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9). Given that in many applications such parent-class supervised information is not available, the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. The authors use MNIST data set to study different aspects of the proposed approach.\n\nAltough the idea of using specific domain parent-class labels makes the use of ACOL more practical, the ideas introduced in this paper is very incremental.  I also would prefer to see the performance of the proposed method on more that one data set.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]}},{"tddate":null,"ddate":null,"tmdate":1512222618050,"tcdate":1511970768534,"number":1,"cdate":1511970768534,"id":"rkO0PUnlG","invitation":"ICLR.cc/2018/Conference/-/Paper307/Official_Review","forum":"HkMvEOlAb","replyto":"HkMvEOlAb","signatures":["ICLR.cc/2018/Conference/Paper307/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written manuscript with a marginal contribution","rating":"7: Good paper, accept","review":"The paper is well written and clear. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. The idea is to introduce the notion of pseudo labelling. \nPseudo labelling can be obtained by transformations of original input data.\nThe key point is the definition of the transformations. \nOnly whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task.\nSince it is not known in advance what might be a good set of transformations, it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]}},{"tddate":null,"ddate":null,"tmdate":1510438862167,"tcdate":1510438862167,"number":1,"cdate":1510438862167,"id":"ByUAwgHJz","invitation":"ICLR.cc/2018/Conference/-/Paper307/Official_Comment","forum":"HkMvEOlAb","replyto":"Sk_7iiVJG","signatures":["ICLR.cc/2018/Conference/Paper307/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper307/Authors"],"content":{"title":"Citing IMSAT and comparison","comment":"First of all, we would like to thank you for your comment.\n\nWe'll cite this work in the upcoming revision. But I think revisions are currently not allowed as the review process is ongoing. \n\nSo as a quick answer to your comment,  I can say that it's hard to observe any statistically significant differences between the performances of these two models on the MNIST dataset. That is, 98.4 (0.4) for IMSAT vs. 98.32%(±0.08) for our approach. But the SVHN dataset provides a more solid basis of comparison. While IMSAT achieves 57.3 (3.9) clustering accuracy on SVHN, our approach outperforms IMSAT by achieving 76.80%(±1.30). I think it's also worth noting the mechanical differences between these two approaches. For example, while IMSAT uses 960-dimensional GIST features for SVHN, our approach employs raw pixels (32x32x3). Besides, IMSAT uses VAT based (or RPT based) regularization while we adopt graph-based regularization.\n\nI think citing this work will further be helpful for the evaluation of our approach, as it also provides the performances of other approaches (like DEC) on the SVHN dataset.\n\nThank you very much for your feedback.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]}},{"tddate":null,"ddate":null,"tmdate":1510419232270,"tcdate":1510419232270,"number":1,"cdate":1510419232270,"id":"Sk_7iiVJG","invitation":"ICLR.cc/2018/Conference/-/Paper307/Public_Comment","forum":"HkMvEOlAb","replyto":"HkMvEOlAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Request for citation","comment":"I believe that you should also cite “Learning Discrete Representations via Information Maximizing Self-Augmented Training” (ICML 2017) http://proceedings.mlr.press/v70/hu17b.html.\nThis paper is closely related to your work and is also about unsupervised clustering using deep neural networks.\nAs far as I know, the proposed method, IMSAT, is the current state-of-the-art method in deep clustering (November 2017). Could you compare your results against their result?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]}},{"tddate":null,"ddate":null,"tmdate":1509739373690,"tcdate":1509094490060,"number":307,"cdate":1509739371039,"id":"HkMvEOlAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkMvEOlAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization","abstract":"In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.","pdf":"/pdf/e6f7f2b06c90ad28f68bbb0e54be0d573546e06c.pdf","paperhash":"anonymous|learning_latent_representations_in_neural_networks_for_unsupervised_clustering_through_pseudo_supervision_and_graphbased_activity_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkMvEOlAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper307/Authors"],"keywords":["representation learning","unsupervised clustering","pseudo supervision","graph-based activity regularization","auto-clustering output layer"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}