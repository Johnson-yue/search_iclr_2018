{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222722533,"tcdate":1512010742347,"number":2,"cdate":1512010742347,"id":"Bk0lEg6eG","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Review","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Novel model for Collaborative filtering. Seems reasonable overall. Empirical study would be more convincing by including classic recsys datasets˚.","rating":"4: Ok but not good enough - rejection","review":"The authors introduce a novel novel for collaborative filtering. The proposed model combines some of the strengths of factorization machines and of polynomial regression. Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected).\n\nThe paper is well written overall and relatively easy to understand. The study seems fairly thorough (both vanilla and cold-start experiments are reported).\n\nOverall the paper feels a little bit incomplete . This is particularly apparent in the empirical study. Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results. Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets.\n\n2) Report results on well-known CF datasets. Movielens comes to mind.\n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from.\n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\n\n- I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these \"clouds\" are hard to interpret).\n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1512222722573,"tcdate":1511654365290,"number":1,"cdate":1511654365290,"id":"rkSyVFDeG","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Review","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/AnonReviewer3"],"readers":["everyone"],"content":{"title":"proposes a model that is equivalent to known work","rating":"3: Clear rejection","review":"This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i\n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1509739150550,"tcdate":1509132770905,"number":704,"cdate":1509739147889,"id":"HJsk5-Z0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}