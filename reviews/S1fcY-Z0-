{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222722240,"tcdate":1511812734820,"number":3,"cdate":1511812734820,"id":"HJPY0ycef","invitation":"ICLR.cc/2018/Conference/-/Paper702/Official_Review","forum":"S1fcY-Z0-","replyto":"S1fcY-Z0-","signatures":["ICLR.cc/2018/Conference/Paper702/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea. not rigorous. limited novelty","rating":"5: Marginally below acceptance threshold","review":"The authors propose a new method of defining approximate posteriors for use in Bayesian neural networks. The idea of using hypernetworks for Bayesian inference is compelling, and the authors show some promising first results. I see two issues, and would be willing to increase my rating if these were sufficiently addressed.\n\n- The paper says it uses an \"isotropic standard normal prior on the weights of the network\". However, the stochastic part of the generated weights (i.e. the scales) is of a lower dimension than the weights. It seems to me this means that the KL divergence between prior and posterior is undefined, or infinite, as the posterior is only defined on a sub-manifold. What exactly is the loss term that is added to the training objective? And how is this justified?\n\n- The instantiation of Bayesian hypernetworks that is used in experiments seems to be a special case of the method of multiplicative normalizing flows as proposed by Louizos and Welling and discussed in this paper. If the variances / sigmas are zero in the latter method, their approximation seems functionally equivalent to Bayesian hypernetworks (though with different parameterization). Is my understanding correct? If so, the novelty of the proposed method is limited.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]}},{"tddate":null,"ddate":null,"tmdate":1511809274224,"tcdate":1511809274224,"number":1,"cdate":1511809274224,"id":"HkMZb19ef","invitation":"ICLR.cc/2018/Conference/-/Paper702/Public_Comment","forum":"S1fcY-Z0-","replyto":"S1fcY-Z0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Missing literature in recent development of Bayesian Neural Networks","comment":"Is it possible to compare with particle [1] and sample-based [2] methods to learn the weight uncertainty of neural networks? which have shown excellent performance.\n\n[1] Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm, 2016\n[2] Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks, 2016"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222722280,"tcdate":1511778140153,"number":2,"cdate":1511778140153,"id":"rJNPwwYef","invitation":"ICLR.cc/2018/Conference/-/Paper702/Official_Review","forum":"S1fcY-Z0-","replyto":"S1fcY-Z0-","signatures":["ICLR.cc/2018/Conference/Paper702/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental idea with a potential technical issue.","rating":"4: Ok but not good enough - rejection","review":"This paper presents Bayesian Hypernetworks; variational Bayesian neural networks where the variational posterior over the weights is governed by a hyper network that implements a normalizing flow (NF) such as RealNVP and IAF. As directly outputting the weight matrix with a hyper network is computationally expensive the authors instead propose to utilize weight normalisation on the weights and then use the hyper network to output scalar scaling variables for each hidden unit, similarly to what was done at [1]. The main difference with this prior work is that [1] consider these NF scaling variables as auxiliary random variables to a mean field Gaussian distribution over the weights whereas this paper attempts to posit a distribution directly on the weights via the NF. This avoids the nested variational approximation and auxiliary models of [1], which can potentially yield a tighter bound. The proposed method is evaluated on extensive experiments.\n\nThis paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at Section 3.3. More specifically, if you generate the parameters \\theta according to Eq. 7 and posit a prior over \\theta then you will have a problematic variational bound as there will be a KL divergence, KL(q(\\theta) || p(\\theta)), with distributions of different support (since q(\\theta) is defined only along the directions spanned by u), which is infinite. For the KL to be valid you will need to posit a prior distribution over `g`, p(g), and then consider KL(q(g) || p(g)), with q(g) being given by the NF. From the experiment paragraph at page 5 though I deduct that you instead employ “an isotropic standard normal prior over the weights”, i.e. \\theta, thus I believe that you indeed have a problematic bound. How do you actually compute logq(\\theta) when you employ the parametrisation discussed at 3.3? Did you use that parametrisation in every experiment?\n\nOther than that, I believe that it would be interesting to experiment with a `full` hyper network, i.e. generating directly the entire parameter vector \\theta, e.g. at the toy regression experiment where the dimensionality is small. This would then better illustrate the tradeoffs you make when you reduce the flexibility of the hyper-network to just outputting the row scaling variables and the effect this has at the posterior approximation.\n \nTypos:\n(1) Page 3, 3.1.1 log(\\theta) -> logp(\\theta).\n(2) Eq. 6, it needs to be |det \\frac{\\partial h(\\epsilon)}{\\partial \\epsilon}|^{-1} or |det \\frac{\\partial h^{-1}(\\theta)}{\\partial \\theta}| for a valid change of variables formula.\n\n[1] Louizos & Welling, Multiplicative Normalizing Flows for Variational Bayesian Neural Networks.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222722320,"tcdate":1511690674301,"number":1,"cdate":1511690674301,"id":"Hy5hZMulM","invitation":"ICLR.cc/2018/Conference/-/Paper702/Official_Review","forum":"S1fcY-Z0-","replyto":"S1fcY-Z0-","signatures":["ICLR.cc/2018/Conference/Paper702/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper - novel enough?","rating":"6: Marginally above acceptance threshold","review":"This paper proposes Bayesian hypernetworks to carry out Bayesian learning of deep networks. The idea is to construct a generative model capable of approximating the posterior distribution over the parameters of deep networks. I think that the paper is well written and easy to follow. \n\nI like the idea of constructing general approximation strategies for complex posterior distribution and the proposed approach inherits all the scalability properties of modern deep learning techniques. In this respect, I think that the paper tackles a timely topic and is interesting to read. \n\nIt is not entirely clear to me why the Authors name their proposal Bayesian hypernetworks. This seems to suggest that also the hypernetwork is infered using Bayesian inference, but if I understand correctly this is not the case. \n\nI have some comments on novelty and realization of the experiments. In the positioning of the work in the literature, the Authors point out that hypernetworks have been proposed before, so it is not clear what is the actual novelty in the proposal. Is it the use of Real NVPs and IAFs as hypernetworks? These methods have been already proposed and extensively studied in the literature, and even if they have been adapted to be hypernetworks here, I believe that the novelty is fairly limited. \n\nThe experimental part is interesting as it explores a number of learning scenarios. However, I think that it would have been useful to add comparisons with standard variational inference (e.g., Graves, 2011) for deep networks to substantiate the claims that this approach underestimates uncertainty. I believe that this would strengthen the comparative evaluation. \n\nI think the paper would have made a stronger case by including other approaches to approximate posteriors using generative models. For example, the variational Gaussian process paper sounds like an ideal method to include here. \n\n[1] D. Tran, R. Ranganath, and D. M. Blei. Variational Gaussian process. arXiv preprint arXiv:1511.06499, 2015.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]}},{"tddate":null,"ddate":null,"tmdate":1511401188840,"tcdate":1511401188840,"number":1,"cdate":1511401188840,"id":"HkpJDjmez","invitation":"ICLR.cc/2018/Conference/-/Paper702/Official_Comment","forum":"S1fcY-Z0-","replyto":"S1fcY-Z0-","signatures":["ICLR.cc/2018/Conference/Paper702/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper702/AnonReviewer3"],"content":{"title":"some questions","comment":"* You use the same noise sample for all examples in a minibatch. Is this because the computation in the hypernet otherwise becomes too expensive? An advantage of your proposed approach seems to be that by outputting only the scales of the parameters you could easily use different scales for different examples as far as the primary network is concerned.\n\n* You claim to show \"that BHNs act as a regularizer, outperforming dropout and traditional mean field\". However the results shown in e.g. table 1 for CIFAR-10 seem to be quite a bit worse than previous SOTA results obtained with dropout. Why the difference?\n\n* Please expand the caption in Figure 3. Is this MNIST? With the full training set or a restricted set?\n\n* How do your anomaly detection results compare against methods that use ensembles? (e.g. http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739151657,"tcdate":1509132681770,"number":702,"cdate":1509739148987,"id":"S1fcY-Z0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1fcY-Z0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bayesian Hypernetworks","abstract":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, h, is a neural network which learns to transform a simple noise distribution, p(e) = N(0,I), to a distribution q(t) := q(h(e)) over the parameters t of another neural network (the ``primary network). We train q with variational inference, using an invertible h to enable efficient estimation of the variational lower bound on the posterior p(t | D) via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap iid sampling of q(t).  In practice, Bayesian hypernets provide a better defense against adversarial examples than dropout, and also exhibit competitive performance on a suite of tasks which evaluate model uncertainty, including regularization, active learning, and anomaly detection.\n","pdf":"/pdf/dae60ecca7677078ce2759c5a0349ccf9f7813ab.pdf","TL;DR":"We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.","paperhash":"anonymous|bayesian_hypernetworks","_bibtex":"@article{\n  anonymous2018bayesian,\n  title={Bayesian Hypernetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fcY-Z0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper702/Authors"],"keywords":["variational inference","bayesian inference","deep networks"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}