{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222559292,"tcdate":1511906982578,"number":3,"cdate":1511906982578,"id":"rJJ2RIigz","invitation":"ICLR.cc/2018/Conference/-/Paper1141/Official_Review","forum":"SJyVzQ-C-","replyto":"SJyVzQ-C-","signatures":["ICLR.cc/2018/Conference/Paper1141/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Presents an interesting idea fairly clearly, but overall very similar to expectation-linear dropout. The analysis is very general and does not really depend on RNN/NN stuff.","rating":"6: Marginally above acceptance threshold","review":"The proposed method fraternal dropout is a stochastic alternative of the expectation-linear dropout method, where part of the objective is for the dropout mask to have low variance. The first order way to achieve lower variance is to have smaller weights. The second order is by having more evenly spread weights, so there is more concentration around the mean. As a result, it seems that at least part of the effect of explicitly reducing the variance is just stronger weight penalty. The effect of dropout in the first place is the opposite, where variance is introduced deliberately. So I would like to see some comparisons between this method and various dropout rates, and regular weight penalty combinations.\n\nThis work is very closely related to expectation linear dropout, except that you are now actually minimizing the variance: 1/2E[ ||f(s) - f(s')|| ] is used instead of E [ ||f(s) - f_bar|| ].  Eq 5 is very close to this, except the f_bar is not quite the mean, but the value with the mean dropout mask. So all the results should be compared with ELD.\n\nI do not think the method is theoretically well-motivated as presented, but the empirical results seem solid.\nIt is somewhat alarming how the analysis has little to do with the neural networks and how dropout works, let along RNNs, while the strength of the empirical results are all on RNNs.\n\nI feel the ideas interesting and valuable especially in light of strong empirical results, but the authors should do more to clarify what is actually happening.\n\nMinor: why use s_i and s_j, when there is never any reference to i and j? As far as I can tell, i and j serve as constants, more like s_1 and s_2.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fraternal Dropout","abstract":"Recurrent neural networks (RNNs) are an important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised tasks (CIFAR-10).","pdf":"/pdf/e58a67feb2152ae4cd53042cbb8762df63757b73.pdf","TL;DR":"We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.","paperhash":"anonymous|fraternal_dropout","_bibtex":"@article{\n  anonymous2018fraternal,\n  title={Fraternal Dropout},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyVzQ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1141/Authors"],"keywords":["fraternal dropout","activity regularization","recurrent neural networks","RNN","LSTM","faster convergence"]}},{"tddate":null,"ddate":null,"tmdate":1512222559334,"tcdate":1511794218639,"number":2,"cdate":1511794218639,"id":"SkmNLstxG","invitation":"ICLR.cc/2018/Conference/-/Paper1141/Official_Review","forum":"SJyVzQ-C-","replyto":"SJyVzQ-C-","signatures":["ICLR.cc/2018/Conference/Paper1141/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written paper on incremental improvement","rating":"5: Marginally below acceptance threshold","review":"The paper proposes “fraternal dropout”, which passes the same input twice through a model with different dropout masks. The L2 norm of the differences is then used as an additional regulariser. As the authors note, this implicitly minimises the variance of the model under the dropout mask.\n\nThe method is well presented and adequately placed within the related work. The text is well written and easy to follow.\n\nI have only two concerns. The first is that the method is rather incremental and I am uncertain how it will stand the test of time and will be adopted.\n\nThe second is that of the experimental evaluation. They authors write that a full hyper parameter search was not conducted in the fear of having a more thorough evaluation than the base lines, erroneously reporting superior results.\n\nTo me, this is not an acceptable answer. IMHO, the evaluation should be thorough for both the base lines and the proposed method. If authors can get away with a sub standard evaluation because the competing method did, the field might converge to sub standard evaluations overall. This is clearly not in anyones interest. I am open to the author's comments on this, as I understand that spending weeks on tuning a competing method is also not unbiased and work that could be avoided if all software was published.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fraternal Dropout","abstract":"Recurrent neural networks (RNNs) are an important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised tasks (CIFAR-10).","pdf":"/pdf/e58a67feb2152ae4cd53042cbb8762df63757b73.pdf","TL;DR":"We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.","paperhash":"anonymous|fraternal_dropout","_bibtex":"@article{\n  anonymous2018fraternal,\n  title={Fraternal Dropout},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyVzQ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1141/Authors"],"keywords":["fraternal dropout","activity regularization","recurrent neural networks","RNN","LSTM","faster convergence"]}},{"tddate":null,"ddate":null,"tmdate":1512222559372,"tcdate":1511536360622,"number":1,"cdate":1511536360622,"id":"rkblPhrgf","invitation":"ICLR.cc/2018/Conference/-/Paper1141/Official_Review","forum":"SJyVzQ-C-","replyto":"SJyVzQ-C-","signatures":["ICLR.cc/2018/Conference/Paper1141/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Improved expectation-linear dropout","rating":"5: Marginally below acceptance threshold","review":"The authors present Fraternal dropout as an improvement over Expectation-linear dropout (ELD) in terms of convergence and demonstrate the utility of Fraternal dropout on a number of tasks and datasets.\n\nAt test time, more often than not, people apply dropout in deterministic mode while at training time masks are sampled randomly. The paper addresses this issue by trying to reduce the gap.\n\nI have 1.5 high level comments:\n\n- Dropout can be applied by averaging results corresponding to randomly sampled masks ('MC eval'). This should not be ignored, and preferrably included in the evaluation.\n\n- It could be made clearer why the proposed regularization would make the aforementioned gap smaller. Intuitively, the bias of the deterministic approximation (compared to the MC eval) should also play a role. It may be worth asking whether the bias changes? A possibility is that MC and deterministic evaluations meet halfway and with fraternal dropout MC eval is worse than without.\n\nDetails:\n\n- The notation is confusing: p() looks like a probability distribution, z looks like a latent variable, p^t and l^t have superscripts instead of Y having a subscript, z^t is a function of X. Wouldn't f(X_t) be preferrable to p^t(z_t)?\n\n- The experiments are set up and executed with care, but section 4 could be improved by providings details (as much as in section 5). The results on PTB and Wikitext-2 are really good. However, why not compare to ELD here? Section 5 leads the reader to believe that ELD would be equally good.\n\n- Section 5 could be the most interesting part of the paper. This is where different regularization methods are compared (by the way, this is not \"ablation\"). It is somewhat unfortunate that due to lack of computational resources the comparisons are made at a single hyperparameter setting.\n\nAll in all, the results of section 4 are clearly good, but are they better than those of ELD? Evaluation and interpretation of results in section 5 is made difficult by the omission of the most informative quantity which Fraternal dropout is supposed to be approximating.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fraternal Dropout","abstract":"Recurrent neural networks (RNNs) are an important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised tasks (CIFAR-10).","pdf":"/pdf/e58a67feb2152ae4cd53042cbb8762df63757b73.pdf","TL;DR":"We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.","paperhash":"anonymous|fraternal_dropout","_bibtex":"@article{\n  anonymous2018fraternal,\n  title={Fraternal Dropout},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyVzQ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1141/Authors"],"keywords":["fraternal dropout","activity regularization","recurrent neural networks","RNN","LSTM","faster convergence"]}},{"tddate":null,"ddate":null,"tmdate":1510092379881,"tcdate":1509138983141,"number":1141,"cdate":1510092359512,"id":"SJyVzQ-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJyVzQ-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Fraternal Dropout","abstract":"Recurrent neural networks (RNNs) are an important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised tasks (CIFAR-10).","pdf":"/pdf/e58a67feb2152ae4cd53042cbb8762df63757b73.pdf","TL;DR":"We propose to train two identical copies of an recurrent neural network (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.","paperhash":"anonymous|fraternal_dropout","_bibtex":"@article{\n  anonymous2018fraternal,\n  title={Fraternal Dropout},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyVzQ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1141/Authors"],"keywords":["fraternal dropout","activity regularization","recurrent neural networks","RNN","LSTM","faster convergence"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}