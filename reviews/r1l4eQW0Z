{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222556076,"tcdate":1511811678023,"number":3,"cdate":1511811678023,"id":"Hk8v9J5eM","invitation":"ICLR.cc/2018/Conference/-/Paper1106/Official_Review","forum":"r1l4eQW0Z","replyto":"r1l4eQW0Z","signatures":["ICLR.cc/2018/Conference/Paper1106/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea; not clear it scales; needs experiments on quality of ratio estimation and also posterior approximation","rating":"5: Marginally below acceptance threshold","review":"The authors propose Kernel Implicit VI, an algorithm allowing implicit distributions as the posterior approximation by employing kernel ridge regression to estimate a density ratio. Unlike current approaches with adversarial training, the authors argue this avoids the problems of noisy ratio estimation, as well as potentially high-dimensional inputs to the discriminator.  The work has interesting ideas. Unfortunately, I'm not convinced that the method overcomes these difficulties as they argue in Sec 3.2.\n\nAn obvious difficulty with kernel ridge regression in practice is that its complete inaccuracy to estimate high-dimensional density ratios.  This is especially the case given a limited number of samples from both p and q (which is the same problem as previous methods) as well as the RBF kernel. While the RBF kernel still takes the same high-dimensional inputs and does not involve learning massive sets of parameters, it also does not scale well at all for accurate estimation. This is the same problem as related approaches with Stein variational gradient descent; namely, it avoids minimax problems as in adversarial training by implicitly integrating over the discriminator function space using the kernel trick.\n\nThis flaw has rather deep implications. For example, my understanding of the implicit VI on the Bayesian neural network in Sec 4 is that it ends up as cross-entropy minimization subject to a poorly estimated KL regularizer. I'd like to see just how much entropy the implicit approximation has instead of concnetrating toward a point; or more directly, what the implicit posterior approximation looks like compared to a true posterior inferred by, say, HMC as the ground truth. This approach also faces difficulties that the naive Gaussian approximation applied to Bayesian neural nets does not: implicit approximations cannot exploit the local reparameterization trick and are therefore limited to specific architectures that does not involve sampling very large weight matrices.\n\nThe authors report variational lower bounds, which I'm not sure is really a lower bound. Namely, the bias incurred by the ratio estimation makes it difficult to compare numbers. An obvious but very illustrative experiment I'd like to see would be the accuracy of the KL estimator on problems where we can compute it tractably, or where we can Monte Carlo estimate it very well under complicated but tractable densities. I also suggest the authors perform the experiment suggested above with HMC as ground truth on a non-toy problem such as a fairly large Bayesian neural net.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kernel Implicit Variational Inference","abstract":"Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","pdf":"/pdf/37257d67235ce949317db3948d11ee647fcb9743.pdf","paperhash":"anonymous|kernel_implicit_variational_inference","_bibtex":"@article{\n  anonymous2018kernel,\n  title={Kernel Implicit Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1l4eQW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1106/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222556117,"tcdate":1511613891198,"number":2,"cdate":1511613891198,"id":"S1jTB1PlM","invitation":"ICLR.cc/2018/Conference/-/Paper1106/Official_Review","forum":"r1l4eQW0Z","replyto":"r1l4eQW0Z","signatures":["ICLR.cc/2018/Conference/Paper1106/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A good contribution to implicit approximate posterior fitting","rating":"7: Good paper, accept","review":"Thank you for an interesting read. \n\nApproximate inference with implicit distribution has been a recent focus of the research since late 2016. I have seen several papers simultaneously proposing the density ratio estimation idea using GAN approach. This paper, although still doing density ratio estimation, uses kernel estimators instead and thus avoids the usage of discriminators. \n\nFurthermore, the paper proposed a new type of implicit posterior approximation which uses intuitions from matrix factorisation. I do think that another big challenge that we need to address is the construction of good implicit approximations, which is not well studied in previous literature (although this is a very new topic). This paper provides a good start in this direction.\n\nHowever several points need to be clarified and improved:\n1. There are other ways to do implicit posterior inference such as amortising deterministic/stochastic dynamics, and approximating the gradient updates of VI. Please check the literature.\n2. For kernel based density ratio estimation methods, you probably need to cite a bunch of Sugiyama papers besides (Kanamori et al. 2009). \n3. Why do you need to introduce both regression under p and q (the reverse ratio trick)? I didn't see if you have comparisons between the two. From my perspective the reverse ratio trick version is naturally more suitable to VI.\n4. Do you have any speed and numerical issues on differentiating through alpha (which requires differentiating K^{-1})?\n5. For kernel methods, kernel parameters and lambda are key to performances. How did you tune them?\n6. For the celebA part, can you compute some quantitative metric, e.g inception score?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kernel Implicit Variational Inference","abstract":"Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","pdf":"/pdf/37257d67235ce949317db3948d11ee647fcb9743.pdf","paperhash":"anonymous|kernel_implicit_variational_inference","_bibtex":"@article{\n  anonymous2018kernel,\n  title={Kernel Implicit Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1l4eQW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1106/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510650635272,"tcdate":1510650635272,"number":1,"cdate":1510650635272,"id":"Hkmf7Nukz","invitation":"ICLR.cc/2018/Conference/-/Paper1106/Official_Comment","forum":"r1l4eQW0Z","replyto":"r1l4eQW0Z","signatures":["ICLR.cc/2018/Conference/Paper1106/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1106/Authors"],"content":{"title":"Errata","comment":"Figure 1(a) in the toy experiment is incorrectly drawn and thus misinterpreted. The correct figure should be that the Gaussian posterior covers the left mode instead of being between the two modes, since it is initialized from left (see https://drive.google.com/file/d/1nJAVH2-Fl0P6ei-ZwBI3_Z6BvFFAYk9E/view?usp=sharing). The figure of KIVI is correct and we double-checked all the others. We sincerely apologize for this error and will fix it in the revision."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kernel Implicit Variational Inference","abstract":"Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","pdf":"/pdf/37257d67235ce949317db3948d11ee647fcb9743.pdf","paperhash":"anonymous|kernel_implicit_variational_inference","_bibtex":"@article{\n  anonymous2018kernel,\n  title={Kernel Implicit Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1l4eQW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1106/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222556157,"tcdate":1510508650055,"number":1,"cdate":1510508650055,"id":"rkzduZIyf","invitation":"ICLR.cc/2018/Conference/-/Paper1106/Official_Review","forum":"r1l4eQW0Z","replyto":"r1l4eQW0Z","signatures":["ICLR.cc/2018/Conference/Paper1106/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting and novel idea but poor writing quality","rating":"6: Marginally above acceptance threshold","review":"This paper presents Kernel Implicit Variational Inference (KIVI), a novel class of implicit variational distributions. KIVI relies on a kernel approximation to directly estimate the density ratio. Importantly, the optimal kernel approximation in KIVI has closed-form solution, which allows for faster training since it avoids gradient ascent steps that may soon get \"outdated\" as the optimization over the variational distribution runs. The paper presents experiments on a variety of scenarios to show the performance of KIVI.\n\nUp to my knowledge, the idea of estimating the density ratio using kernels is novel. I found it interesting, specially since there is a closed-form solution for this estimate. The closed form solution involves a matrix inversion, but this shouldn't be an issue, as the matrix size is controlled by the number of samples, which is a parameter that the practitioner can choose. I also found interesting the implicit MMNN architecture proposed in Section 4.\n\nThe experiments seem convincing too, although I believe the paper could probably be improved by comparing with other implicit VI methods, such as [Liu & Feng], [Tran et al.], or others.\n\nMy major criticism with the paper is the quality of the writing. I found quite a few errors in every page, which significantly affects readability. I strongly encourage the authors to carefully review the entire paper and search for typos, grammatical errors, unclear sentences, etc.\n\nPlease find below some further comments broken down by section.\n\nSection 1: In the introduction, it is unclear to me what \"protect these models\" means. Also, in the second paragraph, the authors talk about \"often leads to biased inference\". The concept to \"biased inference\" is unclear. Finally, the sentence \"the variational posterior we get in this way does not admit a tractable likelihood\" makes no sense to me; how can a posterior admit (or not admit) a likelihood?\n\nSection 3: The first paragraph of the KIVI section is also unclear to me. In Section 3.1, it looks like the cost function L(\\hat(r)) is different from the loss in Eq. 1, so it should have a different notation. In Eq. 4, I found it confusing whether L(r)=J(r). Also, it would be nice to include a brief description of why the expectation in Eq. 4 is taken w.r.t. p(z) instead of q(z), for those readers who are less familiar with [Kanamori et al.]. Finally, the motivation behind the \"reverse ratio trick\" was unclear to me (the trick is clear, but I didn't fully understand why it's needed).\n\nSection 4: The first paragraph of the example can be improved with a brief discussion of why the methods of [Mescheder et al.] and [Song et al.] \"are nor applicable\". Also, the paragraph above Eq. 11 (\"When modeling a matrix...\") was unclear to me.\n\nSection 6: In Figure 1(a), I think there must be something wrong, because it is well-known that VI tends to cover one of the modes of the posterior only due to the form of the KL divergence (in contrast to EP, which should look like the curve in the figure). Additionally, Figure 3(a) (and the explanation in the text) was unclear to me. Finally, I disagree with the discussion regarding overfitting in Figure 3(b): that plot doesn't show overfitting because it is a plot of the training loss (and overfitting occurs on test); instead it looks like an optimization issue that makes the bound decrease.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kernel Implicit Variational Inference","abstract":"Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","pdf":"/pdf/37257d67235ce949317db3948d11ee647fcb9743.pdf","paperhash":"anonymous|kernel_implicit_variational_inference","_bibtex":"@article{\n  anonymous2018kernel,\n  title={Kernel Implicit Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1l4eQW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1106/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092380705,"tcdate":1509138474327,"number":1106,"cdate":1510092359971,"id":"r1l4eQW0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1l4eQW0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Kernel Implicit Variational Inference","abstract":"Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","pdf":"/pdf/37257d67235ce949317db3948d11ee647fcb9743.pdf","paperhash":"anonymous|kernel_implicit_variational_inference","_bibtex":"@article{\n  anonymous2018kernel,\n  title={Kernel Implicit Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1l4eQW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1106/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}