{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222746665,"tcdate":1512070812370,"number":3,"cdate":1512070812370,"id":"S14j0RTxM","invitation":"ICLR.cc/2018/Conference/-/Paper769/Official_Review","forum":"SktLlGbRZ","replyto":"SktLlGbRZ","signatures":["ICLR.cc/2018/Conference/Paper769/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper extends the previous work on CycleGAN by coupling it with adversarial adaptation approaches. The extension includes a new feature and semantic loss in the overall objective of the CycleGAN. While this extension is straightforward, it is novel. The experimental validation is extensive and clearly shows the benefits of the proposed extension. ","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper proposes  a natural extension of the CycleGAN approach. This is achieved by leveraging the feature and semantic losses to achieve a more realistic image reconstruction. The experiments show that including these additional losses is critical for improving the models performance.  The paper is very well written and technical details are well described and motivated. It would be good to identify the cases where the model fails and comment on those. For instance, what if the source data cannot be well reconstructed from adapted target data? What are the bounds of the domain discrepancy in this case? ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]}},{"tddate":null,"ddate":null,"tmdate":1512222746707,"tcdate":1511987873036,"number":2,"cdate":1511987873036,"id":"SyFscqngM","invitation":"ICLR.cc/2018/Conference/-/Paper769/Official_Review","forum":"SktLlGbRZ","replyto":"SktLlGbRZ","signatures":["ICLR.cc/2018/Conference/Paper769/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Great problem and idea, but without adequate experiments that show that cycle-consistency is the cause of the improvement","rating":"4: Ok but not good enough - rejection","review":"This paper essentially uses CycleGANs for Domain Adaptation. My biggest concern is that it doesn't adequately compare to similar papers that perform adaptation at the pixel level (eg. Shrivastava et al-'Learning from Simulated and Unsupervised Images through Adversarial Training' and Bousmalis et al - 'Unsupervised Pixel-level Domain Adaptation with GANs', two similar papers published in CVPR 2017 -the first one was even a best paper- and available on arXiv since December 2016-before CycleGANs). I believe the authors should have at least done an ablation study to see if they cycle-consistency loss truly makes a difference on top of these works-that would be the biggest selling point of this paper. The experimental section had many experiments, which is great. However I think for semantic segmentation it would be very interesting to see whether using the adapted synthetic GTA5 samples would improve the SOTA on Cityscapes. It wouldn't be unsupervised domain adaptation, but it would be very impactful. Finally I'm not sure the oracle (train on target) mIoU on Table 2 is SOTA, and I believe the proposed model's performance is really far from SOTA.\n\nPros:\n* CycleGANs for domain adaptation! Great idea!\n* I really like the work on semantic segmentation, I think this is a very important direction\n\nCons:\n* I don't think Domain separation networks is a pixel-level transformation-that's a feature-level transformation, you probably mean to use Bousmalis et al. 2017. Also Shrivastava et al is missing from the image-level papers.\n* the authors claim that Bousmalis et al, Liu & Tuzel and Shrivastava et al ahve only been shown to work for small image sizes. There's a recent work by Bousmalis et al. (Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping) that shows these methods working well (w/o cycle-consistency) for settings similar to semantic segmentation at a relatively high resolution. Also it was mentioned that these methods do not necessarily preserve content, when pixel-da explicitly accounts for that with a task loss (identical to the semantic loss used in this submission)\n* The authors talk about the content similarity loss on the foreground in Bousmalis et al. 2017, but they could compare to this method w/o using the content similarity or using a different content similarity tailored to the semantic segmentation tasks, which would be trivial.\n* Math seems wrong in (4) and (6). (4) should be probably have a minus instead of a plus. (6) has an argmin of a min, not sure what is being optimized here. In fact, I'm not sure if eg you use the gradients of f_T for training the generators?\n* The authors mention that the pixel-da approach cross validates with some labeled data. Although I agree that is not an ideal validation, I'm not sure if it's equivalent or not the authors' validation setting, as they don't describe what that is.\n* The authors present the semantic loss as novel, however this is the task loss proposed by the pixel-da paper.\n* I didn't understand what pixel-only and feat-only meant in tables 2, 3, 4. I couldn't find an explanation in captions or in text","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]}},{"tddate":null,"ddate":null,"tmdate":1512222746751,"tcdate":1511724779809,"number":1,"cdate":1511724779809,"id":"S1Elwq_xf","invitation":"ICLR.cc/2018/Conference/-/Paper769/Official_Review","forum":"SktLlGbRZ","replyto":"SktLlGbRZ","signatures":["ICLR.cc/2018/Conference/Paper769/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novelty incremental, results encouraging, writing could be improved","rating":"5: Marginally below acceptance threshold","review":"This paper proposed a domain adaptation approach by extending the CycleGAN with 1) task specific loss functions and 2) loss imposed over both pixels and features. Experiments on digit recognition and semantic segmentation verify the effectiveness of the proposed method.\n\nStrengths:\n+ It is a natural and intuitive application of CycleGAN to domain adaptation. \n+ Some of the implementation techniques may be useful for the future use of CycleGAN or GAN in other applications, e.g., the regularization over both pixels and features, etc.\n+ The experimental results are superior over the past.\n+ The translated images in Figure 6 are amazing. Could the authors show more examples and include some failure cases (if any)?\n\nWeaknesses:\n- The presentation of the paper could be improved. I do not think I can reproduce the experimental results after reading the paper more than twice. Many details are missing and some parts are confusing or even misleading.  As below, I highlight a few points and the authors are referred to the comments by Cedric Nugteren for more suggestions.\n\n-- Equation (4) is incorrect.\n-- In the introduction and approach sections, it reads like a big deal to adapt on both the pixel and feature levels. However, the experiments fail to show that these two levels of adaptation are complementary to each other. Either the introduction is a little misleading or the experiments are insufficient. \n-- What does the “image-space adaptation” mean?\n-- There are three fairly sophisticated training stages in Section 4.2. However, the description of the three stages are extremely short and ambiguous. \n-- What are exactly the network architectures used in the experiments?\n\n- The technical contribution seems like only marginal innovative. \n- The experiments adapting from MNIST to SVHN would be really interesting, given that the MNIST source domain is not as visually rich as the SVHN target. Have the authors conducted the corresponding experiments? How are the results? \n\nSummary:\nThe proposed method is a natural application of CycleGAN to domain adaptation. The technical contribution is only marginal. The results on semantic segmentation are encouraging and may motivate more research along this direction. It is unfortunate that the paper writing leaves many parts of the paper unclear. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]}},{"tddate":null,"ddate":null,"tmdate":1510921593050,"tcdate":1510921593050,"number":2,"cdate":1510921593050,"id":"Sy-YBUn1G","invitation":"ICLR.cc/2018/Conference/-/Paper769/Public_Comment","forum":"SktLlGbRZ","replyto":"BJxW87myM","signatures":["~Lei_Tai1"],"readers":["everyone"],"writers":["~Lei_Tai1"],"content":{"title":"Feature and pixel loss","comment":"This is a public comment. I agree with the comments of Cedric Nugteren especially for the feature and pixel loss. \n\nIn addition:\n1. In the last paragraph of section 3, it said CyCADA can be viewed as CycleGan augmented with an additional task loss, which I think should be the semantic loss here? But in Table I, CyCADA also covers feature loss and CycleGAN doesn't. \nFrom the Equation 5, the loss should be presented as the feature or the pixel loss explicitly. Otherwise, the training in stages from section 4.2 really makes me confused.\n\n2. In right side of Equation 4, L_task(f_s, X_s, p(fs, Xs)) is not a loss function if f_s is pre-trained. It just outputs a constant."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]}},{"tddate":null,"ddate":null,"tmdate":1510319855769,"tcdate":1510319607834,"number":1,"cdate":1510319607834,"id":"BJxW87myM","invitation":"ICLR.cc/2018/Conference/-/Paper769/Public_Comment","forum":"SktLlGbRZ","replyto":"SktLlGbRZ","signatures":["~Cedric_Nugteren1"],"readers":["everyone"],"writers":["~Cedric_Nugteren1"],"content":{"title":"Some remarks on the presentation of the work","comment":"\nThis is a not a review of the work, but just a comment with some suggestions to improve the presentation of the work. Currently there are some things unclear and inconsistent in the presentation; I believe improving this can make the contributions of the paper a lot clearer. Here are some comments (not in any particular order):\n\n* Figure 2 (the diagram with images, networks, and losses) is really helpful. However, it would help if the symbols used in the paper (Xs, Xt, Yt, Lgan, Ft, Lcyc, etc.) are added to make it easier to map the equations to the figure. Also, it would be good to extend the figure with the second cycle loss. I understand that that takes extra space, but it might be worth it. Furthermore, it would be good to picture the missing parts as well (Lsem, Fs) for completeness. Finally, perhaps explicitly adding all networks would help clarifying the overall structure (Ds, Dt are missing now).\n\n* In equation 1 (task-loss) it would be clarifying to put large square brackets around the \"-sum()\" term. Now the equation could be read as \"expectation minus the sum of ...\" whereas it should read as \"expectation of the negated sum of ...\".\n\n* Equation 4 has a typo. The left-hand side contains a Gt->s component but it is not on the right-hand side. It would be furthermore helpful to clarify what the two individual components in this equation represent.\n\n* It would be good to make explicit early on that the source model fs has fixed weights throughout the domain adaptation training. Is this also the case in related work?\n\n* At first it is unclear how the \"pixel\" and \"feature\" approaches discussed in the experiment section map to the explanation in section 3 and figure 2. It would be good to clarify this in section 3 and perhaps in a second version of figure 2? There are some unclarities here:\n  - Are all loss components trained for the feature case?\n  - How are the features obtained? Using the task-model? What if these features are not useful for the target domain (e.g. color information not present in MNIST features but might be useful for SVHN)?\n  - Which networks are shared between the pixel and feature approaches?\n  - How are the two losses optimized - one after each other? Interleaved? Jointly?\n\n* There seem to be some assumptions on the domain change with respect to the fact that the source labels Ys do not need to be transformed to accommodate changed made on the input data Xs by the transformation Gs->t (e.g. no translation, warping, etc.). It would be nice if this is mentioned explicit and perhaps discussed (is Gs->t constrained in such a way?).\n\n* The first paragraph under the section \"Implementation details\" doesn't seem to be an implementation detail at all, but rather a property of the approach.\n\n* The network architecture used (FCN) is quite old in terms of semantic segmentation (2015). It would be interesting to see how this affects your final accuracy. Is this why the only comparison is against \"FCNs in the wild\", perhaps they use the same architecture? If not, how much of your improvement is related to the architecture change and how much related to the method?\n\n* Table 3 contains some results which are better than the oracle (pole, pedestrian, bicycle). Although possible, it would be good to mention this explicitly to make sure this is not a typo.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]}},{"tddate":null,"ddate":null,"tmdate":1509739113043,"tcdate":1509134416795,"number":769,"cdate":1509739110384,"id":"SktLlGbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SktLlGbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation","abstract":"Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.\nRecent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at  mapping images between domains, even without the use of aligned image pairs.\nWe propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.\nCyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.  Our model can be applied in a variety of visual recognition and prediction settings.\nWe show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.","pdf":"/pdf/94b9b7dd1e4802be6138644981ddfe4e6f27b416.pdf","TL;DR":"An unsupervised domain adaptation approach which adapts at both the pixel and feature levels","paperhash":"anonymous|cycada_cycleconsistent_adversarial_domain_adaptation","_bibtex":"@article{\n  anonymous2018cycada:,\n  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SktLlGbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper769/Authors"],"keywords":["domain adaptation","unsupervised learning","classification","semantic segmentation"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}