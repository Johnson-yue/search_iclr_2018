{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222723066,"tcdate":1512094777036,"number":3,"cdate":1512094777036,"id":"H1ZHnV0xz","invitation":"ICLR.cc/2018/Conference/-/Paper709/Official_Review","forum":"S1xzob-0W","replyto":"S1xzob-0W","signatures":["ICLR.cc/2018/Conference/Paper709/AnonReviewer2"],"readers":["everyone"],"content":{"title":"limited novelty","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThe paper presents analytical results comparing various initialization and training methods of transferring knowledge from VGG network to a smaller student network by replacing blocks of layers with single layers. \n\nReview:\nThe paper is well written and explains the experiments well, however the results are somehow obvious. The method also lacks novelty and looks like a generalized version of fitnets.\nThe insights into initialization schemes are valuable, however I don't think this is enough for conference track.\n\nComments:\nIntro mentions that shortcut connections overcome overfitting whereas they are important for preserving signal flow and enabling training of deeper architectures. \nPage4: fine-tine->fine-tune\nDetails of the experimental setup are missing e.g. optimizer.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression","abstract":"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\nWe propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","pdf":"/pdf/dec02177fc07db37b256c63968e6526dee08b5fb.pdf","TL;DR":"We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage","paperhash":"anonymous|deep_net_triage_assessing_the_criticality_of_network_layers_by_structural_compression","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xzob-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper709/Authors"],"keywords":["Deep Compression","Deep Learning","Parent-Teacher Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222723105,"tcdate":1512004599608,"number":2,"cdate":1512004599608,"id":"HkxZ2Ahxz","invitation":"ICLR.cc/2018/Conference/-/Paper709/Official_Review","forum":"S1xzob-0W","replyto":"S1xzob-0W","signatures":["ICLR.cc/2018/Conference/Paper709/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper presented 5 methods for doing 'triaging' or block layer compression for deep networks. However it is not super clear the central thesis of this paper.","rating":"4: Ok but not good enough - rejection","review":"This paper presented 5 methods for doing 'triaging' or block layer compression for deep networks. However it is not super clear the central thesis of this paper.\n\nThis paper proposes 5 ways to perform local block layer compression. Methods include using random weights or mean of the original weights. Global fine-tuning and student-teacher type distilling algorithms were also employed for for some of the algorithms. \n\nThe results show that globally fine-tuned compression works better than non globally fine tuned algorithms and sometimes the compressed network does better than the original uncompressed networks. \n\nWhile demonstrating that compressed networks performs better than the parent is very exciting, it is unclear if those results are statistically significant. The results on Cifar10 is not near to the current state-of-the art. Does the author believe compression of certain layers would always improve performance for all datasets including potentially ImageNet?\n\nIn addition, how do we assess the criticality of the layers? Is it simply by the empirical plots of Figure 2?\n\nOverall this is an ok paper, but it is a bit unclear of the central thesis or take-away of the paper. \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression","abstract":"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\nWe propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","pdf":"/pdf/dec02177fc07db37b256c63968e6526dee08b5fb.pdf","TL;DR":"We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage","paperhash":"anonymous|deep_net_triage_assessing_the_criticality_of_network_layers_by_structural_compression","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xzob-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper709/Authors"],"keywords":["Deep Compression","Deep Learning","Parent-Teacher Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222723147,"tcdate":1511745599666,"number":1,"cdate":1511745599666,"id":"HyuH_1Kgz","invitation":"ICLR.cc/2018/Conference/-/Paper709/Official_Review","forum":"S1xzob-0W","replyto":"S1xzob-0W","signatures":["ICLR.cc/2018/Conference/Paper709/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple, clean paper proposing network compression. Lack of comparison to literature, and novelty/significance is in doubt.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method to compress a block of layers in a NN (particularly CNNs) evaluates several different sub-approaches. It is easy to follow the paper. Given experiments are nicely setup with intuitive explanations. My main concern about the paper is novelty/importance and lack of experimental comparison to other techniques attacking similar complexity problems.\n\nAuthors provide a good intro for the problem and existing common approaches addressing network complexity. They locate themselves well in the literature. They provide a simple compact explanation of what their methods are in section 2.3. I find the coding/notation of the sub-methods a little encrypted though. Simple English or better encoding could be used for better readability. Experimental results are explained pretty well. \n\nI think the given results/messages are valuable to the community. However the generalizability to different models/network architectures is in question as the paper is mostly based on VGG16. Also the methods/significance seem to be more like an experimental paper and not significant enough for ICLR. I am borderline about this. \n\nComparison between the proposed methods is nicely done but comparison to other literature pruning/compressing models is not experimentally shown. Ideally we expect a reader designing a new architecture to have an idea for which way to go for model compression after reading this paper—results are good to decide between CLRW –STN, but not informative on how they compare to literature. I think this is a significant missing point. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression","abstract":"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\nWe propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","pdf":"/pdf/dec02177fc07db37b256c63968e6526dee08b5fb.pdf","TL;DR":"We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage","paperhash":"anonymous|deep_net_triage_assessing_the_criticality_of_network_layers_by_structural_compression","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xzob-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper709/Authors"],"keywords":["Deep Compression","Deep Learning","Parent-Teacher Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739148331,"tcdate":1509133063823,"number":709,"cdate":1509739145601,"id":"S1xzob-0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1xzob-0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression","abstract":"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\nWe propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","pdf":"/pdf/dec02177fc07db37b256c63968e6526dee08b5fb.pdf","TL;DR":"We seek to understand learned representations in compressed networks via an experimental regime we call deep net triage","paperhash":"anonymous|deep_net_triage_assessing_the_criticality_of_network_layers_by_structural_compression","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xzob-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper709/Authors"],"keywords":["Deep Compression","Deep Learning","Parent-Teacher Networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}