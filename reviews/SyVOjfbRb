{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222823533,"tcdate":1511829233618,"number":2,"cdate":1511829233618,"id":"rkcg14qlz","invitation":"ICLR.cc/2018/Conference/-/Paper939/Official_Review","forum":"SyVOjfbRb","replyto":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference/Paper939/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Another application of LSH sampling","rating":"4: Ok but not good enough - rejection","review":"  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss.\nThe basic form of SGD selects an example uniformly.   Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient.\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].\n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger.\n\n  The paper uses LSH structures, computed over the set of examples,\n to quickly sample examples with large inner products with the current parameter vector \\theta.   Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity.\n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings).\n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited. The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.  There are theorems,  but they are trivial, straightforward applications of importance sampling. \n\n The paper is not well written. The presentation is much more complex that need be. References to classic weighted sampling are \n\n  The application is limited to certain loss functions for which we can compute LSH structures.  This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]}},{"tddate":null,"ddate":null,"tmdate":1512222823579,"tcdate":1511740906904,"number":1,"cdate":1511740906904,"id":"HkmgURdlf","invitation":"ICLR.cc/2018/Conference/-/Paper939/Official_Review","forum":"SyVOjfbRb","replyto":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference/Paper939/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Creative Paper Worth Sharing","rating":"8: Top 50% of accepted papers, clear accept","review":"Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well-founded and well-argued.\n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. \n\nExperiments: appreciated the wall clock timings.\n\nSGD comparison: “fixed learning rate.” Didn't see how the initial (well constant here) step size was tuned? Why not use the more standard 1/t decay?\n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Legend backwards?\n\nWhy were so many of the chosen datasets have so few training examples?\n\nPaper is mostly very clearly written, though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]}},{"tddate":null,"ddate":null,"tmdate":1510092384694,"tcdate":1509137261557,"number":939,"cdate":1510092362165,"id":"SyVOjfbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}