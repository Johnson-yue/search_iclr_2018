{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708343,"tcdate":1511822057453,"number":3,"cdate":1511822057453,"id":"S1ZlQfqeM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer3"],"readers":["everyone"],"content":{"title":"empirical evaluation of recurrent models and RL for molecule design","rating":"6: Marginally above acceptance threshold","review":"Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules. The experiments seem extensive, using many recently proposed RL methods, and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception.  \n\nOriginality and significance: \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically. \n\nThe paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques.\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"Despite many advances in the area of computational molecular design, significant challenges remain to render their application commonplace.  This work expands upon recent advances in deep neural networks and reinforcement learning strategies for sequence generation to investigate how to apply these techniques to the chemistry domain with a focus on challenges in drug discovery.  This work proposes 19 benchmarks selected by subject experts to enable usage by those without a background in chemistry, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in algorithms that can be applied to drug discovery and beyond this focused chemistry community.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/2f0a00428a5429f4c81db1998a05bab4327e8542.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222708381,"tcdate":1511786648334,"number":2,"cdate":1511786648334,"id":"rkejdYtxz","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is a solid paper about model evaluation in the chemical domain. ","rating":"7: Good paper, accept","review":"Summary:\nThis work is about model evaluation for molecule generation and design. 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design.\n\non the positive side:\nThe paper is well written, quality and clarity of the work are good. The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation. It is investigated how several RL strategies perform on a large, standardized data set. Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed.  An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym. \n\n\non the negative side:\nThere is no new novel contribution on the methods side.  \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 â€”> see Fig.1\npage 4just before equation 8: the the","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"Despite many advances in the area of computational molecular design, significant challenges remain to render their application commonplace.  This work expands upon recent advances in deep neural networks and reinforcement learning strategies for sequence generation to investigate how to apply these techniques to the chemistry domain with a focus on challenges in drug discovery.  This work proposes 19 benchmarks selected by subject experts to enable usage by those without a background in chemistry, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in algorithms that can be applied to drug discovery and beyond this focused chemistry community.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/2f0a00428a5429f4c81db1998a05bab4327e8542.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222708421,"tcdate":1510228750269,"number":1,"cdate":1510228750269,"id":"rkUfmabyM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a set of benchmarks for molecular design, and compares different deep models against them. The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks. The paper does not seem to include any method development.\n\nThe paper suffers from a lack of focus. Several existing models are discussed to some length, while the benchmarks are introduced quite shortly. The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks? The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title. Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium. I don't see any reason to exlude these. What does \"biological activities on 11538 targets\" mean? \n\nThe paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used. Are the test molecules somehow generated in a directed or undirected fashion? Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion? There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties. Currently it's unclear which type of problems are solved here.\n\nTable 1 lists 5 models, while fig 3 contains 7, why the discrepancy? In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83). Variances should be added, and preferably more than 3 initialisations used.\n\nOverall this is an interesting paper, but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules either.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"Despite many advances in the area of computational molecular design, significant challenges remain to render their application commonplace.  This work expands upon recent advances in deep neural networks and reinforcement learning strategies for sequence generation to investigate how to apply these techniques to the chemistry domain with a focus on challenges in drug discovery.  This work proposes 19 benchmarks selected by subject experts to enable usage by those without a background in chemistry, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in algorithms that can be applied to drug discovery and beyond this focused chemistry community.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/2f0a00428a5429f4c81db1998a05bab4327e8542.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739183099,"tcdate":1509130433985,"number":646,"cdate":1509739180434,"id":"HkcTe-bR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"Despite many advances in the area of computational molecular design, significant challenges remain to render their application commonplace.  This work expands upon recent advances in deep neural networks and reinforcement learning strategies for sequence generation to investigate how to apply these techniques to the chemistry domain with a focus on challenges in drug discovery.  This work proposes 19 benchmarks selected by subject experts to enable usage by those without a background in chemistry, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in algorithms that can be applied to drug discovery and beyond this focused chemistry community.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/2f0a00428a5429f4c81db1998a05bab4327e8542.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}