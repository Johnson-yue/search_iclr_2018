{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222767654,"tcdate":1511798700688,"number":3,"cdate":1511798700688,"id":"rJNnP3Fgz","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice topic, but inadequate investigation","rating":"3: Clear rejection","review":"The paper presents a novel model called Neural Variational Sparse Topic Model (NVSTM), that captures word-level sparse representation by employing a Laplacian prior, and enriches semantics by using external word embeddings. The experimental results have shown that NVSTM yields good results in terms of document classification and sparse word representation against 3 related models. \n\nThere are several issues that the authors should address in the future.\n-\tWord co-occurrence information is very important to shape topics. The paper also mentioned about that.  However, in model perspective, it is surprised that NVSTM does not have any variables (or hyper-parameters) to model each document. It means that NVSTM seems not to capture the word co-occurrence information. \n-\tThe paper only uses the variational autoencoder (VAE) to do inference for NVSTM. The authors should detail the approximate posterior (q), variational parameters $\\Theta$, model parameters $\\Phi$, and objective function $L$ in NVSTM. It is confusing when the encoderâ€™s distribution q(s_n) and the prior p(s_n) of the latent variable $s_n$ are both Laplace(0,b_n)  with the same parameter. There might be some typos here. \n-\tThe assumptions of NVSTM are not different between short and normal text. Why does the paper target at short-text? \n-\tThe paper should survey more about short-text and word embedding as prior knowledge in topic models. The authors should discuss related work, and take some baselines which use word embedding as prior into comparison.\n-\tThe authors should do experiment on more datasets to have higher confidence. Many short and normal text datasets are freely available. \n-\tIn the experiment settings, what is the size of the hidden layer? Is it the number of topics $K$? \n-\tThe settings for the baselines were not described in the paper. To make a fair comparison, the settings for the baselines should be chosen thoughtfully.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge due to the data sparsity problem induced by the characteristics of short texts. A large number of topic models have been proposed to deal with the data sparsity problem in short texts. However, the complex and rigorous inference algorithm become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a popular model sparsityenhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its blackbox inference process. Experimental results onWeb Snippets and 20NewsGroups datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/bb1ac0734b15428ee494923eb41b754d432e76aa.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222767695,"tcdate":1511742004309,"number":2,"cdate":1511742004309,"id":"SynN9CuxG","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The manuscript needs to be clarified more precisely.","rating":"3: Clear rejection","review":"This paper proposes a neural variational sparse topic model to model topics of short text via variational auto encoding and sparse coding. \n\nOverall, many parts of the paper need to be further clarified or justified. I tried to list some major and minor concerns/comments below:\n\n* 'Collapsed document code \\theta': In section 3.2, I'm not sure what 'collapse the document code' means, but from the context, I assume that it means removing prior of the word codes. This is still a valid choice, however, there are many pieces of evidence that removing prior of word codes may lead to a serious overfitting problems such as the case of pLSI. Maybe the structure of the variational distribution relaxes the overfitting problem, but even it's the case, this choice is not justified well in the text. Additionally, if we remove the prior over word codes, then the inference can be tractable in some way I guess, which means we may not need the variational approximation method.\n\n* The word codes need to be non-negative to obtain distributions in Table 2, however, it is not clear how this non-negativity can be obtained during the inference procedure. In the STC, the authors made a strong assumption of non-negativity on document and word codes in order to keep the interpretability of the model. As a consequence, they had to rely on some property of convex function during training, while I cannot find these detail in the current manuscript. In a similar sense, which projection method has been used to project topic dictionary after relu layer? Did you normalise the vector using the sum of elements?\n\n* Reparameterisation: From Equation (5) I found that the natural log can take a negative value if the epsilon is greater than 0.5, over which the log is undefined. In addition, why do you need sign function since the epsilon is always positive by definition?\n\n* Non-stochastic beta?: the word codes are defined as stochastic variables. Why not for the topic dictionary beta? Which variable needs to be stochastic or not?\n\n* The second term in RHS of Equation 3: 's' should be removed from the term if you meant the marginal probability of observations here.\n\n* Experiment details: How the parameters of models are chosen? Are the default parameters used throughout the experiments? \n\n* Please use a single character to denote a matrix. Word embedding matrix WE seems a multiplication of two matrices.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge due to the data sparsity problem induced by the characteristics of short texts. A large number of topic models have been proposed to deal with the data sparsity problem in short texts. However, the complex and rigorous inference algorithm become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a popular model sparsityenhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its blackbox inference process. Experimental results onWeb Snippets and 20NewsGroups datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/bb1ac0734b15428ee494923eb41b754d432e76aa.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222767738,"tcdate":1510900075543,"number":1,"cdate":1510900075543,"id":"ByQd-bhJz","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good illustration of deep-neural network machinery but poor connection to related/prior work","rating":"5: Marginally below acceptance threshold","review":"\"without complicated mathematical inference\" (on end of page 10)\nHeaven forbid we should expect our graduate students to know a bit of statistics!  Sorry,  I am being sarcastic.  Yes, I agree its good to automate as much of the \"tedium\" as possible.  Perhaps word this differently.\n\nFor short text, you missed a whole body of work that uses word background knowledge, like word correlations or embeddings, to regularise topics or as an informed prior on topics.  A lot of progress on this recently, and it produces dramatic improvements using GloVe embeddings, for instance, for the prior or regulariser.  Given the lack of comparison with these, it makes your experimental work lacking.\n\nThere are also many more deep NN methods for topic models,\ne.g., DocNADE.\n\nTable 2 looks very informative, but the words in the table are illegible.\n\nPlease properly capitalise some words in the references. The good Reverend bayes I'm sure is turning in his grave.\n\nSection 3.4 is a nice trick.  There are other ways like Concrete.\n\nPros:\n* its a good illustration of deep-neural network machinery at work, and well put together\n*  the experimental results show it works\nCons:\n* a lot of recent work here, barely mentioned\n* other deep neural topic models\n* traditional topic models effectively using word embeddings\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge due to the data sparsity problem induced by the characteristics of short texts. A large number of topic models have been proposed to deal with the data sparsity problem in short texts. However, the complex and rigorous inference algorithm become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a popular model sparsityenhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its blackbox inference process. Experimental results onWeb Snippets and 20NewsGroups datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/bb1ac0734b15428ee494923eb41b754d432e76aa.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1509739498365,"tcdate":1508919731997,"number":79,"cdate":1509739495713,"id":"Sy33KapTb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge due to the data sparsity problem induced by the characteristics of short texts. A large number of topic models have been proposed to deal with the data sparsity problem in short texts. However, the complex and rigorous inference algorithm become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a popular model sparsityenhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its blackbox inference process. Experimental results onWeb Snippets and 20NewsGroups datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/bb1ac0734b15428ee494923eb41b754d432e76aa.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}