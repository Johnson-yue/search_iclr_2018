{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222677845,"tcdate":1511821061683,"number":2,"cdate":1511821061683,"id":"Hk6bJG9gf","invitation":"ICLR.cc/2018/Conference/-/Paper511/Official_Review","forum":"BJk59JZ0b","replyto":"BJk59JZ0b","signatures":["ICLR.cc/2018/Conference/Paper511/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper introduces a modified actor-critic algorithm where a “guide actor” uses approximate second order methods to aid computation. The experimental results are similar to previously proposed methods. \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results. However, the method is not properly motivated. As far as I can tell, the paper never answers the questions: Why do we need a guide actor? What problem does the guide actor solve? \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and (2) it’s not clear why we should want to use second-order methods in reinforcement learning in the first place. Using second order methods is not an end in itself. The experimental results show the authors have found a way to use second order methods without making performance *worse*. Given the high variability of deep RL, they have not convincingly shown it performs better.\n\nThe paper does not discuss the computational cost of the method. How does it compare to other methods? My worry is that the method is more complicated and slower than existing methods, without significantly improved performance.\n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Guide Actor-Critic for Continuous Control","abstract":"Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.\n","pdf":"/pdf/c7eaf540c0684b0f472975992d04b1304ab38494.pdf","TL;DR":"This paper proposes a novel actor-critic method that uses Hessians of a critic to update an actor.","paperhash":"anonymous|guide_actorcritic_for_continuous_control","_bibtex":"@article{\n  anonymous2018guide,\n  title={Guide Actor-Critic for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJk59JZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper511/Authors"],"keywords":["Reinforcement learning","actor-critic","continuous control"]}},{"tddate":null,"ddate":null,"tmdate":1512222677892,"tcdate":1511815983212,"number":1,"cdate":1511815983212,"id":"rJwNjgqef","invitation":"ICLR.cc/2018/Conference/-/Paper511/Official_Review","forum":"BJk59JZ0b","replyto":"BJk59JZ0b","signatures":["ICLR.cc/2018/Conference/Paper511/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A promising technique, using hessian of the critic, for learning the actor, for continuous control, with competitive results ","rating":"6: Marginally above acceptance threshold","review":"\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL.  They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic). They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best).\n\nThe paper has a technical focus.\n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches.\n\n- Compared to many existing techniques, on 9 tasks\n\ncons:\n\n- no mention of time costs, except that for more samples, S > 1, for\n taylor approximation, it can be very expensive.\n\n- one would expect more information to strictly improve performance,\n  but the results are a bit mixed (perhaps due to convergence to local\n  optima and both actor and critic being learned at same time, \n  or the Gaussian assumptions, etc.).\n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation).\n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)?\n\n- How were the tasks selected? Are they all the continuous control\n  tasks available in open ai?\n\n\n \n\n\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Guide Actor-Critic for Continuous Control","abstract":"Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.\n","pdf":"/pdf/c7eaf540c0684b0f472975992d04b1304ab38494.pdf","TL;DR":"This paper proposes a novel actor-critic method that uses Hessians of a critic to update an actor.","paperhash":"anonymous|guide_actorcritic_for_continuous_control","_bibtex":"@article{\n  anonymous2018guide,\n  title={Guide Actor-Critic for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJk59JZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper511/Authors"],"keywords":["Reinforcement learning","actor-critic","continuous control"]}},{"tddate":null,"ddate":null,"tmdate":1509739262821,"tcdate":1509124743538,"number":511,"cdate":1509739260165,"id":"BJk59JZ0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJk59JZ0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Guide Actor-Critic for Continuous Control","abstract":"Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.\n","pdf":"/pdf/c7eaf540c0684b0f472975992d04b1304ab38494.pdf","TL;DR":"This paper proposes a novel actor-critic method that uses Hessians of a critic to update an actor.","paperhash":"anonymous|guide_actorcritic_for_continuous_control","_bibtex":"@article{\n  anonymous2018guide,\n  title={Guide Actor-Critic for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJk59JZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper511/Authors"],"keywords":["Reinforcement learning","actor-critic","continuous control"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}