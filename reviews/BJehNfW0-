{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222776548,"tcdate":1512033672311,"number":3,"cdate":1512033672311,"id":"B1g5pBTxz","invitation":"ICLR.cc/2018/Conference/-/Paper825/Official_Review","forum":"BJehNfW0-","replyto":"BJehNfW0-","signatures":["ICLR.cc/2018/Conference/Paper825/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper uses birthday paradox to show experimentally that some most popular GAN architectures generate distributions with fairly low support. Also some theoretical explanation for the phenomena is given ","rating":"7: Good paper, accept","review":"The article \"Do GANs Learn the Distribution? Some Theory and Empirics\" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images. The authors argue that GANs in fact generate the distributions with fairly low support.\n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample. This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017). The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective.\n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets. We definitely see many very similar images in fairly small sample generated. So, the general claim is supported. However, if you look closely at some pictures, you can see that they are very different though reported as similar. For example, some deer or truck pictures. That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates.\n\nTo sum up, I think that the general idea looks very natural and the results are supportive. On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do GANs learn the distribution? Some Theory and Empirics","abstract":"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","pdf":"/pdf/2af196ca8ef810d0e36a9046bd78672ce3c8c76d.pdf","TL;DR":"We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well.","paperhash":"anonymous|do_gans_learn_the_distribution_some_theory_and_empirics","_bibtex":"@article{\n  anonymous2018do,\n  title={Do GANs learn the distribution? Some Theory and Empirics},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJehNfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper825/Authors"],"keywords":["Generative Adversarial Networks","mode collapse","birthday paradox","support size estimation"]}},{"tddate":null,"ddate":null,"tmdate":1512222776584,"tcdate":1511813123500,"number":2,"cdate":1511813123500,"id":"B1jWee9eM","invitation":"ICLR.cc/2018/Conference/-/Paper825/Official_Review","forum":"BJehNfW0-","replyto":"BJehNfW0-","signatures":["ICLR.cc/2018/Conference/Paper825/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Extremely interesting topic; insightful but limited method and theory","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples. The main goal is to quantify mode collapse in state-of-the-art generative models. The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.\nUsing the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1).\nThe results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.\nBidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs.\nFinally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator.\n\nThis is a very interesting area and exciting work. The main idea behind the proposed test is very insightful. The main theoretical contribution stimulates and motivates much needed further research in the area. In my opinion both contributions suffer from some significant limitations. However, given how little we know about the behavior of modern generative models, it is a good step in the right direction.\n\n\n1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity. The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2. My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity. Anecdotally, real-life distributions are far from uniform, so this should be a common issue. I would still use the test as a part of a suite of measurements, but I would not solely rely on it. I feel that the authors should give a more prominent disclaimer to potential users of the test.\n\n2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing. The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution. As long as they yield collisions at the same rate, these two generative models are ‘equally diverse’. Isn’t coverage of equal importance?\n\n3. The other main contribution of the paper is Theorem 3, which shows—via a very particular construction on the generator and encoder—that bidirectional GANs can also suffer from serious mode collapse. I welcome and are grateful for any theory in the area. This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice. In light of this, the statement in the introduction that “encoder-decoder training objectives cannot avoid mode collapse” might need to be qualified. In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do GANs learn the distribution? Some Theory and Empirics","abstract":"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","pdf":"/pdf/2af196ca8ef810d0e36a9046bd78672ce3c8c76d.pdf","TL;DR":"We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well.","paperhash":"anonymous|do_gans_learn_the_distribution_some_theory_and_empirics","_bibtex":"@article{\n  anonymous2018do,\n  title={Do GANs learn the distribution? Some Theory and Empirics},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJehNfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper825/Authors"],"keywords":["Generative Adversarial Networks","mode collapse","birthday paradox","support size estimation"]}},{"tddate":null,"ddate":null,"tmdate":1512222776626,"tcdate":1511781811965,"number":1,"cdate":1511781811965,"id":"rkhhruYgM","invitation":"ICLR.cc/2018/Conference/-/Paper825/Official_Review","forum":"BJehNfW0-","replyto":"BJehNfW0-","signatures":["ICLR.cc/2018/Conference/Paper825/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Do GANs learn the distribution? Some Theory and Empirics","rating":"4: Ok but not good enough - rejection","review":"The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution. Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support. The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally. The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > \\sqrt{N}. The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2. One should note that the birthday theorem assumes uniform sampling. However, in the current context it is not clear whether the output distribution would uniform over some set. Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity. Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity. Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above. A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN.\n\n\nEvaluation: \nSignificance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value. Unfortunately, the ideas in this paper does not contribute to this discussion significantly in my opinion. \n\nClarity: The paper is written well and the issues raised are well motivated and proper background is given. However, certain crucial discussions related to estimation technique using birthday theorem are missing.\n\nOriginality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new. However, the idea has limitations that leads to raising doubts about the quality of the estimate.\n\nQuality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs. However, in my opinion this goal has not been convincingly achieved. The estimation method based on birthday theorem has its limitations and the discussion on quality of the estimate achieved using this idea is missing.\n\nAs discussed just before Theorem 1, that the estimate would be off if the output distribution is non-uniform and “a lot of the probability being assigned to a few images”. The discussion on why it is fine to assume that the output distribution is uniform (or close to uniform) is crucially missing. Also, the authors make the statement that “such non-uniformity is the only failure mode of birthday paradox”. This statement needs to be formalised for any reasonable discussion. Due to the lack of formalisation, it is not clear how to convince oneself that Theorem 1 and 2 support the above statement.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do GANs learn the distribution? Some Theory and Empirics","abstract":"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","pdf":"/pdf/2af196ca8ef810d0e36a9046bd78672ce3c8c76d.pdf","TL;DR":"We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well.","paperhash":"anonymous|do_gans_learn_the_distribution_some_theory_and_empirics","_bibtex":"@article{\n  anonymous2018do,\n  title={Do GANs learn the distribution? Some Theory and Empirics},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJehNfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper825/Authors"],"keywords":["Generative Adversarial Networks","mode collapse","birthday paradox","support size estimation"]}},{"tddate":null,"ddate":null,"tmdate":1509739081066,"tcdate":1509135527863,"number":825,"cdate":1509739078410,"id":"BJehNfW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJehNfW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Do GANs learn the distribution? Some Theory and Empirics","abstract":"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","pdf":"/pdf/2af196ca8ef810d0e36a9046bd78672ce3c8c76d.pdf","TL;DR":"We propose a support size estimator of GANs's learned distribution to show they indeed suffer from mode collapse, and we prove that encoder-decoder GANs do not avoid the issue as well.","paperhash":"anonymous|do_gans_learn_the_distribution_some_theory_and_empirics","_bibtex":"@article{\n  anonymous2018do,\n  title={Do GANs learn the distribution? Some Theory and Empirics},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJehNfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper825/Authors"],"keywords":["Generative Adversarial Networks","mode collapse","birthday paradox","support size estimation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}