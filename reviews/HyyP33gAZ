{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222645053,"tcdate":1511820274890,"number":3,"cdate":1511820274890,"id":"SJ5g2WcgM","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Thorough investigation and extension of class-aware GAN approaches","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper is a thorough investigation of various “class aware” GAN architectures. It purposes a variety of modifications on existing approaches and additionally provides extensive analysis of the commonly used Inception Score evaluation metric.\n\nThe paper starts by introducing and analyzing two previous class aware GANs - a variant of the Improved GAN architecture used for semi-supervised results (named Label GAN in this work) and AC-GAN, which augments the standard discriminator with an auxiliary classifier to classify both real and generated samples as specific classes. \n\nThe paper then discusses the differences between these two approaches and analyzes the loss functions and their corresponding gradients. Label GAN’s loss encourages the generator to assign all probability mass cumulatively across the k-different label classes while the discriminator tries to assign all probability mass to the k+1th output corresponding to a “generated” class. The paper views the generators loss as a form of implicit class target loss.\n\nThis analysis motivates the paper’s proposed extension, called Activation Maximization. It corresponds to a variant of Label GAN where the generator is encouraged to maximize the probability of a specific class for every sample instead of just the cumulative probability assigned to label classes. The proposed approach performs strongly according to inception score on CIFAR-10 and includes additional experiments on Tiny Imagenet to further increase confidence in the results.\n\nA discussion throughout the paper involves dealing with the issue of mode collapse - a problem plaguing standard GAN variants. In particular the paper discusses how variants of class conditioning effect this problem. The paper presents a useful experimental finding - dynamic labeling, where targets are assigned based on whatever the discriminator thinks is the most likely label, helps prevent mode collapse compared to the predefined assignment approach used in AC-GAN / standard class conditioning.\n\nI am unclear how exactly predefined vs dynamic labeling is applied in the case of the Label GAN results in Table 1. The definition of dynamic labeling is specific to the generator as I interpreted it. But Label GAN includes no class specific loss for the generator. I assume it refers to the form of generator - whether it is class conditional or not - even though it would have no explicit loss for the class conditional version. It would be nice if the authors could clarify the details of this setup.\n\nThe paper additionally performs a thorough investigation of the inception score and proposes a new metric the AM score. Through analysis of the behavior of the inception score has been lacking so this is an important contribution as well.\n\nAs a reader, I found this paper to be thorough, honest, and thoughtful. It is a strong contribution to the “class aware” GAN literature.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1511764916456,"tcdate":1511764916456,"number":3,"cdate":1511764916456,"id":"rknnXVKxz","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"B1ZnkAYkz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Discussion on Fréchet Inception Distance (FID)","comment":"Thanks for your reference to Fréchet Inception Distance (FID). \n\nFID measures the distance between two distributions using their means and variances after a fixed mapping, e.g. Inception Network, which works well in practice as illustrated in [1]. As another evaluation metric for generative models, we would add discussions in the revision.\n\nA concern on FID is that the mean and variance of the distribution are not sufficient to represent the whole distribution. That is, for any given distribution, we can always design another distribution which is totally different from the given distribution but has the same mean and variance. We are not sure whether this would cause a problem in practice.\n\nAlso, FID is actually orthogonal to Inception Score and AM Score. FID directly measures the distance between generated distribution and real-data distribution, while Inception Score mainly measures the sample diversity and AM Score mainly measures the sample quality.\n\nAs for the failure of Inception Score on CelebA illustrated in [1], according to our analysis, Inception Score works as a diversity measurement and we might need a more suitable classifier (maybe a classifier trained on a face dataset) to make it work on CelebA. FID seems to have the benefit of being not sensitive to the choice of the mapping function, though it also remains uncertain whether the Inception Network is always the best choice as the mapping function for variant models."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1512222646402,"tcdate":1511477358052,"number":2,"cdate":1511477358052,"id":"HyUOlCNlf","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer2"],"readers":["everyone"],"content":{"title":"good paper with thorough experiments!","rating":"7: Good paper, accept","review":"+ Pros:\n- The paper properly compares and discusses the connection between AM-GAN and class conditional GANs in the literature (AC-GAN, LabelGAN)\n- The experiments are thorough\n- Relation to activation maximization in neural visualization is also properly mentioned\n- The authors publish code and honestly share that they could not reproduce AC-GAN's results and thus using to its best variant AC-GAN* that they come up with. I find this an important practice worth encouraging!\n- The analysis of Inception score is sound.\n+ Cons:\n- A few presentation/clarity issues as below\n- This paper leaves me wonder why AM-GAN rather than simply characterizing D as a 2K-way classifier (1K real vs 1K fake).\n\n+ Clarity: \nThe paper is generally well-written. However, there are a few places that can be improved:\n- In 2.2, the authors mentioned \"In fact, the above formulation is a modified version of the original AC-GAN..\", which puts readers confusion whether they were previously just discussed AC-GAN or AC-GAN* (because the previous paragraph says \"AC-GAN are defined as..\".\n- Fig. 2: it's not clear what the authors trying to say if looking at only figures and caption. I'd suggest describe more in the caption and follow the concept figure in Odena et al. 2016.\n- A few typos here and there e.g. \"[a]n diversity measurement\"\n\n+ Originality: AM-GAN is an incremental work by applying AM to GAN. However, I have no problems with this.\n+ Significance: \n- Authors show that in quantitative measures, AM-GAN is better than existing GANs on CIFAR-10 / TinyImageNet. Although I don't find much a real difference by visually comparing of samples of AM-GAN to AC-GAN*.\n\nOverall, this is a good paper with thorough experiments supporting their findings regarding AM-GAN and Inception score!","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1512222646446,"tcdate":1510962904891,"number":1,"cdate":1510962904891,"id":"HJZyvxT1G","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of Activation Maximization","rating":"5: Marginally below acceptance threshold","review":"The authors describe a new version of a generative adversarial network (GAN) for generating images that is heavily related to class-conditional GAN's. The authors highlight several additional results on evaluation metrics and demonstrate some favorable analyses using their new proposed GAN.\n\nMajor comments:\n1) Unfocused presentation. The paper presents a superfluous and extended background section that needs to be cut down substantially. The authors should aim for a concise presentation of their work in 8 pages. Additionally, the authors present several results (e.g. Section 5.1 on dynamic labeling, Section 6.1 on Inception score) that do not appear to improve the results of the paper, but merely provide commentary. The authors should either defend why these sections are useful or central to the arguments in the paper; otherwise, remove them.\n\n2) Quantitative evaluation highlight small gains. The gains in Table 1 seem to be quite small and additionally there are no error bars so it is hard to assess what is statistically meaningful. Table 2 highlights some error bars but again the gains some quite small. Given that the AM-GAN seems like a small change from an AC-GAN model, I am not convinced there is much gained using this model.\n\n3) MS-SSIM. The authors' discussion of MS-SSIM is fairly confusing. MS-SSIM is a measure of image similarity between a pair of images. However, the authors quote an MS-SSIM for various GAN models in Table 3. What does this number mean?  I suspect the authors are calculating some cumululative statistics across many images, but I was not able to find a description, nor understand what these statistics mean.\n\n4) 'Inception score as a diversity measurement.' This argument is not clear to me. Inception scores can be quite high for an individual image indicating that the image 'looks' like a given class in a discriminative model.  If a generative model always generates a single, good image of a 'dog', then the classification score would be quite high but the generative model would be very poor because the images are not diverse. Hence, I do not see how the inception score captures this property.\n\nIf the authors can address all of these points in a substantive manner, I would consider raising my rating.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1510756264583,"tcdate":1510756264583,"number":2,"cdate":1510756264583,"id":"B1ZnkAYkz","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Fréchet Inception Distance (FID) to evaluate GANs","comment":"[1] proposed the Fréchet Inception Distance (FID) to evaluate GANs which is the Fréchet distance aka Wasserstein-2 distance between the real world and generated samples statistics.  The statistics consists of the first two moments such that the sample quality (first moment match) and variation (second moment match) are covered.\n\nAs highlighted here in Section 6.2, for datasets not covering all ImageNet classes e.g. celebA, CIFAR-10 etc, the entropy of E_x~G[C(x)] is going down not up as soon as a trained GAN starts producing correctly samples falling only in some of the ImageNet classes.  [1] also showed inconistent behaviour of the Inception Score in their experiments (see Appendix A1). Especially interesting here is experiment 6 where a dataset (celebA) is increasingly mixed with ImageNet samples. The Inception Score shows a contradictory behaviour, while the FID captures this contamination, and other disturbance variants, very well. \n\nThe authors should discuss their proposed AM Score compared to the FID, also under consideration that the FID does not\nneed an accordingly pretrained classifier.\n\n[1] https://arxiv.org/abs/1706.08500"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1510092427289,"tcdate":1509862874846,"number":1,"cdate":1509862874846,"id":"Sy7107h0b","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"SJC9zbqCZ","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Thanks for your correction.","comment":"Yeah, it is indeed a mistake. We will correct it in the revision. Thanks. ^_^"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1509720725697,"tcdate":1509720725697,"number":1,"cdate":1509720725697,"id":"SJC9zbqCZ","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Minor comment","comment":"In Table 2, the citation for SGAN should be Huang et al. instead."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]}},{"tddate":null,"ddate":null,"tmdate":1509739321799,"tcdate":1509112919152,"number":403,"cdate":1509739319139,"id":"HyyP33gAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class label information has been empirically proven to be very useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study current variants of GANs that make use of class label information to reveal how class labels and associated losses influence GAN's training. Based on the analysis, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an alternative solution. We conduct a set of controlled experiments to validate our analysis and study the effectiveness of our solution, where AM-GAN achieves the state-of-the-art Inception Score (8.91) on CIFAR-10. Through the experiments, we realize the common used metric for generative models needs further investigation and refinement. Thus we also delve into the widely-used evaluation metrics and accordingly propose a new metric as compensation to make the entire metrics complete and impartial. The proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/2585b1ab923d4694aedc8924c63451dfd7915c41.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}