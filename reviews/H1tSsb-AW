{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222725616,"tcdate":1511818260570,"number":3,"cdate":1511818260570,"id":"rJaGVZ5lz","invitation":"ICLR.cc/2018/Conference/-/Paper712/Official_Review","forum":"H1tSsb-AW","replyto":"H1tSsb-AW","signatures":["ICLR.cc/2018/Conference/Paper712/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Useful idea for variance reduction with some issues in the experiments","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a variance reduction technique for policy gradient methods. The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines.\n\n\nThe writing and organization of the paper is very well done. It is easy to follow, and succinct while being comprehensive. The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively. There is only one mostly minor issues with the algorithm development and the experiments need to be more polished. \n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0. This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions). However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0. Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied.\n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms. The paper would be a much stronger contribution, if the experiments could be improved. \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc.\n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper. While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline. What was the goal here?\n- In general, the graphs are difficult to read; fonts should be improved and the graphs polished. \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline?\n- It'd be great if Plot (a) and (b) in Figure 5 are swapped.\n\nOverall I think the idea proposed in the paper is beneficial. Better discussing the strong theoretical assumption should be incorporated. Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner. Particularly with some clarity on the experiments, I would be willing to increase the score. \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained? This should be explicitly shown, at least in the appendix. \n2. The listed site for videos and additional results is not active.\n3. Some typos\n- Section 2 - 1st para - last line: \"These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.\".\n- Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i}) \n- Section 4.2 - \\hat{Q} is just Q in many places","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines","abstract":"Policy gradient methods have enjoyed success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high dimensional action spaces. To mitigate this issue, we derive an action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself, and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline both through theoretical analysis as well as numerical results. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks as well as on high dimensional manipulation and multi-agent communication tasks.","pdf":"/pdf/d2cc9c3a100855c1bbc1b5bf3f31328387be3ed7.pdf","TL;DR":"Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.","paperhash":"anonymous|variance_reduction_for_policy_gradient_methods_with_actiondependent_baselines","_bibtex":"@article{\n  anonymous2018variance,\n  title={Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1tSsb-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper712/Authors"],"keywords":["reinforcement learning","policy gradient","variance reduction","baseline","control variates"]}},{"tddate":null,"ddate":null,"tmdate":1512222725659,"tcdate":1511793500358,"number":2,"cdate":1511793500358,"id":"S1VwmoFxz","invitation":"ICLR.cc/2018/Conference/-/Paper712/Official_Review","forum":"H1tSsb-AW","replyto":"H1tSsb-AW","signatures":["ICLR.cc/2018/Conference/Paper712/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs, in particular when they are conditionally independent ('factored'). With the increasing focus on applying RL methods to continuous control problems and RTS type games, this is an important problem and this technique seems like an important addition to the RL toolbox. The paper is well written, the method is easy to implement, and the algorithm seems to have clear positive impact on the presented experiments.\n\n- The derivations in pages 4-6 are somewhat disconnected from the rest of the paper: the optimal baseline derivation is very standard (even if adapted to the slightly different situation situated here), and for reasons highlighted by the authors in this paper, they are not often used; the 'marginalized' baseline is more common, and indeed, the authors adopt this one as well. In light of this (and of the paper being quite a bit over the page limit)- is this material (4.2->4.4) mostly not better suited for the appendix? Same for section 4.6 (which I believe is not used in the experiments).\n\n- The experimental section is very strong; regarding the partial observability experiments, assuming actions are here factored as well, I could see four baselines \n(two choices for whether the baseline has access to the goal location or not, and two choices for whether the baseline has access to the vector $a_{-i}$). It's not clear which two baselines are depicted in 5b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baseline?\n\n(side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act.  Out of curiosity, is it intended as an experiment to verify the need for better baselines? Or as a 'fair' training procedure?)\n\n- Minor: in equation 2- is the correct exponent not t'?  Also since $\\rho_\\pi$ is define with a scaling $(1-\\gamma)$ (to make it an actual distribution), I believe the definition of $\\eta$ should also be multiplied by $(1-\\gamma)$ (as well as equation 2).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines","abstract":"Policy gradient methods have enjoyed success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high dimensional action spaces. To mitigate this issue, we derive an action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself, and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline both through theoretical analysis as well as numerical results. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks as well as on high dimensional manipulation and multi-agent communication tasks.","pdf":"/pdf/d2cc9c3a100855c1bbc1b5bf3f31328387be3ed7.pdf","TL;DR":"Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.","paperhash":"anonymous|variance_reduction_for_policy_gradient_methods_with_actiondependent_baselines","_bibtex":"@article{\n  anonymous2018variance,\n  title={Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1tSsb-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper712/Authors"],"keywords":["reinforcement learning","policy gradient","variance reduction","baseline","control variates"]}},{"tddate":null,"ddate":null,"tmdate":1512222725700,"tcdate":1511733242265,"number":1,"cdate":1511733242265,"id":"ryf-_2ugf","invitation":"ICLR.cc/2018/Conference/-/Paper712/Official_Review","forum":"H1tSsb-AW","replyto":"H1tSsb-AW","signatures":["ICLR.cc/2018/Conference/Paper712/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presents methods to reduce the variance of policy gradient using action dependent baselines when actions have conditionally independent factors.","rating":"7: Good paper, accept","review":"This paper presents methods to reduce the variance of policy gradient using an action dependent baseline. Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state. The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free\n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines.\n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case.\n(4) suggests using MC marginalization and also using the \"average\" action to improve computational feasibility\n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance\n\nThe suggested methods are empirically evaluated on a number of settings. Overall action-dependent baseline outperform state-only versions. Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.\n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space. In light of such results, one might change the policy space to enforce such structure.\n\nNotes:\n- Elaborate further on the assumption made in Eqn 9. Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?\n- Eqn 11 should use \\simeq\n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions? Consider expanding on that in section 4.5.\n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed). How would you train such baselines?\n- Figure 4 is impossible to read in print. The fonts are too small for the numbers and the legends.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines","abstract":"Policy gradient methods have enjoyed success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high dimensional action spaces. To mitigate this issue, we derive an action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself, and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline both through theoretical analysis as well as numerical results. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks as well as on high dimensional manipulation and multi-agent communication tasks.","pdf":"/pdf/d2cc9c3a100855c1bbc1b5bf3f31328387be3ed7.pdf","TL;DR":"Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.","paperhash":"anonymous|variance_reduction_for_policy_gradient_methods_with_actiondependent_baselines","_bibtex":"@article{\n  anonymous2018variance,\n  title={Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1tSsb-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper712/Authors"],"keywords":["reinforcement learning","policy gradient","variance reduction","baseline","control variates"]}},{"tddate":null,"ddate":null,"tmdate":1509739146617,"tcdate":1509133120802,"number":712,"cdate":1509739143957,"id":"H1tSsb-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1tSsb-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines","abstract":"Policy gradient methods have enjoyed success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high dimensional action spaces. To mitigate this issue, we derive an action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself, and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline both through theoretical analysis as well as numerical results. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks as well as on high dimensional manipulation and multi-agent communication tasks.","pdf":"/pdf/d2cc9c3a100855c1bbc1b5bf3f31328387be3ed7.pdf","TL;DR":"Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.","paperhash":"anonymous|variance_reduction_for_policy_gradient_methods_with_actiondependent_baselines","_bibtex":"@article{\n  anonymous2018variance,\n  title={Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1tSsb-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper712/Authors"],"keywords":["reinforcement learning","policy gradient","variance reduction","baseline","control variates"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}