{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222799884,"tcdate":1511813287832,"number":3,"cdate":1511813287832,"id":"HJlhggcgM","invitation":"ICLR.cc/2018/Conference/-/Paper873/Official_Review","forum":"H135uzZ0-","replyto":"H135uzZ0-","signatures":["ICLR.cc/2018/Conference/Paper873/AnonReviewer1"],"readers":["everyone"],"content":{"title":"New setup for CNN with half precision that gets 2X speedup on training","rating":"6: Marginally above acceptance threshold","review":"This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training. The work is clearly presented and the evaluations seem convincing. The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation.  I'm not an expert in this area but the contribution seems relevant to me, and enough for being published.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training of Convolutional Neural Networks using Integer Operations","abstract":"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 \\cite{nvbaidu_mixed}. On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.\nIn particular we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.\nWe propose a shared exponent representation of tensors, and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. \nThe nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. \nWe implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision representation.","pdf":"/pdf/c412c6aeeebf58b37da02b47cf075aa2f6f71ff2.pdf","TL;DR":"Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training;","paperhash":"anonymous|mixed_precision_training_of_convolutional_neural_networks_using_integer_operations","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training of Convolutional Neural Networks using Integer Operations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H135uzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper873/Authors"],"keywords":["deep learning training","reduced precision","imagenet","dynamic fixed point"]}},{"tddate":null,"ddate":null,"tmdate":1512222799922,"tcdate":1511392286257,"number":2,"cdate":1511392286257,"id":"HyIm4t7xz","invitation":"ICLR.cc/2018/Conference/-/Paper873/Official_Review","forum":"H135uzZ0-","replyto":"H135uzZ0-","signatures":["ICLR.cc/2018/Conference/Paper873/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Mixed Precision Training","rating":"7: Good paper, accept","review":"This paper is about low-precision training for ConvNets. It proposed a \"dynamic fixed point\" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format. The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K. According to the paper, this is the first time such kind of performance are demonstrated for limited precision training.\n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance.\n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training of Convolutional Neural Networks using Integer Operations","abstract":"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 \\cite{nvbaidu_mixed}. On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.\nIn particular we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.\nWe propose a shared exponent representation of tensors, and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. \nThe nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. \nWe implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision representation.","pdf":"/pdf/c412c6aeeebf58b37da02b47cf075aa2f6f71ff2.pdf","TL;DR":"Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training;","paperhash":"anonymous|mixed_precision_training_of_convolutional_neural_networks_using_integer_operations","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training of Convolutional Neural Networks using Integer Operations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H135uzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper873/Authors"],"keywords":["deep learning training","reduced precision","imagenet","dynamic fixed point"]}},{"tddate":null,"ddate":null,"tmdate":1512222799963,"tcdate":1511031887343,"number":1,"cdate":1511031887343,"id":"r1vIV-R1z","invitation":"ICLR.cc/2018/Conference/-/Paper873/Official_Review","forum":"H135uzZ0-","replyto":"H135uzZ0-","signatures":["ICLR.cc/2018/Conference/Paper873/AnonReviewer3"],"readers":["everyone"],"content":{"title":"SOTA with reduced precision on large CNNs, however, lacks speedup analysis","rating":"5: Marginally below acceptance threshold","review":"This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations. These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10. However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported. In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16. The authors propose a framework to use such instruction, however it is not clear from the paper whether they actually implemented it, as they no not report any speedup number.  \n\nAlthough the reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs, the paper lacks a speedup analysis. After all, improving speed is the essence of the effort in reducing the number of bits representing weights and activations. It also lacks a direct comparison between FP16 and INT16. Finally it lacks a quantitative analysis on the impact each module of the pipeline has on the overall accuracy. There are many heuristic decision made along the way but only vague statements are offered on their impact on the overall speed and accuracy. For example in section 4.3, overflows handling are described but the tradoffs between complexity and overflow risk and their impact on accuracy are only vaguely addressed: \"...the overheads would be significant and hurt performance.\". \n\nAlthough the paper is written relatively clearly, there are many grammatical errors and typos left, and all the references were not properly bracketed in the text resulting in hard to read sentences.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training of Convolutional Neural Networks using Integer Operations","abstract":"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 \\cite{nvbaidu_mixed}. On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.\nIn particular we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.\nWe propose a shared exponent representation of tensors, and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. \nThe nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. \nWe implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision representation.","pdf":"/pdf/c412c6aeeebf58b37da02b47cf075aa2f6f71ff2.pdf","TL;DR":"Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training;","paperhash":"anonymous|mixed_precision_training_of_convolutional_neural_networks_using_integer_operations","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training of Convolutional Neural Networks using Integer Operations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H135uzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper873/Authors"],"keywords":["deep learning training","reduced precision","imagenet","dynamic fixed point"]}},{"tddate":null,"ddate":null,"tmdate":1509739055014,"tcdate":1509136532052,"number":873,"cdate":1509739052336,"id":"H135uzZ0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H135uzZ0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Mixed Precision Training of Convolutional Neural Networks using Integer Operations","abstract":"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 \\cite{nvbaidu_mixed}. On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.\nIn particular we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.\nWe propose a shared exponent representation of tensors, and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. \nThe nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. \nWe implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision representation.","pdf":"/pdf/c412c6aeeebf58b37da02b47cf075aa2f6f71ff2.pdf","TL;DR":"Mixed precision training pipeline using 16-bit integers on general purpose HW;  SOTA accuracy for ImageNet-class CNNs; Best reported accuracy for ImageNet-1K classification task with any reduced precision training;","paperhash":"anonymous|mixed_precision_training_of_convolutional_neural_networks_using_integer_operations","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training of Convolutional Neural Networks using Integer Operations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H135uzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper873/Authors"],"keywords":["deep learning training","reduced precision","imagenet","dynamic fixed point"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}