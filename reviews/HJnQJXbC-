{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222554337,"tcdate":1511753167405,"number":3,"cdate":1511753167405,"id":"B1vCBbYlz","invitation":"ICLR.cc/2018/Conference/-/Paper1087/Official_Review","forum":"HJnQJXbC-","replyto":"HJnQJXbC-","signatures":["ICLR.cc/2018/Conference/Paper1087/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New approach to asynchrony","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes new direction for asynchronous training. While many synchronous and asynchronous approaches for data parallelism have been proposed and implemented in the past, the space of asynchronous model parallelism hasn't really been explored before. This paper discusses an implementation of this approach and compares the results on dynamic neural networks as compared to existing parallel approaches.\n\nPros:\n- Paper seems to cover and contrast well with the existing approaches and is able to clarify where it differs from existing papers.\n- The new approach seems to show positive results on certain dynamic neural network problems.\n\nCons:\n- Data parallelism is a very commonly used technique for scaling. While the paper mentions support for it, the results are only showed on a toy problem, and it is unclear that it will work well for real problems. It will be great to see more results that use multiple replicas.\n- As the authors mention the messages also encapsulate meta-data or \"state\" as the authors refer to it. This does seem to make their compiler more complex. This doesn't seem to be a requirement for their design and proposal, and it will be good to see explorations to improve on this in the future.\n\nQuestions: \n- It appears that only a single copy of the parameters is kept, thus it is possible that some of the gradients may be computed with newer values than what the forward computation used. Is this true? Does this cause convergence issues?\n\nOverall it seems like a valuable area for exploration, especially given the growing interest in dynamic neural networks.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks","abstract":"\nNew types of compute hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. In particular, models that exploit structured input via complex and instance-dependent control flow are difficult to accelerate using existing algorithms and hardware that typically rely on minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently, even for small minibatch sizes, resulting in shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today.","pdf":"/pdf/19a9932d9f73b3be2d71a8d7f335e6cb2e53236f.pdf","TL;DR":"Using asynchronous gradient updates to accelerate dynamic neural network training","paperhash":"anonymous|ampnet_asynchronous_modelparallel_training_for_dynamic_neural_networks","_bibtex":"@article{\n  anonymous2018ampnet:,\n  title={AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJnQJXbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1087/Authors"],"keywords":["asynchronous","neural network","deep learning","graph","tree","rnn"]}},{"tddate":null,"ddate":null,"tmdate":1512222554385,"tcdate":1511742241437,"number":2,"cdate":1511742241437,"id":"HJKXoRdgf","invitation":"ICLR.cc/2018/Conference/-/Paper1087/Official_Review","forum":"HJnQJXbC-","replyto":"HJnQJXbC-","signatures":["ICLR.cc/2018/Conference/Paper1087/AnonReviewer1"],"readers":["everyone"],"content":{"title":"AMPNet review","rating":"5: Marginally below acceptance threshold","review":"This paper presents AMPNet, that addresses parallel training for dynamic networks. This is accomplished by building a static graph like IR that can serve as a target for compilation for high-level libraries such as tensor flow. In the IR each node of the computation graph is a parallel worker, and synchronization occurs when a sufficient number of gradients have been accumulated. The IR uses constructs such as concat, split, broadcast,.. allowing dynamic, instance dependent control flow decisions. The primary improvement in training performance is from reducing synchronization costs.\n\nComments for the author:\n\nThe paper proposes a solution to an important problem of model parallel training especially over dynamic batching that is increasingly important as we see more complex models where batching is not straightforward. The proposed solution can be effective. However, this is not really evident from the evaluation. Furthermore, the paper can be a little dense read for the ICLR audience. I have the following additional concerns:\n\n1) The paper stresses new hardware throughout the paper. The paper also alludes to â€œsimulator\" of a 1 TFLOPs FPGA in the conclusion. However, your entire evaluation is over CPU. The said simulator is a bunch of sleep() calls (unless some details are skipped). I would encourage the authors to remove these references since these new devices have very different hardware behavior. For example, on a real constrained device, you may not enjoy a large L2 cache which you are benefitting from by doing an entire evaluation over CPUs. Likewise, the vector instruction processing behavior is also very different since these devices have limited power budgets and may not be able to support AVX style instructions. Unless an actual simulator like GEM5 is used, a correct representation of what hardware environment is being used is necessary before making claims that this is ideal for emerging hardware.\n\n2) To continue on the hardware front and the evaluation, I feel for this paper to be accepted or appreciated, a simulated hardware is not necessary. Personally, I found the evaluation with simulated sleep functions more confusing than helpful. An appropriate evaluation for this paper can be just benefits over CPU or GPUs, For example, you have a 7 TFLOPS device (e.g. a GPU or a CPU). Existing algorithms extract X TFLOPs of processing power and using your IR/system one gets Y effective TFLOPs and Y>X. This is all that is required. Currently, looking at your evaluation riddled with hypothetical hardware, it is unclear to me if this is helpful for existing hardware. For example, in Table 1, are Tensorflow numbers only provided over the 1 TFLOPs device (they correspond to the 1 TFLOPs column for all workloads except for MNIST)?  Do you use the parallelism at all in your Tensorflow baseline?  Please clarify.\n\n3) How do you compare for dynamic batching with dynamic IR platforms like pytorch? Furthermore, more details about how dynamic batching is happening in benchmarks mentioned in Table 1 will be nice to have. Finally, an emphasis on the novel contributions of the paper will also be appreciated.\n\n4) Finally, the evaluation appears to be sensitive to the two hyper-parameters introduced. Are they dataset specific? I feel tuning them would be rather cumbersome for every model given how sensitive they are (Figure 5).\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks","abstract":"\nNew types of compute hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. In particular, models that exploit structured input via complex and instance-dependent control flow are difficult to accelerate using existing algorithms and hardware that typically rely on minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently, even for small minibatch sizes, resulting in shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today.","pdf":"/pdf/19a9932d9f73b3be2d71a8d7f335e6cb2e53236f.pdf","TL;DR":"Using asynchronous gradient updates to accelerate dynamic neural network training","paperhash":"anonymous|ampnet_asynchronous_modelparallel_training_for_dynamic_neural_networks","_bibtex":"@article{\n  anonymous2018ampnet:,\n  title={AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJnQJXbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1087/Authors"],"keywords":["asynchronous","neural network","deep learning","graph","tree","rnn"]}},{"tddate":null,"ddate":null,"tmdate":1512222554430,"tcdate":1511569918933,"number":1,"cdate":1511569918933,"id":"HJwWcV8gz","invitation":"ICLR.cc/2018/Conference/-/Paper1087/Official_Review","forum":"HJnQJXbC-","replyto":"HJnQJXbC-","signatures":["ICLR.cc/2018/Conference/Paper1087/AnonReviewer3"],"readers":["everyone"],"content":{"title":"It is an interesting paper about model parallelism for dynamic neural networks. The proposed methodology is novel, but the implementation is not very complete.    ","rating":"6: Marginally above acceptance threshold","review":"The paper describes a model-parallel training framework/algorithm that is specialized for new devises including FPGA. Because of the small memory of those devices, model-parallel training is necessary. Most current other frameworks are for model parallelism, so in this sense, the framework proposed by the authors is different and original. The framework includes a few interesting ideas including using intermediate representation (IR) to express static computation graph and execute it as dynamic control flow, combining pipeline model parallelism and data parallelism by splitting or replicating certain layers, and enabling asynchronous training, etc.  \n\nSome concerns/questions are \n1) The framework is targeted at devices like FPGA, but the implementation is a multicore CPU SMP. It makes the computational result less convincing. Also, does the implementation use threading or message passing?\n2) Pipeline model parallelism seems need a lot of load balance tuning. The reported speedup results confirm this conjecture. Can the limitation of pipeline model parallelism be improved?\n\nPage 4, in the \"min_update_interval\" paragraph, why \"Small min update interval may increase gradient staleness.\"? I would think it decreases staleness. \n\nThe paper is clearly written and easy to follow. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks","abstract":"\nNew types of compute hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. In particular, models that exploit structured input via complex and instance-dependent control flow are difficult to accelerate using existing algorithms and hardware that typically rely on minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently, even for small minibatch sizes, resulting in shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today.","pdf":"/pdf/19a9932d9f73b3be2d71a8d7f335e6cb2e53236f.pdf","TL;DR":"Using asynchronous gradient updates to accelerate dynamic neural network training","paperhash":"anonymous|ampnet_asynchronous_modelparallel_training_for_dynamic_neural_networks","_bibtex":"@article{\n  anonymous2018ampnet:,\n  title={AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJnQJXbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1087/Authors"],"keywords":["asynchronous","neural network","deep learning","graph","tree","rnn"]}},{"tddate":null,"ddate":null,"tmdate":1510092381000,"tcdate":1509138222887,"number":1087,"cdate":1510092360116,"id":"HJnQJXbC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJnQJXbC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks","abstract":"\nNew types of compute hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. In particular, models that exploit structured input via complex and instance-dependent control flow are difficult to accelerate using existing algorithms and hardware that typically rely on minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently, even for small minibatch sizes, resulting in shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today.","pdf":"/pdf/19a9932d9f73b3be2d71a8d7f335e6cb2e53236f.pdf","TL;DR":"Using asynchronous gradient updates to accelerate dynamic neural network training","paperhash":"anonymous|ampnet_asynchronous_modelparallel_training_for_dynamic_neural_networks","_bibtex":"@article{\n  anonymous2018ampnet:,\n  title={AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJnQJXbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1087/Authors"],"keywords":["asynchronous","neural network","deep learning","graph","tree","rnn"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}