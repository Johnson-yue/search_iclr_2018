{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222621943,"tcdate":1511808155801,"number":3,"cdate":1511808155801,"id":"rkVjnRYef","invitation":"ICLR.cc/2018/Conference/-/Paper336/Official_Review","forum":"HkpRBFxRb","replyto":"HkpRBFxRb","signatures":["ICLR.cc/2018/Conference/Paper336/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Alternative definitions for lambda returns","rating":"5: Marginally below acceptance threshold","review":"This paper revisits the idea of exponentially weighted lambda-returns at the heart of TD algorithms. The basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value function. The paper empirically evaluates this idea on Atari games with deep non-linear state representations, compared to state-of-the-art baselines.\n\nThis paper is below the threshold because there are issues with the : 1) motivation, 2) the technical details, and (3) the empirical results.\n\nThe paper begins by stating that the exponential weighting of lambda returns is ad-hoc and unjustified. I would say the idea is well justified in several ways. First the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line version. Second, decades of empirical results illustrating good performance of TD compared with MC methods. And an extensive literature of theoretical results. The paper claims that the exponential has been noted to be ad-hoc, please provide a reference for this.\n\nThere have been several works that have noted that lambda can and perhaps should be changed as a function of state (Sutton and Barto, White and White [1], TD-Gammon). In fact, such works even not that lambda should be related to confidence. The paper should work harder to motivate why adapting lambda as a function of state---which has been studied---is not sufficient.\n\nI don't completely understand the objective. Returns with higher confidence should be weighted higher, according to the confidence estimate around the value function estimate as a function of state? With longer returns, n>>1, the role of the value function in the target is down-weighted by gamma^n---meaning its accuracy is of little relevance to the target. How does your formalism take this into account? The basic idea of the lambda return assumes TD targets are better than MC targets due to variance, which place more weight on shorter returns.\n\nI addition I don't understand how learning confidence of the value function has a realizable target. We do not get supervised targets of the confidence of our value estimates. What is your network updating toward?\n\nThe work of Konidaris et al [1] is a more appropriate reference for this work (rather than the Thomas reference provided). Your paper does not very clearly different itself from Konidaris's work here. Please expand on this.\n\nThe experiments have some issues. One issue is that basic baselines could more clearly illustrate what is going on. There are two such baselines: random fixed weightings of the n-step returns, and persisting with the usual weighting but changing lambda on each time step (either randomly or according to some decay schedule). The first baseline is a sanity check to ensure that you are not observing some random effect. The second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or state.\n\nI would say the current results indicate the conventional approach to TD is working well if not better than the new one. Looking at fig 3, its clear the kangaroo is skewing the results, and that overall the new method is performing worse. This is further conflated by fig7 which attempts to illustrate the quality of the learned value functions. In Kangaroo, the domain where your method does best, the l2 error is worse. On the other hand in sea quest and space invaders, where your method does worse, the l2 error is better. These results seem conflicting, or at least raise more questions than they answer.\n\n[1] A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning . Adam White and Martha White. Autonomous Agents and Multi-agent Systems (AAMAS), 2016\n[2] G. D. Konidaris, S. Niekum, and P. S. Thomas. TDγ: Re-evaluating complex backups in temporal difference learning. In Advances in Neural Information Processing Systems 24, pages 2402–2410. 2011. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning","abstract":"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function. lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads. Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","pdf":"/pdf/693e9a121a6e6395f10d90631016307f86ef9fb9.pdf","TL;DR":"A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns.","paperhash":"anonymous|learning_to_mix_nstep_returns_generalizing_lambdareturns_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpRBFxRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper336/Authors"],"keywords":["Reinforcement Learning","Lambda-Returns"]}},{"tddate":null,"ddate":null,"tmdate":1512222621983,"tcdate":1511782121235,"number":2,"cdate":1511782121235,"id":"BkWlPOFlM","invitation":"ICLR.cc/2018/Conference/-/Paper336/Official_Review","forum":"HkpRBFxRb","replyto":"HkpRBFxRb","signatures":["ICLR.cc/2018/Conference/Paper336/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting adaptive weight method for using eligibility vectors with deep RL. I am a little concerned that a key part of the method is imperfectly described.","rating":"6: Marginally above acceptance threshold","review":"The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state. The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.\n\nHowever, the method by which these confidence estimates are refined could be better described. The authors describe these confidences variously as: \"some notion of confidence that the agent has in the value function estimate\" and \"weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015)\". But the exact method is difficult to piece together from what is written. I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters. This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5. If so, this should be stated explicitly.\n\nThere is another theoretical point that could be clearer. The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor. Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return. In my opinion, the quality of the paper would be much improved by a brief discussion of this, and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captured.\n\nNonetheless, I believe that the paper represents an interesting and worthy submission to the conference. I would strongly urge the authors to improve the method description in the camera read version though. A few additional comments are as follows:\n\n  • The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches. However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better. It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting.\n  • On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with.\n  • On page 3, where delta_t is defined (the j step return TD error, I think the middle term should be $gamma^j V(S_{t+j})$\n  • On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning","abstract":"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function. lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads. Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","pdf":"/pdf/693e9a121a6e6395f10d90631016307f86ef9fb9.pdf","TL;DR":"A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns.","paperhash":"anonymous|learning_to_mix_nstep_returns_generalizing_lambdareturns_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpRBFxRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper336/Authors"],"keywords":["Reinforcement Learning","Lambda-Returns"]}},{"tddate":null,"ddate":null,"tmdate":1512222622020,"tcdate":1511309693626,"number":1,"cdate":1511309693626,"id":"ryLFZHGgG","invitation":"ICLR.cc/2018/Conference/-/Paper336/Official_Review","forum":"HkpRBFxRb","replyto":"HkpRBFxRb","signatures":["ICLR.cc/2018/Conference/Paper336/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper extends the A3C algorithm with lambda returns.  In addition it proposes  an approach for learning the weights of the returns.","rating":"5: Marginally below acceptance threshold","review":"SUMMARY\nThe major contribution of the paper is a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.   These CARs are used in the A3C algorithm.  The weights are based on the confidence of the value function of the n-step return.  Even though this idea in not new the authors propose a simple and robust approach for doing it by using the value function estimation network of A3C.\n\nDuring experiments, the autodidactic returns perform better only half of the time as compared to lambda returns.\n\n\nCOMMENTS\nThe j-step returns TD error is not written correctly\n\nIn Figure 1 it is not obvious how the confidence of the values is estimated.\nFigure 1 is unreadable.\n\n\nA lighter version of Algorithm 1 in Appendix F should be moved in the text, since this is the novelty of the paper.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning","abstract":"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function. lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads. Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","pdf":"/pdf/693e9a121a6e6395f10d90631016307f86ef9fb9.pdf","TL;DR":"A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns.","paperhash":"anonymous|learning_to_mix_nstep_returns_generalizing_lambdareturns_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpRBFxRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper336/Authors"],"keywords":["Reinforcement Learning","Lambda-Returns"]}},{"tddate":null,"ddate":null,"tmdate":1509739357721,"tcdate":1509098964875,"number":336,"cdate":1509739355061,"id":"HkpRBFxRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkpRBFxRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning","abstract":"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function. lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads. Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","pdf":"/pdf/693e9a121a6e6395f10d90631016307f86ef9fb9.pdf","TL;DR":"A novel way to generalize lambda-returns by allowing the RL agent to decide how much it wants to weigh each of the n-step returns.","paperhash":"anonymous|learning_to_mix_nstep_returns_generalizing_lambdareturns_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpRBFxRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper336/Authors"],"keywords":["Reinforcement Learning","Lambda-Returns"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}