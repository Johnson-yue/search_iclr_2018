{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222620213,"tcdate":1511758174812,"number":3,"cdate":1511758174812,"id":"ryDPFMKef","invitation":"ICLR.cc/2018/Conference/-/Paper322/Official_Review","forum":"B1IDRdeCW","replyto":"B1IDRdeCW","signatures":["ICLR.cc/2018/Conference/Paper322/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An insightful analysis on binary neural networks and its learning dynamics","rating":"7: Good paper, accept","review":"This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector. It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \"Dot Product Proportionality Property.\" It also proposes \"Generalized Binarization Transformation\" for the first layer of a neural network.\n\nIn general, I think the paper is written clearly and in detail. Some typos and minor issues are listed in the \"Cons\" part below.\n\nPros:\nThe authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer. Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future.\n\nCons:\n* it seems that there are quite some typos in the paper, for example:\n    1. Section 1, in the second contribution, there are two \"then\"s.\n    2. Section 1, the citation format of \"Bengio et al. (2013)\" should be \"(Bengio et al. 2013)\".\n* Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD. \n* Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear.\n* In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer? I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The High-Dimensional Geometry of Binary Neural Networks","abstract":"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well- approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated good classification performance for such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","pdf":"/pdf/699b16ed3e9ba5c1e2659e9bf264ce8450ad2929.pdf","TL;DR":"Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors","paperhash":"anonymous|the_highdimensional_geometry_of_binary_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The High-Dimensional Geometry of Binary Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1IDRdeCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper322/Authors"],"keywords":["Binary Neural Networks","Neural Network Visualization"]}},{"tddate":null,"ddate":null,"tmdate":1512222620251,"tcdate":1511707928348,"number":2,"cdate":1511707928348,"id":"Ske7rLdeM","invitation":"ICLR.cc/2018/Conference/-/Paper322/Official_Review","forum":"B1IDRdeCW","replyto":"B1IDRdeCW","signatures":["ICLR.cc/2018/Conference/Paper322/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The results are not useful","rating":"4: Ok but not good enough - rejection","review":"This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016). My main concerns are on the usage of the given observations. \n\n1. Can the observations be used to explain more recent works?\n\nIndeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network. However, as the authors mentioned, there are more recent works which give better performance than this one. For example, we can use +1, 0, -1 to approximate the weights. Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance. So, how can the given observations be used to explain more recent works?\n\n2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)?\n\nThe authors call their findings theory. From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations. Besides, giving interesting observations is not good enough. I wish to see how they can be used to improve binary networks.\n\nReference\n[a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The High-Dimensional Geometry of Binary Neural Networks","abstract":"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well- approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated good classification performance for such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","pdf":"/pdf/699b16ed3e9ba5c1e2659e9bf264ce8450ad2929.pdf","TL;DR":"Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors","paperhash":"anonymous|the_highdimensional_geometry_of_binary_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The High-Dimensional Geometry of Binary Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1IDRdeCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper322/Authors"],"keywords":["Binary Neural Networks","Neural Network Visualization"]}},{"tddate":null,"ddate":null,"tmdate":1512222620293,"tcdate":1511639428100,"number":1,"cdate":1511639428100,"id":"SkhtYrwef","invitation":"ICLR.cc/2018/Conference/-/Paper322/Official_Review","forum":"B1IDRdeCW","replyto":"B1IDRdeCW","signatures":["ICLR.cc/2018/Conference/Paper322/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting, yet partially confusing","rating":"6: Marginally above acceptance threshold","review":"This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. Specifically, they observe that:\n\n(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet.\n\n(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations). This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.\n\n(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer.\n\nThe first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge.\n\nThe second observation is much less clear to me. Specifically,\na.\tThe author claim that “A sufficient condition for \\delta u to be the same in both cases is L’(x = f(u)) ~ L’(x = g(u))”. However, I’m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. \nb.\tRelated to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified.\nc.\tFor BNNs, where both the weights and activations are binarized, shouldn’t we compare weights*activations to (binarized weights)*(binarized activations)?\nd.\tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample? If not, then C is not proportional the identity matrix, as claimed in section 5.3.\ne.\tIt is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?), perhaps this should be further clarified. \n\nThe third observation seems less useful to me. Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST). Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.\n\nTo summarize, the first part is interesting and nice, the second part was not clear to me, and the last part does not seem very useful. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The High-Dimensional Geometry of Binary Neural Networks","abstract":"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well- approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated good classification performance for such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","pdf":"/pdf/699b16ed3e9ba5c1e2659e9bf264ce8450ad2929.pdf","TL;DR":"Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors","paperhash":"anonymous|the_highdimensional_geometry_of_binary_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The High-Dimensional Geometry of Binary Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1IDRdeCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper322/Authors"],"keywords":["Binary Neural Networks","Neural Network Visualization"]}},{"tddate":null,"ddate":null,"tmdate":1509739364718,"tcdate":1509097054152,"number":322,"cdate":1509739362067,"id":"B1IDRdeCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1IDRdeCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The High-Dimensional Geometry of Binary Neural Networks","abstract":"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well- approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated good classification performance for such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","pdf":"/pdf/699b16ed3e9ba5c1e2659e9bf264ce8450ad2929.pdf","TL;DR":"Recent successes of Binary Neural Networks can be understood based on the geometry of high-dimensional binary vectors","paperhash":"anonymous|the_highdimensional_geometry_of_binary_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The High-Dimensional Geometry of Binary Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1IDRdeCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper322/Authors"],"keywords":["Binary Neural Networks","Neural Network Visualization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}