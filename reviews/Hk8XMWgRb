{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222590320,"tcdate":1511831466283,"number":2,"cdate":1511831466283,"id":"r1fhwN9eG","invitation":"ICLR.cc/2018/Conference/-/Paper215/Official_Review","forum":"Hk8XMWgRb","replyto":"Hk8XMWgRb","signatures":["ICLR.cc/2018/Conference/Paper215/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting work but need more comparison","rating":"6: Marginally above acceptance threshold","review":"\nIn this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together. The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem. They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity. Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity. Empirically, they show the performances are better than random feature and the LKRF. \n\nI like the way they handle the nonconvexity component of the model. However, there are several issues need to be addressed. \n\n1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly. \n\n2, In the paper, there is an assumption about the peak of random feature \"it is a natural assumption on realistic data that the largest peaks are close to the origin\". I was wondering where this assumption is used? Could you please provide more justification for such assumption?\n\n3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning. Please clarify this in the paper explicitly. \n\n4, The experiment is weak. The algorithm is closely related to boosting and MKL, while there is no such comparison. Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison. \n\nSuggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Not-So-Random Features","abstract":"We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.","pdf":"/pdf/09364e752069d496c21076694eb58eb315ebf2ce.pdf","TL;DR":"A simple and practical algorithm for learning a margin-maximizing translation-invariant or spherically symmetric kernel from training data, using tools from Fourier analysis and regret minimization.","paperhash":"anonymous|notsorandom_features","_bibtex":"@article{\n  anonymous2018not-so-random,\n  title={Not-So-Random Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk8XMWgRb}\n}","keywords":["kernel learning","random features","online learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper215/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222590357,"tcdate":1511737614599,"number":1,"cdate":1511737614599,"id":"SyPGF6dxM","invitation":"ICLR.cc/2018/Conference/-/Paper215/Official_Review","forum":"Hk8XMWgRb","replyto":"Hk8XMWgRb","signatures":["ICLR.cc/2018/Conference/Paper215/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper on kernel learning and random features","rating":"7: Good paper, accept","review":"The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM. Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min-max sense. This perspective leads to some interesting theoretical results and some new interpretation. Synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases.\n\nI like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boosting. As for the related work, it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [1, 2]. It would be interesting to compare these algorithms as well.\n\n[1] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang. Deep Fried Convnets. ICCV 2015.\n[2] Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson. A la Carte â€” Learning Fast Kernels. AISTATS 2015.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Not-So-Random Features","abstract":"We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.","pdf":"/pdf/09364e752069d496c21076694eb58eb315ebf2ce.pdf","TL;DR":"A simple and practical algorithm for learning a margin-maximizing translation-invariant or spherically symmetric kernel from training data, using tools from Fourier analysis and regret minimization.","paperhash":"anonymous|notsorandom_features","_bibtex":"@article{\n  anonymous2018not-so-random,\n  title={Not-So-Random Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk8XMWgRb}\n}","keywords":["kernel learning","random features","online learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper215/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739424828,"tcdate":1509065246218,"number":215,"cdate":1509739422171,"id":"Hk8XMWgRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk8XMWgRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Not-So-Random Features","abstract":"We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods.","pdf":"/pdf/09364e752069d496c21076694eb58eb315ebf2ce.pdf","TL;DR":"A simple and practical algorithm for learning a margin-maximizing translation-invariant or spherically symmetric kernel from training data, using tools from Fourier analysis and regret minimization.","paperhash":"anonymous|notsorandom_features","_bibtex":"@article{\n  anonymous2018not-so-random,\n  title={Not-So-Random Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk8XMWgRb}\n}","keywords":["kernel learning","random features","online learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper215/Authors"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}