{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222770312,"tcdate":1511792785639,"number":2,"cdate":1511792785639,"id":"H15qgiFgf","invitation":"ICLR.cc/2018/Conference/-/Paper807/Official_Review","forum":"ryQu7f-RZ","replyto":"ryQu7f-RZ","signatures":["ICLR.cc/2018/Conference/Paper807/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Noteworthy paper; some (fixable) issues with technical presentation","rating":"8: Top 50% of accepted papers, clear accept","review":"This work identifies a mistake in the existing proof of convergence of\nAdam, which is among the most popular optimization methods in deep\nlearning. Moreover, it gives a simple 1-dimensional counterexample with\nlinear losses on which Adam does not converge. The same issue also\naffects RMSprop, which may be viewed as a special case of Adam without\nmomentum. The problem with Adam is that the \"learning rate\" matrices\nV_t^{1/2}/alpha_t are not monotonically decreasing. A new method, called\nAMSGrad is therefore proposed, which modifies Adam by forcing these\nmatrices to be decreasing. It is then shown that AMSGrad does satisfy\nessentially the same convergence bound as the one previously claimed for\nAdam. Experiments and simulations are provided that support the\ntheoretical analysis.\n\nApart from some issues with the technical presentation (see below), the\npaper is well-written.\n\nGiven the popularity of Adam, I consider this paper to make a very\ninteresting observation. I further believe all issues with the technical\npresentation can be readily addressed.\n\n\n\nIssues with Technical Presentation:\n\n- All theorems should explicitly state the conditions they require\n  instead of referring to \"all the conditions in (Kingma & Ba, 2015)\".\n- Theorem 2 is a repetition of Theorem 1 (except for additional\n  conditions).\n- The proof of Theorem 3 assumes there are no projections, so this\n  should be stated as part of its conditions. (The claim in footnote 2\n  that they can be handled seems highly plausible, but you should be up\n  front about the limitations of your results.)\n- The regret bound Theorem 4 establishes convergence of the optimization\n  method, so it plays the role of a sanity check. However, it is\n  strictly worse than the regret bound O(sqrt{T}) for online gradient\n  descent [Zinkevich,2003], so it cannot explain why the proposed\n  AMSgrad method might be adaptive. (The method may indeed be adaptive\n  in some sense; I am just saying the *bound* does not express that.\n  This is also not a criticism of the current paper; the same remark\n  also applies to the previously claimed regret bound for Adam.)\n- The discussion following Corollary 1 suggests that sum_i\n  hat{v}_{T,i}^{1/2} might be much smaller than d G_infty. This is true,\n  but we should always expect it to be at least a constant, because\n  hat{v}_{t,i} is monotonically increasing by definition of the\n  algorithm, so the bound does not get better than O(sqrt(T)).\n  It is also suggested that sum_i ||g_{1:T,i}|| = sqrt{sum_{t=1}^T\n  g_{t,i}^2} might be much smaller than dG_infty, but this is very\n  unlikely, because this term will typically grow like O(sqrt{T}),\n  unless the data are extremely sparse, so we should at least expect\n  some dependence on T.\n- In the proof of Theorem 1, the initial point is taken to be x_1 = 1,\n  which is perfectly fine, but it is not \"without loss of generality\",\n  as claimed. This should be stated in the statement of the Theorem.\n- The proof of Theorem 6 in appendix B only covers epsilon=1. If it is\n  \"easy to show\" that the same construction also works for other\n  epsilon, as claimed, then please provide the proof for general\n  epsilon.\n\n\nOther remarks:\n\n- Theoretically, nonconvergence of Adam seems a severe problem. Can you\n  speculate on why this issue has not prevented its widespread adoption?\n  Which factors might mitigate the issue in practice?\n- Please define g_t \\circ g_t and g_{1:T,i}\n- I would recommend sticking with standard linear algebra notation for\n  the sqrt and the inverse of a matrix and simply using A^{-1} and\n  A^{1/2} instead of 1/A and sqrt{A}.\n- In theorems 1,2,3, I would recommend stating the dimension (d=1) of\n  your counterexamples, which makes them very nice!\n\nMinor issues:\n\n- Check accent on Nicol\\`o Cesa-Bianchi in bibliography.\n- Near the end of the proof of Theorem 6: I believe you mean Adam\n  suffers a \"regret\" instead of a \"loss\" of at least 2C-4.\n  Also 2C-4=2C-4 is trivial in the second but last display.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Convergence of Adam and Beyond","abstract":" Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.","pdf":"/pdf/576d3128a5655c9b5e3440487c1a7785724c04a7.pdf","TL;DR":"We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ","paperhash":"anonymous|on_the_convergence_of_adam_and_beyond","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Convergence of Adam and Beyond},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryQu7f-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper807/Authors"],"keywords":["optimization","deep learning","adam","rmsprop"]}},{"tddate":null,"ddate":null,"tmdate":1512222772542,"tcdate":1511476851766,"number":1,"cdate":1511476851766,"id":"HkhdRaVlG","invitation":"ICLR.cc/2018/Conference/-/Paper807/Official_Review","forum":"ryQu7f-RZ","replyto":"ryQu7f-RZ","signatures":["ICLR.cc/2018/Conference/Paper807/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"9: Top 15% of accepted papers, strong accept","review":"The paper presents three contributions: 1) it shows that the proof of convergence Adam is wrong; 2) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution (i.e. there is no hope to just fix Adam's proof); 3) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties.\n\nThe contribution of this paper is very relevant to ICLR and, as far as I know, novel.\nThe result is clearly very important for the deep learning community.\nI also checked most of the proofs and they look correct to me: The arguments are quite standard, even if the proofs are very long.\n\nOne note on the generality of the results: the papers states that some of the results could apply to RMSProp too. However, it has been proved that RMSProp with a certain settings of its parameters is nothing else than AdaGrad (see Section 4 in  Mukkamala and Hein, ICML'17). Hence, at least for a certain setting of its parameters, RMSProp will converge. Of course, the proof in the ICML paper could be wrong, I did not check that...\n\nA general note on the learning rate: The fact that most of these algorithms are used with a fixed learning rate while the analysis assume a decaying learning rate should hint to the fact that we are not using the right analysis. Indeed, all these variants of AdaGrad did not really improve the AdaGrad's regret bound. In this view, none of these algorithms contributed in any meaningful way to our understanding of the optimization of deep networks *nor* they advanced in any way the state-of-the-art for optimizing convex Lipschitz functions.\nOn the other hand, analysis of SGD-like algorithms with constant step sizes are known. See, for example, Zhang, ICML'04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems.\nSo, even if I understand this is not the main objective of this paper, it would be nice to see a discussion on this point and the limitations of regret analysis to analyse SGD algorithms.\n\nOverall, I strongly suggest to accept this paper.\n\n\nSuggestions/minor things:\n- To facilitate the reader, I would state from the beginning what are the common settings of beta_1 and beta_2 in Adam. This makes easier to see that, for example, the condition of Theorem 2 is verified.\n- \\hat{v}_{0} is undefined in Algorithm 2.\n- The graphs in figure 2 would gain in readability if the setting of each one of them would be added as their titles.\n- McMahan and Streeter (2010) is missing the title. (Also, kudos for citing both the independent works on AdaGrad)\n- page 11, last equation, 2C-4=2C-4. Same on page 13.\n- Lemma 4 contains x_1,x_2,z_1, and z_2: are x_1 and z_1 the same? also x_2 and z_2?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Convergence of Adam and Beyond","abstract":" Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.","pdf":"/pdf/576d3128a5655c9b5e3440487c1a7785724c04a7.pdf","TL;DR":"We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ","paperhash":"anonymous|on_the_convergence_of_adam_and_beyond","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Convergence of Adam and Beyond},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryQu7f-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper807/Authors"],"keywords":["optimization","deep learning","adam","rmsprop"]}},{"tddate":null,"ddate":null,"tmdate":1511199158165,"tcdate":1511199158165,"number":1,"cdate":1511199158165,"id":"SJRh-9lef","invitation":"ICLR.cc/2018/Conference/-/Paper807/Official_Comment","forum":"ryQu7f-RZ","replyto":"Bye7sLhkM","signatures":["ICLR.cc/2018/Conference/Paper807/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper807/Authors"],"content":{"title":"Re: Hyperparameters for experiments","comment":"We thank you for your interest in our paper and for pointing out this missing detail. We use a decrease step size of alpha/sqrt(t) (as suggested by our theoretical analysis) for the stochastic optimization experiment. The use of decreasing step size leads to a more stable convergence to the optimal solution (especially in scenarios where the variance is reasonably high). We did not use epsilon in this particular experiment since the gradients are reasonably large (in other words, using a small epsilon like 1e-8 should produce more or less identical results). We will add these details in the next revision of our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Convergence of Adam and Beyond","abstract":" Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.","pdf":"/pdf/576d3128a5655c9b5e3440487c1a7785724c04a7.pdf","TL;DR":"We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ","paperhash":"anonymous|on_the_convergence_of_adam_and_beyond","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Convergence of Adam and Beyond},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryQu7f-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper807/Authors"],"keywords":["optimization","deep learning","adam","rmsprop"]}},{"tddate":null,"ddate":null,"tmdate":1510923041241,"tcdate":1510923041241,"number":1,"cdate":1510923041241,"id":"Bye7sLhkM","invitation":"ICLR.cc/2018/Conference/-/Paper807/Public_Comment","forum":"ryQu7f-RZ","replyto":"ryQu7f-RZ","signatures":["~Mario_Ynocente_Castro1"],"readers":["everyone"],"writers":["~Mario_Ynocente_Castro1"],"content":{"title":"Hyperparameters for experiments","comment":"Hello,\n\nI tried implementing AMSGrad (here: https://colab.research.google.com/notebook#fileId=1xXFAuHM2Ae-OmF5M8Cn9ypGCa_HHBgfG) for the experiment on the stochastic optimization setting and obtain that x_t approaches -1 faster that on the paper but convergence seems less stable, so I was wondering about the specific values for other hyperparameters like the learning rate and epsilon which weren't mentioned, in my case I chose a learning of 1e-3 and an epsilon of 1e-8 which seems to be the standard value on most frameworks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Convergence of Adam and Beyond","abstract":" Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.","pdf":"/pdf/576d3128a5655c9b5e3440487c1a7785724c04a7.pdf","TL;DR":"We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ","paperhash":"anonymous|on_the_convergence_of_adam_and_beyond","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Convergence of Adam and Beyond},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryQu7f-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper807/Authors"],"keywords":["optimization","deep learning","adam","rmsprop"]}},{"tddate":null,"ddate":null,"tmdate":1509739090638,"tcdate":1509135211401,"number":807,"cdate":1509739087976,"id":"ryQu7f-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryQu7f-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the Convergence of Adam and Beyond","abstract":" Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.","pdf":"/pdf/576d3128a5655c9b5e3440487c1a7785724c04a7.pdf","TL;DR":"We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. ","paperhash":"anonymous|on_the_convergence_of_adam_and_beyond","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Convergence of Adam and Beyond},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryQu7f-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper807/Authors"],"keywords":["optimization","deep learning","adam","rmsprop"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}