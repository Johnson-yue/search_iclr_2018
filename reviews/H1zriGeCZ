{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222591778,"tcdate":1512028072469,"number":3,"cdate":1512028072469,"id":"Syx3D46ez","invitation":"ICLR.cc/2018/Conference/-/Paper227/Official_Review","forum":"H1zriGeCZ","replyto":"H1zriGeCZ","signatures":["ICLR.cc/2018/Conference/Paper227/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper on hyperparameter optimization using techniques from compressed sensing","rating":"9: Top 15% of accepted papers, strong accept","review":"The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms.\n\nAt a high-level, hyperparameter optimization (for the challenging case of discrete variables) can be seen as a black-box optimization problem where we have only access to a function evaluation oracle (but no gradients etc.). In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters. In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy. In particular, the authors assume that the function admits a representation as a sparse and low-degree polynomial. While the authors do not empirically validate whether this is a good model of the unknown function, it appears to be a reasonable assumption (the authors *do* empirically validate their overall approach).\n\nBased on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization. The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations). The authors derive relevant sample complexity results for their approach. Moreover, the method also yields new algorithms for learning decision trees.\n\nIn addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10. They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work. The best parameters found by Harmonica improve over the hand-tuned results for their \"base architecture\" (ResNets).\n\nOverall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side. Hence I strongly recommend accepting this paper.\n\n\nSmall comments and questions:\n\n1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit).\n\n2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem.\n\n3. The current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques (e.g., Zoph & Le, ICLR 2017). While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper.\n\n4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy? Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set? The current approach could be prone to overfitting.\n\n5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space. It might be worth adding this in the discussion of the related work.\n\n6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?\n\n7. Why does PSR truncate the indices in alpha? At least in \"standard\" compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without).\n\n9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)\n\n10. On Page 3 we assume that K = 1, but Theorem 6 still maintains a dependence on K. It might be cleaner to either treat the general K case throughout, or state the theorem for K = 1.\n\n11. On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet). It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.\n\n12. Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet. The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks. Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hyperparameter optimization: a spectral approach","abstract":"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \nExperiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search 8x.\n  \nAdditionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades.  In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.\n","pdf":"/pdf/fb234f2d48458db49ee5ab233be8a309eadb9823.pdf","TL;DR":"A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing","paperhash":"anonymous|hyperparameter_optimization_a_spectral_approach","_bibtex":"@article{\n  anonymous2018hyperparameter,\n  title={Hyperparameter optimization: a spectral approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1zriGeCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper227/Authors"],"keywords":["Hyperparameter Optimization","Fourier Analysis","Decision Tree","Compressed Sensing"]}},{"tddate":null,"ddate":null,"tmdate":1512222591822,"tcdate":1511922986396,"number":2,"cdate":1511922986396,"id":"SyM469sgf","invitation":"ICLR.cc/2018/Conference/-/Paper227/Official_Review","forum":"H1zriGeCZ","replyto":"H1zriGeCZ","signatures":["ICLR.cc/2018/Conference/Paper227/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting theoretical ideas, but unclear how practical the proposed approach really is","rating":"6: Marginally above acceptance threshold","review":"- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right. Not clear how important they are\n- they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary). Not a realistic setting. In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials. \n- interesting idea but I think it's more theoretical than practical. Feels like a hammer in need of a nail. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hyperparameter optimization: a spectral approach","abstract":"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \nExperiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search 8x.\n  \nAdditionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades.  In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.\n","pdf":"/pdf/fb234f2d48458db49ee5ab233be8a309eadb9823.pdf","TL;DR":"A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing","paperhash":"anonymous|hyperparameter_optimization_a_spectral_approach","_bibtex":"@article{\n  anonymous2018hyperparameter,\n  title={Hyperparameter optimization: a spectral approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1zriGeCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper227/Authors"],"keywords":["Hyperparameter Optimization","Fourier Analysis","Decision Tree","Compressed Sensing"]}},{"tddate":null,"ddate":null,"tmdate":1512222591859,"tcdate":1511880660764,"number":1,"cdate":1511880660764,"id":"H1nRveigz","invitation":"ICLR.cc/2018/Conference/-/Paper227/Official_Review","forum":"H1zriGeCZ","replyto":"H1zriGeCZ","signatures":["ICLR.cc/2018/Conference/Paper227/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper contains a strong theoretical result that is a bit out of context with the main theme of the paper. The algorithm presented shows promising results for optimizing hyperparameters when the number of hyperparameters > 6. ","rating":"6: Marginally above acceptance threshold","review":"This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis. The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter. \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search. Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong. However, I find this result to be out of the context with the main theme of the paper. \n\nI find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity. Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments.\n\nWhen it comes to the experiments only one real-world experiment is present. It is hard to conclude which method is better based on a single real-world experiment. Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time. I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations. If this is the case, some configurations could easily require more time to evaluate than the others. It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots.\n\nIt is not clear what is the stopping criterion for each of the methods used in the experiments. One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned. It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods. \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n\nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hyperparameter optimization: a spectral approach","abstract":"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \nExperiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search 8x.\n  \nAdditionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades.  In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.\n","pdf":"/pdf/fb234f2d48458db49ee5ab233be8a309eadb9823.pdf","TL;DR":"A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing","paperhash":"anonymous|hyperparameter_optimization_a_spectral_approach","_bibtex":"@article{\n  anonymous2018hyperparameter,\n  title={Hyperparameter optimization: a spectral approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1zriGeCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper227/Authors"],"keywords":["Hyperparameter Optimization","Fourier Analysis","Decision Tree","Compressed Sensing"]}},{"tddate":null,"ddate":null,"tmdate":1509739418250,"tcdate":1509071673712,"number":227,"cdate":1509739415589,"id":"H1zriGeCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1zriGeCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hyperparameter optimization: a spectral approach","abstract":"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \nExperiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search 8x.\n  \nAdditionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades.  In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.\n","pdf":"/pdf/fb234f2d48458db49ee5ab233be8a309eadb9823.pdf","TL;DR":"A hyperparameter tuning algorithm using discrete Fourier analysis and compressed sensing","paperhash":"anonymous|hyperparameter_optimization_a_spectral_approach","_bibtex":"@article{\n  anonymous2018hyperparameter,\n  title={Hyperparameter optimization: a spectral approach},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1zriGeCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper227/Authors"],"keywords":["Hyperparameter Optimization","Fourier Analysis","Decision Tree","Compressed Sensing"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}