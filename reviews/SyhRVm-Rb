{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222561285,"tcdate":1511820832257,"number":3,"cdate":1511820832257,"id":"Syx7RZ9eG","invitation":"ICLR.cc/2018/Conference/-/Paper1162/Official_Review","forum":"SyhRVm-Rb","replyto":"SyhRVm-Rb","signatures":["ICLR.cc/2018/Conference/Paper1162/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review for Automatic Goal Generation for Reinforcement Learning Agents","rating":"6: Marginally above acceptance threshold","review":"This paper proposed a method for automatic curriculum generation that allow an agent to learn to reach multiple goals in an environment with considerable sample efficiency. They use a generator network to propose tasks for the agent accomplish. The generator network is trained with GAN.  In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. They compare the Goal GAN method with four baselines, including Uniform sampling, Asymmetric Self-play, SAGG-RIAC, and Rejection sampling. The proposed method is tested on two environments: Free Ant and Maze Ant. The empirical study shows that the proposed method is able to improve policies’ training efficiency comparing to these baselines. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below.\n\n1. it is unclear to me why the proposed method is able to solve tasks with sparse rewards? Is it because of the horizons of the problems considered are not long enough? The author should provide more insight for this contribution.\n\n2. It is unclear to me how R_min and R_max as hyperparameters are obtained and how their settings affect the performance.\n\n3. Another concern I have is regarding the generalizability of the proposed method. One of the assumption is “A policy trained on a sufficient number of goals in some area of the goal-space will learn to interpolate to other goals within that area”. This seems to mean that the area is convex. It might be better if some quantitative analysis can be provided to illustrate geometry of goal space (given complex motor coordination) that is feasible for the proposed method.\n\n4. It is difficult to understand the plots in Figure 4 without more details. Do you assume for every episode, the agent starts from the same state? \n\n5. For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough.\n\n6. According to the experimental details, three rollouts are performed to estimate the empirical return. It there any justification why three rollouts are enough?\n\n7. Minor comments\nAchieve tasks -> achieve goals or accomplish/solve tasks\nA variation of to -> variation of \nAllows a policy to quickly learn to reach …-> allow an agent to be quickly learn a policy to reach…\n…the difficulty of the generated goals -> … the difficulty of reaching\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Automatic Goal Generation for Reinforcement Learning Agents","abstract":"Reinforcement learning (RL) is a powerful technique to train an agent to perform a task.  However, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.  Our method thus automatically produces a curriculum of tasks for the agent to learn.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment (Videos and code available at: https://sites.google.com/view/goalgeneration4rl). Our method can also learn to achieve tasks with sparse rewards, which pose significant challenges for traditional RL methods.","pdf":"/pdf/7efb4d89e4f175f4cdecd4783b8b5a5d8af797cf.pdf","TL;DR":"We efficiently solve multi-task problems with an automatic curriculum generation algorithm based on a generative model that tracks the learning agent's performance.","paperhash":"anonymous|automatic_goal_generation_for_reinforcement_learning_agents","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Goal Generation for Reinforcement Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyhRVm-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1162/Authors"],"keywords":["Reinforcement Learning","Multi-task Learning","Curriculum Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222561327,"tcdate":1511750791653,"number":2,"cdate":1511750791653,"id":"rJg5hxtgf","invitation":"ICLR.cc/2018/Conference/-/Paper1162/Official_Review","forum":"SyhRVm-Rb","replyto":"SyhRVm-Rb","signatures":["ICLR.cc/2018/Conference/Paper1162/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting problem, but approach / results are not completely clear.","rating":"4: Ok but not good enough - rejection","review":"Summary:\n\nThis paper proposes to use a GAN to generate goals to implement a form of curriculum learning. A goal is defined as a subset of the state space. The authors claim that this model can discover all \"goals\" in the environment and their 'difficulty', which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is 'good' if it is a state that the policy can reach after a (small) improvement of the current policy.\n\nTraining the goal GAN is done via labels, which are states together with the achieved reward by the policy that is being learned.\n\nThe benchmark problems are whether the GAN generates goals that allow the agent to reach the end of a U-maze, and a point-mass task.\n\nAuthors compare GAN goal generation vs uniformly choosing a goal and 2 other methods.\n\nMy overall impression is that this work addresses an interesting question, but the experimental setup / results are not clearly worked out. More broadly, the paper does not address how one can combine RL and training a goal GAN in a stable way.\n\nPro:\n- Developing hierarchical learning methods to improve the sample complexity of RL is an important problem.\n- The paper shows that the U-maze can be 'solved' using a variety of methods that generate goals in a non-uniform way.\n\nCon:\n- It is not clear to me how the asymmetric self-play and SAGG-RIAC are implemented and why they are natural baselines.\n- It is not clear to me what the 'goals' are in the point mass experiment. This entire experiment should be explained much more clearly (+image).\n- It is not clear how this method compares qualitatively vs baselines (differences in goals etc).\n- This method doesn't seem to always outperform the asymm-selfplay baseline. The text mentions that baseline is less efficient, but this doesn't make the graph very interpretable.\n- The curriculum in the maze-case consists of regions that just progress along the maze, and hence is a 1-dimensional space. Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well.\n- The experimental maze-setting and point-mass have a simple state / goal structure. How can this method generalize to harder problems?\n-- The entire method is quite complicated (e.g. training GANs can be highly unstable). How do we stabilize / balance training the GAN vs the RL problem?\n-- I don't see how this method could generalize to problems where the goals / subregions of space do not have a simple distribution as in the maze problem, e.g. if there are multiple ways of navigating a maze towards some final goal state. In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? How could you combine RL and training the GAN stably in that case?\n\nDetailed:\n- (2) is a bit strange: shouldn't the indicator say: 1( \\exists t: s_t \\in S^g )? Surely not all states in the rollout (s_0 ... s_t) are in the goal subspace: the indicator does not factorize over the union. Same for other formulas that use \\union.\n- Are goals overlapping or non-overlapping subsets of the state space? \nDefinition around (1) basically says it's non-overlapping, yet the goal GAN seems to predict goals in a 2d space, hence the predicted goals are overlapping? \n- What are the goals that the non-uniform baselines predict? Does the GAN produce better goals?\n- Generating goal labels is\n- Paper should discuss literature on hierarchical methods that use goals learned from data and via variational methods:\n1. Strategic Attentive Writer (STRAW), V. Mnih et al, NIPS 2016\n2. Generating Long-term Trajectories Using Deep Hierarchical Networks. S.\nZheng et al, NIPS 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Automatic Goal Generation for Reinforcement Learning Agents","abstract":"Reinforcement learning (RL) is a powerful technique to train an agent to perform a task.  However, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.  Our method thus automatically produces a curriculum of tasks for the agent to learn.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment (Videos and code available at: https://sites.google.com/view/goalgeneration4rl). Our method can also learn to achieve tasks with sparse rewards, which pose significant challenges for traditional RL methods.","pdf":"/pdf/7efb4d89e4f175f4cdecd4783b8b5a5d8af797cf.pdf","TL;DR":"We efficiently solve multi-task problems with an automatic curriculum generation algorithm based on a generative model that tracks the learning agent's performance.","paperhash":"anonymous|automatic_goal_generation_for_reinforcement_learning_agents","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Goal Generation for Reinforcement Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyhRVm-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1162/Authors"],"keywords":["Reinforcement Learning","Multi-task Learning","Curriculum Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222561373,"tcdate":1511738087355,"number":1,"cdate":1511738087355,"id":"S1kxi6OlM","invitation":"ICLR.cc/2018/Conference/-/Paper1162/Official_Review","forum":"SyhRVm-Rb","replyto":"SyhRVm-Rb","signatures":["ICLR.cc/2018/Conference/Paper1162/AnonReviewer2"],"readers":["everyone"],"content":{"title":"well-written paper, useful addition to literature, doubts about stability ","rating":"7: Good paper, accept","review":"In general I find this to be a good paper and vote for acceptance. The paper is well-written and easy to follow.  The proposed approach is a useful addition to existing literature.\n\nBesides that I have not much to say except one point I would like to discuss:\n\nIn 4.2 I am not fully convinced of using an adversial model for goal generation. RL algorithms generally suffer from poor stability  and GANs themselves can have convergence issues. This imposes another layer of possible instability. \n \nBesides, generating useful reward function, while not trivial, can be seen as easier than solving the full RL problem. \nCan the authors argue why this model class was chosen over other, more simple, generative models?  \nFurthermore, did the authors do experiments with simpler models?\n\nRelated:\n\"We found that the LSGAN works better than other forms of GAN for our problem.\" \nWas this improvement minor, or major, or didn't even work with other GAN types? This question is important, because for me the big question is if this model is universal and stable in a lot of applications or requires careful fine-tuning and monitoring. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Automatic Goal Generation for Reinforcement Learning Agents","abstract":"Reinforcement learning (RL) is a powerful technique to train an agent to perform a task.  However, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.  Our method thus automatically produces a curriculum of tasks for the agent to learn.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment (Videos and code available at: https://sites.google.com/view/goalgeneration4rl). Our method can also learn to achieve tasks with sparse rewards, which pose significant challenges for traditional RL methods.","pdf":"/pdf/7efb4d89e4f175f4cdecd4783b8b5a5d8af797cf.pdf","TL;DR":"We efficiently solve multi-task problems with an automatic curriculum generation algorithm based on a generative model that tracks the learning agent's performance.","paperhash":"anonymous|automatic_goal_generation_for_reinforcement_learning_agents","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Goal Generation for Reinforcement Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyhRVm-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1162/Authors"],"keywords":["Reinforcement Learning","Multi-task Learning","Curriculum Learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092379344,"tcdate":1509139668025,"number":1162,"cdate":1510092359288,"id":"SyhRVm-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyhRVm-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Automatic Goal Generation for Reinforcement Learning Agents","abstract":"Reinforcement learning (RL) is a powerful technique to train an agent to perform a task.  However, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.  Our method thus automatically produces a curriculum of tasks for the agent to learn.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment (Videos and code available at: https://sites.google.com/view/goalgeneration4rl). Our method can also learn to achieve tasks with sparse rewards, which pose significant challenges for traditional RL methods.","pdf":"/pdf/7efb4d89e4f175f4cdecd4783b8b5a5d8af797cf.pdf","TL;DR":"We efficiently solve multi-task problems with an automatic curriculum generation algorithm based on a generative model that tracks the learning agent's performance.","paperhash":"anonymous|automatic_goal_generation_for_reinforcement_learning_agents","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Goal Generation for Reinforcement Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyhRVm-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1162/Authors"],"keywords":["Reinforcement Learning","Multi-task Learning","Curriculum Learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}