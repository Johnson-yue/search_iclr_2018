{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222669784,"tcdate":1511846757990,"number":3,"cdate":1511846757990,"id":"rJADQOqlG","invitation":"ICLR.cc/2018/Conference/-/Paper491/Official_Review","forum":"SkJd_y-Cb","replyto":"SkJd_y-Cb","signatures":["ICLR.cc/2018/Conference/Paper491/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Another tweak on learning word embeddings","rating":"4: Ok but not good enough - rejection","review":"This paper presents another variant on neural language models used to learn word embeddings. In keeping with the formulation of Mikolov et al, the model learned is a set of independent binary classifiers, one per word. As opposed to other work, each classifier is not based on the dot product between an embedding vector and a context vector but instead is a per-word neural network which takes the context as input and produces a score for each term. An interesting consequence of using networks instead of vectors to parametrize the embeddings is that it's easy to see many ways to let the model use side information such as part-of-speech tags. The paper explores one such way, by sharing parameters across networks of all words which have the same POS tag (effectively having different parameterizations for words which occur with multiple POS tags).\n\nThe idea is interesting but the evaluation leaves doubts. Here are my main problems:\n 1. The quantitative likelihood-based evaluation can easily be gamed by making all classifiers output numbers which are close to 1. This is because the model is not normalized, and no attempt at normalization is claimed to be made during the likelihood evaluation. This means it's likely hyperparameter tuning (of, say, how many negative examples to use per positive example) is likely to bias this evaluation to look more positive than it should.\n 2. The qualitative similarity-based evaluation notes, correctly, that the standard metric of dot product / cosine between word embeddings does not work in the case of networks, and instead measures similarity by looking at the similarity of the predictions of the networks. Then all networks are ranked by similarity to a query network to make the now-standard similar word lists. While this approach is interesting, the baseline models were evaluated using the plain dot product. It's unclear whether this new evaluation methodology would have also produced nicer word lists for the baseline methods.\n\nIn the light that the evaluation has these two issues I do not recommend accepting this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word2net: Deep Representations of Language","abstract":"Word embeddings extract semantic features of words from large datasets of text.\nMost embedding methods rely on a log-bilinear model to predict the occurrence\nof a word in a context of other words. Here we propose word2net, a method that\nreplaces their linear parametrization with neural networks. For each term in the\nvocabulary, word2net posits a neural network that takes the context as input and\noutputs a probability of occurrence. Further, word2net can use the hierarchical\norganization of its word networks to incorporate additional meta-data, such as\nsyntactic features, into the embedding model. For example, we show how to share\nparameters across word networks to develop an embedding model that includes\npart-of-speech information. We study word2net with two datasets, a collection\nof Wikipedia articles and a corpus of U.S. Senate speeches. Quantitatively, we\nfound that word2net outperforms popular embedding methods on predicting held-\nout words and that sharing parameters based on part of speech further boosts\nperformance. Qualitatively, word2net learns interpretable semantic representations\nand, compared to vector-based methods, better incorporates syntactic information.","pdf":"/pdf/9a7a0e8409dc920a6c97ed2d4777df567c7dfbb8.pdf","TL;DR":"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features.","paperhash":"anonymous|word2net_deep_representations_of_language","_bibtex":"@article{\n  anonymous2018word2net:,\n  title={Word2net: Deep Representations of Language},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJd_y-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper491/Authors"],"keywords":["neural language models","word embeddings","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222669829,"tcdate":1511760413182,"number":2,"cdate":1511760413182,"id":"SkH7fQYez","invitation":"ICLR.cc/2018/Conference/-/Paper491/Official_Review","forum":"SkJd_y-Cb","replyto":"SkJd_y-Cb","signatures":["ICLR.cc/2018/Conference/Paper491/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice idea but weak experimental section","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method to use non-linear combination of context vectors for learning vector representation of words. The main idea is to replace each word embedding by a neural network, which scores how likely is the current word given the context words. This also allowed them to use other context information (like POS tags) for word vector learning. I like the approach, although not being an expert in the area, cannot comment on whether there are existing approaches for similar objectives.\n\nI think the experimental section is weak. Most work on word vectors are evaluated on several word similarity and analogy tasks (See the Glove paper).  However, this paper only reports numbers on the task of predicting next word.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word2net: Deep Representations of Language","abstract":"Word embeddings extract semantic features of words from large datasets of text.\nMost embedding methods rely on a log-bilinear model to predict the occurrence\nof a word in a context of other words. Here we propose word2net, a method that\nreplaces their linear parametrization with neural networks. For each term in the\nvocabulary, word2net posits a neural network that takes the context as input and\noutputs a probability of occurrence. Further, word2net can use the hierarchical\norganization of its word networks to incorporate additional meta-data, such as\nsyntactic features, into the embedding model. For example, we show how to share\nparameters across word networks to develop an embedding model that includes\npart-of-speech information. We study word2net with two datasets, a collection\nof Wikipedia articles and a corpus of U.S. Senate speeches. Quantitatively, we\nfound that word2net outperforms popular embedding methods on predicting held-\nout words and that sharing parameters based on part of speech further boosts\nperformance. Qualitatively, word2net learns interpretable semantic representations\nand, compared to vector-based methods, better incorporates syntactic information.","pdf":"/pdf/9a7a0e8409dc920a6c97ed2d4777df567c7dfbb8.pdf","TL;DR":"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features.","paperhash":"anonymous|word2net_deep_representations_of_language","_bibtex":"@article{\n  anonymous2018word2net:,\n  title={Word2net: Deep Representations of Language},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJd_y-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper491/Authors"],"keywords":["neural language models","word embeddings","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222669868,"tcdate":1511739853177,"number":1,"cdate":1511739853177,"id":"rkSCZ0uxG","invitation":"ICLR.cc/2018/Conference/-/Paper491/Official_Review","forum":"SkJd_y-Cb","replyto":"SkJd_y-Cb","signatures":["ICLR.cc/2018/Conference/Paper491/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A natural but questionable extension to word2vec ","rating":"5: Marginally below acceptance threshold","review":"The paper extends SGNS as follows. In SGNS, each word x is associated with vectors a_x and r_x. Given a set of context words C, the model calculates the probability that the target word is x by a dot product between a_x and the average of {r_c: c in C}.  The paper generalizes this computation to an arbitrary network: now each word x is associated with some network N_x whose input is a set of context words C and the output is the aforementioned probability. This is essentially an architectural change: from a bag-of-words model to a (3-layer) feedforward model. \n\nAnother contribution of the paper is a new form of regularization by tying a subset of layers between different N_x. In particular, the paper considers incorporating POS tags by tying within each POS group. For instance, the parameters of the first layer are shared across all noun words. (This assumes that POS tags are given.)\n\nWhile this is a natural extension to word2vec, the reviewer has some reservations about the execution of this work. Word embeddings are useful in large part because they can be used to initialize the parameters of a network. None of the chosen experiments shows this. Improvement in the log likelihood over SGNS is somewhat obvious because there are more parameters. The similarity between \"words\" now requires a selection of context vectors (7) which is awkward/arbitrary. The use of POS tags is not very compelling (though harmless). It's not necessary: contrary to the claim in the paper, word embeddings captures syntactic information if the context width is small and/or context information is provided. A more sensible experiment would be to actually plug in the entire pretrained word nets into an external model and see how much they help. \n\nAll of this is closely addressed in the following prior work: \n\nLearning to Embed Words in Context for Syntactic Tasks (Tu et al., 2017)\n\nQuality: Natural but questionable extension, see above. \n\nClarity: Clear. \n\nOriginality: Acceptable, but a very similar idea of embedding contexts is presented in Tu et al. (2017) which is not cited. \n\nSignificance: Minor/moderate, see above. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word2net: Deep Representations of Language","abstract":"Word embeddings extract semantic features of words from large datasets of text.\nMost embedding methods rely on a log-bilinear model to predict the occurrence\nof a word in a context of other words. Here we propose word2net, a method that\nreplaces their linear parametrization with neural networks. For each term in the\nvocabulary, word2net posits a neural network that takes the context as input and\noutputs a probability of occurrence. Further, word2net can use the hierarchical\norganization of its word networks to incorporate additional meta-data, such as\nsyntactic features, into the embedding model. For example, we show how to share\nparameters across word networks to develop an embedding model that includes\npart-of-speech information. We study word2net with two datasets, a collection\nof Wikipedia articles and a corpus of U.S. Senate speeches. Quantitatively, we\nfound that word2net outperforms popular embedding methods on predicting held-\nout words and that sharing parameters based on part of speech further boosts\nperformance. Qualitatively, word2net learns interpretable semantic representations\nand, compared to vector-based methods, better incorporates syntactic information.","pdf":"/pdf/9a7a0e8409dc920a6c97ed2d4777df567c7dfbb8.pdf","TL;DR":"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features.","paperhash":"anonymous|word2net_deep_representations_of_language","_bibtex":"@article{\n  anonymous2018word2net:,\n  title={Word2net: Deep Representations of Language},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJd_y-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper491/Authors"],"keywords":["neural language models","word embeddings","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739273921,"tcdate":1509124198876,"number":491,"cdate":1509739271268,"id":"SkJd_y-Cb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkJd_y-Cb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Word2net: Deep Representations of Language","abstract":"Word embeddings extract semantic features of words from large datasets of text.\nMost embedding methods rely on a log-bilinear model to predict the occurrence\nof a word in a context of other words. Here we propose word2net, a method that\nreplaces their linear parametrization with neural networks. For each term in the\nvocabulary, word2net posits a neural network that takes the context as input and\noutputs a probability of occurrence. Further, word2net can use the hierarchical\norganization of its word networks to incorporate additional meta-data, such as\nsyntactic features, into the embedding model. For example, we show how to share\nparameters across word networks to develop an embedding model that includes\npart-of-speech information. We study word2net with two datasets, a collection\nof Wikipedia articles and a corpus of U.S. Senate speeches. Quantitatively, we\nfound that word2net outperforms popular embedding methods on predicting held-\nout words and that sharing parameters based on part of speech further boosts\nperformance. Qualitatively, word2net learns interpretable semantic representations\nand, compared to vector-based methods, better incorporates syntactic information.","pdf":"/pdf/9a7a0e8409dc920a6c97ed2d4777df567c7dfbb8.pdf","TL;DR":"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features.","paperhash":"anonymous|word2net_deep_representations_of_language","_bibtex":"@article{\n  anonymous2018word2net:,\n  title={Word2net: Deep Representations of Language},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJd_y-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper491/Authors"],"keywords":["neural language models","word embeddings","neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}