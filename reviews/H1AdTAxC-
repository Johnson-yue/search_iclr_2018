{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222659256,"tcdate":1511955648431,"number":3,"cdate":1511955648431,"id":"S1u6hGhgM","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Theory doesn't match practical algorithm","rating":"3: Clear rejection","review":"Summary of paper: This paper learns parameters of a generative model by minimizing the Wasserstein distance between the real generative model and the generative model that is being learned, as has been proposed by Arjovsky et al. (2017). The difference is that this paper uses the Hamming distance instead of the Euclidean distance. This makes the optimization difficult because the condition for the critic function to be K-Lipschitz can no longer be enforced by weight clipping. The paper proposes some sort of approximation to enforce this condition.\n\n---\n\nIt is a straightforward idea: just use Hamming instead of Euclidean distance. I'm a bit concerned with the many approximations that have to made to the algorithm made in practice. In particular:\n- It is not clear how h(x, x') relates to f(x) - f(x') in the practical algorithm.\n- The thing about switching arguments to ensure that h(x, x') = -h(x', x) is OK but but whereas h(x, x') = f(x) - f(x') implies h(x, x') = -h(x', x), the opposite is not true.\n- The rounding of of the generator output is also a bit weird: Does this not make the generator not differentiable? How is that dealt with?\n- Does having to convert to one-hot preclude this from working in high-dimensional spaces?\nIt doesn't seems that the practical algorithm goes too far from the proposed theory. This reduces the value of the theory as an explanation of why the practical algorithm works. The practical algorithm has only been tested on a toy experiment which makes judgement of the practical algorithm alone also difficult.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Promising experimental results on synthetic discrete data, and discretized data for real handwritten digits are provided. ","pdf":"/pdf/7f9e4ae79db80c0c866eaff18f5577ce0dc5e4e5.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1512222659299,"tcdate":1511808403827,"number":2,"cdate":1511808403827,"id":"Bk3c6RKgf","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer1"],"readers":["everyone"],"content":{"title":"cool idea but experiments are rather toy","rating":"5: Marginally below acceptance threshold","review":"Generative adversarial networks (GANs) are state-of-art when it comes to image generation. Many existing works extend GANs to divergences other than the original Jensen-Shannon divergence but also propose ideas for stabilizing training. One such extension is the Wasserstein GAN. This paper extends the Wasserstein GAN to discrete data. This is an important line of work since many observed data are discrete (text is an obvious example).\n\nPros:\n--The paper is very well written. Each choice leading to the final architecture and learning algorithm is well motivated\n--I liked the justification (empirical) of why existing extensions of GANs to discrete data are not optimal. \n--I appreciated the transparency regarding the shortcomings of the proposed approach. We need more of this!\n--I found the synthetic experiment very insightful.\n\nCons:\n--The experiments are very very limited. Given this is work extending GANs to discrete data I was expecting an application to text data. \n--I would have liked to see a theoretical analysis of convergence. What is the optimal discriminator like?\n\nI will improve my rating if I see results on a \"real\" experiment such as text generation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Promising experimental results on synthetic discrete data, and discretized data for real handwritten digits are provided. ","pdf":"/pdf/7f9e4ae79db80c0c866eaff18f5577ce0dc5e4e5.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1512222659340,"tcdate":1510172956115,"number":1,"cdate":1510172956115,"id":"SkNXYkbkz","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea but experiments a long way off what is required","rating":"4: Ok but not good enough - rejection","review":"This paper introduces an algorithm for learning Wasserstein GANs for discrete distributions. Taking the dual form of the discrete Wasserstein distance introduced by [Evans 1997] (which produces a constrained optimization problem) and using this as a basis of a GAN training algorithm. The key algorithmic distinction from conventional GAN approaches is that the critic takes pairs of real and simulated datapoints as inputs and returns a measure of which it thinks is the real datapoint, namely f(x_r)–f(g(z)) where x_r is the data point and g(z) the generator output, rather than the critic corresponding to f directly.  An architecture is proposed for this framework that guarantees that the constraints from the formulation of [Evans 1997] as satisfied.\n\nThe topic of this paper is timely and of clear interest to the ICLR community. The underlying idea is interesting and the architecture seems appropriate for the chosen target. Further, the paper is relatively clear and easy to follow. However, the experimental evaluation is not sufficiently strenuous and produces very underwhelming results. Relatedly, I think the motivation behind using the discrete Wasserstein distance, though seemingly reasonable, needs more careful consideration. Unfortunately, the serious shortfalls in the experiments mean that the paper, in my opinion, falls noticeably below the acceptance threshold for ICLR.  Having said that, my stance might change substantially if more impressive experimental results can be obtained; without this, I am unconvinced the method actually behaves as intended. Regretfully, this is probably beyond the scope of a revision during the rebuttal period though as it will most likely require significant algorithmic changes.\n\n%%%% Shortcomings with Experiments %%%%%\n\nPut simply, I do not think that the experiments demonstrate that the approach works and actually suggest the opposite. The so-called “mode collapse” issue is effectively a sugar-coated way of saying that the method has learned to return a single output rather than learning a generative model. This is a huge issue and needs sorting before the paper can be seriously considered for publication. The attempted at a fix at the end of the paper might be a step in the right direction but is not evaluated sufficiently and the preliminary results provided are not particularly promising.\n\nGoing past this issue, there are still a lot of problems with the experimental evaluation. More numerous and more difficult problems need to be considered. For example, an NLP problem would fit well with the motivation for the approach given in the intro. It is also necessary to demonstrate that the GAN is doing something different to just memorizing previous examples. For example, this is a problem where one can actually reasonably compare to just sampling from the data by checking that more unseen correct samples are generated then incorrect samples. \n\nIt is worrying that the convergence plots show a single line that ostensibly comes from the “best result”. This is not a reasonable scientific comparison method and should be replaced by mean (or median) performance with uncertainty estimates (i.e. +/- some numbers of standard deviations or quantiles). \n\nThe small number of samples from the MNIST problem really doesn’t convey anything meaningful – even if this looked exceptional, a small number of unqualified samples is hardly a serious evaluation metric.\n\nMore iterations for Figure 4a or needed to see if the WGAN keeps improving beyond on the DWGAN, noting that this has not converged at 100%. \n\nAll four methods should be added to Figure 5.\n\n%%%% Is the Discrete Metric Always Better for Training the Generative Model? %%%%%\n\nThough I think it is probably true that the discrete metric should more beneficial from the point of the view of the critic, I am not completely convinced it is always beneficial from the point of view of training the generator, which is at the end of the day what really matters. Note that I am not trying to argue that discrete metric is worse, just that you haven’t done enough to convince me that it is always better. If indeed it is always better, please tear apart my following argument, which will hopefully provide stimulation for improving the motivation of the metric in the paper. If it is not always better, the paper should be updated to outline some of the potential pathologies and the cases were you expect the approach to work well and when you do not.\n\nTo demonstrate my argument, consider the training sample [0, 0, 1; 0, 1, 0] and the following four example generated outputs\n\n(1) [0.333, 0.333, 0.334; \n      0.333, 0.334, 0.333]\n(2) [0, 0, 1; \n       0, 1, 0]\n(3) [0, 0, 1;\n       0.334, 0.333, 0.333]\n(4) [0.333, 0.334, 0.333; \n       0.334, 0.333, 0.333]\n\ngiving respective discrete distances to of 0, 0, 1, and 2; and continuous distances of 1.330668, 0, 0.667334, and 1.334668. Now imagine we are training a GAN with the one example datapoint [0, 0, 1; 0, 1, 0]. Even though (1) will lead to the target sample [0, 0, 1; 0, 1, 0] after passing through the argmax function and (3) will not, (1) is also very close to (4) which has the maximum possible discrete distance. Consequently, the generator for (1) is most likely not at a stable optimum and very close in the space of neural net weights to some very poor generators. During training, we would thus like to guide our network towards generating (2), as this is a stable solution, particularly given we are using stochastic optimization methods. From this perspective, then (3) is perhaps a better generated output than (1), a fact conveyed by the continuous metric but not the discrete metric. In other words, even though (1) is arguably a better final solution than (3), from the perspective of effective training, it may be favorable to use a target function that prefers (3) to (1) to better guide the training to a stable solution.\n\nOnce we consider the fact that our aim is not to replicate a single datapoint but learn a generator that in some way interpolates between datapoints, it becomes even less clear if the discrete metric is better. For example, small changes to the input z for (1) are likely to lead to generating samples that are very different to anything seen in the training data (which is, in this case, a single point).\n\nThough I do not think this argument undermines the suggested approach, I do think it highlights why the supplied motivation for the approach is insufficient and needs more explicitly linking back to the training procedure. At the very least, I think the above argument shows why it is not immediately clear cut that the suggested approach will perform better and so needs backing up with strong empirical evidence, which unfortunately the paper does not currently have, and/or a more convincing argument for why the discrete metric is better.\n\n%%%% Other points %%%% \n\n- Tables 1, 2, and 3 are not worth spending more than a few lines on, let alone nearly a page. They should be substantially compressed or preferably just cut (particularly Tables 2 and 3). The point they convey is obvious and actually somewhat tangential to the key questions.\n\n- It worries me that the method learns an h that explicitly takes y as input, not x. This is a notational issue in the exposition, but also, more importantly, raises questions about whether the original linear programming problem is actually being solved because the inputs from the generative method are not discrete variables.\n\n- It should be made clear that the Appendices are directly following the derivation of Evans 1997, rather than being a new derivation.\n\n- At times the paper is little sloppy at making distinguishing between y ∈ [0, 1] and y ∈ {0, 1}. This makes it a bit confusing at times what is output from the generator, particularly when you talk about it being one-hot encoded. As I understand it, the output is always the former, while data lives in the latter but this is not always clear. I think you should also make a bigger deal of this in terms of the problem with previous methods that ignore that the critic might be able to distinguish based on the fact that the training and generated data points have an explicitly different type (discrete and continuous respectively).\n\n- On the fourth line of the abstract, both instances of GAN should be GANs.\n\n- What do you mean “explained in the sequel”?\n\n- Does k need to be the same for each variable?\n\n%%%% References %%%%\n\nLawrence C Evans. Partial differential equations and Monge-Kantorovich mass transfer. Current developments in mathematics, 1997(1):65–126, 1997.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Promising experimental results on synthetic discrete data, and discretized data for real handwritten digits are provided. ","pdf":"/pdf/7f9e4ae79db80c0c866eaff18f5577ce0dc5e4e5.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1509739286134,"tcdate":1509121398495,"number":467,"cdate":1509739283466,"id":"H1AdTAxC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Promising experimental results on synthetic discrete data, and discretized data for real handwritten digits are provided. ","pdf":"/pdf/7f9e4ae79db80c0c866eaff18f5577ce0dc5e4e5.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}