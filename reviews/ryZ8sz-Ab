{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222820635,"tcdate":1511884464029,"number":3,"cdate":1511884464029,"id":"rk_3IZjlM","invitation":"ICLR.cc/2018/Conference/-/Paper934/Official_Review","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["ICLR.cc/2018/Conference/Paper934/AnonReviewer1"],"readers":["everyone"],"content":{"title":"You don't need to read the entire review to classify it.","rating":"7: Good paper, accept","review":"The paper present a model for fast reading for text classification with mechanisms that allow the model to reread, skip words, or classify early before reading the entire review. The model contains a policy module that makes decisions on whether to reread, skim or stop, which is rewarded for both classification accuracy and computation cost. The entire architecture is trained end-to-end with backpropagation, Monte Carlo rollouts and a baseline for variance reduction. \n\nThe results show that the architecture is able to classify accurately on all syntactic levels, faster than a baseline that reads the entire text. The approach is simple and seems to work well and could be applied to other tasks where inference time is important.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222820676,"tcdate":1511825823013,"number":2,"cdate":1511825823013,"id":"SkDsWQqgz","invitation":"ICLR.cc/2018/Conference/-/Paper934/Official_Review","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["ICLR.cc/2018/Conference/Paper934/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Unclear focus: is it the speed gains or the improved accuracy? Experimental comparison not convincing","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to augment RNNs for text classification with a mechanism that decides whether the RNN should re-read a token, skip a number of tokens, or stop and output the prediction. The motivation is that one can stop reading before the end of the text and/or skip some words and still arrive to the same answer but faster.\n\nThe idea is intriguing, even though not entirely novel. Apart from the Yu et al. (2017) cited, there is older work trying to save computational time in NLP, e.g.: \nDynamic Feature Selection for Dependency Parsing.\nHe He, Hal Daum√© III and Jason Eisner.\nEmpirical Methods in Natural Language Processing (EMNLP), 2013\nthat decides whether to extract a feature or not.\nHowever, what is not clear to me what is achieved here. In the example shown in Figure 5 it seems like what happens is that by not reading the whole text the model avoids passages that might be confusing it. This could improve predictive accuracy (as it seems to do), as long as the model can handle better the earlier part of the text. But this is just an assumption, which is not guaranteed in any way. It could be that the earlier parts of the text are hard for the model. In a way, it feels more like we are addressing a limitation of RNN models in understanding text. \n\nPros:\n- The idea is intersting and if evaluated thoroughly it could be quite influential.\n\nCons:\n- the introduction states that there are two kinds of NLP problems, sequence2sequence and sequence2scalar. I found this rather confusing since text classification falls in the latter presumably, but the output is a label. Similarly, PoS tagging has a linear chain as its output, can't see why it is sequence2scalar. I think there is a confusion between the methods used for a task, and the task itself. Being able to apply a sequence-based model to a task, doesn't make it sequential necessarily.\n\n- the comparison in terms of FLOPs is a good idea. But wouldn't the relative savings depend on the architecture used for the RNN and the RL agent? E.g. it could be that the RL network is more complicated and using it costs more than what it saves in the RNN operations.\n\n- While table 2 reports the savings vs the full reading model, we don't know how much worse the model got for these savings. \n\n- Having a trade-off between savings and accuracy is a good idea too. I would have liked to see an experiment showing how many FLOPs we can save for the same performance, which should be achievable by adjusting the alpha parameter.\n\n- The experiments are conducted on previously published datasets. It would be good to have some previously published results on them to get a sense of how good the RNN model used is.\n\n- Why not use smaller chunks? 20 words or one sentence at the time is rather coarse. If anything, it should help the model proposed achieve greater savings. How much does the choice of chunk matter?\n\n- it is stated in the conclusion that the advantage actor-critic used is beneficial, however no experimental comparison is shown. Was it used for the Yu et al. baseline too?\n\n- It is stated that model hardly relies on any hyperparameters; in comparison to what? It is better to quantify such statements,","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222820752,"tcdate":1511802213582,"number":1,"cdate":1511802213582,"id":"HyAwBpKeG","invitation":"ICLR.cc/2018/Conference/-/Paper934/Official_Review","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["ICLR.cc/2018/Conference/Paper934/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting model but strong lacks in related works","rating":"5: Marginally below acceptance threshold","review":"The authors propose a sequential algorithm that tackles text classification while introducing the ability to stop the reading when the decision to make is confident enough. This sequential framework -reinforcement learning with budget constraint- will be applied to document classification tasks. The authors propose a unified framework enabling the recurrent network to reread or skip some parts of a document. Then, the authors describe the process to ensure both a good classification ability & a reduced budget.\nExperiments are conducted on the IMDB dataset and the authors demonstrate the interest to select the relevant part of the document to make their decision. They improve both the accuracy & decision budget.\n\n\nIn the architecture, fig 1, it is strange to see that the decision to stop is taken before considering the label probability distribution. This choice is probably made to fit with classical sequential decision algorithms, assuming that the confidence level can be extracted from the latent representation... However, it should be discussed.\n\nThe interest of rereading a word/sentence is not clear for me: we simply choose to overweight the recent past wrt the further. Can it be seen as a way to overcome a weakness in the information extraction?\n\nAt the end of page 4, the difference between the early stopping model & partial reading model is not clear for me. How can the partial reading model be overcome by the early-stopping approach? They operate on the same data, with the same computational cost (ie with the same algorithm?)\n\nAt the end of page 6, authors claim that their advantage over Yu et al. 2017 comes from their rereading & early stopping abilities:\n- given the length of the reviews may the K-skip ability of Yu et al. 2017 be seen as an early stopping approach?\n- are the authors confident about the implementation of the Yu et al. 2017' strategy?\n- Regarding the re-reading ability: the experimental section is very poor and we wonder:\n  -- how the performance is impacted by rereading?\n  -- how many time does the algorithm choose to reread?\n  -- experiments on early-stopping VS early-stopping + skipping + rereading are interesting... We want to see the impact of the other aspects of the contribution.\n\nOn the sentiment analysis task, how does the model behave wrt the state of the art?\n\nGiven the chosen tasks, this work should be compared to the beermind system:\nhttp://deepx.ucsd.edu/#/home/beermind\nand the associated publication\nhttp://arxiv.org/pdf/1511.03683.pdf\nBut the authors should also refer to previous work on their topic:\nhttps://arxiv.org/pdf/1107.1322\nThe above mentioned reference is really close to their work.\n\n\nThis article describes an interesting approach but its main weakness resides in the lack of positioning wrt the literature and the lack of comparison with state-of-the-art models on the considered tasks.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1511776367626,"tcdate":1511776367626,"number":2,"cdate":1511776367626,"id":"BkOOxwFxz","invitation":"ICLR.cc/2018/Conference/-/Paper934/Public_Comment","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related Workd","comment":"''Text Classification: A Sequential Reading Approach.'' published in 2011 is also clearly related. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1511765901662,"tcdate":1511765901662,"number":1,"cdate":1511765901662,"id":"ry89PEKgM","invitation":"ICLR.cc/2018/Conference/-/Paper934/Public_Comment","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Early stopping","comment":"The early stopping idea is already implemented in the related work \"Learning to Skim Text\", i.e., the reading will stop if 0 is sampled from the jumping softmax. This can be seen in the two examples of their last experiment."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1510865702873,"tcdate":1509397249993,"number":1,"cdate":1509397249993,"id":"Hkc-XfHA-","invitation":"ICLR.cc/2018/Conference/-/Paper927/Public_Comment","forum":"ryZ8sz-Ab","replyto":"ryZ8sz-Ab","signatures":["~Rahul_Ravu1"],"readers":["everyone"],"writers":["~Rahul_Ravu1"],"content":{"title":"Related Work","comment":"The paper referenced in Related Work, \"Rationalizing Neural Predictions\" seems a bit similar to the current work. Also, the comment that they use attention for generating rationales is a bit confusing as the encoder which generates the label only sees the rationales(subset of the original text unless I am mistaken)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1510092385684,"tcdate":1509137226974,"number":934,"cdate":1510092362328,"id":"ryZ8sz-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryZ8sz-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping","abstract":"Recent advances in recurrent neural nets (RNNs) have shown much promise in many applications in natural language processing. For most of these tasks, such as sentiment analysis of customer reviews, a recurrent neural net model parses the entire review before forming a decision. We argue that reading the entire input is not always necessary in practice, since a lot of reviews are often easy to classify, i.e., a decision can be formed after reading some crucial sentences or words in the provided text. In this paper, we present an approach of fast reading for text classification. Inspired by several well-known human reading techniques, our approach implements an intelligent recurrent agent which evaluates the importance of the current snippet in order to decide whether to make a prediction, or to skip some texts, or to re-read part of the sentence. Our agent uses an RNN module to encode information from the past and the current tokens, and applies a policy module to form decisions. With an end-to-end training algorithm based on policy gradient, we train and test our agent on several text classification datasets and achieve both higher efficiency and better accuracy compared to previous approaches. \n","pdf":"/pdf/748926bc02b774ab9382a6589bd0940a11059f4b.pdf","TL;DR":"We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to seq2scalar tasks. ","paperhash":"anonymous|fast_and_accurate_text_classification_skimming_rereading_and_early_stopping","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryZ8sz-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper934/Authors"],"keywords":["Topic Classification","Sentiment Analysis","Natural Language Processing"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}