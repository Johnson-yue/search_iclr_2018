{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222826620,"tcdate":1511822604786,"number":2,"cdate":1511822604786,"id":"ByrfSMcgz","invitation":"ICLR.cc/2018/Conference/-/Paper946/Official_Review","forum":"SyfiiMZA-","replyto":"SyfiiMZA-","signatures":["ICLR.cc/2018/Conference/Paper946/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice to see this topic pop up again, but paper is lacking comparisons and insights.","rating":"4: Ok but not good enough - rejection","review":"I'm glad to see the concept of jointly learning to control and evolve pop up again!\n\nUnfortunately, this paper has a number of weak points that - I believe - make it unfit for publication in its current state.\nMain weak points:\n- No comparisons to other methods (e.g. switch between policy optimization for the controller and CMA-ES for the mechanical parameters). The basic result of the paper is that allowing PPO to optimize more parameters, achieves better results...\n- One can argue that this is not true joint optimization Mechanical and control parameters are still treated differently. This begs the question: How should one define mechanical \"variables\" in order for them to behave similarly to other optimization variables (assuming that mechanical and control parameters influence the performance in a similar way)?\n\nAdditional relevant papers (slightly different approach):\nhttp://www.pnas.org/content/108/4/1234.full#sec-1\nhttp://ai2-s2-pdfs.s3.amazonaws.com/ad27/0104325010f54d1765fdced3af925ecbfeda.pdf\n\nMinor issues:\nFigure 1: please add labels/captions\nFigure 2: please label the axes\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning","abstract":"The physical design of a robot and the policy that controls its motion are inherently coupled. However, existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases, which requires expert intuition throughout and risks convergence to suboptimal designs. In this work, we propose a method that jointly optimizes over the physical design of a robot and the corresponding control policy in a model-free fashion, without any need for expert supervision. Given an arbitrary robot morphology, our method maintains a distribution over the design parameters and uses reinforcement learning to train a neural network controller. Throughout training, we refine the robot distribution to maximize the expected reward. This results in an assignment to the robot parameters and neural network policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel robot designs and walking gaits for several different morphologies, achieving performance comparable to or better than that of hand-crafted designs.","pdf":"/pdf/61209d2fa0c99fe17b00d5e5c2decb02599cf2f4.pdf","TL;DR":"Use deep reinforcement learning to design the physical attributes of a robot jointly with a control policy.","paperhash":"anonymous|jointly_learning_to_construct_and_control_agents_using_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018jointly,\n  title={Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyfiiMZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper946/Authors"],"keywords":["robot locomotion","reinforcement learning","policy gradients","physical design","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222826666,"tcdate":1511667426998,"number":1,"cdate":1511667426998,"id":"SksyD3Dgz","invitation":"ICLR.cc/2018/Conference/-/Paper946/Official_Review","forum":"SyfiiMZA-","replyto":"SyfiiMZA-","signatures":["ICLR.cc/2018/Conference/Paper946/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Paper of broad interest for control tasks","rating":"9: Top 15% of accepted papers, strong accept","review":"This is a well written paper, very nice work.\nIt makes progress on the problem of co-optimization of the physical parameters of a design\nand its control system.  While it is not the first to explore this kind of direction,\nthe method is efficient for what it does; it shows that at least for some systems, \nthe physical parameters can be optimized without optimizing the controller for each \nindividual configuration. Instead, they require that the same controller works over an evolving\ndistribution of the agents.  This is a simple-but-solid insight that makes it possible\nto make real progress on a difficult problem.\n\nPros:  simple idea with impact;  the problem being tackled is a difficult one\nCons:  not many;  real systems have constraints between physical dimensions and the forces/torques they can exert\n       Some additional related work to consider citing.  The resulting solutions are not necessarily natural configurations, \n   given the use of torques instead of musculotendon-modeling.  But the current system is a great start.\n\nThe introduction could also promote that over an evolutionary time-frame, the body and\ncontrol system (reflexes, muscle capabilities, etc.) presumably co-evolved.\n\nThe following papers all optimize over both the motion control and the physical configuration of the agents.\nThey all use derivative free optimization, and thus do not require detailed supervision or precise models\nof the dynamics.\n\n- Geijtenbeek, T., van de Panne, M., & van der Stappen, A. F. (2013). Flexible muscle-based locomotion\n  for bipedal creatures. ACM Transactions on Graphics (TOG), 32(6), 206.\n  (muscle routing parameters, including insertion and attachment points) are optimized along with the control).\n\n- Sims, K. (1994, July). Evolving virtual creatures. In Proceedings of the 21st annual conference on\n  Computer graphics and interactive techniques (pp. 15-22). ACM.\n  (a combination of morphology, and control are co-optimized)\n\n- Agrawal, S., Shen, S., & van de Panne, M. (2014). Diverse Motions and Character Shapes for Simulated\n  Skills. IEEE transactions on visualization and computer graphics, 20(10), 1345-1355.\n  (diversity in control and diversity in body morphology are explored for fixed tasks)\n\nre: heavier feet requiring stronger ankles\nThis commment is worth revisiting.  Stronger ankles are more generally correlated with \na heavier body rather than heavy feet, given that a key role of the ankle is to be able\nto provide a \"push\" to the body at the end of a stride, and perhaps less for \"lifting the foot\".\n\nI am surprised that the optimization does not converge to more degenerate solutions\ngiven that the capability to generate forces and torques is independent of the actual\nlink masses, whereas in nature, larger muscles (and therefore larger masses) would correlate\nwith the ability to generate larger forces and torques.  The work of Sims takes these kinds of \nconstraints loosely into account (see end of sec 3.3).\n\nIt would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.\n\nI suspect that the reward function that penalizes torques in a uniform fashion across all joints would\nfavor body configurations that more evenly distribute the motion effort across all joints, in an effort\nto avoid large torques. \n\nAre the four mixture components over the robot parameters updated independently of each other\nwhen the parameter-exploring policy gradients updates are applied?  It would be interesting\nto know a bit more about how the mean and variances of these modes behave over time during\nthe optimization, i.e., do multiple modes end up converging to the same mean? What does the\nevolution of the variances look like for the various modes?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning","abstract":"The physical design of a robot and the policy that controls its motion are inherently coupled. However, existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases, which requires expert intuition throughout and risks convergence to suboptimal designs. In this work, we propose a method that jointly optimizes over the physical design of a robot and the corresponding control policy in a model-free fashion, without any need for expert supervision. Given an arbitrary robot morphology, our method maintains a distribution over the design parameters and uses reinforcement learning to train a neural network controller. Throughout training, we refine the robot distribution to maximize the expected reward. This results in an assignment to the robot parameters and neural network policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel robot designs and walking gaits for several different morphologies, achieving performance comparable to or better than that of hand-crafted designs.","pdf":"/pdf/61209d2fa0c99fe17b00d5e5c2decb02599cf2f4.pdf","TL;DR":"Use deep reinforcement learning to design the physical attributes of a robot jointly with a control policy.","paperhash":"anonymous|jointly_learning_to_construct_and_control_agents_using_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018jointly,\n  title={Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyfiiMZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper946/Authors"],"keywords":["robot locomotion","reinforcement learning","policy gradients","physical design","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092383833,"tcdate":1509137311082,"number":946,"cdate":1510092362054,"id":"SyfiiMZA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyfiiMZA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning","abstract":"The physical design of a robot and the policy that controls its motion are inherently coupled. However, existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases, which requires expert intuition throughout and risks convergence to suboptimal designs. In this work, we propose a method that jointly optimizes over the physical design of a robot and the corresponding control policy in a model-free fashion, without any need for expert supervision. Given an arbitrary robot morphology, our method maintains a distribution over the design parameters and uses reinforcement learning to train a neural network controller. Throughout training, we refine the robot distribution to maximize the expected reward. This results in an assignment to the robot parameters and neural network policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel robot designs and walking gaits for several different morphologies, achieving performance comparable to or better than that of hand-crafted designs.","pdf":"/pdf/61209d2fa0c99fe17b00d5e5c2decb02599cf2f4.pdf","TL;DR":"Use deep reinforcement learning to design the physical attributes of a robot jointly with a control policy.","paperhash":"anonymous|jointly_learning_to_construct_and_control_agents_using_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018jointly,\n  title={Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyfiiMZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper946/Authors"],"keywords":["robot locomotion","reinforcement learning","policy gradients","physical design","deep learning"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}