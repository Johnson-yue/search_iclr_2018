{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222658773,"tcdate":1511810199658,"number":3,"cdate":1511810199658,"id":"BkgsE19xz","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer1"],"readers":["everyone"],"content":{"title":"New framework for an important problem, supported with experiments in basic cases.","rating":"6: Marginally above acceptance threshold","review":"The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning. The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations. \n\nThe paper is well written, the problem is described neatly in conjunction with the past work, and the proposed algorithm is supported by experiments. The work is a useful addition to the community.\n\nMy main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/5e7346b695d353922e8de56336006a4d0e9cfef1.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222658813,"tcdate":1511800885096,"number":2,"cdate":1511800885096,"id":"H1T4epKeM","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Seemingly significant finding, but the title should be rephrased","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method, called VCL, for continual learning. This method is a combination of the online variational inference for streaming environment with Monte Carlo method. The authors further propose to maintain a coreset which consists of representative data points from the past tasks. Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning. Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods. \n\nThe authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem. Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning. Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning. From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role. \n\nThe finding of the effectiveness of idea (1) seems to be significant. The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance.\n\nThere are some concerns the authors should consider:\n- Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper. When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000]. Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well.\n- It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect.\n- What is the trade-off when the size of the coreset increases?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/5e7346b695d353922e8de56336006a4d0e9cfef1.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222658854,"tcdate":1511521232840,"number":1,"cdate":1511521232840,"id":"SyF0odSef","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper introduces a varaitional continual learning framework for neural networks. ","rating":"6: Marginally above acceptance threshold","review":"Overall, the idea of this paper is simple but interesting. Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty.\n\nThe paper is written well, and literature review is sufficient. My comment is mainly about its importance for large-scale computer vision applications. The neural networks in the experiments are shallow. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/5e7346b695d353922e8de56336006a4d0e9cfef1.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1509739290105,"tcdate":1509120651023,"number":463,"cdate":1509739287438,"id":"BkQqq0gRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/5e7346b695d353922e8de56336006a4d0e9cfef1.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}