{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222564074,"tcdate":1512005166136,"number":3,"cdate":1512005166136,"id":"BJ8VRRhgM","invitation":"ICLR.cc/2018/Conference/-/Paper1172/Official_Review","forum":"H1T2hmZAb","replyto":"H1T2hmZAb","signatures":["ICLR.cc/2018/Conference/Paper1172/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Using complex numbers for neural networks , but why?","rating":"4: Ok but not good enough - rejection","review":"Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions. Their \"related work section\" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations. However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued.\n\nTheir contributions are:\n\n1. Formulate complex valued convolution\n2. Formulate two complex-valued alternatives to ReLU and compare them\n3. Formulate complex batch normalization as a \"whitening\" operation on complex domain\n4. Formulate complex analogue of Glorot weight normalization scheme\n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case. For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward. However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic. Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Complex Networks","abstract":"At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech spectrum prediction using TIMIT. We achieve state-of-the-art performance on these audio-related tasks.","pdf":"/pdf/21bc670e37fcb28f944d33f287f626306b316875.pdf","paperhash":"anonymous|deep_complex_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Complex Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1T2hmZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1172/Authors"],"keywords":["deep learning","complex-valued neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222564117,"tcdate":1511901293138,"number":2,"cdate":1511901293138,"id":"rJH_dHjeG","invitation":"ICLR.cc/2018/Conference/-/Paper1172/Official_Review","forum":"H1T2hmZAb","replyto":"H1T2hmZAb","signatures":["ICLR.cc/2018/Conference/Paper1172/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An extensive framework for complex-valued neural networks is presented.","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper presents an extensive framework for complex-valued neural networks. Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more. \n\nThe contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks. Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc. In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network. \n\nEmpirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks. Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Complex Networks","abstract":"At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech spectrum prediction using TIMIT. We achieve state-of-the-art performance on these audio-related tasks.","pdf":"/pdf/21bc670e37fcb28f944d33f287f626306b316875.pdf","paperhash":"anonymous|deep_complex_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Complex Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1T2hmZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1172/Authors"],"keywords":["deep learning","complex-valued neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222564162,"tcdate":1511892982636,"number":1,"cdate":1511892982636,"id":"SyJZuXjlG","invitation":"ICLR.cc/2018/Conference/-/Paper1172/Official_Review","forum":"H1T2hmZAb","replyto":"H1T2hmZAb","signatures":["ICLR.cc/2018/Conference/Paper1172/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well-written, but unclear what happens with phase information","rating":"5: Marginally below acceptance threshold","review":"This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy. The writing is clear, concise and easy to follow.\n\nAn important argument in favour of using complex-valued networks is said to be the propagation of phase information. However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat. CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise. It definitely does not preserve phase, like modReLU would.\n\nThis makes me wonder whether the \"complex numbers\" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in \"Compressing neural networks with the hashing trick\" by Chen et al.). Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this.\n\nThe image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach. The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated. It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague. Once again, a more in-depth analysis of this phase behavior would be very welcome.\n\nI'm on the fence about this work: I like the ideas and they are explained well, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used).\n\n\nComments:\n\n- The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work. Maybe some subsection titles would help make it feel a bit more cohesive.\n\n- page 3: \"(cite a couple of them)\" should be replaced by some actual references :)\n\n- Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer? It would be nice to discuss computational cost as well.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Complex Networks","abstract":"At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech spectrum prediction using TIMIT. We achieve state-of-the-art performance on these audio-related tasks.","pdf":"/pdf/21bc670e37fcb28f944d33f287f626306b316875.pdf","paperhash":"anonymous|deep_complex_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Complex Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1T2hmZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1172/Authors"],"keywords":["deep learning","complex-valued neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1510092378910,"tcdate":1509141685037,"number":1172,"cdate":1510092359103,"id":"H1T2hmZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1T2hmZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Complex Networks","abstract":"At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech spectrum prediction using TIMIT. We achieve state-of-the-art performance on these audio-related tasks.","pdf":"/pdf/21bc670e37fcb28f944d33f287f626306b316875.pdf","paperhash":"anonymous|deep_complex_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Complex Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1T2hmZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1172/Authors"],"keywords":["deep learning","complex-valued neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}