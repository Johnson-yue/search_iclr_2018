{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222703375,"tcdate":1510423753197,"number":1,"cdate":1510423753197,"id":"BkZAh3E1M","invitation":"ICLR.cc/2018/Conference/-/Paper625/Official_Review","forum":"ryPW2e-A-","replyto":"ryPW2e-A-","signatures":["ICLR.cc/2018/Conference/Paper625/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Using hard negatives in triplet loss improves embedding. Not enough novelty for ICLR.","rating":"4: Ok but not good enough - rejection","review":"The paper describes learning joint embedding of sentences and images. The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets. This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K. \n\nThe main issue with this paper is novelty. Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications. For instance recently Wu et all in ICCV2017, bu also many other papers. \nWhen used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label. In these cases focusing on the hardest negative reduces performance. \n\nWhile it is good to know that using hard negatives improves recall measures on coco, it is not clear that this paper provides enough novel insight to be interesting enough for the ICLR audience. It may be a better fit in a conference that stresses empirical performance, like in machine vision conferences. \n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives","abstract":"We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).","pdf":"/pdf/70312a77b15314a5fbc321aa8d14fcd34a6f3a9f.pdf","TL;DR":"A new loss based on relatively hard negatives that achieves state-of-the-art performance in image-caption retrieval.","paperhash":"anonymous|vse_improving_visualsemantic_embeddings_with_hard_negatives","_bibtex":"@article{\n  anonymous2018vse++:,\n  title={VSE++: Improving Visual-Semantic Embeddings with Hard Negatives},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryPW2e-A-}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper625/Authors"],"keywords":["Joint embeddings","Hard Negatives","Visual-semantic embeddings","Cross-modal retrieval","Ranking"]}}],"limit":2000,"offset":0}