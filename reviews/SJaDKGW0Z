{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222803836,"tcdate":1511895137914,"number":3,"cdate":1511895137914,"id":"SJ5vx4ixM","invitation":"ICLR.cc/2018/Conference/-/Paper889/Official_Review","forum":"SJaDKGW0Z","replyto":"SJaDKGW0Z","signatures":["ICLR.cc/2018/Conference/Paper889/AnonReviewer2"],"readers":["everyone"],"content":{"title":"incremental learning as transfer learning, no comparison to other approaches, paper organization should be improved","rating":"4: Ok but not good enough - rejection","review":"Strengths:\n* Simple approach to incremental learning.\n* Can operate in scenarios where new classes are incrementally introduced over time.\n* By design, the procedure retains the ability to recognize old classes while learning new classes.\n* A comprehensive study of energy consumption is presented to analyze energy efficiency during training.\n\nWeaknesses:\n* The proposed procedure is commonly used for transfer learning. The only novelty lies in the application and in the use of a separate branch to learn to recognize new classes while retaining the ability to discriminate old classes.\n* The approach is gradually introduced by presenting three different variants of increasing sophistication. While I understand that this is done to motivate the need for the more complex method, it makes the presentation of the technical approach unnecessarily long. The reader loses the full picture by the time he/she reaches the section describing the final approach. Even more concerning is the fact that each variant is presented and tested using a different network architecture and a different dataset, with no experimental comparison among these variants. This makes it hard to assess the practical value of each method. \n* The final approach is compared against a baseline that involves no parameter sharing. There is no experimental comparison to prior work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing","abstract":"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","pdf":"/pdf/8daed5bfae4170779872ce77a2b14b5d81a84027.pdf","TL;DR":"The paper is about a new energy-efficient methodology for Incremental learning","paperhash":"anonymous|incremental_learning_in_deep_convolutional_neural_networks_using_partial_network_sharing","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJaDKGW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper889/Authors"],"keywords":["Deep learning","Incremental learning","energy-efficient learning","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222803877,"tcdate":1511757946858,"number":2,"cdate":1511757946858,"id":"HJXF_zKlM","invitation":"ICLR.cc/2018/Conference/-/Paper889/Official_Review","forum":"SJaDKGW0Z","replyto":"SJaDKGW0Z","signatures":["ICLR.cc/2018/Conference/Paper889/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review for Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing","rating":"2: Strong rejection","review":"In this paper, the authors propose a method to train deep convolutional neural networks in an incremental fashion, in which data are available in small batches over a period of time. In the proposed approach, parts of the networks are shared among all the tasks, while new layers are added for new categories. The paper is easy to follow and the idea is clearly explained. The authors show results on different datasets, but fail to compare with state-of-the-art models on this subject.\n\nSome strong negative points about the paper:\n- The method proposed by the authors is the most trivial possible way to try to avoid catastrophic forgetting. Given that the shared features are not fine-tuned on the new categories, this method is likely not optimal for the new tasks either. \n- The literature on incremental learning with deep learning is plentiful. Many authors have been working on this interesting problems (eg, \"iCarl\", rebuffi et al., \"Learning without Forgetting\", Li and Hoeim, \"Gradient Episodic Memory for Continual Learning\", Lopez-Paz and Ranzato, Progressive Networks, Ruzu et al., etc.). These works should be cited and directly compared.\n- The experimental section is also missing multiples experiments. The authors should validate the proposed method with state-of-the-art models on the task instead of only comparing to themselves.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing","abstract":"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","pdf":"/pdf/8daed5bfae4170779872ce77a2b14b5d81a84027.pdf","TL;DR":"The paper is about a new energy-efficient methodology for Incremental learning","paperhash":"anonymous|incremental_learning_in_deep_convolutional_neural_networks_using_partial_network_sharing","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJaDKGW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper889/Authors"],"keywords":["Deep learning","Incremental learning","energy-efficient learning","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222806780,"tcdate":1510963655160,"number":1,"cdate":1510963655160,"id":"rkJCKea1G","invitation":"ICLR.cc/2018/Conference/-/Paper889/Official_Review","forum":"SJaDKGW0Z","replyto":"SJaDKGW0Z","signatures":["ICLR.cc/2018/Conference/Paper889/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Important problem, limited novelty, no reference/comparison to related works","rating":"4: Ok but not good enough - rejection","review":"\n----------------- Summary -----------------\nThe paper presents an approach to class-incremental learning using deep networks. It proposes three different learning strategies, in the final/best approach, when new classes arrive, the current network's last n layers are retrained using the new classes. It is shown, by doing this, the updated network can learn to discriminate the new classes and still not completely lose the accuracy on the previous classes. It further makes some arguments about energy efficiency of incrementally-learned networks with fixed layers versus retraining all layers.\n\n----------------- Overall -----------------\nThe work is tackling a crucial problem for learning systems which is the ability to learn from new data/classes as they emerge, without the prohibitively-expensive need to train from scratch. It conducts many experiments regarding this. However, it fails to correctly and conclusively put itself in the context of the prior works on (class-)incremental learning. It does not compare its method to any other approach. Also, does not provide important details of its implementation. Finally, the novelty of the work is also limited to the common approach of fine-tuning last layers of a pre-trained network. So, all in all, at this point the paper is not ready for publication at ICLR.\n\n\n----------------- Details -----------------\n- The related-works section covers important topics. There are recent works on incremental learning for deep networks as well as catastrophic forgetting, few(zero)-shot learning, life-long learning that are quite relevant to this work. Here's a few, but I'd suggest the authors to make a more thorough literature study on the aforementioned subjects:\n1) Learning without Forgetting ECCV 2016\n2) Incremental Classifier and Representation Learning CVPR 2017\n3) Expert Gate: Lifelong Learning with a Network of Experts CVPR 2017\n4) Incremental learning of object detectors without catastrophic forgetting ICCV 2017\n\n- In the experiments section no baseline is included on the alternative approaches. This is important to understand the significance of the work in its proper context.\n\n- The three approaches which are presented are not properly compared. They use different architectures on different datasets.\n\n- As more classes are added and the network is (partially) retrained, it will be informative to report the change of the accuracy on only the previous classes --to see how much forgetting/confusion is introduced at each step.\n\n- The hypter-parameters settings are missing. It is crucially important how the learning rate is chosen; especially to get the best results in the trade-off of forgetting previous classses and learning the new classes. A too-high learning rate would forget everything while a too-low learning rate would not learn the new classes properly. Thus, it is important for the learning rate to be properly cross-validated and not optimized for the evaluation set.\n\n- The plot in Figure 2.b is interesting. For sufficiently-high learning-rate regimes, one would expect the no-sharing point in Figure 2.b to not necessarily be the best option. That is, the trend in Figure 2.b would correlate with the choice of hyper parameters.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing","abstract":"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","pdf":"/pdf/8daed5bfae4170779872ce77a2b14b5d81a84027.pdf","TL;DR":"The paper is about a new energy-efficient methodology for Incremental learning","paperhash":"anonymous|incremental_learning_in_deep_convolutional_neural_networks_using_partial_network_sharing","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJaDKGW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper889/Authors"],"keywords":["Deep learning","Incremental learning","energy-efficient learning","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092386358,"tcdate":1509136740990,"number":889,"cdate":1510092362754,"id":"SJaDKGW0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJaDKGW0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing","abstract":"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements. Also, previously seen training samples may not be available at the time of retraining. We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes. An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network. We evaluated the proposed scheme on several recognition applications. The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","pdf":"/pdf/8daed5bfae4170779872ce77a2b14b5d81a84027.pdf","TL;DR":"The paper is about a new energy-efficient methodology for Incremental learning","paperhash":"anonymous|incremental_learning_in_deep_convolutional_neural_networks_using_partial_network_sharing","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJaDKGW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper889/Authors"],"keywords":["Deep learning","Incremental learning","energy-efficient learning","supervised learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}