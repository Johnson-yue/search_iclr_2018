{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222605736,"tcdate":1511760392544,"number":3,"cdate":1511760392544,"id":"SJ1GzQKxz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice paper, however, I would be happier if more experiments on larger datasets are presented","rating":"6: Marginally above acceptance threshold","review":"In this paper, authors discuss the use of block-diagonal hessian when computing the updates. The block-diagonal hessian makes it easier to solve the \"newton\" directions, as the CG can be run only on smaller blocks (and hence less CG iterations are needed).\n\nThe paper is nicely written and all was clear to me. In general, I agree that having larger batch-size is the way to go, for very large datasets and a pure SGD type of methods are having problems to efficiently utilize large clusters.\n\nThe only negative thing I find in the paper is the lack of more numerical results. Indeed, the paper is clearly not a theoretical paper, is proposing a new algorithm, hence there should be evidence that it works. For example, I would like to see how the choice of hyper-parameters influences the speed of the algorithm. Was \"CG\" cost included in the \"x\"-axis? i.e. if we put \"passes\" over the data as x-axis, then 1 update \\approx 30 CG + some more == 32 batch evaluation.\nSo please try to make the \"x\"-axis more fair.\n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1512222605780,"tcdate":1511654437607,"number":2,"cdate":1511654437607,"id":"BkCQ4Ywgz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes a block-diagonal second order method for training deep networks. The algorithm is not  novel. Experimental results are good in training auto-encoder and LSTM (in terms of number of updates). ","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a block-diagonal hessian-free method for training deep networks. \n\n- The block-diagonal approximation has been used in [1]. Although [1] is using Gauss-Newton matrix, the idea of \"block-diagonal\" approximation is similar. \n\n- Is the computational time (per iteration) of the proposed method similar to SGD/Adam? All the figures are showing the comparison in terms of number of updates, but it is not clear whether this speedup can be reflected in the training time. \n\n- Comparing block-diagonal approximation vs original HF method: \nIt is not clear to me what's the benefit using block-diagonal approximation. Is the time cost per iteration similar or faster? \nOr the main benefit is to reduce #CG iterations? (but it seems #CG iterations are fixed for both methods in the experiments). \nAlso, the paper mentioned that \"the HF method requires many hundreds of CG iterations for one update\". Is this true?\n Usually we can set a stopping condition for solving the Newton system.\n\n- It seems the benefit of block-diagonal approximation is marginal in CNN. \n\n[1] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1512222608205,"tcdate":1511122359313,"number":1,"cdate":1511122359313,"id":"SyJaBw1eG","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer3"],"readers":["everyone"],"content":{"title":"There is nothing particularly wrong with the paper - it is a nice work, that is based a lot on previous attempts, like those in Martens. Coming from a more theoretical background, I would like to see more theory. Nevetheless this does not lessens the value of the paper. For the moment, weak accept until I also read the other reviews. ","rating":"6: Marginally above acceptance threshold","review":"Summary: \nThe paper considers second-order optimization methods for training of neural networks.\nIn particular, the contribution of the paper is a Hessian-free method that works on blocks of parameters (this is a user defined splitting of the parameters in blocks, e.g., parameters of each layer is one block, or parameters in several layers could constitute a block). \nThis results into a block-diagonal approximation to the curvature matrix, in order to improve Hessian-free convergence properties: in the latter, a single step might require many CG steps, so the benefit from using second-order information is not apparent.\nThis is mainly an experimental work, where the authors show the merits of their approach on deep autoencoders, convolutional networks and LSTMs: results show favourable performance compared to the original Hessian-free approach and the Adam method.\n\nOriginality: \nThe paper is based on the works of Collobert (2004) and Le Roux et al. (2008), as well as the work of Martens: the twist is that each layer of the neural network is considered a parameter block, so that gradient interactions among weights in a single layer are more useful than those between weights in different layers. This increases the separability of the problem and reduces the complexity. \n\nImportance: \nUnderstanding the difference between first- and second-order methods for NN training is an important topic. Using second-order methods could be considered at its infancy, compared to the wide variety of first-order methods. Having new results on second-order methods with interesting results would definitely attract some attention at the conference. \n\nPresentation/Clarity: \nThe paper is well structured and well written. The authors clearly place their work w.r.t. state of the art and previous works, so that it is clear what is new and what is known.\n\nComments:\n1. It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods. Is there a reasoning for this statement? Or is it just because second-order methods are kind-of the only other alternative we have?\n\n2. Assuming we can perform a second-order method, like Newton's method, on a deep NN. Since originally Newton's method was designed to find solutions that have gradient equal to zero, and since NNs have saddle points (probably many more than local minima), even if we could perfectly perform second-order Newton motions, there is no guarantee whether we converge to a local minimum or a saddle point. However, since we perform Newton's method approximately in practice, this might help escaping saddle points. Any comment on this aspect (I'm not aware whether this is already commented in Schraudolph 2002, where the Gauss-Newton matrix was proposed instead of the Hessian)?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1509739394871,"tcdate":1509085318641,"number":268,"cdate":1509739392207,"id":"SJ19eUg0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}