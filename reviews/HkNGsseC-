{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222627435,"tcdate":1511921090896,"number":2,"cdate":1511921090896,"id":"HkopHcseG","invitation":"ICLR.cc/2018/Conference/-/Paper374/Official_Review","forum":"HkNGsseC-","replyto":"HkNGsseC-","signatures":["ICLR.cc/2018/Conference/Paper374/AnonReviewer1"],"readers":["everyone"],"content":{"title":"On the Expressive Power of Overlapping Architectures of Deep Learning","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields. To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs.  The grid tensors are matricized and the ranks of the resultant matrices are compared. The paper obtains a lower bound on the rank of the resultant grid tensors, and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs. Assuming that the result carries over to ConvNets, I find this result to be very interesting.  While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same. This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Expressive Power of Overlapping Architectures of Deep Learning","abstract":"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).\nOur analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","pdf":"/pdf/aeb06b79a65d8c4d72b8749cf50eaae790af05fa.pdf","TL;DR":"We analyze how the degree of overlaps between the receptive fields of a convolutional network affects its expressive power.","paperhash":"anonymous|on_the_expressive_power_of_overlapping_architectures_of_deep_learning","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Expressive Power of Overlapping Architectures of Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkNGsseC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper374/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Overlapping","Receptive Fields"]}},{"tddate":null,"ddate":null,"tmdate":1512222627474,"tcdate":1511911976618,"number":1,"cdate":1511911976618,"id":"BJZ4zdslf","invitation":"ICLR.cc/2018/Conference/-/Paper374/Official_Review","forum":"HkNGsseC-","replyto":"HkNGsseC-","signatures":["ICLR.cc/2018/Conference/Paper374/AnonReviewer2"],"readers":["everyone"],"content":{"title":"It's shown that a small amount of overlap could result in a large increase in expressibility in CNNs. The caveats are that the analysis is not for regular CNNs, and does not say much about approximation.","rating":"6: Marginally above acceptance threshold","review":"The paper studies the expressive power provided by \"overlap\" in convolution layers of DNNs.  Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling.  Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width).  The key part of the proof is showing a lower bound on the rank for networks with overlap.  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \"generic\" tensor.\n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs, which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth).\n3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank.\n\nThat said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets, as it gives some intuition about how much overlap might 'suffice'.  \n\nI recommend weak accept.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Expressive Power of Overlapping Architectures of Deep Learning","abstract":"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).\nOur analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","pdf":"/pdf/aeb06b79a65d8c4d72b8749cf50eaae790af05fa.pdf","TL;DR":"We analyze how the degree of overlaps between the receptive fields of a convolutional network affects its expressive power.","paperhash":"anonymous|on_the_expressive_power_of_overlapping_architectures_of_deep_learning","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Expressive Power of Overlapping Architectures of Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkNGsseC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper374/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Overlapping","Receptive Fields"]}},{"tddate":null,"ddate":null,"tmdate":1509739337903,"tcdate":1509108491909,"number":374,"cdate":1509739335241,"id":"HkNGsseC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkNGsseC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the Expressive Power of Overlapping Architectures of Deep Learning","abstract":"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).\nOur analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","pdf":"/pdf/aeb06b79a65d8c4d72b8749cf50eaae790af05fa.pdf","TL;DR":"We analyze how the degree of overlaps between the receptive fields of a convolutional network affects its expressive power.","paperhash":"anonymous|on_the_expressive_power_of_overlapping_architectures_of_deep_learning","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Expressive Power of Overlapping Architectures of Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkNGsseC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper374/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Overlapping","Receptive Fields"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}