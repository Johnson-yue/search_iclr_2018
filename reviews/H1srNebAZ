{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222693150,"tcdate":1511916326162,"number":3,"cdate":1511916326162,"id":"rJ07mFogG","invitation":"ICLR.cc/2018/Conference/-/Paper576/Official_Review","forum":"H1srNebAZ","replyto":"H1srNebAZ","signatures":["ICLR.cc/2018/Conference/Paper576/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Potentially amazing results obscured by poor (but fixable!) explanation","rating":"6: Marginally above acceptance threshold","review":"I want to love this paper. The results seem like they may be very important. However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions. I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained.\n\nUnfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1. Without understanding this first result, it’s difficult to decide to what extent the rest of the paper’s results are to be believed.\n\nFig 1 shows “the histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers.” Let’s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons? For now let’s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value. The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs. Let’s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, …]. Now what do we do with this list? As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1. But we can’t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect?\n\nFurther in this direction, Section 4.1 claims “Zero partial derivatives are ignored to make the signal more clear.” Are these zero partial derivatives of the post-relu or pre-relu? The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once). Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? In this case we would be excluding a large set (about half!) of the gradient values, and it didn’t seem from the context in the paper that this would be desirable.\n\nIt would be great if the above could be addressed. Below are some less important comments.\n\nSec 5.1: great results!\n\nFig 3: This figure studies “the first and last layers of each network”. Is the last layer really the last linear layer, the one followed by a softmax? In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). Or is the layer shown (e.g. “stage3layer2”) the penultimate layer? Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from.\n\nSec 5.2 states “neuron partitions the inputs in two distinct but overlapping categories of quasi equal size.” This experiment only shows that this is true in aggregate, not for specific neurons? I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct? Perhaps this statement could be qualified.\n\nTable 1: “52th percentile vs actual 53 percentile shown”. \n\n> Table 1: The more fuzzy, the higher the percentile rank of the threshold\n\nThis is true for the CIFAR net but the opposite is true for ResNet, right?\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discovering the mechanics of hidden neurons","abstract":"Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.","pdf":"/pdf/7695d2312e9a311dd9e066278ef02d57816848dd.pdf","TL;DR":"We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing","paperhash":"anonymous|discovering_the_mechanics_of_hidden_neurons","_bibtex":"@article{\n  anonymous2018discovering,\n  title={Discovering the mechanics of hidden neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1srNebAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper576/Authors"],"keywords":["deep learning","experimental analysis","hidden neurons"]}},{"tddate":null,"ddate":null,"tmdate":1512222693188,"tcdate":1511813576613,"number":2,"cdate":1511813576613,"id":"S1bAZxcxf","invitation":"ICLR.cc/2018/Conference/-/Paper576/Official_Review","forum":"H1srNebAZ","replyto":"H1srNebAZ","signatures":["ICLR.cc/2018/Conference/Paper576/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Experimental study on how units of CNNs behave as binary classifiers","rating":"5: Marginally below acceptance threshold","review":"This paper presents an experimental study on the behavior of the units of neural networks. In particular, authors aim to show that units behave as binary classifiers during training and testing. \n\nI found the paper unnecessarily longer than the suggested 8 pages. The focus of the paper is confusing: while the introduction discusses about works on CNN model interpretability, the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier, without analyzing anything in relation to interpretability.  I think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the paper. Also, quantitative figures would be useful to get the big picture. For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer. It would be also useful to see a comparison of different CNNs and see how the observation holds more or less depending on the performance of the network.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discovering the mechanics of hidden neurons","abstract":"Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.","pdf":"/pdf/7695d2312e9a311dd9e066278ef02d57816848dd.pdf","TL;DR":"We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing","paperhash":"anonymous|discovering_the_mechanics_of_hidden_neurons","_bibtex":"@article{\n  anonymous2018discovering,\n  title={Discovering the mechanics of hidden neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1srNebAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper576/Authors"],"keywords":["deep learning","experimental analysis","hidden neurons"]}},{"tddate":null,"ddate":null,"tmdate":1512222693227,"tcdate":1511732226298,"number":1,"cdate":1511732226298,"id":"Hk5Z4hOlf","invitation":"ICLR.cc/2018/Conference/-/Paper576/Official_Review","forum":"H1srNebAZ","replyto":"H1srNebAZ","signatures":["ICLR.cc/2018/Conference/Paper576/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting finding but lacking an explanation","rating":"4: Ok but not good enough - rejection","review":"The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks. This is an important area and findings in this paper are interesting!\n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments.\n- Could we look at the two distributions of inputs that each neuron tries to separate? \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results).\n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition. 2014\n\n+ Clarity: The paper is easy to read. A few minor presentation issues:\n- ReLu --> ReLU\n\n+ Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014).\n\n+ Significance:\nWhile the results are interesting, the contribution is not significant as the paper misses an important explanation for the phenomenon. I'm not sure what key insights can be taken away from this.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discovering the mechanics of hidden neurons","abstract":"Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.","pdf":"/pdf/7695d2312e9a311dd9e066278ef02d57816848dd.pdf","TL;DR":"We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing","paperhash":"anonymous|discovering_the_mechanics_of_hidden_neurons","_bibtex":"@article{\n  anonymous2018discovering,\n  title={Discovering the mechanics of hidden neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1srNebAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper576/Authors"],"keywords":["deep learning","experimental analysis","hidden neurons"]}},{"tddate":null,"ddate":null,"tmdate":1511586404139,"tcdate":1511586404139,"number":1,"cdate":1511586404139,"id":"rk3wqOUxz","invitation":"ICLR.cc/2018/Conference/-/Paper576/Public_Comment","forum":"H1srNebAZ","replyto":"H1srNebAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"You may discuss the relation to the recent NIPS publication https://arxiv.org/abs/1710.10328","comment":"You may discuss the relation to the recent NIPS publication https://arxiv.org/abs/1710.10328. \n\nApparently your experimental findings can be explained in the lens of generalized hamming distance as introduced in the NIPS paper.  But I am not sure if there are anything more fundamental disclosed by your interesting experimental results. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discovering the mechanics of hidden neurons","abstract":"Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.","pdf":"/pdf/7695d2312e9a311dd9e066278ef02d57816848dd.pdf","TL;DR":"We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing","paperhash":"anonymous|discovering_the_mechanics_of_hidden_neurons","_bibtex":"@article{\n  anonymous2018discovering,\n  title={Discovering the mechanics of hidden neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1srNebAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper576/Authors"],"keywords":["deep learning","experimental analysis","hidden neurons"]}},{"tddate":null,"ddate":null,"tmdate":1509739227337,"tcdate":1509127234719,"number":576,"cdate":1509739224684,"id":"H1srNebAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1srNebAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discovering the mechanics of hidden neurons","abstract":"Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.","pdf":"/pdf/7695d2312e9a311dd9e066278ef02d57816848dd.pdf","TL;DR":"We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing","paperhash":"anonymous|discovering_the_mechanics_of_hidden_neurons","_bibtex":"@article{\n  anonymous2018discovering,\n  title={Discovering the mechanics of hidden neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1srNebAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper576/Authors"],"keywords":["deep learning","experimental analysis","hidden neurons"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}