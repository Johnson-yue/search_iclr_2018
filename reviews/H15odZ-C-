{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222721947,"tcdate":1511974373230,"number":3,"cdate":1511974373230,"id":"rka1Lw2xf","invitation":"ICLR.cc/2018/Conference/-/Paper700/Official_Review","forum":"H15odZ-C-","replyto":"H15odZ-C-","signatures":["ICLR.cc/2018/Conference/Paper700/AnonReviewer1"],"readers":["everyone"],"content":{"title":"In general, the proposed work is very interesting and the idea is neat. It is a useful contribution to the community of GANs and implicit generative models. I am impressed with the structure and presentation of the paper. Easy to follow and well supported. ","rating":"7: Good paper, accept","review":"The authors propose the use of a gamma prior as the distribution over \nthe latent representation space in GANs. The motivation behind it is that \nin GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions. The use of the proposed gamma distribution, as a simple alternative, overcomes this problem. \n\nIn general, the proposed work is very interesting and the idea is neat. \nThe paper is well presented and I want to underline the importance of this. \nThe authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow. \n\nThe work itself is interesting and can provide useful alternatives for the distribution over the latent space. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semantic Interpolation in Implicit Models","abstract":"In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths.  Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.","pdf":"/pdf/6982e7e20cdda42f5ba20b83df32e9e9e03dc526.pdf","paperhash":"anonymous|semantic_interpolation_in_implicit_models","_bibtex":"@article{\n  anonymous2018semantic,\n  title={Semantic Interpolation in Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15odZ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper700/Authors"],"keywords":["Deep Generative Models","GANs"]}},{"tddate":null,"ddate":null,"tmdate":1512222721987,"tcdate":1511809310648,"number":2,"cdate":1511809310648,"id":"BJDXbk5lM","invitation":"ICLR.cc/2018/Conference/-/Paper700/Official_Review","forum":"H15odZ-C-","replyto":"H15odZ-C-","signatures":["ICLR.cc/2018/Conference/Paper700/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Neat idea, needs more work.","rating":"5: Marginally below acceptance threshold","review":"The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets.\n\nThe method involves a nice, albeit minor, trick, where the chi-squared distribution of the sum of the z_{i}^{2} has its dependence on the dimensionality removed. However I am not convinced by the distribution of \\|z^\\prime\\|^{2} in the first place (eqn (2)): the samples from the gaussian will be approximately orthogonal in high dimensions, but the inner product will be at least O(1). Thus although the \\|z_{0}\\|^{2} and \\|z_{1}\\|^{2} are chi-squared/gamma, I don't think \\|z^\\prime\\|^{2} is exactly gamma in general.\n\nThe experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome. Figures 2 and 3 do not add anything to the story, since 2 is just a plot of gamma pdfs and 3 shows the difference between the constant KL and the normal case that is linear in d. \n\nOverall I think the trick needs to be motivated better, and the experiments improved to really show the import of the d-independence of the KL. Thus I think this paper is below the acceptance threshold.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semantic Interpolation in Implicit Models","abstract":"In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths.  Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.","pdf":"/pdf/6982e7e20cdda42f5ba20b83df32e9e9e03dc526.pdf","paperhash":"anonymous|semantic_interpolation_in_implicit_models","_bibtex":"@article{\n  anonymous2018semantic,\n  title={Semantic Interpolation in Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15odZ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper700/Authors"],"keywords":["Deep Generative Models","GANs"]}},{"tddate":null,"ddate":null,"tmdate":1511772156363,"tcdate":1511772156363,"number":1,"cdate":1511772156363,"id":"ByNZgIKgG","invitation":"ICLR.cc/2018/Conference/-/Paper700/Public_Comment","forum":"H15odZ-C-","replyto":"H15odZ-C-","signatures":["~Christian_A_Naesseth1"],"readers":["everyone"],"writers":["~Christian_A_Naesseth1"],"content":{"title":"Interesting!","comment":"This seems very interesting. A quick question, do you think it would be useful to also learn the rate parameter of your Gamma distribution rather than fixing it? This could be achieved by e.g.\nNaesseth, Ruiz, Linderman, Blei, \"Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms\", 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semantic Interpolation in Implicit Models","abstract":"In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths.  Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.","pdf":"/pdf/6982e7e20cdda42f5ba20b83df32e9e9e03dc526.pdf","paperhash":"anonymous|semantic_interpolation_in_implicit_models","_bibtex":"@article{\n  anonymous2018semantic,\n  title={Semantic Interpolation in Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15odZ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper700/Authors"],"keywords":["Deep Generative Models","GANs"]}},{"tddate":null,"ddate":null,"tmdate":1512222722033,"tcdate":1511763972878,"number":1,"cdate":1511763972878,"id":"S16ZxNFgz","invitation":"ICLR.cc/2018/Conference/-/Paper700/Official_Review","forum":"H15odZ-C-","replyto":"H15odZ-C-","signatures":["ICLR.cc/2018/Conference/Paper700/AnonReviewer3"],"readers":["everyone"],"content":{"title":"code distributions for implicit models","rating":"6: Marginally above acceptance threshold","review":"The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs. The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution. The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines. To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution). This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations.\n\nWhile the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. The paper is clearly written and presents the theory and experimental results nicely. I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semantic Interpolation in Implicit Models","abstract":"In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths.  Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.","pdf":"/pdf/6982e7e20cdda42f5ba20b83df32e9e9e03dc526.pdf","paperhash":"anonymous|semantic_interpolation_in_implicit_models","_bibtex":"@article{\n  anonymous2018semantic,\n  title={Semantic Interpolation in Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15odZ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper700/Authors"],"keywords":["Deep Generative Models","GANs"]}},{"tddate":null,"ddate":null,"tmdate":1509739152778,"tcdate":1509132450325,"number":700,"cdate":1509739150110,"id":"H15odZ-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H15odZ-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Semantic Interpolation in Implicit Models","abstract":"In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths.  Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.","pdf":"/pdf/6982e7e20cdda42f5ba20b83df32e9e9e03dc526.pdf","paperhash":"anonymous|semantic_interpolation_in_implicit_models","_bibtex":"@article{\n  anonymous2018semantic,\n  title={Semantic Interpolation in Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15odZ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper700/Authors"],"keywords":["Deep Generative Models","GANs"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}