{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222718754,"tcdate":1512015166145,"number":3,"cdate":1512015166145,"id":"HkLBrbaef","invitation":"ICLR.cc/2018/Conference/-/Paper679/Official_Review","forum":"rkgOLb-0W","replyto":"rkgOLb-0W","signatures":["ICLR.cc/2018/Conference/Paper679/AnonReviewer2"],"readers":["everyone"],"content":{"title":"solid experiments and interesting model","rating":"7: Good paper, accept","review":"The paper proposes Parsing-Reading-Predict Networks (PRPN), a new model jointly learns syntax and lexicon. The main idea of this model is to add skip-connections to integrate syntax relationships into the context of predicting the next word (i.e. language modeling task).\n\nTo model this, the authors introduce hidden variable l_t, which break down to the decisions of a soft version of gate variable values in the previous possible positions. These variables are then parameterized using syntactic distance to ensure that the final structure inferred by the model has no overlapping ranges so that it will be a valid syntax tree.\n\nI think the paper is in general clearly written. The model is interesting and the experiment section is quite solid. The model reaches state-of-the-art level performance in language modeling and the performance on unsupervised parsing task (which is a by-product of the model) is also quite promising.\n\nMy main question is that the motivation/intuition of introducing the syntactic distance variable. I understand that they basically make sure the tree is valid, but the paper did not explain too much about what's the intuition behind this or is there a good way to interpret this. What motivates these d variables?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]}},{"tddate":null,"ddate":null,"tmdate":1512222718796,"tcdate":1511822381731,"number":2,"cdate":1511822381731,"id":"ByUN4M9xM","invitation":"ICLR.cc/2018/Conference/-/Paper679/Official_Review","forum":"rkgOLb-0W","replyto":"rkgOLb-0W","signatures":["ICLR.cc/2018/Conference/Paper679/AnonReviewer1"],"readers":["everyone"],"content":{"title":"REVIEW","rating":"6: Marginally above acceptance threshold","review":"The main contribution of this paper is to introduce a new recurrent neural network for language modeling, which incorporates a tree structure More precisely, the model learns constituency trees (without any supervision), to capture syntactic information. This information is then used to define skip connections in the language model, to capture longer dependencies between words. The update of the hidden state does not depend only on the previous hidden state, but also on the hidden states corresponding to the following words: all the previous words belonging to the smallest subtree containing the current word, such that the current word is not the left-most one. The authors propose to parametrize trees using \"syntactic distances\" between adjacent words (a scalar value for each pair of adjacent words w_t, w_{t+1}). Given these distances, it is possible to obtain the constituents and the corresponding gating activations for the skip connections. These different operations can be relaxed to differentiable operations, so that stochastic gradient descent can be used to learn the parameters. The model is evaluated on three language modeling benchmarks: character level PTB, word level PTB and word level text8. The induced constituency trees are also evaluated, for sentence of length 10 or less (which is the standard setting for unsupervised parsing).\n\nOverall, I really like the main idea of the paper. The use of \"syntactic distances\" to parametrize the trees is clever, as they can easily be computed using only partial information up to time t. From these distances, it is also relatively straightforward to obtain which constituents (or subtrees) a word belongs to (and thus, the corresponding gating activations). Moreover, the operations can easily be relaxed to obtain a differentiable model, which can easily be trained using stochastic gradient descent.\n\nThe results reported on the language modeling experiments are strong. One minor comment here is that it would be nice to have an ablation analysis, as it is possible to obtain similarly strong results with simpler models (such as plain LSTM).\n\nMy main concern regarding the paper is that it is a bit hard to understand. In particular in section 4, the authors alternates between discrete and relaxed values: end of section 4.1, it is implied that alpha are in [0, 1], but in equation 6, alpha are in {0, 1}, then relaxed in equation 9 to [0, 1] again. I am also wondering whether it would make more sense to start by introducing the syntactic distances, then the alphas and finally the gates? I also found the section 5 to be quite confusing. While I get the\tgeneral idea, I am not sure what is the relation between hidden states h and m (section 5.1). Is there a mixup between h defined in equation 10 and h from section 5.1? I am aware that it is not straightforward to describe the proposed method, but believe it would be a much stronger paper if written more clearly.\n\nTo conclude, I really like the method proposed in this paper, and believe that the experimental results are quite strong.\nMy main concern\tregarding the paper is its clarity: I will gladly increase my score if the authors can improve the writing.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]}},{"tddate":null,"ddate":null,"tmdate":1512222718836,"tcdate":1511790166861,"number":1,"cdate":1511790166861,"id":"rkJwIctlf","invitation":"ICLR.cc/2018/Conference/-/Paper679/Official_Review","forum":"rkgOLb-0W","replyto":"rkgOLb-0W","signatures":["ICLR.cc/2018/Conference/Paper679/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review ","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary: the paper proposes a novel method to leverage tree structures in an unsupervised learning manner. The key idea is to make use of “syntactic distance” to identify phrases, thus building up a tree for input sentence. The proposed model achieves SOTA on a char-level language modeling task and is demonstrated to yield reasonable tree structures.\n\nComment: I like the paper a lot. The idea is very creative and interesting. The paper is well written.\n\nBesides the official comment that the authors already replied, I have some more:\n- I was still wondering how to compute the left hand side of eq 3 by marginalizing over all possible unfinished structures so far. (Of course, what the authors do is showed to be a fast and good approximation.)\n- Using CNN to compute d has a disadvantage that the range of look-back must be predefined. Looking at fig 3, in order to make sure that d6 is smaller than d2, the look-back should have a wide coverage so that the computation for d6 has some knowledge about d2 (in some cases the local information can help to avoid it, but not always). I therefore think that using an RNN is more suitable than using a CNN.\n- Is it possible to extend this framework to dependency structure?\n- It would be great if the authors show whether the model can leverage given tree structures (like SPINN) (for instance we can do a multitask learning where a task is parsing given a treebank to train)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]}},{"ddate":null,"tddate":1511284333116,"tmdate":1511294196210,"tcdate":1511284247809,"number":2,"cdate":1511284247809,"id":"rkemARblM","invitation":"ICLR.cc/2018/Conference/-/Paper679/Official_Comment","forum":"rkgOLb-0W","replyto":"SJdy1V01z","signatures":["ICLR.cc/2018/Conference/Paper679/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper679/Authors"],"content":{"title":"Responses to questions / comment","comment":"Thank you for enlightening comments.\n\nRegarding \"marginalizing over g\":\nAs discussed in section 4.1 and Appendix B, we replace discrete g by its expectation. Thus, we can have a computationally less expensive approximation for p(x_t+1|x0...x_t).\n\nRegarding \"linguistic theory for 'syntactic distance'\"\nThe idea of using a \"syntactic distance\" is inspired by the binary parse tree, which is related to linguistic theory. We introduced the \"syntactic distance\" while trying to render the binary parse tree into a learnable, soft tree structure in the framework of language modeling. So it can be deemed as a set of boundaries which defines the binary parse tree.\n\nRegarding \"try other activation functions\"\nThank you for this enlightening comment. We recently tried to replace sigmoid by ReLU, which makes the model achieve more stable performance regardless of different temperature parameter \\tau.\n\nRegarding \"try any word embedding\"\nIn this experiment, we want to prove the model's ability to learn from scratch, but pretrained word embedding can contain syntactic information. We will use word embedding in future work that focuses on obtaining better syntactic distance.\n\nThis article will be further revised and polished according to your suggestions.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]}},{"tddate":null,"ddate":null,"tmdate":1511043823485,"tcdate":1511042783963,"number":1,"cdate":1511042783963,"id":"SJdy1V01z","invitation":"ICLR.cc/2018/Conference/-/Paper679/Official_Comment","forum":"rkgOLb-0W","replyto":"rkgOLb-0W","signatures":["ICLR.cc/2018/Conference/Paper679/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper679/AnonReviewer3"],"content":{"title":"questions / comment ","comment":"The paper proposes a very cute idea. My questions/comment: \n\n1. can you compute p(x_t+1|x0...x_t) (eq 3) by marginalising over g?\n\n2. is the idea of using \"syntactic distance\" related to any linguistic theory?  \n\n3. I think eq 5 has a typo: is it g_i or g_t'? \n\n4. the last line on page 4: d_{K-1} (capital K). Shouldn't d index start from 1? (you say that there are K-1 variables)\n\n5. Eq 11: I think d doesn't need to be in [0, 1]. Did you try other activation functions (e.g Tanh)?\n\n6. The line right after Eq 12: shouldn't t+1 be superscript? (It's better to be coherent with the notations above)\n\n7. In 6.3, did you try any word embedding? As suggested by [1], word embeddings can be very helpful. \n\n\n1. Le & Zuidema. Unsupervised Dependency Parsing: Let’s Use Supervised Parsers\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]}},{"tddate":null,"ddate":null,"tmdate":1509739163830,"tcdate":1509131880292,"number":679,"cdate":1509739161168,"id":"rkgOLb-0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkgOLb-0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Language Modeling by Jointly Learning Syntax and Lexicon","abstract":"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.","pdf":"/pdf/2eccfe9c684bcdf1eb65db32fc87d25332452089.pdf","TL;DR":"In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.","paperhash":"anonymous|neural_language_modeling_by_jointly_learning_syntax_and_lexicon","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkgOLb-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper679/Authors"],"keywords":["Language model","unsupervised parsing"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}