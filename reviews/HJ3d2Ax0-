{"notes":[{"tddate":null,"ddate":null,"tmdate":1512233570799,"tcdate":1512233570799,"number":1,"cdate":1512233570799,"id":"BysD9IxbG","invitation":"ICLR.cc/2018/Conference/-/Paper465/Public_Comment","forum":"HJ3d2Ax0-","replyto":"B1L-7zDgz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response to Reviewer 2","comment":"We thank the reviewer for the time and feedback. Following is our response. \n\nAddressing the reviewer's main comment, indeed Theorem 1 does not include a separation between two deep recurrent networks of different depths - our analysis refers to the enhanced memory capacity of multiple layered recurrent networks versus single layered recurrent networks. Our treatment is analogous to a much wider and more mature line of work that deals with depth separation in feed-forward neural networks. In the feed-forward domain, various important and widely accepted papers give results which only separate between shallow and deep networks in order to show the advantages of depth in these networks (such as [1,2]). Formal theoretical literature which provides any such similar results on recurrent networks is scarce at best, and such questions have not been answered (or formally asked) to date for recurrent networks. Our focus on separating between shallow and deep networks is the natural starting point for this line of research; Theorem 1, which establishes a separation in the ability to integrate data throughout time between shallow and deep recurrent networks, constitutes a first of its kind theoretical assertion of superiority of deep recurrent networks.  \n\nWe wish to emphasize that even once the question is raised - \"can the notion of depth enhanced long term-memory in recurrent networks be formalized?\" and a mathematical infrastructure is set-up in the form of the Start-End separation rank with grid tensors rank bounding it, establishing Theorem 1 is highly non-trivial. As can be seen in the supplementary material, the proof involves a considerable \"legwork\" which integrates tools and results from various fields (measure theory, tensorial analysis, combinatorics, graph theory and quantum physics). Accordingly, given the importance and contribution of the result in Theorem 1, we found the suggested task of separating the memory capacity of two arbitrary deep recurrent networks subsidiary in terms of contribution to the message of this paper. We agree that a finer investigation separating two recurrent networks of arbitrary depth is of relevance - it is in fact a part of a follow-up work, indicated by this paper, which we are presently pursuing (described in the last paragraph of the discussion section).\n\nRegarding the sub-comment made by the reviewer, our theoretical results guarantee that for almost any setting of the recurrent network's weights, theorem 1 holds. We have performed several sanity checks, which agreed with our conclusions. Having proved the theorem, we did not feel the need to include such empirical validations. However, we will gladly do so if it helps clarify or convey any message. We would appreciate a clarification regarding what specific convincing experiments the reviewer had in mind.\n\nReferences\n----------------\n[1] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems, pages 666–674, 2011.\n[2] Guido F. Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Benefits of Depth for Long-Term Memory of Recurrent Networks","abstract":"The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized by the recurrent network from a function that models no interaction whatsoever between the beginning and end of the input sequence. We prove that deep recurrent networks support Start-End separation ranks which are exponentially higher than those supported by their shallow counterparts. Moreover, we show that the ability of deep recurrent networks to correlate different parts of the input sequence increases exponentially as the input sequence extends, while that of vanilla shallow recurrent networks does not adapt to the sequence length at all. Thus, we establish that depth brings forth an overwhelming advantage in the ability of recurrent networks to model long-term dependencies, and provide an exemplar of quantifying this key attribute which may be readily extended to other RNN architectures of interest, e.g. variants of LSTM networks. We obtain our results by considering a class of recurrent networks referred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden state with the input via the Multiplicative Integration operation.","pdf":"/pdf/967bc97f0d7c0b7f6d7adc80dc2ecd8c9683bda9.pdf","TL;DR":"We propose a measure of long-term memory and prove that deep recurrent networks are much better fit to model long-term temporal dependencies than shallow ones.","paperhash":"anonymous|benefits_of_depth_for_longterm_memory_of_recurrent_networks","_bibtex":"@article{\n  anonymous2018benefits,\n  title={Benefits of Depth for Long-Term Memory of Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ3d2Ax0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper465/Authors"],"keywords":["recurrent neural networks","deep networks","correlations","long term memory","tensor networks","tensor analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222659047,"tcdate":1511625469626,"number":1,"cdate":1511625469626,"id":"B1L-7zDgz","invitation":"ICLR.cc/2018/Conference/-/Paper465/Official_Review","forum":"HJ3d2Ax0-","replyto":"HJ3d2Ax0-","signatures":["ICLR.cc/2018/Conference/Paper465/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An effect of increase of $L$ should be evaluated.","rating":"5: Marginally below acceptance threshold","review":"This paper investigates an effect of time dependencies in a specific type of RNN.\n\nThe idea is important and this paper seems sound. However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently.\n\n--Main comment\nAbout the deep network case in Theorem 1, how $L$ affects the bound on ranks? In the current statement, the result seems independent to $L$ when $L \\geq 2$. I think that this paper should quantify the effect of an increase of $L$.\n\n--Sub comment\nNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result. Only a simple example will make this paper more convincing.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Benefits of Depth for Long-Term Memory of Recurrent Networks","abstract":"The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized by the recurrent network from a function that models no interaction whatsoever between the beginning and end of the input sequence. We prove that deep recurrent networks support Start-End separation ranks which are exponentially higher than those supported by their shallow counterparts. Moreover, we show that the ability of deep recurrent networks to correlate different parts of the input sequence increases exponentially as the input sequence extends, while that of vanilla shallow recurrent networks does not adapt to the sequence length at all. Thus, we establish that depth brings forth an overwhelming advantage in the ability of recurrent networks to model long-term dependencies, and provide an exemplar of quantifying this key attribute which may be readily extended to other RNN architectures of interest, e.g. variants of LSTM networks. We obtain our results by considering a class of recurrent networks referred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden state with the input via the Multiplicative Integration operation.","pdf":"/pdf/967bc97f0d7c0b7f6d7adc80dc2ecd8c9683bda9.pdf","TL;DR":"We propose a measure of long-term memory and prove that deep recurrent networks are much better fit to model long-term temporal dependencies than shallow ones.","paperhash":"anonymous|benefits_of_depth_for_longterm_memory_of_recurrent_networks","_bibtex":"@article{\n  anonymous2018benefits,\n  title={Benefits of Depth for Long-Term Memory of Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ3d2Ax0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper465/Authors"],"keywords":["recurrent neural networks","deep networks","correlations","long term memory","tensor networks","tensor analysis"]}},{"tddate":null,"ddate":null,"tmdate":1509739288713,"tcdate":1509121140381,"number":465,"cdate":1509739286057,"id":"HJ3d2Ax0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJ3d2Ax0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Benefits of Depth for Long-Term Memory of Recurrent Networks","abstract":"The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized by the recurrent network from a function that models no interaction whatsoever between the beginning and end of the input sequence. We prove that deep recurrent networks support Start-End separation ranks which are exponentially higher than those supported by their shallow counterparts. Moreover, we show that the ability of deep recurrent networks to correlate different parts of the input sequence increases exponentially as the input sequence extends, while that of vanilla shallow recurrent networks does not adapt to the sequence length at all. Thus, we establish that depth brings forth an overwhelming advantage in the ability of recurrent networks to model long-term dependencies, and provide an exemplar of quantifying this key attribute which may be readily extended to other RNN architectures of interest, e.g. variants of LSTM networks. We obtain our results by considering a class of recurrent networks referred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden state with the input via the Multiplicative Integration operation.","pdf":"/pdf/967bc97f0d7c0b7f6d7adc80dc2ecd8c9683bda9.pdf","TL;DR":"We propose a measure of long-term memory and prove that deep recurrent networks are much better fit to model long-term temporal dependencies than shallow ones.","paperhash":"anonymous|benefits_of_depth_for_longterm_memory_of_recurrent_networks","_bibtex":"@article{\n  anonymous2018benefits,\n  title={Benefits of Depth for Long-Term Memory of Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ3d2Ax0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper465/Authors"],"keywords":["recurrent neural networks","deep networks","correlations","long term memory","tensor networks","tensor analysis"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}