{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222612907,"tcdate":1511866112597,"number":3,"cdate":1511866112597,"id":"HkFWypcgf","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Improved multitask deep reinforcement learning with active learning","rating":"7: Good paper, accept","review":"In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games. A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works.\n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper. Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper. The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs.\n\nAlthough performance is still reduced compared to single task learning in some cases, this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale.\n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222612960,"tcdate":1511806772929,"number":2,"cdate":1511806772929,"id":"BkTVPAYeG","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An active learning approach to multitask RL with significant potential","rating":"7: Good paper, accept","review":"\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL. The domain is video games.   All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).\n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL\n\n- a number of approaches developed for the basic idea\n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)\n\n- paper is overall well written/clear\n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling)\nCouldn't comparisons be made, in some way, to other multitask work?\n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!).\n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling).\n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section).\n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space). Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper.\n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features. Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting.\n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game\n    (that's similar to games already trained on), compared to learning from\n    scratch?\n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ? Could go either way since the network\n   has to allocate resources to learn other games too.  But is there a pattern?\n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5.\n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage? Why not explore both\n  possibilities? what are the pros/cons? (section 3)\n\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222614256,"tcdate":1511785883297,"number":1,"cdate":1511785883297,"id":"r1XoHKtlf","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer3"],"readers":["everyone"],"content":{"title":"New online algorithms for learning multiple sequential problems","rating":"4: Ok but not good enough - rejection","review":"The paper present online algorithms for learning multiple sequential problems. The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm. Experimental results are given on different multi-task instances. The contributions are interesting and experimental results seem promising. But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages). Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known. This is a very strong assumption. In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption. Therefore, I am not convinced that the paper is ready for publication at ICLR'18.\n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks. The gap is so large that I am not convinced on the fairness of the comparison. For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge.\n* I do not see how the single output layer is defined.\n* As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm.\n* Section 7.1. It is not clear why degradation does not happen. It seems to be only an experimental fact.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511666296672,"tcdate":1511666296672,"number":3,"cdate":1511666296672,"id":"rybYG3Pgz","invitation":"ICLR.cc/2018/Conference/-/Paper284/Public_Comment","forum":"B1nZ1weCZ","replyto":"H1kXk_weG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reply to author(s)","comment":"Thank you for the confirmation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511649173635,"tcdate":1511649046742,"number":1,"cdate":1511649046742,"id":"H1kXk_weG","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"H1KJtmmez","signatures":["ICLR.cc/2018/Conference/Paper284/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/Authors"],"content":{"title":"Regarding the discrepancy in plots","comment":"We thank the reader for pointing out the error.\n\nThere indeed is a discrepancy in the STA3C scores on Page 7 and 8. We checked our results again and we found that the STA3C scores in Figure 3 on Page 7 are correct (they can be verified in Appendix J of the paper). We'll make the corrections in Figure 4 and update the paper once we're allowed to.\n\nAs explained in the paper, the STA3C scores are of no importance during the learning of the DUA4C agent. Thus, we checked our results for the final values of the performance metrics (since they do depend on the STA3C agent scores) in case they needed to be changed as well. We found that the performance metrics (p_am, q_am, etc.) were all calculated using the correct baseline scores and thus they are ALL CORRECT. That is the results reports in Table 2 are correct.\n\nWhile we agree that DUA4C does better on Demon Attack only (and is nearly equal to STA3C on Seaquest), the purpose of a Multi-tasking network is to do reasonably well on ALL the tasks, which might come at the cost of not doing better than the baseline on some tasks. We had included this discussion in the paper in the performance metrics section where we motivated the new performance metric q_am.\n\nFinally, no we didn't use a different set of parameters for the A3C scores. We're not sure of how the error crept only into the plot. We apologize again for the mistake in Figure 4."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511368928760,"tcdate":1511368928760,"number":1,"cdate":1511368928760,"id":"H1KJtmmez","invitation":"ICLR.cc/2018/Conference/-/Paper284/Public_Comment","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Difference in baseline scores","comment":"There seems to be a difference in the baseline scores reported between page 7 and page 8 for A3C/STA3C scores. If I use the baseline scores from page 7 and compare them against DUA4C, I think DUA4C does better in Demon Attack only?\n\nHave you used a different set of parameters to compute the A3C scores for the DUA4C experiments?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739386198,"tcdate":1509089027548,"number":284,"cdate":1509739383536,"id":"B1nZ1weCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/1e8ead2c094dd5f51c81b7dbac7f5193dde482cb.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}