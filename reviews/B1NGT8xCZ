{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222611114,"tcdate":1511947885044,"number":3,"cdate":1511947885044,"id":"SySd0xngf","invitation":"ICLR.cc/2018/Conference/-/Paper278/Official_Review","forum":"B1NGT8xCZ","replyto":"B1NGT8xCZ","signatures":["ICLR.cc/2018/Conference/Paper278/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A probabilistic framework for domain adaptation, some more recent baselines missing","rating":"5: Marginally below acceptance threshold","review":"The authors propose a probabilistic framework for semi-supervised learning and domain adaptation. By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation. A domain confusion loss is added to learn domain-invariant feature representations. The authors compared Dauto with several baseline methods on several datasets and showed improvement. \n\nThe paper is well-organized and easy to follow. The probabilistic framework itself is quite straight-forward. The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme. \n\nThe proposed DAuto is essentially DANN+autoencoder.  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator. It would be interesting to see if the additional auto-encoder part help address the issue. \n\nThe experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017). \n\nIt could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block. DAuto does seem to offer more boost in domain pairs that are less similar. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Principled Hybrids of Generative and Discriminative Domain Adaptation","abstract":"We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. This provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from \\emph{both} source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model,  \\emph{DAuto}. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.\n","pdf":"/pdf/5ebbee0481156ed573d869f8e4eb5b753a86793f.pdf","paperhash":"anonymous|principled_hybrids_of_generative_and_discriminative_domain_adaptation","_bibtex":"@article{\n  anonymous2018principled,\n  title={Principled Hybrids of Generative and Discriminative Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NGT8xCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper278/Authors"],"keywords":["domain adaptation","neural networks","generative models","discriminative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222611153,"tcdate":1511819111682,"number":2,"cdate":1511819111682,"id":"SyeuDWqef","invitation":"ICLR.cc/2018/Conference/-/Paper278/Official_Review","forum":"B1NGT8xCZ","replyto":"B1NGT8xCZ","signatures":["ICLR.cc/2018/Conference/Paper278/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An enjoyable paper, with some room for improvements","rating":"7: Good paper, accept","review":"This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al.\nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem.\n\nThe only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model.  \n\nThe authors perform numerous empirical experiments on several types of problems. They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks. On the downside, all these experiments concern predictive (discriminative) problems. Given the paper title, I would have expected some experiments in a generative context. Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus.\nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016).\n\nMinor comments:\n- Paragraph below Equation 6:  The meaning of $\\phi(\\psi)$ is unclear \n- Equation (7): phi and psi seems inverted \n- Section 4: The acronym MLP is used but never defined.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Principled Hybrids of Generative and Discriminative Domain Adaptation","abstract":"We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. This provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from \\emph{both} source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model,  \\emph{DAuto}. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.\n","pdf":"/pdf/5ebbee0481156ed573d869f8e4eb5b753a86793f.pdf","paperhash":"anonymous|principled_hybrids_of_generative_and_discriminative_domain_adaptation","_bibtex":"@article{\n  anonymous2018principled,\n  title={Principled Hybrids of Generative and Discriminative Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NGT8xCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper278/Authors"],"keywords":["domain adaptation","neural networks","generative models","discriminative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222611200,"tcdate":1511795012642,"number":1,"cdate":1511795012642,"id":"HkTrtoKlf","invitation":"ICLR.cc/2018/Conference/-/Paper278/Official_Review","forum":"B1NGT8xCZ","replyto":"B1NGT8xCZ","signatures":["ICLR.cc/2018/Conference/Paper278/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Different angle to explain the DA methods, but the novelty is insufficient ","rating":"5: Marginally below acceptance threshold","review":"This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances.  \nHowever, I have the following concerns on novelty. \n\n1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation.  At this point, the novelty is weaken.\n2. In the introduction, the authors mentioned “limitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings.” However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features. It is not clear why mSDA cannot handle time-series data but DAuto can.  DAuto does not consider the sequence/ordering of data either. \n3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss). As far as I know, this kind of loss is commonly used in most existing methods. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Principled Hybrids of Generative and Discriminative Domain Adaptation","abstract":"We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. This provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from \\emph{both} source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model,  \\emph{DAuto}. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.\n","pdf":"/pdf/5ebbee0481156ed573d869f8e4eb5b753a86793f.pdf","paperhash":"anonymous|principled_hybrids_of_generative_and_discriminative_domain_adaptation","_bibtex":"@article{\n  anonymous2018principled,\n  title={Principled Hybrids of Generative and Discriminative Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NGT8xCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper278/Authors"],"keywords":["domain adaptation","neural networks","generative models","discriminative models"]}},{"tddate":null,"ddate":null,"tmdate":1509739389418,"tcdate":1509088524424,"number":278,"cdate":1509739386760,"id":"B1NGT8xCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1NGT8xCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Principled Hybrids of Generative and Discriminative Domain Adaptation","abstract":"We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. This provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from \\emph{both} source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model,  \\emph{DAuto}. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.\n","pdf":"/pdf/5ebbee0481156ed573d869f8e4eb5b753a86793f.pdf","paperhash":"anonymous|principled_hybrids_of_generative_and_discriminative_domain_adaptation","_bibtex":"@article{\n  anonymous2018principled,\n  title={Principled Hybrids of Generative and Discriminative Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NGT8xCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper278/Authors"],"keywords":["domain adaptation","neural networks","generative models","discriminative models"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}