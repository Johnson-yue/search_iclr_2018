{"notes":[{"tddate":null,"ddate":null,"tmdate":1512165603283,"tcdate":1512165513658,"number":5,"cdate":1512165513658,"id":"HJMcxIJWf","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Comment","forum":"SyqShMZRb","replyto":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference/Paper958/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper958/Authors"],"content":{"title":"Our reconstruction performance without kekulization on Zinc dataset","comment":"To further clarify the reconstruction accuracy, we here report performance (our model and baselines) without using the kekulization transformation on Zinc dataset, in supplement to numbers using kekulization already reported in our manuscript. We include baseline results from GVAE paper for direct comparison. \n\nSD-VAE (ours): 76.2%; GVAE: 53.7%; CVAE: 44.6%\n\nCompare to what reported for SD-VAE with kekulization in current revision (72.8%), our performance is slightly boosted without kekulization. This shows that kekulization itself doesn’t have positive impact for reconstruction in our method. Our conclusion that the reconstruction accuracy of our SD-VAE is much better than all baselines still holds. \n\nNevertheless, to avoid possible misunderstanding, we’ll refine the experiment section by including more experiments, once the open review system allows. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512030388086,"tcdate":1512030186804,"number":4,"cdate":1512030186804,"id":"BkQglHagz","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Comment","forum":"SyqShMZRb","replyto":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference/Paper958/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper958/Authors"],"content":{"title":"We released baseline CVAE code, data and evaluation code for clarification","comment":"To address the anonymous commenter’s concerns on the CVAE baseline, the initial release of CVAE’s code (training code based on GVAE’s authors’code), with two versions of kekule data and vanilla data and the reconstruction evaluation script,  are available at \n\nhttps://github.com/anonymous-author-80ee48b2f87/cvae-baseline \n\nwhere we also uploaded our trained CVAE, together with pretrained model obtained from GVAE’s authors. \n\nHere we briefly summarize the current results:\n(1) - CVAE, vanilla setting, pretrained model : 44.854%\n(2) - CVAE, vanilla setting, our retraining: 43.218%\n(3) - CVAE, Marvin Suite kekulised **tried for all methods in our paper**: 11.6%\n(4) - CVAE, rdkit kekulised (provided by anonymous commenter, never been tried in our paper): 38.17% \n\nWe reported the best form of SMILES for CVAE in our paper. If you believe there’s any issue, please let us know asap and we are happy to investigate.\n\nFinally, we thank all the anonymous comments about the paper. If you have any concerns about the paper, please make the comments public while you specifying readers. Making such comments to reviewers only will not allow us to address the possible misunderstandings, or improve the paper timely when we make possible mistakes. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512054249880,"tcdate":1512030073534,"number":3,"cdate":1512030073534,"id":"SkWtkrTlG","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Comment","forum":"SyqShMZRb","replyto":"BJqZpU2xf","signatures":["ICLR.cc/2018/Conference/Paper958/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper958/Authors"],"content":{"title":"Discrepancy in data and evaluation code on where your concern is -- part 2","comment":"(Due to the space limit, see comment below for part 1)\n\n== Code:\n\nAs mentioned before, for training CVAE baseline we use the code from GVAE (https://github.com/mkusner/grammarVAE) instead of making an implementation in other framework. **However, the reconstruction evaluation code is not available in this repo. Thus we made our evaluation code following GVAE paper’s protocol , and made it public.**\n\nWhat we achieved with CVAE and our evaluation code for exact sequence reconstruction is:\n```\n(1) - CVAE, vanilla data, pretrained model : 44.854% (As we showed in the paper)\n(2) - CVAE, vanilla data, our retraining: 43.218%\n(3) - CVAE, our kekulised data: 11.6%\n(4) - CVAE, your kekulised data: 38.17% \n```\nWith the following scripts (If you want to evaluate on your own, please put them on the GVAE’s repo and run the first one after modifying paths to model weight / data files):\nhttps://gist.github.com/anonymous-author-80ee48b2f87/9f025de58123450c15e8d95d29797826\nhttps://gist.github.com/anonymous-author-80ee48b2f87/aa0ff838eabc372d537c57199ebc31f4\n\nNote that (1) is consistent with what GVAE paper reports, and (2) is consistent with (1), which gives us confidence on the correctness of both our evaluation script and our running of code from GVAE’s repo.\n\nSee NOTE 1 below for everything needed to reproduce our evaluation.\n\nFOCUS:\n\nThe observation is that CVAE performs worse on kekulised data (  (1, 2)  vs. (3, 4) in evaluation above), as we have mentioned before, **in contrast to** receiving huge performance gain to become a strong baseline ( (1, 2) vs. 75% as you have claimed).  Again, in our paper, we ran the baselines on both settings and report the best results!\n\nSo new we observe discrepancy in the data and the metric evaluation between our findings and your argument. Since data and metric are keystones for your argument, we would like to verify it by asking you how do you **in detail** get your number (75%). Could you please point to the code for evaluation, CVAE implementation for training and data you used for a fair comparison? \n\nNOTE 1: the initial release of baseline’s code (with two versions of kekule data and vanilla data and the reconstruction accuracy evaluation script; training code based on GVAE’s authors’code)  are available at https://github.com/anonymous-author-80ee48b2f87/cvae-baseline , where we also uploaded our trained CVAE, together with pretrained model obtained from GVAE’s authors. \n\nNOTE 2: Before getting numbers for the experiment above, we fixed a problem in our script that collects statistics for the previous reply, so now the reported sequence level accuracy for CVAE on kekulised data is ~11.6%, and character level accuracy is >97%. Since this is a bug in collecting numbers rather than the model, our conclusion, which says kekulisation makes it harder for CVAE instead of making it perform better as you argued, remains **the same**.\n\n== Final words:\n\nFinally, thank you for the follow up. But more details about what you’ve tried are required, which is necessary for the scientific discussion among us. We look forward to calibrating mismatching of numbers in the metric which are fundamentals of further discussion.\n\nNevertheless, we will also report the results of our method on the vanilla settings in the revision of our manuscript, once allowed by openreview.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512051391508,"tcdate":1512030040866,"number":2,"cdate":1512030040866,"id":"HJ-PkBpez","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Comment","forum":"SyqShMZRb","replyto":"BJqZpU2xf","signatures":["ICLR.cc/2018/Conference/Paper958/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper958/Authors"],"content":{"title":"Discrepancy in data and evaluation code on where your concern is -- part 1","comment":"We need to reiterate that for CVAE and GVAE, **we also tried the kekule form of the data** along with **many other possible settings**. In Table 1, GVAE got similar results as before, but CVAE got worse results. So we report whatever best result for the baselines, using the code from GVAE implementation. This is supported by the improvement of GVAE in Table 2. \n\nWe never try to hide any details. Instead we elaborate all details and only due to the space limit, we put some of the stuffs in appendix (but still referred in main paper). That’s why you know details about datasets, settings, etc. So we suggest to figure out the discrepancy in scientific facts first, before making moral judgement. \n\nPlease note that we mostly take issue with your argument is on the sequence reconstruct accuracy you get for baseline CVAE (75%) with your data, on top of which as a premise you argue that the performance we report may not surpass the baseline. However, its validity looks questionable to us, and we investigated into the details about your evidence by running the experiments on every possible settings including the one you use, and our finding **does not** support the soundness of your premise. We provided the codes and models that support our claim, and would like to calibrate this discrepancy first to make further discussion meaningful.\n\n== Kekulisation (data):\n\nWe first introduce a basic common knowledge in Biochemistry that ``kekulisation form’’ is *NOT* unique. For the same underlying molecule, there may be many variants of kekulisation form due to different rules in rewriting SMILES, and it is pretty normal that two toolkits shows different kekulisations. \n\nWe appreciate your providing the scripts. The script looks good, but it generates different data than ours. Your script uses kekulisation from **rdkit**, which leads to data form different from our kekulisation using **Marvin Suite**. This leads to the following discrepancy:\n```\nNon kekulised:\n    max 120 min 9 mean 44.31076 std 9.32734\nour kekulised:\n    max 114 min 10 mean 48.73391 std 10.03919\nyour kekulised (as your script does):\n    max 85 min 9 mean 44.85288 std 9.61040\n```\nThis indicates even just from these numbers, non-kekulised and kekulised (both ours and yours) form of the ZINC are quite similar, in contrast to your argument that the later is much simpler. As we mentioned, alternating single and double bonds makes the kekule form much harder for some baselines. \n\nWe would like to emphasize two points: First, both non-kekulised & kekulised are the representation corresponding to the same underlying molecules. Second, not only limited to our method but also for a large family of sequence models in applications as translation, summarization, etc., the distribution of length of sequence matters more than maximum length in analysis of data space, so we do not agree with your estimation of difficulty with a lsose bound inferred solely from maximum length.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1511915345603,"tcdate":1511915345603,"number":1,"cdate":1511915345603,"id":"Sk58kFogf","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Comment","forum":"SyqShMZRb","replyto":"B1Cm0ihyf","signatures":["ICLR.cc/2018/Conference/Paper958/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper958/Authors"],"content":{"title":"Fighting for our honestness","comment":"We will not accept the accusation of “deliberately misleading and dishonest”. Please read the paper carefully before writing down the public comments of groundless moral judgement, which will lead to misunderstanding to public. \n\nFirst of all, we need to mention that, for the two baselines (CVAE and GVAE), **we also tried the kekule form of the data** along with **many other possible settings**, as we mentioned in the paper in page 15, right above Appendix C. We report whatever best result these baselines can achieve in our paper (page 8) from these settings. That’s why for GVAE, the result reported by us in Table 2 is **even better** than what reported in GVAE paper! Before going further, we would like to emphasize that these are our efforts to be honest with baselines.\n\nSecondly, we are not reporting “valid reconstruction” as you mentioned. We are reporting two completely separate metrics namely (1) **exact** reconstruction and (2) valid **prior**, faithfully following the protocol in GVAE’s paper (in its Appendix C). From the number you are mentioned, we highly doubt that you are referring to accuracy in single character-level (not reconstructing entire SMILES), since we also get ~75% character accuracy for CVAE with kekule data. \n\nThirdly, we don’t think the kekule form makes the problem simpler. (1) Take the benzene ring for example. Non-kekule form is “c1ccccc1” but the kekule form is “C1=CC=CC=C1”. The generator should output the alternating single and double bonds in the exact same way. This actually makes the problem harder for some baselines! (2) Also the SMILES space you mentioned “26^85 with kekulisation vs 36^120 without” is puzzling without further clarifying how it is calculated, as the magic numbers differ from statistics of the ZINC data, where in kekule data, the maximum length is similar (114 vs 120), while average length is longer but still close (48 vs 44). We used “Marvin (https://chemaxon.com/products)” to get the kekule form. (3) We don’t think kekulisation is “lossy” as you mentioned, at least for Zinc dataset. What we did is to get the canonical form (using RDKIT) of both kekule and normal SMILES, and we verified that for all SMILES the canonical form matched. We would argue that at least for our experiment that covers our method and previous baselines, the two forms have the same theoretical representation power in this dataset.\n\nGiven the above reasons, it is reasonable that the result we got for CVAE is much worse with kekule form under our metrics. Again, we emphasize the metric used in the code in CVAE is totally different. If you also use the CVAE code from https://github.com/mkusner/grammarVAE, then you will see it outputs something like [“loss”, “acc”]. We run the same code for kekule data, and it outputs “acc” roughly 75%, which is similar to what you mentioned. **However**, this is the single character-level accuracy, not the accuracy of recovering entire SMILES. Accuracy for successfully reconstructing the entire SMILES as a whole is almost zero as we tried (e.g., 0.75^100).\n\nSo we suggest to please make several things clear first for a constructive discussion:\n1) Is the same Kekule form used? Our data has 114 max length but you reported 85.\n2) Is the same CVAE code from grammarVAE used? \n3) Are we reporting the same metric? Since our single character-level accuracy matches your number which is roughly 75%, this may indicate a confusion in choice of accuracy. **Note that reconstructing the entire sequence as a whole exactly is significantly harder for sequence model**.\n\nWe reiterate that what you mentioned under the clarification above is already addressed in our paper. If you think your experiments are consistent with above (code, data, metric, etc), could you please kindly share your code and kekule data through anonymous link, so that we can calibrate our possible disagreement and make clear the possible issue? \n\nFinally we expect the anonymity can post the comments to public for timely discussion, otherwise, we cannot receive the update email from the OpenReview system in time, which may delay our reply and hurt the constructiveness of the discussion.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222827834,"tcdate":1511816606483,"number":3,"cdate":1511816606483,"id":"SkUs6e5lG","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Review","forum":"SyqShMZRb","replyto":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference/Paper958/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Strong paper presents state-of-the-art results","rating":"7: Good paper, accept","review":"NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset? Fair comparison of the data is a serious concern. I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author's response.\n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose “stochastic lazy attributes”, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid. Using Bayesian Optimization, search over this space can yield decodings with targeted properties.\n\nMany of the ideas presented are novel. The results presented are state-of-the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem. This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning.\n\n*Remarks on methodology*\n\nBy initializing a decoding by “guessing” a value, the decoder will focus on high-probability starting regions of the space of possible structures. It is not clear to me immediately how this will affect the output distribution. Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space. Does this sacrifice exploration for exploitation in some quantifiable way? Some exploration of this issue or commentary would be valuable. \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background. In my opinion, this section could benefit from a little more expansion and conceptual definition.\n\nThe first 3 sections of the paper are very clearly written, but the remainder has many typos and grammatical errors (often word omission). The draft could use a few more passes before publication.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"ddate":null,"tddate":1511820998055,"tmdate":1512222827875,"tcdate":1511815261391,"number":2,"cdate":1511815261391,"id":"ByHD_eqxf","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Review","forum":"SyqShMZRb","replyto":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference/Paper958/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess. \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity. \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones.\nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar.\nThe authors evaluate their methods on two tasks, program generation and molecule generation. \n\nTheir method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model. \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.  \nHowever, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this. \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though. \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format?\n\nTypos:\n- Page 5: \"while in sampling procedure\" -> \"while in the sampling procedure\"\n- Page 6: \"a deep convolution neural networks\" -> \"a deep convolutional neural network\"\n- Page 6: \"KL-divergence that proposed in\" -> \"KL-divergence that was proposed in\" \n- Page 6: \"since in training time\" -> \"since at training time\"\n- Page 6: \"can effectively computed\" -> \"can effectively be computed\"\n- Page 7: \"reset for training\" -> \"rest for training\" ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222827916,"tcdate":1511804154635,"number":1,"cdate":1511804154635,"id":"rJ7ZTaYxf","invitation":"ICLR.cc/2018/Conference/-/Paper958/Official_Review","forum":"SyqShMZRb","replyto":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference/Paper958/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but poor presentation","rating":"3: Clear rejection","review":"The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable.  The idea presented seems to have merit , however, I found the presentation lacking. Many sentences are poorly written making the paper hard to read, especially when not familiar with the presented methods. The experimental section could be organized better. I didn't like that two types of experiment are now presented in parallel. Finally, the paper stops abruptly without any final discussion and/or conclusion. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1510092383488,"tcdate":1509137482313,"number":958,"cdate":1510092361133,"id":"SyqShMZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyqShMZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Syntax-Directed Variational Autoencoder for Structured Data","abstract":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, \\eg, computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing \\emph{stochastic lazy attributes}. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only \\emph{syntactically} valid, but also \\emph{semantically} reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.","pdf":"/pdf/bcb84d5d815b673a07f8784ab66d446ba4069f2a.pdf","TL;DR":"A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance","paperhash":"anonymous|syntaxdirected_variational_autoencoder_for_structured_data","_bibtex":"@article{\n  anonymous2018syntax-directed,\n  title={Syntax-Directed Variational Autoencoder for Structured Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqShMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper958/Authors"],"keywords":["generative model for structured data","syntax-directed generation","molecule and program optimization","variational autoencoder"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}