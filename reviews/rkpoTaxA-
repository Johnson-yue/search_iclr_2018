{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222652878,"tcdate":1511927632005,"number":3,"cdate":1511927632005,"id":"S1OIJnigM","invitation":"ICLR.cc/2018/Conference/-/Paper435/Official_Review","forum":"rkpoTaxA-","replyto":"rkpoTaxA-","signatures":["ICLR.cc/2018/Conference/Paper435/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The method was not particularly novel but using \"self-ensembling\" seemed to win the VISDA 2017 domain adaptation competition.","rating":"7: Good paper, accept","review":"The paper was very well-written, and mostly clear, making it easy to follow. The originality of the main method was not immediately apparent to me. However, the authors clearly outline the tricks they had to do to achieve good performance on multiple domain adaptation tasks: confidence thresholding, particular data augmentation, and a loss to deal with imbalanced target datasets, all of which seem like good tricks-of-the-trade for future work. The experimentation was extensive and convincing.\n\nPros:\n* Winning entry to the VISDA 2017 visual domain adaptation challenge competition.\n* Extensive experimentation on established toy datasets (USPS<>MNIST, SVHN<>MNIST, SVHN, GTSRB) and other more real-world datasets (including the VISDA one)\n\nCons:\n* Literature review on domain adaptation was lacking. Recent CVPR papers on transforming samples from source to target should be referred to, one of them was by Shrivastava et al., Learning from Simulated and Unsupervised Images through Adversarial Training, and another by Bousmalis et al., Unsupervised Pixel-level Domain Adaptation with GANs. Also you might want to mention Domain Separation Networks which uses gradient reversal (Ganin et al.) and autoencoders (Ghifary et al.). There was no mention of MMD-based methods, on which there are a few papers. The authors might want to mention non-Deep Learning methods also, or that this review relates to neural networks,\n* On p. 4 it wasn't clear to me how the semi-supervised tasks by Tarvainen and Laine were different to domain adaptation. Did you want to say that the data distributions are different? How does this make the task different. Having source and target come in different minibatches is purely an implementation decision.\n* It was unclear to me what  footnote a. on p. 6 means. Why would you combine results from Ganin et al. and Ghifary et al. ?\n* To preserve anonymity keep acknowledgements out of blind submissions. (although not a big deal with your acknowledgements)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-ensembling for visual domain adaptation","abstract":"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.","pdf":"/pdf/1398f314b3d5ac46c9a47acd1773d9ae2db4efba.pdf","TL;DR":"Self-ensembling based label propagation algorithm for visual domain adaptation, won VisDA-2017 image classification domain adaptation challenge.","paperhash":"anonymous|selfensembling_for_visual_domain_adaptation","_bibtex":"@article{\n  anonymous2018self-ensembling,\n  title={Self-ensembling for visual domain adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkpoTaxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper435/Authors"],"keywords":["deep learning","neural networks","domain adaptation","images","visual","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1512222652918,"tcdate":1511914256548,"number":2,"cdate":1511914256548,"id":"r1uziOjxf","invitation":"ICLR.cc/2018/Conference/-/Paper435/Official_Review","forum":"rkpoTaxA-","replyto":"rkpoTaxA-","signatures":["ICLR.cc/2018/Conference/Paper435/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper authors domain adaptation problems using techniques from semi-supervised learning and achieves impressive empirical results.","rating":"7: Good paper, accept","review":"The paper addresses the problem of domain adaptation: Say you have a source dataset S of labeled examples and you have a target dataset T of unlabeled examples and you want to label examples from the target dataset. \n\nThe main idea in the paper is to train two parallel networks, a 'teacher network' and a 'student network', where the student network has a loss term that takes into account labeled examples and there is an additional loss term coming from the teacher network that compares the probabilities placed by the two networks on the outputs. This is motivated by a similar network introduced in the context of semi-supervised learning by Tarvainen and Valpola (2017). The parameters are then optimized by gradient descent where the weight of the loss-term associated with the unsupervised learning part follows a Gaussian curve (with time). No clear explanation is provided for why this may be a good thing to try. The authors also use other techniques like data augmentation to enhance their algorithms.\n\nThe experimental results in the paper are quite nice. They apply the methodology to various standard vision datasets with noticeable improvements/gains and in one case by including additional tricks manage to better than other methods for VISDA-2017 domain adaptation challenge. In the latter, the challenge is to use computer-generated labeled examples and use this information to label real photographic images. The present paper does substantially better than the competition for this challenge. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-ensembling for visual domain adaptation","abstract":"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.","pdf":"/pdf/1398f314b3d5ac46c9a47acd1773d9ae2db4efba.pdf","TL;DR":"Self-ensembling based label propagation algorithm for visual domain adaptation, won VisDA-2017 image classification domain adaptation challenge.","paperhash":"anonymous|selfensembling_for_visual_domain_adaptation","_bibtex":"@article{\n  anonymous2018self-ensembling,\n  title={Self-ensembling for visual domain adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkpoTaxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper435/Authors"],"keywords":["deep learning","neural networks","domain adaptation","images","visual","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1512222652958,"tcdate":1511809564598,"number":1,"cdate":1511809564598,"id":"S1HXzycxf","invitation":"ICLR.cc/2018/Conference/-/Paper435/Official_Review","forum":"rkpoTaxA-","replyto":"rkpoTaxA-","signatures":["ICLR.cc/2018/Conference/Paper435/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper presents a domain adaptation algorithm based on the self-ensembling method proposed by [Tarvainen & Valpola, 2017]. The main idea is to enforce the agreement between the predictions of the teacher and the student classifiers on the target domain samples while training the student to perform well on the source domain. The teacher network is simply an exponential moving average of different versions of the student network over time.   \n\nPros:\n+ The paper is well-written and easy to read\n+ The proposed method is a natural extension of the mean teacher semi-supervised learning model by [Tarvainen & Valpola, 2017]\n+ The model achieves state-of-the-art results on a range of visual domain adaptation benchmarks (including top performance in the VisDA17 challenge)\n\nCons:\n- The model is tailored to the image domain as it makes heavy use of the data augmentation. That restricts its applicability quite significantly. I’m also very interested to know how the proposed method works when no augmentation is employed (for fair comparison with some of the entries in Table 1).\n- I’m not particularly fond of the engineering tricks like confidence thresholding and the class balance loss. They seem to be essential for good performance and thus, in my opinion, reduce the value of the main idea.\n- Related to the previous point, the final VisDA17 model seems to be engineered too heavily to work well on a particular dataset. I’m not sure if it provides many interesting insights for the scientific community at large.\n\nIn my opinion, it’s a borderline paper. While the best reported quantitative results are quite good, it seems that achieving those requires a significant engineering effort beyond just applying the self-ensembling idea. \n\nNotes:\n* The paper somewhat breaks the anonymity of the authors by mentioning the “winning entry in the VISDA-2017”. Maybe it’s not a big issue but in my opinion it’s better to remove references to the competition entry.\n* Page 2, 2.1, line 2, typo: “stanrdard” -> “standard”","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-ensembling for visual domain adaptation","abstract":"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.","pdf":"/pdf/1398f314b3d5ac46c9a47acd1773d9ae2db4efba.pdf","TL;DR":"Self-ensembling based label propagation algorithm for visual domain adaptation, won VisDA-2017 image classification domain adaptation challenge.","paperhash":"anonymous|selfensembling_for_visual_domain_adaptation","_bibtex":"@article{\n  anonymous2018self-ensembling,\n  title={Self-ensembling for visual domain adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkpoTaxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper435/Authors"],"keywords":["deep learning","neural networks","domain adaptation","images","visual","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1510152415224,"tcdate":1510152415224,"number":1,"cdate":1510152415224,"id":"B1vyYcxyG","invitation":"ICLR.cc/2018/Conference/-/Paper435/Official_Comment","forum":"rkpoTaxA-","replyto":"rkpoTaxA-","signatures":["ICLR.cc/2018/Conference/Paper435/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper435/Authors"],"content":{"title":"Corrections to text in red","comment":"There are three instances of text in red that indicate items that we would like to correct.\n\nFirstly in the conclusions section on page 9 the word 'check' in red was a 'note to self' to verify the fact that our networks also exhibit strong performance on sample from the source domain. At submission time our experiment logs from our small image benchmarks backed this claim up. At the time we had not managed to verify this claim for the VisDA experiments, hence the 'note to self'. This has since been done and the claim holds. Given our approach to training (simultaneous supervised training on source domain and unsupervised training on target domain) we had a strong reason to believe this claim to be true at the time of submission.\n\nIn tables 1 and 2 on page 6 there are results in red, as they result from averaging less than the 5 independent runs as claimed in the table 1 caption. We have since run more experiments to get the full 5 results. The only substantial change is the 11.11 +/- 0 result for STL -> CIFAR in the 'Mean teacher' row which has now changed to 15.51 +/- 8.7. The rest are within a few tenths of a % of the results shown in the submitted version.\n\nFurthermore, since submission we discovered a bug in our image augmentation code that affects the small colour image benchmarks (STL <-> CIFAR, Syn Digits -> SVHN and SynSigns -> GTSRB). Fixing the bug looks set to yield improved results (so far by looking at the results from the experiments that have completed). We would like to update tables 1 and 2 to reflect this."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-ensembling for visual domain adaptation","abstract":"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.","pdf":"/pdf/1398f314b3d5ac46c9a47acd1773d9ae2db4efba.pdf","TL;DR":"Self-ensembling based label propagation algorithm for visual domain adaptation, won VisDA-2017 image classification domain adaptation challenge.","paperhash":"anonymous|selfensembling_for_visual_domain_adaptation","_bibtex":"@article{\n  anonymous2018self-ensembling,\n  title={Self-ensembling for visual domain adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkpoTaxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper435/Authors"],"keywords":["deep learning","neural networks","domain adaptation","images","visual","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1509739305050,"tcdate":1509117349328,"number":435,"cdate":1509739302395,"id":"rkpoTaxA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkpoTaxA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Self-ensembling for visual domain adaptation","abstract":"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et al. 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.","pdf":"/pdf/1398f314b3d5ac46c9a47acd1773d9ae2db4efba.pdf","TL;DR":"Self-ensembling based label propagation algorithm for visual domain adaptation, won VisDA-2017 image classification domain adaptation challenge.","paperhash":"anonymous|selfensembling_for_visual_domain_adaptation","_bibtex":"@article{\n  anonymous2018self-ensembling,\n  title={Self-ensembling for visual domain adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkpoTaxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper435/Authors"],"keywords":["deep learning","neural networks","domain adaptation","images","visual","computer vision"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}