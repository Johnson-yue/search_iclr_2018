{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222624107,"tcdate":1511918729978,"number":3,"cdate":1511918729978,"id":"ryMchFseG","invitation":"ICLR.cc/2018/Conference/-/Paper351/Official_Review","forum":"SyjsLqxR-","replyto":"SyjsLqxR-","signatures":["ICLR.cc/2018/Conference/Paper351/AnonReviewer2"],"readers":["everyone"],"content":{"title":"I like the message conveyed in this paper. However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are. Moreover, is CIFAR 10 experiments conclusive enought. ","rating":"6: Marginally above acceptance threshold","review":"This paper investigates the effect of adversarial training. Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against \"shared\" adversarial perturbation, in particular against universal perturbation. In contrast, it is less effective to protect against singular perturbations. Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective. Finally, they show that singular perturbations can be easily detected.\n\nI like the message conveyed in this paper. However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are. Moreover, is CIFAR 10 experiments conclusive enough. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222624146,"tcdate":1511835706983,"number":2,"cdate":1511835706983,"id":"HkmB_HqlG","invitation":"ICLR.cc/2018/Conference/-/Paper351/Official_Review","forum":"SyjsLqxR-","replyto":"SyjsLqxR-","signatures":["ICLR.cc/2018/Conference/Paper351/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Work would benefit from real world tests","rating":"6: Marginally above acceptance threshold","review":"This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples. It also analyses how adversarial training affects detection.\n\nThe robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem. The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect. It's also unclear from which epoch the adversarial examples were generated from in figure 5. Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios.\n\nThe results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surprising. There is also no analysis of what happens for adversarial examples for the detector.\n\nAlso, it's not clear from section 3.1 what inputs are used to generate the adversarial examples. Are they a random sample across the whole dataset?\n\nFinally, the paper spends significant time on describing MaxMin and MinMax and the graphical visualizations but the paper fails to show these graphical profiles for real models.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222624182,"tcdate":1511767551847,"number":1,"cdate":1511767551847,"id":"rydWCNKxz","invitation":"ICLR.cc/2018/Conference/-/Paper351/Official_Review","forum":"SyjsLqxR-","replyto":"SyjsLqxR-","signatures":["ICLR.cc/2018/Conference/Paper351/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting area of research, but paper lacks focus, does not give new theoretical insight and provides limited experiments.","rating":"3: Clear rejection","review":"Summary:\n\nThis paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations. Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR.\n\nThe authors conclude that in this experimental setting:\n- AT seems to defend models against shared dx's.\n- This is visible on universal perturbations, which become less effective as more AT is applied.\n- AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change.\n- Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT.\n\nPro:\n- Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking.\n- The visualizations of universal perturbations as they change during AT are nice.\n- The basic observation wrt the behavior of AT is clearly communicated.\n\nCon:\n- The experiments performed are interesting directions, although unfocused and rather limited in scope. For instance, does the same phenomenon happen for different datasets? Different models?\n- What happens when we use adversarial attacks different from FGSM? Do we get similar results?\n- The papers lacks a more in-depth theoretical analysis. Is there a principled reason AT+FGSM defends against universal perturbations?\n\nOverall:\n- As is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of AT. A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations.\n\nDetailed:\n-\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510668363988,"tcdate":1510668363988,"number":1,"cdate":1510668363988,"id":"r14Id_uyM","invitation":"ICLR.cc/2018/Conference/-/Paper351/Official_Comment","forum":"SyjsLqxR-","replyto":"BJ29Juukz","signatures":["ICLR.cc/2018/Conference/Paper351/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper351/Authors"],"content":{"title":"reply to code request","comment":"Thanks for your interest. At the moment, we cannot release code unfortunately. Feel free to ask any questions you have when trying to replicate our results."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510666132416,"tcdate":1510666132416,"number":1,"cdate":1510666132416,"id":"BJ29Juukz","invitation":"ICLR.cc/2018/Conference/-/Paper351/Public_Comment","forum":"SyjsLqxR-","replyto":"SyjsLqxR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"code request","comment":"Hi, I am participating in the reproducibility challenge, would you mind sharing your code?\n\nThanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739349525,"tcdate":1509103266603,"number":351,"cdate":1509739346866,"id":"SyjsLqxR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyjsLqxR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training","abstract":"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of classifiers against such adversarial perturbations, it leaves classifiers sensitive to them on a non-negligible fraction of the inputs. We argue that there are two different kinds of adversarial perturbations: shared perturbations which fool a classifier on many inputs and singular perturbations which only fool the classifier on a small fraction of the data. We find that adversarial training increases the robustness of classifiers against shared perturbations. Moreover, it is particularly effective in removing universal perturbations, which can be seen as an extreme form of shared perturbations. Unfortunately, adversarial training does not consistently increase the robustness against singular perturbations on unseen inputs. However, we find that adversarial training decreases robustness of the remaining perturbations against image transformations such as changes to contrast and brightness or  Gaussian blurring. It thus makes successful attacks on the classifier in the physical world less likely. Finally, we show that even singular perturbations can be easily detected and must thus exhibit generalizable patterns even though the perturbations are specific for certain inputs. ","pdf":"/pdf/38a8d315d361b56305e668b5bced89cbac9ee478.pdf","TL;DR":"We empirically show that adversarial training is effective for removing universal perturbations, makes adversarial examples less robust to image transformations, and leaves them detectable for a detection approach.","paperhash":"anonymous|universality_robustness_and_detectability_of_adversarial_perturbations_under_adversarial_training","_bibtex":"@article{\n  anonymous2018universality,,\n  title={Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjsLqxR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper351/Authors"],"keywords":["adversarial examples","adversarial training","universal perturbations","safety","deep learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}