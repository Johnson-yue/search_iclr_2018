{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222677589,"tcdate":1511809418001,"number":2,"cdate":1511809418001,"id":"BJGc-k9xG","invitation":"ICLR.cc/2018/Conference/-/Paper51/Official_Review","forum":"B1QgVti6Z","replyto":"B1QgVti6Z","signatures":["ICLR.cc/2018/Conference/Paper51/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting attempt on uniform convergence of general DNNs","rating":"7: Good paper, accept","review":"This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs). Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks. The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts. 2) Uniform convergence of the empirical risk to population risk. 3) Generalization bound based on stability. The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations.\n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points. Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW. Should the risk bound only depends on the dimensions of the matrix W?\n\n2) The comparison with Bartlett & Maassâ€™s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations. Thus, the authors need to refine BM's result for comparison.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Empirical Risk Landscape Analysis for Understanding Deep Neural Networks","abstract":"This work aims to provide  comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an $l$-layer linear neural network consisting of $\\dm_i$ neurons in the $i$-th layer, we prove the gradient of its empirical risk  uniformly converges to the one of its population risk, at the rate of $\\mathcal{O}(r^{2l} \\sqrt{l\\sqrt{\\max_i \\dm_i} s\\log(d/l)/n})$. Here $d$ is the total weight dimension, $s$ is the number of nonzero entries  of all the  weights and the magnitude  of weights per layer is upper bounded by $r$. Moreover, we prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks and provide convergence guarantee for each pair. We also establish the uniform convergence of the empirical risk to its population counterpart and further derive the stability and  generalization bounds for the empirical risk. In addition, we  analyze these properties for deep \\emph{nonlinear} neural networks with sigmoid activation functions. We prove  similar results for convergence behavior of their empirical risk gradients, non-degenerate stationary points as well as the empirical risk itself.\n\nTo our best knowledge, this work is the first one theoretically characterizing the uniform convergence of the gradient and stationary points of the empirical risk of DNN models, which benefits the theoretical understanding on  how  the neural network depth $l$, the layer width $\\dm_i$, the network size $d$, the sparsity in weight and the parameter magnitude $r$ determine the neural network landscape.","pdf":"/pdf/7370a0f8417b6613375b9c908ee368dd9a19e7cb.pdf","paperhash":"anonymous|empirical_risk_landscape_analysis_for_understanding_deep_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Risk Landscape Analysis for Understanding Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QgVti6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper51/Authors"],"keywords":["Deep Learning Analysis","Deep Learning Theory","Empirical Risk","Landscape Analysis","Nonconvex Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222677628,"tcdate":1511801752750,"number":1,"cdate":1511801752750,"id":"H1Wo7pKgM","invitation":"ICLR.cc/2018/Conference/-/Paper51/Official_Review","forum":"B1QgVti6Z","replyto":"B1QgVti6Z","signatures":["ICLR.cc/2018/Conference/Paper51/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This theoretical paper lacks rigor and references.","rating":"3: Clear rejection","review":"This paper studies empirical risk in deep neural networks. Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks.\nResults for deep linear neural networks are puzzling. Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression. So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play? \nAlso this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D. In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Empirical Risk Landscape Analysis for Understanding Deep Neural Networks","abstract":"This work aims to provide  comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an $l$-layer linear neural network consisting of $\\dm_i$ neurons in the $i$-th layer, we prove the gradient of its empirical risk  uniformly converges to the one of its population risk, at the rate of $\\mathcal{O}(r^{2l} \\sqrt{l\\sqrt{\\max_i \\dm_i} s\\log(d/l)/n})$. Here $d$ is the total weight dimension, $s$ is the number of nonzero entries  of all the  weights and the magnitude  of weights per layer is upper bounded by $r$. Moreover, we prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks and provide convergence guarantee for each pair. We also establish the uniform convergence of the empirical risk to its population counterpart and further derive the stability and  generalization bounds for the empirical risk. In addition, we  analyze these properties for deep \\emph{nonlinear} neural networks with sigmoid activation functions. We prove  similar results for convergence behavior of their empirical risk gradients, non-degenerate stationary points as well as the empirical risk itself.\n\nTo our best knowledge, this work is the first one theoretically characterizing the uniform convergence of the gradient and stationary points of the empirical risk of DNN models, which benefits the theoretical understanding on  how  the neural network depth $l$, the layer width $\\dm_i$, the network size $d$, the sparsity in weight and the parameter magnitude $r$ determine the neural network landscape.","pdf":"/pdf/7370a0f8417b6613375b9c908ee368dd9a19e7cb.pdf","paperhash":"anonymous|empirical_risk_landscape_analysis_for_understanding_deep_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Risk Landscape Analysis for Understanding Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QgVti6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper51/Authors"],"keywords":["Deep Learning Analysis","Deep Learning Theory","Empirical Risk","Landscape Analysis","Nonconvex Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739512604,"tcdate":1508770795285,"number":51,"cdate":1509739509948,"id":"B1QgVti6Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1QgVti6Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Empirical Risk Landscape Analysis for Understanding Deep Neural Networks","abstract":"This work aims to provide  comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an $l$-layer linear neural network consisting of $\\dm_i$ neurons in the $i$-th layer, we prove the gradient of its empirical risk  uniformly converges to the one of its population risk, at the rate of $\\mathcal{O}(r^{2l} \\sqrt{l\\sqrt{\\max_i \\dm_i} s\\log(d/l)/n})$. Here $d$ is the total weight dimension, $s$ is the number of nonzero entries  of all the  weights and the magnitude  of weights per layer is upper bounded by $r$. Moreover, we prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks and provide convergence guarantee for each pair. We also establish the uniform convergence of the empirical risk to its population counterpart and further derive the stability and  generalization bounds for the empirical risk. In addition, we  analyze these properties for deep \\emph{nonlinear} neural networks with sigmoid activation functions. We prove  similar results for convergence behavior of their empirical risk gradients, non-degenerate stationary points as well as the empirical risk itself.\n\nTo our best knowledge, this work is the first one theoretically characterizing the uniform convergence of the gradient and stationary points of the empirical risk of DNN models, which benefits the theoretical understanding on  how  the neural network depth $l$, the layer width $\\dm_i$, the network size $d$, the sparsity in weight and the parameter magnitude $r$ determine the neural network landscape.","pdf":"/pdf/7370a0f8417b6613375b9c908ee368dd9a19e7cb.pdf","paperhash":"anonymous|empirical_risk_landscape_analysis_for_understanding_deep_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Risk Landscape Analysis for Understanding Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QgVti6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper51/Authors"],"keywords":["Deep Learning Analysis","Deep Learning Theory","Empirical Risk","Landscape Analysis","Nonconvex Optimization"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}