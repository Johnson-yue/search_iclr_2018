{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222810260,"tcdate":1511820723832,"number":3,"cdate":1511820723832,"id":"BynhTbclf","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Authors explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. They show that this last helps to improve the performance of CNNs the most.","rating":"7: Good paper, accept","review":"It is an original contribution towards  global aggregation over time. Results are significant. \n\nThe idea that the attention mechanism can be viewed as a special case of a global aggregation operation along the time axis, and has learnable parameters is interesting.\n\nThe model is well described and motivated.\n\nPlease precise fig 4, not easy to read. \nHowever other results / tables are very efficent.\n\nps : complete link end of section 1.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.","pdf":"/pdf/fc650eda1357ae45dae0b8c4bac79fd119e5cc13.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1512222810302,"tcdate":1511818739284,"number":2,"cdate":1511818739284,"id":"Skjx8WqxG","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting dataset that is not fully utilized, could be clearer on models as well","rating":"4: Ok but not good enough - rejection","review":"This paper describes the use of convolutional neural networks for classification and similarity of solo vocal performances.  It compares vanilla CNNs, ResNets and ResNeXts on two tasks: singer classification using softmax outputs and singer clustering using Siamese networks.  It also compares using fixed pooling versus feed-forward attention to combine the output of the convoultional layers into a fixed-sized representation.  Experimental results show that the singer classification task seems to work well, while the qualitative results on singer clustering are less conclusive.\n\nBecause this is a very empirical comparison of several system variations, its conclusions rely to a very large extent on the data that it is trained and evaluated on.  It is difficult to understand from the paper what that is, exactly.  The dataset is described as an addition to the DAMP dataset containing more singers, but the actual dataset used in the experiments is a tiny subset of either this new dataset or the original DAMP dataset.  It is not clear why it is so small.  The description of the DAMP-balanced dataset is unclear as well.  Where did the extra music in the DAMP-balanced dataset come from? The same source as DAMP?  Are all 3462 singers from DAMP also in DAMP-balanced?  In appendix A, how can this dataset be balanced if some songs have more performances than others?  How can 5429 singers have all performed all of these songs if all songs have fewer than 5429 performances?\n\nThe use of the tiny subset also seems to have affected the performance of the networks.  The conclusions report that feed-forward attention \"makes learning faster but also is more probably to overfit the data\" and that \"The experiment results do not suggest that deper architecture leads to better performance...\"  Both of these could be explained by using too little data to train the models.  If there is so much data available, artificially limiting the data to be used in experiments doesn't make sense.\n\nIn addition to incomplete descriptions of the data used, the models could be defined more completely as well.  On page 3 and in figure 2, it is not clear which layers are the max pooling layers, what the relationship between cardinality and group = 32 is, or how the first layer of (b) can have 64 output dimensions and the second layer have 256 input dimensions.  \n\nIn general in terms of system design, 2D convolution for audio would make more sense if it were performed on a spectrogram with a logarithmic frequency axis, like some constant-Q transforms, in which case changes in pitch result in shifts in frequency.  The current paper uses mel spectrograms, which do not have this property, since mel frequency is linear below 1000 Hz.  In addition, the feedforward attention mechanism seems to be able to find a single pattern to pay attention to, since it only uses a single weight vector and not a matrix.  It would be interesting to explore more sophisticated attention mechanisms.\n\nFinally, in terms of experimental validation, the results in figure 4, showing PCA projections of songs colored by artist, are not very convincing.  It is not clear to me that coloring by singer looks more \"clustered\" than coloring by song.  It would be much more convincing to perform k-nearest neighbors classification with the learned embeddings for singer verification.  That way performance could be measured quantitatively.  In addition, you could use different singers in the test set with an \"enrollment\" set of examples from each one that are processed by the siamese network but not trained on it.\n\n\n\nMinor Comments\n--------------\n\np2: \"When the same song is performed by different singers, ... -- ie., the 'song effect'\" citation?\n\np2: \"songs sang by a singer\" typo: \"songs sung by a singer\"\n\np2: \"Lastly, related work is described in section ??\" typo\n\np4: Please number all equations so that readers and reviewers can refer to them!\n\np6: \"all the neural network models far exceeded it by at least 25%\" is that relative or absolute?  Either way, it looks like more than that.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.","pdf":"/pdf/fc650eda1357ae45dae0b8c4bac79fd119e5cc13.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1512222810342,"tcdate":1511416914945,"number":1,"cdate":1511416914945,"id":"H1iUNk4gz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting application but not much novelty","rating":"5: Marginally below acceptance threshold","review":"This paper discusses the application of modern deep convolutional network architectures to the task of singer identification.  It also compares different ways of aggregating a sequence over time to produce a single embedding.  The paper is mainly empirical.\n\nI am a bit on the fence about this paper, because on the one hand I think it's important to study the relative benefits of different convolutional models on tasks other than vision, but on the other hand I'm worried the results presented in this paper have limited applicability outside of the experiment/domain presented.  If the paper discussed applied convolutional networks to many various sequence classification tasks (document classification, speaker identification, etc), I might be more convinced that general lessons could be learned from these results.  The paper also does not propose anything novel - all of the models and techniques are taken from existing work (by contrast, imagine that instead the paper proposing some novel temporal integration method and showing that it gave amazing results compared to max-pooling, average-pooling, or feed-forward attention).  In particular, the model seems very similar to that of Raffel & Ellis 2016, except that the task is somewhat different (though still audio-related) and various different (pre-existing) convolutional models and temporal integration techniques are compared.  For these reasons I don't think it makes a lot of sense to publish this at ICLR, but I think it would be well-suited to a more (audio) application-specific venue.\n\nSome specific comments/suggestions:\n\n-  \" Ideally, it should be possible to identify 'singing style' or 'singing characteristics' by looking (and listening) to the clusters formed from the projections of audio recordings onto the embedding space.\"  Unless the embedding space is 2 (or 3) dimensional, you can't really *look* at clusters in it - you have to also project it down to 2 (or 3) dimensions.  Probably worth mentioning this.\n- \"The interfering 'song effect' is even more dominant in the singing voice case than that of the environment/pose effect in face recognition.\" Citation needed!\n- May be worth mentioning another benefit of embeddings - namely it makes it so that comparing two long sequences (spectrograms) can be done efficiently with a simple Euclidian distance operation.  You can also precompute embeddings for all of the entries in some large database of spectrograms and efficiently find the closest match to a query, etc.\n- Along those lines, I think you can actually unify singer classification and singer performance embedding by doing k-nearest-neighbors in embedding space against a database of recordings of different singers (as opposed to using a softmax to classify singer, as you propose).  This is the approach sometimes taken e.g. in speaker identification and face recognition.\n- LaTeX typo: \"related work is described insection ??.\"\n- Vague ask, but I think Figure 1 could be a lot more descriptive - maybe illustrating that the input starts as a sequence, which is then transformed to a bank of (filtered) sequences, then collapsed across time to a single vector via the temporal aggregation, etc...\n- For completeness, you should provide equations for the different convolutional blocks you are describing.\n- Suggestions for future work: 1) Try striding the convolutions instead of max-pooling; this seems like more common practice these days. 2) Your temporal pooling operations ignore temporal order (e.g. if you permuted the order of the input sequence, the output won't change).  It was shown in \"Attention is All You Need\" that if you include an encoding of the temporal position, attention mechanisms of this type can benefit.  You might try this too.\n- I think you can shorten some of the description of computing the mel spectrograms, or move some of this information into the appendix.\n- Casey et al. (2008) should be \\citep\n- Your train/test accuracies are very close in some cases - can you run a significance analysis?\n- LaTeX typo:  \"earlier in section .\"\n- It seems like you should be using a more powerful dimensionality reduction method (like t-SNE) instead of PCA for visualization, unless you have an important reason to use PCA.\n- To be honest, it is not obvious at all that the networks learned to cluster songs from the same singer - the plots look like they have a lot of overlap between the clusters corresponding to each singer.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.","pdf":"/pdf/fc650eda1357ae45dae0b8c4bac79fd119e5cc13.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1509739492931,"tcdate":1508951551261,"number":90,"cdate":1509739490278,"id":"SJwZUHRTZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation.","pdf":"/pdf/fc650eda1357ae45dae0b8c4bac79fd119e5cc13.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}