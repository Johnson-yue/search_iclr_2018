{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222647018,"tcdate":1511811059316,"number":3,"cdate":1511811059316,"id":"ryse_yclM","invitation":"ICLR.cc/2018/Conference/-/Paper41/Official_Review","forum":"Skk3Jm96W","replyto":"Skk3Jm96W","signatures":["ICLR.cc/2018/Conference/Paper41/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A new exploration algorithm for reinforcement learning","rating":"4: Ok but not good enough - rejection","review":"Summary: this paper proposes algorithmic extensions to two existing RL algorithms to improve exploration in meta-reinforcement learning. The new approach is compared to the baselines on which they are built on a new domain, and a grid-world.\n\nThis paper needs substantial revision. The first and primary issue is that authors claim their exists not prior work on \"exploration in Meta-RL\". This appears to be the case because the authors did not use the usual names for this: life-long learning, learning-to-learn, continual learning, multi-task learning, etc. If you use these terms you see that much of the work in these settings is about how to utilize and adapt exploration. Either given a \"free learning phases\", exploration based in internal drives (curiosity, intrinsic motivation). These are subfields with too much literature to list here. The paper under-review must survey such literature and discuss why these new approaches are a unique contribution.\n\nThe empirical results do not currently support the claimed contributions of the paper. The first batch of results in on a new task introduced by this paper. Why was a new domain introduced? How are existing domains not suitable. This is problematic because domains can easily exhibit designer bias, which is difficult to detect. Designing domains are very difficult and why benchmark domains that have been well vetted by the community are such an important standard. In the experiment, the parameters were randomly sampled---is a very non-conventional choice. Usually one performance a search for the best setting and then compares the results. This would introduce substantial variance in the results, requiring many more runs to make statistically significant conclusions.\n\nThe results on the first task are not clear. In fig4 one could argue that e-maml is perhaps performing the best, but the variance of the individual lines makes it difficult to conclude much. In fig5 rl2 gets the best final performance---do you have a hypothesis as to why? Much more analysis of the results is needed.\n\nThere are well-known measures used in transfer learning to access performance, such as jump-start. Why did you define new ones here?\n \nFigure 6 is difficult to read. Why not define the Gap and then plot the gap. These are very unclear plots especially bottom right. It's your job to sub-select and highlight results to clearly support the contribution of the paper---that is not the case here. Same thing with figure 7. I am not sure what to conclude from this graph.\n\nThe paper, overall is very informal and unpolished. The text is littered with colloquial language, which though fun, is not as precise as required for technical documents. Meta-RL is never formally and precisely defined. There are many strong statements e.g., : \"which indicates that at the very least the meta learning is able to do system identification correctly.\">> none of the results support such a claim. Expectations and policies are defined with U which is never formally defined. The background states the problem of study is a finite horizon MDP, but I think they mean episodic tasks. The word heuristic is used, when really should be metric or measure.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Some Considerations on Learning to Explore via Meta-Reinforcement Learning","abstract":"We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2. Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments. We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.","pdf":"/pdf/1a685daef9e9fcd0e261d64d33251d7809aa33a3.pdf","TL;DR":"Modifications to MAML and RL2 that should allow for better exploration. ","paperhash":"anonymous|some_considerations_on_learning_to_explore_via_metareinforcement_learning","_bibtex":"@article{\n  anonymous2018some,\n  title={Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skk3Jm96W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper41/Authors"],"keywords":["reinforcement learning","rl","exploration","meta learning","meta reinforcement learning","curiosity"]}},{"tddate":null,"ddate":null,"tmdate":1512222647068,"tcdate":1511629771783,"number":2,"cdate":1511629771783,"id":"SkE07mveG","invitation":"ICLR.cc/2018/Conference/-/Paper41/Official_Review","forum":"Skk3Jm96W","replyto":"Skk3Jm96W","signatures":["ICLR.cc/2018/Conference/Paper41/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction for exploration in meta-RL. Many relations to prior work missing though. Let's wait for rebuttal.","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a trick of extending objective functions to drive exploration in meta-RL on top of two recent so-called meta-RL algorithms, Model-Agnostic Meta-Learning (MAML) and RL^2. \n\nPros:\n\n+ Quite simple but promising idea to augment exploration in MAML and RL^2 by taking initial sampling distribution into account. \n\n+ Excellent analysis of learning curves with variances across two different environments. Charts across different random seeds and hyperparameters indicate  reproducibility. \n\n\nCons/Typos/Suggestions:\n\n- The brief introduction to meta-RL is missing lots of related work - see below.\n\n- Equation (3) and equations on the top of page 4: Mathematically, it looks better to swap \\mathrm{d}\\tau and \\mathrm{d}\\bar{\\tau}, to obtain a consistent ordering with the double integrals. \n\n- In page 4, last paragraph before Section 5, “However, during backward pass, the future discounted returns for the policy gradient computation will zero out the contributions from exploratory episodes”:  I did not fully understand this - please explain better. \n\n- It is not very clear if the authors use REINFORCE or more advanced approaches like TRPO/PPO/DDPG to perform policy gradient updates?\n\n- I'd like to see more detailed hyperparameter settings. \n\n- Figures 10, 11, 12, 13, 14: Too small to see clearly. I would propose to re-arrange the figures in either [2, 2]-layout, or a single column layout, particularly for Figure 14. \n\n- Figures 5, 6, 9: Wouldn't it be better to also use log-scale on the x-axis for consistent comparison with curves in Krazy World experiments ?\n\n3. It could be very interesting to benchmark also in Mujoco environments, such as modified Ant Maze. \n\nOverall, the idea proposed in this paper is interesting. I agree with the authors that a good learner should be able to generalize to new tasks with very few trials compared with learning each task from scratch. This, however, is usually called transfer learning, not metalearning. As mentioned above, experiments in more complex, continuous control tasks with Mujoco simulators might be illuminating. \n\nRelation to prior work:\n\np 2: Authors write: \"Recently, a flurry of new work in Deep Reinforcement Learning has provided the foundations for tackling RL problems that were previously thought intractable. This work includes: 1) Mnih et al. (2015; 2016), which allow for discrete control in complex environments directly from raw images. 2) Schulman et al. (2015); Mnih et al. (2016); Schulman et al. (2017); Lillicrap et al. (2015), which have allowed for high-dimensional continuous control in complex environments from raw state information.\"\n\nHere it should be mentioned that the first RL for high-dimensional continuous control in complex environments from raw state information was actually published in mid 2013:\n\n(1) Koutnik, J., Cuccu, G., Schmidhuber, J., and Gomez, F. (July 2013). Evolving large-scale neural networks for vision-based reinforcement learning. GECCO 2013, pages 1061-1068, Amsterdam. ACM.\n\np2: Authors write: \"In practice, these methods are often not used due to difficulties with high-dimensional observations, difficulty in implementation on arbitrary domains, and lack of promising results.\"\n\nNot quite true - RL robots with high-dimensional video inputs and intrinsic motivation learned to explore in 2015: \n\n(2) Kompella, Stollenga, Luciw, Schmidhuber. Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots. Artificial Intelligence, 2015.\n\np2: Authors write: \"Although this line of work does not explicitly deal with exploration in meta learning, it remains a large source of inspiration for this work.\"\n\np2: Authors write: \"To the best of our knowledge, there does not exist any literature addressing the topic of exploration in meta RL.\"\n\nBut there is such literature - see the following meta-RL work where exploration is the central issue:\n\n(3) J. Schmidhuber. Exploring the Predictable. In Ghosh, S. Tsutsui, eds., Advances in Evolutionary Computing, p. 579-612, Springer, 2002.\n\nThe RL method of this paper is the one from the original meta-RL work:\n\n(4) J. Schmidhuber. On learning how to learn learning strategies. Technical Report FKI-198-94, Fakultät für Informatik, Technische Universität München, November 1994.\n\nWhich then led to:\n\n(5) J. Schmidhuber, J.  Zhao, N. Schraudolph. Reinforcement learning with self-modifying policies. In S. Thrun and L. Pratt, eds., Learning to learn, Kluwer, pages 293-309, 1997.\n\np2: \"In hierarchical RL, a major focus is on learning primitives that can be reused and strung together. These primitives will frequently enable better exploration, since they’ll often relate to better coverage over state visitation frequencies. Recent work in this direction includes (Vezhnevets et al., 2017; Bacon & Precup, 2015; Tessler et al., 2016; Rusu et al., 2016).\"\n\nThese are very recent refs - one should cite original work on hierarchical RL including:\n\nJ.  Schmidhuber. Learning to generate sub-goals for action sequences. In T. Kohonen, K. Mäkisara, O. Simula, and J. Kangas, editors, Artificial Neural Networks, pages 967-972. Elsevier Science Publishers B.V., North-Holland, 1991.\n\nM. B. Ring. Incremental Development of Complex Behaviors through Automatic Construction of Sensory-Motor Hierarchies. Machine Learning: Proceedings of the Eighth International Workshop, L. Birnbaum and G. Collins, 343-347, Morgan Kaufmann, 1991.\n\nM. Wiering and J. Schmidhuber. HQ-Learning. Adaptive Behavior 6(2):219-246, 1997\n\nReferences to original work on meta-RL are missing. How does the approach of the authors relate to the following approaches? \n\n(6) J. Schmidhuber. Gödel machines: Fully Self-Referential Optimal Universal Self-Improvers. In B. Goertzel and C. Pennachin, eds.: Artificial General Intelligence, p. 119-226, 2006. \n\n(7) J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook. Diploma thesis, TUM, 1987. \n \nPapers (4,5) above describe a universal self-referential, self-modifying RL machine. It can implement and run all kinds of learning algorithms on itself, but cannot learn them by gradient descent (because it's RL). Instead it uses what was later called the success-story algorithm (5) to handle all the meta-learning and meta-meta-learning etc.\n\nRef (7) above also has a universal programming language such that the system can learn to implement and run all kinds of computable learning algorithms, and uses what's now called Genetic Programming (GP), but applied to itself, to recursively evolve better GP methods through meta-GP and meta-meta-GP etc. \n\nRef (6) is about an optimal way of learning or the initial code of a learning machine through self-modifications, again with a universal programming language such that the system can learn to implement and run all kinds of computable learning algorithms.\n\nGeneral recommendation: Accept, provided the comments are taken into account, and the relation to previous work is established.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Some Considerations on Learning to Explore via Meta-Reinforcement Learning","abstract":"We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2. Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments. We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.","pdf":"/pdf/1a685daef9e9fcd0e261d64d33251d7809aa33a3.pdf","TL;DR":"Modifications to MAML and RL2 that should allow for better exploration. ","paperhash":"anonymous|some_considerations_on_learning_to_explore_via_metareinforcement_learning","_bibtex":"@article{\n  anonymous2018some,\n  title={Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skk3Jm96W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper41/Authors"],"keywords":["reinforcement learning","rl","exploration","meta learning","meta reinforcement learning","curiosity"]}},{"tddate":null,"ddate":null,"tmdate":1512222647122,"tcdate":1511540774475,"number":1,"cdate":1511540774475,"id":"SJ0Q_6Hlf","invitation":"ICLR.cc/2018/Conference/-/Paper41/Official_Review","forum":"Skk3Jm96W","replyto":"Skk3Jm96W","signatures":["ICLR.cc/2018/Conference/Paper41/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This is an interesting paper about correcting some of the myopic bias in meta RL. For two existing algorithms (MAML, RL2) it proposes a modification of the metaloss that encourages more exploration in the first (couple of) test episodes. The approach is a reasonable one, the proposed methods seem to work, the (toy) domains are appropriate, and the paper is well-rounded with background, motivation and a lot of auxiliary results.\n\nNevertheless, it could be substantially improved:\n\nSection 4 is of mixed rigor: some aspects are formally defined and clear, others are not defined at all, and in the current state many things are either incomplete or redundant. Please be more rigorous throughout, define all the terms you use (e.g. \\tau, R, \\bar{\\tau}, ...). Actually, the text never makes it clear how \\tau and \\ber{\\tau} relate to each other: make this connection in a formal way, please.\n\nIn your (Elman) formulation, “L” is not an RNN, but just a feed-forward mapping?\n\nEquation 3 is over-complicated: it is actually just a product of two integrals, because all the terms are separable. \n\nThe integral notation is not meaningful: you can’t sample something in the subscript the way you would in an expectation. Please make this rigorous.\n\nThe variability across seems extremely large, so it might be worth averaging over mores seeds for the learning curves, so that differences are more likely to be significant.\n\nFigure fontsizes are too small to read, and the figures in the appendix are impossible to read. Also, I’d recommend always plotting std instead of variance, so that the units or reward remain comparable.\n\nI understand that you built a rich, flexible domain. But please describe the variant you actually use, cleanly, without all the other variants. Or, alternatively, run experiments on multiple variants.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Some Considerations on Learning to Explore via Meta-Reinforcement Learning","abstract":"We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2. Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments. We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.","pdf":"/pdf/1a685daef9e9fcd0e261d64d33251d7809aa33a3.pdf","TL;DR":"Modifications to MAML and RL2 that should allow for better exploration. ","paperhash":"anonymous|some_considerations_on_learning_to_explore_via_metareinforcement_learning","_bibtex":"@article{\n  anonymous2018some,\n  title={Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skk3Jm96W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper41/Authors"],"keywords":["reinforcement learning","rl","exploration","meta learning","meta reinforcement learning","curiosity"]}},{"tddate":null,"ddate":null,"tmdate":1509739516392,"tcdate":1508679591075,"number":41,"cdate":1509739513737,"id":"Skk3Jm96W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Skk3Jm96W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Some Considerations on Learning to Explore via Meta-Reinforcement Learning","abstract":"We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2. Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments. We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.","pdf":"/pdf/1a685daef9e9fcd0e261d64d33251d7809aa33a3.pdf","TL;DR":"Modifications to MAML and RL2 that should allow for better exploration. ","paperhash":"anonymous|some_considerations_on_learning_to_explore_via_metareinforcement_learning","_bibtex":"@article{\n  anonymous2018some,\n  title={Some Considerations on Learning to Explore via Meta-Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skk3Jm96W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper41/Authors"],"keywords":["reinforcement learning","rl","exploration","meta learning","meta reinforcement learning","curiosity"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}