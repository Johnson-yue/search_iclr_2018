{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222826904,"tcdate":1511782124088,"number":3,"cdate":1511782124088,"id":"SkNxPOYlf","invitation":"ICLR.cc/2018/Conference/-/Paper95/Official_Review","forum":"ryALZdAT-","replyto":"ryALZdAT-","signatures":["ICLR.cc/2018/Conference/Paper95/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting analysis of feature norm, but the paper needs improvements","rating":"5: Marginally below acceptance threshold","review":"The analyses of this paper (1) increasing the feature norm of correctly-classified examples induce smaller training loss, (2) increasing the feature norm of mis-classified examples upweight the contribution from hard examples, are interesting. The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses. \n\nHowever, the presentation of this paper need to be largely improved. For example, Figure 3 seems to be not relevant to Property2 and may be show the feature norm is lower when the samples is hard example. Therefore, the author used reciprocal norm loss which increases feature norm as shown in Figure 4. However, both Figures are not explained in the main text, and thus hard to understand the relation of Figure 3 and 4. The author should refer all Figures and Tables. \n\nOther issues are:\n-Large-margin Soft max in Figure 2 is not explained in the introduction section. \n-In Eq.(7), P_j^I is not defined. \n- In the Property3, The author wrote “ where r is lower bound of feature norm”. \n However, r is not used.\n-In the experimental results, “RN” is not defined.\n-In the Table3, the order of \\lambda should be increasing or decreasing order. \n- Table 5 is not referred in the main text. \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feature Incay for Representation Regularization","abstract":"Softmax-based loss is widely used in deep learning for multi-class classification, where each class is represented by a weight vector and each sample is represented as a feature vector. Different from traditional learning algorithms where features are pre-defined and only weight vectors are tunable through training, feature vectors are also tunable as representation learning in deep learning. Thus we investigate how to improve the classification performance by better adjusting the features. One main observation is that elongating the feature norm of both correctly-classified and mis-classified feature vectors improves learning: (1) increasing the feature norm of correctly-classified examples induce smaller training loss; (2) increasing the feature norm of mis-classified examples can upweight the contribution from hard examples. Accordingly, we propose feature incay to regularize representation learning by encouraging larger feature norm. In contrast to weight decay which shrinks the weight norm, feature incay is proposed to stretch the feature norm. Extensive empirical results on MNIST, CIFAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay. ","pdf":"/pdf/6ead4af37219f9a975eff84d1121172fca2ce2fa.pdf","paperhash":"anonymous|feature_incay_for_representation_regularization","_bibtex":"@article{\n  anonymous2018feature,\n  title={Feature Incay for Representation Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryALZdAT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper95/Authors"],"keywords":["feature norm","regularization","softmax loss","feature incay"]}},{"tddate":null,"ddate":null,"tmdate":1512222826947,"tcdate":1511777606228,"number":2,"cdate":1511777606228,"id":"ryRBHPFxz","invitation":"ICLR.cc/2018/Conference/-/Paper95/Official_Review","forum":"ryALZdAT-","replyto":"ryALZdAT-","signatures":["ICLR.cc/2018/Conference/Paper95/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The idea is interesting, but motivation requires more justification. Results are good, but not very impressive","rating":"6: Marginally above acceptance threshold","review":"The manuscript proposes to increase the norm of the last hidden layer to promote better classification accuracy. However, the motivation is a bit less convincing. Here are a few motivations that are mentioned.\n(1) Increasing the feature norm of correctly classified examples helps cross entropy, which is of course correct. However, it only decreases the training loss. How do we know it will not lead to overfitting?\n(2) Increasing the feature norm of mis-classified examples will make gradient larger for self-correction. And the manuscript proves it in property 2. However, the proof seems not complete. In Eq (7), increasing the feature norm would also affect the value of the term in parenthesis. As an example, if a negative example is already mis-classified as a positive, and its current probability is very close to 1, then further increasing feature norm would make the probability even closer to 1, leading to saturation and smaller gradient.\n(3) Figure 1 shows that examples with larger feature norm tend to be predicted well. However, it is not very convincing since it is only a correlation rather than causality. Let's use simple linear softmax regression as a sanity check, where features to softmax are real features rather than hidden units. Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1.\n\nThe manuscript states that the feature norm won't be infinitely increased since there is an upper bound. However, the proof of property 3 seems to only apply to the certain cases where K<2D. In addition, alpha is in the formula of upper bound, but what is the upper bound of alpha?\n\nThe manuscript does comprehensive experiments to test the proposed method. The results are good, since the proposed method outperforms other baselines in most datasets. But the results are not impressively strong.\n\nMinor issues:\n(1) For proof of property 3, it seems that alpha and beta are used before defined. Are they the radius of two circles?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feature Incay for Representation Regularization","abstract":"Softmax-based loss is widely used in deep learning for multi-class classification, where each class is represented by a weight vector and each sample is represented as a feature vector. Different from traditional learning algorithms where features are pre-defined and only weight vectors are tunable through training, feature vectors are also tunable as representation learning in deep learning. Thus we investigate how to improve the classification performance by better adjusting the features. One main observation is that elongating the feature norm of both correctly-classified and mis-classified feature vectors improves learning: (1) increasing the feature norm of correctly-classified examples induce smaller training loss; (2) increasing the feature norm of mis-classified examples can upweight the contribution from hard examples. Accordingly, we propose feature incay to regularize representation learning by encouraging larger feature norm. In contrast to weight decay which shrinks the weight norm, feature incay is proposed to stretch the feature norm. Extensive empirical results on MNIST, CIFAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay. ","pdf":"/pdf/6ead4af37219f9a975eff84d1121172fca2ce2fa.pdf","paperhash":"anonymous|feature_incay_for_representation_regularization","_bibtex":"@article{\n  anonymous2018feature,\n  title={Feature Incay for Representation Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryALZdAT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper95/Authors"],"keywords":["feature norm","regularization","softmax loss","feature incay"]}},{"tddate":null,"ddate":null,"tmdate":1512222826992,"tcdate":1511768459885,"number":1,"cdate":1511768459885,"id":"BkEcWHKlf","invitation":"ICLR.cc/2018/Conference/-/Paper95/Official_Review","forum":"ryALZdAT-","replyto":"ryALZdAT-","signatures":["ICLR.cc/2018/Conference/Paper95/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper investigates how to finetune feature norms of correctly-classified and mis-classified feature vectors to improve learning process. Based on the analysis, they proposed feature incay to encourage larger feature norm. Experimental results on four datasets demonstrate the effectiveness of the proposed method.","rating":"6: Marginally above acceptance threshold","review":"Pros:\n1. It provided theoretic analysis why larger feature norm is preferred in feature representation learning.\n\n2. A new regularization method (feature incay) is proposed.\n\nCons:\nIt seems there is not much comparison between this proposed method and the concurrent work \"COCO(Liu et al. (2017c))\".","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feature Incay for Representation Regularization","abstract":"Softmax-based loss is widely used in deep learning for multi-class classification, where each class is represented by a weight vector and each sample is represented as a feature vector. Different from traditional learning algorithms where features are pre-defined and only weight vectors are tunable through training, feature vectors are also tunable as representation learning in deep learning. Thus we investigate how to improve the classification performance by better adjusting the features. One main observation is that elongating the feature norm of both correctly-classified and mis-classified feature vectors improves learning: (1) increasing the feature norm of correctly-classified examples induce smaller training loss; (2) increasing the feature norm of mis-classified examples can upweight the contribution from hard examples. Accordingly, we propose feature incay to regularize representation learning by encouraging larger feature norm. In contrast to weight decay which shrinks the weight norm, feature incay is proposed to stretch the feature norm. Extensive empirical results on MNIST, CIFAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay. ","pdf":"/pdf/6ead4af37219f9a975eff84d1121172fca2ce2fa.pdf","paperhash":"anonymous|feature_incay_for_representation_regularization","_bibtex":"@article{\n  anonymous2018feature,\n  title={Feature Incay for Representation Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryALZdAT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper95/Authors"],"keywords":["feature norm","regularization","softmax loss","feature incay"]}},{"tddate":null,"ddate":null,"tmdate":1509739487506,"tcdate":1508962646459,"number":95,"cdate":1509739484845,"id":"ryALZdAT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryALZdAT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Feature Incay for Representation Regularization","abstract":"Softmax-based loss is widely used in deep learning for multi-class classification, where each class is represented by a weight vector and each sample is represented as a feature vector. Different from traditional learning algorithms where features are pre-defined and only weight vectors are tunable through training, feature vectors are also tunable as representation learning in deep learning. Thus we investigate how to improve the classification performance by better adjusting the features. One main observation is that elongating the feature norm of both correctly-classified and mis-classified feature vectors improves learning: (1) increasing the feature norm of correctly-classified examples induce smaller training loss; (2) increasing the feature norm of mis-classified examples can upweight the contribution from hard examples. Accordingly, we propose feature incay to regularize representation learning by encouraging larger feature norm. In contrast to weight decay which shrinks the weight norm, feature incay is proposed to stretch the feature norm. Extensive empirical results on MNIST, CIFAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay. ","pdf":"/pdf/6ead4af37219f9a975eff84d1121172fca2ce2fa.pdf","paperhash":"anonymous|feature_incay_for_representation_regularization","_bibtex":"@article{\n  anonymous2018feature,\n  title={Feature Incay for Representation Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryALZdAT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper95/Authors"],"keywords":["feature norm","regularization","softmax loss","feature incay"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}