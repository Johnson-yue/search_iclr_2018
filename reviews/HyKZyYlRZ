{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222620505,"tcdate":1511815236768,"number":3,"cdate":1511815236768,"id":"rJ6r_eqlf","invitation":"ICLR.cc/2018/Conference/-/Paper324/Official_Review","forum":"HyKZyYlRZ","replyto":"HyKZyYlRZ","signatures":["ICLR.cc/2018/Conference/Paper324/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.\n\nThe paper is well written and easy to follow.\n\nThe contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.\n\n* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.\n\n* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).\n\n* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26â€“30.\nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.\nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Multi-Domain Multi-Task Learning with MultiModel","abstract":"Deep learning yields great results across many fields,\nfrom speech recognition, image classification, to translation.\nBut for each problem, getting a deep model to work well involves\nresearch into the architecture and a long period of tuning.\n\nWe present a single model that yields good results on a number\nof problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks,\nimage captioning (COCO dataset), a speech recognition corpus,\nand an English parsing task. \n\nOur model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism,\nand sparsely-gated layers.\n\nEach of these computational blocks is crucial for a subset of\nthe tasks we train on. Interestingly, even if a block is not\ncrucial for a task, we observe that adding it never hurts performance\nand in most cases improves it on all tasks.\n\nWe also show that tasks with less data benefit largely from joint\ntraining with other tasks, while performance on large tasks degrades\nonly slightly if at all.","pdf":"/pdf/b97225e2cf2f1bb831e53ce1b589bc2a6fa12ab2.pdf","TL;DR":"Large scale multi-task architecture solves ImageNet and translation together and shows transfer learning.","paperhash":"anonymous|large_scale_multidomain_multitask_learning_with_multimodel","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Multi-Domain Multi-Task Learning with MultiModel},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyKZyYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper324/Authors"],"keywords":["multi-task learning","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222620544,"tcdate":1511400104277,"number":2,"cdate":1511400104277,"id":"rke2Ms7ez","invitation":"ICLR.cc/2018/Conference/-/Paper324/Official_Review","forum":"HyKZyYlRZ","replyto":"HyKZyYlRZ","signatures":["ICLR.cc/2018/Conference/Paper324/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Multi-task learning does not hurt performance across different domains","rating":"6: Marginally above acceptance threshold","review":"The paper presents a multi-task architecture that can perform multiple tasks across multiple different domains. The authors design an architecture that works on image captioning, image classification, machine translation and parsing.\n\nThe proposed model can maintain performance of single-task models and in some cases show slight improvements. This is the main take-away from this paper. \n\nThere is a factually incorrect statement - depthwise separable convolutions were not introduced in Chollet 2016. Section 2 of the same paper also notes it (depthwise convolutions can be traced back to at least 2012).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Multi-Domain Multi-Task Learning with MultiModel","abstract":"Deep learning yields great results across many fields,\nfrom speech recognition, image classification, to translation.\nBut for each problem, getting a deep model to work well involves\nresearch into the architecture and a long period of tuning.\n\nWe present a single model that yields good results on a number\nof problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks,\nimage captioning (COCO dataset), a speech recognition corpus,\nand an English parsing task. \n\nOur model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism,\nand sparsely-gated layers.\n\nEach of these computational blocks is crucial for a subset of\nthe tasks we train on. Interestingly, even if a block is not\ncrucial for a task, we observe that adding it never hurts performance\nand in most cases improves it on all tasks.\n\nWe also show that tasks with less data benefit largely from joint\ntraining with other tasks, while performance on large tasks degrades\nonly slightly if at all.","pdf":"/pdf/b97225e2cf2f1bb831e53ce1b589bc2a6fa12ab2.pdf","TL;DR":"Large scale multi-task architecture solves ImageNet and translation together and shows transfer learning.","paperhash":"anonymous|large_scale_multidomain_multitask_learning_with_multimodel","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Multi-Domain Multi-Task Learning with MultiModel},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyKZyYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper324/Authors"],"keywords":["multi-task learning","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222620582,"tcdate":1511237131885,"number":1,"cdate":1511237131885,"id":"S1VMUmbeM","invitation":"ICLR.cc/2018/Conference/-/Paper324/Official_Review","forum":"HyKZyYlRZ","replyto":"HyKZyYlRZ","signatures":["ICLR.cc/2018/Conference/Paper324/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Deep architecture, shallow insights","rating":"3: Clear rejection","review":"The paper describes a neural end-to-end architecture to solve multiple tasks at once.  The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks.  The engineering endeavor is impressive, but the paper has little scientific value.  Below are a few suggestions to make the paper stronger.\n\nIt is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input.  One way to analyze whether this happens is to predict the identity of the task from the hidden vectors.  If this is the case, how to prevent it from happening?  If this does not happen, what is being shared across tasks?  This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space.\n\nWhy multitask learning help the model perform better is still unclear.  If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples.  The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning?\n\nIf the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal.  If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task?\n\nReporting per-token accuracies, such as those in Table 2, is problematic.  It's unclear how to compute per-token accuracies for structured prediction tasks, such as speech recognition, parsing, and translation.  Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task.  The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Multi-Domain Multi-Task Learning with MultiModel","abstract":"Deep learning yields great results across many fields,\nfrom speech recognition, image classification, to translation.\nBut for each problem, getting a deep model to work well involves\nresearch into the architecture and a long period of tuning.\n\nWe present a single model that yields good results on a number\nof problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks,\nimage captioning (COCO dataset), a speech recognition corpus,\nand an English parsing task. \n\nOur model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism,\nand sparsely-gated layers.\n\nEach of these computational blocks is crucial for a subset of\nthe tasks we train on. Interestingly, even if a block is not\ncrucial for a task, we observe that adding it never hurts performance\nand in most cases improves it on all tasks.\n\nWe also show that tasks with less data benefit largely from joint\ntraining with other tasks, while performance on large tasks degrades\nonly slightly if at all.","pdf":"/pdf/b97225e2cf2f1bb831e53ce1b589bc2a6fa12ab2.pdf","TL;DR":"Large scale multi-task architecture solves ImageNet and translation together and shows transfer learning.","paperhash":"anonymous|large_scale_multidomain_multitask_learning_with_multimodel","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Multi-Domain Multi-Task Learning with MultiModel},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyKZyYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper324/Authors"],"keywords":["multi-task learning","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739363653,"tcdate":1509097216927,"number":324,"cdate":1509739360999,"id":"HyKZyYlRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyKZyYlRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large Scale Multi-Domain Multi-Task Learning with MultiModel","abstract":"Deep learning yields great results across many fields,\nfrom speech recognition, image classification, to translation.\nBut for each problem, getting a deep model to work well involves\nresearch into the architecture and a long period of tuning.\n\nWe present a single model that yields good results on a number\nof problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks,\nimage captioning (COCO dataset), a speech recognition corpus,\nand an English parsing task. \n\nOur model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism,\nand sparsely-gated layers.\n\nEach of these computational blocks is crucial for a subset of\nthe tasks we train on. Interestingly, even if a block is not\ncrucial for a task, we observe that adding it never hurts performance\nand in most cases improves it on all tasks.\n\nWe also show that tasks with less data benefit largely from joint\ntraining with other tasks, while performance on large tasks degrades\nonly slightly if at all.","pdf":"/pdf/b97225e2cf2f1bb831e53ce1b589bc2a6fa12ab2.pdf","TL;DR":"Large scale multi-task architecture solves ImageNet and translation together and shows transfer learning.","paperhash":"anonymous|large_scale_multidomain_multitask_learning_with_multimodel","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Multi-Domain Multi-Task Learning with MultiModel},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyKZyYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper324/Authors"],"keywords":["multi-task learning","transfer learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}