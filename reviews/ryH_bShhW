{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222769034,"tcdate":1511845131298,"number":3,"cdate":1511845131298,"id":"BJQGTw5lM","invitation":"ICLR.cc/2018/Conference/-/Paper8/Official_Review","forum":"ryH_bShhW","replyto":"ryH_bShhW","signatures":["ICLR.cc/2018/Conference/Paper8/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clear reject","rating":"2: Strong rejection","review":"This manuscript explores the idea of adding noise to the adversary's play in GAN dynamics over an RKHS. This is equivalent to adding noise to the gradient update, using the duality of reproducing kernels. Unfortunately, the evaluation here is wholly unsatisfactory to justify the manuscript's claims. No concrete practical algorithm specification is given (only a couple of ideas to inject noise listed), only a qualitative one on a 2-dimensional latent space in MNIST, and an inconclusive one using the much-doubted Parzen window KDE method. The idea as stated in the abstract and introduction may well be worth pursuing, but not on the evidence provided by the rest of the manuscript.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER","abstract":"Any autoencoder network can be turned into a generative model by imposing an arbitrary prior distribution on its hidden code vector. Variational Autoencoder uses a KL divergence penalty to impose the prior, whereas Adversarial Autoencoder uses generative adversarial networks.  A straightforward modification of Adversarial Autoencoder can be achieved by replacing the adversarial network with maximum mean discrepancy (MMD) network. This replacement leads to a new set of probabilistic autoencoder which is also discussed in our paper.\n\nHowever, an essential challenge remains in both of these probabilistic autoencoders, namely that the only source of randomness at the output of encoder, is the training data itself.  Lack of enough stochasticity can make the optimization problem non-trivial. As a result, they can lead to degenerate solutions where the generator collapses into sampling only a few modes.\n\nOur proposal is to replace the adversary of the adversarial autoencoder by a space of {\\it stochastic} functions. This replacement introduces a a new source of randomness which can be considered as a continuous control for encouraging {\\it explorations}. This prevents the adversary from fitting too closely to the generator and therefore leads to more diverse set of generated samples. Consequently, the decoder serves as a better generative network which unlike MMD nets scales linearly with the amount of data. We provide mathematical and empirical evidence on how this replacement outperforms the pre-existing architectures.   ","pdf":"/pdf/13ec102384aceb61d2b5723b10040d38ce2d4952.pdf","paperhash":"anonymous|doubly_stochastic_adversarial_autoencoder","_bibtex":"@article{\n  anonymous2018doubly,\n  title={DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryH_bShhW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper8/Authors"],"keywords":["Generative adversarial Networks","Deep Generative models","Kernel Methods"]}},{"tddate":null,"ddate":null,"tmdate":1512222769072,"tcdate":1511829917529,"number":2,"cdate":1511829917529,"id":"B1BsWE9lM","invitation":"ICLR.cc/2018/Conference/-/Paper8/Official_Review","forum":"ryH_bShhW","replyto":"ryH_bShhW","signatures":["ICLR.cc/2018/Conference/Paper8/AnonReviewer2"],"readers":["everyone"],"content":{"title":"a straightforward extension of existing algorithms","rating":"3: Clear rejection","review":"\nIn this paper, the authors propose doubly stochastic adversarial autoencoder, which is essentially applying the doubly stochastic gradient for the variational form of maximum mean discrepancy. \n\nThe most severe issue is lacking novelty. It is a straightforward combination of existing work, therefore, the contribution of this work is rare. \n\nMoreover, some of the claims in the paper are not appropriate. For example, using random features to approximate the kernel function does not bring extra stochasticity. The random features are fixed once sampled from the base measure of the corresponding kernel. Basically, you can view the random feature approximation as a linear combination of fixed nonlinear basis which are sampled from some distribution. \n\nFinally, the experiments are promising. However, to be more convincing, more benchmarks, e.g., cifar10/100 and CelebA, are needed. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER","abstract":"Any autoencoder network can be turned into a generative model by imposing an arbitrary prior distribution on its hidden code vector. Variational Autoencoder uses a KL divergence penalty to impose the prior, whereas Adversarial Autoencoder uses generative adversarial networks.  A straightforward modification of Adversarial Autoencoder can be achieved by replacing the adversarial network with maximum mean discrepancy (MMD) network. This replacement leads to a new set of probabilistic autoencoder which is also discussed in our paper.\n\nHowever, an essential challenge remains in both of these probabilistic autoencoders, namely that the only source of randomness at the output of encoder, is the training data itself.  Lack of enough stochasticity can make the optimization problem non-trivial. As a result, they can lead to degenerate solutions where the generator collapses into sampling only a few modes.\n\nOur proposal is to replace the adversary of the adversarial autoencoder by a space of {\\it stochastic} functions. This replacement introduces a a new source of randomness which can be considered as a continuous control for encouraging {\\it explorations}. This prevents the adversary from fitting too closely to the generator and therefore leads to more diverse set of generated samples. Consequently, the decoder serves as a better generative network which unlike MMD nets scales linearly with the amount of data. We provide mathematical and empirical evidence on how this replacement outperforms the pre-existing architectures.   ","pdf":"/pdf/13ec102384aceb61d2b5723b10040d38ce2d4952.pdf","paperhash":"anonymous|doubly_stochastic_adversarial_autoencoder","_bibtex":"@article{\n  anonymous2018doubly,\n  title={DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryH_bShhW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper8/Authors"],"keywords":["Generative adversarial Networks","Deep Generative models","Kernel Methods"]}},{"tddate":null,"ddate":null,"tmdate":1512222769111,"tcdate":1511535675417,"number":1,"cdate":1511535675417,"id":"By7B42BxM","invitation":"ICLR.cc/2018/Conference/-/Paper8/Official_Review","forum":"ryH_bShhW","replyto":"ryH_bShhW","signatures":["ICLR.cc/2018/Conference/Paper8/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is not mature enough to be accepted","rating":"3: Clear rejection","review":"The paper describes a generative model that replaces the GAN loss in the adversarial auto-encoder with MMD loss. Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following:\n1. write down MMD as an integral probability metric (IPM)\n2. say the test function, which originally should be in an RKHS, will be approximated using random feature approximations.\n\nAlthough the authors explained the intuition a bit and showed some empirical results, I still don't see why this method should work better than directly minimising MMD. Also it is not preferred to look at the generated images and claim diversity, instead it's better to have some kind of quantitative metric such as the inception score.\n\nFinally, given the fact that we have too many GAN related papers now, I don't think the innovation contained in the paper (which is using random features) is good enough to be published at ICLR. Also the paper is not clearly written, and I would suggest better not to copy-past paragraphs in the abstract and intro.\n\nThat said, I would welcome for the authors feedback and see if I have misunderstood something.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER","abstract":"Any autoencoder network can be turned into a generative model by imposing an arbitrary prior distribution on its hidden code vector. Variational Autoencoder uses a KL divergence penalty to impose the prior, whereas Adversarial Autoencoder uses generative adversarial networks.  A straightforward modification of Adversarial Autoencoder can be achieved by replacing the adversarial network with maximum mean discrepancy (MMD) network. This replacement leads to a new set of probabilistic autoencoder which is also discussed in our paper.\n\nHowever, an essential challenge remains in both of these probabilistic autoencoders, namely that the only source of randomness at the output of encoder, is the training data itself.  Lack of enough stochasticity can make the optimization problem non-trivial. As a result, they can lead to degenerate solutions where the generator collapses into sampling only a few modes.\n\nOur proposal is to replace the adversary of the adversarial autoencoder by a space of {\\it stochastic} functions. This replacement introduces a a new source of randomness which can be considered as a continuous control for encouraging {\\it explorations}. This prevents the adversary from fitting too closely to the generator and therefore leads to more diverse set of generated samples. Consequently, the decoder serves as a better generative network which unlike MMD nets scales linearly with the amount of data. We provide mathematical and empirical evidence on how this replacement outperforms the pre-existing architectures.   ","pdf":"/pdf/13ec102384aceb61d2b5723b10040d38ce2d4952.pdf","paperhash":"anonymous|doubly_stochastic_adversarial_autoencoder","_bibtex":"@article{\n  anonymous2018doubly,\n  title={DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryH_bShhW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper8/Authors"],"keywords":["Generative adversarial Networks","Deep Generative models","Kernel Methods"]}},{"tddate":null,"ddate":null,"tmdate":1509739533692,"tcdate":1507770732947,"number":8,"cdate":1509739531032,"id":"ryH_bShhW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryH_bShhW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER","abstract":"Any autoencoder network can be turned into a generative model by imposing an arbitrary prior distribution on its hidden code vector. Variational Autoencoder uses a KL divergence penalty to impose the prior, whereas Adversarial Autoencoder uses generative adversarial networks.  A straightforward modification of Adversarial Autoencoder can be achieved by replacing the adversarial network with maximum mean discrepancy (MMD) network. This replacement leads to a new set of probabilistic autoencoder which is also discussed in our paper.\n\nHowever, an essential challenge remains in both of these probabilistic autoencoders, namely that the only source of randomness at the output of encoder, is the training data itself.  Lack of enough stochasticity can make the optimization problem non-trivial. As a result, they can lead to degenerate solutions where the generator collapses into sampling only a few modes.\n\nOur proposal is to replace the adversary of the adversarial autoencoder by a space of {\\it stochastic} functions. This replacement introduces a a new source of randomness which can be considered as a continuous control for encouraging {\\it explorations}. This prevents the adversary from fitting too closely to the generator and therefore leads to more diverse set of generated samples. Consequently, the decoder serves as a better generative network which unlike MMD nets scales linearly with the amount of data. We provide mathematical and empirical evidence on how this replacement outperforms the pre-existing architectures.   ","pdf":"/pdf/13ec102384aceb61d2b5723b10040d38ce2d4952.pdf","paperhash":"anonymous|doubly_stochastic_adversarial_autoencoder","_bibtex":"@article{\n  anonymous2018doubly,\n  title={DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryH_bShhW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper8/Authors"],"keywords":["Generative adversarial Networks","Deep Generative models","Kernel Methods"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}