{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222690300,"tcdate":1511841118772,"number":2,"cdate":1511841118772,"id":"rkDPp89xz","invitation":"ICLR.cc/2018/Conference/-/Paper555/Official_Review","forum":"HJMN-xWC-","replyto":"HJMN-xWC-","signatures":["ICLR.cc/2018/Conference/Paper555/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Learning Parsimonious Deep Feed-forward Networks","rating":"4: Ok but not good enough - rejection","review":"This paper introduces a skip-connection based design of fully connected networks, which is loosely based on learning latent variable tree structure learning via mutual information criteria. The goal is to learn sparse structures across layers of fully connected networks.  Compared to prior work (hierarchical latent tree model), this work introduces skip-paths. \nAuthors refer to prior work for methods to learn this backbone model. Liu et.al (http://www.cse.ust.hk/~lzhang/ltm/index.htm) and Chen et.al. (https://arxiv.org/abs/1508.00973) and (https://arxiv.org/pdf/1605.06650.pdf). \n\nAs far as I understand, the methods for learning backbone structure and the skip-path are performed independently, i.e. there is no end-to-end training of the structure and parameters of the layers. This will limit the applicability of the approach in most applications where fully connected networks are currently used. \n\nOriginality - The paper heavily builds upon prior work on hierarchical latent tree analysis and adds 'skip path' formulation to the architecture, however the structure learning is not performed end-to-end and in conjunction with the parameters. \n\nClarity - The paper is not self-contained in terms of methodology.\n\nQuality and Significance - There is a disconnect between premise of the paper (improving efficiency of fully connected layers by learning sparser structures) and applicability of the approach (slow EM based method to learn structure first, then learn the parameters).  As is, the applicability of the method is limited. \nAlso in terms of experiments, there is not enough exploration of simpler sparse learning methods such as heavy regularization of the weights. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Parsimonious Deep Feed-forward Networks","abstract":"Convolutional neural networks and recurrent neural networks are designed with network structures well suited to the nature of spacial and sequential data respectively. However, the structure of standard feed-forward neural networks (FNNs) is simply a stack of fully connected layers, regardless of the feature correlations in data. In addition, the number of layers and the number of neurons are manually tuned on validation data, which is time-consuming and may lead to suboptimal networks. In this paper, we propose an unsupervised structure learning method for learning parsimonious deep FNNs. Our method determines the number of layers, the number of neurons at each layer, and the sparse connectivity between adjacent layers automatically from data. The resulting models are called Backbone-Skippath Neural Networks (BSNNs). Experiments on 17 tasks show that, in comparison with FNNs,  BSNNs can achieve better or comparable classification performance with much fewer parameters. The interpretability of BSNNs is also shown to be better than that of FNNs.","pdf":"/pdf/79f3d2b55c75315503bf5adf2928823311b396c4.pdf","TL;DR":"An unsupervised structure learning method for Parsimonious Deep Feed-forward Networks.","paperhash":"anonymous|learning_parsimonious_deep_feedforward_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Parsimonious Deep Feed-forward Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMN-xWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper555/Authors"],"keywords":["Parsimonious Deep Feed-forward Networks","structure learning","classification","overfitting","fewer parameters","high interpretability"]}},{"tddate":null,"ddate":null,"tmdate":1512222690343,"tcdate":1511718762737,"number":1,"cdate":1511718762737,"id":"HkQdJFuef","invitation":"ICLR.cc/2018/Conference/-/Paper555/Official_Review","forum":"HJMN-xWC-","replyto":"HJMN-xWC-","signatures":["ICLR.cc/2018/Conference/Paper555/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Learning Parsimonious Deep Feed-forward Networks","rating":"5: Marginally below acceptance threshold","review":"There is a vast literature on structure learning for constructing neural networks (topologies, layers, learning rates, etc.) in an automatic fashion. Your work falls under a similar category. I am a bit surprised that you have not discussed it in the paper not to mention provided a baseline to compare your method to. Also, without knowing intricate details about each of 17 tasks you mentioned it is really hard to make any judgement as to how significant is improvement coming from your approach. There has been some work done on constructing interpretable neural networks, such as stimulated training in speech recognition, unfortunately these are not discussed in the paper despite interpretability being considered important in this paper. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Parsimonious Deep Feed-forward Networks","abstract":"Convolutional neural networks and recurrent neural networks are designed with network structures well suited to the nature of spacial and sequential data respectively. However, the structure of standard feed-forward neural networks (FNNs) is simply a stack of fully connected layers, regardless of the feature correlations in data. In addition, the number of layers and the number of neurons are manually tuned on validation data, which is time-consuming and may lead to suboptimal networks. In this paper, we propose an unsupervised structure learning method for learning parsimonious deep FNNs. Our method determines the number of layers, the number of neurons at each layer, and the sparse connectivity between adjacent layers automatically from data. The resulting models are called Backbone-Skippath Neural Networks (BSNNs). Experiments on 17 tasks show that, in comparison with FNNs,  BSNNs can achieve better or comparable classification performance with much fewer parameters. The interpretability of BSNNs is also shown to be better than that of FNNs.","pdf":"/pdf/79f3d2b55c75315503bf5adf2928823311b396c4.pdf","TL;DR":"An unsupervised structure learning method for Parsimonious Deep Feed-forward Networks.","paperhash":"anonymous|learning_parsimonious_deep_feedforward_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Parsimonious Deep Feed-forward Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMN-xWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper555/Authors"],"keywords":["Parsimonious Deep Feed-forward Networks","structure learning","classification","overfitting","fewer parameters","high interpretability"]}},{"tddate":null,"ddate":null,"tmdate":1509739238512,"tcdate":1509126441657,"number":555,"cdate":1509739235848,"id":"HJMN-xWC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJMN-xWC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Parsimonious Deep Feed-forward Networks","abstract":"Convolutional neural networks and recurrent neural networks are designed with network structures well suited to the nature of spacial and sequential data respectively. However, the structure of standard feed-forward neural networks (FNNs) is simply a stack of fully connected layers, regardless of the feature correlations in data. In addition, the number of layers and the number of neurons are manually tuned on validation data, which is time-consuming and may lead to suboptimal networks. In this paper, we propose an unsupervised structure learning method for learning parsimonious deep FNNs. Our method determines the number of layers, the number of neurons at each layer, and the sparse connectivity between adjacent layers automatically from data. The resulting models are called Backbone-Skippath Neural Networks (BSNNs). Experiments on 17 tasks show that, in comparison with FNNs,  BSNNs can achieve better or comparable classification performance with much fewer parameters. The interpretability of BSNNs is also shown to be better than that of FNNs.","pdf":"/pdf/79f3d2b55c75315503bf5adf2928823311b396c4.pdf","TL;DR":"An unsupervised structure learning method for Parsimonious Deep Feed-forward Networks.","paperhash":"anonymous|learning_parsimonious_deep_feedforward_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Parsimonious Deep Feed-forward Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJMN-xWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper555/Authors"],"keywords":["Parsimonious Deep Feed-forward Networks","structure learning","classification","overfitting","fewer parameters","high interpretability"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}