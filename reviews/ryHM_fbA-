{"notes":[{"tddate":null,"ddate":null,"tmdate":1511877137086,"tcdate":1511877137086,"number":3,"cdate":1511877137086,"id":"r1tz5yoeM","invitation":"ICLR.cc/2018/Conference/-/Paper867/Public_Comment","forum":"ryHM_fbA-","replyto":"B1siqc_yz","signatures":["~Eloise_Huang1"],"readers":["everyone"],"writers":["~Eloise_Huang1"],"content":{"title":"Detail of hidden layer in classifier","comment":"\"The classifier is a feed forward neural network with a single hidden layer and a tanh activation function.\" What kind of hidden layer?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222798217,"tcdate":1511837443963,"number":3,"cdate":1511837443963,"id":"B1KZkIqxG","invitation":"ICLR.cc/2018/Conference/-/Paper867/Official_Review","forum":"ryHM_fbA-","replyto":"ryHM_fbA-","signatures":["ICLR.cc/2018/Conference/Paper867/AnonReviewer1"],"readers":["everyone"],"content":{"title":"No comparison against recent SOTA in text representation","rating":"2: Strong rejection","review":"This paper proposes using CNNs with a skip-gram like objective as a fast way to output document embeddings and much faster compared to skip-thought and RNN type models.\n\nWhile the problem is an important one, the paper only compares speed with the RNN-type model and doesn't make any inference speed comparison with paragraph vectors (the main competing baseline in the paper). Paragraph vectors are also parallelizable so it's not obvious that this method would be superior to it. The paper in the introduction also states that doc2vec is trained using localized contexts (5 to 10 words) and never sees the whole document. If this was the case then paragraph vectors wouldn't work when representing a whole document, which it already does as can be seen in table 2.\n\nThe paper also fails to compare with the significant amount of existing literature on state of the art document embeddings. Many of these are likely to be faster than the method described in the paper. For example:\n\n\nArora, S., Liang, Y., & Ma, T. A simple but tough-to-beat baseline for sentence embeddings. ICLR 2017.\nChen, M. Efficient vector representation for documents through corruption. ICLR 2017.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222798254,"tcdate":1511801693472,"number":2,"cdate":1511801693472,"id":"BkBvQaFez","invitation":"ICLR.cc/2018/Conference/-/Paper867/Official_Review","forum":"ryHM_fbA-","replyto":"ryHM_fbA-","signatures":["ICLR.cc/2018/Conference/Paper867/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An okay paper that fails to document its contribution","rating":"4: Ok but not good enough - rejection","review":"This paper uses CNNs to build document embeddings.  The main advantage over other methods is that CNNs are very fast.\n\nFirst and foremost I think this: \"The code with the full model architecture will be released â€¦ and we thus omit going into further details here.\"  is not acceptable.  Releasing code is commendable, but it is not a substitute for actually explaining what you have done.  This is especially true when the main contribution of the work is a network architecture.  If you're going to propose a specific architecture I expect you to actually tell me what it is.\n\nI'm a bit confused by section 3.1 on language modelling.  I think the claim that it is showing \"a direct connection to language modelling\" and that \"we explore this relationship in detail\" are both very much overstated.  I think it would be more accurate to say this paper takes some tricks that people have used for language modelling and applies them to learning document embeddings.\n\nThis paper proposed both a model and a training objective, and I would have liked to see some attempt to disentangle their effect.  If there is indeed a direct connection between embedding models and language models then I would have also expected to see some feedback effect from document embedding to language modeling.  Does the embedding objective proposed here also lead to better language models?\n\nOverall I do not see a substantial contribution from this paper. The main claims seem to be that CNNs are fast, and can be used for NLP, neither of which are new.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222798294,"tcdate":1511633930711,"number":1,"cdate":1511633930711,"id":"HJmMNVDlz","invitation":"ICLR.cc/2018/Conference/-/Paper867/Official_Review","forum":"ryHM_fbA-","replyto":"ryHM_fbA-","signatures":["ICLR.cc/2018/Conference/Paper867/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review of \"Learning Document Embeddings With CNNs\"","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new model for the general task of inducing document representations (embeddings). The approach uses a CNN architecture, distinguishing it from the majority of prior efforts on this problem, which have tended to use RNNs. This affords obvious computational advantages, as training may be parallelized. \n\nOverall, the model presented is relatively simple (a good thing, in my view) and it indeed seems fast. I can thus see potential practical uses of this CNN based approach to document embedding in future work on language tasks. The training strategy, which entails selecting documents and then indexes within them stochastically, is also neat. Furthermore, the work is presented relatively clearly. That said, my main concerns regarding this paper are that: (1) there's not much new here, and, (2) the experimental setup may be flawed, in that it would seem model hyperparams were tuned for the proposed approach but not for the baselines; I elaborate on these concerns below.\n\nSpecific comments:\n---\n- It's hard to tease out exactly what's new here: the various elements used are all well known. But perhaps there is merit in putting the specific pieces together. Essentially, the novelty is using a CNN rather than an RNN to induce document embeddings. \n\n- In Section 4.1, the authors write that they report results for their after running \"parameter sweeps ...\" -- I presume that these were performed on a validation set, but the authors should say so. In any case, a very potential weakness here: were analagous parameter sweeps for this dataset performed for the baseline models? It would seem not, as the authors write \"the IMDB training data using the default hyper-parameters\" for skip-thought. Surely it is unfair comparison if one model has been tuned to a given dataset while others use only the default hyper-parameters? \n\n- Many important questions were left unaddressed in the experiments. For example, does one really need to use the gating mechanism borrowed from the Dauphin et al. paper? What happens if not? How big of an effect does the stochastic sampling of document indices have on the learned embeddings? Does the specific underlying CNN architecture affect results, and how much? None of these questions are explored. \n\n- I was left a bit confused regarding how the v_{1:i-1} embedding is actually estimated; I think the details here are insufficient in the current presentation. The authors write that this is a \"function of all words up to w_{i-1}\". This would seem to imply that at test time, prediction is not in fact parallelizable, no? Yet this seems to be one of the main arguments the authors make in favor of the model (in contrast to RNN based methods). In fact, I think the authors are proposing using the (aggregated) filter activation vectors (h^l(x)) in eq. 5, but for some reason this is not made explicit. \n\nMinor comments:\n\n- In Eq. 4, should the product be element-wise to realize the desired gating (as per the Dauhpin paper)? This should be made explicit in the notation.\n\n- On the bottom of page 3, the authors claim \"Expanding the prediction to multiple words makes the problem more difficult since the only way to achieve that is by 'understanding' the preceding sequence.\" This claim should either by made more precise or removed. It is not clear exactly what is meant here, nor what evidence supports it.\n\n- Commas are missing in a few. For example on page 2, probably want a comma after \"in parallel\" (before \"significantly\"); also after \"parallelize\" above \"Approach\".\n\n- Page 4: \"In contrast, our model addresses only requires\" --> drop the \"addresses\". ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1511476640542,"tcdate":1511476640542,"number":2,"cdate":1511476640542,"id":"SJOi664ez","invitation":"ICLR.cc/2018/Conference/-/Paper867/Public_Comment","forum":"ryHM_fbA-","replyto":"ryHM_fbA-","signatures":["~Eloise_Huang1"],"readers":["everyone"],"writers":["~Eloise_Huang1"],"content":{"title":"When and where will the code be released?","comment":"So I'm just wondering when and where will the code be released?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1510677155166,"tcdate":1510677155166,"number":1,"cdate":1510677155166,"id":"B1siqc_yz","invitation":"ICLR.cc/2018/Conference/-/Paper867/Official_Comment","forum":"ryHM_fbA-","replyto":"ry21MYLJG","signatures":["ICLR.cc/2018/Conference/Paper867/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper867/Authors"],"content":{"title":"Model details","comment":"Hi Marc,\n\nThank you for taking the interest in our work. Below are some further details of our CNN model and the classifier, let us know if you have further questions. We are currently working on cleaning up and refactoring the code and aim to release it in the next few weeks. \n\nThe classifier is a feed forward neural network with a single hidden layer and a tanh activation function. We train the classifier for 500 epochs, with a batch size of 100 and a momentum optimizer, with a learning rate of 0.0008 and momentum value of 0.9. We compute the test classification accuracy after every epoch and take the highest attained value for each model.\n\nFor the CNN model we use dropout of 0.8 (prob to keep), 300-900 kernels in each convolutional layer, gating activation function and residual connections every other layer [see Dauphin et al ICML 2017 for analogous architecture]. Words are represented using a pre-trained word2vec model with 300 dimensions and we update word vectors together with CNN during training. We use mini-batches of size 100 and predict 10 words forward for each example in the mini-batch using 50 negative samples to balance the classification objective. All CNN models use Adam optimizer with a learning rate of 0.0003."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1510539748125,"tcdate":1510539748125,"number":1,"cdate":1510539748125,"id":"ry21MYLJG","invitation":"ICLR.cc/2018/Conference/-/Paper867/Public_Comment","forum":"ryHM_fbA-","replyto":"ryHM_fbA-","signatures":["~Marc_Jin1"],"readers":["everyone"],"writers":["~Marc_Jin1"],"content":{"title":"Reveal the details of this paper","comment":"Our team is currently considering reproducing your paper. However the details of this paper, which are vital for our reproduction, appear to be vague. For example, which \"shallow classifier\" do you use? Just wondering when you will reveal the details or the code.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]}},{"tddate":null,"ddate":null,"tmdate":1509739059769,"tcdate":1509136397193,"number":867,"cdate":1509739057056,"id":"ryHM_fbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryHM_fbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Document Embeddings With CNNs","abstract":"This paper proposes a new model for document embedding. Existing approaches either require complex inference or use recurrent neural networks that are difficult to parallelize. We take a different route and use recent advances in language modeling to develop a convolutional neural network embedding model. This allows us to train deeper architectures that are fully parallelizable. Stacking layers together increases the receptive filed allowing each successive layer to model increasingly longer range semantic dependences within the document. Empirically we demonstrate superior results on two publicly available benchmarks. Full code will be released with the final version of this paper.","pdf":"/pdf/7dde4526398d464473538e0cfaaf34af06538728.pdf","TL;DR":"Convolutional neural network model for unsupervised document embedding.","paperhash":"anonymous|learning_document_embeddings_with_cnns","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Document Embeddings With CNNs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryHM_fbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper867/Authors"],"keywords":["unsupervised embedding","convolutional neural network"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}