{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222624247,"tcdate":1512088629929,"number":3,"cdate":1512088629929,"id":"B104VQCgM","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Review","forum":"Sk9yuql0Z","replyto":"Sk9yuql0Z","signatures":["ICLR.cc/2018/Conference/Paper352/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple idea but seems work in well in some cases","rating":"6: Marginally above acceptance threshold","review":"The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable.\n\nSome of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results.\n\n\nTypically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images?\n\nThe proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes.\n\nSection 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. \n\nIn section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model?\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222624290,"tcdate":1512080143035,"number":2,"cdate":1512080143035,"id":"ByRWmWAxM","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Review","forum":"Sk9yuql0Z","replyto":"Sk9yuql0Z","signatures":["ICLR.cc/2018/Conference/Paper352/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple new baseline (/additional evaluation technique) for defenses against adversarial attacks.","rating":"7: Good paper, accept","review":"This paper proposes an extremely simple methodology to improve the network's performance by adding extra random perturbations (resizing/padding) at evaluation time.\n\nAlthough the paper is very basic, it creates a good baseline for defending about various types of attacks and got good results in kaggle competition.\n\nThe main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent.\n\nCons of the paper: there is not much novel insight or really exciting new ideas presented.\n\nPros: It gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses, since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree. So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222624326,"tcdate":1511763647303,"number":1,"cdate":1511763647303,"id":"BJDaCQFxM","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Review","forum":"Sk9yuql0Z","replyto":"Sk9yuql0Z","signatures":["ICLR.cc/2018/Conference/Paper352/AnonReviewer1"],"readers":["everyone"],"content":{"title":"unclear effects of randomization","rating":"6: Marginally above acceptance threshold","review":"The authors propose a simple defense against adversarial attacks, which is to add randomization in the input of the CNNs. They experiment with different CNNs and published adversarial training techniques and show that randomized inputs mitigate adversarial attacks. \n\nPros:\n(+) The idea introduced is simple and flexible to be used for any CNN architecture\n(+) Experiments on ImageNet1k prove demonstrate its effectiveness\nCons:\n(-) Experiments are not thorougly explained\n(-) Novelty is extremely limited\n(-) Some baselines missing\n\n\nThe experimental section of the paper was rather confusing. The authors should explain the experiments and the settings in the table, as those are not very clear. In particular, it was not clear whether the defense model was trained with the input randomization layers? Also, in Tables 1-6, how was the target model trained? How do the training procedures of target vs. defense model differ? In those tables, what is the testing procedure for the target model and how does it compare to the defense model? \n\nThe gap between the target and defense model in Table 4 (ensemble pattern attack scenario) shrinks for single step attack methods. This means that when the attacker is aware of the randomization parameters, the effect of randomization might diminish. A baseline that reports the performance when the attacker is fully aware of the randomization of the defender (parameters, patterns etc.) is missing but is very useful.\n\nWhile the experiments show that the randomization layers mitigate the effect of randomization attacks, it's not clear whether the effectiveness of this very simple approach is heavily biased towards the published ways of generating adversarial attacks and the particular problem (i.e. classification). The form of attacks studied in the paper is that of additive noise. But there is many types of attacks that could be closely related to the randomization procedure of the input and that could lead to very different results.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510549872786,"tcdate":1510549195392,"number":3,"cdate":1510549195392,"id":"rJ70Io8kf","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Comment","forum":"Sk9yuql0Z","replyto":"rJXvqfIJG","signatures":["ICLR.cc/2018/Conference/Paper352/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper352/Authors"],"content":{"title":"Re: evaluations are not convincing ","comment":"Thanks for your comments.\n\nFirst of all, we would like to highlight two important things in our work. 1). This work is done on large-scale datasets like ImageNet, and only a few defenses (including adversarial training) have demonstrated the effectiveness before. Though MNIST is an interesting dataset on which to test defense ideas, the conclusions may not be readily applied to ImageNet. 2). The attack scenarios considered in our paper are much stronger than black-box attacks. I.e., the network structures and parameters are completely known by the attackers.\n\nIn the paper, we demonstrate the effectiveness of our method on basic C&W attacks, which are very challenging already, and were not well studied on ImageNet before. In order to overcome the problem that randomization models are not differentiable, we considered single-pattern attacks and ensemble-pattern attacks in the experiments. The experimental results indicate that adversarial examples generated under ensemble-pattern attacks are stronger than others. Note that the C&W attacks are very slow. Take the basic C&W attack against inception-resnet-v2 for example. It takes ~17 mins to generate adversarial examples for a batch of 30 images under vanilla attack scenario, and takes ~8 mins to generate adversarial examples for 1 image under ensemble-pattern attack scenario. Generating higher-confidence adversarial examples will significantly increase the time consumption even further, and thus, may not be practical. So we focus on basic C&W attacks in our experiments at current stage. \n\nFor the baseline included in this paper, we want to point out three things. 1). To the best of our knowledge, adversarial training is the most effectiveness method on large-scale dataset like ImageNet. We are confused by the words “the state-of-the-art defense”, please refer to it explicitly. 2). The adversarially trained model is not robust to iterative attacks, and is used more like a network backbone rather than baseline in our paper. We combine the adversarially trained model, which is robust to single-step attacks, and randomization layers, which improve network robustness to iterative attacks, together to form our best defense model. 3). There are 100+ defense teams and 150+ attacks teams participate in this public adversarial defense challenge, and our model is ranked top 2. We argue that this challenge provides us sufficient baselines (including very strong ones) to compare with, which convincingly demonstrates the effectiveness of our method in real world scenario.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"ddate":null,"tddate":1510513272345,"tmdate":1510513855705,"tcdate":1510513243277,"number":2,"cdate":1510513243277,"id":"rJXvqfIJG","invitation":"ICLR.cc/2018/Conference/-/Paper352/Public_Comment","forum":"Sk9yuql0Z","replyto":"Sy3pQrByM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"evaluations are not convincing","comment":"I still think it is problematic if you do not evaluate high-confidence transferable adversarial examples generated by C&W attacks. Since you use randomization, the model is no longer differentiable. Therefore, high-confidence transferable adversarial examples should be used to attack the defense. If such adversarial examples are not evaluated, the experimental results may be misleading. You can take a look at this paper:\n\nAdversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.\n\nThe reference [1] you pointed out for detecting adversarial examples is actually broken.\n\nAlso, it is useful to compare with state-of-the-art defense instead of adversarial training alone, because adversarial training is known to be not robust for state-of-the-art attacks.   \n  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510500592536,"tcdate":1510458884052,"number":2,"cdate":1510458884052,"id":"r13-LSB1M","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Comment","forum":"Sk9yuql0Z","replyto":"Sk9yuql0Z","signatures":["ICLR.cc/2018/Conference/Paper352/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper352/Authors"],"content":{"title":"update on public challenge evaluation results","comment":"Our submission ranked Top 2 (among 100+ teams) at the final round of a public adversarial defense challenge, where the number of test images is increased to 5000, and the number of different attack methods is increase to 150+. It reached a normalized score of 0.924, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56).  \n\nWe will reveal the URL of the challenge once the revision period is over."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510500678941,"tcdate":1510458308300,"number":1,"cdate":1510458308300,"id":"Sy3pQrByM","invitation":"ICLR.cc/2018/Conference/-/Paper352/Official_Comment","forum":"Sk9yuql0Z","replyto":"rJkSBVByG","signatures":["ICLR.cc/2018/Conference/Paper352/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper352/Authors"],"content":{"title":"Re: evaluations are not convincing ","comment":"Thanks for your comments.\n\n(1) C&W attacks is a strong attack, and we follow other papers, e.g., [1], to evaluate basic C&W attacks at current stage. Furthermore, the attacks scenarios considered here are much stronger than black-box attack, while other papers have not studied these before. We will conduct experiments to see how our defense model performs under vanilla attack, single-pattern attack and ensemble attack when confidence increases.\n\nWe want to highlight our defense is evaluated on large-scale real image dataset, e.g., ImageNet, which is much harder than defense on small dataset, like MNIST and CIFAR. Meanwhile, the conclusions on small dataset may not be valid on large dataset. For example,  adversarial training helps model get better performance on MNIST, but causes performance to drop on ImageNet (see table 1 at [2])\n\n(2) To the best of my knowledge, there are no randomization-based defense methods available on ImageNet (except some concurrent submissions at ICLR). If you know such reference on ImageNet, please send it to us.\n\n(3) We are not aware of such defense on ImageNet. If you know such reference on ImageNet, please send it to us. Meanwhile, the performance drop on clean images (see table 1) of our best defense model, ens-adv-Inception-ResNet-v2, is only from 100% to 99.2%, which is an acceptable degradation.  \n \n[1] Feinman, Reuben, et al. \"Detecting Adversarial Samples from Artifacts.\" arXiv preprint arXiv:1703.00410 (2017).\n[2] Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. \"Adversarial machine learning at scale.\" arXiv preprint arXiv:1611.01236 (2016)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510454583493,"tcdate":1510454583493,"number":1,"cdate":1510454583493,"id":"rJkSBVByG","invitation":"ICLR.cc/2018/Conference/-/Paper352/Public_Comment","forum":"Sk9yuql0Z","replyto":"Sk9yuql0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"evaluations are not convincing","comment":"Mitigating adversarial manipulations to deep neural networks via randomization is a promising direction. However, I found evaluations in this paper are not convincing.\n\n1. High-confidence transferable adversarial examples generated by C&W attacks are not evaluated. The paper only evaluated basic C&W attacks.\n\n2. The paper did not compare with recent randomization-based defense. For instance, the paper did not compare with the following paper \"Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification\", arxiv 2017. \n\n3. The method proposed in this paper decreases classification accuracy for normal examples in order to increase robustness against adversarial examples. However, there already exists defense that does not decrease classification accuracy for normal examples, but has the same or even better robustness than the proposed method. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739348982,"tcdate":1509103586174,"number":352,"cdate":1509739346320,"id":"Sk9yuql0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk9yuql0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Mitigating Adversarial Effects Through Randomization","abstract":"Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it get a normalized score of 0.921 in a public challenge on defending against adversarial examples, which is far better than using adversarial training alone with a normalized score of 0.830.","pdf":"/pdf/1e70fb7da1dcb384616dbcd4fe06e31f8cfeed88.pdf","paperhash":"anonymous|mitigating_adversarial_effects_through_randomization","_bibtex":"@article{\n  anonymous2018mitigating,\n  title={Mitigating Adversarial Effects Through Randomization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk9yuql0Z}\n}","keywords":["adversarial examples"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper352/Authors"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}