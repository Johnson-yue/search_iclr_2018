{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222763638,"tcdate":1511844931015,"number":3,"cdate":1511844931015,"id":"B1ornw9xG","invitation":"ICLR.cc/2018/Conference/-/Paper778/Official_Review","forum":"BJJ9bz-0-","replyto":"BJJ9bz-0-","signatures":["ICLR.cc/2018/Conference/Paper778/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Unclear derivations; Novelty is not obvious","rating":"5: Marginally below acceptance threshold","review":"SUMMARY:\n\nThe motivation for this work is to have an RL algorithm that can use imperfect demonstrations to accelerate learning. The paper proposes an actor-critic algorithm, called Normalized Actor-Critic (NAC), based on the entropy-regularized formulation of RL, which is defined by adding the entropy of the policy as an additional term in the reward function.\nEntropy-regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017].\nThe paper benefits from such a relationship and derives an actor-critic algorithm. Specifically, the paper only parametrizes the Q function, and computes the policy gradient using the relation between the policy and Q function (Appendix A.1).\n\nThrough a set of experiments, the paper shows the effectiveness of the method.\n\n\nEVALUATION:\n\nI think exploring and understanding entropy-regularized RL algorithm is important. It is also important to be able to benefit from off-policy data. I also find the empirical results encouraging. But I have some concerns about this paper:\n\n- The derivations of the paper are unclear.\n- The relation with other recent work in entropy-regularized RL should be expanded.\n- The work is less about benefiting from demonstration data and more about using off-policy data.\n- The algorithm that performs well is not the one that was actually derived.\n\n* Unclear derivations:\nThe derivations of Appendix A.1 is unclear. It makes it difficult to verify the derivations.\n\nTo begin with, what is the loss function of which (9) and (10) are its gradients?\n\nTo be more specific, the choices of \\hat{Q} in (15) and \\hat{V} in (19) are not clear.  For example, just after (18) it is said that “\\hat{Q} could be obtained through bootstrapping by R + gamma V_Q”. But if it is the case, shouldn’t we have a gradient of Q in (15) too? (or show that it can be ignored?)\n\nIt appears that \\hat{Q} and \\hat{V} are parameterized independently from Q (which is a function of theta). Later in the paper they are estimated using a target network, but this is not specified in the derivations.\n\nThe main problem boils down to the fact that the paper does not start from a loss function and compute all the gradients in a systematic way. Instead it starts from gradient terms, each of which seems to be from different papers, and then simplifies them. For example, the policy gradient in (8), which is further decomposed in Appendix A.1 as (15) and (16) and simplified, appears to be Eq. (50) of [Schulman et al., 2017] (https://arxiv.org/abs/1704.06440). In that paper we have Q_pi instead of \\hat{Q} though.\n\nI suggest that the authors start from a loss function and clearly derive all necessary steps.\n\n\n* Unclear relation with other papers:\nWhat part of the derivations of this work are novel? Currently the novelty is not obvious.\nFor example, having the gradient of both Q and V, as in (9), has been stated by [Haarnoja et al., 2017] (very similar formulation is developed in Appendix B of https://arxiv.org/abs/1702.08165).\nAn algorithm that can work with off-policy data has also been developed by [Nachum, 2017] (in the form of a Bellman residual minimization algorithm, as opposed to this work which essentially uses a Fitted Q-Iteration algorithm as the critic).\n\nI think the paper could do a better job differentiating from those other papers.\n\n\n* The claim that this paper is about learning from demonstration is a bit questionable. The paper essentially introduces a method to use off-policy data, which is of course important, but does not cover the important scenario where we only have access to (state,action) pairs given by an expert. Here it appears from the description of Algorithm 1 that the transitions in the demonstration data have the same semantic as the interaction data, i.e., (s,a,r,s’).\nThis makes it different from the work by [Kim et al., 2013], [Piot et al., 2014], and [Chemali et al., 2015], which do not require such a restriction on the demonstration data.\n\n\n* The paper mentions that to formalize the method as a policy gradient one, importance sampling should be used (the paragraph after (12)), but the performance of such a formulation is bad, as depicted in Figure 2. As a result, Algorithm 1 does not use importance sampling.\nThis basically suggests that by ignoring the fact that the data is collected off-policy, and treating it as an on-policy data, the agent might perform better. This is an interesting phenomenon and deservers further study, as currently doing the “wrong” things is better than doing the “right” thing. I think a good paper should investigate this fact more.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning from Imperfect Demonstrations","abstract":"Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.","pdf":"/pdf/87cf8126e00d80481ecb6de10dfafa4f387d15b5.pdf","paperhash":"anonymous|reinforcement_learning_from_imperfect_demonstrations","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning from Imperfect Demonstrations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJ9bz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper778/Authors"],"keywords":["learning from demonstration","reinforcement learning","maximum entropy learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222763680,"tcdate":1511714987854,"number":2,"cdate":1511714987854,"id":"ryE3gOulz","invitation":"ICLR.cc/2018/Conference/-/Paper778/Official_Review","forum":"BJJ9bz-0-","replyto":"BJJ9bz-0-","signatures":["ICLR.cc/2018/Conference/Paper778/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Similar to previous work - Experimental result not so convincing","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations. The method is inspired by the recent work on max entropy reinforcement learning and links between Q-learning and policy gradient methods. Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers). \n\nI'm also not sur to see much differences with the previous work by Haarnoja et al and Schulman et al. It uses demonstrations to learn in an off-policy manner as in these papers. Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too. \n\nThe authors say they compare to DQfD but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit (L2 regularization is also used). It seems this has not been implemented for comparison and that overfitting may come from this method missing. \n\nI'm also uncomfortable with the way most of the expert data are generated for experiments. Using data generated by a pre-trained network is usually not representative of what will happen in real life. Also, corrupting actions with noise in the replay buffer is not simulating correctly what would happen in reality. Indeed, a single error in some given state will often generate totally different trajectories and not affect a single transition. So imperfect demonstration have very typical distributions. I acknowledge that some real human demonstrations are used but there is not much about them and the experiment is very shortly described. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning from Imperfect Demonstrations","abstract":"Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.","pdf":"/pdf/87cf8126e00d80481ecb6de10dfafa4f387d15b5.pdf","paperhash":"anonymous|reinforcement_learning_from_imperfect_demonstrations","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning from Imperfect Demonstrations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJ9bz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper778/Authors"],"keywords":["learning from demonstration","reinforcement learning","maximum entropy learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222763722,"tcdate":1511536787694,"number":1,"cdate":1511536787694,"id":"rynqOnBez","invitation":"ICLR.cc/2018/Conference/-/Paper778/Official_Review","forum":"BJJ9bz-0-","replyto":"BJJ9bz-0-","signatures":["ICLR.cc/2018/Conference/Paper778/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New approach or new application?","rating":"4: Ok but not good enough - rejection","review":"My problem with this paper that all the theoretical contributions / the new approach refer to 2 arXiv papers, what's then left is an application of that approach to learning form imperfect demonstrations.\n\nQuality\n======\nThe approach seems sound but the paper does not provide many details on the underlying approach. The application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy samples. The experiments are OK but I would have liked a more thorough analysis.\n\nClarity\n=====\nThe paper reads well, but it is not really clear what the claimed contribution is.\n\nOriginality\n=========\nThe application seems original.\n\nSignificance\n==========\nHaving an RL approach that can benefit from truly off-policy samples is highly relevant.\n\nPros and Cons\n============\n+ good results\n+ interesting idea of using the algorithm for RLfD\n- weak experiments for an application paper\n- not clear what's new","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning from Imperfect Demonstrations","abstract":"Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.","pdf":"/pdf/87cf8126e00d80481ecb6de10dfafa4f387d15b5.pdf","paperhash":"anonymous|reinforcement_learning_from_imperfect_demonstrations","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning from Imperfect Demonstrations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJ9bz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper778/Authors"],"keywords":["learning from demonstration","reinforcement learning","maximum entropy learning"]}},{"tddate":null,"ddate":null,"tmdate":1511540275079,"tcdate":1511535019147,"number":1,"cdate":1511535019147,"id":"Hym3W3ref","invitation":"ICLR.cc/2018/Conference/-/Paper778/Official_Comment","forum":"BJJ9bz-0-","replyto":"BJJ9bz-0-","signatures":["ICLR.cc/2018/Conference/Paper778/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper778/AnonReviewer2"],"content":{"title":"So what's new?","comment":"The paper proposes to employ an algorithm from Haarnoja 2017 and Schulman 2017 for reinforcement learning where some of the data comes from potentially adversarial user demonstrations.\nThe paper does not seem to be able to make up its mind whether the approach is novel or not. \"We propose a unified LfD approach\" but then all the algorithmic details are \"as shown by Haarnoja / Schulman 2017\". It gets a bit messy with arXiv papers (in this case they don't seem to have appeared yet in a peer-reviewed venue) but the authors are treating these two papers as published, so I'll also treat this paper as an extension of these. Which means that this paper boils down to \"just\" taking an RL approach from Haarnoja / Schulman 2017 that works well with data that is truly off-policy and employing it for learning from (imperfect) demonstrations. Hence we have essentially an experimental paper.\nThe approach does reasonably well, but I'd like to have seen more extensive discussions and an analysis WHY it performs better (or in some experiments actually worse) compared to the baselines.\n\nMinor comments\n===============\nEq (1): \\operatorname{argmax} https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics\nEq (4): I found this very strange, the text reads like this result is from 2010 while the entropy formulation above is claimed to be from 2017...\nSect. 3: \"Equation 8\" -> \"Equation (8)\" \\eqref{}\nSect. 3.1: So what's new compared to Haarnoja / Schulman 2017? \nSect. 3: you talk about normalization but only explain in Sect 3.2 (and even there the discussion should be improved)  which part of the update corresponds to this normalization\nSect. 5.2: \"method(Mnih\" => \"method (Mnih\""},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning from Imperfect Demonstrations","abstract":"Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.","pdf":"/pdf/87cf8126e00d80481ecb6de10dfafa4f387d15b5.pdf","paperhash":"anonymous|reinforcement_learning_from_imperfect_demonstrations","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning from Imperfect Demonstrations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJ9bz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper778/Authors"],"keywords":["learning from demonstration","reinforcement learning","maximum entropy learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739108218,"tcdate":1509134727209,"number":778,"cdate":1509739105550,"id":"BJJ9bz-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJJ9bz-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Reinforcement Learning from Imperfect Demonstrations","abstract":"Robust real-world  learning should benefit from both demonstrations and interaction with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning  to further improve performance based on reward from the environment. These tasks have divergent losses which are difficult to jointly optimize; further, such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.  Our Normalized Actor-Critic (NAC) method can learn from demonstration data of arbitrary quality and also leverages rewards from an interactive environment.  NAC learns an initial policy network from demonstration and refines the policy in a real environment. Crucially, both learning from demonstration and interactive refinement use exactly the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data, since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and  outperform existing baselines when evaluated on several realistic driving games.","pdf":"/pdf/87cf8126e00d80481ecb6de10dfafa4f387d15b5.pdf","paperhash":"anonymous|reinforcement_learning_from_imperfect_demonstrations","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning from Imperfect Demonstrations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJJ9bz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper778/Authors"],"keywords":["learning from demonstration","reinforcement learning","maximum entropy learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}