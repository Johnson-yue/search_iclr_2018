{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222738253,"tcdate":1511818578007,"number":3,"cdate":1511818578007,"id":"rycLSbcgf","invitation":"ICLR.cc/2018/Conference/-/Paper739/Official_Review","forum":"r1pW0WZAW","replyto":"r1pW0WZAW","signatures":["ICLR.cc/2018/Conference/Paper739/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"Summary: The authors introduce a variant of NARX RNNs, which has an additional attention mechanism and a reset mechanism. The attention is only applied on subsets of hidden states, referred as delays. The delays are aggregated into a vector using the attention coefficients as weights, and then this vector is multiplied by the reset gates. \n\nThe model sounds a bit incremental, however, the performance improvements over pMNIST, copy and MobiAct tasks are interesting.\n\nA similar kind of architecture has been already proposed:\n[1] Soltani et al. “Higher Order Recurrent Neural Networks”, arXiv 1605.00064\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies","abstract":"Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.","pdf":"/pdf/217af153c6009d64e989075b19ca4b18074b7e55.pdf","TL;DR":"We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.","paperhash":"anonymous|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies","_bibtex":"@article{\n  anonymous2018analyzing,\n  title={Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1pW0WZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper739/Authors"],"keywords":["recurrent neural networks","long-term dependencies","long short-term memory","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1512222738296,"tcdate":1511733311657,"number":2,"cdate":1511733311657,"id":"H1OSO2dlz","invitation":"ICLR.cc/2018/Conference/-/Paper739/Official_Review","forum":"r1pW0WZAW","replyto":"r1pW0WZAW","signatures":["ICLR.cc/2018/Conference/Paper739/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces a variant of the well-known (but as of today not very frequently used) NARX architecture for Recurrent Neural Networks. It is demonstrated that with the proposed method (MIST RNNs), good performance is achieved on several common RNN problems.","rating":"7: Good paper, accept","review":"The presented MIST architecture certainly has got its merits, but in my opinion is not very novel, given the fact that NARX RNNs have been described 20 years ago, and Clockwork RNNs (which, as the authors point out in section 2, have a similar structure) have also been in use for several years. Still, the presented results are good, with standard LSTMs being substantially outperformed in three out of five standard RNN/LSTM benchmark tasks. The analysis in section 3 is decent (see however the minor comments below), but does not offer revolutionary new insights - it's perhaps more like a corollary of previous work (Pascanu et al., 2013).\n\nRegarding the concrete results, I would have wished for a more detailed analysis of the more surprising results, in particular, for the copy task (section 5.2): Is it really true that Clockwork RNNs fail because they make it \"difficult to learn long-term behavior that must be detected at high frequency\" [section 2]? How relevant are the results in figure 2 (yes, the gradient properties are very different, but is this an issue for accuracy)? In the sequential pMNIST classification, what about increasing the LSTM number of hidden units? If this brings the error rate further down, one could ask why exactly the LSTM captures long-term structure so differently with different number of units?\n\nIn summary, for me this paper is solid, and although the architecture is not that new, it is worth bringing it again into the focus of attention.\n\n\nMinor comments:\n- In several places, the formulas are rather strange and/or occasionally incorrect. In particular,\n* on the right-hand sind of the inline formula in section 3.1, the symbol v is missing completely, which cannot be right;\n* in formula 16, the primes seem to be misplaced, and the symbols t', t''', etc. should be defined;\n* the \\theta_l in the beginning of section 3.3 (formula 13) is completely superfluous.\n- The position of the tables and figures is rather weird, making the paper less readable than necessary. The authors should consider moving floating parts around (one could also move figure three to the bottom of a suitable page, for example).\n- It is a matter of taste, but since all experimental results except the ones on the copy task are tabulated, one could think of adding a table with the results now contained in figure 3.\n\nRelation to prior work: the authors are aware of most relevant work. \n\nOn p2 they write: \"Many other approaches have also been proposed to capture long-term dependencies.\" There is one that seems close to what the authors do: \n\nJ. Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234-242, 1992\n\nIt is related to clockwork RNNs, about which the authors write:\n\n\"A recent architecture that is similar in spirit to our work is that of Clockwork RNNs (Koutnik et al., 2014), which split weights and hidden units into partitions, each with a distinct period. When it’s not a partition’s time to tick, its hidden units are passed through unchanged, thus in some ways mimicking the behavior of NARX RNNs. However Clockwork RNNs differ in two key ways. First, Clockwork RNNs sever high-frequency-to-low-frequency paths, thus making it difficult to learn long-term behavior that must be detected at high frequency (for example, learning to depend on quick motions from the past for activity recognition). Second, Clockwork RNNs require hidden units to be partitioned a priori, which in practice is difficult to do in any meaningful way. NARX RNNs suffer from neither of these drawbacks.\"\n\nThe neural history compressor, however, adapts to the frequency of unexpected events, by ticking only when there is an unpredictable event, thus overcoming some of the issues above. Perhaps this trick could further improve the system of the authors, as well as the Clockwork RNNs, at least for certain tasks?\n\nGeneral recommendation: Accept, provided the comments are taken into account.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies","abstract":"Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.","pdf":"/pdf/217af153c6009d64e989075b19ca4b18074b7e55.pdf","TL;DR":"We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.","paperhash":"anonymous|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies","_bibtex":"@article{\n  anonymous2018analyzing,\n  title={Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1pW0WZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper739/Authors"],"keywords":["recurrent neural networks","long-term dependencies","long short-term memory","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1512222738343,"tcdate":1511727290042,"number":1,"cdate":1511727290042,"id":"BJMTxiOlG","invitation":"ICLR.cc/2018/Conference/-/Paper739/Official_Review","forum":"r1pW0WZAW","replyto":"r1pW0WZAW","signatures":["ICLR.cc/2018/Conference/Paper739/AnonReviewer3"],"readers":["everyone"],"content":{"title":"little novelty and unconvincing","rating":"3: Clear rejection","review":"The followings are my main critics of the paper: \n1. Analysis does not provide any new insights. \n2. Similar work (recurrent skip coefficient and the corresponding architecture in [1]) has been done, but has not been mentioned. \n3. The experimental results are not convincing. This includes 1. the choices of tasks are limited -- very small in size, 2. the performance in pMNIST is worse than [1], under the same settings.\n\nHence I think the novelty of the paper is very little, and the experiments are not convincing.\n\n[1] Architectural Complexity Measures of Recurrent Neural Networks. Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio. NIPS, 2016. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies","abstract":"Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.","pdf":"/pdf/217af153c6009d64e989075b19ca4b18074b7e55.pdf","TL;DR":"We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.","paperhash":"anonymous|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies","_bibtex":"@article{\n  anonymous2018analyzing,\n  title={Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1pW0WZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper739/Authors"],"keywords":["recurrent neural networks","long-term dependencies","long short-term memory","LSTM"]}},{"tddate":null,"ddate":null,"tmdate":1509739130663,"tcdate":1509133828993,"number":739,"cdate":1509739128008,"id":"r1pW0WZAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1pW0WZAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies","abstract":"Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.","pdf":"/pdf/217af153c6009d64e989075b19ca4b18074b7e55.pdf","TL;DR":"We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.","paperhash":"anonymous|analyzing_and_exploiting_narx_recurrent_neural_networks_for_longterm_dependencies","_bibtex":"@article{\n  anonymous2018analyzing,\n  title={Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1pW0WZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper739/Authors"],"keywords":["recurrent neural networks","long-term dependencies","long short-term memory","LSTM"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}