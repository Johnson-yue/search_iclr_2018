{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222725769,"tcdate":1511821873279,"number":3,"cdate":1511821873279,"id":"S1MVzM5ez","invitation":"ICLR.cc/2018/Conference/-/Paper713/Official_Review","forum":"HyZoi-WRb","replyto":"HyZoi-WRb","signatures":["ICLR.cc/2018/Conference/Paper713/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting analysis","rating":"7: Good paper, accept","review":"This paper provides an interesting analysis of the importance sampled estimate of the LL bound and proposes to use Jackknife to correct for the bias. The experiments show that the proposed method works for model evaluation and that computing the correction is archivable at a reasonable computational cost. It also contains an insightful analysis.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511821623050,"tcdate":1511821623050,"number":2,"cdate":1511821623050,"id":"H164Zf9eG","invitation":"ICLR.cc/2018/Conference/-/Paper713/Official_Comment","forum":"HyZoi-WRb","replyto":"HyZoi-WRb","signatures":["ICLR.cc/2018/Conference/Paper713/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper713/AnonReviewer1"],"content":{"title":"Interesting analysis","comment":"Hello, \n\nInteresting analysis. But I’m not particularly surprised that JVI during training does not result in better models compared to IWAE. We often observe very small effective sampling sizes when training big models with only few of the normalized importance weights is close to 1. It seems JVI would result in very similar gradients under these circumstances. I think it could strengthen the paper if this would be investigated and discussed.\n\nWouldn’t a common scale for the LL plots in Figure 3 make it easier to read? \n\nDo you think it would be preferable if the community continued to report biased bounds instead of JVI estimated LLs? This provides a natural partial protection against overestimating due to low effective sampling sizes, doesn’t it? "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222725816,"tcdate":1511818448360,"number":2,"cdate":1511818448360,"id":"SkuC4bqez","invitation":"ICLR.cc/2018/Conference/-/Paper713/Official_Review","forum":"HyZoi-WRb","replyto":"HyZoi-WRb","signatures":["ICLR.cc/2018/Conference/Paper713/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting statistical analysis and ideas; experiments are limited","rating":"6: Marginally above acceptance threshold","review":"The authors analyze the bias and variance of the IWAE bound from Burda et al. (2015), and with explicit formulas up to vanishing polynomial terms and intractable moments. This leads them to derive a jacknife approach to estimate the moments as a way to debias the IWAE for finite importance weighted samples. They apply it for training and also as an evaluation method to assess the marginal likelihood at test time.\n\nThe paper is well-written and offers an interesting combination of ideas motivated from statistical analysis. Following classical results from the debiasing literature, they show a jacknife approach has reduced bias (unknown for variance). In practice, this involves an enumerated subset of calculations leading to a linear cost with respect to the number of samples which I'm inclined to agree is not too expensive.\n\nThe experiments are unfortunately limited to binarized MNIST.  Also, all benchmarks measure lower bound estimates with respect to importance samples, when it's more accurate to measure with respect to runtime. This would be far more convincing as a way to explain how that constant to the linear-time affects computation in practice.  The same would be useful to compare the estimate of the marginal likelihood over training runtime.  Also, I wasn't sure if the JVI estimator still produced a lower bound to make the comparisons. It would be useful if the authors could clarify these details.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222725858,"tcdate":1511787182471,"number":1,"cdate":1511787182471,"id":"HyUn5KYxz","invitation":"ICLR.cc/2018/Conference/-/Paper713/Official_Review","forum":"HyZoi-WRb","replyto":"HyZoi-WRb","signatures":["ICLR.cc/2018/Conference/Paper713/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting debiasing methods for Monte Carlo objectives","rating":"6: Marginally above acceptance threshold","review":"The authors introduce jackknife variational inference (JVI), a method for debiasing Monte Carlo objectives such as the importance weighted auto-encoder. Starting by studying the bias of the IWAE bound for approximating log-marginal likelihood, the authors propose to make use of debiasing techniques to improve the approximation. For the binarized MNIST the authors show improved approximations given the same number of samples from the auxiliary distribution q(z|x).\n\nJVI seems to be an interesting extension of, and perspective on, the IWAE bound (and other Monte Carlo objectives). Some questions and comments:\n\n* The Cremer et al. (2017) paper contains some errors when interpreting the IWAE bound as a standard ELBO with a more flexible variational approximation distribution. For example eq. (1) in their paper does not correspond to an actual distribution, it is not properly normalized. This makes the connection in their section 2.1 unclear. I would suggest citing the following paper instead for this connection and the relation to importance sampling (IS):\nNaesseth, Linderman, Ranganath, Blei, \"Variational Sequential Monte Carlo\", 2017.\n\n* Regarding the analysis of the IWAE bound the paper by Rainforth et al. (2017) mentioned in the comments seems very relevant. Also, because of the strong connection between IWAE and IS detailed in the Naesseth et al. (2017) paper it is possible to make use of a standard Taylor approximation/delta methods to derive Prop. 1 and Prop. 2, see e.g. Robert & Casella, \"Monte Carlo Statistical Methods\" or Liu's \"Monte Carlo Strategies for Scientific Computing\".\n\n* It could be worth mentioning that the JVI objective function is now no longer (I think?) a lower bound to the log-evidence.\n\n* Could the surprising issue (IWAE-learned, JV1-evaluated being better than JV1-learned, JV1-evaluated) in Table 1 be because of different local optima?\n\nMinor comments:\n* p(x) -> p_\\theta(x)\n* In the last paragraph of section 1 it seems like you claim that the expressiveness of p_\\theta(x|z) is a limitation of VAE. It was a bit unclear to me what was actually a general limitation of maximum likelihood versus the approximation based on VAEs.\n* Last paragraph of section 1, \"strong bound\" -> \"tight bound\"\n* Last paragraph of section 2, citation missing for DVI","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1510092426952,"tcdate":1509577463881,"number":1,"cdate":1509577463881,"id":"B1xZQ0DCZ","invitation":"ICLR.cc/2018/Conference/-/Paper713/Official_Comment","forum":"HyZoi-WRb","replyto":"rkSCR0HCW","signatures":["ICLR.cc/2018/Conference/Paper713/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper713/Authors"],"content":{"title":"Thanks","comment":"Thank you for pointing to this related work, it seems clearly relevant; we will read it and will add a citation and potentially a more detailed discussion of the relation to the next version of our submission."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509449421025,"tcdate":1509449421025,"number":1,"cdate":1509449421025,"id":"rkSCR0HCW","invitation":"ICLR.cc/2018/Conference/-/Paper713/Public_Comment","forum":"HyZoi-WRb","replyto":"HyZoi-WRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Similar results on IWAE bound missing from literature review","comment":"Hey\n\nI just wanted to draw your attention to the recent preprint https://arxiv.org/pdf/1709.06181.pdf which includes a very similar set of results to your analysis of the IWAE bound in section 3.  They consider convergence bounds for a more general class of problems by using bias-variance decompositions and consider the IWAE (in section 6.5) as a particular example which leads to a result which is effectively equivalent to your Propositions 1 and 2 up to a constant factor.  To see this, note that your result in corresponds to the case of N=1, M=K, and that the C^2_0 ς^4_1 / 4M^2 + O(1/M^3) terms in their bound constitute the biased squared with the variance comprising of all the other terms in their bound (see proof of Theorem 3).  They thus show the same key high-level result that the bias and variance are both O(1/K).\n\nGiven how recent this related work is and that it is only a preprint with a predominantly different focus, I don't think this detracts too much from your current submission, but you may wish to revise the paper to acknowledge that this very similar result was previously independently derived and to highlight the differences of your results from theirs."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509739146064,"tcdate":1509133208547,"number":713,"cdate":1509739143407,"id":"HyZoi-WRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyZoi-WRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference","abstract":"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI),\na family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders.","pdf":"/pdf/b72e18f808fc5439ccff562587663dd9ffa21eac.pdf","TL;DR":"Variational inference is biased, let's debias it.","paperhash":"anonymous|debiasing_evidence_approximations_on_importanceweighted_autoencoders_and_jackknife_variational_inference","_bibtex":"@article{\n  anonymous2018debiasing,\n  title={Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyZoi-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper713/Authors"],"keywords":["variational inference","approximate inference","generative models"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}