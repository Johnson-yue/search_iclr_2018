{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222690410,"tcdate":1512129030161,"number":3,"cdate":1512129030161,"id":"HkCWGa0xM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Revisiting auto-regressive models for graph generation","rating":"6: Marginally above acceptance threshold","review":"The authors introduce a sequential/recurrent model for generation of small graphs. The recurrent model takes the form of a graph neural network. Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state. \n\nThe paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field.\n\nThe proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not. [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi-step process is left for future work. Training the generative model with fixed ground-truth ordering was similarly performed in [1] (“strong supervision”) and is thus not particularly novel.\n\nEqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v’)? Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)?\n\nOrdering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth. A short discussion of this result would make the paper stronger.\n\nFor chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful.\n\nOther minor points:\n- In the definition of f_nodes: What is p(y)? It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V.  \n- The following statement is unclear to me: “but building a varying set of objects is challenging in the first place, and the graph model provides a way to do it.” Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])?\n- Typos in this sentence: “Lastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well.”\n\nOverall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem.\n\n[1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016\n[3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017\n[4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222690461,"tcdate":1511816520874,"number":2,"cdate":1511816520874,"id":"ByZ8Tx9ez","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising generative model for graphs","rating":"6: Marginally above acceptance threshold","review":"The authors proposed a graph neural network based architecture for learning generative models of graphs. Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data. The representation is clear with detailed empirical studies. I support its acceptance.\n\nThe draft does need some improvements and here is my suggestions.\n1. Figure 1 could be improved using a concrete example like in Figure 6. If space allowed, an example of different ordering leads to the same graph will also help.\n\n2. More details on how node embedding vectors are initialized. How does different initializations affect results? Why is nodes at different stages with the same initialization problematic?\n\n3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation.\n\n4. The sequence ordering is important. While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment. I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering.\n\n5. In Section 4.1, the choice of ER random graph as a baseline is too simplistic. It does not provide a meaningful comparison. A better generative model for cycles and trees could help.\n\n6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222690507,"tcdate":1511785793558,"number":1,"cdate":1511785793558,"id":"S1crSKYgM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Learning deep generative models for graphs","rating":"5: Marginally below acceptance threshold","review":"The paper introduces a generative model for graphs. The three main decision functions in the sequential process are computed with neural nets. The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T. The paper is well written but, in my opinion, a description of the learning framework should be given in the paper. Also, a summary of the hyperparameters used in the proposed system should be given. It is claimed that all possible types of graphs can be learned which seems rather optimistic. For instance, when learning trees, the system is tweaked for generating trees. Also, it is not clear whether models for large graphs can be learned. The paper contain many interesting contributions but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs. Therefore, I am not convinced that the paper is ready for publication at ICLR'18.\n\n* Introduction. I am not convinced by the discussion on graph grammars in the second paragraph. It is known that there does not exist a definition of regular grammars in graph (see Courcelle and Engelfriet, graph structure and monadic second-order logic ...). Moreover, many problems are known to be undecidable. For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words. Therefore I does not seem pertinent here. A more complete reference is \"handbook of weighted automata\" by Droste. Also, many decision problems for wighted automata are known to be undecidable. I am not sure that the paragraph is useful for the paper. A discussion on learning as in footnote 1 shoud me more interesting.\n* Related work. I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.\n* Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs. This leads to the question of cheating in the learning process.\n* Section 3.2. The functions f_m and g_m for defining graph embedding are left undefined. As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified.\n* Section 3. As said before, a general description of the learning framework should be given. Also, it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process. Therefore, it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learned.  Consequently, it is difficult to see the influence of T. Also, it should be said whether the node embeddings and graph embeddings for the output graph can be useful.\n* Section 3. A summary of all the hyperparameters should be given.\n* Section 4.1. The number of steps is not given. Do you present the same graph multiple times. Why T=2 and not 1 or 10 ?\n* Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20. Do you use tweaks in the generation process.\n* Section 4.3. The generation process is adapted for generating trees which seems to be cheating. Again the choice of T seems ad hoc and based on computational burden.\n* Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs.\n* Section 5. The discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparameters.\n* acronyms should be expansed at their first use","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510837174661,"tcdate":1510837174661,"number":1,"cdate":1510837174661,"id":"r1kajbsyG","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"HkzAQO_kM","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"Reply","comment":"Thanks for the comment and bringing up this related paper.  We will update our paper with more discussion and citations to related work (we are not allowed to make changes to our submission at the moment).\n\nThe main difference between our work and Johnson (2017) is that our goal in this paper is to learn and represent unconditional or conditional densities on a space of graphs given a representative sample of graphs, whereas Johnson is primarily interested in using graphs as intermediate representations in reasoning tasks.  However, Johnson (2017) do offer a probabilistic semantics for their graphs (the soft, real-valued node and connectivity strengths).  But, as a generative model, Johnson (2017) did make a few strong assumptions for the generation process, e.g. a fixed number of nodes for each sentence, independent probability for edges given a batch of new nodes, etc.; while our model doesn't make any of these assumptions.\n\nOn the other side, as we are modeling graph structures, the samples from our model are graphs where an edge or node either exists or does not exist; whereas in Johnson (2017) all the graph components, e.g. existence of a node or edge, are all soft, and it is this form of soft node / edge connectivity that was been used for other reasoning tasks.  Dense and soft representation may be good for some applications, while the sparse discrete graph structures may be good for others.  Potentially, our graph generative model can also be used in an end-to-end pipeline to solve prediction problems as well, like Johnson (2017)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510667210034,"tcdate":1510667210034,"number":1,"cdate":1510667210034,"id":"HkzAQO_kM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Public_Comment","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relation to Learning Graphical State Transitions","comment":"I've enjoyed reading this paper, but I'm wondering if the authors are aware of \"Learning Graphical State Transitions\" (Johnson, ICLR'17 oral). The work presented here feels like a generalization, but it shares many ideas with the earlier paper, and a discussion of the differences would definitely be very helpful."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739237959,"tcdate":1509126502795,"number":556,"cdate":1509739235291,"id":"Hy1d-ebAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/d6218dcb8c1821a916cf60eea0e12f8842488009.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}