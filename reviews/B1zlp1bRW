{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222679624,"tcdate":1511997906456,"number":2,"cdate":1511997906456,"id":"B1cR-6neM","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Review","forum":"B1zlp1bRW","replyto":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting paper on large scale optimal transport, though \"overstating\" some properties ","rating":"6: Marginally above acceptance threshold","review":"The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems.\n\nI like the weak convergence results, but this is just weak convergence. It appears to be an overstatement to claim that the approach \"nearly-optimally\" transports one distribution to the other (Cf e.g. Conclusion). There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2. Also, near-optimality would refer to some parameters being chosen in the best possible way. I do not see that from the paper. However, the weak convergence results are good.\n\nA better result, hinting on how \"optimal\" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist). This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum. \n\nI also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? After all, the L2 one is just an approximation of the entropic one.\n\nTypoes:\n\n1- Kanthorovich -> Kantorovich (Intro)\n2- Cal C <-> C (eq. 4)","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1512222679667,"tcdate":1511806941875,"number":1,"cdate":1511806941875,"id":"rJ81OAtgM","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Review","forum":"B1zlp1bRW","replyto":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper presents interestings results about consistency of learning OT/Monge maps although weak and stochastic learning algorithms able to scale, however some parts should deserve more discussion and experimental evaluation is limited.","rating":"6: Marginally above acceptance threshold","review":"Quality\nThe theoretical results presented in the paper appear to be correct. However, the experimental evaluation is globally limited,  hyperparameter tuning on test which is not fair.\n\nClarity\nThe paper is mostly clear, even though some parts deserve more discussion/clarification (algorithm, experimental evaluation).\n\nOriginality\nThe theoretical results are original, and the SGD approach is a priori original as well.\n\nSignificance\nThe relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area, the other aspects of the paper are limited.\n\nPros:\n-Theoretical results on the convergence of OT/Monge maps\n-Regularized formulation compatible with SGD\nCons\n-Experimental evaluation limited\n-The large scale aspect lacks of thorough analysis\n-The paper presents 2 contributions but at then end of the day, the development of each of them appears limited\n\nComments:\n\n-The weak convergence results are interesting. However, the fact that no convergence rate is given makes the result weak. \nIn particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential.\nThis can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012).\nThen, I think that the claim of page 6 saying that Domain Adaptation can be performed \"nearly optimally\" has then to be rephrased.\nI think that results show that the approach is theoretically justified but optimality is not here yet.\n\nTheorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization?\n\n-The experimental evaluation on the running time is limited to one particular problem. If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations.\nIt is also surprising that the efficiency the L2-regularized version is not evaluated.\nFor a paper interesting in large scale aspects, the experimental evaluation is rather weak.\n \nThe 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found?\n\n-Algorithm 1 is presented without any discussion about complexity, rate of convergence. Could the authors discuss this aspect?\nThe presentation of this algo is a bit short and could deserve more space (in the supplementary)\n\n-For the DA application, the considered datasets are classic but not really \"large scale\", anyway this is a minor remark.\nThe setup is not completely clear, since the approach is interesting for out of sample data, so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map. This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification.\nHyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup: it seems that the parameters are tuned on test (for all methods), which is not fair since target label information will not be available from a practical standpoint.\n\nThe authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline.\n\nExperiments on generative optimal transport are interesting and probably generate more discussion/perspectives.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1509739256239,"tcdate":1509125354302,"number":523,"cdate":1509739253576,"id":"B1zlp1bRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}