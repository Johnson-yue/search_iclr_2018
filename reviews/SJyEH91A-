{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222576448,"tcdate":1512023333344,"number":3,"cdate":1512023333344,"id":"SkpXB7TlM","invitation":"ICLR.cc/2018/Conference/-/Paper147/Official_Review","forum":"SJyEH91A-","replyto":"SJyEH91A-","signatures":["ICLR.cc/2018/Conference/Paper147/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A clearly written, novel, straightforward and practical approach to Wasserstein distance--based image embeddings.","rating":"7: Good paper, accept","review":"This paper proposes approximating the Wasserstein distance between normalized greyscale images based on a learnable approximately isometric embedding of images into Euclidean space. The paper is well written with clear and generally thorough prose. It presents a novel, straightforward and practical solution to efficiently computing Wasserstein distances and performing related image manipulations.\n\nMajor comments:\n\nIt sounds like the same image may be present in the training set and eval set. This is methodologically suspect, since the embedding may well work better for images seen during training. This affects all experimental results.\n\nI was pleased to see a comparison between using exact and approximate Wasserstein distances for image manipulation in Figure 5, since that's a crucial aspect of whether the method is useful in practice. However the exact computation (OT LP) appears to be quite poor. Please explain why the approximation is better than the exact Wasserstein difference for interpolation. Relatedly, please summarize the argument in Cuturi and Peyre that is cited (\"as already explained in\").\n\nMinor comments:\n\nIn section 3.1 and 4.1, \"histogram\" is used to mean normalized-to-sum-to-1 images, which is not the conventional meaning.\n\nIt would help to pick one of \"Wasserstein Deep Learning\" and \"Deep Wasserstein Embedding\" and use it and the acronym consistently throughout.\n\n\"Disposing of a decoder network\" in section 3.1 should be \"using a decoder network\"?\n\nIn section 4.1, the architectural details could be clarified. What size are the input images? What type of padding for the convolutions? Was there any reason behind the chosen architecture? In particular the use of a dense layers followed by convolutional layers seems peculiar.\n\nIt would be helpful to say explicitly what \"quadratic ground metric\" means (i.e. W_2, I presume) in section 4.2 and elsewhere.\n\nIt would be helpful to give a sense of scale for the numbers in Table 1, e.g. give the 95th percentile Wasserstein distance. Perhaps use the L2 distance passed through a 1D-to-1D learned warping as a baseline.\n\nMention that OT stands for optimal transport in section 4.3.\n\nSuggest mentioning \"there is no reason for a Wasserstein barycenter to be a realistic sample\" in the main text when first discussing barycenters.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Wasserstein Embeddings","abstract":"The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.","pdf":"/pdf/9d18a11db984c2c5d0f611c1d2983fb3fdc2cea5.pdf","TL;DR":"We show that it is possible to fastly approximate Wasserstein distances computation by finding an appropriate embedding where Euclidean distance emulates the Wasserstein distance","paperhash":"anonymous|learning_wasserstein_embeddings","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Wasserstein Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyEH91A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper147/Authors"],"keywords":["Wasserstein distance","metric embedding","Siamese architecture"]}},{"tddate":null,"ddate":null,"tmdate":1512222576530,"tcdate":1512002279341,"number":2,"cdate":1512002279341,"id":"r11xXR3xf","invitation":"ICLR.cc/2018/Conference/-/Paper147/Official_Review","forum":"SJyEH91A-","replyto":"SJyEH91A-","signatures":["ICLR.cc/2018/Conference/Paper147/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple idea that is potentially useful in practice.","rating":"7: Good paper, accept","review":"The paper presents a simple idea to reduce the computational cost of computing Wasserstein distance between a pair of histograms. Specifically, the paper proposes learning an embedding on the original histograms into a new space where Euclidean distance in the latter relates to the Wasserstein distance in the original space. Despite simplicity of the idea, I think it can potentially be useful practical tool, as it allows for very fast approximation of Wasserstein distance. The empirical results show that embeddings learned by the proposed model indeed provide a good approximation to the actual Wasserstein distances.\n\nThe paper is well-written and is easy to follow and understand. There are some grammar/spelling issues that can be fixed by a careful proofreading. Overall, I find the paper simple and interesting.\n\nMy biggest concern however is the applicability of this approach to high-dimensional data. The experiments in the paper are performed on 2D histograms (images). However, the number of cells in the histogram grows exponentially in dimension. This may turn this approach impractical even in a moderate-sized dimensionality, because the input to the learning scheme  requires explicit representation of the histogram, and the proposed method may quickly run into memory problems. In contrast, if one uses the non-learning based approach (standard LP formulation of Wasserstein distance), at least in case of W_1, one can avoid memory issues caused by the dimensionality by switching to the dual form of the LP. I believe that is an important property that has made computation of Wasserstein distance practical in high dimensional settings, but seems inapplicable to the learning scheme. If there is a workaround, please specify.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Wasserstein Embeddings","abstract":"The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.","pdf":"/pdf/9d18a11db984c2c5d0f611c1d2983fb3fdc2cea5.pdf","TL;DR":"We show that it is possible to fastly approximate Wasserstein distances computation by finding an appropriate embedding where Euclidean distance emulates the Wasserstein distance","paperhash":"anonymous|learning_wasserstein_embeddings","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Wasserstein Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyEH91A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper147/Authors"],"keywords":["Wasserstein distance","metric embedding","Siamese architecture"]}},{"tddate":null,"ddate":null,"tmdate":1512222576578,"tcdate":1511984688651,"number":1,"cdate":1511984688651,"id":"S1FE0K2eG","invitation":"ICLR.cc/2018/Conference/-/Paper147/Official_Review","forum":"SJyEH91A-","replyto":"SJyEH91A-","signatures":["ICLR.cc/2018/Conference/Paper147/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An efficient approach to compute Wasserstein distances and to perform various related analyses.","rating":"7: Good paper, accept","review":"The paper proposes to use a deep neural network to embed probability distributions in a vector space, where the Euclidean distance in that space matches the Wasserstein distance in the original space of probability distributions. A dataset of pairs of probability distributions and their Wasserstein distance is collected, and serves as a target to be predicted by the deep network.\n\nThe method is straightforward, and clearly explained. Two analyses based on Wasserstein distances (computing barycenters, and performing geodesic analysis) are then performed directly in the embedded space.\n\nThe authors claim that the proposed method produces sharper barycenters than those learned using the standard (smooth) Wasserstein distance. It is unclear from the paper whether the advantage comes from the ability of the method to scale better and use more examples, or to be able to use the non-smooth Wasserstein distance, or finally, whether the learning of a deep embedding yields improved extrapolation properties. A short discussion could be added. It would also be interesting to provide some guidance on what is a good structure for the encoder (e.g. should it include spatial pooling layers?)\n\nThe term “Wasserstein deep learning” is probably too broad, “deep Wasserstein embedding” could be more appropriate.\n\nThe last line of future work in the conclusion seems to describe the experiment of Table 1.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Wasserstein Embeddings","abstract":"The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.","pdf":"/pdf/9d18a11db984c2c5d0f611c1d2983fb3fdc2cea5.pdf","TL;DR":"We show that it is possible to fastly approximate Wasserstein distances computation by finding an appropriate embedding where Euclidean distance emulates the Wasserstein distance","paperhash":"anonymous|learning_wasserstein_embeddings","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Wasserstein Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyEH91A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper147/Authors"],"keywords":["Wasserstein distance","metric embedding","Siamese architecture"]}},{"tddate":null,"ddate":null,"tmdate":1509739460295,"tcdate":1509037350704,"number":147,"cdate":1509739457639,"id":"SJyEH91A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJyEH91A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Wasserstein Embeddings","abstract":"The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.","pdf":"/pdf/9d18a11db984c2c5d0f611c1d2983fb3fdc2cea5.pdf","TL;DR":"We show that it is possible to fastly approximate Wasserstein distances computation by finding an appropriate embedding where Euclidean distance emulates the Wasserstein distance","paperhash":"anonymous|learning_wasserstein_embeddings","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Wasserstein Embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJyEH91A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper147/Authors"],"keywords":["Wasserstein distance","metric embedding","Siamese architecture"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}