{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222798890,"tcdate":1512157823886,"number":3,"cdate":1512157823886,"id":"rJOKM41-M","invitation":"ICLR.cc/2018/Conference/-/Paper871/Official_Review","forum":"r1lUOzWCW","replyto":"r1lUOzWCW","signatures":["ICLR.cc/2018/Conference/Paper871/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good overview; main contribution is theoretical proof","rating":"6: Marginally above acceptance threshold","review":"The main contribution of the paper is that authors extend some work of Bellemare: they show that MMD GANs [which includes the Cramer GAN as a subset] do possess unbiased gradients. They provide a lot of context for the utility of this claim, and in the experiments section they provide a few different metrics for comparing GANs [as this is a known tricky problem]. The authors finally show that an MMD GAN can achieve comparable performance with a much smaller network used in the discriminator.\n\nAs previously mentioned, the big contribution of the paper is the proof that MMD GANs permit unbiased gradients. This is a useful result; however, given the lack of other outstanding theoretical or empirical results, it almost seems like this paper would be better shaped as a theory paper for a journal. I could be swayed to accept this paper however if others feel positive about it.\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]}},{"tddate":null,"ddate":null,"tmdate":1512222798929,"tcdate":1511830135170,"number":2,"cdate":1511830135170,"id":"rkkFfN5gz","invitation":"ICLR.cc/2018/Conference/-/Paper871/Official_Review","forum":"r1lUOzWCW","replyto":"r1lUOzWCW","signatures":["ICLR.cc/2018/Conference/Paper871/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clearly written review of MMD gans with some good insights","rating":"7: Good paper, accept","review":"The quality and clarity of this work are very good. The introduction of the kernel inception metric is well-motivated and novel, to my knowledge. With the mention of a bit more related work (although this is already quite good), I believe that this could be a significant resource for understanding MMD GANs and how they fit into the larger model zoo.\n\nPros\n - best description of MMD GANs that I have encountered\n - good contextualization of related work and descriptions of relationships, at least among the works surveyed\n - reasonable proposed metric (KID) and comparison with other scores\n - proof of unbiased gradient estimates is a solid contribution\n\nCons\n - although the review of related work is very good, it does focus on ~3 recent papers. As a review, it would be nice to see mention (even just in a list with citations) of how other models in the zoo fit in\n - connection between IPMs and MMD gets a bit lost; a figure (e.g.  flow chart) would help\n - wavers a bit between proposing/proving novel things vs. reviewing and lacks some overall structure/storyline\n - Figure 1 is a bit confusing; why is KID tested without replacement, and FID with? Why 100 vs 10 samples? The comparison is good to have, but it's hard to draw any insight with these differences in the subfigures. The figure caption should also explain what we are supposed to get out of looking at this figure.\n\nSpecific comments:\n - I suggest bolding terms where they are defined; this makes it easy for people to scan/find (e.g. Jensen-Shannon divergence, Integral Probability Metrics, witness functions, Wasserstein distance, etc.) \n - Although they are common knowledge in the field, because this is a review it could be helpful to provide references or brief explanations of e.g. JSD, KL, Wasserstein distance, RKHS, etc.\n - a flow chart (of GANs, IPMs, MMD, etc., mentioning a few more models than are discussed in depth here, would be *very* helpful.\n - page 2, middle paragraph, you mention \"...constraints to ensure the kernel distribution embeddings remained injective\"; it would be helpful to add a sentence here to explain why that's a good thing.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]}},{"tddate":null,"ddate":null,"tmdate":1511712041453,"tcdate":1511712041453,"number":1,"cdate":1511712041453,"id":"S1b4HDuez","invitation":"ICLR.cc/2018/Conference/-/Paper871/Official_Comment","forum":"r1lUOzWCW","replyto":"S1YDTsmgG","signatures":["ICLR.cc/2018/Conference/Paper871/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper871/Authors"],"content":{"title":"Re: Difference from MMD GAN?","comment":"It's true that the model we consider here is also described in Section 5.5 of the latest version of Li et al. This result was not in the original version of the paper, however, and only appeared in the revised arXiv submission of November 6, a week and a half after the ICLR deadline; we were not aware of the new version until you pointed it out (thanks for doing so).\n\nThat said, the main point of our paper is not to propose yet another GAN variation (YAGAN?). The idea of using an MMD as critic with a kernel defined on deep convolutional features is not new, nor is the idea of regularizing the gradient of the critic witness function: we cite the papers where (to our knowledge) these ideas were first proposed.  The point of our paper is to understand (and \"demystify\") the MMD GAN, and its relation with other integral probability metric-based GANs. In this direction, our new results are in three areas:\n\n* We clarify the relationship between MMD GANs and Cramér GANs, and the relationship of the MMD GAN critic and witness function to those of WGAN-GPs (thus explaining why the gradient penalty makes sense for the MMD GAN).\n\n* We formally show the unbiasedness of the gradient of the MMD estimator wrt the network parameters, in Theorem 1.  This is our main theoretical result, and an important property to establish when the MMD is used as a GAN critic.\n\n* Our main new experimental finding is that MMD GANs seem to work about as well as WGAN-GPs that use much larger critic networks. Thus, for a given generator, MMD GANs will be simpler and faster to train than WGAN-GPs. Our understanding of why this happens is described in detail in the paper.\n\nAlong the way, we also proposed the KID score, which is a more natural metric of generative model convergence than the Inception score, and inherits many of the nice properties of the previously-proposed FID, but is much easier and more intuitive to estimate."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]}},{"tddate":null,"ddate":null,"tmdate":1512222798971,"tcdate":1511698195327,"number":1,"cdate":1511698195327,"id":"SJsGyNugf","invitation":"ICLR.cc/2018/Conference/-/Paper871/Official_Review","forum":"r1lUOzWCW","replyto":"r1lUOzWCW","signatures":["ICLR.cc/2018/Conference/Paper871/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The contribution is too incremental!","rating":"4: Ok but not good enough - rejection","review":"This paper claims to demystify MMD-GAN, a generative adversarial network with the maximum mean discrepancy (MMD) as a critic, by showing that the usual estimator for MMD yields unbiased gradient estimates (Theorem 1). It was noted by the authors that biased gradient estimate can cause problem when performing stochastic gradient descent, as also noted previously by Bellemare et al. The authors also proposed a kernel inception distance (KID) as a quantitative evaluation metric for GAN. The KID is defined to be the squared MMD between inception representation of the distributions. In experiments, the authors compared the quality of samples generated by MMD-GAN with various kernels with the ones generated from WGAN-GP (Gulrajani et al., 2017) and Cramer GAN (Bellemare et al., 2017). The empirical results show the benefits of using the MMD on top of deep convolutional features. \n\nThe major flaw of this paper is that its contribution is not really clear. Showing that the expectation and gradient can be interchanged (Theorem 1) does not seem to provide sufficient significance. Unbiasedness of the gradient alone does not guarantee that training will be successful and that the resulting models will better reflect the underlying data distribution, as evident by other successful variants of GANs, e.g., WGAN, which employ biased estimate. Indeed, since the training process relies on a small mini-batch, a small bias could help counteract the potentially high variance of the gradient estimate. The key is rather a good balance of both bias and variance during the training process and a guarantee that the estimate is asymptotically unbiased wrt the training iterations. Lastly, I do not see how the empirical results would demystify MMD-GANs, as claimed by the paper.\n\nThe paper is clearly written. \n\nSome minor comments:\n\n- The proof of the main result, Theorem 1, should be placed in the main paper.\n- Page 7, 2nd paragraph: later --> layer","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]}},{"tddate":null,"ddate":null,"tmdate":1511402848901,"tcdate":1511402848901,"number":1,"cdate":1511402848901,"id":"S1YDTsmgG","invitation":"ICLR.cc/2018/Conference/-/Paper871/Public_Comment","forum":"r1lUOzWCW","replyto":"r1lUOzWCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Difference from MMD GAN? ","comment":"What is the main difference between this paper and the MMD GAN paper [Li et al. 2017]. For my understanding, it is just the MMD GAN work which replaced clipping with gradient penalty. GP was also mentioned and tried in [Li et al. 2017] (last version on arXiv) https://arxiv.org/pdf/1705.08584.pdf  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]}},{"tddate":null,"ddate":null,"tmdate":1509739056077,"tcdate":1509136455675,"number":871,"cdate":1509739053423,"id":"r1lUOzWCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1lUOzWCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Demystifying MMD GANs","abstract":"We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we show that the natural estimator for maximum mean discrepancies yields unbiased gradient estimates, which is essential in providing a good signal to the generator. We discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an intergal probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance.","pdf":"/pdf/37e2db86ed222ed46afeaeee8ee9e22ba459d7eb.pdf","TL;DR":"MMD GANs have theoretical advantages for SGD and work with smaller critic networks than WGAN-GPs.","paperhash":"anonymous|demystifying_mmd_gans","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={Demystifying MMD GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lUOzWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper871/Authors"],"keywords":["gans","mmd","ipms","wgan","gradient penalty","unbiased gradients"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}