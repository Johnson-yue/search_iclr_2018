{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222679167,"tcdate":1512078286571,"number":3,"cdate":1512078286571,"id":"rJwCseCez","invitation":"ICLR.cc/2018/Conference/-/Paper520/Official_Review","forum":"H1mp21ZCZ","replyto":"H1mp21ZCZ","signatures":["ICLR.cc/2018/Conference/Paper520/AnonReviewer1"],"readers":["everyone"],"content":{"title":"analogies between matrix and tensor norms to study regularized tensor completion","rating":"5: Marginally below acceptance threshold","review":"The paper proposes to use a regularizer for tensor completion problems that can be written in a similar fashion as the variational factorization formulation of the trace norm aka nuclear norm for matrices. \nThe paper introduces the regularizer with the nice argument that the gradient of the L3 norm to the power of 3rd will be easy to compute, but if we were raising the L2 norm to the 3rd power it would not be the case. They mention that their argument can generalize from D=3 to higher order tensors. \nAuthors mention the paper by Friedland and Lim that introduces this norm and provides first theoretical results on it. Authors develop on the tensor equivalent of the matrix max norm which is built with the motivation of bringing robustness to heavy nodes in the graph (very popular content). This is again straightforward on the technical side.\nEmpirical results are fine but do not show huge improvements compared to baselines so I do not think this is a strong argument for accepting the paper.\nOn the scalability, authors do not show that their approach is better suited than baselines.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nuclear p-norms for large tensor completion","abstract":"We present algorithms for tensor completion using regularizers based\non tensor nuclear p-norms. For the particular case of the nuclear\ninfinity-norm, we generalize to higher-order tensors the theoretical\nguarantees of the max-norm for matrix completion. From a practical  perspective,\nwe present two algorithms based on stochastic gradients to regularize the canonical\ndecomposition of tensors, and show on large-scale benchmark datasets\nfor knowledge base completion that (a) contrary to what suggested prior results in the literature, the canonical decomposition of tensors can achieve state-of-the-art level performance on FB15K and WN, and (b) our new regularizations reach or outperform the state-of-the-art on task where the canonical decomposition alone is not reaching it. In particular, we provide evidence that the nuclear 3-norm can replace the structures and/or regularization terms of existing link prediction models, and\nleads to better performance.","pdf":"/pdf/7be84d4573e72b0fd14ff9e52459383d07a7660e.pdf","TL;DR":"Efficient regularizers for tensor completion  applied to large scale link prediction in knowledge bases.","paperhash":"anonymous|nuclear_pnorms_for_large_tensor_completion","_bibtex":"@article{\n  anonymous2018nuclear,\n  title={Nuclear p-norms for large tensor completion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mp21ZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper520/Authors"],"keywords":["tensor completion","knowledge base completion","relational learning","tensor norms"]}},{"tddate":null,"ddate":null,"tmdate":1512222679210,"tcdate":1511853277461,"number":2,"cdate":1511853277461,"id":"H1SJ6tqeM","invitation":"ICLR.cc/2018/Conference/-/Paper520/Official_Review","forum":"H1mp21ZCZ","replyto":"H1mp21ZCZ","signatures":["ICLR.cc/2018/Conference/Paper520/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Solid paper","rating":"7: Good paper, accept","review":"This paper explores the application of tensor nuclear norm regularization in the context of large scale tensor completion. Tensor nulcear norms have been mainly studied in theoretical ML community because they are convex regularizers and recovery guarantees can be shown but they are generally NP hard to compute. On the other hand, large scale tensor completion has attracted considerable interest in applied ML community because of applications to network analysis and knowledge base completion.\n\nThe contribution of this paper is to show that the convergence guarantee of non-convex minibatch proximal SGD algorithm (Reddi et al, 2016) can be applied to the minimization of nuclear 3-norm and nuclear infinity-norm regularizers and to empirically study their performance.\n\nEmpirically the authors show that on small datasets (nation, umls, kin), ComplEx embedding combined with nuclear 3-norm regularizer matches the state-of-the art performance.\n\nOn the large scale benchmark datasets (FB15k and WN18) the authors show that contrary to common belief, pure CANDECOMP/PARAFAC decomposition without any regularization performs the best when large enough ranks are used. I personally found this result worth spreading because these datasets are widely used.\n\nFinally on the SVO dataet the authors demonstrate again that the proposed nuclear 3-norm regularization performs better than the weight decay (L2) regularization.\n\nI found the presentation of the paper clean and orderly. The finding that CP with large enough rank achieves state-of-the art on the commonly used FP15k and WN18 datasets is surprising and worth informing the community. I also like that the authors bridges between the theoretical ML community and the applied ML community. A shortcoming of the paper is that the performance benefit of the method is only demonstrated on the SVO dataset and small scale datasets. More analysis of why regularization is unnecessary for FB15k and WN18 would greatly improve the paper. For example, does regularization help when we add artificial noise to these datasets?\n\nMinor comments\n * The comment about the non-convexity of the L2 regularizer (p7) is probably confusing for most readers. I think the authors are saying that there is no tensor norm corresponding to the L2 regularizer (and L2 is not equal to the variational form of nuclear 2-norm) but does that explain the large variance of L2 in Figure 1? I think Reddi et al's convergence guarantee holds also for L2 as well. Please comment.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nuclear p-norms for large tensor completion","abstract":"We present algorithms for tensor completion using regularizers based\non tensor nuclear p-norms. For the particular case of the nuclear\ninfinity-norm, we generalize to higher-order tensors the theoretical\nguarantees of the max-norm for matrix completion. From a practical  perspective,\nwe present two algorithms based on stochastic gradients to regularize the canonical\ndecomposition of tensors, and show on large-scale benchmark datasets\nfor knowledge base completion that (a) contrary to what suggested prior results in the literature, the canonical decomposition of tensors can achieve state-of-the-art level performance on FB15K and WN, and (b) our new regularizations reach or outperform the state-of-the-art on task where the canonical decomposition alone is not reaching it. In particular, we provide evidence that the nuclear 3-norm can replace the structures and/or regularization terms of existing link prediction models, and\nleads to better performance.","pdf":"/pdf/7be84d4573e72b0fd14ff9e52459383d07a7660e.pdf","TL;DR":"Efficient regularizers for tensor completion  applied to large scale link prediction in knowledge bases.","paperhash":"anonymous|nuclear_pnorms_for_large_tensor_completion","_bibtex":"@article{\n  anonymous2018nuclear,\n  title={Nuclear p-norms for large tensor completion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mp21ZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper520/Authors"],"keywords":["tensor completion","knowledge base completion","relational learning","tensor norms"]}},{"tddate":null,"ddate":null,"tmdate":1512222679252,"tcdate":1511815855236,"number":1,"cdate":1511815855236,"id":"r1P25xqxf","invitation":"ICLR.cc/2018/Conference/-/Paper520/Official_Review","forum":"H1mp21ZCZ","replyto":"H1mp21ZCZ","signatures":["ICLR.cc/2018/Conference/Paper520/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Tensor completion","rating":"5: Marginally below acceptance threshold","review":"The paper proposes nuclear-p norm as regularization for scalable tensor completion. For order-3 tensor, the optimization is posed through a variational characterization of nuc-3 and nuc-inifty norm in the (CP) factor space. The models are mainly compared to estimates from similarly (CP) factored representations with l2 regularization on the factors. \n\nI find the experiments in the paper somewhat insufficient to explain he efficacy of the proposed models. Although there are some interesting observations. Detailed comments/questions below:\n———\n1.   The efficacy of the nuclear-p norm over l2 norm is mainly demonstrated for small datasets (sec 5.1)\n \n1. a. nuclear-p norm show a smoother variation of validation error with changing regularization as compared to l2- regularization. This is kind of surprising. \n\nBut the authors attribute this to “non-convexity” of the regularizer. If I understood the baseline correctly in *both* formulations  (with l2 norm reg on factors and nuclear-p norm reg on factors), the loss function is non-convex in the factorized representation while regularizers themselves are convex - it is not clear why the authors attribute the unstability of the l2-regularization to the optimization objective. \n\nI also would like to know how carefully the learning rates were tuned for the baseline models.\n\n1.b. In these small data sets, the results should also be compared against convex formation of tensor completion, like Liu et al. 2013 “Tensor Completion for Estimating Missing Values in Visual Data”;  Tomioka and Suzuki. \"Convex tensor decomposition via structured Schatten norm regularization”  (code available) \n(also missing references). Even though these convex formulations are computationally very expensive and might have better performance, it is still useful to see what the gap in performance is. \n\n1.c. another missing baseline is CP factorization with alternating minimization/gradient descent without any regularization. \n——\n\n2. The main advantage of the factorization based algorithms is the scalability, hence experiments large datasets should be the main focus of experiments. However, for large datasets, the nucleus-p norm is shown to be advantageous over l2 regularization only in one of the three datasets (SVO) - this is fine (and not my concern) as I think it is important to show experiments where the algorithm is not advantageous too. But my concern is that, the details of the results on SVO are kind of limited.\n\n2.a, I would like to see the results on other ranking metrics besides H@5 - like H@1, H@10, MRR. \n\n2.b. The authors claim experiments for CP are in appendix, but there are not additional experiments in the appendix\n\n2.c. Again, I would like to see performance of CP without regularization included the experiments","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Nuclear p-norms for large tensor completion","abstract":"We present algorithms for tensor completion using regularizers based\non tensor nuclear p-norms. For the particular case of the nuclear\ninfinity-norm, we generalize to higher-order tensors the theoretical\nguarantees of the max-norm for matrix completion. From a practical  perspective,\nwe present two algorithms based on stochastic gradients to regularize the canonical\ndecomposition of tensors, and show on large-scale benchmark datasets\nfor knowledge base completion that (a) contrary to what suggested prior results in the literature, the canonical decomposition of tensors can achieve state-of-the-art level performance on FB15K and WN, and (b) our new regularizations reach or outperform the state-of-the-art on task where the canonical decomposition alone is not reaching it. In particular, we provide evidence that the nuclear 3-norm can replace the structures and/or regularization terms of existing link prediction models, and\nleads to better performance.","pdf":"/pdf/7be84d4573e72b0fd14ff9e52459383d07a7660e.pdf","TL;DR":"Efficient regularizers for tensor completion  applied to large scale link prediction in knowledge bases.","paperhash":"anonymous|nuclear_pnorms_for_large_tensor_completion","_bibtex":"@article{\n  anonymous2018nuclear,\n  title={Nuclear p-norms for large tensor completion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mp21ZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper520/Authors"],"keywords":["tensor completion","knowledge base completion","relational learning","tensor norms"]}},{"tddate":null,"ddate":null,"tmdate":1510092386797,"tcdate":1509125306890,"number":520,"cdate":1510092369102,"id":"H1mp21ZCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1mp21ZCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Nuclear p-norms for large tensor completion","abstract":"We present algorithms for tensor completion using regularizers based\non tensor nuclear p-norms. For the particular case of the nuclear\ninfinity-norm, we generalize to higher-order tensors the theoretical\nguarantees of the max-norm for matrix completion. From a practical  perspective,\nwe present two algorithms based on stochastic gradients to regularize the canonical\ndecomposition of tensors, and show on large-scale benchmark datasets\nfor knowledge base completion that (a) contrary to what suggested prior results in the literature, the canonical decomposition of tensors can achieve state-of-the-art level performance on FB15K and WN, and (b) our new regularizations reach or outperform the state-of-the-art on task where the canonical decomposition alone is not reaching it. In particular, we provide evidence that the nuclear 3-norm can replace the structures and/or regularization terms of existing link prediction models, and\nleads to better performance.","pdf":"/pdf/7be84d4573e72b0fd14ff9e52459383d07a7660e.pdf","TL;DR":"Efficient regularizers for tensor completion  applied to large scale link prediction in knowledge bases.","paperhash":"anonymous|nuclear_pnorms_for_large_tensor_completion","_bibtex":"@article{\n  anonymous2018nuclear,\n  title={Nuclear p-norms for large tensor completion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mp21ZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper520/Authors"],"keywords":["tensor completion","knowledge base completion","relational learning","tensor norms"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}