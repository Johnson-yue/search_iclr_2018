{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222797653,"tcdate":1511933180490,"number":3,"cdate":1511933180490,"id":"rJ4WHpjgM","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Review","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Claiming much of common intuition around tricks for avoiding gradient issues are incorrect. ","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper makes some bold claims. In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate. \n\nThis is either a very important paper or the analysis is incorrect but it's not my area of expertise. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail. \n\nRegardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside. As such it's not even clear if this is proper for a conference. The authors have already provided several pages worth of additional comments on the website on further related work. I view this as an issue in and of itself. Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow. While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards. I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point. Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics.\n\nIn addition to being quite bold in claims, it is also somewhat confrontational in style. I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary. \n\nSo, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1512222797694,"tcdate":1511823455846,"number":2,"cdate":1511823455846,"id":"Skuwdz5eM","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Review","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official Review for Gradients explode, deep nets are shallow, resnet explained","rating":"5: Marginally below acceptance threshold","review":"Paper Summary:\nThis is a very long paper (55 pages), and I did not read it in its entirety. The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed. To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers. The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?) \n\nThey focus on linear MLPs in the paper for computational simplicity. With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5). They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. \n\nThey then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i).  Expanding out, each term is a product of some of the r_i and some of the identities I. If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth. This is described in Veit et al, 2016. While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100.\n\nThey also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm.\n\nMain Comments:\nThis could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance. In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text. Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis. For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \"easy to train\". But typically easy to train does not involve a specific preprocessing of gradients. Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them. I think the assumptions made in the results should also be clearer. (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?)\n\nI also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100? This doesn't really look anything like networks that are trained in practice. Calling these \"popular architectures\" is misleading. \n\nIn summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1512222797738,"tcdate":1511719872835,"number":1,"cdate":1511719872835,"id":"Byt6QYOxf","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Review","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper needs some major reworking to emphasize the overall narrative introducing the Gradient Scale Coefficient and its potential impact. ","rating":"3: Clear rejection","review":"Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. \n\nReview - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. \n\nThe authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors’ overall points.\n\nWhile the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. \n\nThis paper has a lot of content, but not all of it appears to be relevant to the authors’ central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. \n\n\n\nPros - \n* This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. \n* At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. \n* The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. \n\n\nCons - \n* The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors’ first contribution is hidden among the text presentation of section 2. \n* The paper relies heavily on the supplement to make their central points. \n* It is nearly double the recommended page length with a nearly 30 page supplement\n\n\nMinor issues - \n* Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. \n* The citation style of Authors (YEAR) at times leads to awkward sentence parsing. \n* Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. \n* The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. \n* All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. \n* In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text).\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1510792868925,"tcdate":1510792868925,"number":5,"cdate":1510792868925,"id":"B1TiCL51z","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Comment","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper860/Authors"],"content":{"title":"Legend in figure 5 is incorrect","comment":"The legend located in the top center graph in figure 5 is incorrect. From top to bottom it should be layer-tanh, batch-tanh, layer-ReLU, batch-ReLU, layer-SeLU. This colors match those in figure 3. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1510791352696,"tcdate":1510791352696,"number":4,"cdate":1510791352696,"id":"HkbTdUcyf","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Comment","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper860/Authors"],"content":{"title":"Further related work (1/4)","comment":"Dear Reviewers,\n\nI have recently become aware of two lines of work that are quite relevant to this work: ODE-based ResNets and Mean field analysis of deep networks. I will address both these strands in the next revision of the paper, mostly in section 9 but making references throughout the main body of the paper where appropriate. Below, I give a preview (note that this is split between 3 comments).\n\n\n+++++ Mean field analysis +++++\n\n[2] and its precessor [1] are the closest works to our paper. The authors use infinitely wide networks to study the expected behavior of forward activations and gradients in the initialized state. They identify two distinct regimes, order and chaos, based on whether an infinitesimal perturbation shrinks or grows in expectation respectively as it is propagated forward. This corresponds to the expected qm norm of the layer-Jacobian being smaller or larger than 1 respectively. They show that in the chaotic regime, gradients explode whereas in the ordered regime, gradients vanish. Further, they show that for tanh MLPs the correlation between forward activations corresponding to two different data inputs converges to 1 (`unit limit correlation') in the ordered regime as activations are propagated forward and to some value less than 1 in the chaotic regime. Specifically, in a tanh MLP without biases, in the chaotic regime, the correlation converges to 0.\n\nLike [1,2], much of our analysis relies on the expected behavior of networks in their randomly initialized state. Further, it is clear that the order / chaos dichotomy bears similarity to the exploding gradient problem / collapsing domain problem dichotomy as presented in this paper. However, there are also important differences.\n\n- We argue in this paper that the GSC is a better measure for the presence of pathological exploding or vanishing gradients than the raw scale of the gradient. Using the GSC, we obtain very different regions of order, chaos and stability for popular architectures. For a tanh MLP with no biases, using raw gradients, order is achieved for $\\sigma_w < 1$, stability for $\\sigma_w = 1$ and chaos for $\\sigma_w > 1$. For a tanh MLP with no biases, using the GSC, order is impossible, stability is achieved for $\\sigma_w \\le 1$ and chaos for $\\sigma_w > 1$. For a ReLU MLP with no biases, using raw gradients, order is achieved for $\\sigma_w < \\sqrt{2}$, stability for $\\sigma_w = \\sqrt{2}$ and chaos for $\\sigma_w > \\sqrt{2}$. For a ReLU MLP with no biases, using the GSC, stability is inevitable.\n- While [1] showed that order / chaos corresponds to unit limit correlation / non-unit limit correlation in a tanh MLP, this is not true in general. In a ReLU MLP with no biases and $\\sigma_w > \\sqrt{2}$, infinitesimal noise grows (chaos), yet correlation still converges to 1. Exploding gradient problem / collapsing domain problem is not a strict dichotomy and is thus able to accomodate such cases. \n\nSimilarly, the concepts of unit limit correlation and the collapsing domain problem are not the same. In fact, the former can be seen as a special case of the latter. In a tanh MLP with no bias and $\\sigma_w$ slightly larger than 1, correlation converges to 0 and eventually, gradients explode. Yet the domain can still collapse dramatically in the short term as shown in figure 1 to cause pseudo-linearity. In a tanh MLP with no bias and $\\sigma_w$ very large, again, correlation converges to 0 and gradients explode. However, the tanh layer maps all points close to the corners of the hypercube, which corresponds to domain collapse."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1510791308051,"tcdate":1510791308051,"number":3,"cdate":1510791308051,"id":"B1EqOIqJf","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Comment","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper860/Authors"],"content":{"title":"Further related work (2/4)","comment":"+++++ Mean field analysis continued +++++\n\nWe do not use the assumption of infinite width in our analysis. The only possible exception is that the SSD assumption in proposition 10 can be viewed as implying infinite width. \n\nWhile [2] conjectures that stability is necessary for training very deep networks, our paper provides somewhat contrary evidence. Our two best performing vanilla architectures, SeLU and layer-tanh, are both inside the chaotic regime whereas ReLU, layer-ReLU and tanh, which are all stable, exhibit a higher training classification error. Clearly, chaotic architectures avoid pseudo-linearity. The difference between our experiments and those in [2] is that we allowed the step size to vary between layers. This had a large impact, as can be seen in table 2. We believe that our results underscore the importance of choosing appropriate step sizes when comparing the behavior of different neural architectures or training algorithms in general.\n\nIn section 4, we present a rigorous argument for the harmful nature of exploding gradients, and thus of chaos, at high depth. \n\nIt is not clear a priori whether a unit limit correlation is harmful for accuracy. After all, correlation information is a rather small part of the information present in the data, so the remaining information might be sufficient for learning. In section 6, we show how pseudo-linearity can arise under unit limit correlation and explain how it can harm expressivity and thus accuracy."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1510791323621,"tcdate":1510791099604,"number":2,"cdate":1510791099604,"id":"Bk46DI9yG","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Comment","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper860/Authors"],"content":{"title":"Further related work (3/4)","comment":"+++++ Mean field analysis continued +++++\n\n[3] uses a framework similar to [1,2] to propose to combat gradient growth by downscaling the weights on the residual path in a ResNet. This corresponds to increased dilution, which indeed reduces gradient growth as shown in section 7. However, we also show in proposition 10 that the reduction achievable in this way may be limited. [3] also proposes to combat the exploding gradient problem by changing the width of intermediate layers. Our analysis in section 4.4 strongly suggests that this is not effective in reducing the growth of the GSC. [3] concludes that changing the width combats the exploding gradient problem because they implicitly assume that the pathology of exploding gradients is determined by the scale of individual components of the gradient vector rather than the length of the entire vector or the GSC. They do not justify this assumption. We propose the GSC as a standard for assessing pathological exploding gradients to avoid such ambiguity.\n\n\n[1] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, S. Ganguli. Exponential expressivity in deep neural networks through transient chaos. NIPS 2016. https://arxiv.org/abs/1606.05340v1\n\n[2] S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein. Deep information propagation. ICLR 2017. https://openreview.net/forum?id=H1W1UN9gg\n\n[3] Anonymous. Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion. ICLR 2018. https://openreview.net/forum?id=rJGY8GbR-\n\n\nPS: There seems to be another relevant paper: \"Mean Field Residual Networks: On the Edge of Chaos\" that will be published at NIPS this year. Unfortunately, I have been unable to obtain a copy so far. If you have a link to this paper, I would love to have it."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1510791235834,"tcdate":1510791036884,"number":1,"cdate":1510791036884,"id":"B1HYDI5kz","invitation":"ICLR.cc/2018/Conference/-/Paper860/Official_Comment","forum":"HkpYwMZRb","replyto":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference/Paper860/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper860/Authors"],"content":{"title":"Further related work (4/4)","comment":"+++++ ODE-based ResNets +++++\n\n\nRecently, [1-4] proposed ResNet architectures inspired by dynamical systems and numerical methods for ordinary differential equations. The central claim is that these architectures are stable at arbitrary depth, i.e. both forward activations and gradients (and hence GSC) are bounded as depth goes to infinity. They propose four practical strategies for building and training ResNets: (a) ensuring that residual and skip functions compute vectors orthogonal to each other by using e.g. skew-symmetric weight matrices (b) ensuring that the Jacobian of the skip function has eigenvalues with negative real part by using e.g. weight matrices factorized as -C^TC (c) scaling each residual function by 1/B where B is the number of residual blocks in the network and (d) regularizing weights in successive blocks to be similar via a fusion penalty.\n\n\narchitecture             GSC (base 10 log)            GSC dilution-corrected  (base 10 log)\nbatch-ReLU (i)             0.337                               4.23\nbatch-ReLU (ii)            0.329                               4.06\nbatch-ReLU (iii)           6.164                              68.37\nbatch-ReLU (iv)            0.313                               7.22\nlayer-tanh (i)             0.136                               2.17\nlayer-tanh (ii)            0.114                               1.91\nlayer-tanh (iii)           3.325                               5.46\nlayer-tanh (iv)            0.143                               2.31\nTable 1\n\n\nWe evaluated those strategies empirically. In table 1, we show the value of the GSC across the network for 8 different architectures in their initialized state applied to Gaussian noise (see section 9.9.2 for details). All architectures use residual blocks containing a single normalization layer, a single nonlinearity layer and a single linear layer. We initialize the linear layer in four different ways: (i) Gaussian initialization, (ii) skew-symmetric initialization, (iii) initialization as -C^TC where C is Gaussian initialized and (iv) Gaussian initialization where weight matrices in successive blocks have correlation 0.5. Initializations (ii), (iii) and (iv) mimic strategies (a), (b) and (d) respectively. To enable the comparison of the four initialization styles, we normalize each weight matrix to have a unit qm norm. We study all four initializations for both batch-ReLU and layer-tanh. \n\nInitialization (ii) improves slightly over initialization (i). This is expected given theorem 3. One of the key assumptions is that skip and residual function be orthogonal in expectation. While initialization (i) achieves this, under (ii), the two functions are orthogonal with probability 1. \n\nInitialization (iii) has gradients that grow much faster than initialization (i). On the one hand, this is surprising as [2] states that eigenvalues with negative real parts in the residual Jacobian supposedly slow gradient growth. On the other hand, it is not surprising because introducing correlation between the residual and skip path breaks the conditions of theorem 3. \n\nInitialization (iv) performs comparably to initialization (i) in reducing gradient growth, but requires a larger amount of dilution to achieve this result. Again, introducing correlation between successive blocks and thus between skip and residual function breaks the conditions of theorem 3 and weakens the power of dilution.\n\nWhile we did not investigate the exact architectures proposed in [2,3], our results show that more theoretical and empirical evaluation is necessary to determine whether architectures based on (a), (b) and (d) are indeed capable of increasing stability. Of course, those architectures might still confer benefits in terms of e.g. inductive bias or regularization.\n\nFinally, strategy (c), the scaling of either residual and/or skip function with constants is a technique already widely used in regular ResNets. In fact, our study suggests that in order to bound the GSC at arbitrary depth in a regular ResNet, it is sufficient to downscale each residual function by only 1/sqrt(B) instead of 1/B as [1-4] suggest. \n\n\n[1] E. Haber, L. Ruthotto, E. Holtham. Learning Across Scales - Multiscale Methods for Convolution Neural Networks. arXiv 2017. https://xtract.ai/wp-content/uploads/2017/05/Learning-Across-Scales.pdf\n\n[2] E. Haber, L. Ruthotto. Stable Architectures for Deep Neural Networks. arXiv 2017. https://arxiv.org/abs/1705.03341\n\n[3] B. Chang, L. Meng, E. Haber, L. Ruthotto, D. Begert, E. Holtham. Reversible Architectures for Arbitrarily Deep Residual Neural Networks. arXiv 2017. https://export.arxiv.org/abs/1709.03698\n\n[4] Anonymous. Multi-level residual networks from dynamical systems view. ICLR 2018 submission. https://openreview.net/forum?id=SyJS-OgR-\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]}},{"tddate":null,"ddate":null,"tmdate":1509739062767,"tcdate":1509136261164,"number":860,"cdate":1509739060104,"id":"HkpYwMZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkpYwMZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gradients explode - Deep Networks are shallow - ResNet explained","abstract":"Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities ``solve'' the exploding gradient problem, we show that this is not the case and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the {\\it collapsing domain problem}, which can arise in architectures that avoid exploding gradients. \n\nResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks, which we show is a consequence of a surprising mathematical property. By noticing that {\\it any neural network is a residual network}, we devise the {\\it residual trick}, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.","pdf":"/pdf/8e11d7224b4edc873e9cac08b303d76bbead6cd8.pdf","TL;DR":"We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.","paperhash":"anonymous|gradients_explode_deep_networks_are_shallow_resnet_explained","_bibtex":"@article{\n  anonymous2018gradients,\n  title={Gradients explode - Deep Networks are shallow - ResNet explained},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkpYwMZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper860/Authors"],"keywords":["deep learning","MLP","ResNet","residual network","exploding gradient problem","vanishing gradient problem","effective depth","batch normalization","covariate shift"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}