{"notes":[{"tddate":null,"ddate":null,"tmdate":1512248713481,"tcdate":1512248713481,"number":2,"cdate":1512248713481,"id":"ryW5rcl-f","invitation":"ICLR.cc/2018/Conference/-/Paper714/Public_Comment","forum":"BkJ3ibb0-","replyto":"BkJ3ibb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"CW is an optimization based attack","comment":"In your appendix you claim the combined model is hard to attack, but I suspect that might not be the case. \n\n1. CW is an optimization based attack. \n\n2. If you just set up the CW optimization attack, and find some local minima for z* that corresponds to an adversarial attack -- I suspect it might be pretty close to the z* you converge on after a few steps of GD. Perhaps worth a shot trying to just combine the two models and add ||G(z)-x|| as another term in the optimization objective. I suspect CW would work pretty well then. \n\nminimize CW loss function + 0.1*||z*-x|| \n\nsubject y=f(x)\n              z*=G(z) or something like this. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222726647,"tcdate":1511878963676,"number":3,"cdate":1511878963676,"id":"rJOVWxjez","invitation":"ICLR.cc/2018/Conference/-/Paper714/Official_Review","forum":"BkJ3ibb0-","replyto":"BkJ3ibb0-","signatures":["ICLR.cc/2018/Conference/Paper714/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A novel idea with room for future work. ","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM). They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation. In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution. The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator. \n\nUtilizing a trained GAN, the authors propose the following defense at inference time. Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD. Then apply any classifier trained on the true distribution on the resulting x* = G(z*). \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective. In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model. Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks. In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks. However, it is not clear to me that the method is invulnerable to novel white-box attacks. In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream.\n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value). It is novel and should generate further research with respect to understanding its vulnerabilities more completely. \n\nMinor Comments:\nThe sentence starting “Unless otherwise specified…” at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed.  \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222726690,"tcdate":1511768264561,"number":2,"cdate":1511768264561,"id":"By-CxBKgz","invitation":"ICLR.cc/2018/Conference/-/Paper714/Official_Review","forum":"BkJ3ibb0-","replyto":"BkJ3ibb0-","signatures":["ICLR.cc/2018/Conference/Paper714/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting but hard to conclude decisively from the current experiments","rating":"5: Marginally below acceptance threshold","review":"This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. The GAN is a WGAN trained on the train set (only to keep the generator). The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x.\n\n+ The paper is easy to follow.\n+ It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses.\n+ Simple/directly applicable approach that seems to work experimentally, but\n- A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set.\n- Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST.\n- Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image.\n? MagNet results were very often worse than no defense in Table 4, could you comment on that?\n- In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222726734,"tcdate":1511648955344,"number":1,"cdate":1511648955344,"id":"BympCwwgf","invitation":"ICLR.cc/2018/Conference/-/Paper714/Official_Review","forum":"BkJ3ibb0-","replyto":"BkJ3ibb0-","signatures":["ICLR.cc/2018/Conference/Paper714/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.  Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).  This projected input is then used to produce the classification probabilities.  The authors test their method on various adversarially constructed inputs (with varying degrees of noise). \n\nQuestions/Comments:\n\n- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.  Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?  If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?  \n\n- Is there anything special about the GAN approach, versus other generative approaches? \n\n- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?  Is that what is meant by the \"defense network\" (in experiments bullet 2)?\n\n- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? \n\nQuality: The method appears to be technically correct.\n\nClarity: This paper clearly written; both method and experiments are presented well. \n\nOriginality: I am not familiar enough with adversarial learning to assess the novelty of this approach. \n\nSignificance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509926415577,"tcdate":1509926415577,"number":1,"cdate":1509926415577,"id":"SkdMUQaAZ","invitation":"ICLR.cc/2018/Conference/-/Paper714/Public_Comment","forum":"BkJ3ibb0-","replyto":"BkJ3ibb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Testing on Datasets Other than MNIST/Adversarial Examples of Generator","comment":"Have you tested your method on other datasets? I wonder if it works with datasets such as CIFAR. \n\nMoreover, it's not clear whether this method can defend against existing attacks, without introducing new vulnerabilities. Here are some possible new attack methods:\n\n1- The generator can certainly output examples that are adversarial for the classifier. Hence, the attacker only needs to find out such examples and perturb the input image to make it similar to them.\n\n2- The attacker can target the minimization block, which uses \"L steps of Gradient Descent.\" By forcing it to output a wrong set of Z_L, the rest of the algorithm (combination of generator/classifier) becomes ineffective, i.e., the minimization block can be the bottleneck. \n\n3- The algorithm takes as input a seed, along with the image. Since for a given seed, the random number generator is deterministic, the attacker can test different seeds and use the one for which the algorithm fails. This attack may work even without perturbing the image. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739145524,"tcdate":1509133222929,"number":714,"cdate":1509739142859,"id":"BkJ3ibb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkJ3ibb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models","abstract":"In recent years, neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images and, at inference time, finds a close output to a given image. This output will not contain the adversarial changes and is fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.","pdf":"/pdf/d22adf53a90ce42685ff58fdc09c968bd93a4e87.pdf","TL;DR":"Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.","paperhash":"anonymous|defensegan_protecting_classifiers_against_adversarial_attacks_using_generative_models","_bibtex":"@article{\n  anonymous2018defense-gan:,\n  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkJ3ibb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper714/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}