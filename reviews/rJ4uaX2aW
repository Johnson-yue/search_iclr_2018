{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222683327,"tcdate":1511839305140,"number":3,"cdate":1511839305140,"id":"S1b88I5xM","invitation":"ICLR.cc/2018/Conference/-/Paper55/Official_Review","forum":"rJ4uaX2aW","replyto":"rJ4uaX2aW","signatures":["ICLR.cc/2018/Conference/Paper55/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes a training algorithm based on Layer-wise Adaptive Rate Scaling (LARS) to overcome the optimization difficulties for training with large batch size. The authors use a linear scaling and warm-up scheme to train AlexNet on ImageNet. The results show promising performance when using a relatively large batch size. The presented method is interesting. However, the experiments are poorly organized since some necessary descriptions and discussions are missing. ","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a training algorithm based on Layer-wise Adaptive Rate Scaling (LARS) to overcome the optimization difficulties for training with large batch size. The authors use a linear scaling and warm-up scheme to train AlexNet on ImageNet. The results show promising performance when using a relatively large batch size. The presented method is interesting. However, the experiments are poorly organized since some necessary descriptions and discussions are missing. My detailed comments are as follows.\n\nContributions：\n\n1.\tThe authors propose a training algorithm based LARS with the adaptive learning rate for each layer, and train the AlexNet and ResNet-50 to a batch size of 16K. \n2.\tThe training method shows stable performance and helps to avoid gradient vanishing or exploding.\n\nWeak points:\n\nThe training algorithm does not overcome the optimization difficulties when the batch size becomes larger (e.g. 32K), where the training becomes unstable, and the training based on LARS and warm-up can’t improve the accuracy compared to the baselines. \n\nSpecific comments: \n\n1.\tIn Algorithm 1, how to choose $ \\eta $ and $ \\beta $ in the experiment?\n2.\tUnder the line of Equation (3), $ \\nabla L(x_j, w_{t+1}) \\approx L(x_j, w_{t}) $ should be $ \\nabla L(x_j, w_{t+1}) \\approx \\nabla L(x_j, w_{t}) $.\n3.\tHow can the training algorithm based on LARS improve the generalization for the large batch? \n4.\tIn the experiments, what is the parameter iter_size? How to choose it?\n5.\tIn the experiments, no descriptions and discussions are given for Table 3, Figure 4, Table 4, Figure 5, Table 5 and Table 6. The authors should give more discussions on these tables and figures. Furthermore, the captions of these tables and figures confusing.\n6.\tOn page 4, there is a statement “The ratio is high during the initial phase, and it is rapidly decreasing after few epochs (see Figure 2).” This is quite confusing, since Figure 2 is showing the change of learning rates w.r.t. training epochs.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling","abstract":"A common way to speed up training of large convolutional networks is to add  computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. However,  training with a large batch  often results in lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome these optimization difficulties, we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled AlexNet  and ResNet-50 to a batch size of 16K.","pdf":"/pdf/12ec07d022c4fa9608466e96dc6e70e1c654e71d.pdf","TL;DR":"A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.","paperhash":"anonymous|large_batch_training_of_convolutional_networks_with_layerwise_adaptive_rate_scaling","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ4uaX2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper55/Authors"],"keywords":["large batch","LARS","adaptive rate scaling"]}},{"tddate":null,"ddate":null,"tmdate":1512222683364,"tcdate":1511826233427,"number":2,"cdate":1511826233427,"id":"Sk-rQ7qxf","invitation":"ICLR.cc/2018/Conference/-/Paper55/Official_Review","forum":"rJ4uaX2aW","replyto":"rJ4uaX2aW","signatures":["ICLR.cc/2018/Conference/Paper55/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes a heuristic for layer-wise learning rate selection in convolutional networks. The contribution is relatively minor and is not evaluated with sufficient depth.","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a new approach to determine learning late for convolutional neural networks. It starts from observation that for batch learning with a fixed number of epochs, the accuracy drops when the batch size is too large.  Assuming that the number or epochs and batch size are fixed, the contribution of the paper is a heuristic that assigns different learning late to each layer of a network depending on a ratio of the norms of weights and gradients in a layer.  The experimental results show that the proposed heuristic helps AlexNet and ResNet end up in a larger accuracy on ImageNet data.\n  Positives:\n- the proposed approach is intuitively justified\n- the experimental results are encouraging\n  Negatives:\n- the methodological contribution is minor\n- no attempt is made to theoretically justify the proposed heuristic\n- the method introduces one or two new hyperparameters and it is not clear from the experimental results what overhead is this adding to network training\n- the experiments are done only on a single data set, which is not sufficient to establish superiority of an approach\n  Suggestions:\n- consider using different abbreviation (LARS is used for least-angle regression) \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling","abstract":"A common way to speed up training of large convolutional networks is to add  computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. However,  training with a large batch  often results in lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome these optimization difficulties, we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled AlexNet  and ResNet-50 to a batch size of 16K.","pdf":"/pdf/12ec07d022c4fa9608466e96dc6e70e1c654e71d.pdf","TL;DR":"A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.","paperhash":"anonymous|large_batch_training_of_convolutional_networks_with_layerwise_adaptive_rate_scaling","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ4uaX2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper55/Authors"],"keywords":["large batch","LARS","adaptive rate scaling"]}},{"tddate":null,"ddate":null,"tmdate":1512222683403,"tcdate":1511789235806,"number":1,"cdate":1511789235806,"id":"ry33G5FxM","invitation":"ICLR.cc/2018/Conference/-/Paper55/Official_Review","forum":"rJ4uaX2aW","replyto":"rJ4uaX2aW","signatures":["ICLR.cc/2018/Conference/Paper55/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A layer-wise learning rate is proposed. Some state-of-the-art baselines are missing in comparison. ","rating":"5: Marginally below acceptance threshold","review":"This paper provides an optimization approach for large batch training of CNN with layer-wise adaptive learning rates. \nIt starts from the observation that the ratio between the L2-norm of parameters and that of gradients on parameters varies\nsignificantly in the optimization,  and then introduce a local learning rate to consider this observation for a more stable and efficient optimization. Experimental results show improvements compared with the state-of-the-art algorithm.\n\nReview:\n(1) Pros\nThe proposed optimization method considers the dynamic self-adjustment of the learning rate in the optimization based on the ratio between the L2-norm of parameters and that of gradients on parameters  when the batch size increases, and shows improvements in experiments compared with previous methods.\n\n(2) Cons\ni) LR \"warm-up\" can mitigate the unstable training in the initial phase and the proposed method is also motivated by the stability but uses a different approach. However, it seems that the authors also combine with LR \"warm-up\" in your proposed method in the experimental part, e.g., Table 3. So does it mean that the proposed method cannot handle the problem in general?\n\nii) There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. In fact, as can be seen in the Algorithm 1, this coefficient can be as an independent hyper-parameter (even is put with the global learning rate together as one fix term).\n\niii) In the section 6, when increase the training steps, experiments compared with previous methods should be implemented since they can also get better results with more epochs.\n\niv) Writing should be improved, e.g., the first paragraph in section 6. Some parts are confusing, for example, the authors claim that they use initial LR=0.01, but in Table 1(a) it is 0.02.  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling","abstract":"A common way to speed up training of large convolutional networks is to add  computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. However,  training with a large batch  often results in lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome these optimization difficulties, we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled AlexNet  and ResNet-50 to a batch size of 16K.","pdf":"/pdf/12ec07d022c4fa9608466e96dc6e70e1c654e71d.pdf","TL;DR":"A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.","paperhash":"anonymous|large_batch_training_of_convolutional_networks_with_layerwise_adaptive_rate_scaling","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ4uaX2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper55/Authors"],"keywords":["large batch","LARS","adaptive rate scaling"]}},{"tddate":null,"ddate":null,"tmdate":1509739510434,"tcdate":1508814187827,"number":55,"cdate":1509739507782,"id":"rJ4uaX2aW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJ4uaX2aW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling","abstract":"A common way to speed up training of large convolutional networks is to add  computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. However,  training with a large batch  often results in lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome these optimization difficulties, we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled AlexNet  and ResNet-50 to a batch size of 16K.","pdf":"/pdf/12ec07d022c4fa9608466e96dc6e70e1c654e71d.pdf","TL;DR":"A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.","paperhash":"anonymous|large_batch_training_of_convolutional_networks_with_layerwise_adaptive_rate_scaling","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ4uaX2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper55/Authors"],"keywords":["large batch","LARS","adaptive rate scaling"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}