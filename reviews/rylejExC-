{"notes":[{"tddate":null,"ddate":null,"tmdate":1511767409002,"tcdate":1511767344387,"number":1,"cdate":1511767344387,"id":"HJu46VFxG","invitation":"ICLR.cc/2018/Conference/-/Paper243/Public_Comment","forum":"rylejExC-","replyto":"rylejExC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Very interesting paper and experimental results. The theoretical analysis needs a bit more work. ","comment":"The paper is addressing a very interesting problem with imminent importance and industrial impact. \n\nIn the proof of the unbiased estimator for gradient: that is the proof of theorem 1, two lines above equation (10), Z^(l) inside \\sigma'(Z^(l)) also depends on the random sampling, no? \n\nThe situation is similar to doubly stochastic gradient descent: \nDai et al. Scalable Kernel Methods via Doubly Stochastic Gradients, NIPS 2016\nhttps://arxiv.org/pdf/1407.5599.pdf\nLine 5 Algorithm 1. \nThe analysis of the paper is able to take this source of bias into account. \n\nThe proof of the current paper could also be fixed accordingly. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Training of Graph Convolutional Networks","abstract":"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes nodes' representation recursively from their neighbors, making the receptive field size grow exponentially with the number of layers.  Previous attempts on reducing the receptive field size by subsampling neighbors do not have any convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop a preprocessing strategy and two control variate based algorithms to further reduce the receptive field size. Our algorithms are guaranteed to converge to GCN's local optimum regardless of the neighbor sampling size. Empirical results show that our algorithms have a similar convergence speed per epoch with the exact algorithm even using only two neighbors per node. The time consumption of our algorithm on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.","pdf":"/pdf/0c81f261ad21439b140dca9522d99bd624236469.pdf","TL;DR":"A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.","paperhash":"anonymous|stochastic_training_of_graph_convolutional_networks","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Training of Graph Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rylejExC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper243/Authors"],"keywords":["Graph convolutional networks","stochastic gradient descent","variance reduction","control variate"]}},{"tddate":null,"ddate":null,"tmdate":1512222593886,"tcdate":1511726728086,"number":3,"cdate":1511726728086,"id":"S1g5R5Ogz","invitation":"ICLR.cc/2018/Conference/-/Paper243/Official_Review","forum":"rylejExC-","replyto":"rylejExC-","signatures":["ICLR.cc/2018/Conference/Paper243/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Existing training algorithms for graph convolutional nets are slow. This paper develops new novel methods, with a nice mix of theory, practicalities and experiments.","rating":"7: Good paper, accept","review":"Existing training algorithms for graph convolutional nets are slow. This paper develops new novel methods, with a nice mix of theory, practicalities, and experiments.\n\nLet me caution that I am not familiar with convolutional nets applied to graph data.\n\nClearly, the existing best algorithm - neighborhood sampling is slow as well as not theoretically sound. This paper proposes two key ideas - preprocessing and better sampling based on historical activations. The value of these ideas is demonstrated very well via theoretical and experimental analysis. I have skimmed through the theoretical analysis. They seem fine, but I haven't carefully gone through the details in the appendices.\n\nAll the nets considered in the experiments have two layers. The role of preprocessing to add efficiency is important here. It would be useful to know how much the training speed will suffer if we use three or more layers, say, via one more experiment on a couple of key datasets. This will help see the limitations of the ideas proposed in this paper.\n\nIn subsection 4.3 the authors prove reduced variance under certain assumptions. While I can see that this is done to make the analysis simple, how well does this analysis correlate with what is seen in practice? For example, how well does the analysis results given in Table 2 correlate with the standard deviation numbers of Figure 5 especially when comparing NS+PP and CV+PP?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Training of Graph Convolutional Networks","abstract":"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes nodes' representation recursively from their neighbors, making the receptive field size grow exponentially with the number of layers.  Previous attempts on reducing the receptive field size by subsampling neighbors do not have any convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop a preprocessing strategy and two control variate based algorithms to further reduce the receptive field size. Our algorithms are guaranteed to converge to GCN's local optimum regardless of the neighbor sampling size. Empirical results show that our algorithms have a similar convergence speed per epoch with the exact algorithm even using only two neighbors per node. The time consumption of our algorithm on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.","pdf":"/pdf/0c81f261ad21439b140dca9522d99bd624236469.pdf","TL;DR":"A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.","paperhash":"anonymous|stochastic_training_of_graph_convolutional_networks","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Training of Graph Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rylejExC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper243/Authors"],"keywords":["Graph convolutional networks","stochastic gradient descent","variance reduction","control variate"]}},{"tddate":null,"ddate":null,"tmdate":1512222593923,"tcdate":1511718209337,"number":2,"cdate":1511718209337,"id":"B1FrpdOeM","invitation":"ICLR.cc/2018/Conference/-/Paper243/Official_Review","forum":"rylejExC-","replyto":"rylejExC-","signatures":["ICLR.cc/2018/Conference/Paper243/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting but not enough","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method to speed up the training of graph convolutional networks, which are quite slow for large graphs. The key insight is to improve the estimates of the average neighbor activations (via neighbor sampling) so that we can either sample less neighbors or have higher accuracy for the same number of sampled neighbors. The idea is quite simple: estimate the current average neighbor activations as a delta over the minibatch running average. I was hoping the method would also include importance sampling, but it doesn’t. The assumption that activations in a graph convolution are independent Gaussians is quite odd (and unproven). \n\nQuality: Statistically, the paper seems sound. There are some odd assumptions (independent Gaussian activations in a graph convolution embedding?!?) but otherwise the proposed methodology is rather straightforward. \n\nClarity: It is well written and the reader is able to follow most of the details. I wish the authors had spent more time discussing the independent Gaussian assumption, rather than just arguing that a graph convolution (where units are not interacting through a simple grid like in a CNN) is equivalent to the setting of Wang and Manning (I don’t see the equivalence). Wang and Manning are looking at MLPs, not even CNNs, which clearly have more independent activations than a CNN or a graph convolution. \n\nSignificance: Not very significant. The problem of computing better averages for a specific problem (neighbor embedding average) seems a bit too narrow. The solution is straightforward, while some of the approximations make some odd simplifying assumptions (independent activations in a convolution, infinitesimal learning rates). \n\nTheorem 2 is not too useful, unfortunately: Showing that the estimated gradient is asymptotically unbiased with learning rates approaching zero over Lipchitz functions does not seem like an useful statement. Learning rates will never be close enough to zero (specially for large batch sizes). And if the running activation average converges to the true value, the training is probably over. The method should show it helps when the values are oscillating in the early stages of the training, not when the training is done near the local optimum.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Training of Graph Convolutional Networks","abstract":"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes nodes' representation recursively from their neighbors, making the receptive field size grow exponentially with the number of layers.  Previous attempts on reducing the receptive field size by subsampling neighbors do not have any convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop a preprocessing strategy and two control variate based algorithms to further reduce the receptive field size. Our algorithms are guaranteed to converge to GCN's local optimum regardless of the neighbor sampling size. Empirical results show that our algorithms have a similar convergence speed per epoch with the exact algorithm even using only two neighbors per node. The time consumption of our algorithm on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.","pdf":"/pdf/0c81f261ad21439b140dca9522d99bd624236469.pdf","TL;DR":"A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.","paperhash":"anonymous|stochastic_training_of_graph_convolutional_networks","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Training of Graph Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rylejExC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper243/Authors"],"keywords":["Graph convolutional networks","stochastic gradient descent","variance reduction","control variate"]}},{"tddate":null,"ddate":null,"tmdate":1512222593963,"tcdate":1511094837764,"number":1,"cdate":1511094837764,"id":"rJA4cxJlf","invitation":"ICLR.cc/2018/Conference/-/Paper243/Official_Review","forum":"rylejExC-","replyto":"rylejExC-","signatures":["ICLR.cc/2018/Conference/Paper243/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A new training method of graph convolutional networks. Good but there are some errors.","rating":"3: Clear rejection","review":"This paper proposes a new training method for graph convolutional networks. The experimental results look interesting. However, this paper has some issues.\n\nThis paper is hard to read. There are some undefined or multi-used notations. For instance, sigma is used for two different meanings: an activation function and variance. Some details that need to be explained are omitted. For example, what kind of dropout is used to obtain the table and figures in Section 5? Forward and backward propagation processes are not clearly explained\n\nIn section 4.2, it is not clear why we have to multiply sqrt{D}. Why should we make the variance from dropout sigma^2? \n\nProposition 1 is wrong. First, \\|A\\|_\\infty should be max_{ij} |A_ij| not A_{ij}. Second, there is no order between \\|AB\\|_\\infty and \\|A\\|_\\infty \\|B\\|_\\infty. When A=[1 1] and B is the transpose matrix of A, \\|AB\\|_\\infty =2 and \\|A\\|_\\infty \\|B\\|_\\infty = 1. When, A’=[1 -1] and B is the same matrix defined just before, \\|A’ B \\|_\\infty = 0 and \\|A’\\|_\\infty \\|B\\|_\\infty =1. So, both \\|AB\\|_\\infty \\le \\|A\\|_\\infty \\|B\\|_\\infty and \\|AB\\|_\\infty \\ge \\|A\\|_\\infty \\|B\\|_\\infty are not true. I cannot believe the proof of Theorem 2.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic Training of Graph Convolutional Networks","abstract":"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes nodes' representation recursively from their neighbors, making the receptive field size grow exponentially with the number of layers.  Previous attempts on reducing the receptive field size by subsampling neighbors do not have any convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop a preprocessing strategy and two control variate based algorithms to further reduce the receptive field size. Our algorithms are guaranteed to converge to GCN's local optimum regardless of the neighbor sampling size. Empirical results show that our algorithms have a similar convergence speed per epoch with the exact algorithm even using only two neighbors per node. The time consumption of our algorithm on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.","pdf":"/pdf/0c81f261ad21439b140dca9522d99bd624236469.pdf","TL;DR":"A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.","paperhash":"anonymous|stochastic_training_of_graph_convolutional_networks","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Training of Graph Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rylejExC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper243/Authors"],"keywords":["Graph convolutional networks","stochastic gradient descent","variance reduction","control variate"]}},{"tddate":null,"ddate":null,"tmdate":1509739410143,"tcdate":1509079784199,"number":243,"cdate":1509739407486,"id":"rylejExC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rylejExC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stochastic Training of Graph Convolutional Networks","abstract":"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes nodes' representation recursively from their neighbors, making the receptive field size grow exponentially with the number of layers.  Previous attempts on reducing the receptive field size by subsampling neighbors do not have any convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop a preprocessing strategy and two control variate based algorithms to further reduce the receptive field size. Our algorithms are guaranteed to converge to GCN's local optimum regardless of the neighbor sampling size. Empirical results show that our algorithms have a similar convergence speed per epoch with the exact algorithm even using only two neighbors per node. The time consumption of our algorithm on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.","pdf":"/pdf/0c81f261ad21439b140dca9522d99bd624236469.pdf","TL;DR":"A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.","paperhash":"anonymous|stochastic_training_of_graph_convolutional_networks","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic Training of Graph Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rylejExC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper243/Authors"],"keywords":["Graph convolutional networks","stochastic gradient descent","variance reduction","control variate"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}