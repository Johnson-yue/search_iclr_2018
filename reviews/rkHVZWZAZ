{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708967,"tcdate":1511823122752,"number":3,"cdate":1511823122752,"id":"rksMwz9xG","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice integration of recent deep RL advances with decent empirical results","rating":"7: Good paper, accept","review":"This paper presents a new reinforcement learning architecture called Reactor by combining various improvements in\ndeep reinforcement learning algorithms and architectures into a single model. The main contributions of the paper\nare to achieve a better bias-variance trade-off in policy gradient updates, multi-step off-policy updates with\ndistributional RL, and prioritized experience replay for transition sequences. The different modules are integrated\nwell and the empirical results are very promising. The experiments (though limited to Atari) are well carried out and\nthe evaluation is performed on both sample efficiency and training time.\n\nPros:\n1. Nice integration of several recent improvements in deep RL, along with a few novel tricks to improve training.\n2. The empirical results on 57 Atari games are impressive, in terms of final scores as well as real-time training speed.\n\nCons:\n1. Reactor is still less sample-efficient than Rainbow, with significantly lower scores after 200M frames. While the\nreactor trains much faster, it does use more parallel compute, so the comparison with Rainbow on wall clock time is\n not entirely fair. Would a distributed version of Rainbow perform better in this respect?\n2. Empirical comparisons are restricted to the Atari domain. The conclusions of the paper will be much stronger if\nresults are also shown on other environments like Mujoco/Vizdoom/Deepmind Lab.\n3. Since the paper introduces a few new ideas like prioritized sequence replay, it would help if a more detailed analysis\n was performed on the impact of these individual schemes, even if in a model simpler than the Reactor. For instance, one could investigate the impact of prioritized sequence replay in models like multi-step DQN or recurrent DQN. This will help us understand  the impact of each of these ideas in a more comprehensive fashion.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/79176417ac2023cf22df0fb5463b72a363003b3c.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1512222709003,"tcdate":1511804746530,"number":2,"cdate":1511804746530,"id":"r1MU1AtlG","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting work with several contributions and large experiments with some but not all recent approaches","rating":"7: Good paper, accept","review":"This paper proposes a novel reinforcement learning algorithm containing several contributions made by the authors: 1) a policy gradient algorithm that uses value function estimates to improve the policy gradient, 2) a distributed multi-step off-policy algorithm to estimate the value function, 3) an experience replay buffer mechanism that can handle sequences and (4) a distributed architecture, where threads are dedicated to either learning or interracting with the environment. Most contributions consist in improvements to handle multi-step trajectories instead of single step transitions. The resulting algorithm is evaluated on the ATARI domain and shown to outperform other similar algorithms, both in terms of score and training time. Ablation studies are also performed to study the interest of the 4 contributions. \n\nI find the paper interesting. It is also well written and reasonably clear. The experiments are large, although I was disappointed that PPO was not included in the evaluation, as this algorithm also trains much faster than other algorithms.\n\nquality\n+ several contributions\n+ impressive experiments\n\nclarity\n- I found the replay buffer not as clear as the other parts of the paper.\n. run time comparison: source of the code for the baseline methods?\n+ ablation study showing the merits of the different contributions\n- Methods not clearly labeled. For example, what is the difference between Reactor and Reactor 500M?\n\noriginality\n+ 4 contributions\n\nsignificance\n+ important problem, very active area of research\n+ comparison to very recent algorithms\n- but no PPO in the evaluation","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/79176417ac2023cf22df0fb5463b72a363003b3c.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1512222709041,"tcdate":1511803558125,"number":1,"cdate":1511803558125,"id":"SJRs56Ylz","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising \"Rainbow-like\" combination of deep RL techniques, unfortunately with no proper comparison to Rainbow","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a novel reinforcement learning algorithm (« The Reactor ») based on the combination of several improvements to DQN: a distributional version of Retrace, a policy gradient update rule called beta-LOO aiming at variance reduction, a variant of prioritized experience replay for sequences, and a parallel training architecture. Experiments on Atari games show a significant improvement over prioritized dueling networks in particular, and competitive performance compared to Rainbow, at a fraction of the training time.\n\nThere are definitely several interesting and meaningful contributions in this submission, and I like the motivations behind them. They are not groundbreaking (essentially extending existing techniques) but are still very relevant to current RL research.\n\nUnfortunately I also see it as a step back in terms of comparison to other algorithms. The recent Rainbow paper finally established a long overdue clear benchmark on Atari. We have seen with the « Deep Reinforcement Learning that Matters » paper how important (and difficult) it is to properly compare algorithms on deep RL problems. I assume that this submission was mostly written before Rainbow came out, and that comparisons to Rainbow were hastily added just before the ICLR deadline: this would explain why they are quite limited, but in my opinion it remains a major issue, which is the main reason why I am advocating for rejection.\n\nMore precisely, focusing on the comparison to Rainbow which is the main competitor here, my concerns are the following:\n- There is almost no discussion on the differences between Reactor and Rainbow (actually the paper lacks a « related work » section). In particular Rainbow also uses a version of distributional multi-step, which as far as I can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « off-policyness » of the replay data), but still seems to work well on Atari.\n- Rainbow is not distributed. This was a deliberate choice by its authors to focus on algorithmic comparisons. However, it seems to me that it could benefit from a parallel training scheme like Reactor’s. I believe a comparison between Reactor and Rainbow needs to either have them both parallelized or none of them (especially for a comparison on time efficiency like in Fig. 2)\n- Rainbow uses the traditional feedforward DQN architecture while Reactor uses a recurrent network. It is not clear to which extent this has an impact on the results.\n- Rainbow was stopped at 200M steps, at which point it seems to be overall superior to Reactor at 200M steps. The results as presented here emphasize the superiority of Reactor at 500M steps, but a proper comparison would require Rainbow results at 500M steps as well.\n\nIn addition, although I found most of the paper to be clear enough, some parts were confusing to me, in particular:\n- « multi-step distributional Bellman operator » in 3.2: not clear exactly what the target distribution is. If I understand correctly this is the same as the Rainbow extension, but this link is not mentioned.\n- 3.4.1 (network architecture): a simple diagram in the appendix would make it much easier to understand (Table 3 is still hard to read because it is not clear which layers are connected together)\n- 3.3 (prioritized sequence replay): again a visual illustration of the partitioning scheme would in my opinion help clarify the approach\n\nA few minor points to conclude:\n- In eq. 6, 7 and the rest of this section, A does not depend (directly) on theta so it should probably be removed to avoid confusion. Note also that using the letter A may not be best since A is used to denote an action in 3.1.\n- In 3.1: « Let us assume that for the chosen action A we have access to an estimate R(A) of Qπ(A) » => « unbiased estimate »\n- In last equation of p.5 it is not clear what q_i^n is\n- There is a lambda missing on p.6 in the equation showing that alphas are non-negative on average, just before the min\n- In the equation above eq. 12 there is a sum over « i=1 »\n- That same equation ends with some h_z_i that are not defined\n- In Fig. 2 (left) for Reactor we see one worker using large batches and another one using many threads. This is confusing.\n- 3.3 mentions sequences of length 32 but 3.4 says length 33.\n- 3.3 says tree operations are in O(n ln(n)) but it should be O(ln(n))\n- At very end of 3.3 it is not clear what « total variation » is.\n- In 3.4 please specify the frequency at which the learner thread downloads shared parameters and uploads updates\n- Caption of Fig. 3 talks about « changing the number of workers » for the left plot while it is in the right plot\n- The explanation on what the variants of Reactor (ND and 500M) mean comes after results are shown in Fig. 2.\n- Section 4 starts with Fig. 3 without explaining what the task is, how performance is measured, etc. It also claims that Distributional Retrace helps while this is not the case in Fig. 3 (I realize it is explained afterwards, but it is confusing when reading the sentence « We can also see... »). Finally it says priorization is the most important component while the beta-LOO ablation seems to perform just the same.\n- Footnote 3 should say it is 200M observations except for Reactor 500M\n- End of 4.1: « The algorithms that we compare Reactor against are » => missing ACER, A3C and Rainbow\n- There are two references for « Sample efficient actor-critic with experience replay »\n- I do not see the added benefit of the Elo computation. It seems to convey essentially the same information as average rank.\n\nAnd a few typos:\n- Just above 2.1.3: « increasing » => increasingly\n- In 3.1: « where V is a baseline that depend » => depends\n- p.7: « hight » => high, and « to all other sequences » => of all other sequences\n- Double parentheses in Bellemare citation at beginning of section 4\n- Several typos in appendix (too many to list)\n\nNote: I did not have time to carefully read Appendix 6.3 (contextual priority tree)","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/79176417ac2023cf22df0fb5463b72a363003b3c.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1509739180854,"tcdate":1509130540849,"number":650,"cdate":1509739178196,"id":"rkHVZWZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/79176417ac2023cf22df0fb5463b72a363003b3c.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}