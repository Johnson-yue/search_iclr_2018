{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222543117,"tcdate":1511840518620,"number":3,"cdate":1511840518620,"id":"SJyMoI5gG","invitation":"ICLR.cc/2018/Conference/-/Paper1027/Official_Review","forum":"HyiRazbRb","replyto":"HyiRazbRb","signatures":["ICLR.cc/2018/Conference/Paper1027/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A potentially interesting convergence result (with errors)","rating":"2: Strong rejection","review":"This paper shows that an idealized version of stochastic gradient descent converges when learning autoencoders with ReLu non-linearities under strong sparsity assumptions. Convergence rates are also determined. The result is another one in the emerging line of proving convergence guarantees for non-convex optimization problems arising in machine learning, and aims to explain certain phenomena experienced in practice.\n\nThe paper is generally nicely written, providing intuitions, but there are several typos (both in the text and in the math, e.g., missing indices), which should also be corrected.\n\nOn the negative side, while the proof technique in general looks plausible, there seem to be some mistakes in the derivations, which must be corrected before the paper can be accepted. Also, the assumptions in the in the paper seem quite restrictive, and their implications are not discussed thoroughly. \n\nThe assumptions are the following:\n1. The input data is coming from a mixture distribution, in the form x=w_I + eps, where {w_1,...,w_k} is a collection of unit vectors, I is uniform in {1,...,K}, eps is some noise (independent for each sample). \n2. The maximum norm of the noise is O(1/k).\n3. The number n of hidden neurons in the autoencoder is Omega(k) (this is not explicitly assumed but is necessary to make the probability of \"incorrect\" initialization small as well as the results to hold).\n\nUnder these assumptions it is shown that the weights of the autoencoder converge to the centers {w_1,...,w_k} (i.e., for any i the autoencoder has at least one weight converging to w_i). The rate of convergence depends on the coherence of the vectors w_i: the less coherent they are the faster the convergence is.\n\nFirst notice that some assumptions are missing from the main statement, as the error probability delta is certainly connected to the probability of incorrect initialization: when n=1<k, the convergence result clearly cannot hold. This comes from the mistake that in Theorem 3 you state the bound for the probability P(F^\\infty) instead of the conditional probability P(F^\\infty|E_o) (this is present everywhere in the proof). Theorem 3 should also depend on delta_o, which is used in the definition of F^\\infty. \n\nTheorem 2 also seems incorrect. Intuitively, the question is why it cannot happen that two neurons contribute to reproducing a given w_i, and so neither of their weights converge to w_i: E.g., assuming that {w_1,...,w_k,w_1',...,w_k'} form an orthogonal system and the noise is 0, the weight matrix of size n=2k defined as W_{2i-1,*}^T = 1/sqrt{2}(w_i + w'_i) and W_{2i,*}^T=1/sqrt{2}(w_i - w'_i), i \\in [k], with 0 bias can exactly recover any x=w_i (indeed, W_{2j-1,*} x= W_{2j,*} x = 1/sqrt{2}, while the other products are 0, and so W^T W x = W^T W w_j = 1/sqrt{2}(W_{2j-1,*}+W_{2j,*})^T = w_j). Then SGD does not change the weights and hence cannot recover the original weights {w_i }, in particular, it cannot increase the coherence in any step, contradicting Theorem 2. This counterexample can be extended even to the situation when k>d, as--in fact--we only need that the existence of a single j such that w_j and w'_j are orthogonal and also orthogonal to the other basis vectors.\n\nThe assumptions are also very strange in the sense that the norm of the noise is bounded by O(1/k), thus the more modes the input distribution has the more separable they become. What motivates this scaling? Furthermore, the parameters of the algorithm for which the convergence is claimed heavily depend on the problem parameters, which are not known. How can you instantiate the algorithm then (accepting the ideal definition of b)? What are the consequences?\n\nGiven the above, at this point I cannot recommend the paper for acceptance. However, if the above problems are resolved, I would be very happy to see the paper at the conference.\n\n\nOther comments\n-----------------------\n- Add a short derivation why the weights of the autoencoder should converge to the w_i.\n- Definition 3: C_j is not defined in the main text.\n- While it is mentioned multiple times that the interesting regime is d<n, this is actually never used, nor needed (personally, I have never seen such an autoencoder--please give some references). What is really needed is n>k, which is natural if one wants to preserve the information, and also k>d for a rich family of distributions.\n- The area of the spherical cap is well understood (up to multiplicative constants), and better bounds than yours are readily available: with a cap of height 1-t, for sqrt{2/d}<t<1, the relative surface of the cap is between P/6 and P/2 where \nP=1/(t \\sqrt{d}) (1-t^2)^{(d-1)/2}; see, e.g., A. Brieden, P. Gritzmann, R. Kannan, V. Klee, L. Lovasz, and M. Simonovits. Deterministic and randomized polynomial-time approximation of radii. Mathematika. A Journal of Pure and Applied Mathematics, 48(1-2):63â€“105, 2001. \n- The notation section should be brought forward (or referred the fist time the notation is actually used).\n- Instead of unit spherical Gaussian you could simply say uniform distribution on the unit sphere\n- While Algorithm 1 is called \"norm-controlled SGD training,\" it does not control the norm at all.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION","abstract":"Auto-encoder is commonly used for unsupervised representation learning and for\npre-training deeper neural networks. When its activation function is linear and the\nencoding dimension (width of hidden layer) is smaller than the input dimension,\nit is well known that auto-encoder is optimized to learn the principal components\nof the data distribution Oja (1982). However, when the activation is nonlinear\nand when the width is larger than the input dimension, auto-encoder behaves differently\nfrom PCA, with the ability to capture multi-modal aspects of the input\ndistribution.\nWe provide a theoretical explanation for this empirically observed phenomenon,\nwhen rectified-linear unit (ReLu) is adopted as the activation function and the\nhidden-layer width is set to be large. In this case, we show that, with significant\nprobability, initializing the weight matrix of an auto-encoder by sampling\nfrom a spherical Gaussian distribution followed by stochastic gradient descent\n(SGD) training converges towards the ground-truth representation for a class of\nsparse dictionary learning models. In addition, we can show that, conditioning\non convergence, the expected convergence rate is O(1/t), where t is the number\nof updates. Our analysis quantifies how increasing hidden layer width helps the\ntraining performance when random initialization is used, and how the norm of\nnetwork weights influence the speed of SGD convergence.","pdf":"/pdf/4b61e03c354a1c7f2ab7ec1f071d53c9ba111a40.pdf","TL;DR":"theoretical analysis of nonlinear wide autoencoder","paperhash":"anonymous|demystifying_wide_nonlinear_autoencoders_fast_sgd_convergence_towards_sparse_representation_from_random_initialization","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyiRazbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1027/Authors"],"keywords":["stochastic gradient descent","autoencoders","nonconvex optimization","representation learning","theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222543168,"tcdate":1511198524387,"number":2,"cdate":1511198524387,"id":"HkErJ5eez","invitation":"ICLR.cc/2018/Conference/-/Paper1027/Official_Review","forum":"HyiRazbRb","replyto":"HyiRazbRb","signatures":["ICLR.cc/2018/Conference/Paper1027/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting work, but appears to contain a crucial bug","rating":"3: Clear rejection","review":"The paper considers training single-hidden-layer auto-encoders, using stochastic gradient descent, for data generated from a noisy sparse dictionary model. The main result shows that under suitable conditions, the algorithm is likely to recover the ground-truth parameters. \n\nAlthough non-convex dictionary learning has been extensively studied for linear models, extending such convergence results to nonlinear models is interesting, and the result (if true) would be quite nice. Unfortunately (and unless I missed something), there appears to be a crucial bug in the argument, which requires that random initialization lead to dictionary elements sufficiently close to the ground truth. Specifically, definition 1 and lemma 1 give a bound on the success probability, which is exponentially small in the dimension d (as it should, since it essentially bounds the probability that an O(1)-norm random vector has \\Omega(1) inner product with some fixed unit vector). However, the d exponent disappears when the lemma is used to prove the main theorem (bottom of pg. 10), as well as in the theorem statement, making it seem that the success probability is large. Of course, a result which holds with exponentially small probability is not very interesting. I should also say that I did not check the rest of the proof carefully.\n\nA few relatively more minor issues:\n- The paper makes the strong assumption that the data is generated from a 1-sparse dictionary model. In other words, each data point is simply a randomly-chosen dictionary element, plus zero-mean noise. With this model, dictionary learning is quite easy and could be solved directly by other methods (although I see the value of analyzing specifically the behavior of SGD on auto-encoders). \n- To make things go through, the paper makes a non-trivial assumption on how the bias terms are updated (not quite according to SGD). But unless I'm missing something, a bias term isn't even needed to learn in their model, so wouldn't it be simpler and more natural to just assume that the auto-encoder doesn't have a bias term (i.e., x-> W's(Wx))?.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION","abstract":"Auto-encoder is commonly used for unsupervised representation learning and for\npre-training deeper neural networks. When its activation function is linear and the\nencoding dimension (width of hidden layer) is smaller than the input dimension,\nit is well known that auto-encoder is optimized to learn the principal components\nof the data distribution Oja (1982). However, when the activation is nonlinear\nand when the width is larger than the input dimension, auto-encoder behaves differently\nfrom PCA, with the ability to capture multi-modal aspects of the input\ndistribution.\nWe provide a theoretical explanation for this empirically observed phenomenon,\nwhen rectified-linear unit (ReLu) is adopted as the activation function and the\nhidden-layer width is set to be large. In this case, we show that, with significant\nprobability, initializing the weight matrix of an auto-encoder by sampling\nfrom a spherical Gaussian distribution followed by stochastic gradient descent\n(SGD) training converges towards the ground-truth representation for a class of\nsparse dictionary learning models. In addition, we can show that, conditioning\non convergence, the expected convergence rate is O(1/t), where t is the number\nof updates. Our analysis quantifies how increasing hidden layer width helps the\ntraining performance when random initialization is used, and how the norm of\nnetwork weights influence the speed of SGD convergence.","pdf":"/pdf/4b61e03c354a1c7f2ab7ec1f071d53c9ba111a40.pdf","TL;DR":"theoretical analysis of nonlinear wide autoencoder","paperhash":"anonymous|demystifying_wide_nonlinear_autoencoders_fast_sgd_convergence_towards_sparse_representation_from_random_initialization","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyiRazbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1027/Authors"],"keywords":["stochastic gradient descent","autoencoders","nonconvex optimization","representation learning","theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222543214,"tcdate":1510258559826,"number":1,"cdate":1510258559826,"id":"HyutwEMJG","invitation":"ICLR.cc/2018/Conference/-/Paper1027/Official_Review","forum":"HyiRazbRb","replyto":"HyiRazbRb","signatures":["ICLR.cc/2018/Conference/Paper1027/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Limited progress, and some doubts about correctness","rating":"2: Strong rejection","review":"The authors study the convergence of a procedure for learning\nan autoencoder with a ReLu non-linearity.  The procedure is akin\nto stochastic gradient descent, with some parameters updated at\neach iteration in a manner that performs optimization with respect\nto the population risk.\n\nThe autoencoders that they study tie the weights of the decoder to\nthe weights of the encoder, which is a common practice.  There\nare no bias terms in the decoder, however.  I do not see where they\nmotivate this restriction, and it seems to limit the usefulness of\nthe bias terms in the encoder.\n\nTheir analysis is with respect to a mixture model.  This is described\nin the abstract as a sparse dictionary model, which it is, I guess.\nThey assume that the gaussians are very well separated.  \n\nThe statement of Theorem says that it concerns Algorithm 1.  The\ndescription of Algorithm 1 describes a procedure, with an\naside that describes a \"version used in the analysis\".\n\nThey write in the text that the rows of W^t are projected onto\na ball of radius c in each update, but this is not included\nin the description of Algorithm 1.  The statement of Theorem 1\nincludes the condition that all rows of W^t are always equal to\nc, but this may not be consistent with the updates given\nin Algorithm 1.  My best guess is that they intend of\nthe rows of W^t to be normalized after each update (which is\ndifferent than projecting onto the ball of radius c).  This\naspect of their procedure seems restrict its applicability.\n\nSuccessful initialization looks like a very strong condition to\nme, something that will occur exponentially rarely, as a function\nof d. (See Fact 10 of \"Agnostically learning halfspaces\", by Kalai, et al.)\nFor each row of W^*, the probability that any one row of W^o will be\nclose enough is exponentially small, so exponentially many rows\nare needed for the probability that any row is close enough to\nbe, say, 1/2.  I don't see anything in the conditions of Theorem 1\nthat says that n is large relative to d, so it seems like its\nclaim includes the case where k and n are constants, like 5.\nBut, in this case, it seems like the claim of the probability\nof successful initialization cannot be correct when d is large.\n\nIt looks like, after \"successful initialization\", especially\ngiven the strong separation condition, the model as already\n\"got it\".  In particular, the effect of the ReLUs seems to\nbe limited in this regime.\n\nI have some other concerns about correctness, but I do not think\nthat the paper can be accepted even if they are unfounded.\n\nThe exposition is uneven.  They tell us that W^T is the transpose\nof W, but do not indicate that 1_{a^t (x') > 0} is a componentwise\nindicator function, and that x' 1_{a^t (x') > 0} is its\ncomponentwise product with x' (if this is correct).\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION","abstract":"Auto-encoder is commonly used for unsupervised representation learning and for\npre-training deeper neural networks. When its activation function is linear and the\nencoding dimension (width of hidden layer) is smaller than the input dimension,\nit is well known that auto-encoder is optimized to learn the principal components\nof the data distribution Oja (1982). However, when the activation is nonlinear\nand when the width is larger than the input dimension, auto-encoder behaves differently\nfrom PCA, with the ability to capture multi-modal aspects of the input\ndistribution.\nWe provide a theoretical explanation for this empirically observed phenomenon,\nwhen rectified-linear unit (ReLu) is adopted as the activation function and the\nhidden-layer width is set to be large. In this case, we show that, with significant\nprobability, initializing the weight matrix of an auto-encoder by sampling\nfrom a spherical Gaussian distribution followed by stochastic gradient descent\n(SGD) training converges towards the ground-truth representation for a class of\nsparse dictionary learning models. In addition, we can show that, conditioning\non convergence, the expected convergence rate is O(1/t), where t is the number\nof updates. Our analysis quantifies how increasing hidden layer width helps the\ntraining performance when random initialization is used, and how the norm of\nnetwork weights influence the speed of SGD convergence.","pdf":"/pdf/4b61e03c354a1c7f2ab7ec1f071d53c9ba111a40.pdf","TL;DR":"theoretical analysis of nonlinear wide autoencoder","paperhash":"anonymous|demystifying_wide_nonlinear_autoencoders_fast_sgd_convergence_towards_sparse_representation_from_random_initialization","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyiRazbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1027/Authors"],"keywords":["stochastic gradient descent","autoencoders","nonconvex optimization","representation learning","theory"]}},{"tddate":null,"ddate":null,"tmdate":1510092381993,"tcdate":1509137880588,"number":1027,"cdate":1510092360531,"id":"HyiRazbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyiRazbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION","abstract":"Auto-encoder is commonly used for unsupervised representation learning and for\npre-training deeper neural networks. When its activation function is linear and the\nencoding dimension (width of hidden layer) is smaller than the input dimension,\nit is well known that auto-encoder is optimized to learn the principal components\nof the data distribution Oja (1982). However, when the activation is nonlinear\nand when the width is larger than the input dimension, auto-encoder behaves differently\nfrom PCA, with the ability to capture multi-modal aspects of the input\ndistribution.\nWe provide a theoretical explanation for this empirically observed phenomenon,\nwhen rectified-linear unit (ReLu) is adopted as the activation function and the\nhidden-layer width is set to be large. In this case, we show that, with significant\nprobability, initializing the weight matrix of an auto-encoder by sampling\nfrom a spherical Gaussian distribution followed by stochastic gradient descent\n(SGD) training converges towards the ground-truth representation for a class of\nsparse dictionary learning models. In addition, we can show that, conditioning\non convergence, the expected convergence rate is O(1/t), where t is the number\nof updates. Our analysis quantifies how increasing hidden layer width helps the\ntraining performance when random initialization is used, and how the norm of\nnetwork weights influence the speed of SGD convergence.","pdf":"/pdf/4b61e03c354a1c7f2ab7ec1f071d53c9ba111a40.pdf","TL;DR":"theoretical analysis of nonlinear wide autoencoder","paperhash":"anonymous|demystifying_wide_nonlinear_autoencoders_fast_sgd_convergence_towards_sparse_representation_from_random_initialization","_bibtex":"@article{\n  anonymous2018demystifying,\n  title={DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyiRazbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1027/Authors"],"keywords":["stochastic gradient descent","autoencoders","nonconvex optimization","representation learning","theory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}