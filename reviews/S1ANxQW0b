{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222556601,"tcdate":1512060070902,"number":2,"cdate":1512060070902,"id":"H1y3N2alf","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Review","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting off-policy algorithms with nice results","rating":"6: Marginally above acceptance threshold","review":"This is an interesting policy-as-inference approach, presented in a reasonably clear and well-motivated way. I have a couple questions which somewhat echo questions of other commenters here. Unfortunately, I am not sufficiently familiar with the relevant recent policy learning literature to judge novelty. However, as best I am aware the empirical results presented here seem quite impressive for off-policy learning.\n\n- When is it possible to normalize the non-parametric q(a|s) in equation (6)? It seems to me this will be challenging in most any situation where the action space is continuous. Is this guaranteed to be Gaussian? If so, I don’t understand why.\n\n– In equations (5) and (10), a KL divergence regularizer is replaced by a “hard” constraint. However, for optimization purposes, in C.3 the hard constraint is then replaced by a soft constraint (with Lagrange multipliers), which depend on values of epsilon. Are these values of epsilon easy to pick in practice? If so, why are they easier to pick than e.g. the lambda value in eq (10)?\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511984566837,"tcdate":1511984566837,"number":3,"cdate":1511984566837,"id":"HkJ6aF2xf","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"content":{"title":"Few comments...","comment":"\nI do have a few comments / corrections / questions about the paper:\n\n- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?\n\n- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    \n\n- I did not get whether the additional \"one-step KL regularisation\" is obtained from the lower bound or just added as additional regularisation? Could you explain?\n\n- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?\n\n- I think the following experiments would be very informative:\n\n   - MPO without trust region in M-step\n   \n   - MPO without retrace algorithm for getting the Q-value\n\n   - test different epsilons for E and M step\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1512222556650,"tcdate":1511984388978,"number":1,"cdate":1511984388978,"id":"rypb6tngM","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Review","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents an interesting new algorithm for deep reinforcement learning which outperforms state of the art methods. ","rating":"7: Good paper, accept","review":"The paper presents a new algorithm for inference-based reinforcement learning for deep RL. The algorithm decomposes the policy update in two steps, an E and an M-step. In the E-step, the algorithm estimates a variational distribution q which is subsequentially used for the M-step to obtain a new policy. Two versions of the algorithm are presented, using a parametric or a non-parametric (sample-based) distribution for q. The algorithm is used in combination with the retrace algorithm to estimate the q-function, which is also needed in the policy update.\n\nThis is a well written paper presenting an interesting algorithm. The algorithm is similar to other inference-based RL algorithm, but is the first application of inference based RL to deep reinforcement learning. The results look very promising and define a new state of the art or deep reinforcement learning in continuous control, which is a very active topic right now. Hence, I think the paper should be accepted. \n\n\nI do have a few comments / corrections / questions about the paper:\n\n- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?\n\n- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    \n\n- I did not get whether the additional \"one-step KL regularisation\" is obtained from the lower bound or just added as additional regularisation? Could you explain?\n\n- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?\n\n- I think the following experiments would be very informative:\n\n   - MPO without trust region in M-step\n   \n   - MPO without retrace algorithm for getting the Q-value\n\n   - test different epsilons for E and M step\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511861375520,"tcdate":1511861375520,"number":2,"cdate":1511861375520,"id":"S1DYhi9gz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"Hkk3DISC-","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Re: Clarifications ","comment":"Thank you for carefully reading of the paper and uncovering a few minor mistakes.\n\n> Firstly, I think it would be helpfull to formally define what $$q(\\rho)$$ is.  My current assumption is: $$q(\\rho) = p(s_0) \\prod_1^\\infty p(s_{t+1}|a_t, s_t) q(a_t|s_t)$$.\nYour assumption is correct. q(\\rho) is analogous to p(\\rho) (as described in the background section on MDPs). We will add this definition. \n\n>1. I think at the end of the line you should have $$+ \\log p(\\theta)$$ rather than $$+ p(\\theta)$$ (I believe this is a typo)\nCorrect, this is indeed a typo and will be fixed in the next revision of the paper.\n\n> 2. In the definition of the log-probabilities, the $$\\alpha$$ parameter appears only in the definition of 'p(O=1|\\rho)'. The way it appears is as a denominator in the log-probability. In line 4 of equation (1) it has suddenly appeared as a multiplier in front of the log-densities of $$\\pi(a|s_t)$$ and $$q(a|s_t)$$. This is possible if we factor out the $$\\alpha^{-1}$$ from the sum of the rewards, but then on that line, there should be a prefactor of $$\\alpha^{-1}$$ in front of the expectation over 'q' which seems missing. (I believe this is a typo as well).\n\nIn this step we indeed just multiplied with the (non-zero) \\alpha. We presume you meant that alpha is then, however, missing in front of the prior p(\\theta) here. You are correct and this will be also fixed in the next revision.\n\n> 3. In the resulting expectation, it is a bit unclear how did the discount factors $$\\gamma^t$$ have appeared as well as in front of the rewards also in front of the KL divergences? From the context provided I really failed to be able to account for this, and given that for the rest of the paper this form has been used more than once I was wondering if you could provide some clarification on the derivation of the equation as it is not obvious to at least some of the common readers of the paper.\n\nThank you for pointing out this inconsistency which has arisen due to some last minute changes in notation that we introduced when we unified the notation in the paper - switching from presenting the finite-horizon, undiscounted, setting to using the infinite-horizon formulation. As pointed out by previous work (e.g. Rawlik et al.)  there is a direct correspondence between learning / inference in an appropriately constructed graphical model (as suggested by the first line of Eq. 1) and the regularized control objective in the finite horizon, undiscounted case. The regularized RL objective still exists in the discounted, infinite horizon case (e.g. Rawlik et al. or see [1] for another construction), but an equivalent graphical model is harder to construct (and is not of the form currently presented in the paper; e.g. see [1]). We will fix this and clarify the relation in the revision\n\n[1] Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes, Marc Toussaint, Amos Storkey, ICML 2004"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511861301074,"tcdate":1511861301074,"number":1,"cdate":1511861301074,"id":"By6E3iqxz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"HkX4eXIC-","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Thank you for spotting some minor inconsistencies","comment":"Thank you for your thorough read of the paper. \n\n> The derivation of \"one-step KL regularised objective\" is unclear to me and this seems to be related to a partial E-step. \n\nWe will clarify the relationship between the one-step objective and Eq. 1 in more detail in a revised version of the paper. We will also include a proof that the the specific \"partial\" update we use in the E-step leads to an improvement in Eq. (1) and guarantees monotonic improvement of the overall procedure.\n\nIn short, the relation between objective (1) and formula (4) is as follows:\ninstead of optimizing objective (1) directly in the E-step (which would entail running soft-Q-learning to convergence - e.g. Q-learning with additional KL terms of subsequent time-steps in a trajectory added to the rewards) we start from the \"unregularized\" Q-function (Eq. (3)) and expand it via the \"regularized\"  Bellman operator T Q(s,a) = E_a[Q(s,a)] + \\alpha KL(q || \\pi). We thus only consider the KL at a given state s in the E-step and not the \"full\" objective from (1). Nonetheless, as mentioned above we have now prepared a proof that this still leads to an improvement in (1).\n\n> (2) As far as I know, the previous works on variational RL maximize the marginal log-likelihood p(O=1|\\theta) (Toussaint (2009) and Rawlik (2012)), whereas you maximizes the unnormalized posterior p(O=1, \\theta) with the prior assumption on $\\theta$. I wonder if the prior assumption enhances the performance. \n\nCorrect. The prior p(\\theta) allows us to add regularization to the M-step of our procedure (enforcing a trust-region on the policy). We found this to be important when dealing with hihg-dimensional systems like the humanoid where the M-step could otherwise overfit (as the integral over action is only evaluated using 30 samples in our experiments).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1509466155110,"tcdate":1509466155110,"number":2,"cdate":1509466155110,"id":"HkX4eXIC-","invitation":"ICLR.cc/2018/Conference/-/Paper1094/Public_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comments","comment":"(1) Clarification of Equation 4\n\nThe derivation of \"one-step KL regularised objective\" is unclear to me and this seems to be related to a partial E-step. \n\nWould you explain this part in more detail?\n\n(2) As far as I know, the previous works on variational RL maximize the marginal log-likelihood p(O=1|\\theta) (Toussaint (2009) and Rawlik (2012)), whereas you maximizes the unnormalized posterior p(O=1, \\theta) with the prior assumption on $\\theta$. \nI wonder if the prior assumption enhances the performance. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1509414823044,"tcdate":1509414823044,"number":1,"cdate":1509414823044,"id":"Hkk3DISC-","invitation":"ICLR.cc/2018/Conference/-/Paper1094/Public_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Clarification of Equation 1","comment":"These might be very obvious questions, but I failed to derive the last line (line 4) in equation (1) in the paper. \n\nFirstly, I think it would be helpfull to formally define what $$q(\\rho)$$ is. My current assumption is:\n$$q(\\rho) = p(s_0) \\prod_1^\\infty p(s_{t+1}|a_t, s_t) q(a_t|s_t)$$\nwhere the 'p' distributions are taken to be equal to the real environmental state transitions.\n\nNow, there are a few problems that I encountered when trying to derive equation (1):\n\n1. I think at the end of the line you should have $$+ \\log p(\\theta)$$ rather than $$+ p(\\theta)$$ (I believe this is a typo)\n\n2. In the definition of the log-probabilities, the $$\\alpha$$ parameter appears only in the definition of 'p(O=1|\\rho)'. The way it appears is as a denominator in the log-probability. In line 4 of equation (1) it has suddenly appeared as a multiplier in front of the log-densities of $$\\pi(a|s_t)$$ and $$q(a|s_t)$$. This is possible if we factor out the $$\\alpha^{-1}$$ from the sum of the rewards, but then on that line, there should be a prefactor of $$\\alpha^{-1}$$ in front of the expectation over 'q' which seems missing. (I believe this is a typo as well).\n\n3. In the resulting expectation, it is a bit unclear how did the discount factors $$\\gamma^t$$ have appeared as well as in front of the rewards also in front of the KL divergences? From the context provided I really failed to be able to account for this, and given that for the rest of the paper this form has been used more than once I was wondering if you could provide some clarification on the derivation of the equation as it is not obvious to at least some of the common readers of the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1510092380592,"tcdate":1509138486475,"number":1110,"cdate":1510092359919,"id":"S1ANxQW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/84a8906ab0166521e2bafc00d0b1a21a077f4f8d.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}