{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222559157,"tcdate":1512184433607,"number":3,"cdate":1512184433607,"id":"By5_q5y-z","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clear rejection.","rating":"4: Ok but not good enough - rejection","review":"This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. \n\nOverall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.\n\nThe main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup.\n\nI recomend this paper for a workshop presentation.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What are image captions made of?","abstract":"This paper focuses on the `image' side of image captioning.  We investigate why end-to-end neural image captioning systems seemingly work so well, and how they can be improved by better utilizing different image representations in an informed manner. In this paper, we study the properties of different types of image representations and how they affect the performance of end-to-end image captioning models. Our empirical analysis provides interesting insights into the representational properties and suggests that the model implicitly learns a `visual-semantic' sub-space. We also provide insights into the generalization capabilities of the model. Our analysis specially focuses on interpreting the discriminative quality of the feature representations, some properties of the induced space and uniqueness of generated image captions. Our results suggest that explicitly modeling the presence of objects and basic object interactions in necessary for tasks that require semantic understanding and better generalization.","pdf":"/pdf/0e647e0120fb1714b378c172dbf1934d6c901237.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_are_image_captions_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1512222559194,"tcdate":1512087594283,"number":2,"cdate":1512087594283,"id":"r1fNl7Cgz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not clear contributions, lack of comparisons with other methods and weak results.","rating":"4: Ok but not good enough - rejection","review":"The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect.\n\nThe paper lacks novelty, just reports some results without proper analysis or insights.\n\nMain weakness of the paper:\n - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)\n - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be.\n- The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results\n - To provide a fair comparison authors, should compare their results with other paper results.\n - Tables 2 and 3 are missing the original baselines.\nThe evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.\n- Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. \n- The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.\n\nOne interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.\n\n\nTypos:\n - synsest-level -> synsets-level","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What are image captions made of?","abstract":"This paper focuses on the `image' side of image captioning.  We investigate why end-to-end neural image captioning systems seemingly work so well, and how they can be improved by better utilizing different image representations in an informed manner. In this paper, we study the properties of different types of image representations and how they affect the performance of end-to-end image captioning models. Our empirical analysis provides interesting insights into the representational properties and suggests that the model implicitly learns a `visual-semantic' sub-space. We also provide insights into the generalization capabilities of the model. Our analysis specially focuses on interpreting the discriminative quality of the feature representations, some properties of the induced space and uniqueness of generated image captions. Our results suggest that explicitly modeling the presence of objects and basic object interactions in necessary for tasks that require semantic understanding and better generalization.","pdf":"/pdf/0e647e0120fb1714b378c172dbf1934d6c901237.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_are_image_captions_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1512222559231,"tcdate":1511391224608,"number":1,"cdate":1511391224608,"id":"BybWlKXgz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Needs work","rating":"4: Ok but not good enough - rejection","review":"This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation.\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.\n- The introduction also states that the authors will propose ways to improve image captioning. This is never discussed.\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods.\n- Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4.\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.\n\nExperimental Setup and Training Details\n- How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1?\n\nMain claim of the paper\nDevlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers.\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps.\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.\n\nMinor comments\n- Figure 1 is hard to read on paper. Please improve it.\n- Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What are image captions made of?","abstract":"This paper focuses on the `image' side of image captioning.  We investigate why end-to-end neural image captioning systems seemingly work so well, and how they can be improved by better utilizing different image representations in an informed manner. In this paper, we study the properties of different types of image representations and how they affect the performance of end-to-end image captioning models. Our empirical analysis provides interesting insights into the representational properties and suggests that the model implicitly learns a `visual-semantic' sub-space. We also provide insights into the generalization capabilities of the model. Our analysis specially focuses on interpreting the discriminative quality of the feature representations, some properties of the induced space and uniqueness of generated image captions. Our results suggest that explicitly modeling the presence of objects and basic object interactions in necessary for tasks that require semantic understanding and better generalization.","pdf":"/pdf/0e647e0120fb1714b378c172dbf1934d6c901237.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_are_image_captions_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1510092379923,"tcdate":1509138956347,"number":1140,"cdate":1510092359527,"id":"HJNGGmZ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"What are image captions made of?","abstract":"This paper focuses on the `image' side of image captioning.  We investigate why end-to-end neural image captioning systems seemingly work so well, and how they can be improved by better utilizing different image representations in an informed manner. In this paper, we study the properties of different types of image representations and how they affect the performance of end-to-end image captioning models. Our empirical analysis provides interesting insights into the representational properties and suggests that the model implicitly learns a `visual-semantic' sub-space. We also provide insights into the generalization capabilities of the model. Our analysis specially focuses on interpreting the discriminative quality of the feature representations, some properties of the induced space and uniqueness of generated image captions. Our results suggest that explicitly modeling the presence of objects and basic object interactions in necessary for tasks that require semantic understanding and better generalization.","pdf":"/pdf/0e647e0120fb1714b378c172dbf1934d6c901237.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_are_image_captions_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}