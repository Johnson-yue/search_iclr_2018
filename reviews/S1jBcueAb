{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222619320,"tcdate":1511816963778,"number":3,"cdate":1511816963778,"id":"rJ9-yZ9lM","invitation":"ICLR.cc/2018/Conference/-/Paper316/Official_Review","forum":"S1jBcueAb","replyto":"S1jBcueAb","signatures":["ICLR.cc/2018/Conference/Paper316/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper presents the SliceNet architecture, an sequence-to-sequence model based on super-dilated convolutions, which allow to reduce the computational cost of the model compared to standard convolution. The proposed model is then evaluated on machine translation and yields competitive performance compared to state-of-the-art approaches.\n\nIn terms of clarity, the paper is overall easy to follow, however I am a bit confused by Section 2 about what is related work and what is a novel contribution, although the section is called “Our Contribution”. For instance, it seems that the separable convolution presented in Section 2.1 were introduced by (Chollet, 2016) and are not part of the contribution of this paper. The authors should thus clarify the contributions of the paper.\n\nIn terms of significance, the SliceNet architecture is interesting and is a solid contribution for reducing computation cost of sequence-to-sequence models. The experiments on NMT are convincing and gives interesting insights, although I would like to see some pointers about why in Table 3 the Transformer approach (Vaswani et al. 2017) outperforms SliceNet.\n\nI wonder if the proposed approach could be applied to other sequence-to-sequence tasks in NLP or even in speech recognition ? \n\nMinor comment: \n* The equations are not easy to follow, they should be numbered. The three equations just before Section 2.2 should also be adapted as they seem redundant with Table 1.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depthwise Separable Convolutions for Neural Machine Translation","abstract":"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.\nThey have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results.\nIn addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models.","pdf":"/pdf/e4c9f74c09046efbad234b37348c7c482bfcbc43.pdf","TL;DR":"Depthwise separable convolutions improve neural machine translation: the more separable the better.","paperhash":"anonymous|depthwise_separable_convolutions_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018depthwise,\n  title={Depthwise Separable Convolutions for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1jBcueAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper316/Authors"],"keywords":["convolutions","neural machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1512222619360,"tcdate":1511816550938,"number":2,"cdate":1511816550938,"id":"BkCwTl9lG","invitation":"ICLR.cc/2018/Conference/-/Paper316/Official_Review","forum":"S1jBcueAb","replyto":"S1jBcueAb","signatures":["ICLR.cc/2018/Conference/Paper316/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"The paper proposes to use depthwise separable convolution layers in a fully convolutional neural machine translation model. The authors also introduce a new \"super-separable\" convolution layer, which further reduces the computational cost of depthwise separable convolutions. Results are presented on the WMT English to German translation task, where the method is shown to perform second-best behind the Transformer model.\n\nThe paper's greatest strength is in my opinion the quality of its exposition of the proposed method. The relationship between spatial convolutions, pointwise convolutions, depthwise convolutions, depthwise separable convolutions, grouped convolutions, and super-separable convolutions is explained very clearly, and the authors properly introduce each model component.\n\nPerhaps as a consequence of this, the experimental section feels squeezed in comparison. Quantitative results are presented in two fairly dense tables (especially Table 2) which, although parsable after reading the paper carefully, could benefit from a little bit more information on how they should be read. The conclusions that are drawn in the text are stated without citing metrics or architectural configurations, leaving it up to the reader to connect the conclusions to the table contents.\n\nOverall, I feel that the results presented make a compelling case both for the effectiveness of depthwise separable convolutions and larger convolution windows, as well as the overall performance achievable by such an architecture. I think the paper constitutes a good contribution, and adjustments to the experimental section could make it a great contribution.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depthwise Separable Convolutions for Neural Machine Translation","abstract":"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.\nThey have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results.\nIn addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models.","pdf":"/pdf/e4c9f74c09046efbad234b37348c7c482bfcbc43.pdf","TL;DR":"Depthwise separable convolutions improve neural machine translation: the more separable the better.","paperhash":"anonymous|depthwise_separable_convolutions_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018depthwise,\n  title={Depthwise Separable Convolutions for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1jBcueAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper316/Authors"],"keywords":["convolutions","neural machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1512222619397,"tcdate":1511613895621,"number":1,"cdate":1511613895621,"id":"rJeAByPlf","invitation":"ICLR.cc/2018/Conference/-/Paper316/Official_Review","forum":"S1jBcueAb","replyto":"S1jBcueAb","signatures":["ICLR.cc/2018/Conference/Paper316/AnonReviewer2"],"readers":["everyone"],"content":{"title":"More experiments","rating":"5: Marginally below acceptance threshold","review":"Pros:\n- new module\n- good performances (not state-of-the-art)\nCons:\n- additional experiments\n\nThe paper is well motivated, and is purely experimental and proposes a new architecture. However, I believe that more experiments should be performed and the explanations could be more concise.\n\nThe section 3 is difficult to read because the notations of the different formula are a little bit heavy. They were nicely summarised on the Figure 1: each of the formula' block could be replaced by a figure, which would make this section faster to read and understand.\n\nI would have enjoyed a parameter comparison in Table 3 as it is claimed this architecture has less parameters and additional experiments would be welcome. As it does not reach the state-of-the-art, \"super separable convolutions\" could be compared on other tasks?\n\nminor:\n\"In contrast, regular convolutional layers break\nthis creed by learning filters that must simultaneously perform the extraction of spatial features and\ntheir merger into channel dimensions; an inefficient and ineffective use of parameters.\" - a verb is missing?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depthwise Separable Convolutions for Neural Machine Translation","abstract":"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.\nThey have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results.\nIn addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models.","pdf":"/pdf/e4c9f74c09046efbad234b37348c7c482bfcbc43.pdf","TL;DR":"Depthwise separable convolutions improve neural machine translation: the more separable the better.","paperhash":"anonymous|depthwise_separable_convolutions_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018depthwise,\n  title={Depthwise Separable Convolutions for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1jBcueAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper316/Authors"],"keywords":["convolutions","neural machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1509739367973,"tcdate":1509096003376,"number":316,"cdate":1509739365314,"id":"S1jBcueAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1jBcueAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Depthwise Separable Convolutions for Neural Machine Translation","abstract":"Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.\nThey have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results.\nIn addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models.","pdf":"/pdf/e4c9f74c09046efbad234b37348c7c482bfcbc43.pdf","TL;DR":"Depthwise separable convolutions improve neural machine translation: the more separable the better.","paperhash":"anonymous|depthwise_separable_convolutions_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018depthwise,\n  title={Depthwise Separable Convolutions for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1jBcueAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper316/Authors"],"keywords":["convolutions","neural machine translation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}