{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222603868,"tcdate":1512068521436,"number":3,"cdate":1512068521436,"id":"BJZ2B0agf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose a novel framework for task-/domain- transfer in an unsupervised setting (no labels for target data). The idea of defining the similarity based-clustering within domain adaptation /transfer learning framework is novel. Experiments are thorough and show clear benefits of the proposed learning strategy.  ","rating":"9: Top 15% of accepted papers, strong accept","review":"pros:\nThis is a great paper - I enjoyed reading it. The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion. The paper is clearly written and easy to understand. Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework.  Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases. \n\ncons/suggestions: \n- the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail. How to deal with these cases, or better, how to detect these before deploying this method?\n- the pair-wise similarity prediction network can become very dense: how to deal with extreme cases?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511991238316,"tcdate":1511991238316,"number":2,"cdate":1511991238316,"id":"ryA6vo2gf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Public_Comment","forum":"ByRWCqvT-","replyto":"HktVdZdlG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"About cross-domain transfer, Figure 5","comment":"Did you train the model depicted in Figure 5 end-to-end including backbone with classification model (except for G)?\nWhat are your thoughts on the applicability of LCO for cross-domain transfer in fields other than vision and language modelling? "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1512222603911,"tcdate":1511849784887,"number":2,"cdate":1511849784887,"id":"BJbSJYcgG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New method with a rough presentation","rating":"6: Marginally above acceptance threshold","review":"The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class. Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \"soft labels\" for whether or not pairs of target data belong to the same class. A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points. \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest. The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task.\n\nThe way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on. To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters. However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear. Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO. Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works.\n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings. Strong results in multiple settings indicate that the proposed method is effective and generalizable. Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup.\n\nAt times, the experimental details are a little unclear. Consistent use of the A, T, and S' dataset abbreviations would help. Also, the results section seems to switch off between calling the method CCN and LCO interchangeably. Finally, a few of the experimental settings differ from their baselines in nontrivial ways. For the Office experiment, the LCO appears to be trained on ImageNet data. While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data. Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting. In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature.\n\nOverall, the paper seems to have both a novel contribution and strong technical merit. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1512222603953,"tcdate":1511783968220,"number":1,"cdate":1511783968220,"id":"Byum0OYlG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems. The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence). What's similar vs dissimilar is trained with a binary classifier.\n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning.\n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel.\n\n(Cons)\n1. The authors overclaim to be state of the art. For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset. I checked the numbers in table 2 and the numbers aren't on par with the recent methods. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2.\n2. There are some typos in the related works section and the inferece procedure isn't clearly explained. Perhaps the authors can clear this up in the text after sec 4.3.\n\n(Assessment)\nBorderline. Refer to the Cons section above.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511688240735,"tcdate":1511688240735,"number":1,"cdate":1511688240735,"id":"HktVdZdlG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"rkVJuH4ez","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Question Answering","comment":"Thank you very much for your interest. The responses of each point are in below:\na) It dependents on how hard the dataset is. In our experiments, the backbone networks and G are randomly initialized for the experiments on Omniglot and MNIST, and they perform well. For the experiments on Office-31 and the subsets of ImageNet, we do initialize the backbones with pre-training. When dealing with real-world photos, it is a common practice of pre-training, especially when the target dataset is small, e.g., Office-31. The general suggestion is: If the dataset needs a pre-trained network to help it performs well on classification, then it would be better also to use a pre-trained network in our approach.\nb) We believe the fixed value 2 is sufficient for most of the case. Our experiments involve the datasets of different complexity (e.g. MNIST vs ImageNet), unbalanced dataset (e.g. Office-31), and the varied number of categories (e.g. Omniglot alphabets). It shows the same setting performs well on the diverse conditions. In practice, we do see sometimes the performance could be improved by setting a larger margin (e.g. 2~5), but that extra gain seems dataset-dependent. Therefore we use the fixed value 2 as a conservative but universal setting.\nc) As you mentioned, it is for making the distance metric symmetric. Using only one part will introduce a hard question: Which one should be chosen? I have no clear answer for this. But from the implementation aspect, the symmetric form has an efficient vectorization thus it adds neglectable computational time compared to using only one part."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511442847834,"tcdate":1511442395634,"number":1,"cdate":1511442395634,"id":"rkVJuH4ez","invitation":"ICLR.cc/2018/Conference/-/Paper26/Public_Comment","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Alternatives for Backbone network","comment":"Great work! Thank you for your contribution and I have three questions.\na) Do you have any suggestion for situations when a pre-trained backbone network is not available, it seems very important for getting good results. As far as I understand, training backbone end-to-end in the proposed solution would not be easy due to G's dependence over it. \nb) What range of values for sigma do you recommend? In paper you used fixed value i.e. 2 for all experiments. \nc) In actual implementation of equation 1 and 3, do you add terms D_KL(P* || Q) + D_KL(Q* || P) (or 2 x D_KL(P* || Q) as they are symmetric or is it fine to just optimize one e.g. D_KL(P* || Q)?\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1509739524266,"tcdate":1508515333691,"number":26,"cdate":1509739521602,"id":"ByRWCqvT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to cluster in order to Transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/a2392bb8c92bc279fafa48a1e98c9eaa90cfb2e4.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}