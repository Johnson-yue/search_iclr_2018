{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222628294,"tcdate":1511900058012,"number":3,"cdate":1511900058012,"id":"rJfo7HsxG","invitation":"ICLR.cc/2018/Conference/-/Paper381/Official_Review","forum":"SkxqZngC-","replyto":"SkxqZngC-","signatures":["ICLR.cc/2018/Conference/Paper381/AnonReviewer1"],"readers":["everyone"],"content":{"title":"seems to miss comparable non-deep comparisons","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a VAE inference network for a non-parametric topic model.\n\nThe model on page 4 is confusing to me since this is a topic model, so document-specific topic distributions are required, but what is shown is only stick-breaking for a mixture model.\n\nFrom what I can tell, the model itself is not new, only the fact that a VAE is used to approximate the posterior. In this case, if the model is nonparametric, then comparing with Wang, et al (2011) seems the most relevant non-deep approach. Given the factorization used in that paper, the q distributions are provably optimal by the standard method. Therefore, something must be gained by the VAE due to a non-factorized q. This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders","abstract":"Topic modeling of text documents is one of the most important tasks in representation learning. In this work, we propose iTM-VAE, which is a Bayesian nonparametric (BNP) topic model with variational auto-encoders. On one hand, as a BNP topic model, iTM-VAE potentially has infinite topics and can adapt the topic number to data automatically. On the other hand, different with the other BNP topic models, the inference of iTM-VAE is modeled by neural networks, which has rich representation capacity and can be computed in a simple feed-forward manner. Two variants of iTM-VAE are also proposed in this paper, where iTM-VAE-Prod models the generative process in products-of-experts fashion for better performance and iTM-VAE-G places a prior over the concentration parameter such that the model can adapt a suitable concentration parameter to data automatically. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-the-arts in terms of perplexity, topic coherence and document retrieval tasks. Moreover, the ability of adjusting the concentration parameter to data is also confirmed by experiments.","pdf":"/pdf/74900c752aec30ffc54ac30769d69767399769be.pdf","TL;DR":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks.","paperhash":"anonymous|a_bayesian_nonparametric_topic_model_with_variational_autoencoders","_bibtex":"@article{\n  anonymous2018a,\n  title={A Bayesian Nonparametric Topic Model with Variational Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkxqZngC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper381/Authors"],"keywords":["topic model","Bayesian nonparametric","variational auto-encoder","document modeling"]}},{"tddate":null,"ddate":null,"tmdate":1512222628341,"tcdate":1511748628284,"number":2,"cdate":1511748628284,"id":"SyhzVlKez","invitation":"ICLR.cc/2018/Conference/-/Paper381/Official_Review","forum":"SkxqZngC-","replyto":"SkxqZngC-","signatures":["ICLR.cc/2018/Conference/Paper381/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper constructs infinite Topic Model with Variational Auto-Encoders by combining stick-breaking variational auto-encoder (SB-VAE) of Nalisnick & Smyth (2017) with latent Dirichlet allocation (LDA). The paper is not sufficiently novel and the proposed Bayesian nonparametric generative model is not theoretically sound.","rating":"3: Clear rejection","review":"The paper constructs infinite Topic Model with Variational Auto-Encoders (iTM-VAE) by combining stick-breaking variational auto-encoder (SB-VAE) of Nalisnick & Smyth (2017) with latent Dirichlet allocation (LDA) and several inference techniques used in Miao et al. (2016 & 2017). A main difference from Autoencoded Variational Inference For Topic Model (AVITM) of Srivastava & Sutton (2017), which had already applied VAE to LDA, is that the Dirichlet-distributed topic distribution vector for each document is now imposed with a stick-breaking prior. To address the challenge of reparameterizing the beta distributions used in stick-breaking, the paper follows SB-VAE to use the Kumaraswamy distributions to approximate the beta distributions.\n\nThe novelty of the paper does not appear to be significant, considering that most of the key techniques used in the paper had already appeared in several related papers, such as Nalisnick & Smyth (2017), Srivastava & Sutton (2017), and Miao et al. (2016 & 2017). \n\nWhile experiments show the proposed models outperform the others quantitatively (perplexity and coherence), the paper does not provide sufficient justifications on why ITM-VAE is better. In particular, it provides little information about how the two baselines, LDA and HDP, are implemented (e.g., via mean-field variational inference, VAE, or MCMC?) and how their perplexities and topic coherences are computed. In addition, achieving the best performance with about 20 topics seem quite surprising for 20news and RCV-v2. It is hard to imagine 20 news, which consists of articles in 20 different newsgroups, can be well characterized by about 20 different topics. Is there a tuning parameter that significantly impacts the number of topics inferred by iTM-VAE?\n\nAnother clear problem of the paper is that the “Bayesian nonparametric” generative procedure specified in Section 4.1 is not correct in theory. More specifically, independently drawing the document-specific pi vectors from the stick-breaking processes will lead to zero sharing between the atoms of different stick-breaking process draws. To make the paper theoretically sound as a Bayesian nonparametric topic model that uses the stick-breaking construction, please refer to Teh et al. (2006, 2008) and Wang et al. (2011) for the correct construction that ties the document-specific pi vectors with a globally shared stick-breaking process. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders","abstract":"Topic modeling of text documents is one of the most important tasks in representation learning. In this work, we propose iTM-VAE, which is a Bayesian nonparametric (BNP) topic model with variational auto-encoders. On one hand, as a BNP topic model, iTM-VAE potentially has infinite topics and can adapt the topic number to data automatically. On the other hand, different with the other BNP topic models, the inference of iTM-VAE is modeled by neural networks, which has rich representation capacity and can be computed in a simple feed-forward manner. Two variants of iTM-VAE are also proposed in this paper, where iTM-VAE-Prod models the generative process in products-of-experts fashion for better performance and iTM-VAE-G places a prior over the concentration parameter such that the model can adapt a suitable concentration parameter to data automatically. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-the-arts in terms of perplexity, topic coherence and document retrieval tasks. Moreover, the ability of adjusting the concentration parameter to data is also confirmed by experiments.","pdf":"/pdf/74900c752aec30ffc54ac30769d69767399769be.pdf","TL;DR":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks.","paperhash":"anonymous|a_bayesian_nonparametric_topic_model_with_variational_autoencoders","_bibtex":"@article{\n  anonymous2018a,\n  title={A Bayesian Nonparametric Topic Model with Variational Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkxqZngC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper381/Authors"],"keywords":["topic model","Bayesian nonparametric","variational auto-encoder","document modeling"]}},{"tddate":null,"ddate":null,"tmdate":1512222628386,"tcdate":1510898260665,"number":1,"cdate":1510898260665,"id":"ByaL9g21M","invitation":"ICLR.cc/2018/Conference/-/Paper381/Official_Review","forum":"SkxqZngC-","replyto":"SkxqZngC-","signatures":["ICLR.cc/2018/Conference/Paper381/AnonReviewer3"],"readers":["everyone"],"content":{"title":"well rounded work with good experimention and nice ideas, but some questions","rating":"7: Good paper, accept","review":"\"topic modeling of text documents one of most important tasks\"\nDoes this claim have any backing?\n\n\"inference of HDP is more complicated and not easy to be applied to new models\"  Really an artifact of the misguided nature of earlier work. The posterior for the $\\vec\\pi$ of a elements of DP or HDP can be made a Dirichlet, made finite by keeping a \"remainder\" term and appropriate augmentation.  Hughes, Kim and Sudderth (2015) have avoided stick-breaking and CRPs altogether, as have others in earlier work. Extensive models building on simple HDP doing all sorts of things have been developed.\n\nVariational stick-breaking methods never seemed to have worked well.  I suspect you could achieve better results by replacing them as well, but you would have to replace the tree of betas and extend your Kumaraswamy distribution, so it may not work.  Anyway, perhaps an avenue for future work.\n\n\"infinite topic models\" I've always taken the view that the use of the word \"infinite\" in machine learning is a kind of NIPSian machismo. In HDP-LDA at least, the major benefit in model performance comes from fitting what you call $\\vec\\pi$, which is uniform in vanilla LDA, and note that the number of topics \"found\" by a HDP-LDA sampler can be made to vary quite widely by varying what you call $\\alpha$, so any statement about the \"right\" number of topics is questionable.  So the claim in 3rd paragraph of Section 2, \"superior\" and \"self-determined topic number\" I'd say are misguided.  Plenty of experimental work to support this.\n\nIn Related Work, you seem to only mention HDP for non-parametric topic models.  More work exists, for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and don't rely on the memory hungry HCRP.\n\nGood to see a prior is placed on the concentration parameter.  Very important and not well done in the community, usually.\n\nThe Prod version is a very nice idea.  Great results.  This looks original, but I'm not expert enough in the huge masses of new deep neural network research popping up.\n\nYou've upped the standard a bit by doing good experimental work.  Oftentimes this is done poorly and one is left wondering.  A lot of effort went into this.\n\nWhat code is used for HDP-LDA?  Teh's original Matlab HCRP sampler does pretty well because at least he samples\nhyperparameters and can scale to 100k documents (yes, I tried). The comparison with LDA makes me suspicious.\nFor instance, on 20News, a good non-parametric LDA will find well over 400 topics and roundly beat LDA\non just 50 or 200.  If reporting LDA, or HDP-LDA, it should be standard to do hyperparameter fitting and\nyou need to mention what you did as this makes a big difference.\n\nPros:   \n* interesting new prod model with good results\n* alternative \"deep\" approach to a HDL-LDA model\n* good experimental work\nCons:\n* results on HDP algorithm questionable\n* poor knowledge of non-parametric LDA literature\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders","abstract":"Topic modeling of text documents is one of the most important tasks in representation learning. In this work, we propose iTM-VAE, which is a Bayesian nonparametric (BNP) topic model with variational auto-encoders. On one hand, as a BNP topic model, iTM-VAE potentially has infinite topics and can adapt the topic number to data automatically. On the other hand, different with the other BNP topic models, the inference of iTM-VAE is modeled by neural networks, which has rich representation capacity and can be computed in a simple feed-forward manner. Two variants of iTM-VAE are also proposed in this paper, where iTM-VAE-Prod models the generative process in products-of-experts fashion for better performance and iTM-VAE-G places a prior over the concentration parameter such that the model can adapt a suitable concentration parameter to data automatically. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-the-arts in terms of perplexity, topic coherence and document retrieval tasks. Moreover, the ability of adjusting the concentration parameter to data is also confirmed by experiments.","pdf":"/pdf/74900c752aec30ffc54ac30769d69767399769be.pdf","TL;DR":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks.","paperhash":"anonymous|a_bayesian_nonparametric_topic_model_with_variational_autoencoders","_bibtex":"@article{\n  anonymous2018a,\n  title={A Bayesian Nonparametric Topic Model with Variational Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkxqZngC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper381/Authors"],"keywords":["topic model","Bayesian nonparametric","variational auto-encoder","document modeling"]}},{"tddate":null,"ddate":null,"tmdate":1509739334065,"tcdate":1509110152006,"number":381,"cdate":1509739331412,"id":"SkxqZngC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkxqZngC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders","abstract":"Topic modeling of text documents is one of the most important tasks in representation learning. In this work, we propose iTM-VAE, which is a Bayesian nonparametric (BNP) topic model with variational auto-encoders. On one hand, as a BNP topic model, iTM-VAE potentially has infinite topics and can adapt the topic number to data automatically. On the other hand, different with the other BNP topic models, the inference of iTM-VAE is modeled by neural networks, which has rich representation capacity and can be computed in a simple feed-forward manner. Two variants of iTM-VAE are also proposed in this paper, where iTM-VAE-Prod models the generative process in products-of-experts fashion for better performance and iTM-VAE-G places a prior over the concentration parameter such that the model can adapt a suitable concentration parameter to data automatically. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-the-arts in terms of perplexity, topic coherence and document retrieval tasks. Moreover, the ability of adjusting the concentration parameter to data is also confirmed by experiments.","pdf":"/pdf/74900c752aec30ffc54ac30769d69767399769be.pdf","TL;DR":"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks.","paperhash":"anonymous|a_bayesian_nonparametric_topic_model_with_variational_autoencoders","_bibtex":"@article{\n  anonymous2018a,\n  title={A Bayesian Nonparametric Topic Model with Variational Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkxqZngC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper381/Authors"],"keywords":["topic model","Bayesian nonparametric","variational auto-encoder","document modeling"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}