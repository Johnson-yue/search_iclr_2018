{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222783540,"tcdate":1511010510340,"number":1,"cdate":1511010510340,"id":"BkL0g3a1f","invitation":"ICLR.cc/2018/Conference/-/Paper834/Official_Review","forum":"ByxLBMZCb","replyto":"ByxLBMZCb","signatures":["ICLR.cc/2018/Conference/Paper834/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice presentation, useful results, interesting future directions. Weak accept mostly for the reason that the paper mostly reproduces similar results in the literature with a different methodology. I'm not strongly opinioned until I see also the other reviews.","rating":"6: Marginally above acceptance threshold","review":"Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc. The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks.\nAs the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y).\n\nOriginality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work. In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results. Thus, I would say original.\n\nImportance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points. While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference.\n\nClarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version.\n\nComments:\n1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5).\n\n2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector?\n\n3. In section 5, the authors make a connection with the work of Ge et al. 2016. They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016. However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation). It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work. If that is the case, could the authors describe this a bit further?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Models: Critical Points and Local Openness","abstract":"With the increasing interest in deeper understanding of  the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models,  we provide sufficient conditions under which any local optimum of the resulting optimization problem is  global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all  non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are  global minima.  Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global.  Unlike many existing results in the literature, our result assumes no assumption  on the target data matrix Y. For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem  that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.","pdf":"/pdf/b35ce17ff6dd71d5c76d47ff50ae081efbdc2045.pdf","paperhash":"anonymous|learning_deep_models_critical_points_and_local_openness","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Models: Critical Points and Local Openness},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByxLBMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper834/Authors"],"keywords":["Deep Learning","Local and Global minima","Local Openness"]}},{"tddate":null,"ddate":null,"tmdate":1509739076208,"tcdate":1509135688027,"number":834,"cdate":1509739073547,"id":"ByxLBMZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByxLBMZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep Models: Critical Points and Local Openness","abstract":"With the increasing interest in deeper understanding of  the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models,  we provide sufficient conditions under which any local optimum of the resulting optimization problem is  global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all  non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are  global minima.  Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global.  Unlike many existing results in the literature, our result assumes no assumption  on the target data matrix Y. For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem  that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.","pdf":"/pdf/b35ce17ff6dd71d5c76d47ff50ae081efbdc2045.pdf","paperhash":"anonymous|learning_deep_models_critical_points_and_local_openness","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Models: Critical Points and Local Openness},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByxLBMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper834/Authors"],"keywords":["Deep Learning","Local and Global minima","Local Openness"]},"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}