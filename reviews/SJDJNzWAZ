{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222774947,"tcdate":1511941973773,"number":3,"cdate":1511941973773,"id":"rk0LDknlz","invitation":"ICLR.cc/2018/Conference/-/Paper813/Official_Review","forum":"SJDJNzWAZ","replyto":"SJDJNzWAZ","signatures":["ICLR.cc/2018/Conference/Paper813/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Approach is good","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a set of methods for using temporal information in event sequence prediction. Two methods for time-dependent event representation are proposed. Also two methods for using next event duration are introduced.\n\nThe motivation of the paper is interesting and I like the approach. The proposed methods seem valid. Only concern is that the proposed methods do not outperform others much with some level of significance. More advance models may be needed.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Time-Dependent Representation for Neural Event Sequence Prediction","abstract":"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.","pdf":"/pdf/99fb65f7b345e1de43290a89aa9eae5337455145.pdf","TL;DR":"Proposed methods for time-dependent event representation and regularization for sequence prediction; Evaluated these methods on five datasets that involve a range of sequence prediction tasks.","paperhash":"anonymous|timedependent_representation_for_neural_event_sequence_prediction","_bibtex":"@article{\n  anonymous2018time-dependent,\n  title={Time-Dependent Representation for Neural Event Sequence Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJDJNzWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper813/Authors"],"keywords":["Neural sequence prediction","Embedding","LSTM","Regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222774989,"tcdate":1511883368460,"number":2,"cdate":1511883368460,"id":"S1edG-sxf","invitation":"ICLR.cc/2018/Conference/-/Paper813/Official_Review","forum":"SJDJNzWAZ","replyto":"SJDJNzWAZ","signatures":["ICLR.cc/2018/Conference/Paper813/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Minor technical contribution, not significant gains with state of the art, misrepresents previous work","rating":"4: Ok but not good enough - rejection","review":"The authors present a model base on an RNN to predict marks and duration of events in a temporal point process. The main innovation of the paper is a new representation of a point process with duration (which could also be understood as marks), which allows them to use a \"time mask\", following the idea of word mask introduced by Choi et al, 2016. In Addition to the mask, the authors also propose a discretization of the duration using one hot encoding and using the event duration as a regularizer. They compare their method to several variations of their own method, two trivial baselines, and one state of the art method (RMTPP) using several real-world datasets and report small gains with respect to that state of the art method.\n\nOverall, the technical contribution of the paper is minor, the gains in performance with respect to a single state of the art are minimal, and the authors oversell their contribution specially in comparison with the related literature. More specifically, my concerns, which prevent me from recommending acceptance, are as follows:\n\n- The authors assume the point process contains duration and intervals, however, point processes generally do not have duration per event but they are discrete events localized in particular time points. Moreover, the duration in their representation (Figure 1) is sometimes an interevent time and sometimes a duration, which makes the whole construction inconsistent. Moreover, what happens then to the representation depicted in Figure 1 when duration is nonexistent or zero?\n\n- The use of \"time mask\" is not properly justified and the authors are just extending the idea of word mask to their setting -- it is unclear why the duration of an event is going to provide context and in any case this seems like a minor technical contribution. \n\n- The use of a time mask does not appear \"more principled\" than previous work (Due et al., Mei & Esiner, Xiao et al.). Previous work use the framework of temporal point processes in a principled way, the current work does not. I would encourage to authors to tone down their language.\n\n- The regularization proposed by the authors uses a Gaussian on the \"prediction error\" of the duration or just cross entropy on a discretization of the duration. Given the inconsistency in the definition of the duration (sometimes it is duration, sometimes is interevent time), the resulting regularization may lead to unexpected/undesirable results. Moreover, it is unclear why the authors do not model the duration time with an appropriate distribution (e.g., Weibull) and add the log-likelihood of the durations under that distribution as regularization. \n\n- The difference in performance  with respect to a single nontrivial baseline (the remaining baselines are trivial or versions of their own model) is minimal. Moreover, the authors fail to compare with other methods, e.g., the method by Mei & Eisner, which beats RMTPP. This is specially surprising since the authors mention such work in the related work and there is available source code at https://github.com/HMEIatJHU/neurawkes.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Time-Dependent Representation for Neural Event Sequence Prediction","abstract":"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.","pdf":"/pdf/99fb65f7b345e1de43290a89aa9eae5337455145.pdf","TL;DR":"Proposed methods for time-dependent event representation and regularization for sequence prediction; Evaluated these methods on five datasets that involve a range of sequence prediction tasks.","paperhash":"anonymous|timedependent_representation_for_neural_event_sequence_prediction","_bibtex":"@article{\n  anonymous2018time-dependent,\n  title={Time-Dependent Representation for Neural Event Sequence Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJDJNzWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper813/Authors"],"keywords":["Neural sequence prediction","Embedding","LSTM","Regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222775039,"tcdate":1510528392356,"number":1,"cdate":1510528392356,"id":"Sye5BLIyG","invitation":"ICLR.cc/2018/Conference/-/Paper813/Official_Review","forum":"SJDJNzWAZ","replyto":"SJDJNzWAZ","signatures":["ICLR.cc/2018/Conference/Paper813/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting attempt for time-event information fusion, but can be much improved.","rating":"4: Ok but not good enough - rejection","review":"Quality above threshold.\nClarity above threshold.\nOriginality slightly below threshold.\nSignificance slightly below threshold.\n\nPros:\nThis paper proposed a RNN for event sequence prediction. It provides two constructed choices for combining time(duration) information to event. Experiments on various datasets were conducted and most details are provided.\n\nCons (concerns):\n\n1. Event sequence prediction is a hard problem as there’s no clear way to fuse the features about event and the information about the time. It is a nice attempt that in this work, duration is used for event representation. However, the choices are not “principled” as claimed in the paper. E.g., the duration is simply a scaler, but \"time mask\" approache converts that to a multi-dimensional vector while there’s not much information to regularize it.\n\n2. Event-time joint embedding sounds sensible as it essentially remaps from the original value to some segments. E.g., 10 minutes and 11 minutes might have same effect on next event in one dataset while 3 days and a week might have similar effect on next event prediction. But the way how the experiments are designed and analyzed do not provide such insights.\n\n3. The experimental results are not persuasive as no other baselines besides RNN-based methods are provided. Parametric and nonparametric methods both exist for this event prediction problem in previous work. In the results provided, no significant difference between the listed model choices is found, partly because only using event type and duration is not enough. Other info such as time of day, day of week matters a lot. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Time-Dependent Representation for Neural Event Sequence Prediction","abstract":"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.","pdf":"/pdf/99fb65f7b345e1de43290a89aa9eae5337455145.pdf","TL;DR":"Proposed methods for time-dependent event representation and regularization for sequence prediction; Evaluated these methods on five datasets that involve a range of sequence prediction tasks.","paperhash":"anonymous|timedependent_representation_for_neural_event_sequence_prediction","_bibtex":"@article{\n  anonymous2018time-dependent,\n  title={Time-Dependent Representation for Neural Event Sequence Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJDJNzWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper813/Authors"],"keywords":["Neural sequence prediction","Embedding","LSTM","Regularization"]}},{"tddate":null,"ddate":null,"tmdate":1509739087306,"tcdate":1509135327430,"number":813,"cdate":1509739084650,"id":"SJDJNzWAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJDJNzWAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Time-Dependent Representation for Neural Event Sequence Prediction","abstract":"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.","pdf":"/pdf/99fb65f7b345e1de43290a89aa9eae5337455145.pdf","TL;DR":"Proposed methods for time-dependent event representation and regularization for sequence prediction; Evaluated these methods on five datasets that involve a range of sequence prediction tasks.","paperhash":"anonymous|timedependent_representation_for_neural_event_sequence_prediction","_bibtex":"@article{\n  anonymous2018time-dependent,\n  title={Time-Dependent Representation for Neural Event Sequence Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJDJNzWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper813/Authors"],"keywords":["Neural sequence prediction","Embedding","LSTM","Regularization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}