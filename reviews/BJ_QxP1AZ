{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222566014,"tcdate":1512008530936,"number":3,"cdate":1512008530936,"id":"rkj8j16lM","invitation":"ICLR.cc/2018/Conference/-/Paper129/Official_Review","forum":"BJ_QxP1AZ","replyto":"BJ_QxP1AZ","signatures":["ICLR.cc/2018/Conference/Paper129/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A paper with limited novelty","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method for few-shot learning using a new image representation called visual concept embedding. Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN. For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts. Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions. Experiments are conducted on the Mini-ImageNet dataset and the PASCAL3D+ dataset for few-shot learning.\n\nPositives:\n- The three properties of visual concepts described in the paper are interesting.\n\nNegatives:\n- The novelty of the paper is limited. The idea of visual concept has been proposed in Wang et al. 2015. Using a embedding representation based on visual concepts is straightforward. The two baseline methods for few-shot learning provide limited insights in solving the few-shot learning problem.\n\n- The paper uses a hard thresholding  in the visual concept embedding. It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unleashing the Potential of CNNs for Interpretable Few-Shot Learning","abstract":"Convolutional neural networks (CNNs) have been generally acknowledged as one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is the difficulty of interpreting them and understanding their inner workings, which is important for diagnosing their failures and correcting them. Another is that standard CNNs require large amounts of annotated data, which is sometimes very hard to obtain. Hence, it is desirable to enable them to learn from few examples. In this work, we address these two limitations of CNNs by developing novel and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts, which are interpretable visual cues represented within CNNs. We first use qualitative visualizations and quantitative statistics, to uncover several key properties of feature encoding using visual concepts. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being much more flexible and interpretable than previous state-of-the-art few-shot learning methods. We conclude that visual concepts expose the natural capability of CNNs for few-shot learning.\n","pdf":"/pdf/e9817ffdca4c2feb66d2d805571de4ba9cb3141d.pdf","TL;DR":"We enable ordinary CNNs for few-shot learning by exploiting visual concepts which are interpretable visual cues learnt within CNNs.","paperhash":"anonymous|unleashing_the_potential_of_cnns_for_interpretable_fewshot_learning","_bibtex":"@article{\n  anonymous2018unleashing,\n  title={Unleashing the Potential of CNNs for Interpretable Few-Shot Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_QxP1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper129/Authors"],"keywords":["Few-Shot Learning","Neural Network Understanding","Visual Concepts"]}},{"tddate":null,"ddate":null,"tmdate":1512222566051,"tcdate":1511818248541,"number":2,"cdate":1511818248541,"id":"rJxG4bqlG","invitation":"ICLR.cc/2018/Conference/-/Paper129/Official_Review","forum":"BJ_QxP1AZ","replyto":"BJ_QxP1AZ","signatures":["ICLR.cc/2018/Conference/Paper129/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental paper, but good results","rating":"7: Good paper, accept","review":"The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015). This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. The clustered representations are the visual concepts. This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images.\n\n A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class.\n\nThe results a are convincing, even if they are not state of the art in all the trials. \nThe paper is very easy to follows, and the results are explained in a very simple way.\n\n\nFew comments:\nThe authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unleashing the Potential of CNNs for Interpretable Few-Shot Learning","abstract":"Convolutional neural networks (CNNs) have been generally acknowledged as one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is the difficulty of interpreting them and understanding their inner workings, which is important for diagnosing their failures and correcting them. Another is that standard CNNs require large amounts of annotated data, which is sometimes very hard to obtain. Hence, it is desirable to enable them to learn from few examples. In this work, we address these two limitations of CNNs by developing novel and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts, which are interpretable visual cues represented within CNNs. We first use qualitative visualizations and quantitative statistics, to uncover several key properties of feature encoding using visual concepts. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being much more flexible and interpretable than previous state-of-the-art few-shot learning methods. We conclude that visual concepts expose the natural capability of CNNs for few-shot learning.\n","pdf":"/pdf/e9817ffdca4c2feb66d2d805571de4ba9cb3141d.pdf","TL;DR":"We enable ordinary CNNs for few-shot learning by exploiting visual concepts which are interpretable visual cues learnt within CNNs.","paperhash":"anonymous|unleashing_the_potential_of_cnns_for_interpretable_fewshot_learning","_bibtex":"@article{\n  anonymous2018unleashing,\n  title={Unleashing the Potential of CNNs for Interpretable Few-Shot Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_QxP1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper129/Authors"],"keywords":["Few-Shot Learning","Neural Network Understanding","Visual Concepts"]}},{"tddate":null,"ddate":null,"tmdate":1512222566089,"tcdate":1511772230766,"number":1,"cdate":1511772230766,"id":"By1Lg8Ygz","invitation":"ICLR.cc/2018/Conference/-/Paper129/Official_Review","forum":"BJ_QxP1AZ","replyto":"BJ_QxP1AZ","signatures":["ICLR.cc/2018/Conference/Paper129/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not enough novelty and important details unclear","rating":"3: Clear rejection","review":"My main concern for this paper is that the description of the Visual Concepts is completely unclear for me. At some point I thought I did understand it, but then the next equation didnt make sense anymore... If I understand correctly, f_p is a representation of *all images* of a specific layer *k* at/around pixel \"p\", (According to last line of page 3). That would make sense, given that then the dimensions of the vector f_p is a scalar (activation value) per image for that image, in layer k, around pixel p. Then f_v is one of the centroids (named VCs). However, this doesnt seem to be the case, given that it is impossible to construct VC activations for specific images from this definition. So, it should be something else, but it does not become clear, what this f_p is. This is crucial in order to follow / judge the rest of the paper. Still I give it a try.\n\nSection 4.1 is the second most important section of the paper, where properties of VCs are discussed. It has a few shortcomings. First, iIt is unclear why coverage should be >=0.8 and firerate ~ 1, according to the motivation firerate should equal to coverage: that is each pixel f_p is assigned to a single VC centroid. Second, \"VCs tent to occur for a specific class\", that seems rather a bold statement from a 6 class, 3 VCs experiment, where the class sensitivity is in the order 40-77%. Also the second experiment, which shows the spatial clustering for the \"car wheel\" VC, is unclear, how is the name \"car wheel\" assigned to the VC? That has have to be named after the EM process, given that EM is unsupervised. Finally the cost effectiveness training (3c), how come that the same \"car wheel\" (as in 3b) is discovered by the EM clustering? Is that coincidence? Or is there some form of supervision involved? \n\n\nMinor remarks\n- Table 1: the reported results of the Matching Network are different from the results in the paper of Vinyals (2016).\n- It is unclear what the influence of the smoothing is, and how the smoothing parameter is estimated / set.\n- The VCs are introduced for few-shot classification, unclear how this is different from \"previous few-shot methods\" (sect 5). \n- 36x36 patches have a plausible size within a 84x84 image, this is rather large, do semantic parts really cover 20% of the image?\n- How are the networks trained, with what objective, how validated, which training images? What is the influence of the layer on the performance? \n- Influence of the clustering method on VCs, eg k-means, gaussian, von-mises (the last one is proposed)?\n\nOn a personal note, I've difficulties with part of the writing. For example, the introduction is written rather \"arrogant\" (not completely the right word, sorry for that), with a sentence, like \"we have only limited insights into why CNNs are effective\" seems overkill for the main research body. The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty. Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere). Finally, the introduction paragraph of Section 5 is rather bold, \"resembles the learning process of human beings\"? Not so sure that is true, and it is not supported by a reference (or an experiment). \n\nIn conclusion:\nThis paper presents a method for creating features from a (pre-trained) ConvNet. \nIt clusters features from a specific pooling layer, and then creates a binary assignment between per image extracted feature vectors and the cluster centroids. These are used in a 1-NN classifier and a (smoothed) Naive Bayes classifier. The results show promising results, yet lack exploration of the model, at least to draw conclusions like \"we address the challenge of understanding the internal visual cues of CNNs\". I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc). Moreover, the introduction should be rewritten, and the the background section of VCs (Sect 3) should be clarified. Therefore, I rate the current manuscript as a reject. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unleashing the Potential of CNNs for Interpretable Few-Shot Learning","abstract":"Convolutional neural networks (CNNs) have been generally acknowledged as one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is the difficulty of interpreting them and understanding their inner workings, which is important for diagnosing their failures and correcting them. Another is that standard CNNs require large amounts of annotated data, which is sometimes very hard to obtain. Hence, it is desirable to enable them to learn from few examples. In this work, we address these two limitations of CNNs by developing novel and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts, which are interpretable visual cues represented within CNNs. We first use qualitative visualizations and quantitative statistics, to uncover several key properties of feature encoding using visual concepts. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being much more flexible and interpretable than previous state-of-the-art few-shot learning methods. We conclude that visual concepts expose the natural capability of CNNs for few-shot learning.\n","pdf":"/pdf/e9817ffdca4c2feb66d2d805571de4ba9cb3141d.pdf","TL;DR":"We enable ordinary CNNs for few-shot learning by exploiting visual concepts which are interpretable visual cues learnt within CNNs.","paperhash":"anonymous|unleashing_the_potential_of_cnns_for_interpretable_fewshot_learning","_bibtex":"@article{\n  anonymous2018unleashing,\n  title={Unleashing the Potential of CNNs for Interpretable Few-Shot Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_QxP1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper129/Authors"],"keywords":["Few-Shot Learning","Neural Network Understanding","Visual Concepts"]}},{"tddate":null,"ddate":null,"tmdate":1509739469041,"tcdate":1509023776283,"number":129,"cdate":1509739466383,"id":"BJ_QxP1AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ_QxP1AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unleashing the Potential of CNNs for Interpretable Few-Shot Learning","abstract":"Convolutional neural networks (CNNs) have been generally acknowledged as one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is the difficulty of interpreting them and understanding their inner workings, which is important for diagnosing their failures and correcting them. Another is that standard CNNs require large amounts of annotated data, which is sometimes very hard to obtain. Hence, it is desirable to enable them to learn from few examples. In this work, we address these two limitations of CNNs by developing novel and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts, which are interpretable visual cues represented within CNNs. We first use qualitative visualizations and quantitative statistics, to uncover several key properties of feature encoding using visual concepts. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being much more flexible and interpretable than previous state-of-the-art few-shot learning methods. We conclude that visual concepts expose the natural capability of CNNs for few-shot learning.\n","pdf":"/pdf/e9817ffdca4c2feb66d2d805571de4ba9cb3141d.pdf","TL;DR":"We enable ordinary CNNs for few-shot learning by exploiting visual concepts which are interpretable visual cues learnt within CNNs.","paperhash":"anonymous|unleashing_the_potential_of_cnns_for_interpretable_fewshot_learning","_bibtex":"@article{\n  anonymous2018unleashing,\n  title={Unleashing the Potential of CNNs for Interpretable Few-Shot Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_QxP1AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper129/Authors"],"keywords":["Few-Shot Learning","Neural Network Understanding","Visual Concepts"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}