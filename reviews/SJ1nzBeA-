{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222599909,"tcdate":1511860352977,"number":3,"cdate":1511860352977,"id":"BkYFdiqez","invitation":"ICLR.cc/2018/Conference/-/Paper250/Official_Review","forum":"SJ1nzBeA-","replyto":"SJ1nzBeA-","signatures":["ICLR.cc/2018/Conference/Paper250/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper presents a multi-task learning architecture for document ranking and query suggestion, based on (predicted) user clicks.","rating":"7: Good paper, accept","review":"The work is interesting and novel. The novelty lies not in the methods used (existing methods are used), but in the way these methods are combined to solve two problems (that so far have been treated separately in IR) simultaneously. The fitness of the proposed architecture and methodological choices to the task at hand is sufficiently argued. \n\nThe experimental evaluation is not the strongest, in terms of datasets and evaluation measures. While I understand why the AOL dataset was used, the document ranking experiments should also include runs on any of the conventional TREC datasets of documents, queries and actual (not simulated) relevance assessments. Simulating document relevance from clicks is a good enough approximation, but why not also use datasets with real human relevance assessments, especially since so many of them exist and are so easy to access?\n\nWhen evaluating ranking, MAP and NDCG are indeed two popular measures. But the choice of NDCG@1,3,10 seems a bit adhoc. Why not NDCG@5? Furthermore, as the aim seems to be to assess early precision, why not also report MRR? \n\nThe paper reports that the M-NSRF query suggestion method outperforms all baselines. This is not true. Table 2 shoes that M-NSRF is best for BLEU-1/2, but not for BLEU-3/4. \n\nThree final points:\n\n- Out of the contributions enumerated at the end of Section 1, only the novel model and the code & data release are contributions. The rigorous comparison to soa and its detailed analysis are the necessary evaluation parts of any empirical paper.  \n- The conclusion states that this work provides useful intuitions about the advantages of multi-task learning involving deep neural networks for IR tasks. What are these? Where were they discussed? They should be outlined here, or referred to somehow.\n- Although the writing is coherent, there are a couple of recurrent English language mistakes (e.g. missing articles). The paper should be proofread and corrected.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning for Document Ranking and Query Suggestion","abstract":"We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search.  It consists of two major components, document ranker and query recommender. Document ranker combines current query and session information and compares the combined representation with document  representation to rank the documents. Query  recommender  tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. Both components are trained across search sessions by sharing parameters through session recurrence, which encodes session information. Comprehensive experiments including rigorous comparisons with state-of-the-art techniques are performed on the public AOL search log, and the promising results endorse the effectiveness of the joint learning framework.","pdf":"/pdf/ca62b9560a2d81bce337ee17c946c21f54de80dd.pdf","paperhash":"anonymous|multitask_learning_for_document_ranking_and_query_suggestion","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning for Document Ranking and Query Suggestion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1nzBeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper250/Authors"],"keywords":["Multitask Learning","Document Ranking","Query Suggestion"]}},{"tddate":null,"ddate":null,"tmdate":1512222599952,"tcdate":1511765292934,"number":2,"cdate":1511765292934,"id":"HkHVBVYxz","invitation":"ICLR.cc/2018/Conference/-/Paper250/Official_Review","forum":"SJ1nzBeA-","replyto":"SJ1nzBeA-","signatures":["ICLR.cc/2018/Conference/Paper250/AnonReviewer3"],"readers":["everyone"],"content":{"title":"I like the idea of a joint learning framework for document ranking and query suggestion, but the writing and analysis can be improved.","rating":"6: Marginally above acceptance threshold","review":"This paper presents a joint learning framework for document ranking and query suggestion. It introduces the session embeddings to capture the connections between queries in a session, and potential impact of previous queries in a session to the document ranking of the current query. I like the idea in general. \n\nHowever, I have a few comments as follows:\n\n- Multi-task Match Tensor model, which is important in the experiments (best results), is only briefly introduced in Section 3.4. It is not very clear how to extend from match tensor model to a multi-task match tensor model. This makes me feel like this paper is not self-contained. The setting for this model is not introduced either in Section 4.2. \n\n- Section 3 is written mostly about what has been done but not why doing this. More intuition should be added to better explain the idea. \n\n- I like the analysis about testing the impact of the different model components in Section 4.4, especially analyzing the impact of the session. It would be nice to have some real examples to see the impact of session embeddings on document ranking. One more related question is how the clicked documents of a previous query in the same session influence the document ranking of this current query? Would that be feasible to consider in this proposed framework?\n\n- Session seems to play an important role in this multi task learning framework. This paper used the fixed 30 minute window of idle time to define a session. It would be nice to know how sensitive this model is to the definition / segmentation of sessions. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning for Document Ranking and Query Suggestion","abstract":"We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search.  It consists of two major components, document ranker and query recommender. Document ranker combines current query and session information and compares the combined representation with document  representation to rank the documents. Query  recommender  tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. Both components are trained across search sessions by sharing parameters through session recurrence, which encodes session information. Comprehensive experiments including rigorous comparisons with state-of-the-art techniques are performed on the public AOL search log, and the promising results endorse the effectiveness of the joint learning framework.","pdf":"/pdf/ca62b9560a2d81bce337ee17c946c21f54de80dd.pdf","paperhash":"anonymous|multitask_learning_for_document_ranking_and_query_suggestion","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning for Document Ranking and Query Suggestion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1nzBeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper250/Authors"],"keywords":["Multitask Learning","Document Ranking","Query Suggestion"]}},{"tddate":null,"ddate":null,"tmdate":1512222599996,"tcdate":1511753611884,"number":1,"cdate":1511753611884,"id":"SJe5w-Ylz","invitation":"ICLR.cc/2018/Conference/-/Paper250/Official_Review","forum":"SJ1nzBeA-","replyto":"SJ1nzBeA-","signatures":["ICLR.cc/2018/Conference/Paper250/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This work proposes a multi-task learning framework, M-NSRF, that can jointly learn document ranking and query suggestion. A session recurrence is introduced and modeled by an extra LSTM. The authors carry out extensive experiments to verify their algorithm. But I think this work is not of enough novelty and the experiment design can be improved. ","rating":"4: Ok but not good enough - rejection","review":"Novelty: It looks quite straightforward to combine document ranking and query suggestion.  For the model architecture, it is a standard multi-task learning framework. For the “session encoder”, it is also proposed (at least, used) in (Sordoni et al., CIKM 2015). Therefore, I think the technical novelty of the work is limited. \n\nClarify: The paper is in general well written. One minor suggestion is to replace Figure 1 with Figure 3, which is more intuitive. \n\nExperiments: \n1.\tWhy don’t you try deep LSTM models and attention mechanisms (although you mentioned them as future work)? There are many open-source tools for deep LSTM/GRU and attention models, and I see no obstacle to implement your algorithms on their top. \n2.\tIn Table 2, M-NSRF with regularization significantly outperforms the version without regularization. This indicates that it might be the regularization that works rather than multi-task learning. For fair comparison, the regularization trick should also be applied to the baselines. \n3.\tFor the evaluation metric of query suggestion, why not using BLEU score? At least, you should compare with the metrics used in (Sordoni et al., 2015) for fairness. \n4.\tThe experiments are not very comprehensive – currently, there is only one experiment in the paper, from which one can hardly draw convincing conclusions.\n5.\tHow many words are there in your documents? What is the average length of each document? You only mention that “our goal is to rank candidate documents titles……” in Page 6, 2nd paragraph. It might be quite different for long document retrieval vs. short document retrievel. \n6.\tHow did you split the dataset into training, validation and test sets?  It seems that you used a different splitting rule from (Sordoni et al., 2015), why? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning for Document Ranking and Query Suggestion","abstract":"We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search.  It consists of two major components, document ranker and query recommender. Document ranker combines current query and session information and compares the combined representation with document  representation to rank the documents. Query  recommender  tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. Both components are trained across search sessions by sharing parameters through session recurrence, which encodes session information. Comprehensive experiments including rigorous comparisons with state-of-the-art techniques are performed on the public AOL search log, and the promising results endorse the effectiveness of the joint learning framework.","pdf":"/pdf/ca62b9560a2d81bce337ee17c946c21f54de80dd.pdf","paperhash":"anonymous|multitask_learning_for_document_ranking_and_query_suggestion","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning for Document Ranking and Query Suggestion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1nzBeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper250/Authors"],"keywords":["Multitask Learning","Document Ranking","Query Suggestion"]}},{"tddate":null,"ddate":null,"tmdate":1509739404188,"tcdate":1509081767079,"number":250,"cdate":1509739401526,"id":"SJ1nzBeA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ1nzBeA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multi-Task Learning for Document Ranking and Query Suggestion","abstract":"We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search.  It consists of two major components, document ranker and query recommender. Document ranker combines current query and session information and compares the combined representation with document  representation to rank the documents. Query  recommender  tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. Both components are trained across search sessions by sharing parameters through session recurrence, which encodes session information. Comprehensive experiments including rigorous comparisons with state-of-the-art techniques are performed on the public AOL search log, and the promising results endorse the effectiveness of the joint learning framework.","pdf":"/pdf/ca62b9560a2d81bce337ee17c946c21f54de80dd.pdf","paperhash":"anonymous|multitask_learning_for_document_ranking_and_query_suggestion","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning for Document Ranking and Query Suggestion},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1nzBeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper250/Authors"],"keywords":["Multitask Learning","Document Ranking","Query Suggestion"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}