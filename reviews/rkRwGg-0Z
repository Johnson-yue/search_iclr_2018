{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222691429,"tcdate":1511801961212,"number":3,"cdate":1511801961212,"id":"r1k_ETYlM","invitation":"ICLR.cc/2018/Conference/-/Paper562/Official_Review","forum":"rkRwGg-0Z","replyto":"rkRwGg-0Z","signatures":["ICLR.cc/2018/Conference/Paper562/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting fine-grained model which lacks of a proper comparison with the state of the art","rating":"4: Ok but not good enough - rejection","review":"This article aims at understanding the role played by the different words in a sentence, taking into account their order in the sentence. In sentiment analysis for instance, this capacity is critical to model properly negation.\nAs state-of-the-art approaches rely on LSTM, the authors want to understand which information comes from which gate. After a short remainder regarding LSTM, the authors propose a framework to disambiguate interactions between gates. In order to obtain an analytic formulation of the decomposition, the authors propose to linearize activation functions in the network.\nIn the experiment section, authors compare themselves to a standard logistic regression (based on a bag of words representation). They also check the unigram sentiment scores (without context).\nThe main issue consists in modeling the dynamics inside a sentence (when a negation or a 'used to be' reverses the sentiment). The proposed approach works fine on selected samples.\n\n\nThe related work section is entirely focused on deep learning while the experiment section is dedicated to sentiment analysis. This section should be rebalanced. Even if the authors claim that their approach is general, they also show that it fits well the sentiment analysis task in particular.\n\nOn top of that, a lot of fine-grained sentiment analysis tools has been developed outside deep-learning: the authors should refer to those works.\n\nFinally, authors should provide some quantitative analysis on sentiment classification: a lot of standard benchmarks are widely use in the literature and we need to see how the proposed method performs with respect to the state-of-the-art.\n\n\nGiven the chosen tasks, this work should be compared to the beermind system:\nhttp://deepx.ucsd.edu/#/home/beermind\nand the associated publication\nhttp://arxiv.org/pdf/1511.03683.pdf","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs","abstract":"The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), a novel algorithm for capturing the contributions of combinations of words or variables in terms of CD scores. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.","pdf":"/pdf/315f203e6ac45524a42a6b5bbd710e1d771428ed.pdf","TL;DR":"We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score","paperhash":"anonymous|beyond_word_importance_contextual_decomposition_to_extract_interactions_from_lstms","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkRwGg-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper562/Authors"],"keywords":["interpretability","LSTM","natural language processing","sentiment analysis","interactions"]}},{"tddate":null,"ddate":null,"tmdate":1512222691476,"tcdate":1511796786076,"number":2,"cdate":1511796786076,"id":"B1qEe3txM","invitation":"ICLR.cc/2018/Conference/-/Paper562/Official_Review","forum":"rkRwGg-0Z","replyto":"rkRwGg-0Z","signatures":["ICLR.cc/2018/Conference/Paper562/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A promising paper on how interpretability can boost the performance LSTMs","rating":"7: Good paper, accept","review":"The authors address the problem of making LSTMs more interpretable via the contextual decomposition of the state vectors. By linearizing the updates in the recurrent network, the proposed scheme allows one to extract word importance information directly from the gating dynamics and infer word-to-word interactions. \n\nThe problem of making neural networks more understandable is important in general. For NLP, this relates to the ability of capturing phrase features that go beyond single-word importance scores. A nice contribution of the paper is to show that this can highly improve classification performance on the task of sentiment analysis. However, the author could have spent some more time in explaining the technical consequences of the proposed linear approximation. For example, why is the linear approximation always good? And what is the performance loss compared to a fully nonlinear network? \n\nThe experimental results suggest that the algorithm can outperform state-of-the-art methods on various tasks.\n\nSome questions:\n- is any of the existing algorithms used for the comparison supposed to capture interactions between words and phrases? If not, why is the proposed algorithm compared to them on interaction related tasks?\n- why the output of the algorithms is compared with the logistic regression score? May the fact that logistic regression is a linear model be linked to the good performance of the proposed method? Would it be possible to compare the output of the algorithms with human given scores on a small subset of words? \n- the recent success of LSTMs is often associated with their ability to learn complex and non-linear relationships but the proposed method is based on the linearization of the network. How can the algorithm be able to capture non-linear interactions? What is the difference between the proposed model and a simple linear model?   ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs","abstract":"The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), a novel algorithm for capturing the contributions of combinations of words or variables in terms of CD scores. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.","pdf":"/pdf/315f203e6ac45524a42a6b5bbd710e1d771428ed.pdf","TL;DR":"We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score","paperhash":"anonymous|beyond_word_importance_contextual_decomposition_to_extract_interactions_from_lstms","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkRwGg-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper562/Authors"],"keywords":["interpretability","LSTM","natural language processing","sentiment analysis","interactions"]}},{"tddate":null,"ddate":null,"tmdate":1512222691521,"tcdate":1511793324814,"number":1,"cdate":1511793324814,"id":"rJHhMjFgG","invitation":"ICLR.cc/2018/Conference/-/Paper562/Official_Review","forum":"rkRwGg-0Z","replyto":"rkRwGg-0Z","signatures":["ICLR.cc/2018/Conference/Paper562/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes a decomposition of LSTM's gates allowing to interpret the contribution of each element in a sequence to the prediction and how their representations are combined.","rating":"6: Marginally above acceptance threshold","review":"In this paper, the authors propose a new LSTM variant that allows to produce interpretations by capturing the contributions of the words to the final prediction and the way their learned representations are combined in order to yield that prediction. They propose a new approach that they call Contextual Decomposition (CD). Their approach consists of disambiguating interaction between LSTM’s gates where gates are linearized so the products between them is over linear sums of contributions from different factors. The hidden and cell states are also decomposed in terms of contributions to the “phrase” in question and contributions from elements outside of the phrase. The motivation of the proposed decomposition using LSTMs is that the latter are powerful at capturing complex non-linear interactions, so, it would be useful to observe how these interactions are handled and to interpret the LSTM’s predictions. As the authors intention is to build a way of interpreting LSTMs output and not to increase the model’s accuracy, the empirical results illustrate the ability of their decomposition of giving a plausible interpretation to the elements of a sentence. They compare their method with different existing method by illustrating samples from the Yelp Polarity and SST datasets. They also show the ability of separating the distribution of CD scores related to positive and negative phrases on respectively Yelp Polarity and SST.\n\nThe proposed approach is potentially of great benefit as it is simple and elegant and could lead to new methods in the same direction of research. The sample illustrations, the scatter plots and the CD score distributions are helpful to asses the benefit of the proposed approach.\n\nThe writing could be improved as it contains parts where it leads to confusion. The details related to the linearization (section 3.2.2), the training (4.1) could be improved. In equation 25, it is not clear what  π_{i}^(-1) and x_{π_{i}} represent but the example in equation 26 makes it clearer. The section would be clearer if each index and notation used is explained explicitly.\n\n(CD in known for Contrastive Divergence in the deep learning community. It would be better if Contextual Decomposition is not referred by CD.)\n\nTraining details are given in section 4.1 where the authors mention the integrated gradient baseline without mentioning the reference paper to it (however they do mention the reference paper at each of table 1 and 2). it would be clearer for the reader if the references are also mentioned in section 4.1 where integrated gradient is introduced. Along with the reference, a brief description of that baseline could be given. \n\nThe “Leave one out” baseline is never mentioned in text before section 4.4 (and tables 1 and 2). Neither the reference nor the description of the baseline are given. It would have been clearer to the reader if this had been the case.\n\nOverall, the paper contribution is of great benefit. The quality of the paper could be improved if the above explanations and details are given explicitly in the text.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs","abstract":"The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), a novel algorithm for capturing the contributions of combinations of words or variables in terms of CD scores. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.","pdf":"/pdf/315f203e6ac45524a42a6b5bbd710e1d771428ed.pdf","TL;DR":"We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score","paperhash":"anonymous|beyond_word_importance_contextual_decomposition_to_extract_interactions_from_lstms","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkRwGg-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper562/Authors"],"keywords":["interpretability","LSTM","natural language processing","sentiment analysis","interactions"]}},{"tddate":null,"ddate":null,"tmdate":1509739234549,"tcdate":1509126757919,"number":562,"cdate":1509739231899,"id":"rkRwGg-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkRwGg-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs","abstract":"The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), a novel algorithm for capturing the contributions of combinations of words or variables in terms of CD scores. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.","pdf":"/pdf/315f203e6ac45524a42a6b5bbd710e1d771428ed.pdf","TL;DR":"We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score","paperhash":"anonymous|beyond_word_importance_contextual_decomposition_to_extract_interactions_from_lstms","_bibtex":"@article{\n  anonymous2018beyond,\n  title={Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkRwGg-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper562/Authors"],"keywords":["interpretability","LSTM","natural language processing","sentiment analysis","interactions"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}