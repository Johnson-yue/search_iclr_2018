{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222828653,"tcdate":1511798238027,"number":2,"cdate":1511798238027,"id":"BkaCH2KlG","invitation":"ICLR.cc/2018/Conference/-/Paper967/Official_Review","forum":"BJ8c3f-0b","replyto":"BJ8c3f-0b","signatures":["ICLR.cc/2018/Conference/Paper967/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting extension of IWAEs using SMC","rating":"6: Marginally above acceptance threshold","review":"The authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure. The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.\n\nThe approach is interesting and the paper is well-written, however, I have some comments and questions:\n\n- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. This seems to mean that we should not expect for q -> p when K increases?\n- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made. If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC? Have you estimated the probability for positive vs. negative gradient values for  K=10? To me it looks like the probability of it being larger than zero is something like 2/3. K>10 is difficult to see from this plot alone.\n- Is there a typo in the bound given by eq. (17)? Seems like there are two identical terms. Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?\n- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS. Have you tried this for high-dimensional models as well? Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO. For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1? Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles? I suspect this lower number of particles might be model-dependent.\n\nMinor comments:\n- Section 1, first paragraph, last sentence, \"that\" -> \"than\"?\n- Section 3.2, \"... using which...\" formulation in two places in the firsth and second paragraph was a bit confusing\n- Page 7, second line, just \"IS\"?\n- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?\n- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Auto-Encoding Sequential Monte Carlo","abstract":"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","pdf":"/pdf/1336e9629689e1bc9c007ab1e51fe412ca3581c8.pdf","TL;DR":"We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.","paperhash":"anonymous|autoencoding_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018auto-encoding,\n  title={Auto-Encoding Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8c3f-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper967/Authors"],"keywords":["Variational Autoencoders","Inference amortization","Model learning","Sequential Monte Carlo","ELBOs"]}},{"tddate":null,"ddate":null,"tmdate":1512222828697,"tcdate":1511308626397,"number":1,"cdate":1511308626397,"id":"HkqLp4GeM","invitation":"ICLR.cc/2018/Conference/-/Paper967/Official_Review","forum":"BJ8c3f-0b","replyto":"BJ8c3f-0b","signatures":["ICLR.cc/2018/Conference/Paper967/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Auto-encoding Sequential Monte Carlo","rating":"3: Clear rejection","review":"Overall:\nI had a really hard time reading this paper because I found the writing to be quite confusing. For this reason I cannot recommend publication as I am not sure how to evaluate the paperâ€™s contribution. \n\nSummary\nThe authors study state space models in the unsupervised learning case. We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters. We want to infer a posterior density given some data.\n\nThe authors propose an algorithm which uses sequential Monte Carlo + autoencoders. They use a REINFORCE-like algorithm to differentiate through the Monte Carlo. The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters.\n\nThe authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments. \n\nThe proposed algorithm seems novel, but I do not understand a few points which make it hard to judge the contribution. Note that here I am assuming full technical correctness of the paper (and still cannot recommend acceptance).\n\nIs the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)? This was very unclear to me.\n\nThe introduction/experiments section of the paper is not well motivated. What is the problem the authors are trying to solve with AESMC (over existing methods)? Is it scalability? Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)? \n\nThe experiments feel lacking. There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) method of IWAE. We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge? Does AESMC give a better generative model?). The authors discuss the advantages of SMC and say that is scales better than other methods, it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Auto-Encoding Sequential Monte Carlo","abstract":"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","pdf":"/pdf/1336e9629689e1bc9c007ab1e51fe412ca3581c8.pdf","TL;DR":"We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.","paperhash":"anonymous|autoencoding_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018auto-encoding,\n  title={Auto-Encoding Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8c3f-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper967/Authors"],"keywords":["Variational Autoencoders","Inference amortization","Model learning","Sequential Monte Carlo","ELBOs"]}},{"tddate":null,"ddate":null,"tmdate":1510092383291,"tcdate":1509137567295,"number":967,"cdate":1510092361045,"id":"BJ8c3f-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ8c3f-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Auto-Encoding Sequential Monte Carlo","abstract":"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","pdf":"/pdf/1336e9629689e1bc9c007ab1e51fe412ca3581c8.pdf","TL;DR":"We build on auto-encoding sequential Monte Carlo, gain new theoretical insights and develop an improved training procedure based on those insights.","paperhash":"anonymous|autoencoding_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018auto-encoding,\n  title={Auto-Encoding Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8c3f-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper967/Authors"],"keywords":["Variational Autoencoders","Inference amortization","Model learning","Sequential Monte Carlo","ELBOs"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}