{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222592184,"tcdate":1511885877817,"number":3,"cdate":1511885877817,"id":"B10Nn-jlf","invitation":"ICLR.cc/2018/Conference/-/Paper230/Official_Review","forum":"SkF2D7g0b","replyto":"SkF2D7g0b","signatures":["ICLR.cc/2018/Conference/Paper230/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A very thorough empirical examination of practical black-box attacks on NN classifiers. ","rating":"7: Good paper, accept","review":"The authors consider new attacks for generating adversarial samples against neural networks. In particular, they are interested in approximating gradient-based white-box attacks such as FGSM in a black-box setting by estimating gradients from queries to the classifier. They assume that the attacker is able to query, for any example x, the vector of probabilities p(x) corresponding to each class.\n\nGiven such query access it’s trivial to estimate the gradients of p using finite differences. As a consequence one can implement FGSM using these estimates assuming cross-entropy loss, as well as a logit-based loss. They consider both iterative and single-step FGSM attacks in the targeted (i.e. the adversary’s goal is to switch the example’s label to a specific alternative label) and un-targeted settings (any mislabelling is a success). They compare themselves to transfer black-box attacks, where the adversary trains a proxy model and generates the adversarial sample by running a white-box attack on that model.  For a number of target classifiers on both MNIST and CIFAR-10, they show that these attacks outperform the transfer-based attacks, and are comparable to white-box attacks, while maintaining low distortion on the attack samples. \n\nOne drawback of estimating gradients using finite differences is that the number of queries required scales with the dimensionality of the examples, which can be prohibitive in the case of images. They therefore describe two practical approaches for query reduction — one based on random feature grouping, and the other on PCA (which requires access to training data). They once again demonstrate the effectiveness of these methods across a number of models and datasets, including models deploying adversarially trained defenses. \n\nFinally, they demonstrate compelling real-world deployment against Clarifai classification models designed to flag “Not Safe for Work” content. \n\nOverall, the paper provides a very thorough experimental examination of a practical black-box attack that can be deployed against real-world systems. While there are some similarities with Chen et al. with respect to utilizing finite-differences to estimate gradients, I believe the work is still valuable for its very thorough experimental verification, as well as the practicality of their methods. The authors may want to be more explicit about their claim in the Related Work section that the running time of their attack is “40x” less than that of Chen et al. While this is believable, there is no running time comparison in the body of the paper. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1512222592223,"tcdate":1511835658236,"number":2,"cdate":1511835658236,"id":"rJGGOrcxz","invitation":"ICLR.cc/2018/Conference/-/Paper230/Official_Review","forum":"SkF2D7g0b","replyto":"SkF2D7g0b","signatures":["ICLR.cc/2018/Conference/Paper230/AnonReviewer1"],"readers":["everyone"],"content":{"title":"About assumptions and metrics","rating":"3: Clear rejection","review":"\nQuality: The paper studies an important problem given that public ML APIs are now becoming available. More specifically, the authors study black-box attacks based on gradient estimation. This means that adversaries have no access to the underlying model.\n\nClarity: The paper is clear and well-written. Some parts are a bit redundant, so more space of the main body of the paper could be devoted information provided in the appendix and would help with the flow (e.g., description of the models A, B, C; logit-based loss; etc.). This would also provide room for discussing the targeted attacks and the tranferability-based attacks.\n\nOriginality: While black-box attacks are of greater interest than withe-box attacks, I found the case considered here of modest interest. The assumption that the loss would be known, but not the gradient is relatively narrow. And why is not possible to compute the gradient exactly in this case? Also, it was not clear what how \\delta can be chosen in practice to increase the performance of the attack. Could the authors comment on that?\n\nSignificance: The results in the paper are encouraging, but it is not clear whether the setting is realistic. The main weakness of this paper is that it does not state the assumptions made and under which conditions these attacks are valid. Those have to be deduced from the main text and not all are clear and many questions remain, making it difficult to see when such an attack is a risk and what is the actual experimental set-up. For example, what does it mean that attackers have access to the training set and when does that occur? Is it assumed that the API uses the adversarial example for training as well or not? How are the surrogate models trained and what are they trying to optimize and/or what do they match? In which situations do attackers have access to the loss, but not the gradient? How sensitive are the results to a loss mismatch? Finally, I do not understand the performance metric proposed by the authors. It is always possible to get an arbitrarily high success rate unless one fixes the distortion. What would be the success rate if the distortion was equal to the distortion of white-box attacks?  And how sensitive are the results to \\epsilon (and how can it be chosen by an attacker in practice)?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1512222592262,"tcdate":1511810242059,"number":1,"cdate":1511810242059,"id":"Hk96V1clf","invitation":"ICLR.cc/2018/Conference/-/Paper230/Official_Review","forum":"SkF2D7g0b","replyto":"SkF2D7g0b","signatures":["ICLR.cc/2018/Conference/Paper230/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Empirical study of standard adversarial attacks + basic gradient approximation methods","rating":"4: Ok but not good enough - rejection","review":"This paper generates adversarial examples using the fast gradient sign (FGS) and iterated fast gradient sign (IFGS) methods, but replacing the gradient computation with finite differences or another gradient approximation method. Since finite differences is expensive in high dimensions, the authors propose using directional derivatives based on random feature groupings or PCA. \n\nThis paper would be much stronger if it surveyed a wider variety of gradient-free optimization methods. Notably, there's two important black-box optimization baselines that were not included: simultaneous perturbation stochastic approximation ( https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation), which avoids computing the gradient explicitly, and evolutionary strategies ( https://blog.openai.com/evolution-strategies/ ), a similar method that uses several random directions to estimate a better descent direction.\n\nThe gradient approximation methods proposed in this paper may or may not be better than SPSA or ES. Without a direct comparison, it's hard to know.  Thus, the main contribution of this paper is in demonstrating that gradient approximation methods are sufficient for generating good adversarial attacks and applying those attacks to Clarifai models. That's interesting and useful to know, but is still a relatively small contribution, making this paper borderline. I lean towards rejection, since the paper proposes new methods without comparing to or even mentioning well-known alternatives.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1511171051005,"tcdate":1511171051005,"number":2,"cdate":1511171051005,"id":"ry7lNQexM","invitation":"ICLR.cc/2018/Conference/-/Paper230/Public_Comment","forum":"SkF2D7g0b","replyto":"ryDHmbY1G","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Clarification comment","comment":"Hi, so it seems that the claimed novelty here is a way of reducing the number of queries using finite differences, however the ZOO attack also uses some novel techniques to reduce queries. It would be extremely useful to provide a side-by-side attack comparison with ZOO so we can infer which attack is more effective under various settings. You can find their code here https://github.com/huanzhang12/ZOO-Attack "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1510703959871,"tcdate":1510703935480,"number":1,"cdate":1510703935480,"id":"ryDHmbY1G","invitation":"ICLR.cc/2018/Conference/-/Paper230/Official_Comment","forum":"SkF2D7g0b","replyto":"SkYyvAX1G","signatures":["ICLR.cc/2018/Conference/Paper230/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper230/Authors"],"content":{"title":"Clarification of similarities and differences with regard to Chen et al. (2017)","comment":"The concurrent work from Chen et al. also proposes a black-box attack that uses queries from a model that exposes confidence scores. As you rightly note, both ZOO and our proposed methods do have in common that they use finite differences to estimate the derivative of a function. This shared part is a well-known method, for which we provide a citation (Spall, 2005).\n\nBeyond that, the attack methods proceed differently. We propose attacks that compute an adversarial perturbation, approximating FGSM and iterative FGS. On the other hand, ZOO approximates the Adam optimizer, while trying to perform coordinate descent on the loss function proposed by Carlini and Wagner (2016).\n\nWe further provide new ways of reducing the number of queries required. Thus, our claim to novelty is not in using finite differences to estimate the gradient of a model, but the idea of estimating the gradient in a number of new query-reduced ways. Our work evaluates new attacks that use these estimates as well as known black-box attacks, as an additional contribution.\n\nBecause of the relevance of Chen et al.’s work to the threat model, we will add a clarification in the “Related Work” section of the Introduction, as well as in Section 3, noting the fact that Chen et al. used the finite difference technique in a similar setting.\n\nThank you for the comment!\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1510364897249,"tcdate":1510364897249,"number":1,"cdate":1510364897249,"id":"SkYyvAX1G","invitation":"ICLR.cc/2018/Conference/-/Paper230/Public_Comment","forum":"SkF2D7g0b","replyto":"SkF2D7g0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Claim of novelty over Chen et al. (2017)","comment":"The submission cites the paper by Chen et al. (2017), which proposes \"ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models\". However, the submission goes on to claim the method of finite differences as a novel contribution, even though the cited paper by Chen et al. has already proposed it.\n\nThe \"Gradient Estimation black-box attack based on the method of finite differences\" presented in Section 3 and Section 3.1 of this submission, using a \"two-sided approximation of the gradient\", is identical to what is proposed in ZOO, which uses the \"symmetric difference quotient to estimate the gradient\" (Chen et al. 2017, equation 6).\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]}},{"tddate":null,"ddate":null,"tmdate":1509739416610,"tcdate":1509074864623,"number":230,"cdate":1509739413954,"id":"SkF2D7g0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkF2D7g0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring the Space of Black-box Attacks on Deep Neural Networks","abstract":"Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model, which do not rely on transferability. We also propose strategies to reduce the number of queries required to generate each adversarial sample to a constant. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks, and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to white-box attacks. We also apply the Gradient Estimation attacks against a real-world content moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the- art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.","pdf":"/pdf/b2c9be583135dabdb918357920bddd9e2a53bca5.pdf","TL;DR":"Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks","paperhash":"anonymous|exploring_the_space_of_blackbox_attacks_on_deep_neural_networks","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring the Space of Black-box Attacks on Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkF2D7g0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper230/Authors"],"keywords":["adversarial machine learning","black-box attacks"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}