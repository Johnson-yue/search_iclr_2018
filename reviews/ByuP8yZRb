{"notes":[{"tddate":null,"ddate":null,"tmdate":1512236459866,"tcdate":1512236459866,"number":3,"cdate":1512236459866,"id":"B143HDlWM","invitation":"ICLR.cc/2018/Conference/-/Paper483/Official_Review","forum":"ByuP8yZRb","replyto":"ByuP8yZRb","signatures":["ICLR.cc/2018/Conference/Paper483/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Minor theoretical contribution; main focus on experiments but experiments may be unfair as some model-dependent parameters kept fixed across the methods","rating":"4: Ok but not good enough - rejection","review":"The authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized. The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model.\n\nThe way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.\n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good?\n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Censoring Representations with Multiple-Adversaries over Random Subspaces","abstract":"In practice, often there are explicit constraints on the representations that are acceptable in a machine learning application in the real world; for example, representations of data must not contain identifying information so as to avoid privacy issues. This paper improves the performance of the recently proposed adversarial feature learning approach (AFL) for incorporating such explicit constraints, by introducing the concept of the vulnerableness of the adversary. In AFL, the censoring representation is done by training the networks to deceive the adversary that try to predict the sensitive information from the network, and the success of the AFL relies on the choice of the adversary. This motivate the use of high capacity networks as an adversary for improving the performance; however, this approach does not work well in practice as reported in this paper. Instead of the capacity of networks, this paper proposes to consider the vulnerableness in design of the adversary, i.e., the adversary should be designed not to be easily fooled. We also propose a method multiple adversaries over random subspaces (MARS) that instantiate the concept, and provides empirical validations on the efficacy of the proposed method compared to the various baselines, indicating the importance of the proposed concept. \nThis is significant because it gives new implications about designing the adversary, which is important to improve the performance of AFL framework.","pdf":"/pdf/113b90dea7413f96af2de0dd292742e50c6064b8.pdf","TL;DR":"This paper improves the quality of the recently proposed adversarial feature leaning (AFL) approach for incorporating explicit constrains to representations, by introducing the concept of the {\\em vulnerableness} of the adversary. ","paperhash":"anonymous|censoring_representations_with_multipleadversaries_over_random_subspaces","_bibtex":"@article{\n  anonymous2018censoring,\n  title={Censoring Representations with Multiple-Adversaries over Random Subspaces},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByuP8yZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper483/Authors"],"keywords":["Adversarial Training","Privacy Protection","Random Subspace"]}},{"tddate":null,"ddate":null,"tmdate":1512222666050,"tcdate":1511902763562,"number":2,"cdate":1511902763562,"id":"rJNERHjlf","invitation":"ICLR.cc/2018/Conference/-/Paper483/Official_Review","forum":"ByuP8yZRb","replyto":"ByuP8yZRb","signatures":["ICLR.cc/2018/Conference/Paper483/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Multiple adversaries for increased privacy performs well","rating":"6: Marginally above acceptance threshold","review":"MARS is suggested to combine multiple adversaries with different roles.\nExperiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables.\n\nExperiments a are satisfying and show good performance when compared to other methods.\n\nIt could be made clearer how significance is tested given the frequent usage of the term.\n\nThe idea is slightly novel, and the framework otherwise state-of-the-art.\n\nThe paper is well written, but can use some proof-reading.\n\nReferencing is okay.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Censoring Representations with Multiple-Adversaries over Random Subspaces","abstract":"In practice, often there are explicit constraints on the representations that are acceptable in a machine learning application in the real world; for example, representations of data must not contain identifying information so as to avoid privacy issues. This paper improves the performance of the recently proposed adversarial feature learning approach (AFL) for incorporating such explicit constraints, by introducing the concept of the vulnerableness of the adversary. In AFL, the censoring representation is done by training the networks to deceive the adversary that try to predict the sensitive information from the network, and the success of the AFL relies on the choice of the adversary. This motivate the use of high capacity networks as an adversary for improving the performance; however, this approach does not work well in practice as reported in this paper. Instead of the capacity of networks, this paper proposes to consider the vulnerableness in design of the adversary, i.e., the adversary should be designed not to be easily fooled. We also propose a method multiple adversaries over random subspaces (MARS) that instantiate the concept, and provides empirical validations on the efficacy of the proposed method compared to the various baselines, indicating the importance of the proposed concept. \nThis is significant because it gives new implications about designing the adversary, which is important to improve the performance of AFL framework.","pdf":"/pdf/113b90dea7413f96af2de0dd292742e50c6064b8.pdf","TL;DR":"This paper improves the quality of the recently proposed adversarial feature leaning (AFL) approach for incorporating explicit constrains to representations, by introducing the concept of the {\\em vulnerableness} of the adversary. ","paperhash":"anonymous|censoring_representations_with_multipleadversaries_over_random_subspaces","_bibtex":"@article{\n  anonymous2018censoring,\n  title={Censoring Representations with Multiple-Adversaries over Random Subspaces},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByuP8yZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper483/Authors"],"keywords":["Adversarial Training","Privacy Protection","Random Subspace"]}},{"tddate":null,"ddate":null,"tmdate":1512222667333,"tcdate":1511814897402,"number":1,"cdate":1511814897402,"id":"SJtlwgqlf","invitation":"ICLR.cc/2018/Conference/-/Paper483/Official_Review","forum":"ByuP8yZRb","replyto":"ByuP8yZRb","signatures":["ICLR.cc/2018/Conference/Paper483/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Improved performance in obtaining censoring representations, but contribution not quite signficant enough","rating":"5: Marginally below acceptance threshold","review":"- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations. They show that their idea is effective in reducing private information leakage, but this idea alone might not be signifcant enough as a contribution. \n\n- The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction. Indeed judging from the large variance in the accuracy of predicting S in Table 1a-c for single adversaries, I suspect one of the main advantage of the current MARS method comes from variance reduction. The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks.  \n\n- The definition of S, the private information set, is not clear. There is no statement about it in the experiments section, and I assume S is the subject identity. But this makes the train-test split described in 4.1 rather odd, since there is no overlap of subjects in the train-test split. We need clarifications on these experimental details. \n\n- Judging from Figure 2 and Table 1, all the methods tested are not effective in hiding the private information S in the learned representation. Even though the proposed method works better, the prediction accuracies of S are still high.  \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Censoring Representations with Multiple-Adversaries over Random Subspaces","abstract":"In practice, often there are explicit constraints on the representations that are acceptable in a machine learning application in the real world; for example, representations of data must not contain identifying information so as to avoid privacy issues. This paper improves the performance of the recently proposed adversarial feature learning approach (AFL) for incorporating such explicit constraints, by introducing the concept of the vulnerableness of the adversary. In AFL, the censoring representation is done by training the networks to deceive the adversary that try to predict the sensitive information from the network, and the success of the AFL relies on the choice of the adversary. This motivate the use of high capacity networks as an adversary for improving the performance; however, this approach does not work well in practice as reported in this paper. Instead of the capacity of networks, this paper proposes to consider the vulnerableness in design of the adversary, i.e., the adversary should be designed not to be easily fooled. We also propose a method multiple adversaries over random subspaces (MARS) that instantiate the concept, and provides empirical validations on the efficacy of the proposed method compared to the various baselines, indicating the importance of the proposed concept. \nThis is significant because it gives new implications about designing the adversary, which is important to improve the performance of AFL framework.","pdf":"/pdf/113b90dea7413f96af2de0dd292742e50c6064b8.pdf","TL;DR":"This paper improves the quality of the recently proposed adversarial feature leaning (AFL) approach for incorporating explicit constrains to representations, by introducing the concept of the {\\em vulnerableness} of the adversary. ","paperhash":"anonymous|censoring_representations_with_multipleadversaries_over_random_subspaces","_bibtex":"@article{\n  anonymous2018censoring,\n  title={Censoring Representations with Multiple-Adversaries over Random Subspaces},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByuP8yZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper483/Authors"],"keywords":["Adversarial Training","Privacy Protection","Random Subspace"]}},{"tddate":null,"ddate":null,"tmdate":1509739277774,"tcdate":1509123680171,"number":483,"cdate":1509739275118,"id":"ByuP8yZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByuP8yZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Censoring Representations with Multiple-Adversaries over Random Subspaces","abstract":"In practice, often there are explicit constraints on the representations that are acceptable in a machine learning application in the real world; for example, representations of data must not contain identifying information so as to avoid privacy issues. This paper improves the performance of the recently proposed adversarial feature learning approach (AFL) for incorporating such explicit constraints, by introducing the concept of the vulnerableness of the adversary. In AFL, the censoring representation is done by training the networks to deceive the adversary that try to predict the sensitive information from the network, and the success of the AFL relies on the choice of the adversary. This motivate the use of high capacity networks as an adversary for improving the performance; however, this approach does not work well in practice as reported in this paper. Instead of the capacity of networks, this paper proposes to consider the vulnerableness in design of the adversary, i.e., the adversary should be designed not to be easily fooled. We also propose a method multiple adversaries over random subspaces (MARS) that instantiate the concept, and provides empirical validations on the efficacy of the proposed method compared to the various baselines, indicating the importance of the proposed concept. \nThis is significant because it gives new implications about designing the adversary, which is important to improve the performance of AFL framework.","pdf":"/pdf/113b90dea7413f96af2de0dd292742e50c6064b8.pdf","TL;DR":"This paper improves the quality of the recently proposed adversarial feature leaning (AFL) approach for incorporating explicit constrains to representations, by introducing the concept of the {\\em vulnerableness} of the adversary. ","paperhash":"anonymous|censoring_representations_with_multipleadversaries_over_random_subspaces","_bibtex":"@article{\n  anonymous2018censoring,\n  title={Censoring Representations with Multiple-Adversaries over Random Subspaces},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByuP8yZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper483/Authors"],"keywords":["Adversarial Training","Privacy Protection","Random Subspace"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}