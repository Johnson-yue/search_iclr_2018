{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222702670,"tcdate":1512108560904,"number":3,"cdate":1512108560904,"id":"BkYfM_Rgz","invitation":"ICLR.cc/2018/Conference/-/Paper620/Official_Review","forum":"rkw-jlb0W","replyto":"rkw-jlb0W","signatures":["ICLR.cc/2018/Conference/Paper620/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Marginally below acceptance threshold","rating":"5: Marginally below acceptance threshold","review":"It is clear that the problem studied in this paper is interesting. However, after reading through the manuscript, it is not clear to me what are the real contributions made in this paper. I also failed to find any rigorous results on generalization bounds. In this case, I cannot recommend the acceptance of this paper. ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Lipschitz networks and Dudley GANs","abstract":"Generative adversarial networks (GANs) have enjoyed great success, however often suffer instability during training which motivates many attempts to resolve this issue. Theoretical explanation for the cause of instability is provided in Wasserstein GAN (WGAN), and wasserstein distance is proposed to stablize the training. Though WGAN is indeed more stable than previous GANs, it takes much more iterations and time to train. This is because the ways to ensure Lipschitz condition in WGAN (such as weight-clipping) significantly limit the capacity of the network. In this paper, we argue that it is beneficial to ensure Lipschitz condition as well as maintain sufficient capacity and expressiveness of the network. To facilitate this, we develop both theoretical and practical building blocks, using which one can construct different neural networks using a large range of metrics, as well as ensure Lipschitz condition and sufficient capacity of the networks. Using the proposed building blocks, and a special choice of a metric called Dudley metric, we propose Dudley GAN that outperforms the state of the arts in both convergence and sample quality. We discover a natural link between Dudley GAN (and its extension) and empirical risk minimization, which gives rise to generalization analysis.","pdf":"/pdf/2bcdf0c693486231fa23445806fb9090c6bcd04c.pdf","paperhash":"anonymous|deep_lipschitz_networks_and_dudley_gans","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Lipschitz networks and Dudley GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkw-jlb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper620/Authors"],"keywords":["GAN","Lipschitz neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222702712,"tcdate":1512097098690,"number":2,"cdate":1512097098690,"id":"H17IrS0lz","invitation":"ICLR.cc/2018/Conference/-/Paper620/Official_Review","forum":"rkw-jlb0W","replyto":"rkw-jlb0W","signatures":["ICLR.cc/2018/Conference/Paper620/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well-written, but unlikely a big step forward for GANs","rating":"5: Marginally below acceptance threshold","review":"The authors propose a different type of GAN--the Dudley GAN--that is related to the Dudley metric. In fact, it is very much like the WGAN, but rather than just imposing the function class to have a bounded gradient, they also impose it to be bounded itself. This is argued to be more stable than the WGAN, as gradient clipping is said not necessary for the Dudley GAN. The authors empirically show that the Dudley GAN achieves a greater LL than WGAN for the MNIST and CIFAR-10 datasets.\n\nThe main idea [and its variants] looks solid, but with the plethora of GANs in the literature now, after reading I'm still left wondering why this GAN is significantly better than others [BEGAN, WGAN, etc.]. It is clear that imposing the quadratic penalty in equation (3) is really the same constraint as the Dudley norm? The big contribution of the paper seems to be that adding some L_inf regularization to the function class helps preclude gradient clipping, but after reading I'm unsure why this is \"the right thing\" to do in this case. We know that convergence in the Wasserstein metric is stronger than the Dudley metric, so why is using the weaker metric overweighed by the benefits in training?\n\nNits: Since the function class is parameterized by a NN, the IPM is not actually the Dudley metric between the two distributions. One would have to show that the NN is dense in Dudley unit ball w.r.t. L_inf norm, but this sort of misnaming had started with the \"Wasserstein\" GAN.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Lipschitz networks and Dudley GANs","abstract":"Generative adversarial networks (GANs) have enjoyed great success, however often suffer instability during training which motivates many attempts to resolve this issue. Theoretical explanation for the cause of instability is provided in Wasserstein GAN (WGAN), and wasserstein distance is proposed to stablize the training. Though WGAN is indeed more stable than previous GANs, it takes much more iterations and time to train. This is because the ways to ensure Lipschitz condition in WGAN (such as weight-clipping) significantly limit the capacity of the network. In this paper, we argue that it is beneficial to ensure Lipschitz condition as well as maintain sufficient capacity and expressiveness of the network. To facilitate this, we develop both theoretical and practical building blocks, using which one can construct different neural networks using a large range of metrics, as well as ensure Lipschitz condition and sufficient capacity of the networks. Using the proposed building blocks, and a special choice of a metric called Dudley metric, we propose Dudley GAN that outperforms the state of the arts in both convergence and sample quality. We discover a natural link between Dudley GAN (and its extension) and empirical risk minimization, which gives rise to generalization analysis.","pdf":"/pdf/2bcdf0c693486231fa23445806fb9090c6bcd04c.pdf","paperhash":"anonymous|deep_lipschitz_networks_and_dudley_gans","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Lipschitz networks and Dudley GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkw-jlb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper620/Authors"],"keywords":["GAN","Lipschitz neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222702751,"tcdate":1510797018620,"number":1,"cdate":1510797018620,"id":"SyXkyOqJz","invitation":"ICLR.cc/2018/Conference/-/Paper620/Official_Review","forum":"rkw-jlb0W","replyto":"rkw-jlb0W","signatures":["ICLR.cc/2018/Conference/Paper620/AnonReviewer2"],"readers":["everyone"],"content":{"title":"There are certain contributions in literatures, but the novelty may be not significant.","rating":"8: Top 50% of accepted papers, clear accept","review":"Ensuring Lipschitz condition in neural nets is essential of stablizing GANs. This paper proposes two contraint-based optimzation to ensure the Lips condtions , and these proposed approaches maintain suffcient capacity, as well as expressiveness of the network.  A simple theoritical result is given by emprical risk minimization. The content of this paper\nis written clearly, and there are certain contribution and orginality in the literature. However, I am not sure that the novelty is\nsignificant, since I think that the idea of proposing their optimization is trival. Here I am concerned with the following two questions:\n(1) How to parameterize the function space of f_w or h_w, since they are both multivariate and capacity of the network will be\nreduced if the used way of parametering functions is adopted inappropriatily.\n(2) The theoretical result in (4)  doesnot contain the information of Rademacher complexity, and it may be suboptimal in some sense. Besides, the parameter $\\gamma$ appears in the discriminator, which contradicts its role on the contraint of functions space.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Lipschitz networks and Dudley GANs","abstract":"Generative adversarial networks (GANs) have enjoyed great success, however often suffer instability during training which motivates many attempts to resolve this issue. Theoretical explanation for the cause of instability is provided in Wasserstein GAN (WGAN), and wasserstein distance is proposed to stablize the training. Though WGAN is indeed more stable than previous GANs, it takes much more iterations and time to train. This is because the ways to ensure Lipschitz condition in WGAN (such as weight-clipping) significantly limit the capacity of the network. In this paper, we argue that it is beneficial to ensure Lipschitz condition as well as maintain sufficient capacity and expressiveness of the network. To facilitate this, we develop both theoretical and practical building blocks, using which one can construct different neural networks using a large range of metrics, as well as ensure Lipschitz condition and sufficient capacity of the networks. Using the proposed building blocks, and a special choice of a metric called Dudley metric, we propose Dudley GAN that outperforms the state of the arts in both convergence and sample quality. We discover a natural link between Dudley GAN (and its extension) and empirical risk minimization, which gives rise to generalization analysis.","pdf":"/pdf/2bcdf0c693486231fa23445806fb9090c6bcd04c.pdf","paperhash":"anonymous|deep_lipschitz_networks_and_dudley_gans","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Lipschitz networks and Dudley GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkw-jlb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper620/Authors"],"keywords":["GAN","Lipschitz neural network"]}},{"tddate":null,"ddate":null,"tmdate":1509739197419,"tcdate":1509128959164,"number":620,"cdate":1509739194750,"id":"rkw-jlb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkw-jlb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Lipschitz networks and Dudley GANs","abstract":"Generative adversarial networks (GANs) have enjoyed great success, however often suffer instability during training which motivates many attempts to resolve this issue. Theoretical explanation for the cause of instability is provided in Wasserstein GAN (WGAN), and wasserstein distance is proposed to stablize the training. Though WGAN is indeed more stable than previous GANs, it takes much more iterations and time to train. This is because the ways to ensure Lipschitz condition in WGAN (such as weight-clipping) significantly limit the capacity of the network. In this paper, we argue that it is beneficial to ensure Lipschitz condition as well as maintain sufficient capacity and expressiveness of the network. To facilitate this, we develop both theoretical and practical building blocks, using which one can construct different neural networks using a large range of metrics, as well as ensure Lipschitz condition and sufficient capacity of the networks. Using the proposed building blocks, and a special choice of a metric called Dudley metric, we propose Dudley GAN that outperforms the state of the arts in both convergence and sample quality. We discover a natural link between Dudley GAN (and its extension) and empirical risk minimization, which gives rise to generalization analysis.","pdf":"/pdf/2bcdf0c693486231fa23445806fb9090c6bcd04c.pdf","paperhash":"anonymous|deep_lipschitz_networks_and_dudley_gans","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Lipschitz networks and Dudley GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkw-jlb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper620/Authors"],"keywords":["GAN","Lipschitz neural network"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}