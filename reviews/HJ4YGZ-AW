{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222710651,"tcdate":1512077988678,"number":3,"cdate":1512077988678,"id":"SkTs5lAxf","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Confusing paper, poor experiments and missing a lot of references.","rating":"4: Ok but not good enough - rejection","review":"The paper claims that \"Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures\", which is false, see \"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\", \"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\", \"DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks\" or \"Deep Compositional Question Answering with Neural Module Networks\".\n\nThe Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address.\n\n\"The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles\" Doesn't refer to any specific kind of layer, or what it make it special.\n\nThe idea of using outer product as a layer has already been explored in \"Multimodal compact bilinear pooling for visual question answering and visual grounding\"\n\nThe following paragraph in page 2 is not clear, very confusing:\nThe work reported here .... their categories\n\n\nIn page 3 authors claim that the \"vectors are linearly independent\" but didn't specify how they enforce that.\n\nFigure 3 contradicts Figure 1, not clear what are the inputs for module S.\n\nThe experiments reported in Table1 are useless, there a tons of previous work with much better results, see \nhttps://competitions.codalab.org/competitions/3221#results\n\nEven the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. \n\nThere is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. \n\nThe experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset.\n\nThe text is missing a lot of references, for example:\n - page 2 GSC\n - page 2 The first approach takes the detected by a CNN ....\n - page 3 previous work where TPRs are hand-crafted","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to 1) image captioning, 2)\nclassification of the part of speech of a word, and 3)\nidentification of the phrase structure of a sentence. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/f304dd25eac560332dace51049d091620a549ad2.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning","part of speech tagger","phrase detection"]}},{"tddate":null,"ddate":null,"tmdate":1512222710689,"tcdate":1511801987685,"number":2,"cdate":1511801987685,"id":"HJ2YETtxz","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper adapts a classical idea of Tensor Product Representation (TPN) which basically talks about how connectionist models can capture syntax explicitly and adapts the TPN as an inductive bias in image caption generation, and shows the generality of the learnt representations to tasks such as POS tagging and phrase classification. ","rating":"5: Marginally below acceptance threshold","review":"**Strengths**\nThe approach to sentence generation makes a lot of sense -- and provides a potentially elegant manner to incorporate or provide the model with the inductive bias that language has syntax and semantics, by leveraging a classical idea called Tensor Product Representation and showing how to adapt it to modern deep learning architectures and “learn” syntax and semantics end to end. Results on image captioning models indicate that the proposed approach might be promising. As a by-product, the paper also evaluates the model on POS tagging and shows that one can do fairly well using the representations learned in the TPGN. \n\n**Weakness**\nMy main concerns are the lack of appropriate baselines to establish concretely the contribution of the TPGN. It would be good to address issues under “Baselines” below.\n\nApproach:\n1. It would be nice to explain clearly why the pretraining of the TPGN with the LSTM input is needed. Is the idea that one would want to feed the representation of the entire representation as input in order to infer what “S_n” should be? Why is it then justified to feed in the image input instead? Also, in the second stage, the image features need not correspond to the LSTM feature dimensions, which means that the pretraining seems unprincipled. A better solution would have been to learn a joint embedding of image captions and labels (say via. ranking), and then use the embedding for the caption as input to the TPGN. This would ensure that when we use images, the model sees input that is appropriately “aligned”. A discussion why this is not needed or implementing this seems important.\n\nMinor Points:\n1.  “There are mainly two approaches to natural language generation in image captioning. The first approach takes the words detected by a CNN as input, and uses a probabilistic model, such as a maximum entropy (ME) language model, to arrange the detected words into a sentence.” -- can cite Fang. et.al [A]\n\nBaselines:\n1. The arXiv version and the PAMI version of the Neural Image Captioning paper (Vinyals, 2015) does report numbers on METEOR and CIDEr metrics, so they should be used to populate Table. 1 for completeness. Also, it would be good to clarify which split of MSCOCO Table. 1 reports results on -- is it the 40K large validation split or the 5K validation/test split released by (Karpathy, 2015)? Clarifying this would be nice since the numbers seem a bit on the lower side.\n\n2. What are the relative number of parameters in the Vinyals et.al. baseline and the proposed TPGN model? Would having an LSTM with twice the number of layers (by stacking them) or twice the size of the hidden state do better?\n\n3. What if we used the hidden state of a regular LSTM decoder to do POS tagging? How well would that do? Does the TPN capture any more syntactic structure than an LSTM decoder (Table. 2). This seems to be an important result to report.\n\nClarity:\n1. Page 7.: “We also implemented the latest ResNet feature” -- would be good be explicit which resnet model is used.\n\nReferences:\n[A]: Fang, Hao, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, et al. 2014. “From Captions to Visual Concepts and Back.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1411.4952.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to 1) image captioning, 2)\nclassification of the part of speech of a word, and 3)\nidentification of the phrase structure of a sentence. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/f304dd25eac560332dace51049d091620a549ad2.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning","part of speech tagger","phrase detection"]}},{"tddate":null,"ddate":null,"tmdate":1512222710730,"tcdate":1511452595994,"number":1,"cdate":1511452595994,"id":"HknhJ_Exf","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Worthwhile goal of bringing together GSC and DL; really poor presentation.","rating":"4: Ok but not good enough - rejection","review":"I have a huge amount of sympathy for this work, and was really hoping to read a well-presented paper setting out how to cleanly integrate Smolensky's theory with deep learning, ideally (but not necessarily) with some decent empirical results. If that had been the case, I would certainly have been recommending acceptance, since ICLR would benefit from the alternative perspective that Smolensky's work provides, compared to the majority of work in Deep Learning. The empirical results are decent, but the presentation requires too much work to warrant acceptance at ICLR for this year. There are also some questionable decisions made regarding the NLP evaluations.\n\nMore detailed comments.\n\no Just call the pos tagging task \"POS tagging\". Talking about classification of the part of speech of *a* word makes it sound like you're tagging a single word in isolation.\n\no The statement that language structures can't be integrated with DL is a hopeless misrepresentation of current practice in NLP, and misses a large body of existing work. There's obviously Richard Socher's work on integrating DL and the output of eg the Stanford parser, but also a current raft of work on trying to induce tree structures automatically via a DL framework and task-based objective. Two examples, by no means exhaustive:\n\nLearning to Compose Words into Sentences with Reinforcement Learning\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling.\nICLR 2017.\n\nJointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs\nJean Maillard, Stephen Clark, Dani Yogatama.\n\no It's not at all clear to me what the third task is - something like chunking. Identification of the phrase structure sounds like parsing, but you're not doing full parsing. This needs explaining fully. There are various standard NLP tasks related to identifying phrase structure - I would just do one of those, using one of the standard datasets, then there won't be any confusion.\n\no The description of tagging on p.2 mentions MEMMs too much - these were superseded by CRFs, which I think is what you mean to refer to.\n\no The reference to a maximum entropy language model is a little odd, since as far as I know these never became mainstream (assuming you mean Rosenfeld's whole sentence maxent language models).\n\no N is terrible name for a system!\n\no Not sure about the 5-role schema example on p.4, since presumably these would still be generated one word at a time? So in what sense is the model encoding a schema?\n\no Section 5 is the key section in the paper. Unfortunately I found it hard to follow. I guess the LSTM equations are needed for completeness, but what I really needed was a clear paragraph explaining how the LSTM is used to build the vectors and matrices used by the binding/unbinding network.\n\no There are various oddities in the POS tagging experiments. Why use the Stanford tagger? Are you using this to get the train/test data? Just use the Penn Treebank, or another standard pos tag dataset.\n\no Why precision and recall for tagging? Doesn't each word get assigned a single tag? In which case we just need accuracy.\n\no There are a number of minor comments I could have made re. the presentation, eg funny refs with both names (Chen and Laurence Zitnick). ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to 1) image captioning, 2)\nclassification of the part of speech of a word, and 3)\nidentification of the phrase structure of a sentence. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/f304dd25eac560332dace51049d091620a549ad2.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning","part of speech tagger","phrase detection"]}},{"tddate":null,"ddate":null,"tmdate":1509739175337,"tcdate":1509130876443,"number":660,"cdate":1509739172677,"id":"HJ4YGZ-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to 1) image captioning, 2)\nclassification of the part of speech of a word, and 3)\nidentification of the phrase structure of a sentence. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/f304dd25eac560332dace51049d091620a549ad2.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning","part of speech tagger","phrase detection"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}