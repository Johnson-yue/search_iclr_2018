{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222737160,"tcdate":1512005794309,"number":3,"cdate":1512005794309,"id":"r1qsxyTlG","invitation":"ICLR.cc/2018/Conference/-/Paper730/Official_Review","forum":"r1drp-WCZ","replyto":"r1drp-WCZ","signatures":["ICLR.cc/2018/Conference/Paper730/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A nice formulation of a stochastic LSTM","rating":"7: Good paper, accept","review":"This paper introduces a novel extension of the LSTM which incorporates stochastic inputs at each timestep. These stochastic inputs are themselves dependent on the LSTM state at the previous timestep. Considering the stochastic dependencies, this then yields a highly flexible non-Markov state space model, where the latent variable transitions are partially parameterized by an LSTM update.\n\nNaturally, the challenges are then efficiently estimating parameters and performing inference over the latent states. Here, SMC (and conditional SMC / particle Gibbs) are used for inference over the latent states z. A particularly nice touch is that even when the LSTM model is used for the transitions in the latent space, so long as the conditional distributions p(z_t | z_{1:t-1}) are conjugate with the emission distribution then it is possible to compute the optimal forward filtering proposal distribution in closed form, as done for the conditionally Gaussian (with affine Gaussian observations) and conditionally multinomial models considered here. Note that this really is a special feature of the models under consideration, though: for example, if the emission distribution p(x_t | z_t) is instead a *nonlinear* Gaussian, then one would have to fall back to bootstrap proposals. This probably deserves some mention: equations (13) are not, generally, tractable to integrate or normalize.\n\nI think this paper is missing a few necessary details on how the overall optimization algorithm proceeds, which I would like to see in an update. I understand that particle Gibbs updates (or SMC) are used to approximate the posterior distribution in a Monte Carlo EM algorithm. However, this does leave some questions:\n\n1. For the M step, how are the \\omega parameters (of the LSTM) handled in equation (8)? I understand that due to the particular models considered, maximum likelihood estimates of \\phi can be found in closed form. However, that’s not the case for \\omega. Is a gradient descent algorithm run to convergence? Or is a single gradient step taken, interleaved with a single PG update? Or something else?\n\n2. How reliably does the algorithm as a whole converge? Monte Carlo EM does not in general have convergence guarantees of “standard” EM (i.e. each step is not guaranteed to monotonically improve the lower bound). This might be fine! But, I think requires a bit of discussion.\n\n3. Is it necessary to include a replenishing operation (or independent MCMC steps) in the particle Gibbs algorithm? A known issue when running an iterated conditional SMC algorithm like this is that path degeneracy can make it very difficult for the PG kernel to mix well over the early time steps in the LSTM. Does this issue appear here? How many particles P are needed to efficiently mix, when considering time series of length T?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"State Space LSTM Models with inference using Sequential Monte Carlo","abstract":"Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL), which generalizes the earlier work \\cite{zaheer2017latent} of combining topic models with LSTM. However, unlike \\cite{zaheer2017latent}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.","pdf":"/pdf/cb77a6eb47921ab167b7ba08003788f2e23a59ce.pdf","TL;DR":"We present State Space LSTM models, a combination of state space models and LSTMs, and propose an inference algorithm based on sequential Monte Carlo. ","paperhash":"anonymous|state_space_lstm_models_with_inference_using_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018state,\n  title={State Space LSTM Models with inference using Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1drp-WCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper730/Authors"],"keywords":["recurrent neural networks","state space models","sequential Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1512222737201,"tcdate":1511770989916,"number":2,"cdate":1511770989916,"id":"SyI_srKgz","invitation":"ICLR.cc/2018/Conference/-/Paper730/Official_Review","forum":"r1drp-WCZ","replyto":"r1drp-WCZ","signatures":["ICLR.cc/2018/Conference/Paper730/AnonReviewer1"],"readers":["everyone"],"content":{"title":"PMCMC EM for LSTM-based SSM with unclear contributions","rating":"4: Ok but not good enough - rejection","review":"The authors propose state space models where the transition probabilities are defined using an LSTM. For inference the authors propose to make use of Monte Carlo expectation maximization.\n\nThe model proposed seems to be a special case of previously proposed models that are mentioned in the 2nd paragraph of the related works section, and e.g. the Maddison et al. (2017) paper. The inference method has also been studied previously (but not to my knowledge applied to SSLs/SRNNs), see the following review papers and references therein:\nSchön, Lindsten, Dahlin, W˚agberg, Naesseth, Svensson, Dai, \"Sequential Monte Carlo Methods for System Identification\", 2015\nKantas, Doucet,  Singh, Maciejowski, Chopin, \"On Particle Methods for Parameter Estimation in State-Space Models\", 2015\n\nGiven this it is unclear to me what the novel contributions are. Perhaps the authors can elaborate on this?\n\nMinor comments:\n- Note that generally a state space model only has the Markov assumption, there is no restrictions on the transition and observation models.\n- EKF also requires Gaussian noise\n- It is a bit unclear what is meant by \"forward messages\" e.g. below eq. (6). For this model I believe the exact would generally be unavailable (at least for continuous models) because they would depend on previous messages.\n- Eq. (12) and (14) are exactly the same? The text seems to indicate they are not.\n- The optimal proposal is only locally optimal, minimizing the incremental weight variance\n- \"w\" should be \"x\" in eq. (20)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"State Space LSTM Models with inference using Sequential Monte Carlo","abstract":"Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL), which generalizes the earlier work \\cite{zaheer2017latent} of combining topic models with LSTM. However, unlike \\cite{zaheer2017latent}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.","pdf":"/pdf/cb77a6eb47921ab167b7ba08003788f2e23a59ce.pdf","TL;DR":"We present State Space LSTM models, a combination of state space models and LSTMs, and propose an inference algorithm based on sequential Monte Carlo. ","paperhash":"anonymous|state_space_lstm_models_with_inference_using_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018state,\n  title={State Space LSTM Models with inference using Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1drp-WCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper730/Authors"],"keywords":["recurrent neural networks","state space models","sequential Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1512222737243,"tcdate":1511731261948,"number":1,"cdate":1511731261948,"id":"SJLHg2OxG","invitation":"ICLR.cc/2018/Conference/-/Paper730/Official_Review","forum":"r1drp-WCZ","replyto":"r1drp-WCZ","signatures":["ICLR.cc/2018/Conference/Paper730/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Poor model manipulation and missing many important references.","rating":"3: Clear rejection","review":"This article presents an approach for learning and inference in nonlinear state-space models (SSM) based on LSTMs. Learning is done using a stochastic EM where Particle PMCM is used to sample state trajectories.\n\nThe model is presented assuming that SSMs are linear. This is not necessarily the case since nonlinear SSMs have been used for a long time (see for example Ljung, 1999, \"System Identification, Theory for the User\"). The presented model is a nonlinear SSM with a particular structure that uses LSTMs.\n\nThe model described in the paper is Markovian: if one defines the variable sz_t = {s_t, z_t} there exists a Markov chain for the latent state sz:\n\nsz_t -> sz_{t+1} -> sz_{t+2} -> ...\n\nMarginalizing the latent variables s_t leads to a structure that, in general, is not Markovian. The authors claim that this marginalization \"allows the SSL to have non-Markovian state transition\". The word \"allows\" may mislead the reader in thinking that the model has gained some appealing property whereas the model is still essentially Markovian as evidenced by the Markov chain in sz. Any general algorithm for inference in nonlinear Markovian models could be used for inference of sz.\n\nThe algorithm used for inference and learning is stochastic EM with PMCMC but the authors do not cite important prior work such as: Lindsten (2013) \"An efficient stochastic approximation EM algorithm using conditional particle filters\"\n\n\nPros:\n\nThe model is sound.\n\nThe overall structure of the paper is good.\n\n\nCons:\n\nThe authors formulate the problem in such a way that they are forced to use an algorithm for non-Markovian models when they could have conserved the Markovian structure by choosing the appropriate parameterization.\n\nThe presentation of state-space models, filtering and smoothing shows some lack of familiarity with the literature. The control theory literature has dealt with nonlinear SSMs for decades and there is recent work in the machine learning community on nonlinear SSMs, e.g. Gaussian Process SSMs. \n\nI would advise against the use of non-English expressions unless they are used precisely:\n\n   - sine qua non: LSTMs are not literally an indispensable model for sequence modeling nowadays. If the use of Latin was unavoidable, \"de facto standard\" would have been slightly more accurate.\n\n   - bona fide: I am not sure what the authors wanted to say.\n\n   - naívely: the correct spelling would be naïvely or naively.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"State Space LSTM Models with inference using Sequential Monte Carlo","abstract":"Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL), which generalizes the earlier work \\cite{zaheer2017latent} of combining topic models with LSTM. However, unlike \\cite{zaheer2017latent}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.","pdf":"/pdf/cb77a6eb47921ab167b7ba08003788f2e23a59ce.pdf","TL;DR":"We present State Space LSTM models, a combination of state space models and LSTMs, and propose an inference algorithm based on sequential Monte Carlo. ","paperhash":"anonymous|state_space_lstm_models_with_inference_using_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018state,\n  title={State Space LSTM Models with inference using Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1drp-WCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper730/Authors"],"keywords":["recurrent neural networks","state space models","sequential Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1509739136178,"tcdate":1509133632461,"number":730,"cdate":1509739133502,"id":"r1drp-WCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1drp-WCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"State Space LSTM Models with inference using Sequential Monte Carlo","abstract":"Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL), which generalizes the earlier work \\cite{zaheer2017latent} of combining topic models with LSTM. However, unlike \\cite{zaheer2017latent}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.","pdf":"/pdf/cb77a6eb47921ab167b7ba08003788f2e23a59ce.pdf","TL;DR":"We present State Space LSTM models, a combination of state space models and LSTMs, and propose an inference algorithm based on sequential Monte Carlo. ","paperhash":"anonymous|state_space_lstm_models_with_inference_using_sequential_monte_carlo","_bibtex":"@article{\n  anonymous2018state,\n  title={State Space LSTM Models with inference using Sequential Monte Carlo},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1drp-WCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper730/Authors"],"keywords":["recurrent neural networks","state space models","sequential Monte Carlo"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}