{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222681458,"tcdate":1512015245957,"number":2,"cdate":1512015245957,"id":"BkLqBW6lG","invitation":"ICLR.cc/2018/Conference/-/Paper536/Official_Review","forum":"HJZiRkZC-","replyto":"HJZiRkZC-","signatures":["ICLR.cc/2018/Conference/Paper536/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Related work and other tasks for experiments.","rating":"5: Marginally below acceptance threshold","review":"The authors propose autoencoding text using a byte-level encoding and a convolutional network with shared filters such that the encoder and decoder should exhibit recursive structure. They show that the model can handle various languages and run various experiments testing the ability of the autoencoder to reconstruct the text with varying lengths, perturbations, depths, etc.\n\nThe writing is fairly clear, though many of the charts and tables are hard to decipher without labels (and in Figure 8, training errors are not visible -- maybe they overlap completely?).\n\nMain concern would be the lack of experiments showing that the network learns meaningful representations in the hidden layer. E.g. through semi-supervised learning experiments or experiments on learning semantic relatedness of sentences. Obvious citations such as https://arxiv.org/pdf/1511.06349.pdf and https://arxiv.org/pdf/1503.00075.pdf are missing, along with associated baselines. Although the experiment with randomly permuting the samples is nice, would hesitate to draw any conclusions without results on downstream tasks and a clearer survey of the literature.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Byte-Level Recursive Convolutional Auto-Encoder for Text","abstract":"This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.","pdf":"/pdf/e9db386e7396baa7ce2a4ea228c62c29fff04997.pdf","paperhash":"anonymous|bytelevel_recursive_convolutional_autoencoder_for_text","_bibtex":"@article{\n  anonymous2018byte-level,\n  title={Byte-Level Recursive Convolutional Auto-Encoder for Text},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJZiRkZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper536/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222681497,"tcdate":1511848575572,"number":1,"cdate":1511848575572,"id":"BJuY5u9gf","invitation":"ICLR.cc/2018/Conference/-/Paper536/Official_Review","forum":"HJZiRkZC-","replyto":"HJZiRkZC-","signatures":["ICLR.cc/2018/Conference/Paper536/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Minor step forward, but good investigation & analysis","rating":"7: Good paper, accept","review":"\nThis paper presents a convolutional auto-encoder architecture for text encoding and generation. It works on the character level and contains a recursive structure which scales with the length of the input text. Building on the recent state-of-the-art in terms of architectural components, the paper shows the feasibility of this architecture and compares it to LSTM, showing the cnn superiority for auto-encoding.\n\nThe authors have decided to encode the text into a length of 1024 - Why? Would different lengths result in a better performance?\n\nYou write \"Minimal pre-processing is applied to them since our model can be applied to all languages in the same fashion.\" Please be more specific. Which pre-processing do you apply for each dataset?\n\nI wonder if the comparison to a simple LSTM network is fair. It would be better to use a 2- or 3-layer network. Also, BLSTM are used nowadays.\n\nA strong part of this paper is the large amount of investigation and extra experiments.\nMinor issues:\nPlease correct minor linguistic mistakes as well as spelling mistakes. In Fig. 3, for example, the t of Different is missing.\n\nAn issue making it hard to read the paper is that most of the figures appear on another page than where they are mentioned in the text.\n\nthe authors have chosen to cite a work from 1994 for the vanishing gradient problem. Note, that many (also earlier) works have reported this problem in different ways. A good analysis of all researches is performed in Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. (2001) \"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\".","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Byte-Level Recursive Convolutional Auto-Encoder for Text","abstract":"This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.","pdf":"/pdf/e9db386e7396baa7ce2a4ea228c62c29fff04997.pdf","paperhash":"anonymous|bytelevel_recursive_convolutional_autoencoder_for_text","_bibtex":"@article{\n  anonymous2018byte-level,\n  title={Byte-Level Recursive Convolutional Auto-Encoder for Text},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJZiRkZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper536/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739249016,"tcdate":1509125785308,"number":536,"cdate":1509739246355,"id":"HJZiRkZC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJZiRkZC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Byte-Level Recursive Convolutional Auto-Encoder for Text","abstract":"This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.","pdf":"/pdf/e9db386e7396baa7ce2a4ea228c62c29fff04997.pdf","paperhash":"anonymous|bytelevel_recursive_convolutional_autoencoder_for_text","_bibtex":"@article{\n  anonymous2018byte-level,\n  title={Byte-Level Recursive Convolutional Auto-Encoder for Text},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJZiRkZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper536/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}