{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222694817,"tcdate":1511820336600,"number":3,"cdate":1511820336600,"id":"S1tVnWqxM","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Intuitive solution to important problem; well written ","rating":"7: Good paper, accept","review":"The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly. The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector. Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique. However, this does not invalidate the contributions of this manuscript.\n\nSpecific comments/questions:\n- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting.\n- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described.\n- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique? \n- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate? Otherwise, r_1(.) for each point is 0, leading to a somewhat \"under-estimate\" of the true LID of the normal points in the training set. I understand that it is not an issue in the test set.\n- Section 4 and Alg 1: S we do not really care about the \"labels/targets\" of the examples. All examples in the dataset are considered \"normal\" to start with. Is this assuming that the \"initial training set\" which is used to obtain the \"pre-trained DNN\" free of adversarial examples?\n- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the \"leave-one-out\" version of the estimation.\n- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, the properties of subspaces in the neighborhood of adversarial examples need to be characterized. In particular, effective measures are required to discriminate adversarial examples from normal examples in such subspaces. We tackle this challenge by characterizing the intrinsic dimensional property of adversarial subspaces, via the use of Local Intrinsic Dimensionality. LID assesses the space-filling capability of the subspace surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation affects the LID characteristic of adversarial subspaces. Then, we explain how the LID characteristic can be used to discriminate adversarial examples generated using the state-of-the-art attacks. We empirically show that the LID characteristic can outperform several state-of-the-art detection measures by large margins for five attacks across three benchmark datasets. Our analysis of the LID characteristic for adversarial subspaces not only motivates new directions of effective adversarial defense but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/285561c752b76ac58d36c71711fd61efea15d99d.pdf","TL;DR":"We characterize the intrinsic dimensional property of adversarial subspaces in the neighborhood of adversarial examples, via the use of Local Intrinsic Dimensionality (LID) and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Detection","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222694857,"tcdate":1511769903415,"number":2,"cdate":1511769903415,"id":"H1wVDrtgM","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not familiar with this research topic.","rating":"6: Marginally above acceptance threshold","review":"This paper tried to analyze the subspaces of the adversarial examples neighborhood. More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces. The characteristics and theoretical analysis of the proposed method are discussed and explained. This paper helps others to better understand the vulnerabilities of DNNs.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, the properties of subspaces in the neighborhood of adversarial examples need to be characterized. In particular, effective measures are required to discriminate adversarial examples from normal examples in such subspaces. We tackle this challenge by characterizing the intrinsic dimensional property of adversarial subspaces, via the use of Local Intrinsic Dimensionality. LID assesses the space-filling capability of the subspace surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation affects the LID characteristic of adversarial subspaces. Then, we explain how the LID characteristic can be used to discriminate adversarial examples generated using the state-of-the-art attacks. We empirically show that the LID characteristic can outperform several state-of-the-art detection measures by large margins for five attacks across three benchmark datasets. Our analysis of the LID characteristic for adversarial subspaces not only motivates new directions of effective adversarial defense but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/285561c752b76ac58d36c71711fd61efea15d99d.pdf","TL;DR":"We characterize the intrinsic dimensional property of adversarial subspaces in the neighborhood of adversarial examples, via the use of Local Intrinsic Dimensionality (LID) and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Detection","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222694896,"tcdate":1511613398400,"number":1,"cdate":1511613398400,"id":"rkARQJwez","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Intrinsic dimensionality around the adversarial example is very different from the one of normal or noisy data","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper considers a problem of adversarial examples applied to the deep neural networks. The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples. More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4).  Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time. In other words, the paper proposes a particular approach for the adversarial defence.\n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) . Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours. The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples. For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN. These values are finally used as features in classifying adversarial examples from normal and noisy ones. \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature. The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1).\n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design. Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time. In other words, the proposed technique is rather stable and can not be easily exploited.\n\nI really enjoyed reading this paper. All the statements are very clear, the structure is transparent and easy to follow. The writing is excellent. I found only one typo (page 8, \"We also NOTE that...\"), otherwise I don't actually have any comments on the text.\n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method. However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, the properties of subspaces in the neighborhood of adversarial examples need to be characterized. In particular, effective measures are required to discriminate adversarial examples from normal examples in such subspaces. We tackle this challenge by characterizing the intrinsic dimensional property of adversarial subspaces, via the use of Local Intrinsic Dimensionality. LID assesses the space-filling capability of the subspace surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation affects the LID characteristic of adversarial subspaces. Then, we explain how the LID characteristic can be used to discriminate adversarial examples generated using the state-of-the-art attacks. We empirically show that the LID characteristic can outperform several state-of-the-art detection measures by large margins for five attacks across three benchmark datasets. Our analysis of the LID characteristic for adversarial subspaces not only motivates new directions of effective adversarial defense but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/285561c752b76ac58d36c71711fd61efea15d99d.pdf","TL;DR":"We characterize the intrinsic dimensional property of adversarial subspaces in the neighborhood of adversarial examples, via the use of Local Intrinsic Dimensionality (LID) and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Detection","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739508766,"tcdate":1508822744305,"number":59,"cdate":1509739506101,"id":"B1gJ1L2aW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, the properties of subspaces in the neighborhood of adversarial examples need to be characterized. In particular, effective measures are required to discriminate adversarial examples from normal examples in such subspaces. We tackle this challenge by characterizing the intrinsic dimensional property of adversarial subspaces, via the use of Local Intrinsic Dimensionality. LID assesses the space-filling capability of the subspace surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation affects the LID characteristic of adversarial subspaces. Then, we explain how the LID characteristic can be used to discriminate adversarial examples generated using the state-of-the-art attacks. We empirically show that the LID characteristic can outperform several state-of-the-art detection measures by large margins for five attacks across three benchmark datasets. Our analysis of the LID characteristic for adversarial subspaces not only motivates new directions of effective adversarial defense but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/285561c752b76ac58d36c71711fd61efea15d99d.pdf","TL;DR":"We characterize the intrinsic dimensional property of adversarial subspaces in the neighborhood of adversarial examples, via the use of Local Intrinsic Dimensionality (LID) and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Detection","Adversarial Defense","Deep Neural Networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}