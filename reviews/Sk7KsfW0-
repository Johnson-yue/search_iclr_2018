{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222824991,"tcdate":1512020932461,"number":3,"cdate":1512020932461,"id":"HJnaoMagf","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Method for lifelong learning with neural networks","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors propose a method (Dynamically Expandable Network) that addresses issues of training efficiency, how to dynamically grow the network, and how to prevent catastrophic forgetting.\n\nThe paper is well written with a clear problem statement and description of the method for preventing each of the described issues. Interesting points include the use of an L1 regularization term to enforce sparsity in the weights, as well as the method for identifying which neurons have “drifted” too far and should be split. The use of timestamps is a clever addition as well.\n\nOne question would be how sparse training is done, and how this saves computation, especially with the breadth-first search described on page 5. A critique would be that the base networks (a two layer FF net and LeNet) are not very compelling.\n\nExperiments indicate that the method works well, with a clear improvement over progressive networks. Thus, though there isn’t one particular facet of the paper that leaps out, overall the method and results seem solid and worthy of publication.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/63d0ae2b3f179b39aa548d821c9922de16a3b081.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1512222826329,"tcdate":1511826273945,"number":2,"cdate":1511826273945,"id":"SkcwXXqxM","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Investigation of DENs and variants applied to lifelong learning","rating":"6: Marginally above acceptance threshold","review":"The topic is of great interest to the community, and the ideas explored by the authors are reasonable, but I found the conclusion less-than-clear. Mainly, I was not sure how to interpret the experimental findings, and did not have a clear picture of the various models being investigated (e.g. \"base DNN regularized with l2\"), or even of the criteria being examined. What is \"learning capacity\"? (If it's number of model parameters, the authors should just say, \"number of parameters\"). The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was.\n\nWhat does the \"batch model\" refer to?\n\nre. \" 11.9%p − 51.8%p\"; remove \"p\"?\n\nReference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?\n\nre. \"... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL\": isn't the number of parameters matched? If so, why is the \"learning capacity\" different? What do the authors mean exactly by \"learning capacity\"?\n\nre. Figure 3, e.g. \"Average per-task performance of the models over number of task t\": this is a general point, but usually the expression \"<f(x)> vs. <x>\" is used rather than \"<f(x)> over <x>\" when describing a plot.\n\n\"DNN: dase (sic) DNN\": how is this trained?\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/63d0ae2b3f179b39aa548d821c9922de16a3b081.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1512222826381,"tcdate":1511812629737,"number":1,"cdate":1511812629737,"id":"SJAGR15lz","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach to continual learning","rating":"7: Good paper, accept","review":"The paper was clearly written and pleasant to read. I liked the use of sparsity- and group-sparsity-promoting regularizers to select connections and decide how to expand the network.\n\nA strength of the paper is that the proposed algorithm is interesting and intuitive, even if relatively complex, as it requires chaining a sequence of sub-algorithms. It was good to see the impact of each sub-algorithm studied separately (to some degree) in the experimental section. The results are overall strong.\n\nIt’s hard for me to judge the novelty of the approach though, as I’m not an expert on this topic.\n\nJust a few points below:\n- The experiments focus on a relevant continual learning problem, where each new task corresponds to learning a new class. In this setup, the method consistently outperforms EWC (e.g., Fig. 3), as well as the progressive network baseline.\nDid the authors also check the performance on the permuted MNIST benchmark, as studied by Kirkpatrick et al. and Zenke et al.? It would be important to see how the method fares in this setting, where the tasks are the same, but the inputs have to be remapped, and network expansion is less of an issue.\n\n- Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS, the full L1-penalized problem was solved, while keeping the rest of the pipeline intact.\n\n- Still regarding the proposed selective retraining, the special role played by the last hidden layer seems slightly arbitrary. It may well be that it has the highest task-specificity, though this is not trivial to me. This special role might become problematic when dealing with deeper networks.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/63d0ae2b3f179b39aa548d821c9922de16a3b081.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1510092383938,"tcdate":1509137280893,"number":944,"cdate":1510092362115,"id":"Sk7KsfW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/63d0ae2b3f179b39aa548d821c9922de16a3b081.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}