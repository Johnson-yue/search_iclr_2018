{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222796205,"tcdate":1511775351414,"number":3,"cdate":1511775351414,"id":"ByyFhLYgM","invitation":"ICLR.cc/2018/Conference/-/Paper85/Official_Review","forum":"B1twdMCab","replyto":"B1twdMCab","signatures":["ICLR.cc/2018/Conference/Paper85/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper proposes a new method to generate vector representations of text and background knowledge. The propsed method is evaluated on QA and textual entailment (TE) tasks. ","rating":"6: Marginally above acceptance threshold","review":"The quality of this paper is good. The presentation is clear but I find lack of description of a key topic. The proposed model is not very innovative but works fine for the DQA task. For the TE task, the proposed method does not perform better than the state-of-the-art systems. \n\n- As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM.\n- The reference of ESIM is not correct.\n- Figure 1 is hard to understand. What do you indicate with the box and arrow? Arrows seem to have some different meanings. \n- What corpus did you use to pre-train word vectors? \n- As the proposed method was successful for the QA task, you need to explain QA data sets and how the questions are solved.\n- I also expect performance and  error analysis of the task results.  \n- To claim \"task-agnostic\", you need to try to apply your method to other NLP tasks as well.\n- Page 3. \\Sigma is not defined.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Integration of Background Knowledge in Neural NLU Systems","abstract":"Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.","pdf":"/pdf/8646625d4c11be5b5c030b95799e20c35053009a.pdf","TL;DR":"In this paper we present a task-agnostic reading architecture for the dynamic integration of explicit background knowledge in neural NLU models. ","paperhash":"anonymous|dynamic_integration_of_background_knowledge_in_neural_nlu_systems","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Integration of Background Knowledge in Neural NLU Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1twdMCab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper85/Authors"],"keywords":["natural language processing","background knowledge","word embeddings","question answering","natural language inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222796247,"tcdate":1511734768006,"number":2,"cdate":1511734768006,"id":"BJdxRnOlz","invitation":"ICLR.cc/2018/Conference/-/Paper85/Official_Review","forum":"B1twdMCab","replyto":"B1twdMCab","signatures":["ICLR.cc/2018/Conference/Paper85/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Adding knowledge is good idea; this may be too simple a way to do it?","rating":"5: Marginally below acceptance threshold","review":"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format. However, in this paper, the way it is added is simply by updating word representations based on this extra text. This seems too simple to really be the right way to add background knowledge. \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing; the paper never says exactly how, not even if you read the supplementary material). This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning).\n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \"reading\" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%). A note on the QA results: The QA results are certainly good enough to be in the range of \"good systems\", but none of the results really push the SOTA. The best SQuAD (devset) results are shown as several percent below the SOTA. In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission, but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD. Similar remarks perhaps apply to the NLI results.\n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.\n\nBiggest question:\n - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?\n\nMinor notes:\n - The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on.\n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?\n - On p.3 above sec 3.1: What is u? Was that meant to be z?\n - On p.8, I'm a bit suspicious of the \"Is additional knowledge used?\" experiment which trains with knowledge and then tests without knowledge. It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone.\n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs. This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Integration of Background Knowledge in Neural NLU Systems","abstract":"Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.","pdf":"/pdf/8646625d4c11be5b5c030b95799e20c35053009a.pdf","TL;DR":"In this paper we present a task-agnostic reading architecture for the dynamic integration of explicit background knowledge in neural NLU models. ","paperhash":"anonymous|dynamic_integration_of_background_knowledge_in_neural_nlu_systems","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Integration of Background Knowledge in Neural NLU Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1twdMCab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper85/Authors"],"keywords":["natural language processing","background knowledge","word embeddings","question answering","natural language inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222796289,"tcdate":1511723487876,"number":1,"cdate":1511723487876,"id":"SkdyfcOlf","invitation":"ICLR.cc/2018/Conference/-/Paper85/Official_Review","forum":"B1twdMCab","replyto":"B1twdMCab","signatures":["ICLR.cc/2018/Conference/Paper85/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A model that incorporates background knowledge to improve on multiple NLU tasks","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a model for adding background knowledge to natural language understanding tasks. The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction. The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion).\n\nI think the paper does a fairly good job at doing what it does, it is just hard to get excited by it. \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic. But then they just concept net to augment text. This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation. As is, I don't really see how this motivation has anything to do with getting things out of a KB. A KB is usually a pretty static entity, and things are added to it at a slow pace.\n\n* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI. Specifically they take text and add common sense knowledge from concept net. The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis. However, is this statement enough to cross the acceptance threshold of ICLR? Seems a bit marginal to me.\n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them. To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way. For example, have another RNN read the assertions and somehow integrate that. The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated. There are no comparisons to other possibilities. As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful. As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution.\n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis. The conclusion is that reading background knowledge from concept net boost performance using some architecture. This is nice to know but I think does not cross the acceptance threshold.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Integration of Background Knowledge in Neural NLU Systems","abstract":"Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.","pdf":"/pdf/8646625d4c11be5b5c030b95799e20c35053009a.pdf","TL;DR":"In this paper we present a task-agnostic reading architecture for the dynamic integration of explicit background knowledge in neural NLU models. ","paperhash":"anonymous|dynamic_integration_of_background_knowledge_in_neural_nlu_systems","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Integration of Background Knowledge in Neural NLU Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1twdMCab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper85/Authors"],"keywords":["natural language processing","background knowledge","word embeddings","question answering","natural language inference"]}},{"tddate":null,"ddate":null,"tmdate":1509739495637,"tcdate":1508939873288,"number":85,"cdate":1509739492980,"id":"B1twdMCab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1twdMCab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Dynamic Integration of Background Knowledge in Neural NLU Systems","abstract":"Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.","pdf":"/pdf/8646625d4c11be5b5c030b95799e20c35053009a.pdf","TL;DR":"In this paper we present a task-agnostic reading architecture for the dynamic integration of explicit background knowledge in neural NLU models. ","paperhash":"anonymous|dynamic_integration_of_background_knowledge_in_neural_nlu_systems","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Integration of Background Knowledge in Neural NLU Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1twdMCab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper85/Authors"],"keywords":["natural language processing","background knowledge","word embeddings","question answering","natural language inference"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}