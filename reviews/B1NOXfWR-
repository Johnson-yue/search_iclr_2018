{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222773901,"tcdate":1511904512232,"number":2,"cdate":1511904512232,"id":"B1d-B8jgz","invitation":"ICLR.cc/2018/Conference/-/Paper808/Official_Review","forum":"B1NOXfWR-","replyto":"B1NOXfWR-","signatures":["ICLR.cc/2018/Conference/Paper808/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice idea","rating":"6: Marginally above acceptance threshold","review":"\nSummary: the paper proposes an idea for multi-task learning where tasks have shared dependencies between subtasks as task graph. The proposed framework, task graph solver (NTS), consists of many approximation steps and representations: CNN to capture environment states, task graph parameterization, logical operator approximation; the idea of reward-propagation policy helps pre-training. The framework is evaluated on a relevant multi-task problem.\n\nIn general, the paper proposes an idea to tackle an interesting problem. It is well written, the idea is well articulated and presented. The idea to represent task graphs are quite interesting. However it looks like the task graph itself is still simple and has limited representation power. Specifically, it poses just little constraints and presents no stochasticity (options result in stochastic outcomes).\n\nThe method is evaluated in one experiment with many different settings. The task itself is not too complex which involves 10 objects, and a small set of deterministic options. It might be only complex when the number of dependency layer is large. However, it's still more convinced if the paper method is demonstrated in more domains.\n\n\nAbout the description of problem statement in Section 3:\n\n- How the MDP M and options are defined, e.g. transition functions, are tochastic?\n\n- What is the objective of the problem in section 3\n\nRelated work: many related work in robotics community on the topic of task and motion planning (checkout papers in RSS, ICRA, IJRR, etc.) should also be discussed.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Task Graph Execution","abstract":"In order to develop a scalable multi-task reinforcement learning (RL) agent that is able to execute many complex tasks, this paper introduces a new RL problem where the agent is required to execute a given task graph which describes a set of subtasks and dependencies among them. Unlike existing approaches which explicitly describe what the agent should do, our problem only describes properties of subtasks and relationships between them, which requires the agent to perform a complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural task graph solver (NTS) which encodes the task graph using a recursive neural network. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy that performs back-propagation over a differentiable form of the task graph to compute the influence of each subtask on the other subtasks. Our NTS is pre-trained to approximate the proposed gradient-based policy and fine-tuned through actor-critic method. The experimental results on a 2D visual domain show that our method to pre-train from the gradient-based policy significantly improves the performance of NTS. We also demonstrate that our agent can perform a complex reasoning to find the optimal way of executing the task graph and generalize well to unseen task graphs.","pdf":"/pdf/74b12a6e3a2f92cb20c54fb5fc6eefdb83880e8a.pdf","paperhash":"anonymous|neural_task_graph_execution","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Task Graph Execution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NOXfWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper808/Authors"],"keywords":["deep reinforcement learning","task execution","instruction execution"]}},{"tddate":null,"ddate":null,"tmdate":1512222773953,"tcdate":1511813139521,"number":1,"cdate":1511813139521,"id":"Skjfeg5gG","invitation":"ICLR.cc/2018/Conference/-/Paper808/Official_Review","forum":"B1NOXfWR-","replyto":"B1NOXfWR-","signatures":["ICLR.cc/2018/Conference/Paper808/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neural Task Graph Execution","rating":"4: Ok but not good enough - rejection","review":"In the context of multitask reinforcement learning, this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them, in the form of a task graph. The paper presents a neural task graph solver (NTS), which encodes this as a recursive-reverse-recursive neural network. A method for learning this is presented, and fine tuned with an actor-critic method. The approach is evaluated in a multitask grid world domain.\n\nThis paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks. The method is novel, and the paper is generally well written. I unfortunately have several issues with the paper in its current form, most importantly around the experimental comparisons.\n\nThe paper is severely weakened by not comparing experimentally to other learning (hierarchical) schemes, such as options or HAMs. None of the comparisons in the paper feature any learning. Ideally, one should see the effect of learning with options (and not primitive actions) to fairly compare against the proposed framework. At some level, I question whether the proposed framework is doing any more than just value function propagation at a task level, and these experiments would help resolve this.\n\nAdditionally, the example domain makes no sense. Rather use something more standard, with well-known baselines, such as the taxi domain.\n\nI would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature, notably HTNs.\n\nI found the description of the training of the method to be rather superficial, and I don't think it could be replicated from the paper in its current level of detail.\n\nThe approach raises the natural questions of where the tasks and the task graphs come from. Some acknowledgement and discussion of this would be useful.\n\nThe legend in the middle of Fig 4 obscures the plot (admittedly not substantially).\n\nThere are also a number of grammatical errors in the paper, including the following non-exhaustive list:\n2: as well as how to do -> as well as how to do it\nFig 2 caption: through bottom-up -> through a bottom-up\n3: Let S be a set of state -> Let S be a set of states\n3: form of task graph -> form of a task graph\n3: In addtion -> In addition\n4: which is propagates -> which propagates\n5: investigated following -> investigated the following","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Task Graph Execution","abstract":"In order to develop a scalable multi-task reinforcement learning (RL) agent that is able to execute many complex tasks, this paper introduces a new RL problem where the agent is required to execute a given task graph which describes a set of subtasks and dependencies among them. Unlike existing approaches which explicitly describe what the agent should do, our problem only describes properties of subtasks and relationships between them, which requires the agent to perform a complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural task graph solver (NTS) which encodes the task graph using a recursive neural network. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy that performs back-propagation over a differentiable form of the task graph to compute the influence of each subtask on the other subtasks. Our NTS is pre-trained to approximate the proposed gradient-based policy and fine-tuned through actor-critic method. The experimental results on a 2D visual domain show that our method to pre-train from the gradient-based policy significantly improves the performance of NTS. We also demonstrate that our agent can perform a complex reasoning to find the optimal way of executing the task graph and generalize well to unseen task graphs.","pdf":"/pdf/74b12a6e3a2f92cb20c54fb5fc6eefdb83880e8a.pdf","paperhash":"anonymous|neural_task_graph_execution","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Task Graph Execution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NOXfWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper808/Authors"],"keywords":["deep reinforcement learning","task execution","instruction execution"]}},{"tddate":null,"ddate":null,"tmdate":1512246021834,"tcdate":1509135211549,"number":808,"cdate":1509739087436,"id":"B1NOXfWR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1NOXfWR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Task Graph Execution","abstract":"In order to develop a scalable multi-task reinforcement learning (RL) agent that is able to execute many complex tasks, this paper introduces a new RL problem where the agent is required to execute a given task graph which describes a set of subtasks and dependencies among them. Unlike existing approaches which explicitly describe what the agent should do, our problem only describes properties of subtasks and relationships between them, which requires the agent to perform a complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural task graph solver (NTS) which encodes the task graph using a recursive neural network. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy that performs back-propagation over a differentiable form of the task graph to compute the influence of each subtask on the other subtasks. Our NTS is pre-trained to approximate the proposed gradient-based policy and fine-tuned through actor-critic method. The experimental results on a 2D visual domain show that our method to pre-train from the gradient-based policy significantly improves the performance of NTS. We also demonstrate that our agent can perform a complex reasoning to find the optimal way of executing the task graph and generalize well to unseen task graphs.","pdf":"/pdf/74b12a6e3a2f92cb20c54fb5fc6eefdb83880e8a.pdf","paperhash":"anonymous|neural_task_graph_execution","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Task Graph Execution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1NOXfWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper808/Authors"],"keywords":["deep reinforcement learning","task execution","instruction execution"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}