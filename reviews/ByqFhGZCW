{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222828756,"tcdate":1511879147689,"number":3,"cdate":1511879147689,"id":"B1VlMxjlG","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Well-written, but experiments could be more thorough. ","rating":"5: Marginally below acceptance threshold","review":"The authors describe a mechanism for defending against adversarial learning attacks on classifiers. They first consider the dynamics generated by the following procedure. They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating.  \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent. They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach. \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples). In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps. They conclude with empirical observations about the performance of this algorithm.\n\nThe paper is well-written and easy to follow. However, I found the empirical results to be a little underwhelming. Sens-FGSM outperforms the adversarial training defenses tuned for the “wrong” iteration, but it does not appear to perform particularly well with error rates well above 20%. How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)? Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM? It is my understanding that Sens-FGSM is not trained to a particular iteration of the “cat-and-mouse” game. Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81? With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.), thus I would liked to see more thorough experiments here as well. For example, it’s unlikely that the defender would ever know the attack network utilized by an attacker. How robust is the defense against samples generated by a different attack network? The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying. Any defense can be *evaluated* against samples generated by any attacker strategy. Is it the case that the defenses fall flat against samples generated by different architectures? \n\n\nMinor Comments:\nSection 3.1, First Line. ”f(ul(g(x),y))” appears to be a mistake.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers\ncan be fooled easily by small input perturbations unnoticeable to human eyes.\nSubsequent research has proposed several methods of crafting adversarial exam-\nples, as well as methods of robustifying the classifier against such examples. An\nattacker with the knowledge of the classifier parameters can generate strong ad-\nversarial patterns. In response, a classifier with the knowledge of such patterns\ncan be trained to be robust to them. The cat-and-mouse game nature of the attacks\nand the defenses raises the question of the presence of an equilibrium in the dy-\nnamics. In this paper, we propose a game framework to formulate the interaction\nof attacks and defenses and present the natural notion of the best worst-case de-\nfense and attack. We propose simple algorithms to find those solutions motivated\nby sensitivity penalization. In addition, we show the potentials of learning-based\nattacks, and present the close relationship between the adversarial attack and the\nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-\n10 datasets.","pdf":"/pdf/be1767c6461d00041c1083a39b9e3d4095956260.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_defending_classifiers_against_learningbased_adversarial_attacks","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222828797,"tcdate":1511756899094,"number":2,"cdate":1511756899094,"id":"B1oPNGYez","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting extension and empirical study of GANs (Goodfellow et al. 2014)","rating":"6: Marginally above acceptance threshold","review":"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. It has a lemma which claims that the \"minimax and the maximin solutions provide the best worst-case defense and attack models, respectively\", without proof, although that statement is supported experimentally.\n\n+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain.\n- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5.\n- There is no exact (\"alternating optimization\" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm.\n- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis.\n- It is unclear (from Figures 3 and 7) that \"alternative optimization\" and \"minimax\" converged fully, and/or that the sets of hyperparameters were optimal.\n+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims.\n\n\nNitpicks:\nthe gradient descent -> gradient descent or the gradient descent algorithm\nseeming -> seemingly\narbitrary flexible -> arbitrarily flexible\ncan name \"gradient descent that maximizes\": gradient ascent.\nThe mini- max or the maximin solution is defined -> are defined\nis the follow -> is the follower\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers\ncan be fooled easily by small input perturbations unnoticeable to human eyes.\nSubsequent research has proposed several methods of crafting adversarial exam-\nples, as well as methods of robustifying the classifier against such examples. An\nattacker with the knowledge of the classifier parameters can generate strong ad-\nversarial patterns. In response, a classifier with the knowledge of such patterns\ncan be trained to be robust to them. The cat-and-mouse game nature of the attacks\nand the defenses raises the question of the presence of an equilibrium in the dy-\nnamics. In this paper, we propose a game framework to formulate the interaction\nof attacks and defenses and present the natural notion of the best worst-case de-\nfense and attack. We propose simple algorithms to find those solutions motivated\nby sensitivity penalization. In addition, we show the potentials of learning-based\nattacks, and present the close relationship between the adversarial attack and the\nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-\n10 datasets.","pdf":"/pdf/be1767c6461d00041c1083a39b9e3d4095956260.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_defending_classifiers_against_learningbased_adversarial_attacks","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222828835,"tcdate":1511692022773,"number":1,"cdate":1511692022773,"id":"B1kbvzdxz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer2"],"readers":["everyone"],"content":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","rating":"5: Marginally below acceptance threshold","review":"The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study. \n\nLemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria. Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable.  In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1.　Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims.\n\nAs long as the analysis is experimental, the state of the art should be considered. As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different. In some cases, MNIST is too easy to consider the complex structure of deep architectures. I prefer to have discussions on experimental results with both datasets.\n\nThe main takeaway from the entire paper is not clear very much. It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results.\n\nMinor:\nDefinition of g in the beginning of Sec 3.1 seems to be a typo. What is u? This is revealed in the latter sections but should be specified here.\n\nIn Section 3.1, \n>This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1.\nThe results shown in the table seems to indicate the “perfect” misclassification.\n\nSentence after eq. 15 seems to contain a grammatical error\n\nThe paragraph after eq. 17 is duplicated with a paragraph introduced before\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers\ncan be fooled easily by small input perturbations unnoticeable to human eyes.\nSubsequent research has proposed several methods of crafting adversarial exam-\nples, as well as methods of robustifying the classifier against such examples. An\nattacker with the knowledge of the classifier parameters can generate strong ad-\nversarial patterns. In response, a classifier with the knowledge of such patterns\ncan be trained to be robust to them. The cat-and-mouse game nature of the attacks\nand the defenses raises the question of the presence of an equilibrium in the dy-\nnamics. In this paper, we propose a game framework to formulate the interaction\nof attacks and defenses and present the natural notion of the best worst-case de-\nfense and attack. We propose simple algorithms to find those solutions motivated\nby sensitivity penalization. In addition, we show the potentials of learning-based\nattacks, and present the close relationship between the adversarial attack and the\nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-\n10 datasets.","pdf":"/pdf/be1767c6461d00041c1083a39b9e3d4095956260.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_defending_classifiers_against_learningbased_adversarial_attacks","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509610088487,"tcdate":1509610088487,"number":1,"cdate":1509610088487,"id":"rygOMUuCb","invitation":"ICLR.cc/2018/Conference/-/Paper965/Public_Comment","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["~Seong_Joon_Oh1"],"readers":["everyone"],"writers":["~Seong_Joon_Oh1"],"content":{"title":"untitled","comment":"Thanks a lot for your great work! I think game theory is really one of the few valid ways to study attacks & defenses regarding adversarial examples, as opposed to the \"cat-and-mouse game\" we see in this field these days. Honestly, it is really becoming harder to trust papers saying \"we have a great defense mechanism\" or \"we have a great attack method\". \n\nAlong a similar line of reasoning, we have published a paper at ICCV'17, \"Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective\". We have also proposed a game theoretic framework to find the equilibrium in the dynamics between user and recogniser, trying to thwart/re-enable recognition. Perhaps this paper should also be mentioned in the related work!\n\nI'd like to point out some issues that I'd like to hear your response. First one is the term \"best worst-case defense and attack\". I feel this is contradictory to the fact that \"we can only find local solutions in practice for complex loss functions such as deep networks-based defenders and attackers\" (sec4.2). And this is also to me the biggest hurdle for using game theory with non-convex rewards under this security/privacy setup -- the equilibria, or the saddle points, do not guarantee anything, making the game theoretic analysis inconclusive.\n\nMaybe a minor issue: while I like the cleanness of the formulation in eq7 (\"sensitivity penality\"), it eventually just tries to scale down the image gradients around the training data points (or hopefully around the entire data distribution). So, when FGSM is applied again to sensitivity penalised networks, wouldn't FGSM with larger step size (eta) re-enable high original error rate? Do you have any preliminary results?\n\nWhile game theory has limitations (that it's hard to guarantee upper/lower bounds in non-convex setup), I still think game theory is great in spelling out assumptions explicitly (as we have argued in our ICCV'17 paper). I appreciate that the authors have really discussed the limitations in sec5.1. Overall, I really enjoyed the paper!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers\ncan be fooled easily by small input perturbations unnoticeable to human eyes.\nSubsequent research has proposed several methods of crafting adversarial exam-\nples, as well as methods of robustifying the classifier against such examples. An\nattacker with the knowledge of the classifier parameters can generate strong ad-\nversarial patterns. In response, a classifier with the knowledge of such patterns\ncan be trained to be robust to them. The cat-and-mouse game nature of the attacks\nand the defenses raises the question of the presence of an equilibrium in the dy-\nnamics. In this paper, we propose a game framework to formulate the interaction\nof attacks and defenses and present the natural notion of the best worst-case de-\nfense and attack. We propose simple algorithms to find those solutions motivated\nby sensitivity penalization. In addition, we show the potentials of learning-based\nattacks, and present the close relationship between the adversarial attack and the\nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-\n10 datasets.","pdf":"/pdf/be1767c6461d00041c1083a39b9e3d4095956260.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_defending_classifiers_against_learningbased_adversarial_attacks","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092383370,"tcdate":1509137542888,"number":969,"cdate":1510092361078,"id":"ByqFhGZCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers\ncan be fooled easily by small input perturbations unnoticeable to human eyes.\nSubsequent research has proposed several methods of crafting adversarial exam-\nples, as well as methods of robustifying the classifier against such examples. An\nattacker with the knowledge of the classifier parameters can generate strong ad-\nversarial patterns. In response, a classifier with the knowledge of such patterns\ncan be trained to be robust to them. The cat-and-mouse game nature of the attacks\nand the defenses raises the question of the presence of an equilibrium in the dy-\nnamics. In this paper, we propose a game framework to formulate the interaction\nof attacks and defenses and present the natural notion of the best worst-case de-\nfense and attack. We propose simple algorithms to find those solutions motivated\nby sensitivity penalization. In addition, we show the potentials of learning-based\nattacks, and present the close relationship between the adversarial attack and the\nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-\n10 datasets.","pdf":"/pdf/be1767c6461d00041c1083a39b9e3d4095956260.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_defending_classifiers_against_learningbased_adversarial_attacks","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}