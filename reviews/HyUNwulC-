{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222618528,"tcdate":1511807734369,"number":3,"cdate":1511807734369,"id":"SyAgjAtgG","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Faster RNNs, with novel insights on need for nonlinear recurrence; novel and clear presentation","rating":"7: Good paper, accept","review":"This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure.\n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.\n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs. While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.\n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/1a0e775c000466d7addbfc382311ea5689fcac36.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512222618568,"tcdate":1511792283550,"number":2,"cdate":1511792283550,"id":"ry7sCqtgM","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Authors propose a method to make recurrent learning over 1000s and more time steps possible.","rating":"7: Good paper, accept","review":"# Summary and Assessment\n\nThe paper addresses an important issue–that of making learning of recurrent networks tractable for sequence lengths well beyond 1’000s of time steps. A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor. \nThe authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator. Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively.\nIn the following, the authors explore the landscape of RNNs satisfying the necessary conditions. The performance is investigated in terms of wall clock time. Further, experimental results of problems with previously untacked sequence lengths are reported.\n\nThe paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies.\nTo me, the execution seems sound. The experiments back up the claim.\n\n## Minor\n- I challenge the claim that thousands and millions of time steps are a common issue in “robotics, remote sensing, control systems, speech recognition, medicine and finance”, as claimed in the first paragraph of the introduction. IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I’d appreciate a few examples where this is a case to better justify the method.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/1a0e775c000466d7addbfc382311ea5689fcac36.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512222618608,"tcdate":1511691997090,"number":1,"cdate":1511691997090,"id":"Hkr1wGOeG","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer2"],"readers":["everyone"],"content":{"title":"simple but effective method for RNN speed up","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on accelerating RNN by applying the method from Blelloch (1990). The application is straightforward and thus technical novelty of this paper is limited. But the results are impressive. \n\nOne concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice. Could the authors comment on this potential limitation?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/1a0e775c000466d7addbfc382311ea5689fcac36.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1509739372046,"tcdate":1509095214338,"number":310,"cdate":1509739369384,"id":"HyUNwulC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/1a0e775c000466d7addbfc382311ea5689fcac36.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}