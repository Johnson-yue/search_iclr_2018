{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222621160,"tcdate":1511930862607,"number":3,"cdate":1511930862607,"id":"S1wxhnsef","invitation":"ICLR.cc/2018/Conference/-/Paper33/Official_Review","forum":"SyMvJrdaW","replyto":"SyMvJrdaW","signatures":["ICLR.cc/2018/Conference/Paper33/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review of \"Decoupling the Layers in Residual Networks\"","rating":"6: Marginally above acceptance threshold","review":"The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space.  This expression is used to develop a new layer called a “warp layer” which essentially tries to compute several layers of the residual network using the Taylor expansion expression — however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with.  Finally the authors stack these warp layers to create a “warped resnet” which they show does about as well as an ordinary ResNet but has better parallelization properties.\n\nTo me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple.  However there are several weaknesses to the paper (or maybe just things I didn’t understand).  First,  a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow.  Some of the notation is confusing here — for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case.  It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem).\n\nThroughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear.  The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same.\n\nFinally the experimental results are okay but perhaps a bit preliminary.  I have a few recommendations here:\n* It would be stronger to evaluate results on a larger dataset like ILSVRC.  \n* The relative speed-up of WarpNet compared to ResNet needs to be better explained — the authors break the computation of the WarpNet onto two GPUs, but it’s not clear if they do this for the (vanilla) ResNet as well.  In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch.  Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper.\n* In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion.  It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values.\n\nA few remaining questions for the authors:\n* There is a parallel submission (presumably by different authors called “Residual Connections Encourage Iterative Inference”) which contains some related insights.  I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa?\n* On implementation - the authors mention using Tensorflow’s auto-differentiation.  My question here is — are gradients being re-used intelligently as suggested in Section 3.1?  \n* I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now.  How much of the ideas offered in this paper would then generalize to non-resnet settings?\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoupling the Layers in Residual Networks","abstract":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a ﬁrst order approximation of the output over multiple layers. The ﬁrst order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found experimentally by Veit et al. (2016). We show that the Warped Residual Networks learn invariant models by breaking the redundancy in the weights caused by local symmetries in the input. The proposed network can outperform or achieve comparable predictive performance to the original residual network with the same number of parameters, while achieving a signiﬁcant speedup on the training time. We ﬁnd that as the layers get wider, the speedup in training time (44% with the widest architecture) is closer to the optimal speedup of 50% for skipping over one residual unit. Our architecture opens up a new research direction for methods to train a K layer ResNets in O(1) time as opposed to O(K) time with forward and backward propagation.","pdf":"/pdf/a7b312d8a12963f7547cb1d5089960c03ef63b16.pdf","TL;DR":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. ","paperhash":"anonymous|decoupling_the_layers_in_residual_networks","_bibtex":"@article{\n  anonymous2018decoupling,\n  title={Decoupling the Layers in Residual Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyMvJrdaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper33/Authors"],"keywords":["Warped residual networks","residual networks","symmetry breaking"]}},{"tddate":null,"ddate":null,"tmdate":1512222621202,"tcdate":1511818076257,"number":2,"cdate":1511818076257,"id":"r1NvXZ9ez","invitation":"ICLR.cc/2018/Conference/-/Paper33/Official_Review","forum":"SyMvJrdaW","replyto":"SyMvJrdaW","signatures":["ICLR.cc/2018/Conference/Paper33/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Creative investigation of Resnets","rating":"7: Good paper, accept","review":"Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100. While the experiments are not particularly impressive, I liked the originality of this paper. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoupling the Layers in Residual Networks","abstract":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a ﬁrst order approximation of the output over multiple layers. The ﬁrst order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found experimentally by Veit et al. (2016). We show that the Warped Residual Networks learn invariant models by breaking the redundancy in the weights caused by local symmetries in the input. The proposed network can outperform or achieve comparable predictive performance to the original residual network with the same number of parameters, while achieving a signiﬁcant speedup on the training time. We ﬁnd that as the layers get wider, the speedup in training time (44% with the widest architecture) is closer to the optimal speedup of 50% for skipping over one residual unit. Our architecture opens up a new research direction for methods to train a K layer ResNets in O(1) time as opposed to O(K) time with forward and backward propagation.","pdf":"/pdf/a7b312d8a12963f7547cb1d5089960c03ef63b16.pdf","TL;DR":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. ","paperhash":"anonymous|decoupling_the_layers_in_residual_networks","_bibtex":"@article{\n  anonymous2018decoupling,\n  title={Decoupling the Layers in Residual Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyMvJrdaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper33/Authors"],"keywords":["Warped residual networks","residual networks","symmetry breaking"]}},{"tddate":null,"ddate":null,"tmdate":1512222621236,"tcdate":1511762534044,"number":1,"cdate":1511762534044,"id":"ryCv5QFgz","invitation":"ICLR.cc/2018/Conference/-/Paper33/Official_Review","forum":"SyMvJrdaW","replyto":"SyMvJrdaW","signatures":["ICLR.cc/2018/Conference/Paper33/AnonReviewer1"],"readers":["everyone"],"content":{"title":"warp operator","rating":"6: Marginally above acceptance threshold","review":"Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100.\n\nOne thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from \"rotating\" a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoupling the Layers in Residual Networks","abstract":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a ﬁrst order approximation of the output over multiple layers. The ﬁrst order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found experimentally by Veit et al. (2016). We show that the Warped Residual Networks learn invariant models by breaking the redundancy in the weights caused by local symmetries in the input. The proposed network can outperform or achieve comparable predictive performance to the original residual network with the same number of parameters, while achieving a signiﬁcant speedup on the training time. We ﬁnd that as the layers get wider, the speedup in training time (44% with the widest architecture) is closer to the optimal speedup of 50% for skipping over one residual unit. Our architecture opens up a new research direction for methods to train a K layer ResNets in O(1) time as opposed to O(K) time with forward and backward propagation.","pdf":"/pdf/a7b312d8a12963f7547cb1d5089960c03ef63b16.pdf","TL;DR":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. ","paperhash":"anonymous|decoupling_the_layers_in_residual_networks","_bibtex":"@article{\n  anonymous2018decoupling,\n  title={Decoupling the Layers in Residual Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyMvJrdaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper33/Authors"],"keywords":["Warped residual networks","residual networks","symmetry breaking"]}},{"tddate":null,"ddate":null,"tmdate":1509739520445,"tcdate":1508556633843,"number":33,"cdate":1509739517780,"id":"SyMvJrdaW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyMvJrdaW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Decoupling the Layers in Residual Networks","abstract":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a ﬁrst order approximation of the output over multiple layers. The ﬁrst order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found experimentally by Veit et al. (2016). We show that the Warped Residual Networks learn invariant models by breaking the redundancy in the weights caused by local symmetries in the input. The proposed network can outperform or achieve comparable predictive performance to the original residual network with the same number of parameters, while achieving a signiﬁcant speedup on the training time. We ﬁnd that as the layers get wider, the speedup in training time (44% with the widest architecture) is closer to the optimal speedup of 50% for skipping over one residual unit. Our architecture opens up a new research direction for methods to train a K layer ResNets in O(1) time as opposed to O(K) time with forward and backward propagation.","pdf":"/pdf/a7b312d8a12963f7547cb1d5089960c03ef63b16.pdf","TL;DR":"We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. ","paperhash":"anonymous|decoupling_the_layers_in_residual_networks","_bibtex":"@article{\n  anonymous2018decoupling,\n  title={Decoupling the Layers in Residual Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyMvJrdaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper33/Authors"],"keywords":["Warped residual networks","residual networks","symmetry breaking"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}