{"notes":[{"tddate":null,"ddate":null,"tmdate":1512186910649,"tcdate":1512186655626,"number":5,"cdate":1512186655626,"id":"Skdm7jJ-G","invitation":"ICLR.cc/2018/Conference/-/Paper956/Public_Comment","forum":"HyzbhfWRW","replyto":"ry-5adjxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Results are good, some unclear explanation","comment":"(Not an author, but) \n\nFor equation (2), the authors have stated in the comments that the actual choice for which mapping method to use to have the dimensionalities of local features and the global descriptor align is an implementation detail. They've proposed two methods: \n\n1. Map the local features whose dimensionalities don't align with that of g to the correct dimensionality through densely connected layers.\n\n2. Map g to the dimensionalities of local features  through densely connected layers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222827685,"tcdate":1511914889087,"number":3,"cdate":1511914889087,"id":"ry-5adjxG","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Review","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["ICLR.cc/2018/Conference/Paper956/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Results are good, some unclear explanation ","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map. The goal is to make the learned attention maps highlight the regions of interest while suppressing background clutter. Experiments conducted on image classification and weakly supervised segmentation show the effectiveness of the proposed method.\n\nStrength of this paper:\n1) Most previous work are all implemented as post-hoc additions to fully trained networks while this work is end-to-end trainable. Not only the newly added weights for attention will be learned, so are the original weights in the network.\n2) The generalization ability shown in Table 3 is very good, outperforming other existing network by a large margin.\n3) Visualizations shown in the paper are convincing. \n\nSome weakness:\n1) Some of the notations are unclear in this paper, vector should be bold, hard to differentiate vector and scalar.\n2) In equation (2), l_i and g should have different dimensionality, how does addition work? Same as equation (3)\n3) The choice of layers to add attention modules is unclear to me. The authors just pick three layers from VGG to add attention, why picking those 3 layers? Is it better to add attention to lower layers or higher layers? Why is it the case that having more layers with attention achieves worse performance?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1511841176913,"tcdate":1511840391220,"number":4,"cdate":1511840391220,"id":"SkJcqL5lz","invitation":"ICLR.cc/2018/Conference/-/Paper956/Public_Comment","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"ResNet implementation details","comment":"Does the input go through batch normalization before it is passed to the first convolutional layer?\nWhat number of filters, kernel size and strides are used in the two convolutional layers in level 1? \nIs the second convolutional layer in level 1 followed by ReLU too, or is the only activation used in level 1 the ReLU between the two convolutional layers? \nIs there a max-pooling layer at the end of each of the four levels in the implementation, including level 1 and level 4? If so, what pool size and strides are used in each? \nDoes the implementation use the bottleneck design for residual blocks from the original paper? If not, what design is used for the residual blocks?  (So far, we've assumed the bottleneck design, with Conv(16,1)->BN->ReLu->Conv(16,3)->BN->ReLU->Conv(64,1)->BN->identity addition->ReLU in level 2, same in level 3 but using convolutions with parameters (32,1), (32,3), and (128,1), and same in level 4 but using convolution with parameters (64,1), (64,3), and (256,1))\n\nIs every convolutional layer in the model (except the ones used for dimensionality increases) followed by batch normalization, including convolutional layers in level 1?\nWhat number of filters, kernel size, and strides are used for the final convolutional layer? Is this layer followed by batch normalization and ReLU as well? \nWhat pool size and strides are used for the final pooling layer?\nIs the flattened output of the final pooling layer directly mapped to the dimensionalities of the local features through three separate linear fully-conected layers, or is it first passed through a ReLU-activated fully connected layer?\n\nThank you for the answers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222827725,"tcdate":1511814955520,"number":2,"cdate":1511814955520,"id":"rJ7NDl9xM","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Review","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["ICLR.cc/2018/Conference/Paper956/AnonReviewer2"],"readers":["everyone"],"content":{"title":"No title","rating":"6: Marginally above acceptance threshold","review":"This paper proposed an end-to-end trainable hierarchical attention mechanism for CNN. The proposed method computes 2d spatial attention map at multiple layers in CNN, where each attention map is obtained by computing compatibility scores between the intermediate features and the global feature. The proposed method demonstrated noticeable performance improvement on various discriminative tasks over existing approaches. \n\nOverall, the idea presented in the paper is simple yet solid, and showed good empirical performance. The followings are several concerns and suggestions.  \n\n1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task:\nSeo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016 \n\n2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category). Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.\n\n3. The proposed attention mechanism is mainly demonstrated for single-class classification task, but it would be interesting to see if it can also help the multi-class classification (e.g. image classification on MS-COCO or PASCAL VOC datasets)\n\n4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks. In that perspective, it would be interesting to see the comparisons against other attention mechanisms (e.g. Zhou et al 2016) in terms of localization performance.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222827766,"tcdate":1511787041748,"number":1,"cdate":1511787041748,"id":"ryqX5FFlG","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Review","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["ICLR.cc/2018/Conference/Paper956/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Has interesting idea but needs improvement","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a network with the standard soft-attention mechanism for classification tasks, where the global feature is used to attend on multiple feature maps of local features at different intermediate layers of CNN. The attended features at different feature maps are then used to predict the final classes by either concatenating features or ensembling results from individual attended features. The paper shows that the proposed model outperforms the baseline models in classification and weakly supervised segmentation.\n\nStrength:\n- It is interesting idea to use the global feature as a query in the attention mechanism while classification tasks do not naturally involve a query unlike other tasks such as visual question answering and image captioning.\n\n- The proposed model shows superior performances over GAP in multiple tasks.\n\nWeakness:\n- There are a lot of missing references. There have been a bunch of works using the soft-attention mechanism in many different applications including visual question answering [A-C], attribute prediction [D], image captioning [E,F] and image segmentation [G]. Only two previous works using the soft-attention (Bahdanau et al., 2014; Xu et al., 2015) are mentioned in Introduction but they are not discussed while other types of attention models (Mnih et al., 2014; Jaderberg et al., 2015) are discussed more.\n\n- Section 2 lacks discussions about related work but is more dedicated to emphasizing the contribution of the paper.\n\n- The global feature is used as the query vector for the attention calculation. Thus, if the global feature contains information for a wrong class, the attention quality should be poor too. Justification on this issue can improve the paper.\n\n- [H] reports the performance on the fine-grained bird classification using different type of attention mechanism. Comparison and justification with this method can improve the paper. The performance in [H] is almost 10 % point higher accuracy than the proposed model.\n\n- In the segmentation experiments, the models are trained on extremely small images, which is unnatural in segmentation scenarios. Experiments on realistic settings should be included. Moreover, [G] introduces a method of using an attention model for segmentation, while the paper does not contain any discussion about it.\n\n\nOverall, I am concerned that the proposed model is not well discussed with important previous works. I believe that the comparisons and discussions with these works can greatly improve the paper.\n\nI also have some questions about the experiments:\n- Is there any reasoning why we have to simplify the concatenation into an addition in Section 3.2? They are not equivalent.\n\n- When generating the fooling images of VGG-att, is the attention module involved, or do you use the same fooling images for both VGG and VGG-att?\n\nMinor comments:\n- Fig. 1 -> Fig. 2 in Section 3.1. If not, Fig. 2 is never referred.\n\nReferences\n[A] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, 2016.\n[B] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016.\n[C] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question answering with neural module networks. In CVPR, 2016.\n[D] Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, and Bohyung Han. Hierarchical attention networks. arXiv preprint arXiv:1606.02393, 2016.\n[E] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In CVPR, 2016.\n[F] Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-Guided Attention Model for Image Captioning. AAAI, 2017.\n[G] Seunghoon Hong, Junhyuk Oh, Honglak Lee and Bohyung Han, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, In CVPR, 2016.\n[H] Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Spatial Transformer Networks, NIPS, 2015\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1511588441128,"tcdate":1511588441128,"number":3,"cdate":1511588441128,"id":"r1-vztUlz","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Comment","forum":"HyzbhfWRW","replyto":"SJD74cJxM","signatures":["ICLR.cc/2018/Conference/Paper956/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper956/Authors"],"content":{"title":"Learning rate decay during transfer learning, color normalization algorithm","comment":"Thank you for your questions.\nWhen training the models on CUBS-200-2011 with weights initialised from those learned on CIFAR-100, we continue to use 10^-7 as the learning rate decay. Only the schedule for the learning rates is modified as explained.\n\nWe use the ZCA whitening method[1,2] for mean, standard deviation and color normalisation widely adopted for the pre-processing of CIFAR datasets.\n\n1. Goodfellow, Ian J., et al. \"Maxout networks.\" arXiv preprint arXiv:1302.4389 (2013).\n2. Zagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\" arXiv preprint arXiv:1605.07146 (2016)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1511295853295,"tcdate":1511134238767,"number":3,"cdate":1511134238767,"id":"SJD74cJxM","invitation":"ICLR.cc/2018/Conference/-/Paper956/Public_Comment","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Learning rate decay during transfer learning, color normalization algorithm","comment":"When training the models on CUB-200-2011 with weights initialized from those learned on CIFAR-100, did you still use 10^-7 as learning rate decay, or did you set learning rate decay to 0 and strictly stick to the exact learning rates from the transfer learning schedule?\n\nExactly which color normalization algorithm did you use? Also, in what order were dataset-wide mean and standard deviation normalization and color normalization done?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1510944485230,"tcdate":1510944485230,"number":2,"cdate":1510944485230,"id":"Bk6Jkhnyf","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Comment","forum":"HyzbhfWRW","replyto":"HyKA1qdkf","signatures":["ICLR.cc/2018/Conference/Paper956/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper956/Authors"],"content":{"title":"Clarifications of Implementation Details ","comment":"Thank you for your questions, answered sequentially below:\n\nThe weight vector u used in the parameterised compatibility score calculation corresponds to a given layer s; a different weight vector u_s is learned for each layer. As written, the expression in (2) assumes the context of a layer, and so the s index is implicit. We can make it explicit for better clarity.\n\nThe input image resolution of CIFAR-trained models is 32x32x3. This is the size to which the images of the test datasets are downsampled in the cross-domain classification experiments.\n\nThe local features are the ReLU-activated outputs of the convolutional layers before the corresponding max-pooling operation. This is done to keep the resolution of the attention maps as high as possible.\n\nThe global vector g is mapped to the dimensionality of the local features l (at a given layer s) by a linear layer if and only if their dimensionalities differ. Following from this, yes, the global vector g, once mapped to a given dimensionality, is then shared by the local features from different layers s as long as they are of that dimensionality."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1510749089264,"tcdate":1510674384975,"number":2,"cdate":1510674384975,"id":"HyKA1qdkf","invitation":"ICLR.cc/2018/Conference/-/Paper956/Public_Comment","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Dataset downsampling, weight vector in the parametrised compatibility score calculation, local feature source, more on local features and g.","comment":"Is the weight vector used in parametrised compatibility score calculation ('u') the same for every c calculation, or should a separate u vector be trained for each of the three attention submodules?\nWhat target resolution was used for downsampling images in the cross-domain classification datasets?\nAre the local features fed to the attention submodules extracted from the outputs of the pooling layers or from the outputs of the convolutional layer preceding them? (For example, in VGG-att3, is L_1 extracted directly from the output of the third 256-filter convolutional layer or from the 2x2 max pooling layer right after it?)\nIn the comment below, you proposed that in order to align the dimensionality of global g and the local feature vectors for the compatibility calculation step, local features should first be passed through an additional fully connected layer; should this step be done even if the dimensionality of g and local feature vectors already line up? (For example, as in the vectors from L_3 in VGG-att3)\nAdditionally, you proposed that instead of the local features being mapped to the dimensionality of g, g can be mapped to the dimensionality of the local features; in this case, if there are two or more submodules with local feature vectors of equal dimensionality, should g be mapped to each one separately, or should g only be projected to any given dimensionality once for shared use in the submodules with local features of that dimensionality? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1510092427801,"tcdate":1509993692133,"number":1,"cdate":1509993692133,"id":"rJN1a7CCZ","invitation":"ICLR.cc/2018/Conference/-/Paper956/Official_Comment","forum":"HyzbhfWRW","replyto":"B1eI20uCb","signatures":["ICLR.cc/2018/Conference/Paper956/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper956/Authors"],"content":{"title":"Learning to match the dimensions of local features and g","comment":"Thank you for your question. There is some discussion of this in paragraph 2, Sec. 3.1 though we should perhaps provide more details. We propose the use of one fully connected layer for each CNN layer s, that projects the local feature vectors of s to the dimensionality of g. These linear parameters are learned along with all other network parameters over the end-to-end training. There is an implementation detail, though, which we've neglected to mention: in order to limit the network parameters at the classification stage, we actually project g to the lower-dimensional space of the local features. We can update the section to reflect this in more detail."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1509645383945,"tcdate":1509645383945,"number":1,"cdate":1509645383945,"id":"B1eI20uCb","invitation":"ICLR.cc/2018/Conference/-/Paper956/Public_Comment","forum":"HyzbhfWRW","replyto":"HyzbhfWRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"ResNet Model Architectures","comment":"It's not totally clear what the ResNet model architecture is.  Section A.2 provides some details, but specifically I don't see how you're getting the dimensions of g and the intermediate layers to match up.  Are you able to provide some more detailed information about the architectures?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1510092383648,"tcdate":1509137401829,"number":956,"cdate":1510092361978,"id":"HyzbhfWRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyzbhfWRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learn to Pay Attention","abstract":"We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.","pdf":"/pdf/83c8388e456aca6cdead7f7e65849b828ea46022.pdf","TL;DR":"The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.","paperhash":"anonymous|learn_to_pay_attention","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Pay Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyzbhfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper956/Authors"],"keywords":["deep learning","attention-aware representations","image classification","weakly supervised segmentation","domain shift","classifier generalisation","robustness to adversarial attack"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}