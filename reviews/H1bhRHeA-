{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222604776,"tcdate":1511763485150,"number":2,"cdate":1511763485150,"id":"HJBXAQFeM","invitation":"ICLR.cc/2018/Conference/-/Paper266/Official_Review","forum":"H1bhRHeA-","replyto":"H1bhRHeA-","signatures":["ICLR.cc/2018/Conference/Paper266/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Paper explores SGD based explorations to reduce instability in soft-max minimization","rating":"5: Marginally below acceptance threshold","review":"The problem of numerical instability in applying SGD to soft-max minimization is the motivation. It would have been helpful if the author(s) could have made a formal statement. \nSince the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary. The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms.\n\nThe proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies.\nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\"Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\"\n \nwhich could have shed more light on the matter. \n\nThe title is also misleading in using the word \"exact\". I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\n\nIn summary the algorithms are novel variants of SGD but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing. The choice of word exact is also not clear.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exact scalable softmax optimization","abstract":"Recent state-of-the-art neural network and language models have begun to rely on softmax distributions with an extremely large number of categories. In this context calculating the softmax normalizing constant is prohibitively expensive, which has spurred a growing literature of efficiently computable but biased estimates of the softmax. In this paper we present the first two unbiased algorithms for optimizing the softmax whose work per iteration is independent of the number of classes and datapoints (and does not require extra work at the end of each epoch). We compare their empirical performance to the state-of-the-art on seven real world datasets, with our Implicit SGD algorithm comprehensively outperforming all competitors.","pdf":"/pdf/a14f711c2340d51c718de4e5bed3444c54556cbd.pdf","TL;DR":"Propose first methods for exactly optimizing the softmax distribution using stochastic gradient with runtime independent on the number of classes or datapoints.","paperhash":"anonymous|exact_scalable_softmax_optimization","_bibtex":"@article{\n  anonymous2018exact,\n  title={Exact scalable softmax optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bhRHeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper266/Authors"],"keywords":["softmax","optimization","implicit sgd"]}},{"tddate":null,"ddate":null,"tmdate":1512222604816,"tcdate":1511718991883,"number":1,"cdate":1511718991883,"id":"BJ_UlF_lM","invitation":"ICLR.cc/2018/Conference/-/Paper266/Official_Review","forum":"H1bhRHeA-","replyto":"H1bhRHeA-","signatures":["ICLR.cc/2018/Conference/Paper266/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents interesting algorithms for minimizing softmax with many classes. ","rating":"5: Marginally below acceptance threshold","review":"The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi-class classification problem (using softmax loss) and with linear model. The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it. At each iteration, SGD samples a subset of training samples and labels. The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach. It seems the implicit SGD approach is better in the experimental comparisons. \n\nI found the paper quite interesting, but meanwhile I have the following comments and questions: \n\n- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD. The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch. However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck? Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed. \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? Also, I guess we are assuming the obj is strongly convex?\n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data. As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data). \n\n- All the comparisons are based on \"epochs\", but the competing algorithms are quite different and can have very different running time for each epoch. For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. \n\n- The claim that \"implicit SGD never overshoots the optimum\" needs more supports. Is it proved in some previous papers? \n\n- The presentation can be improved. I think it will be helpful to state the algorithms explicitly in the main paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exact scalable softmax optimization","abstract":"Recent state-of-the-art neural network and language models have begun to rely on softmax distributions with an extremely large number of categories. In this context calculating the softmax normalizing constant is prohibitively expensive, which has spurred a growing literature of efficiently computable but biased estimates of the softmax. In this paper we present the first two unbiased algorithms for optimizing the softmax whose work per iteration is independent of the number of classes and datapoints (and does not require extra work at the end of each epoch). We compare their empirical performance to the state-of-the-art on seven real world datasets, with our Implicit SGD algorithm comprehensively outperforming all competitors.","pdf":"/pdf/a14f711c2340d51c718de4e5bed3444c54556cbd.pdf","TL;DR":"Propose first methods for exactly optimizing the softmax distribution using stochastic gradient with runtime independent on the number of classes or datapoints.","paperhash":"anonymous|exact_scalable_softmax_optimization","_bibtex":"@article{\n  anonymous2018exact,\n  title={Exact scalable softmax optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bhRHeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper266/Authors"],"keywords":["softmax","optimization","implicit sgd"]}},{"tddate":null,"ddate":null,"tmdate":1509739395963,"tcdate":1509084841024,"number":266,"cdate":1509739393303,"id":"H1bhRHeA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1bhRHeA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exact scalable softmax optimization","abstract":"Recent state-of-the-art neural network and language models have begun to rely on softmax distributions with an extremely large number of categories. In this context calculating the softmax normalizing constant is prohibitively expensive, which has spurred a growing literature of efficiently computable but biased estimates of the softmax. In this paper we present the first two unbiased algorithms for optimizing the softmax whose work per iteration is independent of the number of classes and datapoints (and does not require extra work at the end of each epoch). We compare their empirical performance to the state-of-the-art on seven real world datasets, with our Implicit SGD algorithm comprehensively outperforming all competitors.","pdf":"/pdf/a14f711c2340d51c718de4e5bed3444c54556cbd.pdf","TL;DR":"Propose first methods for exactly optimizing the softmax distribution using stochastic gradient with runtime independent on the number of classes or datapoints.","paperhash":"anonymous|exact_scalable_softmax_optimization","_bibtex":"@article{\n  anonymous2018exact,\n  title={Exact scalable softmax optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bhRHeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper266/Authors"],"keywords":["softmax","optimization","implicit sgd"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}