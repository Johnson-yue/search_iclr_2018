{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222678258,"tcdate":1511873847098,"number":3,"cdate":1511873847098,"id":"r11STCqxG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clever trick for making general memory-efficient online unbiased RNN learning possible","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents a generic unbiased low-rank stochastic approximation to full rank matrices that makes it possible to do online RNN training without the O(n^3) overhead of real-time recurrent learning (RTRL). This is an important and long-sought-after goal of connectionist learning and this paper presents a clear and concise description of why their method is a natural way of achieving that goal, along with experiments on classic toy RNN tasks with medium-range time dependencies for which other low-memory-overhead RNN training heuristics fail. My only major complaint with the paper is that it does not extend the method to large-scale problems on real data, for instance work from the last decade on sequence generation, speech recognition or any of the other RNN success stories that have led to their wide adoption (eg Graves 2013, Sutskever, Martens and Hinton 2011 or Graves, Mohamed and Hinton 2013). However, if the paper does achieve what it claims to achieve, I am sure that many people will soon try out UORO to see if the results are in any way comparable.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/b3615d457dc03c8d989e80b8bb5c2a75c3abd116.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222678303,"tcdate":1511819610849,"number":2,"cdate":1511819610849,"id":"B1QPFb5eG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very interesting paper that approaches online training of RNNs in a principled way, although more experiments would make it more convincing","rating":"7: Good paper, accept","review":"This is a very interesting paper. Training RNN's in an online fashion (with no backpropagation through time) is one of those problems which are not well explored in the research community. And I think, this paper approaches this problem in a very principled manner. The authors proposes to use forward approach for the calculation of the gradients. The author proposes to modify RTRL by maintaining a rank one approximation of jacobian matrix (derivative of state w.r.t parameters)  which was done in NoBackTrack Paper. The way I think this paper is different from NoBackTrack Paper is that this version can be implemented in a black box fashion and hence easy to implement using current DL libraries like Pytorch. \n\nPros.\n\n- Its an interesting paper, very easy to follow, and with proper literature survey.\n\nCons:\n\n- The results are quite preliminary. I'll note that this is a very difficult problem.\n- \"The proof of UOROâ€™s convergence to a local optimum is soon to be published Masse & Ollivier (To appear).\"  I think, paper violates the anonymity.  So, I'd encourage the authors to remove this. \n\nSome Points: \n\n- I find the argument of stochastic gradient descent wrong (I could be wrong though). RNN's follow the markov property (wrt hidden states from previous time step and the current input) so from time step t to t+1, if you change the parameters, the hidden state at time t (and all the time steps before) would carry stale information unless until you're using something like eligibility traces from RL literature. I also don't know how to overcome this issue. \n\n- I'd be worried about the variance in the estimate of rank one approximation. All the experiments carried out by the authors are small scale (hidden size = 64). I'm curious if authors tried experimenting with larger networks, I'd guess it wont perform well due to the high variance in the approximation. I'd like to see an experiment with hidden size  = 128/256/512/1024. My intuition is that because of high variance it would be difficult to train this network, but I could be wrong. I'm curious what the authors had to say about this. \n\n- If the variance of the approximation is indeed high, can we use something to control the dynamics of the network which can result in less variance. Have authors thought about this ? \n\n- I'd also like to see experiments on copying task/adding task (as these are standard experiments which are done for analysis of long term dependencies) \n\n- I'd also like to see what effect the length of sequence has on the approximation. As small errors in approximation on each step can compound giving rise to chaotic dynamics. (small change in input => large change in output)\n\n- I'd also like to know how using UORO changes the optimization as compared to Back-propagation through time in the sense, does the two approaches would reach same local minimum ? or is there a possibility that the former can reach \"less\" number of potential local minimas as compared to BPTT. \n\n\nI'm tempted to give high score for this paper( Score - 7) , as it is unexplored direction in our research community, and I think this paper makes a very useful contribution to tackle this problem in a very principled way.  But I'd like some more experiments to be done (which I have mentioned above), failing to do those experiments, I'd be forced to reduce the score (to score - 5) ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/b3615d457dc03c8d989e80b8bb5c2a75c3abd116.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222678348,"tcdate":1511812206743,"number":1,"cdate":1511812206743,"id":"B1Ud2yqgz","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the limitation of truncated backpropagation through time (BPTT) of providing biased gradient estimates.\n\nThe idea is to use a forward computation of the gradient as in Williams and Zipser (1989) with an unbiased approximation of Delta s_t/Delta theta to reduce the memory and computational cost.\n\nThe proposed approach, called UORO, is tested on a few artificial datasets.\n\nThe approach is interesting and could potentially be very useful. However, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.\nRather than with truncated BPTT with smaller truncation than required, which is easy to outperform, I would have expected a comparison with some of the other methods mentioned in the Related Work Section, such as NBT, ESNs, Decoupled Neural Interfaces, etc. Also the evaluation should be extended to other challenging tasks. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/b3615d457dc03c8d989e80b8bb5c2a75c3abd116.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739261187,"tcdate":1509124955034,"number":514,"cdate":1509739258525,"id":"rJQDjk-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/b3615d457dc03c8d989e80b8bb5c2a75c3abd116.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}