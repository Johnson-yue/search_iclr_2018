{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222768752,"tcdate":1512024536203,"number":3,"cdate":1512024536203,"id":"ryeycmTlz","invitation":"ICLR.cc/2018/Conference/-/Paper798/Official_Review","forum":"BkQCGzZ0-","replyto":"BkQCGzZ0-","signatures":["ICLR.cc/2018/Conference/Paper798/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Autoencoders for text with a new method for using discrete latent space","rating":"6: Marginally above acceptance threshold","review":"The authors describe a method for encoding text into a discrete representation / latent space. On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT.\n\nThe proposed method seems effective, and the proposed DSAE metric is nice, though it’s surprising if previous papers have not used metrics similar to normalized reduction in log-ppl. The datasets considered in the experiments are also large, another plus. However, overall, the paper is difficult to read and parse, especially since low-level details are weaved together with higher-level points throughout, and are often not motivated.\n\nThe major critique would be the qualitative nature of results in the sections on “Decipering the latent code” and (to a lesser extent) “Mixed sample-beam decoding.” These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3. Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding.\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Autoencoders for Sequence Models","abstract":"Recurrent models for sequences have been recently successful at many tasks, especially for language modeling\nand machine translation. Nevertheless, it remains challenging to extract good representations from\nthese models. For instance, even though language has a clear hierarchical structure going from characters\nthrough words to sentences, it is not apparent in current language models.\nWe propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to propagate gradients\nthough this discrete representation we introduce an improved semantic hashing technique.\nWe show that this technique performs well on a newly proposed quantitative efficiency measure.\nWe also analyze latent codes produced by the model showing how they correspond to\nwords and phrases. Finally, we present an application of the autoencoder-augmented\nmodel to generating diverse translations.","pdf":"/pdf/8bf732a964e4700f8b7c44a9d071318366b95402.pdf","TL;DR":"Autoencoders for text with a new method for using discrete latent space.","paperhash":"anonymous|discrete_autoencoders_for_sequence_models","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Autoencoders for Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQCGzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper798/Authors"],"keywords":["autoencoders","sequence models","discrete representations"]}},{"tddate":null,"ddate":null,"tmdate":1512222768790,"tcdate":1511826393752,"number":2,"cdate":1511826393752,"id":"HJGJNQcez","invitation":"ICLR.cc/2018/Conference/-/Paper798/Official_Review","forum":"BkQCGzZ0-","replyto":"BkQCGzZ0-","signatures":["ICLR.cc/2018/Conference/Paper798/AnonReviewer2"],"readers":["everyone"],"content":{"title":"lack of valid experiments ","rating":"4: Ok but not good enough - rejection","review":"This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder. \nHowever, the experiments are too weak to demonstrate the effectiveness of using discrete representations.\nThe design of the experiments on language model is problematic.\nThere are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper. \n\n\nGenerally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations. \nHowever, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart. \nThe proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people. \nHow is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model?\nThe qualitative evaluation on 'Deciperhing the Latent Code' is not enough either. \nIn addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively.\nOne suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case.\n\nOverall, this paper is more suitable for the workshop track. It also needs a lot of more studies on related work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Autoencoders for Sequence Models","abstract":"Recurrent models for sequences have been recently successful at many tasks, especially for language modeling\nand machine translation. Nevertheless, it remains challenging to extract good representations from\nthese models. For instance, even though language has a clear hierarchical structure going from characters\nthrough words to sentences, it is not apparent in current language models.\nWe propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to propagate gradients\nthough this discrete representation we introduce an improved semantic hashing technique.\nWe show that this technique performs well on a newly proposed quantitative efficiency measure.\nWe also analyze latent codes produced by the model showing how they correspond to\nwords and phrases. Finally, we present an application of the autoencoder-augmented\nmodel to generating diverse translations.","pdf":"/pdf/8bf732a964e4700f8b7c44a9d071318366b95402.pdf","TL;DR":"Autoencoders for text with a new method for using discrete latent space.","paperhash":"anonymous|discrete_autoencoders_for_sequence_models","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Autoencoders for Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQCGzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper798/Authors"],"keywords":["autoencoders","sequence models","discrete representations"]}},{"tddate":null,"ddate":null,"tmdate":1512222768830,"tcdate":1511716416227,"number":1,"cdate":1511716416227,"id":"Hk_BLd_ef","invitation":"ICLR.cc/2018/Conference/-/Paper798/Official_Review","forum":"BkQCGzZ0-","replyto":"BkQCGzZ0-","signatures":["ICLR.cc/2018/Conference/Paper798/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Discrete Autoencoders for Sequence Models","rating":"5: Marginally below acceptance threshold","review":"The topic is interesting however the description in the paper is lacking clarity. The paper is written in a procedural fashion - I first did that, then I did that and after that I did third. Having proper mathematical description and good diagrams of what you doing would have immensely helped. Another big issue is the lack of proper validation in Section 3.4. Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis. Given that I see only one example I can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having none. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Autoencoders for Sequence Models","abstract":"Recurrent models for sequences have been recently successful at many tasks, especially for language modeling\nand machine translation. Nevertheless, it remains challenging to extract good representations from\nthese models. For instance, even though language has a clear hierarchical structure going from characters\nthrough words to sentences, it is not apparent in current language models.\nWe propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to propagate gradients\nthough this discrete representation we introduce an improved semantic hashing technique.\nWe show that this technique performs well on a newly proposed quantitative efficiency measure.\nWe also analyze latent codes produced by the model showing how they correspond to\nwords and phrases. Finally, we present an application of the autoencoder-augmented\nmodel to generating diverse translations.","pdf":"/pdf/8bf732a964e4700f8b7c44a9d071318366b95402.pdf","TL;DR":"Autoencoders for text with a new method for using discrete latent space.","paperhash":"anonymous|discrete_autoencoders_for_sequence_models","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Autoencoders for Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQCGzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper798/Authors"],"keywords":["autoencoders","sequence models","discrete representations"]}},{"tddate":null,"ddate":null,"tmdate":1509739095782,"tcdate":1509135051009,"number":798,"cdate":1509739093121,"id":"BkQCGzZ0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkQCGzZ0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete Autoencoders for Sequence Models","abstract":"Recurrent models for sequences have been recently successful at many tasks, especially for language modeling\nand machine translation. Nevertheless, it remains challenging to extract good representations from\nthese models. For instance, even though language has a clear hierarchical structure going from characters\nthrough words to sentences, it is not apparent in current language models.\nWe propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to propagate gradients\nthough this discrete representation we introduce an improved semantic hashing technique.\nWe show that this technique performs well on a newly proposed quantitative efficiency measure.\nWe also analyze latent codes produced by the model showing how they correspond to\nwords and phrases. Finally, we present an application of the autoencoder-augmented\nmodel to generating diverse translations.","pdf":"/pdf/8bf732a964e4700f8b7c44a9d071318366b95402.pdf","TL;DR":"Autoencoders for text with a new method for using discrete latent space.","paperhash":"anonymous|discrete_autoencoders_for_sequence_models","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Autoencoders for Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQCGzZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper798/Authors"],"keywords":["autoencoders","sequence models","discrete representations"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}