{"notes":[{"ddate":null,"tddate":1511820717252,"tmdate":1512222718506,"tcdate":1511820535716,"number":3,"cdate":1511820535716,"id":"HklWpb5lz","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer3"],"readers":["everyone"],"content":{"title":"the ideas are simple but seems to work well empirically","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a lightweight neural network that achieves state-of-the-art performance on NER. The network allows efficient active incremental training, which significantly reduces the amount of training data needed to match state-of-the-art performance.\n\nThe paper is well-written. The ideas are simple, but they seem to work well in the experiments. Interestingly, LSTM decoder seems to have slight advantage over CRF decoder although LSTM does not output the best tag sequence. It'd be good to comment on this.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/23f65c06a2dfe6103d32d1bd1aa0b3e98495f3a5.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222718549,"tcdate":1511807369555,"number":2,"cdate":1511807369555,"id":"S1G9tRFgG","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper studies the application of different existing active learning strategies for the deep models for NER. This paper has several strong and weak points listed in the reviews","rating":"5: Marginally below acceptance threshold","review":"This paper studies the application of different existing active learning strategies for the deep models for NER.\n\nPros:\n* Active learning may be used for improving the performance of deep models for NER in practice\n* All the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for NER\n\nCons:\n* The novelty of this paper is marginal. The proposed approaches turn out to be a combination of existing active learning strategies for selecting data to query with the existing deep model for NER. \n* No conclusion can be drawn by comparing with the 4 different strategies.\n.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/23f65c06a2dfe6103d32d1bd1aa0b3e98495f3a5.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222718594,"tcdate":1511567867025,"number":1,"cdate":1511567867025,"id":"ry7bzE8eG","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Active learning for BiLSTM-based NER model","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThis paper applies active learning to a deep neural model (CNN-CNN-LSTM) for  named-entity recognition, which allows the model to match state-of-the-art performance with about 25% of the full training data.\n\nStrength:\nThe paper is relatively easy to follow. Experiments show significant reduction of training samples needed.\n\nWeaknesses:\nAbout half of the content is used to explain the CNN-CNN-LSTM architecture, which seems orthogonal to the active learning angle, except for the efficiency gain from replacing the CRF with an LSTM.\n\nThe difference in performance among the sampling strategies (as shown in Figure 4) seems very tiny. Therefore, it is difficult to tell what we can really learn from these empirical results.\n\nOther questions and comments:\nIn Table 4: Why is the performance of LSTM-LSTM-LSTM not reported for OntoNotes 5.0, or was the model simply too inefficient? \n\nHow is the variance of the model performance? At the early stage of active learning, the model uses as few as 1% of the training samples, which might cause large variance in terms of dev/test accuracy. \n\nThe SUBMOD method is not properly explained in Section 4. As one of the active learning techniques being compared in experiments, it might be better to formally describe the approach instead of putting it in the appendix.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/23f65c06a2dfe6103d32d1bd1aa0b3e98495f3a5.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1509739165964,"tcdate":1509131749590,"number":677,"cdate":1509739163305,"id":"ry018WZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/23f65c06a2dfe6103d32d1bd1aa0b3e98495f3a5.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}