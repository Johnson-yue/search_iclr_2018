{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222834107,"tcdate":1511816357514,"number":2,"cdate":1511816357514,"id":"H1asng9lG","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Review","forum":"ryTp3f-0-","replyto":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference/Paper982/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but hard to know the novelty","rating":"7: Good paper, accept","review":"This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called \"Workflow Guided Exploration\". Workflows are defined through a DSL unique to the domain.\n\nThe paper is clear, very well written, and well-motivated. Exploration is still a challenging problem for RL. The workflows remind me of options though in this paper they appear to be hand-crafted. In that sense, I wonder if this has been done before in another domain. The results suggest that WGE sometimes helps but not consistently. While the experiments show that DOMNET improves over Shi et al, that could be explained as not having to train on raw pixels or not enough episodes.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1512222834149,"tcdate":1510880812299,"number":1,"cdate":1510880812299,"id":"BJ448noJM","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Review","forum":"ryTp3f-0-","replyto":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference/Paper982/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper has good empirical results, but algorithmic novelty is small.  It could benefit from more concrete comparisons in the literaure","rating":"5: Marginally below acceptance threshold","review":"SUMMARY\n\nThe paper deals with the problem of training RL algorithms from demonstration and applying them to various web interfaces such as booking flights.  Specifically, it is applied to the Mini world of Bids benchmark (http://alpha.openai.com/miniwob/).\n\nThe difference from existing work is that rather than training an agent to directly mimic the demonstrations, it uses demonstrations to constrain exploration. By pruning away bad exploration directions. \n\nThe idea is to  build a lattice of workflows from demonstration and randomly sample sequence of actions from this lattice that satisfy the current goal.    Use the sequences of actions to sample trajectories and use the trajectories to learn the RL policy.\n\n\n\nCOMMENTS\n\n\nIn effect, the workflow sequences provide more generalization than simply mimicking, but It not obvious, why they don’t run into overfitting problems.  However experimentally the paper performs better than the previous approach.\n\nThere is a big literature on learning from demonstrations that the authors could compare with, or explain why their work is different.  \n\nIn addition, they make general comparison to RL literature such as hierarchy rather than more concrete comparisons with the problem at hand (learning from demonstrations.)\n\nWhat does DOM stand for?  The paper is not self-contained.  For example, what does DOM stand for?  \n\n\nIn the results of table 1 and Figure 3.  Why more steps mean success?\n\nIn equation 4 there seems to exist an environment model.  Why do we need to use this whole approach in the paper then?  Couldn’t  we just do policy iteration?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1510092383090,"tcdate":1509137617433,"number":982,"cdate":1510092360960,"id":"ryTp3f-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}