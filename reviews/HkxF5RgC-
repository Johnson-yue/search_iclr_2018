{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222658634,"tcdate":1511805582581,"number":3,"cdate":1511805582581,"id":"H1PcMAKeG","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Novel and impactful contributions, but unclear relevance and expected audience","rating":"6: Marginally above acceptance threshold","review":"The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels. The authors further propose model-pruning optimizations which are aware of the persistent implementation.\n\nIt's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations. The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here. For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation.\n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs. The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of 5x over the next best algorithm using only 36 out of a P100's 56 SMs for a hidden layer of size 2304, batch size of 4, and a density of 10%.  Further, our technique allows for models of over 3x the size to fit on a GPU for a speedup of 5x, enabling larger networks to help advance the state-of-the-art.  We present a case study on NMT with LSTMs in the appendix.","pdf":"/pdf/62536e3f9ce4761b1589012fc40cbbd7908f30d6.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1512222658675,"tcdate":1511751829119,"number":2,"cdate":1511751829119,"id":"BJ6cxWFlM","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Sparse Persistent RNNs Review: Limited novelty over persistent RNNs","rating":"5: Marginally below acceptance threshold","review":"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip. The paper describes the use additional mechanisms for synchronization and memory loading. \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).  With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads. Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise. \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip. One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those.\n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs. However, Narang et. al., 2017 already explore this idea, although briefly. Furthermore, the gains from the sparsity appear rather limited over real applications. I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads). Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial. However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of 5x over the next best algorithm using only 36 out of a P100's 56 SMs for a hidden layer of size 2304, batch size of 4, and a density of 10%.  Further, our technique allows for models of over 3x the size to fit on a GPU for a speedup of 5x, enabling larger networks to help advance the state-of-the-art.  We present a case study on NMT with LSTMs in the appendix.","pdf":"/pdf/62536e3f9ce4761b1589012fc40cbbd7908f30d6.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1512222658713,"tcdate":1511335811237,"number":1,"cdate":1511335811237,"id":"rkoKvifef","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper devises sparse GPU kernels for RNNs","rating":"6: Marginally above acceptance threshold","review":"The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper). It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal.\n\nPros:\n- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs\n- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well\n\nCons:\n- This work may be better reviewed at a more GPU-centric conference\n- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of 5x over the next best algorithm using only 36 out of a P100's 56 SMs for a hidden layer of size 2304, batch size of 4, and a density of 10%.  Further, our technique allows for models of over 3x the size to fit on a GPU for a speedup of 5x, enabling larger networks to help advance the state-of-the-art.  We present a case study on NMT with LSTMs in the appendix.","pdf":"/pdf/62536e3f9ce4761b1589012fc40cbbd7908f30d6.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1509739290637,"tcdate":1509120632404,"number":462,"cdate":1509739287988,"id":"HkxF5RgC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of 5x over the next best algorithm using only 36 out of a P100's 56 SMs for a hidden layer of size 2304, batch size of 4, and a density of 10%.  Further, our technique allows for models of over 3x the size to fit on a GPU for a speedup of 5x, enabling larger networks to help advance the state-of-the-art.  We present a case study on NMT with LSTMs in the appendix.","pdf":"/pdf/62536e3f9ce4761b1589012fc40cbbd7908f30d6.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}