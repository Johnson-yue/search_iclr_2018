{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222681167,"tcdate":1511892467599,"number":2,"cdate":1511892467599,"id":"rkseIXolf","invitation":"ICLR.cc/2018/Conference/-/Paper534/Official_Review","forum":"S1tWRJ-R-","replyto":"S1tWRJ-R-","signatures":["ICLR.cc/2018/Conference/Paper534/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes a model for allowing various deep neural network architectures to share weights (parameters) across different datasets. The authors then apply the framework to transfer learning.","rating":"5: Marginally below acceptance threshold","review":"The paper addresses the question of identifying 'shared features' in neural networks trained on different datasets.  Concretely, suppose you have two datasets X1, X2 and you would like to train auto-encoders (with potential augmentation with labeled examples) for the two datasets. One could work on the two separately; here, the authors propose sharing some of the weights to try and exploit/identify common features between the two datasets. The authors formalize by essentially looking to optimize an auto-encoder that take inputs of the form (x1, x2) and employing architectures that allow few nodes to interact with both x1,x2. The authors then try to minimize an appropriate loss function by standard methods. \n\nThe authors then apply the above methodology to transfer learning between various datasets. The empirical results here are interesting but not particularly striking; the most salient feature perhaps is that the architectures and training algorithms are perhaps a bit simpler but the overall improvements over existing methods are not too exciting. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Joint autoencoders: a flexible meta-learning framework","abstract":"The incorporation of prior knowledge into learning is essential in achieving good performance based on small noisy samples. Such knowledge is often incorporated through the availability of related data arising from domains and tasks similar to the one of current interest. Ideally one would like to allow both the data for the current task and for previous related tasks to self-organize the learning system in such a way that commonalities and differences between the tasks are learned in a data-driven fashion. We develop a framework for learning multiple tasks simultaneously, based on sharing features that are common to all tasks, achieved through the use of a modular deep feedforward neural network consisting of shared branches, dealing with the common features of all tasks, and private branches, learning the specific unique aspects of each task. Once an appropriate weight sharing architecture has been established, learning takes place through standard algorithms for feedforward networks, e.g., stochastic gradient descent and its variations. The method deals with meta-learning (such as domain adaptation, transfer and multi-task learning) in a unified fashion, and can easily deal with data arising from different types of sources. Numerical experiments demonstrate the effectiveness of learning in domain adaptation and transfer learning setups, and provide evidence for the flexible and task-oriented representations arising in the network.","pdf":"/pdf/1f98821d6ea04dd61ea75f85fbfde986f2b0187f.pdf","TL;DR":"A generic framework for handling transfer and multi-task learning using pairs of autoencoders with task-specific and shared weights.","paperhash":"anonymous|joint_autoencoders_a_flexible_metalearning_framework","_bibtex":"@article{\n  anonymous2018joint,\n  title={Joint autoencoders: a flexible meta-learning framework},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1tWRJ-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper534/Authors"],"keywords":["transfer learning","domain adaptation","unsupervised learning","autoencoders","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222681208,"tcdate":1511857393990,"number":1,"cdate":1511857393990,"id":"H1cgp9qxf","invitation":"ICLR.cc/2018/Conference/-/Paper534/Official_Review","forum":"S1tWRJ-R-","replyto":"S1tWRJ-R-","signatures":["ICLR.cc/2018/Conference/Paper534/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The work proposed a generic framework for end-to-end transfer learning / domain adaptation with deep neural networks. The idea is to learn a joint autoencoders, containing private branch with task/domain-specific weights, as well as common branch consisting of shared weights used across tasks/domains, as well as task/domain-specific weights.  Supervised losses are added after the encoders to utilize labeled samples from different tasks. Experiments on the MNIST and CIFAR datasets showed improvements over baseline models. Its performance is comparable to / worse than several existing deep domain adaptation works on the MNIST, USPS and SVHN digit datasets.\n\nThe structure of the paper is good, and easy to read.  The idea is fairly straight-forward. It reads as an extension of \"frustratingly easy domain adaptation\" to DNN (please cite this work). Different from most existing work on DNN for multi-task/transfer learning, which focuses on weight sharing in bottom layers, the work emphasizes the importance of weight sharing in deeper layers. The overall novelty of the work is limited though. \n\nThe authors brought up two strategies on learning the shared and private weights at the end of section 3.2. However, no follow-up comparison between the two are provided. It seems like most of the results are coming from the end-to-end learning. \n\nExperimental results:\nsection 4.1: Figure 2 is flawed. The colors do not correspond to the sub-tasks. For example, there are digits 1, 4 in color magenta, which is supposed to be the shared branch of digits of 5~9. Vice versa. \nFrom reducing the capacity of JAE to be the same as the baseline, most of the improvement is gone. It is not clear how much of the improvement will remain if the baseline model gets to see all the samples instead of just those from each sub-task. \n\nsection 4.2.1: The authors demonstrate the influence of shared layer depth in table 2. While it does seem to matter for tasks of dissimilar inputs, have the authors compare having a completely shared branch or sharing more than just a single layer?\n\nThe authors suggested in section 4.1 CIFAR experiment that the proposed method provides more performance boost when the two tasks are more similar, which seems to be contradicting to the results shown in Figure 3, where its performance is worse when transferring between USPS and MNIST, which are more similar tasks vs between SVHN and MNIST. Do the authors have any insight?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Joint autoencoders: a flexible meta-learning framework","abstract":"The incorporation of prior knowledge into learning is essential in achieving good performance based on small noisy samples. Such knowledge is often incorporated through the availability of related data arising from domains and tasks similar to the one of current interest. Ideally one would like to allow both the data for the current task and for previous related tasks to self-organize the learning system in such a way that commonalities and differences between the tasks are learned in a data-driven fashion. We develop a framework for learning multiple tasks simultaneously, based on sharing features that are common to all tasks, achieved through the use of a modular deep feedforward neural network consisting of shared branches, dealing with the common features of all tasks, and private branches, learning the specific unique aspects of each task. Once an appropriate weight sharing architecture has been established, learning takes place through standard algorithms for feedforward networks, e.g., stochastic gradient descent and its variations. The method deals with meta-learning (such as domain adaptation, transfer and multi-task learning) in a unified fashion, and can easily deal with data arising from different types of sources. Numerical experiments demonstrate the effectiveness of learning in domain adaptation and transfer learning setups, and provide evidence for the flexible and task-oriented representations arising in the network.","pdf":"/pdf/1f98821d6ea04dd61ea75f85fbfde986f2b0187f.pdf","TL;DR":"A generic framework for handling transfer and multi-task learning using pairs of autoencoders with task-specific and shared weights.","paperhash":"anonymous|joint_autoencoders_a_flexible_metalearning_framework","_bibtex":"@article{\n  anonymous2018joint,\n  title={Joint autoencoders: a flexible meta-learning framework},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1tWRJ-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper534/Authors"],"keywords":["transfer learning","domain adaptation","unsupervised learning","autoencoders","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739250116,"tcdate":1509125633207,"number":534,"cdate":1509739247457,"id":"S1tWRJ-R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1tWRJ-R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Joint autoencoders: a flexible meta-learning framework","abstract":"The incorporation of prior knowledge into learning is essential in achieving good performance based on small noisy samples. Such knowledge is often incorporated through the availability of related data arising from domains and tasks similar to the one of current interest. Ideally one would like to allow both the data for the current task and for previous related tasks to self-organize the learning system in such a way that commonalities and differences between the tasks are learned in a data-driven fashion. We develop a framework for learning multiple tasks simultaneously, based on sharing features that are common to all tasks, achieved through the use of a modular deep feedforward neural network consisting of shared branches, dealing with the common features of all tasks, and private branches, learning the specific unique aspects of each task. Once an appropriate weight sharing architecture has been established, learning takes place through standard algorithms for feedforward networks, e.g., stochastic gradient descent and its variations. The method deals with meta-learning (such as domain adaptation, transfer and multi-task learning) in a unified fashion, and can easily deal with data arising from different types of sources. Numerical experiments demonstrate the effectiveness of learning in domain adaptation and transfer learning setups, and provide evidence for the flexible and task-oriented representations arising in the network.","pdf":"/pdf/1f98821d6ea04dd61ea75f85fbfde986f2b0187f.pdf","TL;DR":"A generic framework for handling transfer and multi-task learning using pairs of autoencoders with task-specific and shared weights.","paperhash":"anonymous|joint_autoencoders_a_flexible_metalearning_framework","_bibtex":"@article{\n  anonymous2018joint,\n  title={Joint autoencoders: a flexible meta-learning framework},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1tWRJ-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper534/Authors"],"keywords":["transfer learning","domain adaptation","unsupervised learning","autoencoders","multi-task learning"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}