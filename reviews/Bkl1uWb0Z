{"notes":[{"tddate":null,"ddate":null,"tmdate":1511914191288,"tcdate":1511914191288,"number":1,"cdate":1511914191288,"id":"HJwRq_ilz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Public_Comment","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Neat!","comment":"Do you have any examples of the structures learned with hard attention beside the tricky-to-read example in Figure 4?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/812c3c500d32b622a1c9c4abc4e5a6a87c9b361e.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1512222720716,"tcdate":1511828775829,"number":3,"cdate":1511828775829,"id":"rylV679xz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Inducing grammars with and for NMT","rating":"5: Marginally below acceptance threshold","review":"This paper induces latent dependency syntax in the source side for NMT. Experiments are made in En-De and En-Ru.\n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017). In light of this, I see very little novelty in this paper. The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word. Seems thin for a ICLR paper.\n\nCaption of Fig 1: \"subject/object\" are syntactic functions, not semantic roles.\n\nI don't see how the German verb \"orders\" inflects with gender... Can you post the gold German sentence?\n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.\n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.\n\nWhy is hard attention (sec 3.3) necessary? It's not differentiable and requires sampling for training. This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).\n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing.\n\nIn table 3, it would be very helpful to display the English source.\n\nTable 4 is confusing. The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?\n\nComparison with predicted parses by Spacy are by no means \"gold\" parses...\n\nMinor comments:\n- Sec 1: \"... optimization techniques like Adam, Attention, ...\" -> Attention is not an optimization technique, but part of a model\n- Sec 1: \"abilities not its representation\" -> comma before \"not\"\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/812c3c500d32b622a1c9c4abc4e5a6a87c9b361e.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1512222720764,"tcdate":1511807624162,"number":2,"cdate":1511807624162,"id":"Hyg5qAtgf","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Worthwhile ideas give useful insight into latent graph structures for NMT; somewhat weak results.","rating":"6: Marginally above acceptance threshold","review":"This paper describes a method to induce source-side dependency structures in service to neural machine translation. The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited). This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word’s soft head (called Shared Attention in the paper); and (3) they gate when attention is used. I feel that the first two ideas are particularly interesting. Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU. However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all. Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system’s latent dependency decisions.\n\nThis paper has some very good ideas, and asks questions that are very much worth asking. In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile. Unfortunately, this is mostly a negative result, with gains over “flat attention” being relatively small. I also like the “Shared Attention” - it makes a lot of sense to say that if the “semantic” attention mechanism has picked a particular word, one should also attend to that word’s head; it is not something I would have thought of on my own. The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow.\n\nIn terms of specific criticisms:\n\nI found the motivation section to be somewhat weak. We need a better reason than morphology to want to do source-side dependency parsing. All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems.\n\nThe paper mentions “significant improvements” in only two places: the introduction and the conclusion. With BLEU score differences being so low, the authors should specify how statistical significance is measured; ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011).\nEquation (3): I couldn’t find the definition for H anywhere.\n\nSentence before Equation (5): I believe there is a typo here, “f takes z_i” should be “f takes u_t”.\n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence.\n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}. If I’m correct, that’s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = …\n\nI don’t don’t feel like the clause between equations (17) and (18), “when sharing attention weights from the decoder with the encoder” is a good description of your clever “shared attention” idea. In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow.\n\nSection 4.4: It’s very very good that you compared to “flat attention”, but it’s too bad for everyone cheering for linguistically-informed syntax that the results weren’t better.\n\nTable 5: I had a hard time understanding Table 5 and the corresponding discussion. What are “production percentages”?\n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/812c3c500d32b622a1c9c4abc4e5a6a87c9b361e.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1512222720809,"tcdate":1511404431694,"number":1,"cdate":1511404431694,"id":"SyOq7nXxf","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official review","rating":"3: Clear rejection","review":"This paper adds source side dependency syntax trees to an NMT model without explicit supervision. Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results.\n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads. Please define what semantic attention is. You just introduce this concept without any explanation. I believe you mean standard attention, if so, please explain why standard attention is semantic.\n\nClarity. What is shared attention exactly? Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more. Also the example in Figure 3 is not very clear and did not help me in understanding this concept.\n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure. Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms. The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations). It looks as if the improvement comes from adding additional capacity to the model. \n\nEquation 3: please define H.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/812c3c500d32b622a1c9c4abc4e5a6a87c9b361e.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1509739157173,"tcdate":1509132247949,"number":691,"cdate":1509739154518,"id":"Bkl1uWb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/812c3c500d32b622a1c9c4abc4e5a6a87c9b361e.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}