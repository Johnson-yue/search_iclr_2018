{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222739909,"tcdate":1511959047312,"number":3,"cdate":1511959047312,"id":"B1yz5XhgM","invitation":"ICLR.cc/2018/Conference/-/Paper742/Official_Review","forum":"rk6H0ZbRb","replyto":"rk6H0ZbRb","signatures":["ICLR.cc/2018/Conference/Paper742/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting experiments, wrong conclusions.","rating":"3: Clear rejection","review":"This work presents an empirical study aiming at improving the understanding of the vulnerability of neural networks to adversarial examples. Paraphrasing the authors, the main observation of the study is that the vulnerability is due to an inherent uncertainty that neural networks have about their predictions ( the difference between the logits). This is consistent across architectures, datasets. Further, the authors note that \"the universality is not a result of the specific content of these datasets nor the ability of the model to generalize.\"\n\nWhile this empirical study contains valuable information, its above conclusions are factually wrong. It can be theoretically proven at least using two routes. They are also in contradiction with other empirical observations consistent across several previous studies. \n\n1-Constructive counter-argument: Consider a neural network that always outputs a constant prediction. It (1) is by definition independent of any dataset (2) generalizes perfectly (3) has zero adversarial error, hence contradicting the central statement of the paper. \n\n2- Analysis-based counter-argument: Consider a neural network with one hidden layer and two classes. It is easy to show that the difference between the scores (logits) of the two classes is linear in the operator norm of the hidden weight matrix and linear in the L2-norm of the last weight vector. Therefore, the robustness of the model indeed depends on its capability to generalize because the latter is essentially governed by the geometric margin of the linear separator and the spectral norm of the weight matrix (see [1,2,3]). QED.\n\n3- Further, the lack of calibration of neural networks and its causes are well known. Among other things, it is due to the use of building blocks (such as batch-norm [4]), regularization (e.g., weight decay) or the use of softmax+cross-entropy during training. While this is convenient for optimization reasons, it indeed hurts the calibration. The authors should try to train a neural network with a large margin criteria and see if the same phenomenon still holds when they measure the geometric margin. Another alternative is to use a temperature with the softmax[4]. Therefore, the observations of the empirical study cannot be generalized to neural networks and should be explicitly restricted to neural networks using softmax with cross-entropy as criteria. \n\nI believe the conclusions of this study are misleading, hence I recommend to reject the paper. \n\n\n[1] Spectrally Normalized Margin-bounds Margin bounds for neural networks (Bartlett et al., 2017)\n[2] Parseval Networks: Improving Robustness to Adversarial Examples (Cisse et al., 2017) \n[3] Formal Guarantees on the Robustness of a classifier against adversarial examples (Hein et al., 2017)\n[4] On the Calibration of Modern Neural Networks (Guo et al., 2017)","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1512222739952,"tcdate":1511710171235,"number":2,"cdate":1511710171235,"id":"B17JC8dlf","invitation":"ICLR.cc/2018/Conference/-/Paper742/Official_Review","forum":"rk6H0ZbRb","replyto":"rk6H0ZbRb","signatures":["ICLR.cc/2018/Conference/Paper742/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"Very intriguing paper and results to say the least. I like the way it is written, and the neat interpretations that the authors give of what is going on (instead of assuming that readers will see the same). There is a well presented story of experiments to follow which gives us insight into the problem. \n\nInteresting insight into defensive distillation and the effects of uncertainty in neural networks.\n\nQuality/Clarity: well written and was easy for me to read\nOriginality: Brings both new ideas and unexpected experimental results.\nSignificance: Creates more questions than it answers, which imo is a positive as this topic definitely deserves more research.\n\nRemarks:\n- Maybe re-render Figure 3 at a higher resolution?\n- The caption of Figure 5 doesn't match the labels in the figure's legend, and also has a weird wording, making it unclear what (a) and (b) refer to.\n- In section 4 you say you test your models with FGSM accuracy, but in Figure 7 you report stepll and PGD accuracy, could you also plot the same curves for FGSM?\n- In Figure 4, I'm not sure I understand the right-tail of the distributions. Does it mean that when Delta_ij is very large, epsilon can be very small and still cause an adversarial pertubation? If so does it mean that overconfidence in the extreme is also bad?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1512222740024,"tcdate":1511627291165,"number":1,"cdate":1511627291165,"id":"Bkm7cMvgf","invitation":"ICLR.cc/2018/Conference/-/Paper742/Official_Review","forum":"rk6H0ZbRb","replyto":"rk6H0ZbRb","signatures":["ICLR.cc/2018/Conference/Paper742/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Their explanation seems to be done by non-strict argument,  and their proposed methods do not seem related to their discovery so much.","rating":"5: Marginally below acceptance threshold","review":"This paper insists that adversarial error for small adversarial perturbation follows power low as a function of the perturbation size, and explains the cause by the logit-difference distributions using mean-field theory.\nThen, the authors propose two methods for improving adversarial robustness (entropy regularization and NAS with reinforcement learning).\n\n[strong points]\n* Based on experimental results over a broad range of datasets, deep network models and their attacks.\n* Discovery of the fact that adversarial error follows a power low as a function of the perturbation size epsilon for small epsilon.\n* They found entropy regularization improves adversarial robustness.\n* Their neural architecture search (NAS) with reinforcement learning found robust deep networks.\n\n[weak points]\n* Unclear derivation of Eq. (9). (What expansion is used in Eq. (21)?)\n* Non-strict argument using mean-field theory.\n* Unclear connection between their discovered universality and their proposals (entropy regularization and NAS with reinforcement learning).","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1510646895804,"tcdate":1510646837081,"number":1,"cdate":1510646837081,"id":"SJTNVQ_1M","invitation":"ICLR.cc/2018/Conference/-/Paper742/Official_Comment","forum":"rk6H0ZbRb","replyto":"H1mlAzf1f","signatures":["ICLR.cc/2018/Conference/Paper742/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper742/Authors"],"content":{"title":"Adversarial training did include fractional epsilon, and unit L2-norm attacks lead to larger pixel changes. ","comment":"Thanks for the positive comment and the interesting question. Kurakin et al. did use non-integer values of epsilon during training. As mentioned in their paper, epsilon was sampled from a truncated normal defined in [0,16]. \n\nRegarding test-time: as we show in Fig 2c for MNIST, attacks with unit L2 norm have the same power-law form and exponent as FGSM, but allow for much larger change in each pixel value. For ImageNet, unit L2 norm attack has the same power-law form and exponent up to an epsilon of 70; this means that one pixel could change by as large as 70 due to adversarial distortion and still be in the power-law region. We will include an additional plot about this in the next version of our submission.  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1510252010595,"tcdate":1510252010595,"number":1,"cdate":1510252010595,"id":"H1mlAzf1f","invitation":"ICLR.cc/2018/Conference/-/Paper742/Public_Comment","forum":"rk6H0ZbRb","replyto":"rk6H0ZbRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Robustness to very small perturbations","comment":"Very cool insights, I really enjoyed your paper. I had a question about your experiments with FGSM attacks for small epsilon (Figure 1 & 2). What is the rationale for considering non-integer values of epsilon here (especially epsilon < 1), since the resulting perturbed inputs do not actually represent valid RGB images? As I understand it, simply converting the image to a valid RGB representation would remove any perturbation with epsilon < 0.5.\nWhile it is interesting that adversarially trained models did not learn to be robust in that regime, is that really surprising given that Kurakin et al. (2016) seem to only consider integer values of epsilon in their paper?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1509739128996,"tcdate":1509133892775,"number":742,"cdate":1509739126329,"id":"rk6H0ZbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk6H0ZbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Intriguing Properties of Adversarial Examples","abstract":"It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n","pdf":"/pdf/cb3e98d0662b7fbe30748abdb04dde8ce2f6d1db.pdf","TL;DR":"Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.","paperhash":"anonymous|intriguing_properties_of_adversarial_examples","_bibtex":"@article{\n  anonymous2018intriguing,\n  title={Intriguing Properties of Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6H0ZbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper742/Authors"],"keywords":["adversarial examples","universality","neural architecture search"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}