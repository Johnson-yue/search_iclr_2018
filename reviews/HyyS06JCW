{"notes":[{"ddate":null,"tddate":1511972343713,"tmdate":1512222586109,"tcdate":1511972340300,"number":3,"cdate":1511972340300,"id":"rJnlC8hlf","invitation":"ICLR.cc/2018/Conference/-/Paper183/Official_Review","forum":"HyyS06JCW","replyto":"HyyS06JCW","signatures":["ICLR.cc/2018/Conference/Paper183/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea but experiments not convincing enough","rating":"5: Marginally below acceptance threshold","review":"This paper proposes interactive boosting to train two complimentary networks for relative small image classification. This is an interesting idea for small sample training, although I am not sure whether it can be applied to \"really\"  small sample datasets since UIUC or LabelMe contains at least thousands of images.  \n\nThe experimental results are not very convincing, and I wish the final paper could compare with more baselines, including\n1. adapting pretrained model (e.g., from ImageNet) to the proposed problem\n2. deep CNN with dropout \n3. deep CNN with distillation\n4. deep CNN with data augmentation\nwhich all are popular techniques to apply deep network for small sample problems. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Boosting of Neural Networks for Small-sample Image Classification","abstract":"Neural networks have recently shown excellent performance on numerous classification tasks. These networks often have a large number of parameters and thus require much data to train. When the number of training data points is small, however, a network with high flexibility will quickly overfit the training data, resulting in a large model variance and a poor generalization performance. To address this problem, we propose a new ensemble learning method called InterBoost for small-sample image classification. In the training phase, InterBoost first randomly generates two complementary datasets to train two base networks of the same structure, separately, and then next two complementary datasets for further training the networks are generated through interaction (or information sharing) between the two base networks trained previously. This interactive training process continues iteratively until a stop criterion is met. In the testing phase, the outputs of the two networks are combined to obtain one final score for classification. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets demonstrate that the proposed ensemble method outperforms existing ones. Moreover, the confusion matrices of the two base networks trained by our method are shown to be complementary. Detailed analysis of the method is provided for an in-depth understanding of its mechanism.","pdf":"/pdf/0bdc5b9e4be0dbfeaa1e9d4b0e94a952ee54960a.pdf","TL;DR":"In the paper, we proposed an ensemble method called InterBoost for training neural networks for small-sample classification. The method has better generalization performance than other ensemble methods, and reduces variances significantly.","paperhash":"anonymous|interactive_boosting_of_neural_networks_for_smallsample_image_classification","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Boosting of Neural Networks for Small-sample Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyS06JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper183/Authors"],"keywords":["ensemble learning","neural network","small-sample","overfitting","variance"]}},{"tddate":null,"ddate":null,"tmdate":1512222586153,"tcdate":1511450470803,"number":2,"cdate":1511450470803,"id":"BJJOPw4xf","invitation":"ICLR.cc/2018/Conference/-/Paper183/Official_Review","forum":"HyyS06JCW","replyto":"HyyS06JCW","signatures":["ICLR.cc/2018/Conference/Paper183/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A method for building complementary networks - interesting, but many foundational questions haven't been addreesed","rating":"5: Marginally below acceptance threshold","review":"This paper proposes an iterative method to computing complementary weights between two networks for small datasets.\n\nAll in all, there might be something interesting in this topic, but since there's no theoretical analysis and background in the paper, it's not clear why this method should work or be better than others.\n\nComments:\n- On the theoretical front, I'm not sure what is it that is being optimised by the iterative complementary reweighting updates. What is the loss function? What would the best two models? \n- Why is the reweighting only reliant on the posteriors rather than on mistakes (i.e. labels are not being used at all in the computation of sample weights, only the posterior)? \n- Why not to ensemble all the models up the end for the prediction (i.e. what is so special about the last models that you want to only keep them - this relates to the question about what is it that is being optimised)?\n- Since the application is computer vision here, the question is that why not to use/compare to CNNs for this task?\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Boosting of Neural Networks for Small-sample Image Classification","abstract":"Neural networks have recently shown excellent performance on numerous classification tasks. These networks often have a large number of parameters and thus require much data to train. When the number of training data points is small, however, a network with high flexibility will quickly overfit the training data, resulting in a large model variance and a poor generalization performance. To address this problem, we propose a new ensemble learning method called InterBoost for small-sample image classification. In the training phase, InterBoost first randomly generates two complementary datasets to train two base networks of the same structure, separately, and then next two complementary datasets for further training the networks are generated through interaction (or information sharing) between the two base networks trained previously. This interactive training process continues iteratively until a stop criterion is met. In the testing phase, the outputs of the two networks are combined to obtain one final score for classification. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets demonstrate that the proposed ensemble method outperforms existing ones. Moreover, the confusion matrices of the two base networks trained by our method are shown to be complementary. Detailed analysis of the method is provided for an in-depth understanding of its mechanism.","pdf":"/pdf/0bdc5b9e4be0dbfeaa1e9d4b0e94a952ee54960a.pdf","TL;DR":"In the paper, we proposed an ensemble method called InterBoost for training neural networks for small-sample classification. The method has better generalization performance than other ensemble methods, and reduces variances significantly.","paperhash":"anonymous|interactive_boosting_of_neural_networks_for_smallsample_image_classification","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Boosting of Neural Networks for Small-sample Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyS06JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper183/Authors"],"keywords":["ensemble learning","neural network","small-sample","overfitting","variance"]}},{"tddate":null,"ddate":null,"tmdate":1512222586204,"tcdate":1511124894695,"number":1,"cdate":1511124894695,"id":"S1Po1dyeM","invitation":"ICLR.cc/2018/Conference/-/Paper183/Official_Review","forum":"HyyS06JCW","replyto":"HyyS06JCW","signatures":["ICLR.cc/2018/Conference/Paper183/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The proposed method is quite a lot like mixtures of experts, and this needs to be addressed","rating":"4: Ok but not good enough - rejection","review":"The manuscript aims to improve the performance of neural networks on small datasets, by using an ensemble approach to reduce the variance.  This is a worthy goal, and the overall approach of using an ensemble method makes sense, based on the variance-reducing properties of bagging.  Postive results are reported on two datasets, indicating that the method may work as intended.  The experiments varying the size of the training set are also useful for exploring the authors' hypothesis.  It is appreciated that the authors made their code available.\n\nHowever, while the proposed ideas are interesting, there are concerns about the novelty in relation to a long line of previous work.  The approach assigns weights that sum to one for each instance, where the weights associate that instance with different models, and these weights are updated via an iterative procedure.  Thus, each model is allowed to specialize on a subset of the data, according to the weights.  This should immediately bring to mind the notion of a mixture model, trained via EM.  Indeed, the mixture of experts model of Jacobs et al., dating back to 1991, does exactly this in a supervised learning context, as is studied here.  This type of model has been extensively studied for well over two decades:\n\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.\nYuksel, S. E., Wilson, J. N., & Gader, P. D. (2012). Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems, 23(8), 1177-1193.\n\nI am unsure of whether the proposed updates, which are suggested heuristically, will exactly correspond to the EM algorithm for mixtures of experts (MoE).  Regardless, they are very closely related, and this needs to be addressed in the manuscript.  If they are different, the paper must demonstrate that the approach has superior performance to MoE.  If they are not, the novelty of the paper is greatly reduced, although this may help to justify the proposed approach, and could lead to new analyses.  Mixtures of experts have a model-based theoretical basis, while the present work is heuristically motivated.\n\nOnly two datasets are used in the experiments, which leaves questions on whether the results will generalize to other datasets.  Results on at least one more dataset would greatly strengthen confidence in the results.\n\nTo summarize, while the manuscript has some interesting ideas, the relationship to a long line of previous work, the mixture of experts model, is not properly addressed, which means that the paper cannot presently be accepted in its current form.  The experimental results need to be extended, as comparisons to mixtures of experts are important to include (even if the proposed approach ends up being different to that work), and only two datasets are used.  The lack of theoretical justification of the proposed approach becomes more of a concern in light of the fact that its primary competitor, mixtures of experts, has a model-based justification.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Boosting of Neural Networks for Small-sample Image Classification","abstract":"Neural networks have recently shown excellent performance on numerous classification tasks. These networks often have a large number of parameters and thus require much data to train. When the number of training data points is small, however, a network with high flexibility will quickly overfit the training data, resulting in a large model variance and a poor generalization performance. To address this problem, we propose a new ensemble learning method called InterBoost for small-sample image classification. In the training phase, InterBoost first randomly generates two complementary datasets to train two base networks of the same structure, separately, and then next two complementary datasets for further training the networks are generated through interaction (or information sharing) between the two base networks trained previously. This interactive training process continues iteratively until a stop criterion is met. In the testing phase, the outputs of the two networks are combined to obtain one final score for classification. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets demonstrate that the proposed ensemble method outperforms existing ones. Moreover, the confusion matrices of the two base networks trained by our method are shown to be complementary. Detailed analysis of the method is provided for an in-depth understanding of its mechanism.","pdf":"/pdf/0bdc5b9e4be0dbfeaa1e9d4b0e94a952ee54960a.pdf","TL;DR":"In the paper, we proposed an ensemble method called InterBoost for training neural networks for small-sample classification. The method has better generalization performance than other ensemble methods, and reduces variances significantly.","paperhash":"anonymous|interactive_boosting_of_neural_networks_for_smallsample_image_classification","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Boosting of Neural Networks for Small-sample Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyS06JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper183/Authors"],"keywords":["ensemble learning","neural network","small-sample","overfitting","variance"]}},{"tddate":null,"ddate":null,"tmdate":1509739440262,"tcdate":1509051959014,"number":183,"cdate":1509739437605,"id":"HyyS06JCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyyS06JCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Interactive Boosting of Neural Networks for Small-sample Image Classification","abstract":"Neural networks have recently shown excellent performance on numerous classification tasks. These networks often have a large number of parameters and thus require much data to train. When the number of training data points is small, however, a network with high flexibility will quickly overfit the training data, resulting in a large model variance and a poor generalization performance. To address this problem, we propose a new ensemble learning method called InterBoost for small-sample image classification. In the training phase, InterBoost first randomly generates two complementary datasets to train two base networks of the same structure, separately, and then next two complementary datasets for further training the networks are generated through interaction (or information sharing) between the two base networks trained previously. This interactive training process continues iteratively until a stop criterion is met. In the testing phase, the outputs of the two networks are combined to obtain one final score for classification. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets demonstrate that the proposed ensemble method outperforms existing ones. Moreover, the confusion matrices of the two base networks trained by our method are shown to be complementary. Detailed analysis of the method is provided for an in-depth understanding of its mechanism.","pdf":"/pdf/0bdc5b9e4be0dbfeaa1e9d4b0e94a952ee54960a.pdf","TL;DR":"In the paper, we proposed an ensemble method called InterBoost for training neural networks for small-sample classification. The method has better generalization performance than other ensemble methods, and reduces variances significantly.","paperhash":"anonymous|interactive_boosting_of_neural_networks_for_smallsample_image_classification","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Boosting of Neural Networks for Small-sample Image Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyS06JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper183/Authors"],"keywords":["ensemble learning","neural network","small-sample","overfitting","variance"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}