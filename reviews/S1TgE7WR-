{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222560830,"tcdate":1511823528186,"number":3,"cdate":1511823528186,"id":"BJxhdf9lf","invitation":"ICLR.cc/2018/Conference/-/Paper1159/Official_Review","forum":"S1TgE7WR-","replyto":"S1TgE7WR-","signatures":["ICLR.cc/2018/Conference/Paper1159/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice technical contribution with some presentation issues","rating":"6: Marginally above acceptance threshold","review":"The paper introduces a formalism to perform graph classification and regression, so-called \"covariant compositional networks\", which can be seen as a generalization of the recently proposed neural message passing algorithms.\n\nThe authors argue that neural message passing algorithms are not able to sufficiently capture the structure of the graph since their neighborhood aggregation function is permutation invariant. They argue that relying on permutation  invariance will led to some loss of structural information.\n\nIn order to address this issue they introduce covariant comp-nets, which are a hierarchical decompositon of the set of vertices, and propose corresponding aggregation rules based on tensor arithmetic.\n\nTheir new method is evaluated on several graph regression and classification benchmark data sets, showing that it improves the state-of-the-art on a subset of them.\n\nStrong points:\n+ New method that generalizes existing methods\n\nWeak Points:\n- Paper should be made more accessible, especially pages 10-11\n- Should include more data sets for graph classification experiments, e.g., larger data sets such as REDDIT-*\n- Paper does not include proofs, should be included in the appendix\n- Review of literature could be extended\n\nSome Remarks:\n* Section 1: The reference Feragen et al., 2013 is not adequate for kernels based on walks.\n* Section 3 is rather lengthy. I wonder if its contents are really needed in the following.\n* Section 6.5, 2nd paragraph: The sentence is difficult to understand. Moreover, the reference (Kriege et al., 2016) appears not to be adequate: The vectors obtained from one-hot encodings are summed and concatenated, which is different from the approach cited. This step should be clarified.\n* unify first names in references (Marion Neumann vs. R. Kondor)\n* P. 5 (bottom) broken reference","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Covariant Compositional Networks For Learning Graphs","abstract":"Most existing neural networks for learning graphs deal with the issue of permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on some standard graph learning benchmarks. ","pdf":"/pdf/7673e6cf0b07d195633b82c9905e205759f686e9.pdf","TL;DR":"A general framework for creating covariant graph neural networks","paperhash":"anonymous|covariant_compositional_networks_for_learning_graphs","_bibtex":"@article{\n  anonymous2018covariant,\n  title={Covariant Compositional Networks For Learning Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1TgE7WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1159/Authors"],"keywords":["graph neural networks","message passing","label propagation","equivariant representation"]}},{"ddate":null,"tddate":1511799710696,"tmdate":1512222560867,"tcdate":1511792729705,"number":2,"cdate":1511792729705,"id":"HkfwgoYef","invitation":"ICLR.cc/2018/Conference/-/Paper1159/Official_Review","forum":"S1TgE7WR-","replyto":"S1TgE7WR-","signatures":["ICLR.cc/2018/Conference/Paper1159/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN).","rating":"5: Marginally below acceptance threshold","review":"The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN). Although, the paper is well structured and quite well written, its dense information    and its long size made it hard to follow in depth. Some parts of the paper should have been moved to appendix. As far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to SoA methods, but it is not really clear whether the out-performance is of statistical significance. Moreover,  in Table 1, training performances shouldn't be shown, while in Table 3, RMSE it would be nice to be shown in order to gain a complete image of the actual performance.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Covariant Compositional Networks For Learning Graphs","abstract":"Most existing neural networks for learning graphs deal with the issue of permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on some standard graph learning benchmarks. ","pdf":"/pdf/7673e6cf0b07d195633b82c9905e205759f686e9.pdf","TL;DR":"A general framework for creating covariant graph neural networks","paperhash":"anonymous|covariant_compositional_networks_for_learning_graphs","_bibtex":"@article{\n  anonymous2018covariant,\n  title={Covariant Compositional Networks For Learning Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1TgE7WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1159/Authors"],"keywords":["graph neural networks","message passing","label propagation","equivariant representation"]}},{"tddate":null,"ddate":null,"tmdate":1512222560909,"tcdate":1511792442174,"number":1,"cdate":1511792442174,"id":"S1MHyoFgf","invitation":"ICLR.cc/2018/Conference/-/Paper1159/Official_Review","forum":"S1TgE7WR-","replyto":"S1TgE7WR-","signatures":["ICLR.cc/2018/Conference/Paper1159/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Verbose paper that lacks lots of details on experiments","rating":"4: Ok but not good enough - rejection","review":"Thank you for your contribution to ICLR. The paper covers a very interesting topic and presents some though-provoking ideas. \n\nThe paper introduces \"covariant compositional networks\" with the purpose of learning graph representations. An example application also covered in the experimental section is graph classification. \nGiven a finite set S, a compositional network is simply a partially ordered set P where each element of P is a subset of S and where P contains all sets of cardinality 1 and the set S itself. Unfortunately, the presentation of the approach is extremely verbose and introduces old concepts (e.g., partially ordered set) under new names.  The basic idea (which is not new) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations. Moreover, the hierarchical order of the nodes should be invariant to valid permutations of the nodes, that is, two isomorphic graphs should have the same hierarchical order on their nodes and the same feature representations. Since this is the case for graph embedding methods that collect feature representations from their neighbors in the graph (and where the feature aggregation functions are symmetric) it makes sense that \"compositional networks\" generalize graph convolutional networks (and the more general message passing neural networks framework). \n\nThe most challenging problem, however, namely the problem of finding a concrete and suitable permutation invariant hierarchical decomposition of the nodes plus some aggregation/pooling functions to compute the feature representations is not addressed in sufficient detail. The paper spends a lot of time on some theoretical definitions and (trivial) proofs but then fails to make the connection to an approach that works in practice. The description of the experiments and which compositional network is chosen and how it is chosen seems to be missing. The only part hinting at the model that was actually used in the experiments is the second paragraph of the section 'Experimental Setup', consisting of one long sentence that is incomprehensible to me. \n\nInstead of spending a lot of effort on the definitions and (somewhat trivial) propositions in the first half of the paper, the authors should spend much more time on detailing the experiments and the actual model that they used. In an effort to make the framework as general as possible, you ended up making the paper highly verbose and difficult to follow. \n\nPlease address the following points or clarify in your rebuttal if I misunderstood something:\n\n- what precisely is the novel contribution of your work (it cannot be \"compositional networks\" and the propositions concerning those because these are just old concepts under new names)?\n- explain precisely (and/or more directly/less convoluted) how your model used in the experiments looks like; why do you think it is better than the other methods?\n- given that compositional network is a very general concept (partially ordered set imposed on subsets of the graph vertices), what is the principled set of steps one has to follow to arrive at such a compositional network tailored to a particular graph collection? isn't (or shouldn't) that be the contribution of this work? Am I missing something?\n\nIn general, you should write the paper much more to the point and leave out unnecessary math (or move to an appendix).  The paper is currently highly inaccessible.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Covariant Compositional Networks For Learning Graphs","abstract":"Most existing neural networks for learning graphs deal with the issue of permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on some standard graph learning benchmarks. ","pdf":"/pdf/7673e6cf0b07d195633b82c9905e205759f686e9.pdf","TL;DR":"A general framework for creating covariant graph neural networks","paperhash":"anonymous|covariant_compositional_networks_for_learning_graphs","_bibtex":"@article{\n  anonymous2018covariant,\n  title={Covariant Compositional Networks For Learning Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1TgE7WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1159/Authors"],"keywords":["graph neural networks","message passing","label propagation","equivariant representation"]}},{"tddate":null,"ddate":null,"tmdate":1510092379480,"tcdate":1509139445089,"number":1159,"cdate":1510092359343,"id":"S1TgE7WR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1TgE7WR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Covariant Compositional Networks For Learning Graphs","abstract":"Most existing neural networks for learning graphs deal with the issue of permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on some standard graph learning benchmarks. ","pdf":"/pdf/7673e6cf0b07d195633b82c9905e205759f686e9.pdf","TL;DR":"A general framework for creating covariant graph neural networks","paperhash":"anonymous|covariant_compositional_networks_for_learning_graphs","_bibtex":"@article{\n  anonymous2018covariant,\n  title={Covariant Compositional Networks For Learning Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1TgE7WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1159/Authors"],"keywords":["graph neural networks","message passing","label propagation","equivariant representation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}