{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222744210,"tcdate":1511819312571,"number":3,"cdate":1511819312571,"id":"B1FEuWcez","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review of \"Densely connected recurrent neural network for sequence-to-sequence learning\"","rating":"4: Ok but not good enough - rejection","review":"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se). Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs. The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.  \n\nOverall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak. The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?) for which the reason could be a multitude of issues probably related to hyper-parameter tuning. What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation? Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2. Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of $12M$ training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/721d5663659c1a17d97e5c61c8550d7ad51aadb8.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222744248,"tcdate":1511783267394,"number":2,"cdate":1511783267394,"id":"B1owoOFlz","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer3"],"readers":["everyone"],"content":{"title":"incremental to prior work","rating":"6: Marginally above acceptance threshold","review":"This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer. In addition, attention context is computed for each layer, then, combined together as a single context. Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters.\n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable. However I have some concerns to this paper.\n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from. I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms.\n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers. Do you have experiments by fixing the number of layers and varying the hidden size?\n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of $12M$ training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/721d5663659c1a17d97e5c61c8550d7ad51aadb8.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222744288,"tcdate":1511719564566,"number":1,"cdate":1511719564566,"id":"Bkr5fKdlf","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Dense skip-connections between layers improve recurrent networks","rating":"5: Marginally below acceptance threshold","review":"The article proposes to use dense skip-connections on the \"vertical\" (between-layers) connections of recurrent networks. Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to.\n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks.\n\nI am not convinced by the presented results for the following reasons:\n1) the paper introduces two concepts - the dense skip-connections and the multi-head attention. Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention?\n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines. What happens for deeper and narrower baselines that have a similar number of parameters?\n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \"jumps\" in the training curves as signs of learning rate anneal). Maybe this is what helps? I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated?\n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written.\n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial.\n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of $12M$ training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/721d5663659c1a17d97e5c61c8550d7ad51aadb8.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739120899,"tcdate":1509134192205,"number":756,"cdate":1509739118237,"id":"Hk__kGbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of $12M$ training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/721d5663659c1a17d97e5c61c8550d7ad51aadb8.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}