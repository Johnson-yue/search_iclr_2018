{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222623582,"tcdate":1511825604556,"number":3,"cdate":1511825604556,"id":"SyaalQ9lM","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Review","forum":"BJQPG5lR-","replyto":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference/Paper347/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea but suffers from lack of motivation and sound experiments ","rating":"4: Ok but not good enough - rejection","review":"Summary:\nThe contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections.\nThe paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied. It is proposed to optimize the formulation using the method of Lagrange multipliers.\nA toy model with a single unit is used to illustrate the basic ideas behind the method. Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets.\n\nQuality and significance:\nThe proposed methodology is simple and straightforward. The analysis with the toy network is interesting and helps illustrate the method. However, my main concerns with this paper are related to motivation and experiments.\n\nThe motivation of the work is not clear at all. The stated goal is to address some of the issues related to important and role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed. One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper.\n\nFor MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized. Here, the generalization error is measured instead, which is heavily influenced by regularization. Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer. The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results. \n\nFor the CIFAR experiments, the experiment design is reasonable for a general comparison. Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does make a rigorous comparison without employing expensive hyper-parameter searches. This is not the fault of the present paper but an unfortunate tradition in the field. Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences. For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do. Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization.\n\nClarity:\nThe paper is generally well-written and easy to read. There are some clarity issues related to the use of the term \"activation function\" and a typo in an equation but the authors are already aware of these.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1512222623619,"tcdate":1511734954593,"number":2,"cdate":1511734954593,"id":"HkX303_ez","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Review","forum":"BJQPG5lR-","replyto":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference/Paper347/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple idea, not explored thoroughly enough","rating":"3: Clear rejection","review":"The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.\n\nIt’s a nice, simple idea. However, I don’t think it’s sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren’t that good. \n\nComments:\n\n1. Why force the Lag. multipliers to 1 at the end of training? It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I’d be curious as to why.\n\n2.Similarly, it’s not obvious that initializing the multipliers at 0.5 is the best choice. The “looks linear” initialization proposed in “The shattered gradients problem” (Balduzzi et al) implies that alpha=0 may work better. Did the authors try any values besides 0.5? \n\n3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it’s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it? \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1512222623655,"tcdate":1511218749889,"number":1,"cdate":1511218749889,"id":"ByISCAelf","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Review","forum":"BJQPG5lR-","replyto":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference/Paper347/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Useful if somewhat marginal paper with issues.","rating":"3: Clear rejection","review":"In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them. \n\nI think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance, even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of \\lambda as well as the step size for updating \\lambda, and seems to have no significant advantage over just using skip connections throughout training. So my rating based on the message of the paper would be 6/10.\n\nHowever, there appear to be a range of issues. As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6.\n\n+++ Section 3.1 problems +++\n\n- I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs. In the toy example, you add (1 - \\alpha)wx whereas in the VANs you add (1 - \\alpha)x. Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model. While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs.\n- The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds. Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx). Most of the training takes place without the skip connection. In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error. It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training.\n- Regarding the optimization algorithm involving \\alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied. You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. However, this reduction grows to infinity the larger the update is. Does that mean that larger updates are always better? Clearly not. If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes? Hence, the motivation for the algorithm you present is invalid. Here is an example where this algorithm fails: consider the point (x,y,w,\\alpha,\\lambda) = (100, \\sigma(100), 1.0001, 1, 1). Here, w has almost converged to its optimum w* = 1. Correspondingly, the derivative of C is a small negative value. However, \\alpha* is actually 0, and this choice would catapult w far away from w*.\n\nIf I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues.\n\n+++ ResNet scaling +++\n\nThere is a crucial difference between VANs and ResNets. In the VAN initial state (alpha = 0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5. Because of this, the experimental results between the two architectures are incomparable.\n\nIn a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used. I disagree. Let's look at an example. Consider ResNet first. It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b. Now let's assume we insert a scaling constant after each residual block, say c = 0.5. Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B. Therefore, contributions of lower blocks vanish exponentially. This effect is not combated by batch normalization.\n\nSo the learning dynamics for VAN and ResNet are very different because of this scaling. Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling? Without this information, the experiments have limited value. In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections.\n\nIf my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: \n\n- ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l\n- VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}. Also, initialize \\alpha to zero. This ensures that VAN starts out as a regular ResNet.\n\n+++ writing issues +++\n\nTitle:\n\n- \"VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS\" This title can be read in two different ways. (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections. In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false. Since it is not clear from reading the title whether (A) or (B) is true, please reword it.\n\nAbstract:\n\n- \"Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.\" Again, this is ambiguous. To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections. However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence.\n- \"where the proposed method is shown to outperform many architectures without skip-connections\" Again, this sentence makes no sense to me. It seems to imply that VAN has skip connections. But in the abstract you defined VAN as an architecture without skip connections. Please make this more clear.\n\nIntroduction:\n- \"Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\" Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations. In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper.\n\nsection 3.1:\n- replace \"to to\" by \"to\" in the second line\n\nsection 4:\n- \"This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.\" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers. Therefore, it cannot contribute positively to the performance or ResNet.\n\nAs mentioned in earlier comments, please reword / clarify your use of \"activation function\". It is generally used a synonym for \"nonlinearity\", so please use it in this way. Change your claim that VAN is equivalent to PReLU. Please include your description of how your method can be extended to networks which do allow for skip connections.\n\n+++ Hyperparameters +++\n\nSince the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1510854401594,"tcdate":1510854401594,"number":4,"cdate":1510854401594,"id":"HyqZ1Io1z","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Comment","forum":"BJQPG5lR-","replyto":"HJaKBU9kG","signatures":["ICLR.cc/2018/Conference/Paper347/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper347/Authors"],"content":{"title":"Reply","comment":"Thank you for the further questions, please see our replies below. \n\n1) Relation to parametric ReLU\n\nWe agree that our language is ambiguous and will update the manuscript to avoid conflating 'activation function' with \\mathcal{F} (note that currently we are unable to upload a new version). \n\nRegarding the various interpretations of line 4 p4, the reviewers final interpretation is correct and we agree that there is only a correspondence between the proposed method and parametric ReLUs when W is the identity. Our intention was to highlight a similarity with the parametric ReLU. However, we note that our claims regarding the gradient still holds; the gradient with respect to \\alpha will be simple and computationally cheap. \n\n2) Extension to networks with skip-connections\n\nOne possible extension would be to enforce the constraint in equation (5) with an inequality where we constrain the total budget on \\alpha_l across all layers.  This would lead to skip-connections being removed across some layers and retained across others, providing insights into where in a network skip-connections are most important. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1510790533295,"tcdate":1510790533295,"number":3,"cdate":1510790533295,"id":"HJaKBU9kG","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Comment","forum":"BJQPG5lR-","replyto":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference/Paper347/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper347/AnonReviewer2"],"content":{"title":"More questions","comment":"Thank you for your responses so far. I need to ask a few more question before writing my final review.\n\n\"Throughout the experiments we will often take the nonlinearity in Fl to be ReLU in which case the activation function corresponds to the parametric ReLU, for which the term @C can be efficiently computed (He et al., 2015).\"\n\nFirstly, please try to avoid using 'activation function' to refer to \\mathcal{F} as it is generally considered to be a synonym for 'nonlinearity'. However, if you do use 'activation function' to refer to \\mathcal{F}, then the statement \"the activation function corresponds to the parametric ReLU\" makes no sense, because parametric ReLU is a nonlinearity. I take it \\mathcal{F} is not just composed of a nonlienarity, but also of convolutional and / or normalization layers.\n\nHowever, even if I take the statement \"the activation function corresponds to the parametric ReLU\" to mean \"\\mathcal{F} corresponds to a sequence of operations, the last of which is parametric ReLU\", it still makes no sense because the parameter alpha does not impact \\mathcal{F}. In equation (4), it is only applied after \\mathcal{F} is computed. So I take it you use 'activation function' to refer to function that computes x_{l+1} from x_l in this instance? Again, this is ambiguous.\n\nBut even if we take 'activation function' to mean 'the function that  computes x_{l+1} from x_l' and thus \"the activation function corresponds to the parametric ReLU\" to mean \"the function that  computes x_{l+1} from x_l  corresponds to a sequence of operations, the last of which is parametric ReLU\", it is still false if equation (4) is true. Let's assume that \\mathcal{F} = ReLU(Wx) for some W. Then we have x_{l+1} = (1-\\alpha)*x_l + \\alpha*ReLU(Wx_l) according to (4). But this is not the same as PReLU(Wx_l). In fact, we have PReLU(Wx_l) = (1-\\alpha)*Wx_l + \\alpha*ReLU(Wx_l).\n\nPlease clarify.\n\n\n\"However, the proposed method is quite general and can be readily extended to networks which do allow for skip-connections.\" \n\n\nHow?\n\n\n\n\nThanks,\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1510744491644,"tcdate":1510744491644,"number":2,"cdate":1510744491644,"id":"HJV2WiFyf","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Comment","forum":"BJQPG5lR-","replyto":"BJiI_YZkG","signatures":["ICLR.cc/2018/Conference/Paper347/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper347/Authors"],"content":{"title":"Reply","comment":"Dear Reviewer,\n\nThank you for your questions. We respond to each of them below.\n\n1) Use of Lagrange multipliers \n\nThe objective of the proposed method is to extend some of the benefits associated with skip-connections to the context of networks without skip-connections. We propose to do this by working in the framework of constrained optimisation. As such, the use of Lagrange multipliers is not an illustration but a tool we employ in order to solve equation (5), which is the true objective. Equation (6) is the associated Lagrangian, which combines the objective in equation (5) with a term enforcing the constraint. Our updates then seek a saddle point of this Lagrangian, by minimizing it with respect to the parameters W and \\alpha, and maximizing with respect to the Lagrange multipliers \\lambda.\n\n2) + and - signs in equations (7) and (8):\n\nAs stated above, the updates should minimize the Lagrangian with respect to \\alpha and W, while maximizing it with respect to \\lambda. We thank the reviewer for pointing out the mistake in equation (7). It should read as:\n \\alpha_l \\leftarrow  \\alpha_l - \\eta  ( \\frac{\\partial C}{ \\partial \\alpha_l } + \\ \\lambda_l  )\n\nHowever, with respect to equation (8), the plus sign is indeed correct. This is because we wish to maximise with respect to the Lagrange multipliers (in order to enforce the equality constraint with increasing severity). The signs in the equations in the section on the toy model are also correct; the reason they look different is because the constraint has been (equivalently) encoded as 1 - \\alpha there, rather than as \\alpha - 1 in equation (6). \n\n3) Initialization of \\lambda \n\nThroughout our experiments we initialized \\lambda to -1. Because we enforced that \\alpha remain in [0,1], all updates to \\lambda were <=0. These two facts ensured that \\lambda remained negative. An updated version of the manuscript will reflect this. However, as noted in the paragraph after equation (8), the nature of the \\lambda updates dictates that \\lambda will be monotonically decreasing. Thus even if we initialized \\lambda to be positive, subsequent updates would push it towards negative values.\n\n4) Relation between VAN at \\alpha=.5 and ResNets \n\nThe reviewer is correct in noting that when \\alpha=.5 there is a similarity between ResNets and VANs, but the activation is not exactly the same (as the VAN activation is scaled by 0.5). We will correct our language to reflect this. However, what we intended to highlight was that the balance between non-linearity and skip-connection was equal (as in a ResNet) whereas for arbitrary \\alpha, equation (4) defines a sum where the non-linearity and the skip-connection may not necessarily receive equal weights.  Further, when batch normalisation is employed the activation functions are effectively the same as the effects of scaling are removed. \n\n5) Initialization of \\alpha for CIFAR experiments \n\nWe initialized \\alpha=0.5 - see the final paragraph on page 6. This initialization was used throughout all experiments; we will make this clear in the updated manuscript.\n\n6) Activation function and non-linearity. \n\nWe agree with the reviewer that our language regarding activation function and non-linearity could be more clear. When we say \"activation function\" we refer to \\mathcal{F}. In the experiments section we make statements such as \"the ReLU activation function ...\" when we should say \"the ReLU non-linearity\". We will correct this in the updated version of the manuscript. \n\n\nThanks for you questions - we hope to have clarified them. If you have any more questions (or need further details) please let us know :) \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1510270008122,"tcdate":1510213714908,"number":1,"cdate":1510213714908,"id":"BJiI_YZkG","invitation":"ICLR.cc/2018/Conference/-/Paper347/Official_Comment","forum":"BJQPG5lR-","replyto":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference/Paper347/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper347/AnonReviewer2"],"content":{"title":"Lagrangian clarification","comment":"Dear authors, I am one of the reviewers and would like to ask a couple of question. In equation (6), you show the Lagrangian of the constrained problem. Is there a deeper reason why you do this? Or are you simply using the term in equation (6) as your objective function and the fact that it is the Lagrangian is merely an illustration? The reason I ask is that it's been a while since I took Optimization 101 :), meaning I would have to read up on the Lagrangian to remind myself of the associated theory. If you let me know that understanding Lagrangian optimization is necessary for the paper, I will go and do that. If not, I won't. \n\nAlso, there seems to be something wrong with + and - signs throughout section 3. If you take the derivative of (6) with respect to \\alpha and \\lambda, one does not get out (7) and (8). It seems that in (7) the second minus should be a plus and in (8), the first plus should be a minus. The exact same problem occurs at the bottom of page 4 for the toy example.\n\nAlso, to what value do you initialize \\lambda? For equation (6) to make sense, \\lambda has to be initialized to negative values so that the term that is added to \\mathcal{C} is positive, and it needs to be positive as it is a penalty ...\n\nAlso, you say \" In the case of VAN networks the α values were initialized to 0.5 · 1 for all layers. As such, during the initial stages of training VAN networks had the same activation function as ResNets.\" I don't understand this. In the initial state, the weight of the residual path and skip path for ResNet is both 1. But in VAN, the weight of the residual path and the skip path are both 0.5. Hence, as far as I can tell, they are *not* the same. Also, how did you initialize \\alpha for the CIFAR experiments?\n\nAlso, what do you mean by \"activation function\". Do you mean \\mathcal{F} or do you mean just the nonlinearity? You seem to be using \\mathcal{F} for both, which is confusing.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]}},{"tddate":null,"ddate":null,"tmdate":1509739351696,"tcdate":1509102171358,"number":347,"cdate":1509739349045,"id":"BJQPG5lR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJQPG5lR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections","abstract":"Novel architectures such as ResNets have enabled the training of very deep feed-forward networks via the introduction of skip-connections, leading to state-of-the-art results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.","pdf":"/pdf/ad97523bc21bf3a5e658bc0d2e60e6e8340b6060.pdf","TL;DR":"We introduce skip-connections penalized by Lagrange multipliers to train deep feed-forward networks. Skip-connections are thereby introduced during the early stages of training and iteratively phased out in a principled manner. ","paperhash":"anonymous|variable_activation_networks_a_simple_method_to_train_deep_feedforward_networks_without_skipconnections","_bibtex":"@article{\n  anonymous2018variable,\n  title={Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQPG5lR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper347/Authors"],"keywords":["optimization","vanishing gradients","shattered gradients","skip-connections"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}