{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222609552,"tcdate":1511819092594,"number":3,"cdate":1511819092594,"id":"B1aLPb5eM","invitation":"ICLR.cc/2018/Conference/-/Paper269/Official_Review","forum":"HktRlUlAZ","replyto":"HktRlUlAZ","signatures":["ICLR.cc/2018/Conference/Paper269/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very nice and potentially influential paper whose practical usefulness remains to be shown","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid. The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST. A weakness of the paper is that it does not attempt to solve a real-world problem. However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands.\n\nIssues:\n\n- The clutter in SIM2MNIST is so small that predicting the polar origin is essentially trivially solved by a low-pass filter. Although this criticism also applies to most previous work using ‘cluttered’ variants of MNIST, I still think it needs to be considered. What happens if predicting the polar origin is not trivial and prone to errors? These presumably lead to catastrophic failure of the post-transformer network, which is likely to be a problem in any real-world scenario.\n\n- I’m not sure if Section 5.5 strengthens the paper. Unlike the rest of the paper, it feels very ‘quick & dirty’ and not very principled. It doesn’t live up to the promise of rotation and scale equivariance in 3D. If I understand it correctly, it’s simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation. This means that the promise of rotation and scale equivariance holds up only along (x,y). I guess it’s not possible to build full 3D rotation/scale equivariance with the authors’ approach (spherical coordinates probably don’t do the job), but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in 3D. So instead of showing a quick ‘hack’, I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Polar Transformer Networks","abstract":"Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-\nof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.","pdf":"/pdf/e5777f12ebab00b924c5cfb8511cb80fb4d7e509.pdf","TL;DR":"We learn feature maps invariant to translation, and equivariant to rotation and scale.","paperhash":"anonymous|polar_transformer_networks","_bibtex":"@article{\n  anonymous2018polar,\n  title={Polar Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HktRlUlAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper269/Authors"],"keywords":["equivariance","invariance","canonical coordinates"]}},{"tddate":null,"ddate":null,"tmdate":1512222609599,"tcdate":1511725578178,"number":2,"cdate":1511725578178,"id":"rkMG9c_gf","invitation":"ICLR.cc/2018/Conference/-/Paper269/Official_Review","forum":"HktRlUlAZ","replyto":"HktRlUlAZ","signatures":["ICLR.cc/2018/Conference/Paper269/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good idea but not there yet","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling. The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed. The resulting log-polar image is analyzed by a conventional CNN.\n\nI find the basic idea quite compelling. Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina. Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it.\n\nOne fundamental limitation of the proposed approach is that although it is invariant to global translations, it does not have the built-in equivariance to local translations that a ConvNet has. Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance.\n\nThe use of the heatmap centroid as the prediction for the focal point is potentially problematic as well. It would not work if the heatmap is multimodal, e.g. when there are multiple instances in the same image or when there is a lot of clutter.\n\nThere is a minor conceptual confusion on page 4, where it is written that \"Group-convolution requires integrability over a group and identification of the appropriate measure dg. We ignore this detail as implementation requires application of the sum instead of integral.\"\nWhen approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored. Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one. (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this).\n\nIt does not make sense to say that \"The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations\", and then proceed to do exactly that (in canonical coordinates). Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible. The use of canonical coordinates is certainly a sensible choice (for the reason given above), but it does not make an infeasible computation feasible.\n\nThe authors may want to consider citing\n- Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi.\nThis paper also uses a log-polar transform, but lacks the focal point prediction / STN.\nLikewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability, it missed several recent works in this area:\n- Steerable CNNs, Cohen & Welling\n- Dynamic Steerable Blocks in Deep Residual Networks, Jacobsen et al.\n- Learning Steerable Filters for Rotation Equivariant CNNs, Weiler et al.\nThe last paper reports 0.71% error on MNIST-rot, which is slightly better than the PTN-CNN-B++ reported on in this paper.\n\nThe experimental results presented in this paper are quite good, but both MNIST and ModelNet40 seem like simple / toyish datasets. For reasons outlined above, I am not convinced that this approach in its current form would work very well on more complicated problems. If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication.\n\n\nMinor issues & typos\n- Section 3.1, psi_gh = psi_g psi_h. I suppose you use psi for L and L', but this is not very clear.\n- L_h f = f(h^{-1}), p. 4\n- \"coordiantes\", p. 5","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Polar Transformer Networks","abstract":"Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-\nof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.","pdf":"/pdf/e5777f12ebab00b924c5cfb8511cb80fb4d7e509.pdf","TL;DR":"We learn feature maps invariant to translation, and equivariant to rotation and scale.","paperhash":"anonymous|polar_transformer_networks","_bibtex":"@article{\n  anonymous2018polar,\n  title={Polar Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HktRlUlAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper269/Authors"],"keywords":["equivariance","invariance","canonical coordinates"]}},{"tddate":null,"ddate":null,"tmdate":1512222609650,"tcdate":1511714235398,"number":1,"cdate":1511714235398,"id":"r1XT6wdeG","invitation":"ICLR.cc/2018/Conference/-/Paper269/Official_Review","forum":"HktRlUlAZ","replyto":"HktRlUlAZ","signatures":["ICLR.cc/2018/Conference/Paper269/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach to equivariant CNNs","rating":"7: Good paper, accept","review":"This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The idea is to jointly train\n- a network predicting a polar origin,\n- a module transforming the image into a log-polar representation according to the predicted origin,\n- a final classifier performing the desired classification task.\nA (not too large) translation of the input image therefore does not change the log-polar representation.\nRotation and scale from the polar origin result in translation of the log-polar representation. As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image. Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge.\n\n(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs.\n\n(-) The evaluation, however, is quite limited. The approach is evaluated on:\n 1) several variants of MNIST. The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST. This variant is of course very well suited to the proposed method, and a bit artificial.\n 2) 3d voxel occupancy grids with a small resolution. The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation.\n\n(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale? Then the input image could be rectified to a canonical orientation and scale, without needing equivariance. My intuition is that this simpler approach would work better. It should at least be evaluated.\n\nDespite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Polar Transformer Networks","abstract":"Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-\nof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.","pdf":"/pdf/e5777f12ebab00b924c5cfb8511cb80fb4d7e509.pdf","TL;DR":"We learn feature maps invariant to translation, and equivariant to rotation and scale.","paperhash":"anonymous|polar_transformer_networks","_bibtex":"@article{\n  anonymous2018polar,\n  title={Polar Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HktRlUlAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper269/Authors"],"keywords":["equivariance","invariance","canonical coordinates"]}},{"tddate":null,"ddate":null,"tmdate":1509739394318,"tcdate":1509085393220,"number":269,"cdate":1509739391657,"id":"HktRlUlAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HktRlUlAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Polar Transformer Networks","abstract":"Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-\nof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.","pdf":"/pdf/e5777f12ebab00b924c5cfb8511cb80fb4d7e509.pdf","TL;DR":"We learn feature maps invariant to translation, and equivariant to rotation and scale.","paperhash":"anonymous|polar_transformer_networks","_bibtex":"@article{\n  anonymous2018polar,\n  title={Polar Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HktRlUlAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper269/Authors"],"keywords":["equivariance","invariance","canonical coordinates"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}