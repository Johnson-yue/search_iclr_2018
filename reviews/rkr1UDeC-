{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222615367,"tcdate":1512002454139,"number":3,"cdate":1512002454139,"id":"Bk09mAnlG","invitation":"ICLR.cc/2018/Conference/-/Paper291/Official_Review","forum":"rkr1UDeC-","replyto":"rkr1UDeC-","signatures":["ICLR.cc/2018/Conference/Paper291/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clear analysis, good motivation and sufficient verification","rating":"6: Marginally above acceptance threshold","review":"Although I am not an expert on this area, but this paper clearly explains their contribution and provides enough evidences to prove their results.\nOnline distillation technique is introduced to accelerate traditional algorithms for large-scale distributed NN training.\nCould the authors add more results on the CNN ?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large scale distributed neural network training through online distillation","abstract":"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","pdf":"/pdf/e06ba6a0a0b5789a8037fc28c578d88c028e2edf.pdf","TL;DR":"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines.","paperhash":"anonymous|large_scale_distributed_neural_network_training_through_online_distillation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large scale distributed neural network training through online distillation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkr1UDeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper291/Authors"],"keywords":["distillation","distributed training","neural networks","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222615418,"tcdate":1511802784077,"number":2,"cdate":1511802784077,"id":"SyOiDTtef","invitation":"ICLR.cc/2018/Conference/-/Paper291/Official_Review","forum":"rkr1UDeC-","replyto":"rkr1UDeC-","signatures":["ICLR.cc/2018/Conference/Paper291/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental algorithmic novelty and limited experiments","rating":"4: Ok but not good enough - rejection","review":"The paper proposes an online distillation method, called co-distillation, where the two different models are trained to match the predictions of other model in addition to minimizing its own loss. The proposed method is applied to two large-scale datasets and showed to perform better than other baselines such as label smoothing, and the standard ensemble. \n\nThe paper is clearly written and was easy to understand. My major concern is the significance and originality of the proposed method. As written by the authors, the main contribution of the paper is to apply the codistillation method, which is pretty similar to Zhang et. al (2017), at scale. But, because from Zhang's method, I don't see any significant difficulty in applying to large-scale problems, I'm not sure that this can be a significant contribution. Rather, I think, it would have been better for the authors to apply the proposed methods to a smaller scale problems as well in order to explore more various aspects of the proposed methods including the effects of number of different models. In this sense, it is also a limitation that the authors showing experiments where only two models are codistillated. Usually, ensemble becomes stronger as the number of model increases.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large scale distributed neural network training through online distillation","abstract":"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","pdf":"/pdf/e06ba6a0a0b5789a8037fc28c578d88c028e2edf.pdf","TL;DR":"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines.","paperhash":"anonymous|large_scale_distributed_neural_network_training_through_online_distillation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large scale distributed neural network training through online distillation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkr1UDeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper291/Authors"],"keywords":["distillation","distributed training","neural networks","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222615459,"tcdate":1511621210964,"number":1,"cdate":1511621210964,"id":"SJ7PzWDeM","invitation":"ICLR.cc/2018/Conference/-/Paper291/Official_Review","forum":"rkr1UDeC-","replyto":"rkr1UDeC-","signatures":["ICLR.cc/2018/Conference/Paper291/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising and interesting direction to scale distributed training","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper provides a very original & promising method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent. As authors point out, scaling distributed stochastic gradient descent to more workers typically requires larger batch sizes in order to fully utilize computational resource, and increasing the batch size has a diminishing return. This is clearly a very important problem, as it is a major blocker for current machine learning models to scale beyond the size of models and datasets we currently use. Authors propose to use distillation as a mechanism of communication between workers, which is attractive because prediction scores are more compact than model parameters, model-agnostic, and can be considered to be more robust to out-of-sync differences. This is a simple and sensible idea, and empirical experiments convincingly demonstrate the advantage of the method in large scale distributed training.\n\nI would encourage authors to experiment in broader settings, in order to demonstrate that the general applicability of the proposed method, and also to help readers better understand its limitations. Authors only provide a single positive data point; that co-distillation was useful in scaling up from 128 GPUs to 258 GPUs, for the particular language modeling problem (commoncrawl) which others have not previously studied. In order for other researchers who work on different problems and different system infrastructure to judge whether this method will be useful for them, however, they need to understand better when codistillation succeeds and when it fails. It will be more useful to provide experiments with smaller and (if possible) larger number of GPUs (16, 32, 64, and 512?, 1024?), so that we can more clearly understand how useful this method is under the regime mini-batch stochastic gradient continues to scale. Also, more diversity of models would also help understanding robustness of this method to the model. Why not consider ImageNet? Goyal et al reports that it took an hour for them to train ResNet on ImageNet with 256 GPUs, and authors may demonstrate it can be trained faster.\n\nFurthermore, authors briefly mention that staleness of parameters up to tens of thousands of updates did not have any adverse effect, but it would good to know how the learning curve behaves as a function of this delay. Knowing how much delay we can tolerate will motivate us to design different methods of communication between teacher and student models.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large scale distributed neural network training through online distillation","abstract":"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","pdf":"/pdf/e06ba6a0a0b5789a8037fc28c578d88c028e2edf.pdf","TL;DR":"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines.","paperhash":"anonymous|large_scale_distributed_neural_network_training_through_online_distillation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large scale distributed neural network training through online distillation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkr1UDeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper291/Authors"],"keywords":["distillation","distributed training","neural networks","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739382401,"tcdate":1509090780935,"number":291,"cdate":1509739379738,"id":"rkr1UDeC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkr1UDeC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large scale distributed neural network training through online distillation","abstract":"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","pdf":"/pdf/e06ba6a0a0b5789a8037fc28c578d88c028e2edf.pdf","TL;DR":"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines.","paperhash":"anonymous|large_scale_distributed_neural_network_training_through_online_distillation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large scale distributed neural network training through online distillation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkr1UDeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper291/Authors"],"keywords":["distillation","distributed training","neural networks","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}