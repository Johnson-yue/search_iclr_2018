{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222678880,"tcdate":1511828083733,"number":3,"cdate":1511828083733,"id":"Sk2_qmcxf","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple and important insight about LSTMs","rating":"7: Good paper, accept","review":"Summary:\nThis paper shows that incorporating invariance to time transformations in recurrent networks naturally results in a gating mechanism used by LSTMs and their variants. This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known. Experiments demonstrate that the proposed initialization speeds up learning on synthetic tasks, although benefits for next-step prediction tasks are limited.\n\nQuality and significance:\nThe core insight of the paper is the link between recurrent network design and its effect on how the network reacts to time transformations. This insight is simple, elegant and valuable in my opinion. \n\nIt is becoming increasingly apparent recently that the benefits of the gating and cell mechanisms introduced by the LSTM, now also used in feedforward networks, go beyond avoiding vanishing gradients. The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases. Understanding the link between model architecture and behavior is very useful for the field in general, and this paper contributes to this knowledge. In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8. The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive. \n\nClarity:\nThe paper is well-written in general and easy to understand. A minor complaint is that there are an unnecessarily large number of paragraph breaks, especially on pages 3 and 4, which make reading slightly jarring.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/15158e1e891be86f5bf5110fef418198db7eb5a0.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222678919,"tcdate":1511827213935,"number":2,"cdate":1511827213935,"id":"HkIzPXqxM","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting development to gated RNN","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper provides an interesting theoretical explanation on why gated RNN architectures such as LSTM and GRU work well in practice. The paper shows how \"gate values appear as time contraction or time dilation coefficients\". The authors also point out the connection between the gate biases and the range of time dependencies captured in the network. From that, they develop a simple yet effective initialization method which performs well on different datasets.\n\nPros:\n- The idea is interesting, it well explain the success of gated RNNs.\n- Writing: The paper is well written and easy to read. \n\nCons:\nExperiments: only small datasets were used in the experiments, it would be more convincing if the author could use larger datasets. One suggestion to make the experiment more complete is to gradually increase the initial value of the biases to see how it affect the performance. To use 'chrono initialization', one need to estimate the range of time dependency which could be difficult in practice. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/15158e1e891be86f5bf5110fef418198db7eb5a0.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222678960,"tcdate":1511794445283,"number":1,"cdate":1511794445283,"id":"SkrzwsKeG","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Cool theoretical contribution with rather unrelated experiments","rating":"5: Marginally below acceptance threshold","review":"tl;dr: \n - The paper has a really cool theoretical contribution. \n - The experiments do not directly test whether the theoretical insight holds in practice, but instead a derivate method is tested on various benchmarks. \n\nI must say that this paper has cleared up quite a few things for me. I have always been a skeptic wrt LSTM, since I myself did not fully understand when to prefer them over vanilla RNNs for reasons other than “they empirically work much better in many domains.” and “they are less prone to vanishing gradients”. \n\nSection 1 is a bliss: it provides a very useful candidate explanation under which conditions vanilla RNNs fail (or at least, do not efficiently generalise) in contrast to gated cells. I am sincerely happy about the write up and will point many people to it.\n\nThe major problem with the paper, in my eyes, is the lack of experiments specific to test the hypothesis. Obviously, quite a bit of effort has gone into the experimental section. The focus however is comparison to the state of the art in terms of raw performance.  \n\nThat leaves me asking: are gated RNNs superior to vanilla RNNs if the data is warped?\nWell, I don’t know now. I only can say that there is reason to believe so. \n\nI *really* do encourage the authors to go back to the experiments and see if they can come up with an experiment to test the main hypothesis of the paper. E.g. one could make synthetic warpings, apply it to any data set and test if things work out as expected. Such a result would in my opinion be of much more use than the tiny increment in performance that is the main output of the paper as of now, and which will be stomped by some other trick in the months to come. It would be a shame if such a nice theoretical insight got under the carpet because of that. E.g. today we hold [Pascanu 2013] dear not because of the proposed method, but because of the theoretical analysis.\n\nSome minor points.\n- The authors could make use of less footnotes, and try to incorporate them into the text or appendix.\n- A table of results would be nice.\n- Some choices of the experimental section seem arbitrary, e.g. the use of optimiser and to not use clipping of gradients. In general, the evaluation of the hyper parameters is not rigorous.\n- “abruplty” -> “abruptly” on page 5, 2nd paragraph\n\n### References\n[Pascanu 2013] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. \"On the difficulty of training recurrent neural networks.\" International Conference on Machine Learning. 2013.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/15158e1e891be86f5bf5110fef418198db7eb5a0.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739258438,"tcdate":1509125250112,"number":519,"cdate":1509739255776,"id":"SJcKhk-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/15158e1e891be86f5bf5110fef418198db7eb5a0.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}