{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222655254,"tcdate":1511876107983,"number":3,"cdate":1511876107983,"id":"Bk4fUyieG","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novelty","rating":"6: Marginally above acceptance threshold","review":"This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments.\n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm.  They extend the scheme to allow the use of different scaling parameters and to m-bit quantization. Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods. \n\nThe experiments are complete and the writing is good.\n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b). Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222655294,"tcdate":1511685728551,"number":2,"cdate":1511685728551,"id":"Sy_vAgOgz","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A new method for weight quantization. A step in the right direction, with interesting results, but not a huge level of novelty. ","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights). \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose. The experiments are very clearly presented and solidly designed.\n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm. However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$). This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222655332,"tcdate":1511524644071,"number":1,"cdate":1511524644071,"id":"B1h7tYSlM","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Ternarization-based network compression via loss-aware minimization","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors propose a method of compressing network by means of weight ternarization. The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017).\n\nTo this reviewerâ€™s understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work.\n\nWhile the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression.\n\nIt would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1509739296860,"tcdate":1509119805195,"number":452,"cdate":1509739294198,"id":"BkrSv0lA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}