{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222811918,"tcdate":1511673161185,"number":2,"cdate":1511673161185,"id":"HyW8p6vxM","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Review","forum":"r1CE9GWR-","replyto":"r1CE9GWR-","signatures":["ICLR.cc/2018/Conference/Paper900/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This work does not explain GANs, it merely revisits minimum-distance density estimation","rating":"4: Ok but not good enough - rejection","review":"First of all, let me state this upfront: despite the sexy acronym \"GAN\" in the title, this paper does not provide any genuine understanding of GANs. Conceptually, GANs are an algorithmic instantiation of a classic idea in statistics, mamely minimum-distance estimation, originally introduced by Jacob Wolfowitz in 1957 (*). This provides the 'min' part. The 'max' part comes from considering distances that can be expressed as a supremum over a class of test functions. Again, this is not new -- for instance, empirical risk minimization, in both supervised and unsupervised learning, can be phrased as precisely such a minimax problem by casting the convergence analysis in terms of suprema of suitable empirical processes (see, e.g., \"Empirical Processes in M-Estimation\" by Sara Van De Geer). Moreover, even the minimax (and, more broadly, game-theoretic) criteria go back all the way to the foundational papers of Abraham Wald.\n\nNow, the conceptual innovation of GANs is that this minimax formulation can be turned into a zero-sum game played by two algorithmic architectures, the generator and the discriminator. The generator proposes a model (which is assumed to be easy to sample from) and generates a sample starting from a fixed instrumental distribution; the discriminator evaluates the current proposal against a class of test functions, which, again, are assumed to be easily computable, e.g., by a neural net. One can also argue that the essence of GANs is precisely the architectural constraints on both the generator and the discriminator that make their respective problems amenable to 'differentiable' approaches, e.g., gradient descent/ascent with backpropagation. Without such a constraint, the saddle point is either trivial or reduces to finding a worst-case Bayes estimate, as classical statistical theory would predict.\n\nThis paper essentially strips away the essence of GANs and considers a stylized minimum-distance estimation problem, where both the target and the instrumental distributions are Gaussian, and the 'distance' between statistical models is the quadratic Wasserstein distance induced by the Euclidean norm. This, essentially, stacks the deck in favor of linear strategies, and it is not surprising at all that PCA emerges as the solution. It is very hard to see how any of this helps our understanding of either strengths or shortcomings of GANs (such as mode collapse or stability issues). Moreover, the discussion of supervised and unsupervised paradigms is utterly unconvincing, especially in light of the above comment on minimum-distance estimation underlying both of these paradigms. In either setting, a learning algorithm is obtained from the population version of the problem by substituting the empirical distribution of the observed data for the unknown population law.\n\nAdditional minor comments on proper attribution and novelty of results:\n\n1) Lemma 3 (structural result for optimal transport with L_2 Wasserstein cost) is not due to Chernozhukov et al., it is a classic result in the theory of optimal transportation, in various forms due to Brenier, McCann, and others -- cf., e.g., Chapters 2 and  3 of C. Villani, \"Topics in Optimal Transportation.\"\n\n2) The rate-distortion formulation with fixed input and output marginal in Appendix A, while interesting, is also not new. Precise characterizations in terms of optimal transport are available, see, e.g., N. Saldi, T. Linder, and S. Yuksel, \"Randomized Quantization and Source Coding With Constrained Output Distribution,\" IEEE Transactions on Information Theory, vol. 61, no. 1., pp. 91-106, January 2015.\n\n(*) The method of Wolfowitz is not restricted to distance functions in the mathematical sense; it can work equally well with monotone functions of metrics -- e.g., the square of a metric.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1512222811962,"tcdate":1511607663149,"number":1,"cdate":1511607663149,"id":"H1PuapUef","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Review","forum":"r1CE9GWR-","replyto":"r1CE9GWR-","signatures":["ICLR.cc/2018/Conference/Paper900/AnonReviewer1"],"readers":["everyone"],"content":{"title":"GANs studied theoretically under extremely strong assumptions","rating":"4: Ok but not good enough - rejection","review":"*Paper summary*\n\nThe paper considers GANs from a theoretical point of view. The authors approach GANs from the 2-Wasserstein point of view and provide several insights for a very specific setting. In my point of view, the main novel contribution of the paper is to notice the following fact:\n\n(*) It is well known that the 2-Wasserstein distance W2(PY,QY) between multivariate Gaussian PY and its empirical version QY scales as $n^{-2/d}$, i.e. converges very slow as the dimensionality of the space $d$ increases. In other words, QY is not such a good way to estimate PY in this setting. A somewhat better way is use a Gaussian distribution PZ with covariance matrix S computed as a sample covariance of QY. In this case W2(PY, PZ) scales as $\\sqrt{d/n}$.\n\nThe paper introduces this observation in a very strange way within the context of GANs. Moreover, I think the final conclusion of the paper (Eq. 19) has a mistake, which makes it hard to see why (*) has any relation to GANs at all.\n\nThere are several other results presented in the paper regarding relation between PCA and the 2-Wasserstein minimization for Gaussian distributions (Lemma 1 & Theorem 1). This is indeed an interesting point, however the proof is almost trivial and I am not sure if this provides any significant contribution for the future research.\n\nOverall, I think the paper contains several novel ideas, but its structure requires a *significant* rework and in the current form it is not ready for being published. \n\n*Detailed comments*\n\nIn the first part of the paper (Section 2) the authors propose to use the optimal transport distance Wc(PY, g(PX)) between the data distribution PY (or its empirical version QY) and the model as the objective for GAN optimization. This idea is not novel: WGAN [1] proposed (and successfully implemented) to minimize the particular case of W1 distance by going through the dual form, [2] proposed to approach any Wc using auto-encoder reformulation of the primal (and also shoed that [5] is doing exactly W2 minimization), and [3] proposed the same using Sinkhorn algorithm. So this point does not seem to be novel.\n\nThe rest of the paper only considers 2-Wasserstein distance with Gaussian PY and Gaussian g(PX) (which I will abbreviate with R), which looks like an extremely limited scenario (and certainly has almost no connection to the applications of GANs).\n\nSection 3 first establishes a relation between PCA and minimizing 2-Wasserstein distance for Gaussian distributions (Lemma 1, Theorem 1). Then the authors show that if R minimizes W2(PY, R) and QR minimizes W2(QY, QR) then the excess loss W2(PY, QR) - W2(PY, R) approaches zero at the rate $n^{-2/d}$ (both for linear and unconstrained generators). This result basically provides an upper bound showing that GANs need exponentially many samples to minimize W2 distance. I don't find these results novel, as they already appeared in [4] with a matching lower bound for the case of Gaussians (Theorem B.1 in Appendix can be modified easily to show this). As the authors note in the conclusion of Section 3, these results have little to do with GANs, as GANs are known to learn quite quickly (which contradicts the theory of Section 3).\n\nFinally, in Section 4 the authors approach the same W2 problem from its dual form and notice that for the LQG model the optimal discriminator is quadratic. Based on this they reformulate the W2 minimization for LQG as the constrained optimization with respect to p.d. matrix A (Eq 16). The same conclusion does not work unfortunately for W2(QY, R), which is the real training objective of GANs. Theorem 3 shows that nevertheless, if we still constrain discriminator in the dual form of W2(QY, R) to be quadratic, the resulting soliton QR* performs the empirical PCA of Pn. \n\nThis leads to the final conclusion of the paper, which I think contains a mistake. In Eq 19 the first equation, according to the definitions of the authors, reads\n\\[\nW2(PY, QR) = W2(PY, PZ),   (**)\n\\]\nwhere QR is trained to minimize min_R W2(QY, R) and PZ is as defined in (*) in the beginning of these notes. \nHowever, PZ is not the solution of min_R W2(QY, R) as the authors notice in the 2nd paragraph of page 8. Thus (**) is not true (at least, it is not proved in the current version of the text). PZ is a solution of min_R W2(QY, R) *where the discriminator is constrained to be quadratic*. This mismatch is especially strange, given the authors emphasize in the introduction that they provide bounds on divergences which are the same as used during the training (see 2nd paragraph on page 2) --- here the bound is on W2, but the empirical GAN actually does a regularized training (with constrained discriminator).\n\nFinally, I don't think the experiments provide any convincing insights, because the authors use W1-minimization to illustrate properties of the W2. Essentially the authors say \"we don't have a way to perform W2 minimization, so we rather do the W1 minimization and assume that these two are kind of similar\".\n\n* Other comments *\n(1) Discussion in Section 2.1 seems to never play a role in the paper.\n(2) Page 4: in p-Wasserstein distance, ||.|| does not need to be a Euclidean metric. It can be any metric.\n(3) Lemma 2 seems to repeat the result from (Canas and Rosasco, 2012) as later cited by authors on page 7?\n(4) It is not obvious how does Theorem 2 translate to the excess loss? \n(5) Section 4. I am wondering how exactly the authors are going to compute the conjugate of the discriminator, given the discriminator most likely is a deep neural network?\n\n\n[1] Arjovsky et al., Wasserstein GAN, 2017\n[2] Bousquet et al, From optimal transport to generative modeling: the VEGAN cookbook, 2017\n[3] Genevay et al., Learning Generative Models with Sinkhorn Divergences, 2017\n[4] Arora et al, Generalization and equilibrium in GANs, 2017\n[5] Makhazani et al., Adversarial Autoencoders, 2015","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1511562282067,"tcdate":1511562282067,"number":4,"cdate":1511562282067,"id":"BJG4hf8lz","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Comment","forum":"r1CE9GWR-","replyto":"SJFExL7eM","signatures":["ICLR.cc/2018/Conference/Paper900/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper900/Authors"],"content":{"title":"About implementation","comment":"Hi Anna, \n\nThank you for your comment. Below are some clarifications: \n\n1) Our main results show that without constraining the discriminator, quadratic GAN has a poor generalization even in the simple LQG setup. In this setup, we show that the proper constraint on the discriminator is the quadratic constraint which makes its convergence exponentially fast. Note that this constraint does not change the population limit compared to the unconstrained case (see Fig 1 in the paper). Indeed implementation of the quadratic GAN with quadratic discriminator is also straightforward. \n\n2) In this work, we have not analyzed/implemented quadratic GAN  with neural network generators/discriminators. We believe that understanding GANs in the simple setup of the LQG paves the way to understand it (and properly implement it) in a more general setup. We agree with you that implementing the quadratic GAN with neural nets is more challenging than that of the WGAN (Arjovskey et al.) due in part to the computation of the convex conjugate. Note that if you use the gradient descent to implement this part, the extra running time should be comparable with the running time of optimizing the generator and the discriminator. Thus, even though the overall running time may be larger than that of current GAN implementations, it should be in the same order.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1511379897934,"tcdate":1511378993008,"number":4,"cdate":1511378993008,"id":"SJFExL7eM","invitation":"ICLR.cc/2018/Conference/-/Paper900/Public_Comment","forum":"r1CE9GWR-","replyto":"BJ7s1MTyz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Final Empirical conclusion","comment":"Dear Authors,\n\nThanks for your reply. \n\n1) I agree that computing the convex conjugate of a quadratic function \\psi is easy (a very well known result indeed). However, this is cannot be used for implementation of a neural network. Also, one could directly use least squares if you want to stick to the linear setting.\n\n2) Regarding my implementation, I was solving for \\psi^* by gradient descent. What method did you try?\n\nHere are my final empirical conclusions:\n\nThe only practical way to calculate psi^* is to perform many steps of gradient descent for *every* single step of GAN training and this completely destroys the computational efficiency of neural network.  Thus unfortunately this method is not practical by any measure. I wonder if this is the reason why this was not mentioned in \"Wasserstein GAN\" by Arjovsky et. al."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1510969243234,"tcdate":1510969243234,"number":3,"cdate":1510969243234,"id":"BJ7s1MTyz","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Comment","forum":"r1CE9GWR-","replyto":"Sks3SeNJf","signatures":["ICLR.cc/2018/Conference/Paper900/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper900/Authors"],"content":{"title":"About implementation","comment":"Hi Anna,\n\nThank you for your interest in our work. \n\nImplementation of quadratic GAN (which has a convex discriminator) using deep neural nets (DNNs) is challenging due in part to the computation of the convex \nconjugate of the discriminator function \\psi, which is a DNN. We are currently working on this part and haven’t extensively tested it. \n\nHowever, implementation of the method which we show that has a fast generalization rate in the LQG setup (quadratic GAN with convex quadratic discriminator) is easy since there is a closed-form solution for the convex conjugate of a convex quadratic function \\psi.\n\nWe are interested to know how are you implementing the quadratic GAN as the implementation details can affect the convergence. \n\nBest,\nOn behalf of authors"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1510381834286,"tcdate":1510372786983,"number":3,"cdate":1510372786983,"id":"Sks3SeNJf","invitation":"ICLR.cc/2018/Conference/-/Paper900/Public_Comment","forum":"r1CE9GWR-","replyto":"r1CE9GWR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"[Empirical conclusion] Not able to obtain convergence on squared W_2 loss","comment":"Hi\n\nAs suggested to by paper's main result (but not shown experimentally), I tried training GANs with squared W_2 loss on LSUN-Bedrooms dataset over the last few days. However, my neural networks were not converging even after heavily tweaking hyper parameters (like learning rate, batch size etc). \n\nAlso, I was not able to find any suggestions in the longer version of this paper (from arxiv). I was wondering if you have any suggestions which could help me?\n\nI tried using the following generator neural network models:\n 1)  4 relu hidden layers and 512 units (as in Arjovsky's paper).\n 2)  6 relu hidden layers and 512 units.\n 3)  8 relu hidden layers and 256 units.\n\nThanks\nAnna\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1510092430773,"tcdate":1509889850265,"number":2,"cdate":1509889850265,"id":"B1zBD53A-","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Comment","forum":"r1CE9GWR-","replyto":"H1oRM3cCZ","signatures":["ICLR.cc/2018/Conference/Paper900/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper900/Authors"],"content":{"title":"Understanding GANs via linear Gaussians is a good approach","comment":"Response to the 4 comments:\n\n\"Gaussian assumption on data might be reasonable but to analyze GANs linear generators are not natural assumption (as its the non-linearity that makes them powerful)\"\n\nOur result showed the opposite. Without any constraints on the generator and allowing full nonlinearity, lemma 2 shows that the generalization ability of GAN is very poor, needing an exponential number of samples (exponential with the number of samples). With linear generators and quadratic discriminators, the generalization ability is much better, only linear number of  samples (Theorem 3)\n\n\"Simplifying GANs to linear (control) system with feedback removes the very essence GANs!\"\n\nWe do not agree. As can be seen from the paper, there are lots of complexity in the problem even under the LQG setting. \n\n\"\"We start with proposing a general formulation for GANs with a general loss function and a general data distribution\"  Wasn't this proposed by Goodfellow et. al. ? Am I missing something?\"\n\nNo the general formulation is not proposed by Goodfellow. For example, Wasserstein GANs are not covered by Goodfellow's formulation. Our formulation is a generalization of Wasserstein GAN's formulation.  (See Section 2)\n\n\"Under the LGQ model, the paper solves :  min_g E[||Y-g*X||^2] for gaussian X, Y and linear operator g. Since the error term Y-g*X is gaussian , least square solution (obtained in the paper) is the MLE estimator (and MMSE). \"\n\nNo. min_g E[||Y - g*X||^2] is the supervised learning problem. The GAN problem  is unsupervised and is  given by\n\nmin_g min_{P_X,Y} E [||Y - g*X||^2] \n\nNote that for the supervised learning problem, the empirical joint distribution p_X,Y is already given from the data. On the other hand, for the unsupervised learning problem, the joint distribution is not given and is part of the optimization. Hence a more complex problem. See Section 2 for more discussions. \n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1509765843319,"tcdate":1509765843319,"number":2,"cdate":1509765843319,"id":"H1oRM3cCZ","invitation":"ICLR.cc/2018/Conference/-/Paper900/Public_Comment","forum":"r1CE9GWR-","replyto":"BJxqtFwAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Modelling GANS via linear gaussians gone wrong","comment":"\"(i) by assuming a Gaussian data distribution. In this case, the class of linear generators is natural \" - \n      Gaussian assumption on data might be reasonable but to analyze GANs linear generators are not natural assumption (as its the non-linearity that makes them powerful)\n\n(This) \"shed light on appropriate architectures for GANs even without bringing DNNs into the picture\"\n      Simplifying GANs to linear (control) system with feedback removes the very essence GANs!\n\n\"We start with proposing a general formulation for GANs with a general loss function and a general data distribution\"\n      Wasn't this proposed by Goodfellow et. al. ? Am I missing something?\n\n\"The resulting problem is not least squares; it's minimizing the quadratic Wasserstein distance from the true distribution\"\n      Under the LGQ model, the paper solves :  min_g E[||Y-g*X||^2] for gaussian X, Y and linear operator g. Since the error term Y-g*X is gaussian , least square solution (obtained in the paper) is the MLE estimator (and MMSE). "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1510092430824,"tcdate":1509558664225,"number":1,"cdate":1509558664225,"id":"BJxqtFwAW","invitation":"ICLR.cc/2018/Conference/-/Paper900/Official_Comment","forum":"r1CE9GWR-","replyto":"HJkAme4AZ","signatures":["ICLR.cc/2018/Conference/Paper900/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper900/Authors"],"content":{"title":"Understanding GANs via linear Gaussians is a good approach","comment":"Just two points of clarification about our approach to the problem:\n\n1) There are two key aspects to GANs: i) the novel game theoretic learning architecture, ii) the use of neural networks as function classes over which the game theoretic objective is optimized. The complexity of understanding GANs stems from the combination of these two aspects. Our approach allows us to focus on aspect (i) by assuming a Gaussian data distribution. In this case, the class of linear generators is natural. Still it is not obvious what should be the class of discriminators for fast learning. Our result shows that one should use a class of quadratic discriminators to balance against linear generators. Thus our results are non-trivial and shed light on appropriate architectures for GANs even without bringing DNNs into the picture. \n\n2) We want to emphasize we are NOT showing that Least-Square under the Gaussian setting is PCA. We start with proposing a general formulation for GANs with a general loss function and a general data distribution.  Then we show what happens when specializing this formulation to quadratic loss. The resulting problem is not least squares; it's minimizing the quadratic Wasserstein distance from the true distribution.  The connection with PCA is also more subtle. While in the population limit the solution is PCA, we show that without constraints on the discriminator the solution is NOT empirical PCA when there are finite many samples. Only when we put the quadratic constraint on the discriminator do we get back empirical PCA. Again, our results point to the importance of an appropriate constraining of the discriminator."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1509326417765,"tcdate":1509323719163,"number":1,"cdate":1509323719163,"id":"HJkAme4AZ","invitation":"ICLR.cc/2018/Conference/-/Paper900/Public_Comment","forum":"r1CE9GWR-","replyto":"r1CE9GWR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Modelling GANS via linear gaussians gone wrong.","comment":"I came across this paper while searching for submissions on GAN theory. After going through this, my overall impression is that this paper doesn't analyze GANs but instead it trivially shows the solution to Least-Square under the Gaussian setting is PCA (a well known fact for more than 50 years).\n\nQuick summary: This paper considers a GAN setting with a linear generator under squared error loss with Gaussian features. It shows that under the above assumptions, the optimal solution of the GAN is nothing but the PCA solution. The paper also has some basic simulations supporting Theorem 3 (the statement in the previous para). However, the paper has following major weakness:\n\n1) One of the key drivers of NNs is the non-linearity between different layers. However, this paper restricts the generator to be linear, hence missing the key ingredient of NNs.\n\n2) The main result says that GANs under second order Wasserstein loss is equivalent to PCA. However, in practice it is known that GANS are not doing PCA. In fact, GANS are able to achieve results superior than PCA on many datasets of interest. Isn't this a clear mismatch between practical observations and main conclusion of the paper itself? This raises the questions about the suitability of 'LQG' model itself.\n\n3) The theory section of the paper motivates l_2  (2nd order wassertein) over the l_1 loss. However ironically,  the simulations use l_1 loss to justify the use of l2 of loss! Did I miss anything here?\n\n4)  The paper doesn't use the fact that the generator and discriminator are NNs themselves. Thus the papre has nothing to with GANs as commonly understood by the community.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]}},{"tddate":null,"ddate":null,"tmdate":1509739041849,"tcdate":1509136950937,"number":900,"cdate":1509739039193,"id":"r1CE9GWR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1CE9GWR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Understanding GANs: the LQG Setting","abstract":"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this  paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that  in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finite-sample GAN performs simply empirical PCA.","pdf":"/pdf/5fd49847c822b7cc58efd57865bf98665a3abd3b.pdf","paperhash":"anonymous|understanding_gans_the_lqg_setting","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding GANs: the LQG Setting},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1CE9GWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper900/Authors"],"keywords":["Generative Adversarial Networks","Wasserstein","Generalization","PCA"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}