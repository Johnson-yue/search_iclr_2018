{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222783387,"tcdate":1511914456468,"number":3,"cdate":1511914456468,"id":"B1lk3Ojxf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Study on gradient compression","rating":"7: Good paper, accept","review":"The paper is thorough and on the whole clearly presented. However, I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle. The methods proposed are heuristic in nature, and it's not clear what the guiding principle is. E.g., \"momentum correction\". What exactly is the problem without this correction? The authors describe it qualitatively, \"When the gradient sparsity is high, the interval dramatically increases, and thus the significant momentum effect will harm the model performance\". Can the issue be described more precisely? Similarly for gradient clipping, \"The method proposed by Pascanu et al. (2013) rescales the gradients whenever the sum of their L2-norms exceeds a threshold. This step is conventionally executed after gradient aggregation from all nodes. Because we accumulate gradients over iterations on each node independently, we perform the gradient clipping locally before adding the current gradient... \" What exactly is the issue here? It reads like a story of what the authors did, but it's not really clear why they did it.\n\nThe experiments seem quite thorough, with several methods being compared. What is the expected performance of the 1-bit SGD method proposed by Seide et al.?\n\nre. page 2: What exactly is \"layer normalization\"?\n\nre. page 4: What are \"drastic gradients\"?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511897281655,"tcdate":1511897105554,"number":4,"cdate":1511897105554,"id":"Bk5Gd4sxf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~Wei_Wen1"],"readers":["everyone"],"writers":["~Wei_Wen1"],"content":{"title":"Hi from TernGrad","comment":"Hi from TernGrad, \n\nImpressive result, really! \n\nFor the top-1 accuracy in Table 3, I guess the 0.89% accuracy difference of TernGrad comes from the different ways we trained the standard AlexNet? In our work, the baseline AlexNet is trained using the same hyper-parameters of caffe (https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet), and converges to 57.32%. Your baseline got 58.17% because you used different training hyper-parameters in Wilber 2016 as you pointed out?\nReplacing floating SGD by TernGrad, it converges to 57.28%. The loss because of TernGrad is just 0.04% instead of 0.89%? \n\nIs it easy to implement all of the techniques? Do you plan to open source it? I may want to try this.\nThe core of TernGrad can be done within several lines (https://github.com/wenwei202/terngrad/blob/master/terngrad/inception/bingrad_common.py#L159-L166).\n\nAnd just be curious about how does the warmup stage generalize, does the same warmup scheme work in general for all experiments? I am asking since we may not want to tune the warmup stage for several times when training a DNN, which essentially is wasting training time. TernGrad converges with the same hyper-parameters of standard SGD.\n\nThanks,\n-Wei "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222783433,"tcdate":1511826235241,"number":2,"cdate":1511826235241,"id":"rJmrmQ5lG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good empirical results, but requires work to justify the proposed techniques","rating":"5: Marginally below acceptance threshold","review":"This paper proposes additional improvement over gradient dropping(Aji & Heafield) to improve communication efficiency.  \n\n- First of all, the experimental results are thorough and seem to suggest the advantage of the proposed techniques.\n- The result for gradient dropping(Aji & Heafield) should be included in the ImageNet experiment.\n- I am having a hard time understanding the intuition behind v_t introduced in the momentum correction. The authors should provide some form of justifications.\n   - For example, provide an equivalence provide to the original update rule or some error analysis would be great\n   - Did you keep a running sum of v_t overall history? Such sum without damping(the m term in momentum update) is likely lead to the growing dominance of noise and divergence.\n- The momentum masking technique seems to correspond to stop momentum when a gradient is synchronized. A discussion about the relation to asynchronous update is helpful.\n- Do you do non-sparse global synchronization of momentum term? It seems that local update of momentum is likely going to diverge,  and the momentum masking somehow reset that.\n- In the experiment, did you perform local aggregations of gradients between GPU cards before send out to do all0reduce in a network? since doing so will reduce bandwidth requirement.\n\nIn general, this is a paper shows good empirical results. But requires more work to justify the proposed correction techniques.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222783473,"tcdate":1511474578925,"number":1,"cdate":1511474578925,"id":"rJ9crpElM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A useful contribution","rating":"7: Good paper, accept","review":"I think this is a good work that I am sure will have some influence in the near future. I think it should be accepted and my comments are mostly suggestions for improvement or requests for additional information that would be interesting to have.\n\nGenerally, my feeling is that this work is a little bit too dense, and would like to encourage the authors in this case to make use of the non-strict ICLR page limit, or move some details to appendix and focus more on more thorough explanations. With increased clarity, I think my rating (7) would be higher.\n\nSeveral Figures and Tables are never referenced in the text, making it a little harder to properly follow text. Pointing to them from appropriate places would improve clarity I think.\n\nAlgorithm 1 line 14: You never seem to explain what is sparse(G). Sec 3.1: What is it exactly that gets communicated? How do you later calculate the Compression Ratio? This should surely be explained somewhere.\n\nSec 3.2 you mention 1% loss of accuracy. A pointer here would be good, at that point it is not clear if it is in your work later, or in another paper. The efficient momentum correction is great!\n\nAs I was reading the paper, I got to the experiments and realized I still don't understand what is it that you refer to as \"deep gradient compression\". Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments.\n\nI feel the presentation of experimental results is somewhat disorganized. It is not clear what is immediately clear what is the baseline, that should be somewhere stressed. I find it really confusing why you sometimes compare against Gradient Dropping, sometimes against TernGrad, sometimes against neither, sometimes include Gradient Sparsification with momentum correction (not clear again what is the difference from DGC). I recommend reorganizing this and make it more consistent for sake of clarity. Perhaps show here only some highlights, and point to more in the Appendix.\n\nSec 5: Here I feel would be good to comment on several other things not mentioned earlier. \nWhy do you only work with 99.9% sparsity? Does 99% with 64 training nodes lead to almost dense total updates, making it inefficient in your communication model? If yes, does that suggest a scaling limit in terms of number of training nodes? If not, how important is the 99.9% sparsity if you care about communication cost dominating the total runtime? I would really like to better understand how does this change and what is the point beyond which more sparsity is not practically useful. Put differently, is DGC with 600x size reduction in total runtime any better than DGC with 60x reduction?\n\n\nFinally, a side remark:\nUnder eq. (2) you point to something that I think could be more discussed. When you say what you do has the effect of increasing stepsize, why don't you just increase the stepsize? \nThere has recently been this works on training ImageNet in 1 hour, then in 24 minutes, latest in 15 minutes... You cite the former, but highlight different part of their work. Broader idea is that this is trend that potentially makes this kind of work less relevant. While I don't think that makes your work bad or misplaced, I think mentioning this would be useful as an alternative approach to the problems you mention in the introduction and use to motivate your contribution.\n...what would be your reason for using DGC as opposed to just increasing the batch size?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511901508167,"tcdate":1510990581749,"number":3,"cdate":1510990581749,"id":"H1Rx7PTJG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"B1wc_Z5JG","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Hello from your baseline","comment":"Dear Kenneth Heafield,\n\n     Thank you for clarifying the Gradient Dropping, it's very helpful. We will describe the Gradient Dropping in a more rigorous way in the final version.\n     We also appreciate your reminding us of citing these two excellent papers.\n     Here are the answers to your questions.\n\n     - Warm-up training works in general, so was it included in your baseline experiments as well? \n\n       Warm-up training was previously used for improving the large minibatch training proposed by Goyal et. al. They warm up the learning rate linearly in the first several epochs. However, we are the first to warm up the sparsity during the gradient pruning. Therefore, only experiments with DGC adopted warm-up sparsity. It is a simple but effective technique.\n\n      - \"Implementing DGC requires gradient sorting.\"  To be pedantic, it requires top-k selection which we have been talking to NVIDIA about implementing more efficiently in the context of beam search.  I like the hierarchical add-on to the sampling we've been doing too; if too few gradients pass the threshold, do you sample more?  \n\n      We indeed use top-k selection instead of sorting. We do not sample more if too few gradients are selected. Since hierarchical selection is designed to control the communication data size, we will perform top-k selection twice only when too many gradients pass the threshold."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510770831328,"tcdate":1510770831328,"number":3,"cdate":1510770831328,"id":"B1wc_Z5JG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~Kenneth_Heafield1"],"readers":["everyone"],"writers":["~Kenneth_Heafield1"],"content":{"title":"Hello from your baseline","comment":"I'm Kenneth Heafield, one of the authors cited.   \n\nIt's an interesting bag of 4 tricks here and I will likely use them going forward.  \n\n\"Gradient Dropping requires adding a layer normalization.\" Figure 5 in our paper shows that gradient dropping works, admittedly slower, without layer normalization if we determine the threshold locally to each parameter/matrix rather than globally.  \n\nI feel like you're giving us too much credit.  Strom https://s3-us-west-2.amazonaws.com/amazon.jobs-public-documents/strom_interspeech2015.pdf and Dryden et al https://ornlcda.github.io/MLHPC2016/papers/3_Dryden.pdf deserve to be cited too.  \n\nWarm-up training works in general, so was it included in your baseline experiments as well?  \n\n\"incurring 0.3% loss of accuracy on a machine translation task\" It would be better to say BLEU score here, rather than a vague metric.  Parsing people fight over 0.3% while translation people shrug over 0.3% BLEU.  \n\n\"Implementing DGC requires gradient sorting.\"  To be pedantic, it requires top-k selection which we have been talking to NVIDIA about implementing more efficiently in the context of beam search.  I like the hierarchical add-on to the sampling we've been doing too; if too few gradients pass the threshold, do you sample more?  \n\nAbstracts should compare to the strongest baseline, not just the stock baseline.  \n\nLet's talk when you're less anonymous.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510740814691,"tcdate":1510740814691,"number":2,"cdate":1510740814691,"id":"SJPLQcF1M","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"H1cZgIYyM","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Typos and clarification questions","comment":"We really appreciate your comments.\n\n- Equation 1 & 2: shouldn’t k start from 1 if N is the number of training node?\n       Yes. It's a typo. k should start from 1.\n\n- Related Work section: Graidient typo\n- Section 4.2: csparsity typo\n       Thank you for pointing out these typos. They should be \"Gradient\" and \"sparsity\".\n\n- Line 8, 9 in Algorithm 4 in Appendix B: shouldn’t line 8 be U <- mU + G and line 9 be V_t <- V_{t-1} + mU + G\n       These two lines are equivalent to those in Algorithm 4 in Appendix B. \n\n- Is \"Gradient Size\" referring to the average size of the gradient that's larger than the threshold?\n       Yes. \"Gradient Size\" is referring to the size of the sparse gradient, which contains both the gradient values that are larger than the threshold and 16-bit index distances when it comes to DGC."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510739778308,"tcdate":1510739778308,"number":1,"cdate":1510739778308,"id":"SyqSy9YyM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"HJOI6ndJz","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"we have considered these non-ideal effects. ","comment":"we thank the reviewer for the comments.\n\n(1) \nFirst, warm-up training takes only 4 out of 90 epochs for imagenet, 1 out of 70 epochs for librispeech, which is only 1.4%-4% of the total training epochs. Therefore the impact of warmup training is negligible.\n\nSecond, during warm-up training the gradient is also very sparse. On Imagenet, the sparsity for the 4 warm-up epochs are: 75% -> 93.75% -> 98.4375% -> 99.6% (exponentially increase), then 99.9% for the rest 86 epochs. Same warm-up sparsity rule applies to the first four quarter epochs on Librispeech, then 99.9% for the rest 69 epochs. \n\n\n(2) Yes, we already considered the a larger communication volume of summed gradients. \n\nWith N=2^k workers and s sparsity. We need k step to gather these gradients. The density doubles at every step, so the average communication volume is \\sum_{i=0}^{k-1} 2^{i}*M*s/k = (N-1)/log(N)*M*s. The average density increases sub-linearly with the number of nodes by N/log(N), not exponentially. \n\nWe already considered this non-ideal effect in the second paragraph of Section 5: \"the density of sparse data doubles at every aggregation step in the worst case. However, even considering this effect, Deep Gradient Compression still significantly reduces the network communication time, as implied in Figure 6.\" \"For instance, when training AlexNet with 64 nodes, conventional training only achieves nearly 30× speedup with 10Gbps Ethernet (Apache, 2016), while with DGC, more than 40× speedup is achieved even with 1Gbps Ethernet\". With 1Gbps Ethernet, the speedup of TernGrad is 30x, our worse case is 44x (considering this non-ideal effect), our best case is 58x. We reported the worse case, which is 44x speedup (see Figure 6)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510731958467,"tcdate":1510723585550,"number":2,"cdate":1510723585550,"id":"H1cZgIYyM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~quan_vuong1"],"readers":["everyone"],"writers":["~quan_vuong1"],"content":{"title":"Typos and clarification questions","comment":"Thank you for a great paper! The author's intuition really shines through. I just have a few clarifying points:\n\n- Equation 1 & 2: shouldn’t k start from 1 if N is the number of training node ?\n- Related Work section: Graidient typo\n- Section 4.2: csparsity typo\n- Line 8, 9 in Algorithm 4 in Appendix B: shouldn’t line 8 be U <- mU + G and line 9 be V_t <- V_{t-1} + mU + G\n- Is \"Gradient Size\" referring to the average size of the gradient that's larger than the threshold ?\n\nedit 1: add question about gradient size."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510686032179,"tcdate":1510686032179,"number":1,"cdate":1510686032179,"id":"HJOI6ndJz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does compression ratio consider the larger communication overhead in warm-up training and the smaller sparsity after summing?","comment":"For compression ratio in Table 3 & 4, does this work consider the larger communication volume during warm-up training?\nPlease clarify how many iterations it took to \"warm-up\" and what was the sparsity of gradients during warm-up. If it did warm up for 20% of total epochs with 50% sparsity, the compression ratio is bounded by 10x.\n\nDid this work consider a larger communication volume of summed gradients? Suppose there are gradients from k workers  to sum up and the sparsity of gradients is s, the expectation of the sparsity of summed gradients is s^n which exponentially decreases with n. Please clarify this.\n\nThanks"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739076742,"tcdate":1509135651724,"number":833,"cdate":1509739074092,"id":"SkhQHMW0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/ce9002330e8e51ec2b127ce59f28eb361817b9ad.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}