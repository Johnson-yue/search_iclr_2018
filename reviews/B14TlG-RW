{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222763218,"tcdate":1511822760441,"number":3,"cdate":1511822760441,"id":"Hkx2Bz9lM","invitation":"ICLR.cc/2018/Conference/-/Paper775/Official_Review","forum":"B14TlG-RW","replyto":"B14TlG-RW","signatures":["ICLR.cc/2018/Conference/Paper775/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper. Results in an additional dataset needed.","rating":"7: Good paper, accept","review":"Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention. The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference. Authors also propose a paraphrasing based data augmentation method which helps in improving the performance. Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference.\n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written.\n\n1. Why don’t you report your model performance without data augmentation in Table 1? Is it because it does not achieve SOTA? The proposed data augmentation is a general one and it can be used to improve the performance of other models as well. So it does not make sense to compare your model + data augmentation against other models without data augmentation. I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models.\n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset. That will show the generality of the proposed architecture. Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision.\n\n4. Are you willing to release your code to reproduce the results?\n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\n2. In the first contribution bullet point, “that exclusive built upon” should be “that is exclusively built upon”.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1512222763259,"tcdate":1511812082154,"number":2,"cdate":1511812082154,"id":"Hyqx3y5xz","invitation":"ICLR.cc/2018/Conference/-/Paper775/Official_Review","forum":"B14TlG-RW","replyto":"B14TlG-RW","signatures":["ICLR.cc/2018/Conference/Paper775/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting augmentation method","rating":"4: Ok but not good enough - rejection","review":"This paper presents a reading comprehension model using convolutions and attention. This model does not use any recurrent operation but it is not per se simpler than a recurrent model. Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.  On SQuAD dataset, their results show some small improvements using the proposed augmentation technique. Their best results, however, do not outperform the best results reported on the leader board.\n\nOverall, this is an interesting study on SQuAD dataset. I would like to see results on more datasets and more discussion on the data augmentation technique. At the moment, the description in section 3 is fuzzy in my opinion. Interesting information could be:\n- how is the performance of the NMT system? \n- how many new data points are finally added into the training data set?\n- what do ‘data aug’ x 2 or x 3 exactly mean?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1512222763306,"tcdate":1511580897864,"number":1,"cdate":1511580897864,"id":"rycJHDIgf","invitation":"ICLR.cc/2018/Conference/-/Paper775/Official_Review","forum":"B14TlG-RW","replyto":"B14TlG-RW","signatures":["ICLR.cc/2018/Conference/Paper775/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good ideas, but not so good evaluation.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally.\n\nFirstly, I suggest the authors rewrite the end of the introduction. The current version tends to mix everything together and makes the misleading claim. When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1. But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method. \n\nSecondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:\n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC. The novelty is limited but it is a good idea to speed up the RC models. However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs. Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct.\n\n(2) I feel that the model design is the main reason for the good overall RC performance. However, in the paper there is no motivation about why the architecture was designed like this. Moreover, the whole model architecture is only evaluated on the SQuAD dataset. As a result, it is not convincing that the system design has good generalization. If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself.\n\n(3) I like the idea of data augmentation with paraphrasing. Currently, the improvement is only marginal, but there seems many other things to play with. For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\n\nI am looking forward to the test performance of this work on SQuAD.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1510647077642,"tcdate":1510647077642,"number":1,"cdate":1510647077642,"id":"BkCXSXOyf","invitation":"ICLR.cc/2018/Conference/-/Paper775/Official_Comment","forum":"B14TlG-RW","replyto":"HJg2Fk_yf","signatures":["ICLR.cc/2018/Conference/Paper775/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper775/Authors"],"content":{"title":"Response to Implementation details","comment":"Thanks for your interest and the questions! Here are the answers:\n\n1.  The number of heads is 8, which is consistent throughout the layers. The attention key depth is 128, so the per head depth is 128/8=16.\n\n2. It should be \"the kernel sizes are 7 and 5\". \n\nWe will clarify those in the revision. Thanks!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1510631848578,"tcdate":1510631848578,"number":1,"cdate":1510631848578,"id":"HJg2Fk_yf","invitation":"ICLR.cc/2018/Conference/-/Paper775/Public_Comment","forum":"B14TlG-RW","replyto":"B14TlG-RW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Implementation details","comment":"Thank you for your work. It seems the paper lacks some of the implementation details and sometimes includes ambiguous statements.\n1. What is the number of heads used for the multi-head self attention, and is the number consistent throughout the layers? And is the attention key depth per head also 128? I feel that the encoder layer detail is lacking.\n2. Subsection 2.2 in 2. Embedding Encoder Layer, the paper states that kernel size of 7 is used for embedding encoder. However, later on subsection 4.2 Basic Setup describes \"the kernel sizes are 5 and 7\" respectively. Could you please clarify this?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1509739109854,"tcdate":1509134524162,"number":775,"cdate":1509739107192,"id":"B14TlG-RW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B14TlG-RW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Fast and Accurate Reading Comprehension Without Recurrent Networks","abstract":"Current end-to-end machine reading and question answering (Q&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a novel Q&A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We also propose a novel data augmentation technique by paraphrasing. It not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. This technique is of independent interest because it can be readily applied to other natural language processing tasks. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. Our single model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8.","pdf":"/pdf/a49832c3c387e88766fcf87355a421ad16b878ba.pdf","TL;DR":"A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models.","paperhash":"anonymous|fast_and_accurate_reading_comprehension_without_recurrent_networks","_bibtex":"@article{\n  anonymous2018fast,\n  title={Fast and Accurate Reading Comprehension Without Recurrent Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B14TlG-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper775/Authors"],"keywords":["squad","stanford question answering dataset","reading comprehension","attention","text convolutions","question answering"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}