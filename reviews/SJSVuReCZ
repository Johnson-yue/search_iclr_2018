{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222656456,"tcdate":1511995184121,"number":3,"cdate":1511995184121,"id":"Hkd4Dn2eG","invitation":"ICLR.cc/2018/Conference/-/Paper456/Official_Review","forum":"SJSVuReCZ","replyto":"SJSVuReCZ","signatures":["ICLR.cc/2018/Conference/Paper456/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"Summary:\n\nThe paper presents an information theoretic regularizer for deep learning\nalgorithms. The regularizer aims to enforce compression of the learned\nrepresentation while conditioning upon the class label so preventing the\nlearned code from being constant across classes. The presentation of the Z\nlatent variable used to simplify the calculation of the entropy H(Y|C) is \nconfusing and needs revision, but otherwise the paper is interesting.\n\nMajor Comments:\n\n- The statement that I(X;Y) = I(C;Y) + H(Y|C) relies upon several properties\n  of Y which are not apparent in the text (namely that Y is a function of X,\nso I(X;Y) should be maximal, and Y is a smaller code space than X so it should \nbe H(Y)). If Y is a larger code space than X then it should still be true, but\nthe logic is more complicated.\n\n- The latent code for Z is unclear. Given the use of ReLUs it seems like Y\n  will be O or +ve, and Z will be 0 when Y is 0 and 1 otherwise, so I'm\nunclear as to when the value H(Y|Z) will be non-zero. The data is then\npartitioned within a batch based on this Z value, and monte carlo sampling is\nused to estimate the variance of Y conditioned on Z, but it's really unclear\nas to how this behaves as a regularizer, how the z is sampled for each monte\ncarlo run, and how this influences the gradient. The discussion in Appendix C\ndoesn't mention how the Z values are generated.\n\n- The discussion on how this method differs from the information bottleneck is\n  odd, as the bottleneck is usually minimising the encoding mutual information\nI(X;Y) minus the decoding mutual information I(Y;C). So directly minimising\nH(Y|C) is similar to the IB, and also minimising H(Y|C) will affect I(C;Y) as\nI(C;Y) = H(Y) - H(Y|C).\n\n- The fine tuning experiments (Section 4.2) contain no details on the\n  parameters of that tuning (e.g. gradient optimiser, number of epochs,\nbatch size, learning rates etc).\n\n- Section 4.4 is obvious, and I'd consider it a bug if regularising with label\n  information performed worse than regularising without label information.\nEssentially it's still adding supervision after you've removed the\nclassification loss, so it's natural that it would perform better. This\nexperiment could be moved to the appendix without hurting the paper.\n\n- In appendix A an upper bound is given for the reconstruction error in terms\n  of the conditional entropy. This bound should be related to one of the many\nupper bounds (e.g. Hellman & Raviv) for the Bayes rate of a predictor, as\nthere is a fairly wide literature in this area.\n\nMinor Comments:\n\n- The authors do not state what kind of input variations they are trying to\n  make the model invariant to, and as it applies to CNNs there are multiple\ndifferent kinds, many of which are not amenable to a regularization based\nsystem for inducing invariance.\n\n- The authors should remind the reader once that I(X;Y) = H(Y) - H(Y|X) = H(X) -\n  H(X|Y), as this fact is used multiple times throughout the paper, and it may\nnot necessarily be known by readers in the deep learning community.\n\n- Computing H(Y|C) does not necessarily require computing c separate\n  entropies, there are multiple different approaches for computing this\nentropy.\n\n- The exposition in section 3 could be improved by saying that H(X|Y) measures\n  how much the representation compresses the input, with high values meaning\nlarge amounts of compression, as much of X is thrown away when generating Y.\n\n- The figures are difficult to read when printed in grayscale, the graphs\n  should be made more readable when printed this way (e.g. different symbols,\ndashed lines etc).\n\n- There are several typos (e.g. pg 5 \"staking\" -> \"stacking\").\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning","abstract":"Regularization is a big issue for training deep neural networks. In this paper, we propose a new information-theory-based regularization scheme named SHADE for SHAnnon DEcay. The originality of the approach is to define a prior based on conditional entropy, which explicitly decouples the learning of invariant representations in the regularizer and the learning of correlations between inputs and labels in the data fitting term. We explain why this quantity makes our model able to achieve invariance with respect to input variations. We empirically validate the efficiency of our approach to improve classification performances compared to standard regularization schemes on several standard architectures.","pdf":"/pdf/54f5b03ce294e16c9b287ad023e86ab1b83c907b.pdf","paperhash":"anonymous|shade_shannon_decay_informationbased_regularization_for_deep_learning","_bibtex":"@article{\n  anonymous2018shade:,\n  title={SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJSVuReCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper456/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222656529,"tcdate":1511882995730,"number":2,"cdate":1511882995730,"id":"SJ3gWWsxM","invitation":"ICLR.cc/2018/Conference/-/Paper456/Official_Review","forum":"SJSVuReCZ","replyto":"SJSVuReCZ","signatures":["ICLR.cc/2018/Conference/Paper456/AnonReviewer3"],"readers":["everyone"],"content":{"title":"nice and intuitive idea","rating":"7: Good paper, accept","review":"the paper adapts the information bottleneck method where a problem has invariance in its structure. specifically, the constraint on the mutual information is changes to one on the conditional  entropy. the paper involves a technical discription how to develop proper estimators for this conditional entropy etc.\n\nthis is a nice and intuitive idea. how it interacts with classical regularizers or if it completely dominates classical regularizers would be interesting for the readers.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning","abstract":"Regularization is a big issue for training deep neural networks. In this paper, we propose a new information-theory-based regularization scheme named SHADE for SHAnnon DEcay. The originality of the approach is to define a prior based on conditional entropy, which explicitly decouples the learning of invariant representations in the regularizer and the learning of correlations between inputs and labels in the data fitting term. We explain why this quantity makes our model able to achieve invariance with respect to input variations. We empirically validate the efficiency of our approach to improve classification performances compared to standard regularization schemes on several standard architectures.","pdf":"/pdf/54f5b03ce294e16c9b287ad023e86ab1b83c907b.pdf","paperhash":"anonymous|shade_shannon_decay_informationbased_regularization_for_deep_learning","_bibtex":"@article{\n  anonymous2018shade:,\n  title={SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJSVuReCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper456/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222657747,"tcdate":1511799337139,"number":1,"cdate":1511799337139,"id":"By-4qhFeM","invitation":"ICLR.cc/2018/Conference/-/Paper456/Official_Review","forum":"SJSVuReCZ","replyto":"SJSVuReCZ","signatures":["ICLR.cc/2018/Conference/Paper456/AnonReviewer2"],"readers":["everyone"],"content":{"title":"an obvious idea supported by flawed reasoning","rating":"5: Marginally below acceptance threshold","review":"This paper proposes another entropic regularization term for deep neural nets. The key idea can be stated as follows: Let X denote the observed input, C the hidden class label taking values in a finite set, and Y the representation computed by a neural net. Then C -> X -> Y is a Markov chain. Moreover, assuming that the mapping X -> Y is deterministic (as is the case with neural nets or any other deterministic representations), we can write down the mutual information between X and Y as\n\nI(X;Y) = H(Y) - H(Y|X) = H(Y).\n\nA simple manipulation shows that H(Y) = I(C;Y) + H(Y|C). The authors interpret the first term, I(C;Y), as a data fit term that quantifies the statistical correlations between the class label C and the representation Y, whereas the second term, H(Y|C), is the amount by which the representation Y can be compressed knowing the class label C. The authors then propose to 'explicitly decouple' the data-fit term I(C;Y) from the regularization penalty and focus on minimizing H(Y|C). In fact, they replace this term by the sum of conditional entropies of the form H(Y_{i,k}|C), where Y_{i,k} is the activation of the ith neuron in the kth layer of the neural net. The final step is to recognize that the conditional entropy may not admit a scalable and differentiable estimator, so they use the relation between a quantity called entropy power and second moments to replace the entropic penalty with the conditional variance penalty Var[Y_{i,k}|C]. Since the class-conditional distributions are unknown, a surrogate model Q_{Y|C} is used. The authors present some experimental results as well.\n\nHowever, this approach has a number of serious flaws. First of all, if the distribution of X is nonatomic and the mapping X -> Y is continuous (in the case of neural nets, it is even Lipschitz), then the mutual information I(X;Y) is infinite. In that case, the representation of I(X;Y) in terms of entropies is not valid -- indeed, one can write the mutual information between two jointly distributed random variables X and Y in terms of differential entropies as I(X;Y) = h(Y) - h(Y|X), but this is possible only if both terms on the right-hand side exist. This is not the case here, so, in particular, one cannot relate I(X;Y) to I(C;Y). Ironically, I(C;Y) is finite, because C takes values in a finite set, so I(C;Y) is at most the log cardinality of the set of labels. One can start, then, simply with I(C;Y) and express it as H(C) - H(C|Y). Both terms are well-defined Shannon entropies, where the first one does not depend on the representation, whereas the second one involves the representation. But then, if the goal is to _minimize_ the mutual information between I(C;Y), it makes sense to _maximize_ the conditional entropy H(C|Y). In short, the line of reasoning that leads to minimizing H(Y|C) is not convincing. Moreover, why is it a good idea to _minimize_ I(C;Y) in the first place? Shouldn't one aim to maximize it subject to structural constraints on the representation, along the lines of InfoMax?\n\nThe next issue is the chain of reasoning that leads to replacing H(Y|C) with Var[Y|C]. One could start with that instead without changing the essence of the approach, but then the magic words \"Shannon decay\" would have to disappear altogether, and the proposed method would lose all of its appeal.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning","abstract":"Regularization is a big issue for training deep neural networks. In this paper, we propose a new information-theory-based regularization scheme named SHADE for SHAnnon DEcay. The originality of the approach is to define a prior based on conditional entropy, which explicitly decouples the learning of invariant representations in the regularizer and the learning of correlations between inputs and labels in the data fitting term. We explain why this quantity makes our model able to achieve invariance with respect to input variations. We empirically validate the efficiency of our approach to improve classification performances compared to standard regularization schemes on several standard architectures.","pdf":"/pdf/54f5b03ce294e16c9b287ad023e86ab1b83c907b.pdf","paperhash":"anonymous|shade_shannon_decay_informationbased_regularization_for_deep_learning","_bibtex":"@article{\n  anonymous2018shade:,\n  title={SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJSVuReCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper456/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739294714,"tcdate":1509120044875,"number":456,"cdate":1509739292060,"id":"SJSVuReCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJSVuReCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning","abstract":"Regularization is a big issue for training deep neural networks. In this paper, we propose a new information-theory-based regularization scheme named SHADE for SHAnnon DEcay. The originality of the approach is to define a prior based on conditional entropy, which explicitly decouples the learning of invariant representations in the regularizer and the learning of correlations between inputs and labels in the data fitting term. We explain why this quantity makes our model able to achieve invariance with respect to input variations. We empirically validate the efficiency of our approach to improve classification performances compared to standard regularization schemes on several standard architectures.","pdf":"/pdf/54f5b03ce294e16c9b287ad023e86ab1b83c907b.pdf","paperhash":"anonymous|shade_shannon_decay_informationbased_regularization_for_deep_learning","_bibtex":"@article{\n  anonymous2018shade:,\n  title={SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJSVuReCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper456/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}