{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222677454,"tcdate":1511823164597,"number":2,"cdate":1511823164597,"id":"rJrHPz9lG","invitation":"ICLR.cc/2018/Conference/-/Paper508/Official_Review","forum":"BkDB51WR-","replyto":"BkDB51WR-","signatures":["ICLR.cc/2018/Conference/Paper508/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"The papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density function. A Monte Carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead prediction.\n\nThe approach is tested on two artificially generated datasets and on two real-world datasets, and compared with standard approaches such as the autoregressive model, the Kalman filter, and a regression LSTM.\n\nThe paper is quite dense and quite difficult to follow, also due to the complex notation used by the authors.\n\nThe comparison with other methods is very week, the authors compare their approach with two very simple alternatives, namely a first-order autoregressive mode and the Kalman filter.  More sophisticated should have been employed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning temporal evolution of probability distribution with Recurrent Neural Network","abstract":"We propose to tackle a time series regression problem by computing temporal evolution of a probability density function to provide a probabilisitic forecast. A Recurrent Neural Network (RNN) based model is employed to learn a nonlinear operator for temporal evolution of a probability density function. We use a softmax layer for a numerical discretization of a smooth probability density functions, which transforms a function approximation problem to a classification task. Explicit and implicit regularization strategies are introduced to impose a smoothness condition on the estimated probability distribution. A Monte Carlo procedure to compute the temporal evolution of the distribution for a multiple-step forecast is presented. The evaluation of the proposed algorithm on two synthetic and two real data sets shows advantage over the compared baselines.","pdf":"/pdf/5c38344bb275e64d151641957e727f073928fe46.pdf","TL;DR":"Proposed RNN-based algorithm to estimate predictive distribution in one- and multi-step forecasts in time series prediction problems","paperhash":"anonymous|learning_temporal_evolution_of_probability_distribution_with_recurrent_neural_network","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning temporal evolution of probability distribution with Recurrent Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkDB51WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper508/Authors"],"keywords":["predictive distribution estimation","probabilistic RNN","uncertainty in time series prediction"]}},{"tddate":null,"ddate":null,"tmdate":1512222677499,"tcdate":1511127455395,"number":1,"cdate":1511127455395,"id":"rJwjY_1xG","invitation":"ICLR.cc/2018/Conference/-/Paper508/Official_Review","forum":"BkDB51WR-","replyto":"BkDB51WR-","signatures":["ICLR.cc/2018/Conference/Paper508/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay. Unclear if this would work at all in higher-dimensional time series. It is also unclear to me what are the sources of the uncertainties captured.","rating":"6: Marginally above acceptance threshold","review":"Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay. Unclear if this would work at all in higher-dimensional time series. It is also unclear to me what are the sources of the uncertainties captured.\n\n\nThe author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series. The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts. Instead, the conditional density p(y_t|y_{1:t-1|, \\theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM. The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels. Therefore, the uncertainties produced by the model appear to be a black-box. It is probably unlikely that the discretisation method can be generalised to high-dimensional setting?\n\nQuality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth. In the experiments with real data, it's unclear how good the uncertainties produced by the model are. It may be useful to compare to the uncertainty produced by a GP with suitable kernels. In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that?\n\nClarity: The paper is well-written. The presentations of the ideas are pretty clear.\n\nOriginality: Above average. I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever.\n\nSignificance: Average. It would be excellent if the authors can extend this to higher dimensional time series.\n\nI'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning temporal evolution of probability distribution with Recurrent Neural Network","abstract":"We propose to tackle a time series regression problem by computing temporal evolution of a probability density function to provide a probabilisitic forecast. A Recurrent Neural Network (RNN) based model is employed to learn a nonlinear operator for temporal evolution of a probability density function. We use a softmax layer for a numerical discretization of a smooth probability density functions, which transforms a function approximation problem to a classification task. Explicit and implicit regularization strategies are introduced to impose a smoothness condition on the estimated probability distribution. A Monte Carlo procedure to compute the temporal evolution of the distribution for a multiple-step forecast is presented. The evaluation of the proposed algorithm on two synthetic and two real data sets shows advantage over the compared baselines.","pdf":"/pdf/5c38344bb275e64d151641957e727f073928fe46.pdf","TL;DR":"Proposed RNN-based algorithm to estimate predictive distribution in one- and multi-step forecasts in time series prediction problems","paperhash":"anonymous|learning_temporal_evolution_of_probability_distribution_with_recurrent_neural_network","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning temporal evolution of probability distribution with Recurrent Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkDB51WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper508/Authors"],"keywords":["predictive distribution estimation","probabilistic RNN","uncertainty in time series prediction"]}},{"tddate":null,"ddate":null,"tmdate":1509739265100,"tcdate":1509124670691,"number":508,"cdate":1509739262428,"id":"BkDB51WR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkDB51WR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning temporal evolution of probability distribution with Recurrent Neural Network","abstract":"We propose to tackle a time series regression problem by computing temporal evolution of a probability density function to provide a probabilisitic forecast. A Recurrent Neural Network (RNN) based model is employed to learn a nonlinear operator for temporal evolution of a probability density function. We use a softmax layer for a numerical discretization of a smooth probability density functions, which transforms a function approximation problem to a classification task. Explicit and implicit regularization strategies are introduced to impose a smoothness condition on the estimated probability distribution. A Monte Carlo procedure to compute the temporal evolution of the distribution for a multiple-step forecast is presented. The evaluation of the proposed algorithm on two synthetic and two real data sets shows advantage over the compared baselines.","pdf":"/pdf/5c38344bb275e64d151641957e727f073928fe46.pdf","TL;DR":"Proposed RNN-based algorithm to estimate predictive distribution in one- and multi-step forecasts in time series prediction problems","paperhash":"anonymous|learning_temporal_evolution_of_probability_distribution_with_recurrent_neural_network","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning temporal evolution of probability distribution with Recurrent Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkDB51WR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper508/Authors"],"keywords":["predictive distribution estimation","probabilistic RNN","uncertainty in time series prediction"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}