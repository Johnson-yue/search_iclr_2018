{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222565045,"tcdate":1511890750499,"number":3,"cdate":1511890750499,"id":"SyUH1Qjez","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps. The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps. The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks.\n\n\n\nStrength:\n- The experimental results on the simple skip RNNs have shown a good improvement over the previous results.\n\nWeakness:\n- Although the paper shows that skip RNN worked well, I found the appropriate baseline is lacking here. Comparable baselines, I believe, are regular LSTM/GRU whose inputs are randomly dropped out during training.\n\n- Most of the experiments in the main paper are on toy tasks with small LSTMs. I thought the main selling point of the method is the computational gain. Would it make more sense to show that on large RNNs with thousands of hidden units? After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/b0b762183ef5021ec9cbb30107b66a7ecc2b3f60.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1512222565085,"tcdate":1511753705761,"number":2,"cdate":1511753705761,"id":"BkfgO-FgG","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea. requires more experiment to be convincing.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an idea to do faster RNN inference via skip RNN state updates. \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance. But the experiments are not convincing enough. First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task. I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters. Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim.\n\nMinor -- \nCite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/b0b762183ef5021ec9cbb30107b66a7ecc2b3f60.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1512222565126,"tcdate":1511667178652,"number":1,"cdate":1511667178652,"id":"HkmeI2vxM","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting model, but lacking comparison to prior work","rating":"5: Marginally below acceptance threshold","review":"The paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.\n\nPros:\n- Task of reducing computation by skipping inputs is interesting\n- Model is novel and interesting\n- Experiments on multiple tasks and datasets confirm the efficacy of the method\n- Skipping behavior can be controlled via an auxiliary loss term\n- Paper is clearly written\n\nCons:\n- Missing comparison to prior work on sequential MNIST\n- Low performance on Charades dataset, no comparison to prior work\n- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification\n\nThe task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.\n\nHowever, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work.\n\nOn permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task.\n\nFor Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on “fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge”, and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don’t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.\n\nIn a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks.\n\nThough experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results.\n\nOn the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author’s own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.\n\n\nReferences\n\n[1] Le et al, “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”, arXiv 2015\n[2] Arjovsky et al, “Unitary Evolution Recurrent Neural Networks”, ICML 2016\n[3] Cooijmans et al, “Recurrent Batch Normalization”, ICLR 2017\n[4] Zhang et al, “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS 2016\n[5] Sigurdsson et al, “Hollywood in homes: Crowdsourcing data collection for activity understanding”, ECCV 2016\n[6] Sigurdsson et al, “Asynchronous temporal fields for action recognition”, CVPR 2017","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/b0b762183ef5021ec9cbb30107b66a7ecc2b3f60.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1510921663759,"tcdate":1510908834096,"number":1,"cdate":1510908834096,"id":"B15iQQ2kf","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"HkTO-f31G","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"reply to question about Equation 1","comment":"Thanks for pointing this out. Throughout the description of the model we use a generic parametric state transition model, S. Please note that S is not the transition model for the vanilla RNN, but a \"placeholder\" to be filled with the transition model of the RNN architecture being used (LSTM or GRU for the experiments in the paper). Should you use the vanilla RNN, S would be exactly what you wrote -- including the non-linearity. This formulation is similar to the one in [1] and abstracts our proposal from the underlying RNN cell.\n\n[1] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/b0b762183ef5021ec9cbb30107b66a7ecc2b3f60.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1509739473163,"tcdate":1509010991173,"number":121,"cdate":1509739470504,"id":"HkwVAXyCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/b0b762183ef5021ec9cbb30107b66a7ecc2b3f60.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}