{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222584710,"tcdate":1512092401565,"number":3,"cdate":1512092401565,"id":"HkcxQ4Rxf","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper, accept","rating":"7: Good paper, accept","review":"This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training.\n\nI have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted.\n\n\nSpecific comments/questions follow:\n\n\nFigure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width.\n\nPage 7: \"In the supervised setting...\" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7).\n\nIs the \"Karel DSL\" in your experiments the full Karel language, or a subset designed for the paper?\n\nFor the versions of the model that use beam search, what beam width was used? Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/cd2c37c8ae72ba3f17cac85209151475bddc13c3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1512222584752,"tcdate":1511904311368,"number":2,"cdate":1511904311368,"id":"H1JSNUjeG","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The paper presents a reinforcement learning-based approach for program synthesis. The proposed approach claims two advantages over a baseline maximum likelihood estimation-based approach. MLE-based methods penalize syntactically different but semantically equivalent programs. Further, typical program synthesis approaches don't explicitly learn to produce correct syntax. The proposed approach uses a syntax-checker to limit the next-token distribution to syntactically-valid tokens.\n\nThe approach, and its constituent contributions, i.e. of using RL for program synthesis, and limiting to syntactically valid programs, are novel. Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas.\n\nThe paper presents comparisons with baseline methods. The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results.  However, there is no comparison with other approaches in the literature. The authors claim to improve the state-of-the-art, but fail to mention and compare with the state-of-the-art, such as [1]. I do find it hard to trust papers which do not compare with results from other papers.\n\nPros:\n1. Well-written paper, with clear contributions.\n2. Good empirical evaluation with ablations.\n\nCons:\n1. No SOTA comparison.\n2. Only one task / No real-world task, such as Excel Flashfill.\n\n[1]: \"Neural Program Meta-Induction\", Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/cd2c37c8ae72ba3f17cac85209151475bddc13c3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1512222584793,"tcdate":1511841643706,"number":1,"cdate":1511841643706,"id":"Hk4_Jw9xG","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper, could be more clearly written.","rating":"5: Marginally below acceptance threshold","review":"The authors consider the task of program synthesis in the Karel DSL. Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs. Finally they learn a model that predicts correctness of syntax in absence of a syntax checker. \n\nWhile the results in this paper look good, I found many aspects of the exposition difficult to follow. In section 4, the authors define objectives, but do not clearly describe how these objectives are optimized, instead relying on the read to infer from context how REINFORCE and beam search are applied. I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer. The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why. \n\nOn the whole this seems like a promising paper. That said, I think the authors would need to convincingly address issues of clarity in order for this to appear. \n\nSpecific comments \n\n- Figure 2 is too small \n\n- Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward, rather than an estimator of the gradient of the expected reward (which is what REINFORCE is). \n\n- It is not clear the how beam search is carried out. In equation (10) there appear to be two problems. The first is that the index i appears twice (once in i=1..N and once in i \\in 1..C), the second is that λ_r refers to an index that does not appear. More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic. What is the heuristic here? Is syntax checking used at each step of token generation, or something along these lines? \n \n- What is the value of the learned syntax in section 5.2? Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/cd2c37c8ae72ba3f17cac85209151475bddc13c3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1509739444556,"tcdate":1509047643423,"number":175,"cdate":1509739441903,"id":"H1Xw62kRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/cd2c37c8ae72ba3f17cac85209151475bddc13c3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}