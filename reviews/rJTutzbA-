{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222809103,"tcdate":1511838326313,"number":2,"cdate":1511838326313,"id":"Sk0uMIqef","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Review","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Weak Reject","rating":"5: Marginally below acceptance threshold","review":"I wonder how the ASGD compares to other optimization schemes applicable to DL, like Entropy-SGD, which is yet another algorithm that provably improves over SGD. This question is also valid when it comes to other optimization schemes that are designed for deep learning problems. For instance, Entropy-SGD and Path-SGD should be mentioned and compared with. As a consequence, the literature analysis is insufficient. \n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, denoted as ASGD, is a simple to implement stochastic algorithm, based on a relatively less popular version of Nesterov's AGD. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/2a3ed17757044e13a22615ff1fa39608b8ac0d46.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1512222809142,"tcdate":1511644867830,"number":1,"cdate":1511644867830,"id":"Sy3aR8wxz","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Review","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice idea, Like the paper","rating":"7: Good paper, accept","review":"I like the idea of the paper. Momentum and accelerations are proved to be very useful both in deterministic and stochastic optimization. It is natural that it is understood better in the deterministic case. However, this comes quite naturally, as deterministic case is a bit easier ;) Indeed, just recently people start looking an accelerating in stochastic formulations. There is already accelerated SVRG, Jain et al 2017, or even Richtarik et al (arXiv: 1706.01108, arXiv:1710.10737).\n\nI would somehow split the contributions into two parts:\n1) Theoretical contribution: Proposition 3 (+ proofs in appendix)\n2) Experimental comparison.\n\nI like the experimental part (it is written clearly, and all experiments are described in a lot of detail).\n\nI really like the Proposition 3 as this is the most important contribution of the paper. (Indeed, Algorithms 1 and 2 are for reference and Algorithm 3 was basically described in Jain, right?). \n\nSignificance: I think that this paper is important because it shows that the classical HB method cannot achieve acceleration in a stochastic regime.\n\nClarity: I was easy to read the paper and understand it.\n\nFew minor comments:\n1. Page 1, Paragraph 1: It is not known only for smooth problems, it is also true for simple non-smooth (see e.g. https://link.springer.com/article/10.1007/s10107-012-0629-5)\n2. In abstract : Line 6 - not completely true, there is accelerated SVRG method, i.e. the gradient is not exact there, also see Recht (https://arxiv.org/pdf/1701.03863.pdf) or Richtarik et al (arXiv: 1706.01108, arXiv:1710.10737) for some examples where acceleration can be proved when you do not have an exact gradient.\n3. Page 2, block \"4\" missing \".\" in \"SGD We validate\"....\n4. Section 2. I think you are missing 1/2 in the definition of the function. Otherwise, you would have a constant \"2\" in the Hessian, i.e. H= 2 E[xx^T]. So please define the function as  f_i(w) = 1/2 (y - <w,x_i>)^2. The same applies to Section 3.\n5. Page 6, last line, .... was downloaded from \"pre\". I know it is a link, but when printed, it looks weird. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, denoted as ASGD, is a simple to implement stochastic algorithm, based on a relatively less popular version of Nesterov's AGD. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/2a3ed17757044e13a22615ff1fa39608b8ac0d46.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD"]}},{"tddate":null,"ddate":null,"tmdate":1510092386323,"tcdate":1509136757090,"number":890,"cdate":1510092362740,"id":"rJTutzbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, denoted as ASGD, is a simple to implement stochastic algorithm, based on a relatively less popular version of Nesterov's AGD. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/2a3ed17757044e13a22615ff1fa39608b8ac0d46.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}