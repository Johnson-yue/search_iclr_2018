{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222796059,"tcdate":1511902124122,"number":3,"cdate":1511902124122,"id":"HyE2oHixz","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official review","rating":"6: Marginally above acceptance threshold","review":"The paper is well written and easy to follow. I find the results very interesting. In particular the paper shows that many properties of GAN (or generative) models (e.g. interpolation, feature arithmetic) are a in great deal result of the inductive bias of deep CNN’s and can be obtained with simple reconstruction losses. \n\nThe results on CelebA seem quite remarkable for training examples (e.g. interpolation). Samples are quite good but inferior to GANs, but still impressive for the simplicity of the model. The results on SUN are a bit underwhelming, but still deliver the point reasonably well in my view. Naturally, the paper would make a much stronger claim showing good results on different datasets. \n\nThe authors mentioned that the current method can recover all the solutions that could be found by an autoencoder and reach some others. It would be very interesting to empirically explore this statement. Specifically, my intuition is that if we train a traditional autoencoder (with normalization of the latent space to match this setting) and compute the corresponding z vectors for each element in the dataset, the loss function (1) would be lower than that achieved with the proposed model. If that is true, the way of solving the problem is helping find a solution that prevents overfitting. \n\nFollowing with the previous point, the authors mention that different initializations were used for the z vectors in the case of CelebA and LSUN. Does this lead to significantly different results? What would happen if the z values were initialized say with the representations learned by a fully trained deterministic autoencoder (with the normalization as in this work)? It would be good to report and discuss these alternatives in terms of loss function and results (e.g. quality of the samples). \n\nIt seems natural to include VAE baselines (using both of the losses in this work). Also, recent works have used ‘perceptual losses’, for instance for building VAE’s capable of generating sharper images:\n\nLamb, A., et al (2016). Discriminative regularization for generative models. arXiv preprint arXiv:1602.03220.\n\nIt would be good to compare these results with those presented in this work. One could argue that VAE’s are also mainly trained via a regularized reconstruction loss. Conversely, the proposed method can be thought as a form of autoencoder. The encoder could be thought to be implicitly defined by the optimization procedure used to recover the latent vectors in GAN's. Using explicit variables for each image would be a way of solving the optimization problem.\n\nIt would be informative to also shows reconstruction and interpolation results for a set of ‘held out’ images. Where the z values would be found as with GANs. This would test the coverage of the method and might be a way of making the comparison with GANs more relevant. \n\nThe works:\n\nNguyen, Anh, et al. \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.\" Advances in Neural Information Processing Systems. 2016.\n\nHan, Tian, et al. \"Alternating Back-Propagation for Generator Network.\" AAAI. 2017.\n\nSeems very related.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/f5ce598a90419ff6cb0040105fe25a7f930f2834.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222796099,"tcdate":1511802915972,"number":2,"cdate":1511802915972,"id":"SynXdTKeM","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper is a potentially interesting alternative training procedure to GANs.","rating":"6: Marginally above acceptance threshold","review":"In this paper, the authors propose a new architecture for generative neural networks. Rather than the typical adversarial training procedure used to train a generator and a discriminator, the authors train a generator only. To ensure that noise vectors get mapped to images from the target distribution, the generator is trained to map noise vectors to the set of training images as closely as possible. Both the parameters of the generator and the noise vectors themselves are optimized during training. \n\nOverall, I think this paper is useful. The images generated by the model are not (qualitatively and in my opinion) as high quality as extremely recent work on GANs, but do appear to be better than those produced by DCGANs. More importantly than the images produced, however, is the novel training procedure. For all of their positive attributes, the adversarial training procedure for GANs is well known to be fairly difficult to deal with. As a result, the insight that if a mapping from noise vectors to training images is learned directly, other noise images still result in natural images is interesting.\n\nHowever, I do have a few questions for the authors, mostly centered around the choice of noise vectors.\n\nIn the paper, you mention that you \"initialize the z by either sampling them from a Gaussian distribution or by taking the whitened PCA of the raw image pixels.\" What does this mean? Do you sample them from a Gaussian on some tasks, and use PCA on others? Is it fair to assume from this that the initialization of z during training matters? If so, why?\n\nAfter training, you mention that you fit a full Gaussian to the noise vectors learned during training and sample from this to generate new images. I would be interested in seeing some study of the noise vectors learned during training. Are they multimodal, or is a unimodal distribution indeed sufficient? Does a Gaussian do a good job (in terms of likelihood) of fitting the noise vectors, or would some other model (even something like kernel density estimation) allow for higher probability noise vectors (and therefore potentially higher quality images) to be drawn? Does the choice of distribution even matter, or do you think uniform random vectors from the space would produce acceptable images?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/f5ce598a90419ff6cb0040105fe25a7f930f2834.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222796144,"tcdate":1511799117545,"number":1,"cdate":1511799117545,"id":"BkILtntlz","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer1"],"readers":["everyone"],"content":{"title":"OPTIMIZING THE LATENT SPACE OF GENERATIVE NETWORKS","rating":"4: Ok but not good enough - rejection","review":"Summary: The authors observe that the success of GANs can be attributed to two factors; leveraging the inductive bias of deep CNNs and the adversarial training protocol. In order to disentangle the factors of success, and they propose to eliminate the adversarial training protocol while maintaining the first factor. The proposed Generative Latent Optimization (GLO) model maps a learnable noise vector to the real images of the dataset by minimizing a reconstruction loss. The experiments are conducted on CelebA and LSUN-Bedroom datasets. \n\nStrengths: \nThe paper is well written and the topic is relevant for the community.\nThe notations are clear, as far as I can tell, there are no technical errors.\nThe design choices are well motivated in Chapter 2 which makes the main idea easy to grasp. \nThe image reconstruction results are good. \nThe experiments are conducted on two challenging datasets, i.e. CelebA and LSUN-Bedroom.\n\nWeaknesses:\nA relevant model is Generative Moment Matching Network (GMMN) which can also be thought of as a “discriminator-less GAN”. However, the paper does not contrast GLO with GMMN either in the conceptual level or experimentally. \n\nAnother relevant model is Variational Autoencoders (VAE) which also learns the data distribution through a learnable latent representation by minimizing a reconstruction loss. The paper would be more convincing if it provided a comparison with VAE.\n\nIn general, having no comparisons with other models proposed in the literature as improvements over GAN such as ACGAN, InfoGAN, WGAN weakens the experimental section.\n\nThe evaluation protocol is quite weak: CelebA images are 128x128 while LSUN images are 64x64. Especially since it is a common practice nowadays to generate much higher dimensional images, i.e. 256x256, the results presented in this paper appear weak. \n\nAlthough the reconstruction examples (Figure 2 and 3) are good, the image generation results (Figure 4 and 5) are worse than GAN, i.e. the 3rd images in the 2nd row in Figure 4 for instance has unrealistic artifacts, the entire Figure 5 results are quite boxy and unrealistic. The authors mention in Section 3.3.2 that they leave the careful modeling of Z to future work, however the paper is quite incomplete without this.\n\nIn Section 3.3.4, the authors claim that the latent space that GLO learns is interpretable. For example, smiling seems correlated with the hair color in Figure 6. This is a strong claim based on one example, moreover the evidence of this claim is not as obvious (based on the figure) to the reader. Moreover, in Figure 8, the authors claim that the principal components of the GLO latent space is interpretable. However it is not clear from this figure what each eigenvector generates. The authors’ observations on Figure 8 and 9 are not clearly visible through manual inspection.  \n\nFinally, as a minor note, the paper has some vague statements such as\n“A linear interpolation in the noise space will generate a smooth interpolation of visually-appealing images”\n“Several works attempt at recovering the latent representation of an image with respect to a generator.”\nTherefore, a careful proofreading would improve the exposition. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/f5ce598a90419ff6cb0040105fe25a7f930f2834.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739067739,"tcdate":1509136050552,"number":849,"cdate":1509739065072,"id":"ryj38zWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/f5ce598a90419ff6cb0040105fe25a7f930f2834.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}