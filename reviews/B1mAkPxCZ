{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222614335,"tcdate":1511864972718,"number":2,"cdate":1511864972718,"id":"SyBcch5lM","invitation":"ICLR.cc/2018/Conference/-/Paper285/Official_Review","forum":"B1mAkPxCZ","replyto":"B1mAkPxCZ","signatures":["ICLR.cc/2018/Conference/Paper285/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes a semantic approach for data augmentation, but the experiments are weak/unconvincing at this state","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample . Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space. For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features); b) perturbed decoded image; c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space.   \nThe idea is nice and simple, however the current framework has several weaknesses:\n1. The whole pipeline has three (neural network) components: a) input image features are extracted from VGG net pre-trained on auxiliary data; 2) auto-encoder that is trained on data for one-shot learning; 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers. This three networks need to be clearly described; ideally combined into one end-to-end training pipeline.\n2. The empirical performance is very poor. If you look into literature for zero shot learning, work by Z. Akata in CVPR 2015, CVPR2016, the performance on AwA and on CUB-bird goes way above 50%, where in the current paper it is 30.57% and 8.21% at most (for the most recent survey on zero shot learning papers using attribute embeddings, please, refer to Zero-Shot Learning - The Good, the Bad and the Ugly by Xian et al, CVPR 2017). It is important to understand, why there is such a big drop in performance in one-shot learning comparing to zero-shot learning? One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes). In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?). I am not sure, how can the auto-encoder model not overfit completely to the training data instances. Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting. \nOverall, I like the idea, so I am leaning towards accepting the paper, but the empirical evaluations are not convincing. \n\n \n\n \n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING","abstract":"A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ","pdf":"/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf","paperhash":"anonymous|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning","_bibtex":"@article{\n  anonymous2018vocabulary-informed,\n  title={VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1mAkPxCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper285/Authors"],"keywords":["vocabulary-informed learning","data augmentation"]}},{"tddate":null,"ddate":null,"tmdate":1512222614382,"tcdate":1511831570311,"number":1,"cdate":1511831570311,"id":"Sk5zOVceG","invitation":"ICLR.cc/2018/Conference/-/Paper285/Official_Review","forum":"B1mAkPxCZ","replyto":"B1mAkPxCZ","signatures":["ICLR.cc/2018/Conference/Paper285/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Method needs to be clarified; experiments need to be improved","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a feature augmentation method for one-shot learning.  The proposed approach is very interesting. However, the method needs to be further clarified and the experiments need to be improved. \n\nDetails:\n1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read. \n\n2. The approach:\n(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3.  However, under one-shot learning, won’t this  make each class still have only one instance for training? \n\n(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation? \n\n(3) It is not clear how will the vocabulary-information be exploited? In particular, how to ensure the semantic space u to be same as the vocabulary semantic space? How to generate the neighborhood in Neigh(\\hat{u}_i) on page 5? \n\n3.  In the experiments: \n(1) The authors didn’t compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing. \n\n(2) The results are reported for different numbers of augmented instances. Clarification is needed. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING","abstract":"A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ","pdf":"/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf","paperhash":"anonymous|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning","_bibtex":"@article{\n  anonymous2018vocabulary-informed,\n  title={VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1mAkPxCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper285/Authors"],"keywords":["vocabulary-informed learning","data augmentation"]}},{"tddate":null,"ddate":null,"tmdate":1509739385642,"tcdate":1509089226659,"number":285,"cdate":1509739382989,"id":"B1mAkPxCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1mAkPxCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING","abstract":"A natural solution for one-shot learning is to augment training data to handle the data deficiency problem. However, directly augmenting in the image domain may not necessarily generate training data that sufficiently explore the intra-class space for one-shot classification. Inspired by the recent vocabulary-informed learning, we propose to generate synthetic training data with the guide of the semantic word space. Essentially, we train an auto-encoder as a bridge to enable the transformation between the image feature space and the semantic space. Besides directly augmenting image features, we transform the image features to semantic space using the encoder and perform the data augmentation. The decoder then synthesizes the image features for the augmented instances from the semantic space. Experiments on three datasets show that our data augmentation method effectively improves the performance of one-shot classification. An extensive study shows that data augmented from semantic space are complementary with those from the image space, and thus boost the classification accuracy dramatically. Source code and dataset will be available. ","pdf":"/pdf/e2adf5f6451fb60229f6bfd75e678b1a28d6e1ae.pdf","paperhash":"anonymous|vocabularyinformed_visual_feature_augmentation_for_oneshot_learning","_bibtex":"@article{\n  anonymous2018vocabulary-informed,\n  title={VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1mAkPxCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper285/Authors"],"keywords":["vocabulary-informed learning","data augmentation"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}