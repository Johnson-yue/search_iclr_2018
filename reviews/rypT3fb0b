{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222829298,"tcdate":1512124935161,"number":3,"cdate":1512124935161,"id":"rkJfM20eG","invitation":"ICLR.cc/2018/Conference/-/Paper973/Official_Review","forum":"rypT3fb0b","replyto":"rypT3fb0b","signatures":["ICLR.cc/2018/Conference/Paper973/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An incremental improvement for compressing deep neural networks","rating":"7: Good paper, accept","review":"The authors propose to use the group ordered weighted l1 regulariser (GrOWL) combined with clustering of correlated features to select and tie parameters, leading to a sparser representation with a reduced parameter space. They apply the proposed method two well-known benchmark datasets under a fully connected and a convolutional neural network, and demonstrate that in the former case a slight improvement in accuracy can be achieved, while in the latter, the method performs similar to the group-lasso, but at a reduced computational cost for classifying new images due to increased compression of the network.\n\nThe paper is well written and motivated, and the idea seems fairly original, although the regularisation approach itself is not new. Like many new approaches in this field, it is hard to judge from this paper and its two applications alone whether the approach will lead to significant benefits in general, but it certainly seems promising.\n\nPositive points:\n- Demonstrated improved compression with similar performance to the standard weighted decay method.\n- Introduced a regularization technique that had not been previously used in this field, and that improves on the group lasso in terms of compression, without apparent loss of accuracy.\n- Applied an efficient proximal gradient algorithm to train the model.\n\nNegative points:\n- The method is sold as inducing a clustering, but actually, the clustering is a separate step, and the choice of clustering algorithm might well have an influence on the results. It would have been good to see more discussion or exploration of this. I would not claim that, for example, the fused lasso is a clustering algorithm for regression coefficients, even though it demonstrably sets some coefficients to the same value, so it seems wrong to imply the same for GrOWL.\n- In the example applications, it is not clear how the accuracy was obtained (held-out test set? cross-validation?), and it would have been good to get an estimate for the variance of this quantity, to see if the differences between methods are actually meaningful (I suspect not). Also, why is the first example reporting accuracy, but the second example reports error?\n- There is a slight contradiction in the abstract, in that the method is introduced as guarding against overfitting, but then the last line states that there is \"slight or even no loss on generalization performance\". Surely, if we reduce overfitting, then by definition there would have to be an improvement in generalization performance, so should we draw the conclusion that the method has not actually been demonstrated to reduce overfitting?\n\nMinor point:\n- p.5, in the definition of prox_Q(epsilon), the subscript for the argmin should be nu, not theta.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING","abstract":"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. Moreover, although this complexity allows DNNs to learn complex mappings, it may also allow them to over-fit, which has motivated the use of regularization methods, e.g., by encouraging weight sparsity. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.\n","pdf":"/pdf/ddf071a64463033d5f09bd246e47c1ecc358a3f6.pdf","TL;DR":"We have proposed using the recent GrOWL regularizer for simultaneous parameter sparsity and tying in DNN learning. ","paperhash":"anonymous|learning_to_share_simultaneous_parameter_tying_and_sparsification_in_deep_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rypT3fb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper973/Authors"],"keywords":["Compressing neural network","simultaneously parameter tying and sparsification","group ordered l1 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222829340,"tcdate":1511910560326,"number":2,"cdate":1511910560326,"id":"rkPj2vjeM","invitation":"ICLR.cc/2018/Conference/-/Paper973/Official_Review","forum":"rypT3fb0b","replyto":"rypT3fb0b","signatures":["ICLR.cc/2018/Conference/Paper973/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A nice paper that would be more compelling with a comparison with the group elastic net","rating":"5: Marginally below acceptance threshold","review":"SUMMARY\nThe paper proposes to apply GrOWL regularization to the tensors of parameters between each pair of layers. The groups are composed of all coefficients associated to inputs coming from the same neuron in the previous layer. The proposed algorithm is a simple proximal gradient algorithm using the proximal operator of the GrOWL norm. Given that the GrOWL norm tend to empirically reinforce a natural clustering of the vectors of coefficients which occurs in some layers, the paper proposes to cluster the corresponding parameter vectors, to replace them with their centroid and to retrain with the constrain that some vectors are now equal. Experiments show that some sparsity is obtained by the model and that together with the clustering and high compression of the model is obtained which maintaining or improving over a good level of generalization accuracy. In comparison, plain group Lasso yields compressed versions that are too sparse, and tend to degrade performance. The method is also competitive with weight decay with much better compression.\n\nREVIEW\nGiven the well known issue that the Lasso tends to select arbitrarily and in a non stable way variables\nthat are correlated *but* given that the well known elastic-net (and conceptually simpler than GrOWL) was proposed to address that issue already more than 10 years ago, it would seem relevant to compare the proposed method with the group elastic-net.\n\nThe proposed algorithm is a simple proximal gradient algorithm, but since the objective is non-convex it would be relevant to provide references for convergence guarantees of the algorithm.\n\nHow should the step size eta be chosen? I don't see that this is discussed in the paper.\n\nIn the clustering algorithm how is the threshold value chosen?\n\nIs it chosen by cross validation?\n\nIs the performance better with clustering or without?\n\nIs the same threshold chosen for GrOWL and the Lasso?\n\nIt would be useful to know which values of p, Lambda_1 and Lambda_2 are selected in the experiments?\n\nFor Figures 5,7,8,9 given that the matrices do not have particular structures that need to be visualized but that the important thing to compare is the distribution of correlation between pairs, these figures that are hard to read and compare would be advantageously replaced by histograms of the values of the correlations between pairs (of different variables). Indeed, right now one must rely on comparison of shades of colors in the thin lines that display correlation and it is really difficult to appreciate how much of correlation of what level are present in each Figure. Histograms would extract exactly the relevant information...\n\nA brief description of affinity propagation, if only in the appendix, would be relevant.\nWhy this method as opposed to more classical agglomerative clustering?\n\nA brief reminder of what the principle of weight decay is would also be relevant for the paper to more self contained.\n\nThe proposed experiments are compelling, except for the fact that it would be nice to have a comparison with the group elastic-net. \n\nI liked figure 6.d and would vote for inclusion in the main paper.\n\n\nTYPOS etc \n\n3rd last line of sec. 3.2 can fail at selecting -> fail to select\n\nIn eq. (5) theta^t should be theta^{(t)}\n\nIn section 4.1 you that the network has a single fully connected layer of hidden units -> what you mean is that the network has a single hidden layer, which is furthermore fully connected.\n\nYou cite several times Sergey (2015) in section 4.2. It seems you have exchanged first name and last name plus the corresponding reference is quite strange.\n\nAppendix B line 5 \", while.\" -> incomplete sentence.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING","abstract":"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. Moreover, although this complexity allows DNNs to learn complex mappings, it may also allow them to over-fit, which has motivated the use of regularization methods, e.g., by encouraging weight sparsity. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.\n","pdf":"/pdf/ddf071a64463033d5f09bd246e47c1ecc358a3f6.pdf","TL;DR":"We have proposed using the recent GrOWL regularizer for simultaneous parameter sparsity and tying in DNN learning. ","paperhash":"anonymous|learning_to_share_simultaneous_parameter_tying_and_sparsification_in_deep_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rypT3fb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper973/Authors"],"keywords":["Compressing neural network","simultaneously parameter tying and sparsification","group ordered l1 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222829383,"tcdate":1511691370290,"number":1,"cdate":1511691370290,"id":"H1fONf_gG","invitation":"ICLR.cc/2018/Conference/-/Paper973/Official_Review","forum":"rypT3fb0b","replyto":"rypT3fb0b","signatures":["ICLR.cc/2018/Conference/Paper973/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Unclear motivation and insufficient experimental results","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to apply a group ordered weighted l1 (GrOWL) regularization term to promote sparsity and parameter sharing in training deep neural networks and hence compress the model to a light version.\n\nThe GrOWL regularizer (Oswal et al., 2016) penalizes the sorted l2 norms of the rows in a parameter matrix with corresponding ordered regularization strength and the effect is similar to the OSCAR (Bondell & Reich, 2008) method that encouraging similar (rows of) features to be grouped together.\n\nA two-step method is used that the regularizer is applied to a deep neural network at the initial training phase, and after obtaining the parameters, a clustering method is then adopted to force similar parameters to share the same values and then the compacted neural network is retrained. The major concern is that a much more complicated neural network (with regularizations) has already been trained and stored to obtain the uncompressed parameters. What’s the benefit of the compression and retraining the trained neural network?\n\nIn the experiments, the performance of the uncompressed neural network should be evaluated to see how much accuracy loss the regularized methods have. Moreover, since the compressed network loses accuracy, will a smaller neural network can actually achieve similar performance compared to the compressed network from a larger network? If so, one can directly train a smaller network (with similar number of parameters as the compressed network) instead of using a complex two-step method, because the two-step method has to train the original larger network at the first step.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING","abstract":"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. Moreover, although this complexity allows DNNs to learn complex mappings, it may also allow them to over-fit, which has motivated the use of regularization methods, e.g., by encouraging weight sparsity. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.\n","pdf":"/pdf/ddf071a64463033d5f09bd246e47c1ecc358a3f6.pdf","TL;DR":"We have proposed using the recent GrOWL regularizer for simultaneous parameter sparsity and tying in DNN learning. ","paperhash":"anonymous|learning_to_share_simultaneous_parameter_tying_and_sparsification_in_deep_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rypT3fb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper973/Authors"],"keywords":["Compressing neural network","simultaneously parameter tying and sparsification","group ordered l1 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1510092382988,"tcdate":1509137634822,"number":973,"cdate":1510092360943,"id":"rypT3fb0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rypT3fb0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING","abstract":"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. Moreover, although this complexity allows DNNs to learn complex mappings, it may also allow them to over-fit, which has motivated the use of regularization methods, e.g., by encouraging weight sparsity. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.\n","pdf":"/pdf/ddf071a64463033d5f09bd246e47c1ecc358a3f6.pdf","TL;DR":"We have proposed using the recent GrOWL regularizer for simultaneous parameter sparsity and tying in DNN learning. ","paperhash":"anonymous|learning_to_share_simultaneous_parameter_tying_and_sparsification_in_deep_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rypT3fb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper973/Authors"],"keywords":["Compressing neural network","simultaneously parameter tying and sparsification","group ordered l1 regularization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}