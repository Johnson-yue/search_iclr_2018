{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222776254,"tcdate":1511867854539,"number":3,"cdate":1511867854539,"id":"H1U0Hpcef","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting diagnosis but a rudimentary cure","rating":"6: Marginally above acceptance threshold","review":"The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization. When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning. The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \"strong\" penalty parameter. In their experimental results, the phase transition is not observed anymore with their protocol. This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol.\n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked. The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem. I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions. I think that the problem would be better handled that way than with the proposed strategy, which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases.\n\nTo summarize, I believe that the paper addresses an important point, but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere.\n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9) \n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper \n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the training completely fails if the regularization strength goes beyond it. We propose a time-dependent regularization schedule in order to moderate the tolerance level. Experiments show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improve both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/92562736be9db1d6288bc789fded59162b1b27b3.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222776297,"tcdate":1511803687302,"number":2,"cdate":1511803687302,"id":"BJk4jatgM","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors propose a modification on regularization weights in an attempt to impose very strong regularization on backpropataion. There are several issues with the proposal as well as its efficacy in practice. The current statues of the paper needs more work.","rating":"4: Ok but not good enough - rejection","review":"The paper is well motivated and written. However, there are several issues.\n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems. Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks. The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel. Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)? \n1b. Why should we keep increasing the regularization constant beyond a limit? Is this for compressing the networks (for which there are alternate procedures), or anything else. In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers? Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)? \n2. The proposed experiments are not very conclusive. Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks. Secondly, more datasets including imagenet needs to be tested. Unless these two are done, we cannot assertively say that the proposal seems to do interesting things. Thirdly, it is not clear what Figure 5 means in terms of goodness of learning. And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the training completely fails if the regularization strength goes beyond it. We propose a time-dependent regularization schedule in order to moderate the tolerance level. Experiments show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improve both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/92562736be9db1d6288bc789fded59162b1b27b3.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222776337,"tcdate":1511735467754,"number":1,"cdate":1511735467754,"id":"rk43lpulz","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors proposed to apply L1/L2 regularization in training of deep neural networks after 5-10 epochs. ","rating":"6: Marginally above acceptance threshold","review":"The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks. Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16. The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.\n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on.\n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the training completely fails if the regularization strength goes beyond it. We propose a time-dependent regularization schedule in order to moderate the tolerance level. Experiments show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improve both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/92562736be9db1d6288bc789fded59162b1b27b3.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1509739082148,"tcdate":1509135475125,"number":822,"cdate":1509739079484,"id":"Bys_NzbC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the training completely fails if the regularization strength goes beyond it. We propose a time-dependent regularization schedule in order to moderate the tolerance level. Experiments show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improve both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/92562736be9db1d6288bc789fded59162b1b27b3.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}