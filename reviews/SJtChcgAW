{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222625410,"tcdate":1511720946987,"number":3,"cdate":1511720946987,"id":"Skse_Ydxz","invitation":"ICLR.cc/2018/Conference/-/Paper360/Official_Review","forum":"SJtChcgAW","replyto":"SJtChcgAW","signatures":["ICLR.cc/2018/Conference/Paper360/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper provides an interesting but incomplete analysis of a NetTrim-inspired pruning algorithm.","rating":"5: Marginally below acceptance threshold","review":"This paper casts the pruning optimization problem of NetTrim as a difference of convex problems, and uses DCA to obtain the smaller weight matrix; this algorithm is also analyzed theoretically to provide a bound on the generalization error of the pruned network.\n\nHowever, there are many questions that aren't answered in the paper that make it difficult to evaluate: in particular, some experimental results leave open more questions for performance analysis. \n\nQuality: of good quality, but incomplete.\nClarity: clear with some typos\nOriginality: a new approach to the NetTrim algorithm, which is somewhat original, and a new generalization bound for the algorithm.\nSignificance: somewhat significant.\n\nPROS\n- A very efficient algorithm for pruning, which can run orders of magnitude faster than the approaches that were compared to on certain architectures.\n- An interesting generalization bound for the pruned network which is in line experimentally with decreasing robustness to pruning on layers close to the input.\n\nCONS\n- Non-trivial loss of accuracy on the pruned network, which cannot be estimated for larger-scale pruning as the experiments only prune one layer.\n- No in-depth analysis of the generalization bound.\n\nMain questions:\n- You mention you use a variant of DCA: could you detail what differences Alg. 2 has with classical DCA?\n- Where do you use the 0-1 loss in Thm. 3.2?\n- I think your result in Theorem 3.2 would be significantly stronger if you could provide an analysis of the bound you obtain: in which cases can we expect certain terms to be larger or smaller, etc.\n- Your experiments in section 4.2 show a non-trivial degradation of the accuracy with FeTa. Although the time savings seem worth the tradeoff to prune *one* layer, have you run the same experiments when pruning multiple layers? Could you comment on how the accuracy evolves with multiple pruned layers?\n- It would be nice to see the curves for NetTrim and/or LOBS in Fig. 2.\n- Have you tried retraining the network after pruning? Did you observe the same behavior as mentioned in (Dong et al., 2017) and (Wolfe et al., 2017)? \n- It would be interseting to plot the theoretical (pessimistic) GE bound as well as the experimental accuracy degradation. \n\nNitpicks:\n-Ubiquitous (first paragraph)\n-difference of convex problemS\n- The references should be placed before the appendix.\n- The amount of white space should be reduced (e.g. around Eq. (1)).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cheap DNN Pruning with Performance Guarantees ","abstract":"Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.","pdf":"/pdf/5a57dcfae378e42590369943178334b3ad7f88db.pdf","TL;DR":"A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error.","paperhash":"anonymous|cheap_dnn_pruning_with_performance_guarantees","_bibtex":"@article{\n  anonymous2018cheap,\n  title={Cheap DNN Pruning with Performance Guarantees },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJtChcgAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper360/Authors"],"keywords":["pruning","generalisation error","DC optimisation"]}},{"tddate":null,"ddate":null,"tmdate":1512222625448,"tcdate":1511719752878,"number":2,"cdate":1511719752878,"id":"rybUQFOgf","invitation":"ICLR.cc/2018/Conference/-/Paper360/Official_Review","forum":"SJtChcgAW","replyto":"SJtChcgAW","signatures":["ICLR.cc/2018/Conference/Paper360/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting use of DC functions","rating":"5: Marginally below acceptance threshold","review":"The problem of pruning DNNs is an active area of study.\nThis paper addresses this problem by posing the Net-trim objective function as  a Difference of convex(DC) function. This allows for an immediate application of DC function minimization using existing techniques. An analysis of Generalization error \nis also given. \n\nThe main novelty seems to be the interesting connection to DC function minimization. The benefits seem to be a faster algorithm for pruning. \n\nAbout the generalization error the term C_2 needs to be more well defined otherwise the coefficient of  A would be -ve which may lead to complications.\n\nExperimental investigations are reasonable and the results are convincing.\n\nA list of Pros:\n1. Interesting connection to DC function\n2. Attempt to analyze generalization error \n3. Faster speed of convergence empirically\n\nA list of Cons:\n1. The contribution in posing the objective as a DC function looks limited as it is very straightforward. Also the algorithm is \ndirect application\n2. The time complexity analysis is imprecise. Since the proposed algorithm is iterative time complexity would depend on the number of iterations.\n\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cheap DNN Pruning with Performance Guarantees ","abstract":"Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.","pdf":"/pdf/5a57dcfae378e42590369943178334b3ad7f88db.pdf","TL;DR":"A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error.","paperhash":"anonymous|cheap_dnn_pruning_with_performance_guarantees","_bibtex":"@article{\n  anonymous2018cheap,\n  title={Cheap DNN Pruning with Performance Guarantees },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJtChcgAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper360/Authors"],"keywords":["pruning","generalisation error","DC optimisation"]}},{"tddate":null,"ddate":null,"tmdate":1512222625489,"tcdate":1511704139799,"number":1,"cdate":1511704139799,"id":"Sk4UIHOlM","invitation":"ICLR.cc/2018/Conference/-/Paper360/Official_Review","forum":"SJtChcgAW","replyto":"SJtChcgAW","signatures":["ICLR.cc/2018/Conference/Paper360/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs. It reformulates the non-convex optimization problem in (Aghasi et al., 2016) as a difference of convex (DC) problem, which can be solved quite efficiently using the DCA algorithm (Tao and An, 1997). The contribution is valuable since the complexity is significantly reduced, but there are many syntax errors and the accuracy of the model is not satisfactory.","rating":"6: Marginally above acceptance threshold","review":"The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs. The proposed algorithm is an improvement of Net-Trim (Aghasi et al., 2016), which is to enforce the weights to be sparse.\n\nThe main contribution of this manuscript is that the non-convex optimization problem in (Aghasi et al., 2016) is reformulated as a difference of convex (DC) problem, which can be solved quite efficiently using the DCA algorithm (Tao and An, 1997). The complexity of the proposed algorithm is much lower than Net-Trim and its fast version LOBS (Dong et al., 2017). The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).\n\nAlthough this is an incremental work built upon (Aghasi et al., 2016) and an existing algorithm (Tao and An, 1997) is adopted for optimization, the contribution is valuable since the complexity is significantly reduced by utilizing the proposed difference of convex reformulation. Although the main idea is clearly presented, there are many syntax errors and I suggest the authors carefully checking the manuscript.\n\nPros:\n1.\tThe motivation is clear and the presented reformulation is reasonable.\n\n2.\tThe generalization error analysis and the conclusion of “layers closer to the input are exponentially less robust to pruning” is interesting.\n\nCons:\n1.\tThere are many syntax errors, e.g., “Closer to our approach recently in Aghasi et al. (2016) the authors”, “an cheap pruning algorithm”, etc. Besides, there is no discussion for the results in Table 1.\n\n2.\tAlthough the complexity of the proposed method is much lower than the compared approaches (Net-Trim and LOBS), there seems to be a large sacrifice on accuracy. For example, the accuracy drops from 95.2% to 91% compared with Net-Trim in the LeNet-5 model and from 80.5% to 74.6% compared with LOBS in the CifarNet model. The proposed method is only better than hard-thresholding.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cheap DNN Pruning with Performance Guarantees ","abstract":"Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.","pdf":"/pdf/5a57dcfae378e42590369943178334b3ad7f88db.pdf","TL;DR":"A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error.","paperhash":"anonymous|cheap_dnn_pruning_with_performance_guarantees","_bibtex":"@article{\n  anonymous2018cheap,\n  title={Cheap DNN Pruning with Performance Guarantees },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJtChcgAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper360/Authors"],"keywords":["pruning","generalisation error","DC optimisation"]}},{"tddate":null,"ddate":null,"tmdate":1509739344450,"tcdate":1509104849199,"number":360,"cdate":1509739341795,"id":"SJtChcgAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJtChcgAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Cheap DNN Pruning with Performance Guarantees ","abstract":"Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy. However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy. In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network. Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches. Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.","pdf":"/pdf/5a57dcfae378e42590369943178334b3ad7f88db.pdf","TL;DR":"A fast pruning algorithm for fully connected DNN layers with theoretical analysis of degradation in Generalisation Error.","paperhash":"anonymous|cheap_dnn_pruning_with_performance_guarantees","_bibtex":"@article{\n  anonymous2018cheap,\n  title={Cheap DNN Pruning with Performance Guarantees },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJtChcgAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper360/Authors"],"keywords":["pruning","generalisation error","DC optimisation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}