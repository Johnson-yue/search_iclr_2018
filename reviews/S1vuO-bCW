{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222721564,"tcdate":1511834518142,"number":2,"cdate":1511834518142,"id":"BJ0qmr9xf","invitation":"ICLR.cc/2018/Conference/-/Paper698/Official_Review","forum":"S1vuO-bCW","replyto":"S1vuO-bCW","signatures":["ICLR.cc/2018/Conference/Paper698/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple, practical approach for autonomous resets; good experimental results and ablation studies, but no real world tasks.","rating":"6: Marginally above acceptance threshold","review":"The paper solves the problem of how to do autonomous resets, which is an important problem in real world RL. The method is novel, the explanation is clear, and has good experimental results.\n \nPros:\n1. The approach is simple, solves a task of practical importance, and performs well in the experiments. \n2. The experimental section performs good ablation studies wrt fewer reset thresholds, reset attempts, use of ensembles.\n\nCons:\n1. The method is evaluated only for 3 tasks, which are all in simulation, and on no real world tasks. Additional tasks could be useful, especially for qualitative analysis of the learned reset policies.\n2. It seems that while the method does reduce hard resets, it would be more convincing if it can solve tasks which a model without a reset policy couldnt. Right now, the methods without the reset policy perform about equally well on final reward.\n3. The method wont be applicable to RL environments where we will need to take multiple non-invertible actions to achieve the goal (an analogy would be multiple levels in a game). In such situations, one might want to use the reset policy to go back to intermediate “start” states from where we can continue again, rather than the original start state always.\n\nConclusion/Significance: The approach is a step in the right direction, and further refinements can make it a significant contribution to robotics work.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning","abstract":"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","pdf":"/pdf/619b9130178ef5bacaa9703f300cec3a442616c4.pdf","TL;DR":"We propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt.","paperhash":"anonymous|leave_no_trace_learning_to_reset_for_safe_and_autonomous_reinforcement_learning","_bibtex":"@article{\n  anonymous2018leave,\n  title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1vuO-bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper698/Authors"],"keywords":["manual reset","continual learning","reinforcement learning","safety"]}},{"tddate":null,"ddate":null,"tmdate":1512222721604,"tcdate":1511831625882,"number":1,"cdate":1511831625882,"id":"ByMUO4qxG","invitation":"ICLR.cc/2018/Conference/-/Paper698/Official_Review","forum":"S1vuO-bCW","replyto":"S1vuO-bCW","signatures":["ICLR.cc/2018/Conference/Paper698/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A strategy for learning to self-reset in episodic tasks as well as to avoid some types of bad failure","rating":"6: Marginally above acceptance threshold","review":"If one is committed to doing value-function or policy-based RL for an episodic task on a real physical system, then one has to come up with a way of resetting the domain for new trials.  This paper proposes a good way of doing this:  learn a policy for resetting at the same time as learning a policy for solving the problem.  As a side effect, the Q values associated with the reset policy can be used to predict when the system is about to enter an unrecoverable state and \"forbid\" the action.\n\nIt is, of course, necessary that the domain be, in fact, reversible  (or, at least, that it be possible to reach a starting state from at least one goal state--and it's better if that goal state is not significantly harder to reach than other goal states.\n\nThere were a couple of places in the paper that seemed to be to be not strictly technically correct.\n\nIt says that the reset policy is designed to achieve a distribution of final states that is equivalent to a starting distribution on the problem.  This is technically fairly difficult, as a problem, and I don't think it can be achieved through standard RL methods.   Later, it is clearer that there is a set of possible start states and they are all treated as goal states from the perspective of the reset policy.   That is a start set, not a distribution.  And, there's no particular reason to think that the reset policy will not, for example, always end up returning to a particular state.\n\nAnother point is that training a set of Q functions from different starting states generates some kind of an ensemble, but I don't think you can guarantee much about what sort of a distribution on values it will really represent.   Q learning + function approximation can go wrong in a variety of ways, and so some of these values might be really gross over or under estimates of what can be achieved even by the policies associated with those values. \n\nA final, higher-level, methodological concern is that, it seems to me, as the domains become more complex, rather than trying to learn two (or more) policies, it might be more effective to take a model-based approach, learn one model, and do reasoning to decide how to return home (and even to select from a distribution of start states) and/or to decide if a step is likely to remove the robot from the \"resettable\" space.\n\nAll this aside, this seems like a fairly small but well considered and executed piece of work.  I'm rating it as marginally above threshold, but I indeed find it very close to the threshold.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning","abstract":"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","pdf":"/pdf/619b9130178ef5bacaa9703f300cec3a442616c4.pdf","TL;DR":"We propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt.","paperhash":"anonymous|leave_no_trace_learning_to_reset_for_safe_and_autonomous_reinforcement_learning","_bibtex":"@article{\n  anonymous2018leave,\n  title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1vuO-bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper698/Authors"],"keywords":["manual reset","continual learning","reinforcement learning","safety"]}},{"tddate":null,"ddate":null,"tmdate":1509739153856,"tcdate":1509132399107,"number":698,"cdate":1509739151186,"id":"S1vuO-bCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1vuO-bCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning","abstract":"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.","pdf":"/pdf/619b9130178ef5bacaa9703f300cec3a442616c4.pdf","TL;DR":"We propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt.","paperhash":"anonymous|leave_no_trace_learning_to_reset_for_safe_and_autonomous_reinforcement_learning","_bibtex":"@article{\n  anonymous2018leave,\n  title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1vuO-bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper698/Authors"],"keywords":["manual reset","continual learning","reinforcement learning","safety"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}