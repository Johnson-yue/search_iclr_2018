{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222743046,"tcdate":1511818572391,"number":3,"cdate":1511818572391,"id":"BJNUHb5lz","invitation":"ICLR.cc/2018/Conference/-/Paper747/Official_Review","forum":"B1KJJf-R-","replyto":"B1KJJf-R-","signatures":["ICLR.cc/2018/Conference/Paper747/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Paper offers a novel approach to a tricky problem and does really well","rating":"7: Good paper, accept","review":"This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network. This work to my knowledge is the first to use a DSL closer to a full language.\n\nThe paper is very clear and easy to follow. One way it could be improved is if it were compared with another system. The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples","abstract":"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","pdf":"/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf","TL;DR":"Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model","paperhash":"anonymous|neural_program_search_solving_data_processing_tasks_from_description_and_examples","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KJJf-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper747/Authors"],"keywords":["Deep learning","Structured Prediction","Natural Language Processing","Neural Program Synthesis"]}},{"tddate":null,"ddate":null,"tmdate":1512222743089,"tcdate":1511747546723,"number":2,"cdate":1511747546723,"id":"rJmyxltgz","invitation":"ICLR.cc/2018/Conference/-/Paper747/Official_Review","forum":"B1KJJf-R-","replyto":"B1KJJf-R-","signatures":["ICLR.cc/2018/Conference/Paper747/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Decent execution, but not super new or exciting","rating":"5: Marginally below acceptance threshold","review":"This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples. The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs. In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs.  Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results, and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique.\n\nStrengths:\n\n- Reasonable approach, quality is good\n\n- The DSL is richer than that of previous related work like Balog et al. (2016).\n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting.\n\nWeaknesses:\n\n- There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work. Using beam search on tree outputs is a bit of a minor contribution.\n\n- The baselines are just minor variants of the proposed method. It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset.\n\n- Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given.\n\nQuestions:\n\n- Why not compare against Seq2Seq + Search?\n\n- How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions. I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do.\n\n- Are there plans to release the dataset? Could you provide a large sample of the data at an anonymized link? I'd re-evaluate my rating after looking at the data in more detail.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples","abstract":"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","pdf":"/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf","TL;DR":"Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model","paperhash":"anonymous|neural_program_search_solving_data_processing_tasks_from_description_and_examples","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KJJf-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper747/Authors"],"keywords":["Deep learning","Structured Prediction","Natural Language Processing","Neural Program Synthesis"]}},{"tddate":null,"ddate":null,"tmdate":1512222743130,"tcdate":1511599382626,"number":1,"cdate":1511599382626,"id":"SkJQ6oUgG","invitation":"ICLR.cc/2018/Conference/-/Paper747/Official_Review","forum":"B1KJJf-R-","replyto":"B1KJJf-R-","signatures":["ICLR.cc/2018/Conference/Paper747/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Promising direction, but too preliminary","rating":"4: Ok but not good enough - rejection","review":"This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL. The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output. The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types. The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach.\n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications. Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously. However, there are several issues both in the approach and the \ncurrent preliminary evaluation, which unfortunately leads me to a reject score,\nbut the general idea of combining different specifications is quite promising.\n\nFirst, the paper does not compare against a very similar approach of Parisotto et al.\nNeuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network\nfor generating the program tree incrementally by decoding one node at a time.\nCan the authors comment on the similarity/differences between the approaches?\nWould it be possible to empirically evaluate how the R3NN performs on this dataset?\n\nSecond, it seems that the current model does not use the input-output examples at \nall for training the model. The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search. It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy.\n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples.\n\nFinally, there is also an issue with the synthetic evaluation dataset. Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics. The paper mentions that it is prohibitively \nexpensive to obtain human-annotated set, but can it be possible to at least obtain a \nhandful of real tasks to evaluate the learnt model? There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future.\n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100? What happens when it is set to 10^4 or 10^6?\n\nThe Search algorithm only shows an accuracy of 0.6% with MAX_VISITED=100. What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)? \n\nCan you clarify the contributions of the paper in comparison to the R3NN?\n\nMinor typos:\n\npage 2: allows to add constrains --> allows to add constraints\npage 5: over MAX_VISITED programs has been --> over MAX_VISITED programs have been\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples","abstract":"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","pdf":"/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf","TL;DR":"Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model","paperhash":"anonymous|neural_program_search_solving_data_processing_tasks_from_description_and_examples","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KJJf-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper747/Authors"],"keywords":["Deep learning","Structured Prediction","Natural Language Processing","Neural Program Synthesis"]}},{"tddate":null,"ddate":null,"tmdate":1509739125823,"tcdate":1509134049078,"number":747,"cdate":1509739123155,"id":"B1KJJf-R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1KJJf-R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples","abstract":"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","pdf":"/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf","TL;DR":"Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model","paperhash":"anonymous|neural_program_search_solving_data_processing_tasks_from_description_and_examples","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KJJf-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper747/Authors"],"keywords":["Deep learning","Structured Prediction","Natural Language Processing","Neural Program Synthesis"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}