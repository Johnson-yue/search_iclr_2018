{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222554493,"tcdate":1511795148510,"number":2,"cdate":1511795148510,"id":"r1ERFjYef","invitation":"ICLR.cc/2018/Conference/-/Paper1088/Official_Review","forum":"rkZB1XbRZ","replyto":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference/Paper1088/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This work investigates scalable applications of PATE ","rating":"6: Marginally above acceptance threshold","review":"Summary:\nIn this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. This is done by leveraging the synergy between privacy and utility, to make better use of the privacy budget spent when transferring knowledge from teachers to the student. Two aggregation mechanisms are introduced for this reason.  It is demonstrated that sampling from a Gaussian distribution (instead from a Laplacian distribution) facilitates the aggregation of teacher votes in tasks with large number of output classes. \n\non the positive side:\n\nHaving scalable models is important, especially models that can be applied to data with privacy concerns. The extension of an approach for learning with privacy to make it scalable is of merit. The paper is well written, and the idea of the model is clear. \n\n\non the negative side:\n\nIn the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data. This is for sure an important topic. However, in the following paper, the model is applied no neither medical nor healthcare data. The authors mention that the original model PATE was applied to medical record and census data with the UCI diabetes and adult data set. I personally would prefer to see the proposed model applied to this kind of data sets as well. \n\nminor comments: \n\nFigure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222554540,"tcdate":1511746196271,"number":1,"cdate":1511746196271,"id":"r1nc91tez","invitation":"ICLR.cc/2018/Conference/-/Paper1088/Official_Review","forum":"rkZB1XbRZ","replyto":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference/Paper1088/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novel techniques to improve private learning with PATE","rating":"6: Marginally above acceptance threshold","review":"The paper proposes novel techniques for private learning with PATE framework. Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. In the experiments, the efficacy of the proposed techniques has been demonstrated. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. \n\n1. As for noise distribution, I am wondering if the variance of the distribution also plays a role to keep good utility-privacy trade-off. It would be great to discuss and show experimental results for utility-privacy tradeoff with different variances of Laplace and Gaussian noise.\n\n2. It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples. \n\n3. It would be great if there is an explanation about the privacy cost for selective aggregation. Intuitively, if teacher ensemble does not answer, it seems that it would reveal the fact that teachers do not agree, and thus spend some privacy cost.\n\n\n\n\n\n\n\n\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092380963,"tcdate":1509138261294,"number":1088,"cdate":1510092360098,"id":"rkZB1XbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}