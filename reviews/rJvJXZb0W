{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222710784,"tcdate":1511969867750,"number":3,"cdate":1511969867750,"id":"ByVL483xf","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Intuitive model for sentence representations with good performance","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences. Unique aspects of this skip-gram style model include separate target- and context-sentence encoders, as well as a dot-product similarity measure between representations. A battery of experiments indicate that the learned representations have comparable or better performance compared to other, more computationally-intensive models.\n\nWhile the main constituent ideas of this paper are not entirely novel, I think the specific combination of tools has not been explored previously. As such, the novelty of this paper rests in the specific modeling choices and the significance hinges on the good empirical results. For this reason, I believe it is important that additional details regarding the specific architecture and training details be included in the paper. For example, how many layers is the GRU? What type of parameter initialization is used? Releasing source code would help answer these and other questions, but including more details in the paper itself would also be welcome.\n\nRegarding the empirical results, the method does appear to achieve good performance, especially given the compute time. However, the balance between performance and computational complexity is not investigated, and I think such an analysis would add significant value to the paper. For example, I see at least three ways in which performance could be improved at the expense of additional computation: 1) increasing the candidate pool size 2) increasing the corpus size and 3) increasing the embedding size / increasing the encoder capacity. Does the good performance/efficiency reported in the paper depend on achieving a sweet spot among those three hyperparameters?\n\nOverall, the novelty of this paper is fairly low and there is still substantial room for improvement in some of the analysis. On the other hand, I think this paper proposes an intuitive model and demonstrates good performance. I am on the fence, but ultimately I vote to accept this paper for publication.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1512222712128,"tcdate":1511885721558,"number":2,"cdate":1511885721558,"id":"rJMoj-jxf","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An elegant and simple alternative to existing methods, but empirical advantages are unclear ","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a new objective for learning SkipThought-style sentence representations from corpora of ordered sentences. The algorithm is much faster than SkipThoughts as it swaps the word-level decoder for a contrastive classification loss. \n\nComments:\n\nSince one of the key advantages of this method is the speed, I was surprised there was not a more formal comparison of the speed of training different models. For instance, it would be more convincing if two otherwise identical encoders were trained on the same machine on the books corpus with the proposed objective and the skipthoughts decoding objective, and the representations compared after X hours of training. The reported 2 weeks required to train Skipthoughts comes from the paper, but things might be faster now with more up-to-date deep learning libraries etc. If this was what was in fact done, then it's probably just a case of presenting the comparison in a more formal way. I would also lose the sentence \"we are able to train many models in the time it takes to train most unsupervised\" (see next point for reasons why this is questionable).\n\nIt would have been interesting to apply this method with BOW encoders, which should be even faster than RNN-based encoders reported in this paper. The faster BOW models tend to give better performance on cosine-similarity evaluations ( quantifying the nearest-neighbour analysis that the authors use in this paper). Indeed, it would be interesting (although of course not definitive) to see comparison of the proposed algorithm (with BOW and RNN encoders) on cosine sentence similarity evaluations. \n\nThe proposed novelty is simple and intuitive, which I think is a strength of the method. However, a simple idea makes overlap with other proposed approaches more likely, and I'd like the author to check through the public comments to ensure that all previous related ideas are noted in this paper. \n\nI think the authors could do more to emphasise what the point is of trying to learn sentence embeddings. An idea of the eventual applications of these embeddings would make it easier to determine, for instance, whether the supervised ensembling method applied here would be applicable in practice. Moreover, many papers have emphasised the limitations of the evaluations used in this paper (although they are still commonly used) so it would be good to acknowledge that it's hard to draw too many conclusions from such numbers. That said, the numbers are comparable Skipthoughts, so it's clear that this method learns representations of comparable quality. \n\nThe justification for the proposed algorithm is clear in terms of efficiency, but I don't think it's immediately clear from a semantic / linguistic point of view. The statement \"The meaning of a sentence is the property that creates bonds....\" seems to have been cooked up to justify the algorithm, not vice versa. I would cut all of that speculation out and focus on empirically verifiable advantages. \n\nThe section of image embeddings comes completely out of the blue and is very hard to interpret. I'm still not sure I understand this evaluation (short of looking up the Kiros et al. paper), or how the proposed model is applied to a multi-modal task.\n\nThere is much scope to add more structured analysis of the type hinted by the nearest neighbours section. Cherry picked lists don't tell the reader much, but statistics or more general linguistic trends can be found in these neighbours and aggregated, that could be very interesting. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1512222712166,"tcdate":1511129947613,"number":1,"cdate":1511129947613,"id":"SJNPXFyeM","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Great results, with minor concerns","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes an alternative formulation of Kiros's SkipThought objective for training general-purpose sentence encoder RNNs on unlabeled data. This formulation replaces the decoder in that model with a second encoder, and yields substantial improvements to both speed and model performance (as measured on downstream transfer tasks). The resulting model is, for the first time, reasonably competitive even with models that are trained end-to-end on labeled data for the downstream tasks (despite the requirement, imposed by the evaluation procedure, that only the top layer classifier be trained for the downstream tasks here), and is also competitive with models trained on large labeled datasets like SNLI. The idea is reasonable, the topic is important, and the results are quite strong. I recommend acceptance, with some caveats that I hope can be addressed.\n\nConcerns:\n\nA nearly identical idea to the core idea of this paper was proposed in an arXiv paper this spring, as a commenter below pointed out. That work has been out for long enough that I'd urge you to cite it, but it was not published and it reports results that are far less impressive than yours, so that omission isn't a major problem.\n\nI'd like to see more discussion of how you performed your evaluation on the downstream tasks. Did you use the SentEval tool from Conneau et al., as several related recent papers have? If not, does your evaluation procedure differ from theirs or Kiros's in any meaningful way?\n\nI'm also a bit uncomfortable that the paper doesn't directly compare with any baselines that use the exact same codebase, word representations, hyperparameter tuning procedure, etc.. I would be more comfortable with the results if, for example, the authors compared a low-dimensional version of their model with a low-dimensional version of SkipThought, trained in the *exact* same way, or if they implemented the core of their model within the SkipThought codebase and showed strong results there.\n\nMinor points:\n\nThe headers in Table 1 don't make it all that clear which additions (vectors, UMBC) are cumulative with what other additions. This should be an easy fix. \n\nThe use of the check-mark as an output in Figure 1 doesn't make much sense, since the task is not binary classification.\n\n\"Instead of training a model to reconstruct the surface form of the input sentence or its neighbors, our formulation attempts to focus on the semantic aspects of sentences. The meaning of a sentence is the property that creates bonds between a sequence of sentences and makes it logically flow.\" – It's hard to pin down exactly what this means, but it sounds like you're making an empirical claim here: semantic information is more important than non-semantic sources of variation (syntactic/lexical/morphological factors) in predicting the flow of a text. Provide some evidence for this, or cut it.\n\nYou make a similar claim later in the same section: \"In figure 1(a) however, the reconstruction loss forces the model to predict local structural information about target sentences that may be irrelevant to its meaning (e.g., is governed by grammar rules).\" This is a testable prediction: Are purely grammatical (non-semantic) variations in sentence form helpful for your task? I'd suspect that they are, at least in some cases, as they might give you clues as to style, dialect, or framing choices that the author made when writing that specific passage.\n\n\"Our best BookCorpus model (MC-QT) trains in just under 11hrs, compared to skip-thought model’s training time of 2 weeks.\" –  If you say this, you need to offer evidence that your model is faster. If you don't use the same hardware and low-level software (i.e., CuDNN), this comparison tells us nearly nothing. The small-scale replication of SkipThought described above should address this issue, if performed.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509994960021,"tcdate":1509994960021,"number":5,"cdate":1509994960021,"id":"HkdCWV0Ab","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"Sk1dtLqAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Evaluation","comment":"Thanks for your reply.\n\nThere is an interesting difference between the evaluation tasks and the evaluation task used in Siamese CBOW.\n\nIn Siamese CBOW, they mainly focused on unsupervised evaluation tasks, including STS12, 13, and 14. The similarity of 2 sentences is determined by Cosine-similarity, which matches their training objective. Compared with FastSent which applies the dot-product as training objective in FastSent, Siamese CBOW seems to get better results.\n\nCould you also evaluate your proposed model on unsupervised evaluation tasks, like STS14? It would be good to have a comprehensive evaluation of your model. Thanks!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427164,"tcdate":1509742951006,"number":3,"cdate":1509742951006,"id":"Sk1dtLqAW","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"SyctLIIC-","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Siamese CBOW","comment":"Sure, Thanks. \n\nIn this paper a conceptually similar task of identifying context sentences from candidate sentences based on their bag-of-words representations is considered. Our approach is more general than this work in the following ways\n* Our formulation considers more general scoring functions/classifiers. We found inner products to work best. Using cosine distance as is done in this work led to inferior representations. Cosine distance implicitly requires sentence representations to both lie on the unit ball and be similar (in terms of inner product) to context sentences, which can be a strong constraint. The inner products scoring function only requires the latter. \n* This work uses the same set of parameters to encode both input and context sentences, while we consider using different sets of parameters. This helped learn better representations. We briefly discuss this choice in section 3.\n* Our formulation also allows the use of more general encoder architectures.\n\nAlso, we discuss more recent bag-of-words methods in the paper. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509571954878,"tcdate":1509571954878,"number":4,"cdate":1509571954878,"id":"BJidT3DCb","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Data volume question","comment":"Just out of curiosity, do you have any results on how the quantity of unlabeled training data you use impacts model performance?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509480123323,"tcdate":1509480066526,"number":3,"cdate":1509480066526,"id":"SyctLIIC-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"SJpU59rCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Siamese CBOW","comment":"Thank you for your reply! Could you also compare your idea with Siamese CBOW? (ACL2016)\n\nhttp://www.aclweb.org/anthology/P16-1089\n\nThanks again!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427207,"tcdate":1509431893238,"number":2,"cdate":1509431893238,"id":"SJpU59rCW","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"BkzJW9QC-","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Related paper","comment":"Thank you for your comment. We will include the paper in a revised version. Please see our response to the previous comment regarding the same paper. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427248,"tcdate":1509430766886,"number":1,"cdate":1509430766886,"id":"SJPgI5BR-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"SJbLL_zCb","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Objective function","comment":"Thank you for your comments. We will include these literature in a revised version of the paper. \n\nDespite similarities in the objective functions, we would like to point out the following key distinctions.\n\nJernite et al. propose to use paragraph level coherence as a learning signal. The following related task is considered in their paper. Given the first three sentences of a paragraph, they choose the next sentence from five candidate sentences later in the paragraph (Paragraphs of length at least 8 are considered). \nOur objective differs from theirs in the following aspects.\n* This work exploits paragraph level coherence signals for learning, while our work derives motivation from the distributional hypothesis. We don’t restrict ourselves to paragraphs in the data as is done in this work. \n* We consider a large number of candidate sentence choices when predicting a context sentence. This is a discriminative approximation to the generation objective (viewing generation as choosing a sentence from all possible sentences)\n* We use a single input sentence and predict the context sentences surrounding it. Using larger input contexts did not yield any significant empirical benefits.\nOur objective further learns richer representations compared to this work, as evidenced by empirical results. \n\nThe local coherence model of Li & Hovy is a feed-forward network which examines a window of sentence embeddings and classifies them as coherent/incoherent (binary classification). We have some discussion about this objective in the paper (section 3). We point out the following key differences between our objective and theirs. \n* Instead of discriminating context windows as plausible/implausible, we encourage observed contexts (in the data) to be more plausible than contrastive (implausible) ones and formulate it as a multi-class classification problem. We experimentally found that this relaxed constraint helps learn better representations.\n* We use a simple scoring function (inner products) in our objective. When using a parameterized classifier, the model has a tendency to learn poor sentence representations and compensate for it using a strong classifier. This is undesirable since the classifier is discarded and only the sentence encoders are used for feature extraction.\n\nHence, Li & Hovy’s objective is better suited for local coherence modeling than it is for learning sentence representations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509298393854,"tcdate":1509298393854,"number":2,"cdate":1509298393854,"id":"BkzJW9QC-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"a related paper","comment":"https://arxiv.org/pdf/1705.00557.pdf\n\nThe proposed method in the listed paper is quite close to the one proposed in this submission. I think it'll be good to cite this listed paper and discuss it. (Although I know it is not required to cite arxiv papers.)\n\n(Also, I am not related to the listed arxiv paper, but I'd love to some comprehensive comparisons among existing methods.)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509226057339,"tcdate":1509226057339,"number":1,"cdate":1509226057339,"id":"SJbLL_zCb","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some literature review suggestions","comment":"This article proposed a framework to learn sentence representation and demonstrated some good results.\n\nIn terms of the main objective of this task - predicting the next sentence out of a group of sampled sentences has already been proposed multiple times in the NLP community. For example, it appeared earlier this year: https://arxiv.org/abs/1705.00557, and a much earlier work (in 2014) has also used sentence ordering to learn sentence representation: http://web.stanford.edu/~jiweil/paper/emnlp_coherence-v2eh.pdf\n\nI am certain this paper brings unique value and insight into this training objective, and is a much-needed addition to the existing pool of literature. I just hope maybe in a revised version of this paper, the author(s) would reference these previous NLP works.\n\n(To clarify on my intent: I am not related to any of these papers, but would love to see NLP researches get recognized.)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509739174787,"tcdate":1509130975187,"number":661,"cdate":1509739172130,"id":"rJvJXZb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/dcaf5185c3e5a2de9a006cc3cc9c3ccc68ce7563.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}