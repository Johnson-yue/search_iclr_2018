{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222623048,"tcdate":1511852029048,"number":3,"cdate":1511852029048,"id":"SJBZut5gM","invitation":"ICLR.cc/2018/Conference/-/Paper343/Official_Review","forum":"B1CNpYg0-","replyto":"B1CNpYg0-","signatures":["ICLR.cc/2018/Conference/Paper343/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice examination of learning on-the-fly word embeddings, but a fairly small focused contribution","rating":"5: Marginally below acceptance threshold","review":"This paper examines ways of producing word embeddings for rare words on demand. The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets. The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task.\n\nThe contribution is clear but not huge. In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category. The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible. More things could have been considered. Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet. The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3). However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is.\n\nThe paper is clearly written. \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf . While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK. However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words. That method could also be considered as a possible approach to compare against here.\n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.). While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics.\n - p.2: You use pretrained GloVe vectors that you do not update. My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \"only\" 6 billion words – 2 orders of magnitude less data.\n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector?\n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1) but well below current best single models of around 76-78 F1. The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective. This model shows the rather strong performance of spelling models – at least on this task – which he again benefit from training in the context of the end objective. \n - Fig 2: It's weird that only the +dict (left) model learns to connect \"In\" and \"where\". The point made in the text between \"Where\" and \"overseas\" is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words \"where\" and \"in\" both commonly expressing a location.\n - Table 2: These results are interestingly different. Dict is much more useful than spelling here. I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge.\n - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD). My impression is that most papers on NLI use much larger vocabs, no?\n - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct.\n - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Compute Word Embeddings On the Fly","abstract":"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ``long tail'' of this distribution requires enormous amounts of data. \nRepresentations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.\n","pdf":"/pdf/2aa6820d42ac9a9b39eeaf6ddb03d722583ba84e.pdf","TL;DR":"We propose a method to deal with rare words by computing their embedding from definitions.","paperhash":"anonymous|learning_to_compute_word_embeddings_on_the_fly","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Compute Word Embeddings On the Fly},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1CNpYg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper343/Authors"],"keywords":["NLU","word embeddings","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222623087,"tcdate":1511820094822,"number":2,"cdate":1511820094822,"id":"ryDrjZqxM","invitation":"ICLR.cc/2018/Conference/-/Paper343/Official_Review","forum":"B1CNpYg0-","replyto":"B1CNpYg0-","signatures":["ICLR.cc/2018/Conference/Paper343/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good practical approach for rare (and unseen) word handling","rating":"7: Good paper, accept","review":"\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas:\n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older).\n\n* Using character-level models a la Ling et al.\n\n* Using dictionary embeddings a la Hill et al.\n\nNone of these ideas are new before but I haven’t seen them combined in this way before. This is a very practical idea, well-explained with a thorough set of experiments across three different tasks. The paper is not surprising but this seems like an effective technique for people who want to build effective systems with whatever data they’ve got. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Compute Word Embeddings On the Fly","abstract":"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ``long tail'' of this distribution requires enormous amounts of data. \nRepresentations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.\n","pdf":"/pdf/2aa6820d42ac9a9b39eeaf6ddb03d722583ba84e.pdf","TL;DR":"We propose a method to deal with rare words by computing their embedding from definitions.","paperhash":"anonymous|learning_to_compute_word_embeddings_on_the_fly","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Compute Word Embeddings On the Fly},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1CNpYg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper343/Authors"],"keywords":["NLU","word embeddings","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222623125,"tcdate":1511819989519,"number":1,"cdate":1511819989519,"id":"SJTAcW5xf","invitation":"ICLR.cc/2018/Conference/-/Paper343/Official_Review","forum":"B1CNpYg0-","replyto":"B1CNpYg0-","signatures":["ICLR.cc/2018/Conference/Paper343/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions. The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings. The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings. The paper is clearly written, and has useful ablation experiments. However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word. As the authors note, this kind of character level modelling has been used in many previous works. \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss, but I don’t know the area well enough to make specific suggestions \n- I’m a little skeptical about how often this method would really be useful in practice. It seems to assume that you don’t have much unlabelled text (or you’d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well. All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.\n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81. It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Compute Word Embeddings On the Fly","abstract":"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ``long tail'' of this distribution requires enormous amounts of data. \nRepresentations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.\n","pdf":"/pdf/2aa6820d42ac9a9b39eeaf6ddb03d722583ba84e.pdf","TL;DR":"We propose a method to deal with rare words by computing their embedding from definitions.","paperhash":"anonymous|learning_to_compute_word_embeddings_on_the_fly","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Compute Word Embeddings On the Fly},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1CNpYg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper343/Authors"],"keywords":["NLU","word embeddings","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739353856,"tcdate":1509100853775,"number":343,"cdate":1509739351190,"id":"B1CNpYg0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1CNpYg0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Compute Word Embeddings On the Fly","abstract":"Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the ``long tail'' of this distribution requires enormous amounts of data. \nRepresentations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.\n","pdf":"/pdf/2aa6820d42ac9a9b39eeaf6ddb03d722583ba84e.pdf","TL;DR":"We propose a method to deal with rare words by computing their embedding from definitions.","paperhash":"anonymous|learning_to_compute_word_embeddings_on_the_fly","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Compute Word Embeddings On the Fly},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1CNpYg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper343/Authors"],"keywords":["NLU","word embeddings","representation learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}