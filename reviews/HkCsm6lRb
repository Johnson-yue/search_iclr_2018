{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222647403,"tcdate":1511817382739,"number":3,"cdate":1511817382739,"id":"rky3x-5lG","invitation":"ICLR.cc/2018/Conference/-/Paper411/Official_Review","forum":"HkCsm6lRb","replyto":"HkCsm6lRb","signatures":["ICLR.cc/2018/Conference/Paper411/AnonReviewer3"],"readers":["everyone"],"content":{"title":"nice paper, needs some clarification","rating":"7: Good paper, accept","review":"The paper proposes a method for generating images from attributes. The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples, and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples. During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa. The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution. The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e. if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes. Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided, it outperforms when some of the attributes are missing (Figure 4a). It also outperforms existing methods in terms of coverage and compositionality.\n\nMajor comments:\n\nThe paper is well written, and summarizes its contribution succinctly.\n\nI did not fully understand the 'retrofitting' idea. If I understood correctly, the authors first train \\theta and \\phi and then fix \\theta to train \\phi_x and \\phi_y. If that is true, then is \\calL(\\theta, \\phi, \\phi_x, \\phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing \\theta? Please clarify?\n\nMinor comments:\n\n- 'in order of increasing abstraction', does the order of gender-> smiling or not -> hair color matter? Or, is male, *, blackhair a valid option?\n\n- what are the image sizes for the CelebA dataset\n\n- page 5: double the\n\n- Which multi-label classifier is used to classify images in attributes?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generative Models of Visually Grounded Imagination","abstract":"It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C’s). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of (Suzuki et al., 2017) and the BiVCCA method of (Wang et al., 2016)) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015).","pdf":"/pdf/35cf833f13744bdaed11e86f7aa91ae65f243e22.pdf","TL;DR":"A VAE-variant which can create diverse images corresponding to novel concrete or abstract \"concepts\" described using attribute vectors.","paperhash":"anonymous|generative_models_of_visually_grounded_imagination","_bibtex":"@article{\n  anonymous2018generative,\n  title={Generative Models of Visually Grounded Imagination},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkCsm6lRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper411/Authors"],"keywords":["variational autoencoders","generative models","language","vision","abstraction","compositionality","hierarchy"]}},{"tddate":null,"ddate":null,"tmdate":1512222647445,"tcdate":1511646574392,"number":2,"cdate":1511646574392,"id":"S1UuHvwgf","invitation":"ICLR.cc/2018/Conference/-/Paper411/Official_Review","forum":"HkCsm6lRb","replyto":"HkCsm6lRb","signatures":["ICLR.cc/2018/Conference/Paper411/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written paper","rating":"7: Good paper, accept","review":"This paper presented a multi-modal extension of variational autoencoder (VAE) for the task \"visually grounded imagination.\"  In this task,  the model learns a joint embedding of the images and the attributes. The proposed model is novel but incremental comparing to existing frameworks.  The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality. \n\nPros:\n1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community.  \n2. The discussion comparing the related work/baseline methods is insightful. \n3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods. \n\nCons/questions:\n1. The motivation of the model choice of q is not clear.\nComparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q.  BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y).  How does such model choice affect the final performance? \n\n2. Baselines are not necessarily sufficient. \nThe paper compared the vanilla version of BiVCCA but not the one with factorized representation version. In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance]. The author should also compare this extension of VAECCA.\n\n3. Some details are not clear. \na) How to set/learn the scaling parameter \\lambda_y and \\beta_y? If it is set as hyper-parameter, how does the performance change concerning them? \nb) Discussion of the experimental results is not sufficient. For example, why JMVAE performs much better than the proposed model when all attributes are given.  What is the conclusion from Figure 4(b)? The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work.  The same applies to figure 5. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generative Models of Visually Grounded Imagination","abstract":"It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C’s). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of (Suzuki et al., 2017) and the BiVCCA method of (Wang et al., 2016)) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015).","pdf":"/pdf/35cf833f13744bdaed11e86f7aa91ae65f243e22.pdf","TL;DR":"A VAE-variant which can create diverse images corresponding to novel concrete or abstract \"concepts\" described using attribute vectors.","paperhash":"anonymous|generative_models_of_visually_grounded_imagination","_bibtex":"@article{\n  anonymous2018generative,\n  title={Generative Models of Visually Grounded Imagination},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkCsm6lRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper411/Authors"],"keywords":["variational autoencoders","generative models","language","vision","abstraction","compositionality","hierarchy"]}},{"tddate":null,"ddate":null,"tmdate":1512222647492,"tcdate":1511624943420,"number":1,"cdate":1511624943420,"id":"BJDxbMvez","invitation":"ICLR.cc/2018/Conference/-/Paper411/Official_Review","forum":"HkCsm6lRb","replyto":"HkCsm6lRb","signatures":["ICLR.cc/2018/Conference/Paper411/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clear paper with convincing results","rating":"7: Good paper, accept","review":"The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified, and when some are left undefined, creating a more abstract generation task. \n\nPros:\n+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing. Both quantitative and qualitative results are provided. \n+ The paper is fairly clear.\n\nCons:\n- It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b).\n- Fig. 5 could be more convincing; \"bushy eyebrows\" is a difficult attribute to judge, and in the abstract generation when that is the only attribute specified, it is not clear how good the results are.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generative Models of Visually Grounded Imagination","abstract":"It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C’s). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of (Suzuki et al., 2017) and the BiVCCA method of (Wang et al., 2016)) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015).","pdf":"/pdf/35cf833f13744bdaed11e86f7aa91ae65f243e22.pdf","TL;DR":"A VAE-variant which can create diverse images corresponding to novel concrete or abstract \"concepts\" described using attribute vectors.","paperhash":"anonymous|generative_models_of_visually_grounded_imagination","_bibtex":"@article{\n  anonymous2018generative,\n  title={Generative Models of Visually Grounded Imagination},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkCsm6lRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper411/Authors"],"keywords":["variational autoencoders","generative models","language","vision","abstraction","compositionality","hierarchy"]}},{"tddate":null,"ddate":null,"tmdate":1509739319151,"tcdate":1509114790099,"number":411,"cdate":1509739316487,"id":"HkCsm6lRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkCsm6lRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generative Models of Visually Grounded Imagination","abstract":"It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C’s). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of (Suzuki et al., 2017) and the BiVCCA method of (Wang et al., 2016)) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015).","pdf":"/pdf/35cf833f13744bdaed11e86f7aa91ae65f243e22.pdf","TL;DR":"A VAE-variant which can create diverse images corresponding to novel concrete or abstract \"concepts\" described using attribute vectors.","paperhash":"anonymous|generative_models_of_visually_grounded_imagination","_bibtex":"@article{\n  anonymous2018generative,\n  title={Generative Models of Visually Grounded Imagination},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkCsm6lRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper411/Authors"],"keywords":["variational autoencoders","generative models","language","vision","abstraction","compositionality","hierarchy"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}