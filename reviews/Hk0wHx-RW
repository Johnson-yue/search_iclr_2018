{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222694119,"tcdate":1511834198184,"number":3,"cdate":1511834198184,"id":"ByR8Gr5gf","invitation":"ICLR.cc/2018/Conference/-/Paper583/Official_Review","forum":"Hk0wHx-RW","replyto":"Hk0wHx-RW","signatures":["ICLR.cc/2018/Conference/Paper583/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper improved on an existing latent variable model by combining ideas from different but somewhat related papers. Experimental results indeed show some improvements.","rating":"6: Marginally above acceptance threshold","review":"The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version. The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'. However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations?\n\nQuality: Ok. The claims appear to be sufficiently verified in the experiments. However, it would have been great to have an experiment that actually makes use of the learned features to make predictions. I struggle a little to see the relevance of the proposed method without a good motivating example.\n\nClarity: Below average. Section 3 is a little hard to understand. Is q(t|x) in Fig 1 a typo? How about t_j in equation (5)? There is a reference that appeared twice in the bibliography (1st and 2nd).\n\nOriginality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Latent Representations with the Deep Copula Information Bottleneck","abstract":"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep variational information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how to force sparsity of the latent space in the new model.  We evaluate our method on artificial and real data.","pdf":"/pdf/a0baf04c506a8fcaba03a5be20a88fbcc957f83e.pdf","TL;DR":"We apply the copula transformation to the Deep Information Bottleneck which leads to restored invariance properties and an interpretable latent space.","paperhash":"anonymous|learning_sparse_latent_representations_with_the_deep_copula_information_bottleneck","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Latent Representations with the Deep Copula Information Bottleneck},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0wHx-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper583/Authors"],"keywords":["Information Bottleneck","Variational Autoencoder","Sparsity","Disentanglement","Interpretability","Copula"]}},{"tddate":null,"ddate":null,"tmdate":1512222694155,"tcdate":1511729995347,"number":2,"cdate":1511729995347,"id":"ByQLos_xM","invitation":"ICLR.cc/2018/Conference/-/Paper583/Official_Review","forum":"Hk0wHx-RW","replyto":"Hk0wHx-RW","signatures":["ICLR.cc/2018/Conference/Paper583/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An extension to DVIB","rating":"5: Marginally below acceptance threshold","review":"This paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning. Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib. They then go on to explore the sparsity of the latent space\n\nMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me. On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does)\n\nMinor comments\nPage 1: forcing an compact -> forcing a compact\n“and and” =>and\nSection 2: mention that I is mutual information, it is not obvious for everyone\n\nFigure 3: circles/triangles are too small, hard to see \nFigure 5: not really convincing. B does not appear much more structured than a, to me it looks like a simple transformation of a. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Latent Representations with the Deep Copula Information Bottleneck","abstract":"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep variational information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how to force sparsity of the latent space in the new model.  We evaluate our method on artificial and real data.","pdf":"/pdf/a0baf04c506a8fcaba03a5be20a88fbcc957f83e.pdf","TL;DR":"We apply the copula transformation to the Deep Information Bottleneck which leads to restored invariance properties and an interpretable latent space.","paperhash":"anonymous|learning_sparse_latent_representations_with_the_deep_copula_information_bottleneck","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Latent Representations with the Deep Copula Information Bottleneck},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0wHx-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper583/Authors"],"keywords":["Information Bottleneck","Variational Autoencoder","Sparsity","Disentanglement","Interpretability","Copula"]}},{"tddate":null,"ddate":null,"tmdate":1512222694190,"tcdate":1511663169142,"number":1,"cdate":1511663169142,"id":"rJYSUovgG","invitation":"ICLR.cc/2018/Conference/-/Paper583/Official_Review","forum":"Hk0wHx-RW","replyto":"Hk0wHx-RW","signatures":["ICLR.cc/2018/Conference/Paper583/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting work but the clarity needs to be improved. The experimental results on the real dataset are not very strong  ","rating":"5: Marginally below acceptance threshold","review":"This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization. The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information.  Experiments include both artificial data and real data. \n\nThe paper is unclear at some places and writing gets confusing. For example, it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments, and the discussion at the end of Section 3.3 also sounds confusing. It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice. Moreover, what is the form of f_beta and how beta is optimized?  In the first equation on page 5, is tilde y involved? How to choose lambda?\n\nIf MI is invariant to monotone transformations and information curves are determined by MIs, why “transformations basically makes information curve arbitrary”? Can you elaborate?  \n\nAlthough the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks. \n\nMinor comments: \n\n- What is the meaning of the dashed lines and the solid lines respectively in Figure 1? \n- Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term? Is there a typo? \n- typo, find the “most orthogonal” representation if the inputs -> of the inputs \n\nOverall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental. The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Latent Representations with the Deep Copula Information Bottleneck","abstract":"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep variational information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how to force sparsity of the latent space in the new model.  We evaluate our method on artificial and real data.","pdf":"/pdf/a0baf04c506a8fcaba03a5be20a88fbcc957f83e.pdf","TL;DR":"We apply the copula transformation to the Deep Information Bottleneck which leads to restored invariance properties and an interpretable latent space.","paperhash":"anonymous|learning_sparse_latent_representations_with_the_deep_copula_information_bottleneck","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Latent Representations with the Deep Copula Information Bottleneck},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0wHx-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper583/Authors"],"keywords":["Information Bottleneck","Variational Autoencoder","Sparsity","Disentanglement","Interpretability","Copula"]}},{"tddate":null,"ddate":null,"tmdate":1509739221365,"tcdate":1509127526239,"number":583,"cdate":1509739218711,"id":"Hk0wHx-RW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk0wHx-RW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Sparse Latent Representations with the Deep Copula Information Bottleneck","abstract":"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep variational information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how to force sparsity of the latent space in the new model.  We evaluate our method on artificial and real data.","pdf":"/pdf/a0baf04c506a8fcaba03a5be20a88fbcc957f83e.pdf","TL;DR":"We apply the copula transformation to the Deep Information Bottleneck which leads to restored invariance properties and an interpretable latent space.","paperhash":"anonymous|learning_sparse_latent_representations_with_the_deep_copula_information_bottleneck","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Latent Representations with the Deep Copula Information Bottleneck},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk0wHx-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper583/Authors"],"keywords":["Information Bottleneck","Variational Autoencoder","Sparsity","Disentanglement","Interpretability","Copula"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}