{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222694399,"tcdate":1511824163950,"number":2,"cdate":1511824163950,"id":"rJn7jf9xf","invitation":"ICLR.cc/2018/Conference/-/Paper585/Official_Review","forum":"H1VjBebR-","replyto":"H1VjBebR-","signatures":["ICLR.cc/2018/Conference/Paper585/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper presents an interesting new analysis about unsupervised learning of (semantic) mappings with new assumptions and theoretical results that could lead to new theoretical developments in representation learning. On the other hand, the paper is dense and some discussions lack of theoretical justification.","rating":"7: Good paper, accept","review":"Quality:\nThe paper appears to be correct\n\nClarity:\nthe paper is clear, although more formalization would help sometimes\n\nOriginality\nThe paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know.\n\nSignificance\nThe points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.\n\nPros/cons\nPros\n-Adresses an important problem in representation learning\n-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings\n-A new cross domain mapping is proposed\n-Large set of experiments\nCons\n-Some parts deserve more formalization/justification\n-Too many materials for a conference paper\n-The cost of the algorithm seems high \n\nSummary:\nThis paper studies the problem of unsupervised learning of semantic mappings. It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping. A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.\nA large set of experiments are provided to support the claims of the paper.\n\n\nComments:\n\n-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect.\n\n-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss.\nThe approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.\n\n-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.\n\nTypo:\nSection 5.1 is build of -> is built of\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings","abstract":"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.\n\nWe identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.\n\nVarious predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","pdf":"/pdf/2ee302ca9f4fabe532091ee01888d662c8eca540.pdf","TL;DR":"Our hypothesis is that given two domains, the lowest complexity mapping that has a low discrepancy approximates the target mapping.","paperhash":"anonymous|the_role_of_minimal_complexity_functions_in_unsupervised_learning_of_semantic_mappings","_bibtex":"@article{\n  anonymous2018the,\n  title={The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VjBebR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper585/Authors"],"keywords":["Unsupervised learning","cross-domain mapping","Kolmogorov complexity","Occam's razor"]}},{"tddate":null,"ddate":null,"tmdate":1512222694437,"tcdate":1511788842465,"number":1,"cdate":1511788842465,"id":"Skz4Z5KlG","invitation":"ICLR.cc/2018/Conference/-/Paper585/Official_Review","forum":"H1VjBebR-","replyto":"H1VjBebR-","signatures":["ICLR.cc/2018/Conference/Paper585/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper addresses the problem of learning mappings between different domains without any supervision. It belongs to the recent family of papers based on GANs. ","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the problem of learning mappings between different domains without any supervision. It belongs to the recent family of papers based on GANs.\nThe paper states three conjectures (predictions in the paper):\n1. GAN are sufficient to learn « semantic mappings » in an unsupervised way, if the considered networks are small enough\n2. Controlling the complexity of the network, i.e. the number of the layers, is crucial to come up with what is called « semantic » mappings when learning in an unsupervised way. \nMore precisely there is tradeoff to achieve between the complexity of the model and its simplicity. A rich model is required in order to minimize the discrepancy between the distributions of the domains, while a  not too complex model is necessary to avoid mappings that are not « meaningful ».\n To this aim, the authors  introduce a new notion of function complexity which can be seen as a proxy of Kolmogorov complexity. The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function. \nBased on this definition, and assuming identifiability (i.e. uniqueness up to invariants), and for networks with Leaky ReLU activations,  the authors prove that if the number of mappings which preserve a degree of discrepancy (density preserving in the text) is small, then the  set of « minimal » mappings  of complexity C   that achieve the same degree of  discrepancy is also small. \nThis result is related to the third conjecture of the paper that is :\n3. the number of the number of mappings which preserve a degree of discrepancy  is small.\n\nThe authors also prove a byproduct result stating that identifiability holds for Leaky ReLU networks with one hidden layer.\n\nThe paper  comes with a series of experiments to empirically « demonstrate » the conjectures. \n\nThe paper is well written. The different ideas are clearly stated and discussed, and hence open interesting questions and debates.\n\nSome of these questions that need to be addressed IMHO:\n\n- A critical general question: if the addressed problem is the alignment between e.g. images and not image generation, why not formalizing the problem as a similarity search one (using e.g. EMD or any other transport metric). The alignment task  hence reduces to computing a ranking from this similarity. I have the impression that we use a jackhammer to break a small brick here (no offence). But maybe that I’m missing something here.\n- Several works consider the size and the depth of the network as hyper-parameters to optimize, and this is not new. What is the actual contribution of the paper w.r.t. to this body of work?\n- It is considered that the GAN are trained without any problem, and therefore work in an optimal regime. But the training of the GAN is in itself a problem. How does this affect the paper statements and results?\n- Are the results still valid for another measure of discrepancy based for instance on another measure, e.g. Wasserstein?\n\n\nSome minor remarks :\n- p3: the following sentence is not clear  «  Our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic function. »\n- p6: $C^{\\epsilon_0}_{A,B}$ is used (after Def. 2) before being defined. \n- p7: build->built\n\nSection II :\nA diagram explaining  the different mappings (h_A, h_B, h_AB, etc.) and their spaces (D_A, D_B, D_Z) would greatly help the understanding.\n\nPapers 's pros :\n- clarity\n- technical results\n\ncons:\n- doubts about the interest and originality\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings","abstract":"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.\n\nWe identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.\n\nVarious predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","pdf":"/pdf/2ee302ca9f4fabe532091ee01888d662c8eca540.pdf","TL;DR":"Our hypothesis is that given two domains, the lowest complexity mapping that has a low discrepancy approximates the target mapping.","paperhash":"anonymous|the_role_of_minimal_complexity_functions_in_unsupervised_learning_of_semantic_mappings","_bibtex":"@article{\n  anonymous2018the,\n  title={The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VjBebR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper585/Authors"],"keywords":["Unsupervised learning","cross-domain mapping","Kolmogorov complexity","Occam's razor"]}},{"tddate":null,"ddate":null,"tmdate":1509739217419,"tcdate":1509127579769,"number":585,"cdate":1509739214755,"id":"H1VjBebR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1VjBebR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings","abstract":"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.\n\nWe identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.\n\nVarious predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","pdf":"/pdf/2ee302ca9f4fabe532091ee01888d662c8eca540.pdf","TL;DR":"Our hypothesis is that given two domains, the lowest complexity mapping that has a low discrepancy approximates the target mapping.","paperhash":"anonymous|the_role_of_minimal_complexity_functions_in_unsupervised_learning_of_semantic_mappings","_bibtex":"@article{\n  anonymous2018the,\n  title={The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VjBebR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper585/Authors"],"keywords":["Unsupervised learning","cross-domain mapping","Kolmogorov complexity","Occam's razor"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}