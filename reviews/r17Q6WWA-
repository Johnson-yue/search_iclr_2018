{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222736855,"tcdate":1511973728449,"number":3,"cdate":1511973728449,"id":"SyuPmP3lM","invitation":"ICLR.cc/2018/Conference/-/Paper729/Official_Review","forum":"r17Q6WWA-","replyto":"r17Q6WWA-","signatures":["ICLR.cc/2018/Conference/Paper729/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors propose a collaborative block that can be inserted in any deep network for multi-task learning and evaluate the method on multiple tasks related to Faces","rating":"6: Marginally above acceptance threshold","review":"The collaborative block that authors propose is a generalized module that can be inserted in deep architectures for better multi-task learning. The problem is relevant as we are pushing deep networks to learn representation for multiple tasks. The proposed method while simple is novel. The few places where the paper needs improvement are:\n\n1. The authors should test their collaborative block on multiple tasks where the tasks are less related. Ex: Scene and object classification. The current datasets where the model is evaluated is limited to Faces which is a constrained setting. It would be great if Authors provide more experiments beyond Faces to test the universality of the proposed approach.\n2. The Face datasets are rather small. I wonder if the accuracy improvements hold on larger datasets and if authors can comment on any large scale experiments they have done using the proposed architecture. \n\nIn it's current form I would say the experiment section and large scale experiments are two places where the paper falls short.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection","abstract":"Convolutional neural networks (CNN) have become the most successful and popular approach in many vision-related domains. While CNNs are particularly well-suited for capturing a proper hierarchy of concepts from real-world images, they are limited to domains where data is abundant. Recent attempts have looked into mitigating this data scarcity problem by casting their original single-task problem into a new multi-task learning (MTL) problem. The main goal of this inductive transfer mechanism is to leverage domain-specific information from related tasks, in order to improve generalization on the main task. While recent results in the deep learning (DL) community have shown the promising potential of training task-specific CNNs in a soft parameter sharing framework, integrating the recent DL advances for improving knowledge sharing is still an open problem. In this paper, we propose the Deep Collaboration Network (DCNet), a novel approach for connecting task-specific CNNs in a MTL framework. We define connectivity in terms of two distinct non-linear transformation blocks. One aggregates task-specific features into global features, while the other merges back the global features with each task-specific network. Based on the observation that task relevance depends on depth, our transformation blocks use skip connections as suggested by residual network approaches, to more easily deactivate unrelated task-dependent features. To validate our approach, we employed facial landmark detection (FLD) datasets as they are readily amenable to MTL, given the number of tasks they include. Experimental results show that we can achieve up to 24.31% relative improvement in landmark failure rate over other state-of-the-art MTL approaches. We finally perform an ablation study showing that our approach effectively allows knowledge sharing, by leveraging domain-specific features at particular depths from tasks that we know are related.","pdf":"/pdf/e2ce2f1a8545df021b3fee8845c65b4cdc86ca12.pdf","TL;DR":"We propose a novel approach for connecting task-specific networks in a multi-task learning setting based on recent residual network advances.","paperhash":"anonymous|multitask_learning_by_deep_collaboration_and_application_in_facial_landmark_detection","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17Q6WWA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper729/Authors"],"keywords":["multi-task learning","soft parameter sharing","facial landmark detection"]}},{"tddate":null,"ddate":null,"tmdate":1512222736892,"tcdate":1511833846146,"number":2,"cdate":1511833846146,"id":"ry0lbHclz","invitation":"ICLR.cc/2018/Conference/-/Paper729/Official_Review","forum":"r17Q6WWA-","replyto":"r17Q6WWA-","signatures":["ICLR.cc/2018/Conference/Paper729/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting method; better to do comparison with previous methods","rating":"5: Marginally below acceptance threshold","review":"\n\nThis paper proposes a multi-pathway neural network for facial landmark detection with multitask learning. In particular, each pathway corresponds to one task, and the intermediate features are fused at multiple layers. The fused features are added to the task-specific pathway using a residual connection (the input of the residual connection are the concatenation of the task-specific features and the fuse features). The residual connection allows each pathway to selectively use the information from other pathways and focus on its own task.\n\nThis paper is well written. The proposed neural network architectures are reasonable. \n\nThe residual connection can help each pathway to focus on its own task (suggested by Figure 8). This phenomenon is not guaranteed by the training objective but happens automatically due to the architecture, which is interesting. \n\nThe proposed model outperforms several baseline models. On MTFL, when using the AlexNet, the improvement is significant; when using the ResNet18, the improvement is encouraging but not so significant. On AFLW (trained on MTFL), the improvements are significant in both cases. \n\nWhat is missing is the comparison with other methods (besides the baseline). For examples, it will be helpful to compare with existing non-multitask learning methods, like TCDCN (Zhang et al., 2014) (it seems to achieve 25% failure rate on AFLW, which is lower than the numbers in Figure 5), and  multi-task learning method, like MTCNN (Zhang et al., 2016). It is important to show that proposed multitask learning method is useful in practice. \nIn addition, many papers take the average error as the performance metric. Providing results in the average error can make the experiments more comprehensive.\n\nThe proposed architecture is a bit huge. It scales linearly with the number of tasks, which is not quite preferable. It is also not straightforward to add new tasks to finetune a trained model. \n\nIn Figure 5 (left), it is a bit weird that the pretrained model underperforms the nonpretrained one. \n\nI am likely to change the rating based on the comparison with other methods.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection","abstract":"Convolutional neural networks (CNN) have become the most successful and popular approach in many vision-related domains. While CNNs are particularly well-suited for capturing a proper hierarchy of concepts from real-world images, they are limited to domains where data is abundant. Recent attempts have looked into mitigating this data scarcity problem by casting their original single-task problem into a new multi-task learning (MTL) problem. The main goal of this inductive transfer mechanism is to leverage domain-specific information from related tasks, in order to improve generalization on the main task. While recent results in the deep learning (DL) community have shown the promising potential of training task-specific CNNs in a soft parameter sharing framework, integrating the recent DL advances for improving knowledge sharing is still an open problem. In this paper, we propose the Deep Collaboration Network (DCNet), a novel approach for connecting task-specific CNNs in a MTL framework. We define connectivity in terms of two distinct non-linear transformation blocks. One aggregates task-specific features into global features, while the other merges back the global features with each task-specific network. Based on the observation that task relevance depends on depth, our transformation blocks use skip connections as suggested by residual network approaches, to more easily deactivate unrelated task-dependent features. To validate our approach, we employed facial landmark detection (FLD) datasets as they are readily amenable to MTL, given the number of tasks they include. Experimental results show that we can achieve up to 24.31% relative improvement in landmark failure rate over other state-of-the-art MTL approaches. We finally perform an ablation study showing that our approach effectively allows knowledge sharing, by leveraging domain-specific features at particular depths from tasks that we know are related.","pdf":"/pdf/e2ce2f1a8545df021b3fee8845c65b4cdc86ca12.pdf","TL;DR":"We propose a novel approach for connecting task-specific networks in a multi-task learning setting based on recent residual network advances.","paperhash":"anonymous|multitask_learning_by_deep_collaboration_and_application_in_facial_landmark_detection","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17Q6WWA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper729/Authors"],"keywords":["multi-task learning","soft parameter sharing","facial landmark detection"]}},{"tddate":null,"ddate":null,"tmdate":1512222736934,"tcdate":1511762696231,"number":1,"cdate":1511762696231,"id":"Bylfi7tez","invitation":"ICLR.cc/2018/Conference/-/Paper729/Official_Review","forum":"r17Q6WWA-","replyto":"r17Q6WWA-","signatures":["ICLR.cc/2018/Conference/Paper729/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposed a new block to combine domain-specific information from related tasks, in order to improve generalization of the target tasks. Although the relative improvement seems high (24.31%), its novelty is a little limited, and the target task in this submission(5 landmarks detection) is too simple to prove the effectiveness. ","rating":"5: Marginally below acceptance threshold","review":"Pros:\n1. This paper proposed a new block which can aggregate features from different tasks. By doing this, it can take advantage of common information between related tasks and improve the generalization of target tasks.\n\n2. The achievement in this paper seems good, which is 24.31%.\n\nCons:\n1. The novelty of this submission seems a little limited.\n\n2. The target task utilized in this paper is too simple, which only detects 5 facial landmarks. It is hard to say this proposed work can still work when facing more challenging tasks, for example, 60+ facial landmarks prediction.\n\n3. \" Also, one drawback of HyperFace is that the proposed feature fusion is specific to AlexNet,\" In the original submission, HyperFace is based on AlexNet, but does this mean it can only work on AlexNet?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection","abstract":"Convolutional neural networks (CNN) have become the most successful and popular approach in many vision-related domains. While CNNs are particularly well-suited for capturing a proper hierarchy of concepts from real-world images, they are limited to domains where data is abundant. Recent attempts have looked into mitigating this data scarcity problem by casting their original single-task problem into a new multi-task learning (MTL) problem. The main goal of this inductive transfer mechanism is to leverage domain-specific information from related tasks, in order to improve generalization on the main task. While recent results in the deep learning (DL) community have shown the promising potential of training task-specific CNNs in a soft parameter sharing framework, integrating the recent DL advances for improving knowledge sharing is still an open problem. In this paper, we propose the Deep Collaboration Network (DCNet), a novel approach for connecting task-specific CNNs in a MTL framework. We define connectivity in terms of two distinct non-linear transformation blocks. One aggregates task-specific features into global features, while the other merges back the global features with each task-specific network. Based on the observation that task relevance depends on depth, our transformation blocks use skip connections as suggested by residual network approaches, to more easily deactivate unrelated task-dependent features. To validate our approach, we employed facial landmark detection (FLD) datasets as they are readily amenable to MTL, given the number of tasks they include. Experimental results show that we can achieve up to 24.31% relative improvement in landmark failure rate over other state-of-the-art MTL approaches. We finally perform an ablation study showing that our approach effectively allows knowledge sharing, by leveraging domain-specific features at particular depths from tasks that we know are related.","pdf":"/pdf/e2ce2f1a8545df021b3fee8845c65b4cdc86ca12.pdf","TL;DR":"We propose a novel approach for connecting task-specific networks in a multi-task learning setting based on recent residual network advances.","paperhash":"anonymous|multitask_learning_by_deep_collaboration_and_application_in_facial_landmark_detection","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17Q6WWA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper729/Authors"],"keywords":["multi-task learning","soft parameter sharing","facial landmark detection"]}},{"tddate":null,"ddate":null,"tmdate":1509739136718,"tcdate":1509133595054,"number":729,"cdate":1509739134065,"id":"r17Q6WWA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r17Q6WWA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection","abstract":"Convolutional neural networks (CNN) have become the most successful and popular approach in many vision-related domains. While CNNs are particularly well-suited for capturing a proper hierarchy of concepts from real-world images, they are limited to domains where data is abundant. Recent attempts have looked into mitigating this data scarcity problem by casting their original single-task problem into a new multi-task learning (MTL) problem. The main goal of this inductive transfer mechanism is to leverage domain-specific information from related tasks, in order to improve generalization on the main task. While recent results in the deep learning (DL) community have shown the promising potential of training task-specific CNNs in a soft parameter sharing framework, integrating the recent DL advances for improving knowledge sharing is still an open problem. In this paper, we propose the Deep Collaboration Network (DCNet), a novel approach for connecting task-specific CNNs in a MTL framework. We define connectivity in terms of two distinct non-linear transformation blocks. One aggregates task-specific features into global features, while the other merges back the global features with each task-specific network. Based on the observation that task relevance depends on depth, our transformation blocks use skip connections as suggested by residual network approaches, to more easily deactivate unrelated task-dependent features. To validate our approach, we employed facial landmark detection (FLD) datasets as they are readily amenable to MTL, given the number of tasks they include. Experimental results show that we can achieve up to 24.31% relative improvement in landmark failure rate over other state-of-the-art MTL approaches. We finally perform an ablation study showing that our approach effectively allows knowledge sharing, by leveraging domain-specific features at particular depths from tasks that we know are related.","pdf":"/pdf/e2ce2f1a8545df021b3fee8845c65b4cdc86ca12.pdf","TL;DR":"We propose a novel approach for connecting task-specific networks in a multi-task learning setting based on recent residual network advances.","paperhash":"anonymous|multitask_learning_by_deep_collaboration_and_application_in_facial_landmark_detection","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17Q6WWA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper729/Authors"],"keywords":["multi-task learning","soft parameter sharing","facial landmark detection"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}