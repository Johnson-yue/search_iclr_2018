{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222835247,"tcdate":1512158888849,"number":3,"cdate":1512158888849,"id":"B1ZhIN1Zz","invitation":"ICLR.cc/2018/Conference/-/Paper998/Official_Review","forum":"ry7m6fZRW","replyto":"ry7m6fZRW","signatures":["ICLR.cc/2018/Conference/Paper998/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Authors propose using a clustering based loss function at multiple levels of a deepnet as well as using hierarchical structure of the label space to train better representations.","rating":"6: Marginally above acceptance threshold","review":"While I find the paper in it's current form useful and imformative there are a few places it can be improved. I would like to hear the thoughts from the author on the following points:\n\n1. Using the loss function at multiple layers is already proposed before - ex: http://proceedings.mlr.press/v38/lee15a.pdf. While the motivation in this context is important authors should cite the previous work and talk about how it is different.\n2. Compare AlexNet C performance to a state of the art residual net to really see if pre-training on ImageNet for higher capacity models still fall short to the proposed architecture.\n3. Authors mention \"We find that training with hierarchical labels results in the highest performance beating models with more parameters and approaches the performance of models pre-trained with ImageNet weights. This is a very significant finding since ImageNet contains multiple bird classes, so a model pre- trained with ImageNet has seen many more birds than a model pre-trained with pairwise constraints on Birdsnap34.\" - I am concerned about this conclusion on two fronts - we need to do the experiments on whole of birdsnap to make the experiment truly large scale and we also need to be careful in making assumptions of capacity of networks as pure capacity is not a good metric but rather state of the art architectures that use the capacity effectively is a good metric. For ex: As I mentioned before I would like to see a comparison with ResNet pre trained on ImageNet and AlexNet C rather than AlexNet double.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222835292,"tcdate":1512140991461,"number":2,"cdate":1512140991461,"id":"B1vpxly-G","invitation":"ICLR.cc/2018/Conference/-/Paper998/Official_Review","forum":"ry7m6fZRW","replyto":"ry7m6fZRW","signatures":["ICLR.cc/2018/Conference/Paper998/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Needs revision, better experimental evidence and edits.","rating":"3: Clear rejection","review":"This submission proposes to use hierarchical label information to impose additional losses on the intermediate representation in neural network training. \n\nSummary: \nThe idea of the submission is clear, the execution not well done and the description of the method is lacking details. Form a technical perspective the proposition is not entirely novel, intermediate losses based on hierarchical information have been used before. The experimental part is weak, there is only an introspective view, and an experimental comparison with other approaches and more importantly on comprehensive benchmark datasets is missing, \n\n\nDetails: \nThe current state of the paper is not sufficient for publication. It is not technologically strong. More importantly the method may prove to be useful but the only experimental evidence for this comes from one single network on a constructed dataset (Birdsnap-34). The results on CIFAR10 using LeNet are largely un-informative and could be summarised more concisely. The submission closes with the parts that need be done in order to arrive at an informative and interesting contribution: comparison on compelling benchmarks (e.g., image-net using the synset hierarchy), comparison to strong baseline methods and more architectures. Will the reported finding using AlexNet on a single constructed dataset carry over to more realistic settings? If so, the paper can turn into an interesting observation. \n\nOne obvious baseline is a random hierarchy for both datasets. Better results on manually constructed hierarchies would validate the point that it is the similarity that provides better regularisation. This is the main argument of the submission in the Introduction and this claim is not empirically validated.\n\nMore comments: \n\n- I disagree with the statement that “image classification in computer vision seems to do pretty well without using it (similarity)”. (Page1,paragraph2) I am not sure what this alludes to, visual similarity is the driving force behind computer vision classification. The similarity in the latent representations of higher levels in neural networks is strong enough to perform classification, what else if similarity between visually similar objects gives rise to good classification results? At least this statement is unclear. \n- “We have found that a value of two for the margin..” -> “We have found that a value of 2 for the margin..”\n- Up to Section 2.1 it is unclear whether hierarchical information is needed or not. This is misleading, mainly because of the statements : contrastive loss does not require and explicit hierarchy; we only need weak labels for pairs of instances”. (Page 2,p2) and last sentence on page23: “only requires weak labels in the form of pairwise relationships.”. I understood this sentence that no hierarchy is needed. Later on, hierarchies are introduced (Section3 beginning explains how the hierarchies have been constructed for the datasets. ). So more than one pairwise label is needed between any pair of labels, depending on the level of the hierarchy. I suggest to formulate this better, this is a main part of the paper and the claims read stronger than they are.\n- In Table 1 the Warm start classification is unclear. What exactly is the protocol for this experiment?\n- I suggest to update the table or text to make clear that LeNet has only been used for CIFAR 10 and AlexNet for Birdsnap-34. (If this is the case, the sentence, “…extend their framework…” is ambiguous).\n- Include all performance metrics (Clustering, NMI, Purity) for methods trained with Cross-Entropy\n- The results on CIFAR10 are all very similar and pairwise tests are not included. Is this even a good object to study? I would suggest to remove this entirely and swap with a more challenging setup. I understand that this was probably the start point when developing the software but the results are not informative. \n- AlexNet D performing worse than AlexNet C seems to be a bug, Or is there another possible reason for this ?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1512009938071,"tcdate":1512009938071,"number":2,"cdate":1512009938071,"id":"HJ9RgeTxz","invitation":"ICLR.cc/2018/Conference/-/Paper998/Public_Comment","forum":"ry7m6fZRW","replyto":"ry7m6fZRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproducibility Request ","comment":"\nMy group would like to reproduce your results as part of the ICLR 2018 Reproducibility Challenge (http://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html). \n\nWe want to reproduce your work on the birdsnap34 dataset by applying the same methodology.\n\nWould you be able to provide the code you used for training and testing to help us reproduce your results with higher fidelity?\n\nThank you"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222835332,"tcdate":1511714109700,"number":1,"cdate":1511714109700,"id":"Hy8BTDuef","invitation":"ICLR.cc/2018/Conference/-/Paper998/Official_Review","forum":"ry7m6fZRW","replyto":"ry7m6fZRW","signatures":["ICLR.cc/2018/Conference/Paper998/AnonReviewer3"],"readers":["everyone"],"content":{"title":"lacking in terms of novelty, clarity and experiments","rating":"3: Clear rejection","review":"This paper proposes a method for improving classification performance using similarity over multiple hierarchical levels. The basic idea is to introduce a loss at multiple levels in a deep neural network.\n\nI think this paper is clearly not ready for ICLR. I will elaborate the details below.\n\nNovelty\n======\nThe novelty of this paper is very limited. The loss function in Eq(1-4) of this paper is heavily based on [Hsu & Kira(2015)]. From what I can tell, the difference with respect to [Hsu & Kira(2015)] is that the loss is applied on multiple levels in the neural network. I do not think such minor different is sufficient for an ICLR paper.\n\nAlso the idea of using hierarchical labels for fine-grained recognition has been explored in other papers as well (see below). This paper should compare with this work.\n\nYan et al.: HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition, ICCV 2015.\n\nClarity\n=======\nThe writing of this paper should be significantly improved.\n\n1) The paper actually never explains what \"warm-start\" is. And it never explains how Eq.(1-4) is used to improve the classification. This term \"warm-start\" only appears in the abstract and the conclusion. So it is very difficult to understand what the paper is doing. My guess is that it first learns a clustering based model using Eq.(1-4), then fine-tuned the obtained model for a classification task.\n\n2) In Eq (2), both terms are loss(P,Q). What is the difference between them?\n\n3) Figure 1 is never referenced in the paper. I am guessing Figure 1 probably corresponds to Eq(1-4), but the notations used in Figure 1 (e.g. L_1, L_2) never appeared anywhere in Eq(1-4). So it is very confusing.\n\nExperiments\n===========\nThe experiment is probably weakest part of the paper.\n\n1) There is no comparison with any other hierarchical classification method.\n\n2) This paper is built upon [Hsu & Kira(2015)]. Yet there is not even comparion wit [Hsu & Kira(2015)].\n\n3) Table 1 is very confusing. First of all, there is no explanation on what \"classification\" in the table means. I am guessing this means directly training the a standard neural network classifier. If that is the case, it seems the warm-start classification proposed in the paper is not doing anything. For example, on the Birdsnap-34 dataset, it performs much worse than standard classification (50.32% vs 53.50%). Am I missing something here?\n\n4) Looking at Table 2 and Table 3, it is also not clear what information they are conveying. Again, I assume \"cross-entropy\" means learning a standard classifier. If that is the case, it seems in most cases, \"cross-entropy\" performs as good as other alternatives in the tables. It does not seem the proposed method really offers anything.\n\nOverall, it seems this paper is inconlusive. It is not clear to me that the proposed method actually boosts the classification performance.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1510330046653,"tcdate":1510329879564,"number":1,"cdate":1510329879564,"id":"r1lXRHQyM","invitation":"ICLR.cc/2018/Conference/-/Paper998/Official_Comment","forum":"ry7m6fZRW","replyto":"SyG92QQkf","signatures":["ICLR.cc/2018/Conference/Paper998/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper998/Authors"],"content":{"title":"Updated Appendices with modified dataset information ","comment":"Thank you for your question. We have included the 34 classes below. The class ID and the common name are from the full Birdsnap dataset while the family (coarse-grained) label was annotated by the authors. \n\nClass ID, Common Name, Family\n4,  Golden Eagle, Accipitridae\n13, Common Black-Hawk, Accipitridae\n14, Northern Harrier, Accipitridae\n15, Swallow-tailed Kite, Accipitridae\n16, White-tailed Kite, Accipitridae\n17, Bald Eagle, Accipitridae\n18, Mississippi Kite, Accipitridae\n23, Belted Kingfisher, Alcedinidae\n27, Northern Pintail, Anatidae\n42, Bufflehead, Anatidae\n52, Common Merganser, Anatidae\n57, Brant, Anatidae\n65, Fulvous Whistling-Duck, Anatidae\n70, Great Egret, Ardeidae\n72, American Bittern, Ardeidae\n110, American Golden-Plover, Charadriidae\n115, Rock Pigeon, Columbidae\n125, American Crow, Corvidae\n136, Groove-billed Ani, Cuculidae\n174, Crested Caracara, Falconidae\n182, Evening Grosbeak, Fringillidae\n198, Sandhill Crane, Gruidae\n201, Barn Swallow, Hirundinidae\n230, Herring Gull, Laridae\n252, Least Tern, Laridae\n322, Wild Turkey, Phasianidae\n323, Ring-necked Pheasant, Phasianidae\n324, Ruffed Grouse, Phasianidae\n334, Northern Flicker, Picidae\n381, Ruddy Turnstone, Scolopacidae\n397, Short-billed Dowitcher, Scolopacidae\n399, Marbled Godwit, Scolopacidae\n429, White Ibis, Threskiornithidae\n432, Black-chinned Hummingbird, Trochilidae\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1510321290159,"tcdate":1510321290159,"number":1,"cdate":1510321290159,"id":"SyG92QQkf","invitation":"ICLR.cc/2018/Conference/-/Paper998/Public_Comment","forum":"ry7m6fZRW","replyto":"ry7m6fZRW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Modified dataset used for the experiment","comment":"Can you provide the subset of the birdsnap dataset you used for the experiment ? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092382701,"tcdate":1509137691718,"number":998,"cdate":1510092360803,"id":"ry7m6fZRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry7m6fZRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Cluster-based Warm-Start Nets","abstract":"Theories in cognitive psychology postulate that humans use similarity as a basis\nfor object categorization. However, work in image classification generally as-\nsumes disjoint and equally dissimilar classes to achieve super-human levels of\nperformance on certain datasets. In our work, we adapt notions of similarity using\nweak labels over multiple hierarchical levels to boost classification performance.\nInstead of pitting clustering directly against classification, we use a warm-start\nbased evaluation to explicitly provide value to a clustering representation by its\nability to aid classification. We evaluate on CIFAR10 and a fine-grained classifi-\ncation dataset to show improvements in performance with the procedural addition\nof intermediate losses and weak labels based on multiple hierarchy levels. Further-\nmore, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\ntion with intermediate losses outperforms a classification baseline by over 17% on\na subset of Birdsnap dataset. Finally, we show improvement over AlexNet trained\nusing ImageNet pre-trained weights as initializations which further supports our \nclaim of the importance of similarity.","pdf":"/pdf/1b36b2c77f1d79b4e8e8483cd5f9cdced3d6444c.pdf","TL;DR":"Cluster before you classify; using weak labels to improve classification ","paperhash":"anonymous|clusterbased_warmstart_nets","_bibtex":"@article{\n  anonymous2018cluster-based,\n  title={Cluster-based Warm-Start Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry7m6fZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper998/Authors"],"keywords":["hierarchical labels","weak labels","pairwise constraints","clustering","classification"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}