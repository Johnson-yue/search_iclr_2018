{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222667564,"tcdate":1511828330508,"number":3,"cdate":1511828330508,"id":"rkf_j7cgG","invitation":"ICLR.cc/2018/Conference/-/Paper485/Official_Review","forum":"rJ1RPJWAW","replyto":"rJ1RPJWAW","signatures":["ICLR.cc/2018/Conference/Paper485/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper poses interesting questions, but learnability doesn't provide many answers","rating":"4: Ok but not good enough - rejection","review":"Review Summary:\nThe primary claim that there is \"a strong correlation between small generalization errors and high learnability\" is correct and supported by evidence, but it doesn't provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. In fact the relationship between test accuracy and learnability seems quite obvious, which unfortunately undermines the usefulness of the learnability metric which is used in many experiments in the paper.\n\nFor example, consider the results in Table 7. A small network (N1 = 16 neurons) with low test accuracy results in a low learnability, while a large network (N1 = 1024 neurons) gets a higher test accuracy and higher learnability. In this case, the small network can be thought of as applying higher label noise relative to the larger network. Thus it is expected that agreement between N1 and N2 (learnability) will be higher for the larger network, as the predictions of N1 are less noisy. More importantly, this relationship between test accuracy and learnability doesn't answer the original question Q2 posed: \"Do larger neural networks learn simpler patterns compared to neural networks when trained on real data\". It instead draws some obvious conclusions about noisy labeling of training data.\n\nOther results presented in the paper are puzzling and require further experimentation and discussion, such as the trend that the learnability of shallow networks on random data is much higher than 10%, as discussed at the bottom of page 4. The authors provide some possible reasoning, stating that this strange effect could be due to class imbalance, but it isn't convincing enough.\n\nOther comments:\n-Section 3.4 is unrelated to the primary arguments of the paper and seems like a filler.\n-Equations should have equation numbers\n-Learnability numbers reported in all tables should be between 0-1 per the definition on page 3\n-As suggested in the final sentence of the discussion, it would be nice if conclusions drawn from the learnability experiments done in this paper were applied to the design new networks which better generalize","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learnability of Learned Neural Networks","abstract":"This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large minibatch size vs small minibatch size. The notion of simplicity used here is that of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from (in fact often higher than) test accuracy, the results herein suggest that there is a strong correlation between small generalization errors and high learnability.\nThis work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned neural networks might shed light on the right assumptions that can be made for a theoretical study of deep learning.","pdf":"/pdf/8581d56f20f60cfe34788f469525820ffb1b9265.pdf","TL;DR":"Exploring the Learnability of Learned Neural Networks","paperhash":"anonymous|learnability_of_learned_neural_networks","_bibtex":"@article{\n  anonymous2018learnability,\n  title={Learnability of Learned Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ1RPJWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper485/Authors"],"keywords":["Learnability","Generalizability","Understanding Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222667612,"tcdate":1511820129791,"number":2,"cdate":1511820129791,"id":"BJ9PiZ9eG","invitation":"ICLR.cc/2018/Conference/-/Paper485/Official_Review","forum":"rJ1RPJWAW","replyto":"rJ1RPJWAW","signatures":["ICLR.cc/2018/Conference/Paper485/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This is an interesting empirical attempt at understanding the reproducibility of learned neural networks. Great ideas but needs more work.","rating":"4: Ok but not good enough - rejection","review":"The proposed approach to figure out what do deep network learn is interesting -- the approach of learning a learned network. Some aspects needs more work to improve the work. The presentation of the results can be improved further. \n\nFirstly, confidence intervals on many experiments are missing (including Tables 3-9). Also, since we are looking at empirically validating the learnability criterion defined by the authors, all the results (the reported confusion tables) need to be tested statistically (to see whether one dominates the other). \n\nWhat is random label learning of N1 telling us? How different would that be in terms of simply learning random labels on real data directly. Further, the evaluations in Tables 3-6 need more attention since we are interested in the TLP=1 vs. PLP=0 case, and TLP=0 vs. PLP=1 case. \n\nThe influence of depth is not clear -- may be it is because of the way results are reported here. A simple figure with increasing layers vs. learnability values would do a better job at conveying the trends. \n\nThe evaluations in Section 3.5 are not conclusive? What is the question being tested for here? \n\nWhat about the influence of number of classes on learnability trends? Some experiments on large class datasets including cifar100 and/or imagenet need to be reported. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learnability of Learned Neural Networks","abstract":"This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large minibatch size vs small minibatch size. The notion of simplicity used here is that of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from (in fact often higher than) test accuracy, the results herein suggest that there is a strong correlation between small generalization errors and high learnability.\nThis work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned neural networks might shed light on the right assumptions that can be made for a theoretical study of deep learning.","pdf":"/pdf/8581d56f20f60cfe34788f469525820ffb1b9265.pdf","TL;DR":"Exploring the Learnability of Learned Neural Networks","paperhash":"anonymous|learnability_of_learned_neural_networks","_bibtex":"@article{\n  anonymous2018learnability,\n  title={Learnability of Learned Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ1RPJWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper485/Authors"],"keywords":["Learnability","Generalizability","Understanding Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222669010,"tcdate":1511819818599,"number":1,"cdate":1511819818599,"id":"H17N5b5lf","invitation":"ICLR.cc/2018/Conference/-/Paper485/Official_Review","forum":"rJ1RPJWAW","replyto":"rJ1RPJWAW","signatures":["ICLR.cc/2018/Conference/Paper485/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Very nice paper showing how large networks can actually be \"simple\", in spite of their large capacity.","rating":"7: Good paper, accept","review":"Summary:\nThis paper presents very nice experiments comparing the complexity of various different neural networks using the notion of \"learnability\" --- the learnability of a model (N1) is defined as the \"expected agreement\" between the output of N1, and the output of another model N2 which has been trained to match N1 (on a dataset of size n).  The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is --- furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model.\n\nThe paper presents a number of interesting results:\n1) Larger networks are typically more learnable than smaller ones (typically we think of larger networks as being MORE complicated than smaller networks -- this result suggests that in an important sense, large networks are simpler).\n2) Networks trained with random data are significantly less learnable than networks trained on real data.\n3) Networks trained on small mini-batches (larger variance SGD updates) are more learnable than those trained on large minibatches.\n\nThese results are in line with several of the observations made by Zhang et al (2017), which showed that neural networks are able to both (a) fit random data, and (b) generalize well; these results at first seem to run counter to the ideas from statistical learning theory that models with high capacity (VC dimension, radamacher complexity, etc.) have much weaker generalization guarantees than lower capacity models.  These results suggest that models that have high capacity (by one definition) are also capable of being simple (by another definition).  These results nicely complement the work which studies the \"sharpness/curvature\" of the local minima found by neural networks, which argue that the minima which generalize better are those with lower curvature.\n\nReview:\nQuality:  I found this to be high quality work. The paper presents many results across a variety of network architectures.  One area for improvement is presenting results on larger datasets (currently all experiments are on CIFAR-10), and/or on non-convolutional architectures.  Additionally, a discussion of why learnabiblity might imply low generalization error would have been interesting (the more formal, the better), though it is unclear how difficult this would be.\n\nClarity:  The paper is written clearly.  A small point: Step 2 in section 3.1 should specify that argmax of N1(D2) is used to generate labels for the training of the second network.  Also, what dataset D_i is used for tables 3-6? Please specify.\n\nOriginality: The specific questions tackled in this paper are original (learnability on random vs. real data, large vs. small networks, and large vs. small mini-batch training).  But it is unclear to me exactly how original this use of \"learnability\" is in evaluating how simple a model is.  It seems to me that this particular use of \"learnability\" is original, even though PAC learnability was defined a while ago.\n\nSignificance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize.  I believe it is important to find new ways of formally defining the \"simplicity/capacity\" of a model, such that \"simpler\" models can be proven to have smaller generalization gap (between train and test error) relative to more \"complicated\" models. It is clear that VC dimension and radamacher complexity alone are not enough to explain the generalization performance of neural networks, and that neural networks with high capacity by these definitions are likely \"simple\" by other definitions (as we have seen in this paper).  This paper makes an important contribution to this conversation, and could perhaps provide a starting point for theoreticians to better explain why deep networks generalize well.\n\nPros\n- nice experiments, with very interesting results.\n- Helps explain one way in which large networks are in fact \"simple\"\n\nCons\n- The paper does not attempt to relate the notion of learnability to that of generalization performance.  All it says is that these two metrics appear to be well correlated.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learnability of Learned Neural Networks","abstract":"This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large minibatch size vs small minibatch size. The notion of simplicity used here is that of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from (in fact often higher than) test accuracy, the results herein suggest that there is a strong correlation between small generalization errors and high learnability.\nThis work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned neural networks might shed light on the right assumptions that can be made for a theoretical study of deep learning.","pdf":"/pdf/8581d56f20f60cfe34788f469525820ffb1b9265.pdf","TL;DR":"Exploring the Learnability of Learned Neural Networks","paperhash":"anonymous|learnability_of_learned_neural_networks","_bibtex":"@article{\n  anonymous2018learnability,\n  title={Learnability of Learned Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ1RPJWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper485/Authors"],"keywords":["Learnability","Generalizability","Understanding Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739276682,"tcdate":1509124038857,"number":485,"cdate":1509739274024,"id":"rJ1RPJWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJ1RPJWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learnability of Learned Neural Networks","abstract":"This paper explores the simplicity of learned neural networks under various settings: learned on real vs random data, varying size/architecture and using large minibatch size vs small minibatch size. The notion of simplicity used here is that of learnability i.e., how accurately can the prediction function of a neural network be learned from labeled samples from it. While learnability is different from (in fact often higher than) test accuracy, the results herein suggest that there is a strong correlation between small generalization errors and high learnability.\nThis work also shows that there exist significant qualitative differences in shallow networks as compared to popular deep networks. More broadly, this paper extends in a new direction, previous work on understanding the properties of learned neural networks. Our hope is that such an empirical study of understanding learned neural networks might shed light on the right assumptions that can be made for a theoretical study of deep learning.","pdf":"/pdf/8581d56f20f60cfe34788f469525820ffb1b9265.pdf","TL;DR":"Exploring the Learnability of Learned Neural Networks","paperhash":"anonymous|learnability_of_learned_neural_networks","_bibtex":"@article{\n  anonymous2018learnability,\n  title={Learnability of Learned Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ1RPJWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper485/Authors"],"keywords":["Learnability","Generalizability","Understanding Deep Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}