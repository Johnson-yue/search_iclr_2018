{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222662176,"tcdate":1511784687004,"number":3,"cdate":1511784687004,"id":"ByvgbYFeG","invitation":"ICLR.cc/2018/Conference/-/Paper470/Official_Review","forum":"rJlMAAeC-","replyto":"rJlMAAeC-","signatures":["ICLR.cc/2018/Conference/Paper470/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good Paper","rating":"7: Good paper, accept","review":"The authors propose a variant of the neural programmer-interpreter that can support so called combinators for composing an d structuring computations. In a sense, programs in this variant are at a higher level than those in the original neural programmer-interpreter. The distinguishing aspect of the neural programmer-interpreter is that it learns a generic core (which in the variant of the paper corresponds to an interpreter of the programming language) and programs for concrete tasks simultaneously. Increasing the expressivity of the language with combinators has a danger of making the training of core very difficult. The authors avoids this pitfall by carefully re-designing the deterministic part of the core. For instance, they separate out the evaluation of the detector from the LSTM used for the core. Also, they use a fixed routine for parsing the applier instruction. The authors describe two ways of training their variant of the neural programmer-interpreter. The first is similar to the existing methods, and trains the variant using traces. The second is different and trains the variant using just input-output pairs but under carefully designed curriculum. The authors experimentally show that their approach leads to a more stable core of the neural programmer-interpreter that is close to being universal, in the sense that the core knows how to interpret commands.\n\nI found the new architecture of the neural programmer-interpreter very interesting. It is carefully crafted so as to support expressive combinators without making the learning more difficult. I can't quite judge how strong their experimental evaluations are, but I think that learning a neural programmer-interpreter from just input-output pairs using RL techniques is new and worth being pursued further. I am generally positive about accepting this paper to ICLR'18.\n\nI have three complaints, though. First, the paper uses 14 pages well over 8 pages, the recommended limit. Second, it has many typos. Third, the authors claim universality of the approach. When I read this claim, I expected a theorem initially but later I realized that the claim was mostly about informal understanding and got disappointed slightly. I hope that the authors consider these complaints when they revise the paper.\n\n* abstract, p1: is is universal -> is universal\n* p2: may still intractable to provable -> may still be intractable to prove\n* p2: import abstraction -> important abstraction\n* p2: a_(t+1)are -> a_(t+1) are\n* p2: Algorithm 1 The -> Algorithm 1. The\n* Algorithm1, p3: f_lstm(c,p,h) -> f_lstm(s,p,h)\n* p3: learn to interpreting -> learn to interpret\n* p3: it it common -> it is common\n* p3: The two program share -> The two programs share\n* p3: that server as -> that serve as\n* p3: be interpret by -> be interpreted by\n* p3: (le 9 in our -> (<= 9 in our\n* Figure 1, p4: the type of linrec is wrong.\n* p6: f_d et -> f_det\n* p8: it+1 -> i_(t+1)\n* p8: detector. the -> detector. The\n* p9: As I mentioned, I suggest you to make clear that the claim about universality is mostly based on intuition, not on theorem.\n* p9: to to -> to\n* p10: the the set -> the set\n* p11: What are DETs?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction","abstract":"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction can dramatically reduce the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, it is possible to train the CNPI by policy gradient reinforcement learning with an appropriately designed curriculum.","pdf":"/pdf/8b27b457d505b7a95892ecc57ed1ec9ea1ea7c5e.pdf","paperhash":"anonymous|improving_the_universality_and_learnability_of_neural_programmerinterpreters_with_combinator_abstraction","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJlMAAeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper470/Authors"],"keywords":["neural programming","Neural Programmer-Interpreter"]}},{"tddate":null,"ddate":null,"tmdate":1512222663452,"tcdate":1511550563363,"number":2,"cdate":1511550563363,"id":"BkswAkLlG","invitation":"ICLR.cc/2018/Conference/-/Paper470/Official_Review","forum":"rJlMAAeC-","replyto":"rJlMAAeC-","signatures":["ICLR.cc/2018/Conference/Paper470/AnonReviewer3"],"readers":["everyone"],"content":{"title":"promising use of functional programming ideas in neural program induction; model description needs clarification","rating":"7: Good paper, accept","review":"Quality\nThe paper is very interesting and clearly motivated. The idea of importing concepts from functional programming into neural programming looks very promising, helping to address a bit the somewhat naive approach taken so far in the deep learning community towards program induction. However, I found the model description difficult to fully understand and have significant unresolved questions - especially *why* exactly the model should be expected to have better universality compared to NPI and RNPI, given than applier memory is unbounded just like NPI/RNPI program memories are unbounded.\n\nClarity\nThe paper does a good job of summarizing NPI and motivating the universality property of the core module. \n\nI had a lot of questions while reading:\n\nWhat is the purpose of detectors? It is not clear what is being detected. From the context it seems to be encoding observations from the environment, which can vary according to the task and change during program execution. The detector memory is also confusing. In the original NPI, it is assumed that the caller knows which encoder is needed for each program. In CNPI, is this part learned or more general in some way?\n\nAppliers - is it the case that *every* program apart from the four combinators must be written as an applier? For example ADD1, BSTEP, BUBBLESORT, etc all must be implemented as an applier, and programs that cannot be implemented as appliers are not expressible by CNPI?\n\nMemory - combinator memory looks like a 4-way softmax over the four combinators, right? The previous NPI program memory is analogous then to the applier memory.\n\nEqn 3 - binarizing the detector output introduces a non-differentiable operation. How is the detector then trained e.g. from execution traces? Later I see that there is a notion of a “correct condition” for the detector to regress on, which makes me confused again about what exactly the output of a detector means.\n\nComputing the next subprogram - since the size of applier memory is unbounded, the core still needs to be aware of an unlimited number of subprograms. I must be missing something here - how does the proposed model therefore achieve better universality than the original NPI and RNPI models?\n\nAnalysis - for the claim of perfect generalization, I think this will not generally hold true for perceptual inputs. Will the proposed model only be useful in discrete domains for algorithmic tasks, or could it be more broadly applicable, e.g. to robotics tasks?\n\nOriginality\nThis methods proposed in this paper are quite novel and start to bridge an important gap between neural program induction and functional programming, by importing the concept of combinator abstraction into NPI.\n\nSignificance\nThe paper will be significant to people interested in NPI-related models and neural program induction generally, but on the other hand, there is currently not yet a “killer application” to this line of work. \n\nThe experiments appear to show significant new capabilities of CNPI compared to NPI and RNPI in terms of better generalization and universality, as well as being trainable by reinforcement learning.\n\nPros\n- Learns new programs without catastrophic forgetting in the NPI core, in particular where previous NPI models fail.\n- Detector training is decoupled from core and memory training, so that perfect generalization does not have to be re-verified after learning new behaviors.\n\nCons\n- So far lacking useful applications in the real world. Could the techniques in this paper help in robotics extensions to NPI? (see e.g. https://arxiv.org/abs/1710.01813)\n- Adds a significant amount of further structure into the NPI framework, which could potentially make broader applications more complex to implement. Do the proposed modifications reduce generality in any way?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction","abstract":"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction can dramatically reduce the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, it is possible to train the CNPI by policy gradient reinforcement learning with an appropriately designed curriculum.","pdf":"/pdf/8b27b457d505b7a95892ecc57ed1ec9ea1ea7c5e.pdf","paperhash":"anonymous|improving_the_universality_and_learnability_of_neural_programmerinterpreters_with_combinator_abstraction","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJlMAAeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper470/Authors"],"keywords":["neural programming","Neural Programmer-Interpreter"]}},{"tddate":null,"ddate":null,"tmdate":1512222663494,"tcdate":1511512997791,"number":1,"cdate":1511512997791,"id":"rkRojIHxz","invitation":"ICLR.cc/2018/Conference/-/Paper470/Official_Review","forum":"rJlMAAeC-","replyto":"rJlMAAeC-","signatures":["ICLR.cc/2018/Conference/Paper470/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper clearly breaks the submission guidelines. The paper is far too long, 14 pages (+refs and appendix, in total 19 pages), while the page limit is 8 pages (+refs and appendix). Therefore, the paper should be rejected. ","rating":"3: Clear rejection","review":"The paper is interesting to read and gives valuable insights. \n\nHowever, the paper clearly breaks the submission guidelines. The paper is far too long, 14 pages (+refs and appendix, in total 19 pages), while the page limit is 8 pages (+refs and appendix). Therefore, the paper should be rejected. I can not foresee how the authors should be able to squeeze to content into 8 pages. The paper is more suitable for a journal, where page limit is less of an issue.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction","abstract":"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction can dramatically reduce the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, it is possible to train the CNPI by policy gradient reinforcement learning with an appropriately designed curriculum.","pdf":"/pdf/8b27b457d505b7a95892ecc57ed1ec9ea1ea7c5e.pdf","paperhash":"anonymous|improving_the_universality_and_learnability_of_neural_programmerinterpreters_with_combinator_abstraction","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJlMAAeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper470/Authors"],"keywords":["neural programming","Neural Programmer-Interpreter"]}},{"tddate":null,"ddate":null,"tmdate":1509739284471,"tcdate":1509121544278,"number":470,"cdate":1509739281821,"id":"rJlMAAeC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJlMAAeC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction","abstract":"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction can dramatically reduce the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, it is possible to train the CNPI by policy gradient reinforcement learning with an appropriately designed curriculum.","pdf":"/pdf/8b27b457d505b7a95892ecc57ed1ec9ea1ea7c5e.pdf","paperhash":"anonymous|improving_the_universality_and_learnability_of_neural_programmerinterpreters_with_combinator_abstraction","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJlMAAeC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper470/Authors"],"keywords":["neural programming","Neural Programmer-Interpreter"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}